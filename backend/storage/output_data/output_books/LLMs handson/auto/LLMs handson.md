# Hands-On Large Language Models

Language Understanding and Generation

![## Image Analysis: 84cefeebe1d95d704bc6a77518329e97a55bea30393e4aa2c99fa20f16a5e840.jpg

**Conceptual Understanding:**
This image conceptually represents an illustration of a kangaroo. The main purpose of the image is to provide a detailed visual depiction of a kangaroo from its side profile. It communicates the physical characteristics of the animal, including its fur color (browns and beiges), texture (shaggy, fine strokes), and posture (upright on hind legs with tail for balance, small forelegs tucked). The visible text fragment 'eration' in the top-left corner suggests this image is part of a larger document, possibly an 'Illustration' or a section on 'Operation' or 'Generation', though the exact meaning without the full word is ambiguous and does not directly relate to the kangaroo visually except as contextual metadata for the document it appears in.

**Content Interpretation:**
The image conceptually represents an anatomical or illustrative depiction of a kangaroo. Its main purpose is to visually present the physical form of this animal, likely as a static example or a point of reference within a document. The illustration highlights the characteristic features of a kangaroo, such as its powerful hind legs, distinctive tail, and fur texture. There are no processes, relationships, or systems being shown; rather, it is a static illustration of a biological subject. The extracted text 'eration' suggests this image might be part of a larger heading or text, potentially relating to broader document themes such as 'Illustration' or 'Operation', though its direct relevance to the kangaroo itself is not immediately clear from the fragment.

**Key Insights:**
The primary takeaway from this image is the visual representation of a kangaroo, emphasizing its physical characteristics like fur texture, powerful hind legs, and balancing tail. The partial text 'eration' indicates that the image is embedded within a larger document that includes textual headings or content, suggesting that the image itself is not standalone. The key insight is that this is a detailed illustration of an animal. The presence of 'eration' as the only text might imply it is a fragment of a larger, relevant title or caption related to the document's content, even if the kangaroo itself doesn't directly relate to LLMs without further context. The image teaches visual identification of a kangaroo. The text fragment 'eration' provides evidence that this image is part of a larger written work, likely beneath a heading or title, even though the full context of that text is missing.

**Document Context:**
Given the document context 'Hands-On Large Language Models', the image of a kangaroo, along with the partial text 'eration', seems to be an illustrative element rather than a direct process diagram for LLMs. It could serve several purposes: 1. As an example image for an LLM to 'describe' or 'analyze' in a practical exercise. 2. Part of a broader conceptual discussion where a kangaroo is used metaphorically or as a dataset example. 3. The word fragment 'eration' might be part of a term like 'generation' (e.g., text generation, image generation) or 'operation' relevant to LLM processes, and the kangaroo image is an unrelated illustration used in proximity. The clear, comprehensive explanation details the visual characteristics of the kangaroo, emphasizing its brown and beige fur, powerful hind legs, tail, and the presence of a partial word 'eration' in the top-left corner. This description provides a complete textual representation of the image's content, allowing a reader to visualize the image accurately and understand the limited textual context.

**Summary:**
The image displays a detailed illustration of a kangaroo, seen from its left side, with its body facing right. The kangaroo is predominantly brown with lighter beige or tan fur on its underbelly, inner legs, and the underside of its tail. Its fur is depicted with numerous fine, dark strokes, giving it a textured, somewhat shaggy appearance. The outline of the animal is sharply defined with a dark, thin line. The kangaroo stands upright on its powerful hind legs, with its large feet clearly visible on the ground. Its tail extends behind it, acting as a counterbalance. Its smaller forelegs are tucked close to its chest. The background is solid white, and a faint grey shadow is cast beneath the kangaroo, suggesting depth and grounding the illustration. In the top-left corner, a partial word, 'eration', is visible.](images/84cefeebe1d95d704bc6a77518329e97a55bea30393e4aa2c99fa20f16a5e840.jpg)

# Hands-On Large Language Models

AI has acquired startling new language capabilities in just the past few years. Driven by rapid advances in deep learning, language AI systems are able to write and understand text better than ever before. This trend is enabling new features, products, and entire industries. Through this book’s visually educational nature, readers will learn practical tools and concepts they need to use these capabilities today.

You’ll understand how to use pretrained large language models for use cases like copywriting and summarization; create semantic search systems that go beyond keyword matching; and use existing libraries and pretrained models for text classification, search, and clusterings.

This book also helps you:

• Understand the architecture of Transformer language models that excel at text generation and representation   
• Build advanced LLM pipelines to cluster text documents and explore the topics they cover   
• Build semantic search engines that go beyond keyword search, using methods like dense retrieval and rerankers   
• Explore how generative models can be used, from prompt engineering all the way to retrieval-augmented generation   
• Gain a deeper understanding of how to train LLMs and optimize them for specific applications using generative model fine-tuning, contrastive fine-tuning, and in-context learning

“Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics. Their book is a valuable resource for anyone looking to understand the main techniques behind how large language models are built.”

—Andrew Ng founder of DeepLearning.AI “I can’t think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.” —Josh Starmer, StatQuest

Jay Alammar is director and engineering fellow at Cohere.

Maarten Grootendorst is senior clinical data scientist at the Netherlands Comprehensive Cancer Organization (IKNL).

# Praise for Hands-On Large Language Models

This is an exceptional guide to the world of language models and their practical   
applications in industry. Its highly-visual coverage of generative, representational, and   
retrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended!

—Nils Reimers, Director of Machine Learning at Cohere | creator of sentence-transformers

Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built.

—Andrew Ng, founder of DeepLearning.AI

I can’t think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.

—Josh Starmer, StatQuest

If you’re looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology!

—Luis Serrano, PhD, Founder and CEO of Serrano Academy

This book is a must-read for anyone interested in the rapidly-evolving field of generative AI. With a focus on both text and visual embeddings, it’s a great blend of algorithmic evolution, theoretical rigor, and practical guidance. Whether you are a student, researcher, or industry professional, this book will equip you with the use cases and solutions needed to level-up your knowledge of generative AI. Well done!

—Chris Fregly, Principal Solution Architect, Generative AI at AWS

In the heart of the GenAI revolution, this indispensable guide masterfully balances theory and practice, navigating the vast landscape of large language models to equip readers with the knowledge needed for immediate and transformative impact in the field of AI.

—Tarun Narayanan Venkatachalam, AI Researcher, University of Washington

Timely reading to get hands-on experience with language models.

—Emir Muñoz, Genesys

Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books   
leave abstract. The book starts with simple introductory beginnings, and steadily builds in   
scope. By the final chapters, you will be fine-tuning and building your own large language models with confidence.

—Leland McInnes, Researcher at the Tutte Institute for Mathematics and Computing

Finally, a book that not only avoids superficial coverage of large language models but also thoroughly explores the background in a way that is both accessible and engaging. The authors have masterfully created a definitive guide that will remain essential reading despite the fast-paced advancements in the field.

—Prof. DDr. Roman Egger, CEO of Smartvisions.at and Modul University Vienna

# Hands-On Large Language Models Language Understanding and Generation

Jay Alammar and Maarten Grootendorst

# Hands-On Large Language Models

by Jay Alammar and Maarten Grootendorst

Copyright $^ ©$ 2024 Jay Alammar and Maarten Pieter Grootendorst. All rights reserved.

Printed in the United States of America.

Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.

O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com.

Acquisitions Editor: Nicole Butterfield Development Editor: Michele Cronin Production Editor: Ashley Stussy Copyeditor: Charles Roumeliotis Proofreader: Kim Cofer

Indexer: BIM Creatives, LLC Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea

September 2024: First Edition

Revision History for the First Edition 2024-09-10: First Release

See http://oreilly.com/catalog/errata.csp?isbn 9781098150969 for release details.

The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-On Large Language Models, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.

The views expressed in this work are those of the authors and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights.

# Table of Contents

# Part I. Understanding Language Models

1. An Introduction to Large Language Models. . . 3

What Is Language AI? 4   
A Recent History of Language AI 5   
Representing Language as a Bag-of-Words 6   
Better Representations with Dense Vector Embeddings 8   
Types of Embeddings 10   
Encoding and Decoding Context with Attention 11   
Attention Is All You Need 15   
Representation Models: Encoder-Only Models 18   
Generative Models: Decoder-Only Models 20   
The Year of Generative AI 23   
The Moving Definition of a “Large Language Model” 25   
The Training Paradigm of Large Language Models 25   
Large Language Model Applications: What Makes Them So Useful? 27   
Responsible LLM Development and Usage 28   
Limited Resources Are All You Need 28   
Interfacing with Large Language Models 29   
Proprietary, Private Models 29   
Open Models 30   
Open Source Frameworks 31   
Generating Your First Text 32   
Summary 34

# 2. Tokens and Embeddings. . . . . . . . 37

LLM Tokenization 38   
How Tokenizers Prepare the Inputs to the Language Model 38   
Downloading and Running an LLM 39   
How Does the Tokenizer Break Down Text? 43   
Word Versus Subword Versus Character Versus Byte Tokens 44   
Comparing Trained LLM Tokenizers 46   
Tokenizer Properties 55   
Token Embeddings 57   
A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer 57   
Creating Contextualized Word Embeddings with Language Models 58   
Text Embeddings (for Sentences and Whole Documents) 61   
Word Embeddings Beyond LLMs 63   
Using pretrained Word Embeddings 63   
The Word2vec Algorithm and Contrastive Training 64   
Embeddings for Recommendation Systems 67   
Recommending Songs by Embeddings 67   
Training a Song Embedding Model 69   
Summary 71

# 3. Looking Inside Large Language Models. . . . . 73

An Overview of Transformer Models 74   
The Inputs and Outputs of a Trained Transformer LLM 74   
The Components of the Forward Pass 76   
Choosing a Single Token from the Probability Distribution (Sampling/   
Decoding) 79   
Parallel Token Processing and Context Size 81   
Speeding Up Generation by Caching Keys and Values 83   
Inside the Transformer Block 85   
Recent Improvements to the Transformer Architecture 95   
More Efficient Attention 96   
The Transformer Block 101   
Positional Embeddings (RoPE) 102   
Other Architectural Experiments and Improvements 105   
Summary 106

# Part II. Using Pretrained Language Models

4. Text Classification. . . . 111   
The Sentiment of Movie Reviews 112   
Text Classification with Representation Models 113   
Model Selection 115   
Using a Task-Specific Model 116   
Classification Tasks That Leverage Embeddings 120   
Supervised Classification 121   
What If We Do Not Have Labeled Data? 123   
Text Classification with Generative Models 127   
Using the Text-to-Text Transfer Transformer 128   
ChatGPT for Classification 132   
Summary 135

# 5. Text Clustering and Topic Modeling. . . . . 137

ArXiv’s Articles: Computation and Language 138   
A Common Pipeline for Text Clustering 139   
Embedding Documents 139   
Reducing the Dimensionality of Embeddings 140   
Cluster the Reduced Embeddings 142   
Inspecting the Clusters 144   
From Text Clustering to Topic Modeling 146   
BERTopic: A Modular Topic Modeling Framework 148   
Adding a Special Lego Block 156   
The Text Generation Lego Block 160   
Summary 164

# 6. Prompt Engineering. . . . . 167

Using Text Generation Models 167   
Choosing a Text Generation Model 167   
Loading a Text Generation Model 168   
Controlling Model Output 170   
Intro to Prompt Engineering 173   
The Basic Ingredients of a Prompt 173   
Instruction-Based Prompting 175   
Advanced Prompt Engineering 177   
The Potential Complexity of a Prompt 177   
In-Context Learning: Providing Examples 180   
Chain Prompting: Breaking up the Problem 182   
Reasoning with Generative Models 184   
Chain-of-Thought: Think Before Answering 185   
Self-Consistency: Sampling Outputs 188   
Tree-of-Thought: Exploring Intermediate Steps 189   
Output Verification 191   
Providing Examples 192   
Grammar: Constrained Sampling 194

# 7. Advanced Text Generation Techniques and Tools. . . . 199

Model I/O: Loading Quantized Models with LangChain 200   
Chains: Extending the Capabilities of LLMs 202   
A Single Link in the Chain: Prompt Template 203   
A Chain with Multiple Prompts 206   
Memory: Helping LLMs to Remember Conversations 209   
Conversation Buffer 210   
Windowed Conversation Buffer 212   
Conversation Summary 214   
Agents: Creating a System of LLMs 218   
The Driving Power Behind Agents: Step-by-step Reasoning 219   
ReAct in LangChain 221   
Summary 224

# 8. Semantic Search and Retrieval-Augmented Generation. . . . . 225

Overview of Semantic Search and RAG 226   
Semantic Search with Language Models 228   
Dense Retrieval 228   
Reranking 240   
Retrieval Evaluation Metrics 244   
Retrieval-Augmented Generation (RAG) 249   
From Search to RAG 250   
Example: Grounded Generation with an LLM API 252   
Example: RAG with Local Models 252   
Advanced RAG Techniques 255   
RAG Evaluation 257   
Summary 258

# 9. Multimodal Large Language Models. . . 259

Transformers for Vision 260   
Multimodal Embedding Models 263   
CLIP: Connecting Text and Images 265   
How Can CLIP Generate Multimodal Embeddings? 265   
OpenCLIP 268   
Making Text Generation Models Multimodal 273   
BLIP-2: Bridging the Modality Gap 273   
Preprocessing Multimodal Inputs 278   
Use Case 1: Image Captioning 280   
Use Case 2: Multimodal Chat-Based Prompting 283   
Summary 286

# Part III. Training and Fine-Tuning Language Models

10. Creating Text Embedding Models. . . . 289

Embedding Models 289   
What Is Contrastive Learning? 291   
SBERT 293   
Creating an Embedding Model 296   
Generating Contrastive Examples 296   
Train Model 297   
In-Depth Evaluation 300   
Loss Functions 301   
Fine-Tuning an Embedding Model 309   
Supervised 309   
Augmented SBERT 311   
Unsupervised Learning 316   
Transformer-Based Sequential Denoising Auto-Encoder 316   
Using TSDAE for Domain Adaptation 320   
Summary 321

# 11. Fine-Tuning Representation Models for Classification. . . . . 323

Supervised Classification 323   
Fine-Tuning a Pretrained BERT Model 325   
Freezing Layers 328   
Few-Shot Classification 333   
SetFit: Efficient Fine-Tuning with Few Training Examples 333   
Fine-Tuning for Few-Shot Classification 337   
Continued Pretraining with Masked Language Modeling 340   
Named-Entity Recognition 345   
Preparing Data for Named-Entity Recognition 347   
Fine-Tuning for Named-Entity Recognition 352   
Summary 353

# 12. Fine-Tuning Generation Models. . . 355

The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and   
Preference Tuning 355   
Supervised Fine-Tuning (SFT) 357   
Full Fine-Tuning 357   
Parameter-Efficient Fine-Tuning (PEFT) 359   
Instruction Tuning with QLoRA 367   
Templating Instruction Data 367   
Model Quantization 369   
LoRA Configuration 370   
Training Configuration 371   
Training 372   
Merge Weights 373   
Evaluating Generative Models 373   
Word-Level Metrics 374   
Benchmarks 374   
Leaderboards 376   
Automated Evaluation 376   
Human Evaluation 376   
Preference-Tuning / Alignment / RLHF 378   
Automating Preference Evaluation Using Reward Models 379   
The Inputs and Outputs of a Reward Model 380   
Training a Reward Model 380   
Training No Reward Model 384   
Preference Tuning with DPO 385   
Templating Alignment Data 386   
Model Quantization 386   
Training Configuration 387   
Training 388   
Summary 389   
Afterword. . 391   
Index. . 393

Large language models (LLMs) have had a profound and far-reaching impact on the world. By enabling machines to better understand and generate human-like language, LLMs have opened new possibilities in the field of AI and impacted entire industries.

This book provides a comprehensive and highly visual introduction to the world of LLMs, covering both the conceptual foundations and practical applications. From word representations that preceded deep learning to the cutting-edge (at the time of this writing) Transformer architecture, we will explore the history and evolution of LLMs. We delve into the inner workings of LLMs, exploring their architectures, training methods, and fine-tuning techniques. We also examine various applications of LLMs in text classification, clustering, topic modeling, chatbots, search engines, and more.

With its unique blend of intuition-building, applications, and illustrative style, we hope that this book provides the ideal foundation for those looking to explore the exciting world of LLMs. Whether you are a beginner or an expert, we invite you to join us on this journey to start building with LLMs.

# An Intuition-First Philosophy

The main goal of this book is to provide an intuition into the field of LLMs. The pace of development in the Language AI field is incredibly fast and frustration can build trying to keep up with the latest technologies. Instead, we focus on the fundamentals of LLMs and intend to provide a fun and easy learning process.

To achieve this intuition-first philosophy we liberally make use of visual language. Illustrations will help give a visual identity to major concepts and processes involved in the learning process of LLMs.1 With our illustrative method of storytelling, we want to take you on a journey to this exciting and potentially world-changing field.

Throughout the book, we make a clear distinction between representation and gener‐ ative language models. Representation models are LLMs that do not generate text but are commonly used for task-specific use cases, like classification, whereas generation models are LLMs that generate text, like GPT models. Although generative models are typically the first thing that comes to mind when thinking about LLMs, there is still much use for representation models. We are also loosely using the word “large” in large language models and often elect to simply call them language models as size descriptions are often rather arbitrary and not always indicative of capability.

# Prerequisites

This book assumes that you have some experience programming in Python and are familiar with the fundamentals of machine learning. The focus will be on building a strong intuition rather than deriving mathematical equations. As such, illustrations combined with hands-on examples will drive the examples and learning through this book. This book assumes no prior knowledge of popular deep learning frameworks such as PyTorch or TensorFlow nor any prior knowledge of generative modeling.

If you are not familiar with Python, a great place to start is Learn Python, where you will find many tutorials on the basics of the language. To further ease the learning process, we made all the code available on Google Colab, a platform where you can run all of the code without the need to install anything locally.

# Book Structure

The book is broadly divided into three parts. They are illustrated in Figure P-1 to give you a full view of the book. Note that each chapter can be read independently, so feel free to skim chapters you are already familiar with.

# Part I: Understanding Language Models

In Part I of the book, we explore the inner workings of language models both small and large. We start with an overview of the field and common techniques (see Chap‐ ter 1) before moving over to two central components of these models, tokenization and embeddings (see Chapter 2). We finish this part of the book with an updated and expanded version of Jay’s well-known Illustrated Transformer, which dives into the architecture of these models (see Chapter 3). Many terms and definitions will be introduced that are used throughout the book.

![## Image Analysis: 2b4dc2d8a7cfc77660f474400b99b3c2247b9c14e1ea5eba5facb6b0e69431c3.jpg

**Conceptual Understanding:**
The image conceptually represents the complete structure and content outline of a book dedicated to Large Language Models. It functions as a visual table of contents or a roadmap for the reader. The main purpose of the image is to clearly present the book's organization, divided into three major parts, and to list the specific chapters within each part. It aims to convey the logical flow of topics, starting from foundational understanding, progressing to practical application, and concluding with advanced techniques for training and fine-tuning LMs.

**Content Interpretation:**
The image systematically outlines the structure of a book, detailing its three main parts and the twelve chapters distributed among them. It illustrates a pedagogical progression of topics related to large language models (LMs).

**Part 1: Understanding Language Models** (Chapters 1-3) focuses on the foundational theoretical aspects of LMs. This includes an introduction, the concept of tokens and embeddings, and an internal examination of how LMs function. The associated text, "Answering the question: 'How do large language models work?'", explicitly states the purpose of this section.

**Part 2: Using Pretrianed Language Models** (Chapters 4-9) shifts the focus to the practical application of LMs. Topics covered range from basic tasks like text classification and clustering to more advanced techniques like prompt engineering, advanced text generation, semantic search, retrieval-augmented generation, and multimodal LMs. The accompanying text, "Using large language models across a variety of use cases.", clearly indicates the applied nature of this part.

**Part 3: Training and Fine-Tuning Language Models** (Chapters 10-12) addresses the advanced aspects of developing and customizing LMs. This involves creating text embedding models and fine-tuning models for both representation (classification) and generation tasks. The descriptive text, "Exploring the multifaceted components of training and fine-tuning different types of large language models.", highlights the in-depth, advanced nature of these topics.

All extracted text elements, including the part titles, chapter titles, and the descriptive summaries under each part, consistently support the interpretation of the image as a comprehensive curriculum or table of contents for learning about large language models, moving from theory to application and advanced customization.

**Key Insights:**
The main takeaways from this image are:

1.  **Comprehensive Coverage of Large Language Models (LMs):** The book provides a holistic view of LMs, segmented into three logical parts: understanding, using, and training/fine-tuning. This is evidenced by the top-level titles "Part 1 Understanding Language Models," "Part 2 Using Pretrianed Language Models," and "Part 3 Training and Fine-Tuning Language Models."
2.  **Structured Learning Progression:** The content is organized in a progressive manner, starting with fundamental concepts and gradually moving towards more advanced and practical applications. Part 1 focuses on "How do large language models work?" while Part 2 covers "Using large language models across a variety of use cases." Part 3 delves into "Exploring the multifaceted components of training and fine-tuning different types of large language models."
3.  **Key Topics in LM Field:** The chapters list specific crucial topics within the LM domain, such as "Tokens and Embeddings," "Prompt Engineering," "Semantic Search and Retrieval-Augmented Generation," and "Multimodal Large Language Models." These chapter titles indicate the breadth and depth of subjects discussed.
4.  **Emphasis on Both Theory and Practice:** The book balances theoretical understanding (e.g., "Looking Inside Large Language Models") with practical application (e.g., "Text Classification," "Advanced Text Generation Techniques and Tools") and advanced development (e.g., "Fine-Tuning Representation Models for Classification," "Fine-Tuning Generation Models"). This is clear from the content titles within each part.

In essence, the image demonstrates that the accompanying book is a well-structured resource designed to educate readers thoroughly on the theory, application, and advanced development aspects of large language models.

**Document Context:**
This image, labeled "Figure P-1. All parts and chapters of the book.", serves as a critical structural overview at the beginning of the document (implied by 'Part I: Understanding Language Models' in the document context). It visually maps out the entire content of the book, providing readers with a clear roadmap of the topics that will be covered. This helps set expectations and provides a high-level understanding of the book's scope and organization before diving into the individual chapters. It acts as an advanced organizer, showing the logical flow from foundational concepts to advanced applications and techniques, ensuring the reader understands the overarching structure and progression of knowledge presented in the text.

**Summary:**
The image displays a structured overview of a book, outlining its three main parts and the chapters contained within each. It visually represents the progression of topics, starting with foundational understanding, moving to practical application, and concluding with advanced training and fine-tuning techniques for large language models. 

**Part 1: Understanding Language Models** introduces the core concepts. Chapter 1 covers "An Introduction to Large Language Models," followed by Chapter 2 on "Tokens and Embeddings," and Chapter 3 which focuses on "Looking Inside Large Language Models." This part collectively answers the question: "How do large language models work?"

**Part 2: Using Pretrianed Language Models** delves into practical applications. It includes Chapter 4 on "Text Classification," Chapter 5 on "Text Clustering and Topic Modeling," and Chapter 6 on "Prompt Engineering." Further advanced applications are covered in Chapter 7, "Advanced Text Generation Techniques and Tools," Chapter 8, "Semantic Search and Retrieval-Augmented Generation," and Chapter 9, "Multimodal Large Language Models." This section highlights "Using large language models across a variety of use cases."

**Part 3: Training and Fine-Tuning Language Models** focuses on advanced model development. It comprises Chapter 10, "Creating Text Embedding Models," Chapter 11, "Fine-Tuning Representation Models for Classification," and Chapter 12, "Fine-Tuning Generation Models." This part is dedicated to "Exploring the multifaceted components of training and fine-tuning different types of large language models."

Each part builds upon the previous one, providing a comprehensive learning path from basic understanding to advanced deployment and customization of large language models.](images/2b4dc2d8a7cfc77660f474400b99b3c2247b9c14e1ea5eba5facb6b0e69431c3.jpg)
Figure P-1. All parts and chapters of the book.

# Part II: Using Pretrained Language Models

In Part II of the book, we explore how LLMs can be used through common use cases. We use pretrained models and demonstrate their capabilities without the need to fine-tune them.

You learn how to use language models for supervised classification (see Chapter 4), text clustering and topic modeling (see Chapter 5), leveraging embedding models for semantic search (see Chapter 6), generating text (see Chapters 7 and 8), and extending the capabilities of text generation to the visual domain (see Chapter 9).

Learning these individual language model capabilities will equip you with the skill set to problem-solve with LLMs and build more and more advanced systems and pipelines.

# Part III: Training and Fine-Tuning Language Models

In Part III of the book, we explore advanced concepts through training and finetuning all kinds of language models. We will explore how to create and fine-tune an embedding model (see Chapter 10), review how to fine-tune BERT for classification (see Chapter 11), and end the book with several methods for fine-tuning generation models (see Chapter 12).

# Hardware and Software Requirements

Running generative models is generally a compute-intensive task that requires a com‐ puter with a strong GPU. Since those are not available to every reader, all examples in this book are made to run using an online platform, namely Google Colaboratory, often shortened to “Google Colab.” At the time of writing, this platform allows you to use an NVIDIA GPU (T4) for free to run your code. This GPU has 16 GB of VRAM (which is the memory of your GPU), which is the minimum amount of VRAM we expect for the examples throughout the book.

![## Image Analysis: e1700df727ecc305cb9b2115230a6286757935ff5aaf06419cb60a3972f12552.jpg

**Conceptual Understanding:**
Conceptually, this image represents a unique visual identifier or a brand emblem. Its main purpose is to establish a visual identity for an organization, product, or project. The stylized crow/raven motif itself can semantically suggest attributes like intelligence, adaptability, or a connection to specific themes, but without any accompanying text, these remain potential symbolic inferences rather than explicitly communicated messages. It functions as a non-textual brand mark.

**Content Interpretation:**
The image exclusively presents a graphic emblem in the form of a stylized blue silhouette of a crow or raven. It does not depict any processes, concepts, relationships, or systems typically found in diagrams or flowcharts. Its content is purely symbolic, serving as a visual identifier or logo. The absence of any textual elements means its interpretation is based solely on the visual representation of the bird itself, which often carries various symbolic meanings (e.g., intelligence, wisdom, mystery) depending on cultural context, but these are not explicitly communicated here.

**Key Insights:**
The primary knowledge extracted from this image is the visual identification of a brand or entity through its distinctive logo, represented by a crow/raven silhouette. As there is no text within the image, it does not provide any explicit technical data, patterns, or insights directly relevant to 'Hardware and Software Requirements.' Its contribution is limited to brand recognition and symbolic association.

**Document Context:**
Given the document context 'Hardware and Software Requirements,' the image of a stylized crow silhouette most likely functions as a logo or a brand identifier. It is probably associated with the company, product, or project to which the hardware and software requirements pertain. It serves to visually brand the document section or the entity responsible for the specified requirements, rather than providing direct technical information about the requirements themselves. Its role is to establish identity and association.

**Summary:**
The image displays a stylized blue silhouette of a crow or raven. The bird is shown in profile, facing left, standing on two feet. The entire bird figure is rendered in a solid, dark blue color. This blue silhouette is centrally positioned within a white square frame. There is absolutely no text, numbers, symbols, or any other textual elements, including micro-details, annotations, or metadata, present within or around the image. The image is a simple, graphic illustration.](images/e1700df727ecc305cb9b2115230a6286757935ff5aaf06419cb60a3972f12552.jpg)

Not all chapters require a minimum of 16 GB VRAM as some examples, like training and fine-tuning, are more computeintensive than others, such as prompt engineering. In the repos‐ itory, you will find the minimum GPU requirements for each chapter.

All code, requirements, and additional tutorials are available in this book’s repository. If you want to run the examples locally, we recommend access to an NVIDIA GPU with a minimum of $1 6 \mathrm { G B }$ of VRAM. For a local installation, for example with conda, you can follow this setup to create your environment:

conda create -n thellmbook pytho $\ d _ { 1 } = 3 . 1 \ d 6$ conda activate thellmbook

You can install all the necessary dependencies by forking or cloning the repository and then running the following in your newly created Python 3.10 environment:

pip install -r requirements.txt

# API Keys

We use both open source and proprietary models throughout the examples to demonstrate the advantages and disadvantages of both. For the proprietary models, using OpenAI and Cohere’s offering, you will need to create a free account:

# OpenAI

Click “sign up” on the site to create a free account. This account allows you to create an API key, which can be used to access GPT-3.5. Then, go to “API keys” to create a secret key.

# Cohere

Register a free account on the website. Then, go to “API keys” to create a secret key.

Note that with both accounts, rate limits apply and that these free API keys only allow for a limited number of calls per minute. Throughout all examples, we have taken that into account and provided local alternatives if necessary.

For the open source models, you do not need to create an account with the exception of the Llama 2 model in Chapter 2. To use that model, you will need a Hugging Face account:

# Hugging Face

Click “sign up” on the Hugging Face website to create a free account. Then, in “Settings” go to “Access Tokens” to create a token that you can use to download certain LLMs.

# Conventions Used in This Book

The following typographical conventions are used in this book:

Italic

Indicates new terms, URLs, email addresses, filenames, and file extensions.

Constant width

Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords.

# Constant width bold

Shows commands or other text that should be typed literally by the user.

Constant width italic Shows text that should be replaced with user-supplied values or by values deter‐ mined by context.

This element signifies a tip or suggestion.

This element signifies a general note.

# Using Code Examples

Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/HandsOnLLM/Hands-On-Large-Language-Models.

If you have a technical question or a problem using the code examples, please send email to support@oreilly.com.

This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission.

We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Hands-On Large Lan‐ guage Models by Jay Alammar and Maarten Grootendorst (O’Reilly). Copyright 2024 Jay Alammar and Maarten Pieter Grootendorst, 978-1-098-15096-9.”

If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com.

# O’Reilly Online Learning

For more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed.

Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and $^ { 2 0 0 + }$ other publishers. For more information, visit https://oreilly.com.

# How to Contact Us

Please address comments and questions concerning this book to the publisher:

O’Reilly Media, Inc.   
1005 Gravenstein Highway North   
Sebastopol, CA 95472   
800-889-8969 (in the United States or Canada) 707-827-7019 (international or local)   
707-829-0104 (fax)   
support@oreilly.com   
https://www.oreilly.com/about/contact.html

We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/hands_on_LLMs_1e.

For news and information about our books and courses, visit https://oreilly.com.

Find us on LinkedIn: https://linkedin.com/company/oreilly-media.

Watch us on YouTube: https://youtube.com/oreillymedia.

# Acknowledgments

Writing this book has been an incredible experience, collaboration, and journey for us.

The field of (large) language models is one of the most dynamic areas in technology today, and within the span of writing this book, we have witnessed extraordinary advancements. Yet, despite the rapid pace of change, the fundamental principles remain strikingly consistent which made the writing process particularly intriguing. We are grateful to have had the opportunity to explore this field in-depth at such a pivotal moment.

Working with our O’Reilly team was incredible! Special thanks to Michele Cronin for her amazing feedback, support, and enthusiasm for this book from day one. We could not have asked for a better editor—you are amazing! Thank you, Nicole Butterfield, for kicking off this book and helping us maintain a structured approach throughout the writing. Thank you to Karen Montgomery for creating our wonderful cover, we love the kangaroo! Big thanks to Kate Dullea for being so patient with us having to go through hundreds of illustrations many times over. The timely early releases by Clare Laylock helped us see our work grow which was a big motivator, thank you. Thanks to Ashley Stussy and Charles Roumeliotis for the development in the final stages of the book and everyone else at O’Reilly who contributed.

Thanks to our amazing crew of technical reviewers. Invaluable feedback was given by Harm Buisman, Emir Muñoz, Luba Elliott, Guarav Chawla, Rafael V. Pierre, Luba Elliott, Tarun Narayanan, Nikhil Buduma, and Patrick Harrison.

#

I’d love to extend my deepest gratitude to my family for their unwavering support and inspiration. I would like to specifically acknowledge my parents, Abdullah and Mishael, and my aunts, Hussah and Aljoharah.

I’m grateful to the friends, colleagues, and collaborators who helped me understand and explain the tricky concepts covered in this book as well as to the Cohere folks who cultivate a supporting learning and sharing environment. Thank you to Adrien Morisot, Aidan Gomez, Andy Toulis, Anfal Alatawi, Arash Ahmadian, Bharat Venki‐ tesh, Edward Grefenstette, Ivan Zhang, Joao Araújo, Luis Serrano, Matthias Gallé, Meor Amer, Nick Frosst, Patrick Lewis, Phil Blunsom, Sara Hooker, and Suhas Pai.

I couldn’t conceive of this project getting accomplished to the level it has without the extraordinary talent and tireless effort of Maarten, my coauthor. Your ability to repeatedly nail the technical details (from the pinned version of the nth import dependency to the latest in LLM quantization) while weaving some of the world’s best visual narratives is absolutely breathtaking.

Lastly, a tip of the hat to the incredible coffee shop scene of Riyadh, Saudi Arabia for supplying me with caffeine and a good place to focus from dawn until midnight. It’s where I read most of these papers and worked out my understanding (looking at you, Elixir Bunn).

# Maarten

I want to begin by expressing my heartfelt appreciation to my coauthor, Jay. Your insights have made this not only possible but incredibly fulfilling. This journey has been nothing short of amazing and collaborating with you has been an absolute joy.

I want to sincerely thank my wonderful colleagues at IKNL for their continued support throughout this journey. A special mention goes to Harm—our Monday morning coffee breaks discussing this book were a constant source of encouragement.

Thank you to my family and friends for their unwavering support, and to my parents in particular. Pap, despite the challenges you faced, you always found a way to be there for me when I needed it most, thank you. Mam, the conversations we had as aspiring writers were wonderful and motivated me more than you could ever imagine. Thank you both for your endless support and encouragement.

Finally, I am at a loss for words to adequately express my gratitude to my wonderful wife, Ilse. Lieverd, your boundless enthusiasm and patience have been legendary, especially when I droned on about the latest LLM developments for hours on end. You are my greatest support. My apologies to my amazing daughter, Sarah. At just two years old, you already have listened to more about large language models than anyone should have to endure in a lifetime! I promise we’ll make up for it with endless playtime and adventures together.

# Understanding Language Models

# An Introduction to Large Language Models

Humanity is at an inflection point. From 2012 onwards, developments in building AI systems (using deep neural networks) accelerated so that by the end of the decade, they yielded the first software system able to write articles indiscernible from those written by humans. This system was an AI model called Generative Pre-trained Transformer 2, or GPT-2. 2022 marked the release of ChatGPT, which demonstrated how profoundly this technology was poised to revolutionize how we interact with technology and information. Reaching one million active users in five days and then one hundred million active users in two months, the new breed of AI models started out as human-like chatbots but quickly evolved into a monumental shift in our approach to common tasks, like translation, text generation, summarization, and more. It became an invaluable tool for programmers, educators, and researchers.

The success of ChatGPT was unprecedented and popularized more research into the technology behind it, namely large language models (LLMs). Both proprietary and public models were being released at a steady pace, closing in on, and eventually catching up to the performance of ChatGPT. It is not an exaggeration to state that almost all attention was on LLMs.

As a result, 2023 will always be known, at least to us, as the year that drastically changed our field, Language Artificial Intelligence (Language AI), a field character‐ ized by the development of systems capable of understanding and generating human language.

However, LLMs have been around for a while now and smaller models are still rele‐ vant to this day. LLMs are much more than just a single model and there are many other techniques and models in the field of language AI that are worth exploring.

In this book, we aim to give readers a solid understanding of the fundamentals of both LLMs and the field of Language AI in general. This chapter serves as the scaffolding for the rest of the book and will introduce concepts and terms that we will use throughout the chapters.

But mostly, we intend to answer the following questions in this chapter:

• What is Language AI?   
• What are large language models?   
• What are the common use cases and applications of large language models?   
• How can we use large language models ourselves?

# What Is Language AI?

The term artificial intelligence (AI) is often used to describe computer systems dedica‐ ted to performing tasks close to human intelligence, such as speech recognition, lan‐ guage translation, and visual perception. It is the intelligence of software as opposed to the intelligence of humans.

Here is a more formal definition by one of the founders of the artificial intelligence discipline:

[Artificial intelligence is] the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable.

—John McCarthy, $2 0 0 7 ^ { 1 }$

Due to the ever-evolving nature of AI, the term has been used to describe a wide variety of systems, some of which might not truly embody intelligent behavior. For instance, characters in computer games (NPCs [nonplayable characters]) have often been referred to as AI even though many are nothing more than if-else statements.

Language AI refers to a subfield of AI that focuses on developing technologies capable of understanding, processing, and generating human language. The term Language AI can often be used interchangeably with natural language processing (NLP) with the continued success of machine learning methods in tackling language processing problems.

We use the term Language AI to encompass technologies that technically might not be LLMs but still have a significant impact on the field, like how retrieval systems can give LLMs superpowers (see Chapter 8).

Throughout this book, we want to focus on the models that have had a major role in shaping the field of Language AI. This means exploring more than just LLMs in isolation. That, however, brings us to the question: what are large language models? To begin answering this question in this chapter, let’s first explore the history of Language AI.

# A Recent History of Language AI

The history of Language AI encompasses many developments and models aiming to represent and generate language, as illustrated in Figure 1-1.

![## Image Analysis: b207fa42974dee7806a8447996f24a0915123885fce362a4956384ec29fa120f.jpg

**Conceptual Understanding:**
This image conceptually represents a historical timeline charting the evolution of significant models and underlying concepts in the field of Language Artificial Intelligence. The main purpose of the image is to visually demonstrate the chronological progression of key advancements in Language AI, highlighting the major shifts in model architectures over approximately two decades. It aims to illustrate how the field has moved from earlier statistical and word-embedding approaches to the advent and subsequent diversification of transformer-based models. The key ideas being communicated include the emergence of groundbreaking models, the distinction between different neural network architectures (non-transformer, encoder-only, decoder-only, encoder-decoder), and the rapid acceleration of innovation in recent years.

**Content Interpretation:**
The image displays the chronological development of prominent Language AI models and concepts, organized along a horizontal timeline spanning from approximately 2000 to 2023. Each model or concept is represented by a labeled box connected to its respective year on the timeline. A legend at the bottom categorizes these models by their architectural types: 'Decoder-only' (pink), 'Encoder-only' (purple), 'Non-transformer models' (gray), and 'Encoder-decoder' (green). The timeline starts with 'Bag-of-words' (gray, ~2000) and 'word2vec' (gray, 2013), both categorized as 'Non-transformer models'. The 'Attention' mechanism (gray, 2017) is also listed as a 'Non-transformer model', preceding the major transformer era. In 2018, the timeline shows the emergence of 'BERT' (purple), an 'Encoder-only' model, and 'GPT' (pink), a 'Decoder-only' model, marking the beginning of transformer-based architectures. The year 2019 saw further diversification with 'DistilBERT' (purple, 'Encoder-only'), 'RoBERTa' (purple, 'Encoder-only'), and 'GPT-2' (pink, 'Decoder-only'). The year 2020 introduced 'GPT-3' (pink, 'Decoder-only') and 'T5' (green, 'Encoder-decoder'). 'Switch' (green, 'Encoder-decoder') appeared in 2021. In 2022, 'Flan-T5' (green, 'Encoder-decoder') was introduced, followed by 'ChatGPT' (pink, 'Decoder-only') in 2023. The significance lies in showing a clear progression from earlier, simpler NLP methods to sophisticated transformer-based models, and the rapid innovation within the transformer paradigm, moving from single-architecture types (encoder-only, decoder-only) to more comprehensive encoder-decoder models and increasingly powerful generative models like ChatGPT.

**Key Insights:**
The image offers several key takeaways and insights into the evolution of Language AI:

1.  **Transition to Transformer Architectures:** The timeline clearly shows a significant shift from 'Non-transformer models' (e.g., Bag-of-words, word2vec) prevalent around ~2000 to 2013, to the dominance of transformer-based models starting notably in 2018. The 'Attention' mechanism (2017) is a pivotal concept preceding this shift, even though categorized as non-transformer itself. This is evident from the gray boxes representing non-transformer models appearing early, followed by a dense cluster of purple, pink, and green boxes representing transformer models.

2.  **Diversification of Transformer Types:** Post-2018, transformer models rapidly diversified into distinct architectural types: 'Encoder-only' (BERT, DistilBERT, RoBERTa), 'Decoder-only' (GPT, GPT-2, GPT-3, ChatGPT), and 'Encoder-decoder' (T5, Switch, Flan-T5). The color-coded legend explicitly defines these categories, and the models appearing from 2018 onwards exemplify this diversification, demonstrating how different architectural approaches addressed various NLP tasks.

3.  **Accelerated Pace of Innovation:** The density of model releases increases dramatically from 2018 onwards, with multiple significant models appearing annually. This suggests an accelerated pace of research and development in Language AI following the advent of the transformer architecture. For example, 2019 alone saw the introduction of GPT-2, DistilBERT, and RoBERTa.

4.  **Emergence and Evolution of Generative AI:** The later years of the timeline, particularly from 2018 to 2023, highlight the continuous evolution and increasing sophistication of generative AI models. The 'Decoder-only' models (GPT, GPT-2, GPT-3, ChatGPT) show a clear lineage of powerful generative language models, culminating in widely recognized systems like ChatGPT. Simultaneously, 'Encoder-decoder' models like T5 and Flan-T5 represent advancements in models capable of both understanding and generating text for a broader range of tasks.

**Document Context:**
The image, identified as 'Figure 1-1. A peek into the history of Language AI,' directly supports the document section titled 'A Recent History of Language AI'. It provides a foundational visual overview of the major milestones and technological shifts in Language AI development. By presenting a chronological sequence of models and their architectural classifications, it visually primes the reader for more in-depth discussions of these advancements within the document. The image serves to establish the context of how Language AI has evolved over time, highlighting the transition from traditional statistical methods to transformer-based deep learning models, and setting the stage for understanding the current landscape of AI.

**Summary:**
The image presents a chronological timeline illustrating the history and evolution of key models and concepts in Language AI from approximately 2000 to 2023. It visually maps the progression from early non-transformer approaches to the diverse landscape of transformer-based architectures. The timeline begins with foundational concepts like 'Bag-of-words' and 'word2vec', transitions through the pivotal 'Attention' mechanism, and then showcases the explosion of transformer models including 'BERT', 'GPT', 'DistilBERT', 'RoBERTa', 'GPT-2', 'GPT-3', 'T5', 'Switch', 'Flan-T5', and culminates with 'ChatGPT'. Each model is color-coded according to its architectural type: 'Decoder-only' (pink), 'Encoder-only' (purple), 'Non-transformer models' (gray), and 'Encoder-decoder' (green). The visual organization allows readers to quickly grasp the timeline of development, the shift in dominant architectural paradigms, and the increasing pace of innovation in the field of Language AI, providing a clear overview of the subject matter discussed in the accompanying document section.](images/b207fa42974dee7806a8447996f24a0915123885fce362a4956384ec29fa120f.jpg)
Figure 1-1. A peek into the history of Language AI.

Language, however, is a tricky concept for computers. Text is unstructured in nature and loses its meaning when represented by zeros and ones (individual characters). As a result, throughout the history of Language AI, there has been a large focus on representing language in a structured manner so that it can more easily be used by computers. Examples of these Language AI tasks are provided in Figure 1-2.

![## Image Analysis: 0786527f72737086cdfb55ac634c0d7bc49950a25bed0ffec04b40d1ce37cc2a.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental capabilities and operational flow of Language AI. It illustrates how Language AI takes raw, unstructured textual data as input, processes it, and then produces various forms of structured and meaningful outputs. The main purpose is to show the versatility of Language AI in performing tasks such as generating new text, converting text into numerical representations (embeddings), and classifying text to identify specific targets. It communicates the idea that Language AI is a powerful tool capable of diverse text processing tasks.

**Content Interpretation:**
The image depicts a core system where **Language AI** serves as the central processing unit for **Text input**, which is identified as **Unstructured data**. This highlights that Language AI is designed to handle raw, unorganized text.

The "Language AI" box explicitly states "Processes the input text," indicating its function as an analytical and transformational engine.

Following the processing, the diagram branches into three distinct output categories, each representing a different capability or task:

*   **Text output** with the sub-label **Generative modeling**: This indicates Language AI's ability to create new textual content, such as summaries, translations, or creative writing. The visual representation of green lines turning into blue lines within similar rectangular shapes supports the idea of text transformation and generation.
*   **Embeddings** with the sub-label **Numeric values**: This demonstrates the AI's capacity to convert textual data into a numerical format. Embeddings are crucial for many machine learning tasks as they allow algorithms to understand semantic relationships between words and phrases. The five shaded blue bars visually convey this transformation into quantifiable data.
*   **Classification** with the sub-label **Identify targets**: This illustrates Language AI's function in categorizing or tagging text. This could involve sentiment analysis, spam detection, or topic labeling. The visual representation with one blue and two gray lines suggests the identification and distinction of specific targets within the text.

All extracted text elements, such as "Unstructured data," "Processes the input text," "Generative modeling," "Numeric values," and "Identify targets," directly support the interpretation of Language AI as a versatile tool for understanding, transforming, and generating textual content.

**Key Insights:**
The main takeaways and lessons from this image are:

1.  **Language AI processes unstructured text:** The initial "Text input" explicitly labeled as "Unstructured data" highlights that Language AI is built to handle raw, unorganized textual information, which is a common form of data in the real world.
2.  **Language AI is a processing engine:** The central "Language AI" box with the descriptor "Processes the input text" emphasizes its role as the core computational component that transforms and makes sense of the input.
3.  **Language AI has diverse capabilities, primarily in three areas:** The three distinct outputs (Text output, Embeddings, Classification) illustrate the broad range of tasks Language AI can perform.
    *   **Generative modeling** (under "Text output") shows its ability to create new content.
    *   **Numeric values** (under "Embeddings") points to its function in translating human language into a machine-understandable numerical format, which is foundational for many analytical tasks.
    *   **Identify targets** (under "Classification") indicates its skill in pattern recognition and categorization within text, enabling tasks like sentiment analysis or information extraction.

These specific text elements provide clear evidence for the insights, showing that Language AI is a versatile technology capable of taking raw text, processing it, and then delivering various valuable outputs for different applications.

**Document Context:**
This image, appearing in a section titled "A Recent History of Language AI" and followed by the caption "Figure 1-2. Language AI is capable of many tasks by processing textual input," serves as a foundational visual explanation. It precisely illustrates *how* Language AI processes textual input and *what* "many tasks" it is capable of performing, as mentioned in the caption. It provides a high-level architectural overview of a Language AI system's core functions, setting the stage for deeper discussions about its historical development and specific applications within the document.

**Summary:**
This diagram illustrates the fundamental process of Language AI, showing how it interacts with text from input to various outputs. The process begins with **Text input**, which is characterized as **Unstructured data**. This means the AI starts with raw, unorganized text, much like human language as we use it. This unstructured text is then fed into the central system, labeled **Language AI**, whose core function is to **Process the input text**. This processing involves understanding, analyzing, and transforming the textual information.

Following this central processing, the Language AI can produce three distinct types of outputs, showcasing its versatility:

1.  The first output is **Text output**, which is specifically associated with **Generative modeling**. This refers to the AI's ability to create new text, such as writing summaries, generating creative content, or translating languages.
2.  The second output consists of **Embeddings**, which are explained as **Numeric values**. Here, the Language AI converts the input text into a numerical format, allowing computers to understand and process the semantic relationships and meanings within the text. These numerical representations are critical for many advanced AI applications.
3.  The third output is **Classification**, where the AI's task is to **Identify targets**. This means the Language AI can categorize the input text, such as determining its sentiment (positive or negative), identifying specific topics, or flagging certain keywords.

In essence, the diagram shows that Language AI takes raw text, understands and transforms it, and then can either generate new text, convert it into numerical data for further analysis, or classify it to extract specific information or labels.](images/0786527f72737086cdfb55ac634c0d7bc49950a25bed0ffec04b40d1ce37cc2a.jpg)
Figure 1-2. Language AI is capable of many tasks by processing textual input.

# Representing Language as a Bag-of-Words

Our history of Language AI starts with a technique called bag-of-words, a method for representing unstructured text.2 It was first mentioned around the 1950s but became popular around the 2000s.

Bag-of-words works as follows: let’s assume that we have two sentences for which we want to create numerical representations. The first step of the bag-of-words model is tokenization, the process of splitting up the sentences into individual words or subwords (tokens), as illustrated in Figure 1-3.

![## Image Analysis: 989d490a958de17822c93eba848b39b68db6a9d32f0f819da7943ed77daba793.jpg

**Conceptual Understanding:**
This image represents the process of 'tokenization' in natural language processing, specifically focusing on splitting sentences into individual words (tokens) using whitespace as the delimiter. The main purpose is to visually explain how raw textual input is broken down into its most basic meaningful units, which is a foundational step for many text analysis and machine learning tasks. The key idea being communicated is the conversion of a continuous string of characters into a structured sequence of words.

**Content Interpretation:**
The image demonstrates the tokenization process, specifically 'whitespace tokenization'. It shows how a continuous string of text (a sentence) is broken down into its constituent words or 'tokens'. The two parallel examples illustrate that this process is applied consistently to different input sentences. The significance is that individual words, rather than entire sentences, become the basic units of analysis, which is foundational for many NLP tasks. All extracted text elements directly support this interpretation: the 'Input' labels clearly identify the starting point, the sentences themselves are the data to be processed, the 'Split input by a whitespace' label explicitly states the method, and the subsequent individual word boxes ('that', 'is', 'a', 'cute', 'dog', 'my', 'cat', 'is', 'cute') represent the output of this tokenization, confirming that each word has been successfully isolated based on whitespace.

**Key Insights:**
The main takeaway is the practical demonstration of whitespace tokenization, a fundamental concept in natural language processing. It teaches that text data can be systematically broken down into discrete units (words) by identifying spaces between them. The image reinforces that this process results in a collection of individual words, which can then be used for further analysis. The specific text elements 'Input', the example sentences ('That is a cute dog', 'My cat is cute'), the explicit instruction 'Split input by a whitespace', and the resulting individual word tokens ('that', 'is', 'a', 'cute', 'dog', 'my', 'cat', 'is', 'cute') provide direct evidence for understanding how sentences are processed into a list of words, which is a crucial step before constructing a 'bag-of-words' representation.

**Document Context:**
This image directly supports the document section 'Representing Language as a Bag-of-Words' by illustrating the very first step in creating a bag-of-words model: breaking down sentences into individual words. Before language can be represented as a bag of words (a collection of words from a document, disregarding grammar and even word order but keeping multiplicity), it must first be tokenized into those individual words. The image provides a clear visual example of how this initial tokenization, specifically by splitting on whitespace, is performed, which is a prerequisite for subsequent 'bag-of-words' transformations. The description after the image, 'Figure 1-3. Each sentence is split into words (tokens) by splitting on a whitespace,' perfectly aligns with and reinforces the visual information presented.

**Summary:**
The image illustrates the process of splitting two distinct input sentences into individual words, or tokens, by identifying and breaking them at each whitespace character. This fundamental step is crucial in natural language processing (NLP) for preparing text data. For the first input sentence, 'That is a cute dog', the process yields five individual tokens: 'that', 'is', 'a', 'cute', and 'dog'. Similarly, for the second input sentence, 'My cat is cute', the process results in four tokens: 'my', 'cat', 'is', and 'cute'. The diagram clearly shows the transformation from a continuous string of text to a sequence of discrete words, making the underlying text processing concept easy to grasp by visually separating each word.](images/989d490a958de17822c93eba848b39b68db6a9d32f0f819da7943ed77daba793.jpg)
Figure 1-3. Each sentence is split into words (tokens) by splitting on a whitespace.

The most common method for tokenization is by splitting on a whitespace to create individual words. However, this has its disadvantages as some languages, like Man‐ darin, do not have whitespaces around individual words. In the next chapter, we will go in depth about tokenization and how that technique influences language models. As illustrated in Figure 1-4, after tokenization, we combine all unique words from each sentence to create a vocabulary that we can use to represent the sentences.

![## Image Analysis: 5f64cb4255c817dbd83633b9b94ba6fa08bc3c8603ca4e069d6f291b47e148a1.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of building a vocabulary from raw text, specifically from tokenized sentences. Its main purpose is to demonstrate how duplicate words are eliminated to form a unique set of terms, which is a foundational step in many natural language processing tasks, particularly for creating a Bag-of-Words representation. The key idea communicated is the distinction between a sequence of all words (including duplicates) and a vocabulary (which contains only unique words).

**Content Interpretation:**
The image demonstrates the process of natural language processing (NLP) where tokenized sentences are used to create a vocabulary. The first row of boxes represents the individual words (tokens) from two distinct sentences concatenated together: 'that', 'is', 'a', 'cute', 'dog' (from the first sentence) followed by 'my', 'cat', 'is', 'cute' (from the second sentence). The subsequent step, indicated by the arrow labeled 'Create a vocabulary', involves identifying and collecting all unique words from this combined sequence. The resulting bottom row of boxes represents the vocabulary, containing each unique word exactly once: 'that', 'is', 'a', 'cute', 'dog', 'my', 'cat'. The duplicated words 'is' and 'cute' from the second sentence are not repeated in the vocabulary, illustrating the concept of a unique word set. The label 'Vocabulary size' further clarifies that this final set represents the complete collection of distinct terms.

**Key Insights:**
The main takeaway from this image is the clear illustration of how a vocabulary is formed in natural language processing. It teaches that a vocabulary is a collection of all unique words found within a given set of texts or sentences. The process involves tokenizing the sentences into individual words, then removing any duplicate words to create a distinct set. This unique set of words then defines the 'Vocabulary size'. The specific text elements, 'Tokenized sentences' at the top, the sequence of all words from both sentences, the arrow label 'Create a vocabulary', and the final set of unique words followed by 'Vocabulary size', all collectively provide direct evidence for this understanding of vocabulary construction.

**Document Context:**
This image directly supports the document's section 'Representing Language as a Bag-of-Words' by visually detailing the fundamental first step in creating such a representation: constructing the vocabulary. A bag-of-words model relies on a predefined set of unique words (the vocabulary) to represent text. This figure clearly shows how this vocabulary is built by extracting all distinct words from a given set of tokenized sentences, making it highly relevant to understanding the foundational principles discussed in the surrounding text. The 'Text after image' which states 'Figure 1-4. A vocabulary is created by retaining all unique words across both sentences' further solidifies its contextual fit and purpose.

**Summary:**
The image illustrates the process of creating a vocabulary from tokenized sentences. It begins with a sequence of individual words from two different sentences. The first sentence, represented by light green boxes, is 'that is a cute dog'. The second sentence, represented by light blue boxes, is 'my cat is cute'. An arrow points downwards from the tokenized sentences, labeled 'Create a vocabulary', indicating the operation being performed. Below this, a new sequence of yellow boxes shows the resulting vocabulary. This vocabulary contains all unique words from the initial tokenized sentences: 'that', 'is', 'a', 'cute', 'dog', 'my', 'cat'. A double-headed dotted arrow runs beneath this vocabulary, labeled 'Vocabulary size', indicating that this collection of unique words constitutes the vocabulary and its length is its size.](images/5f64cb4255c817dbd83633b9b94ba6fa08bc3c8603ca4e069d6f291b47e148a1.jpg)
Figure 1-4. A vocabulary is created by retaining all unique words across both sentences.

Using our vocabulary, we simply count how often a word in each sentence appears, quite literally creating a bag of words. As a result, a bag-of-words model aims to create representations of text in the form of numbers, also called vectors or vector representations, observed in Figure 1-5. Throughout the book, we refer to these kinds of models as representation models.

![## Image Analysis: 1c4a71cce28f9d871cf81a14d6eb667f5d5bbd6cffb8690d472ce23f31ddd45c.jpg

**Conceptual Understanding:**
The image conceptually represents the process of converting human-readable text into a machine-understandable numerical format, specifically illustrating the Bag-of-Words model in Natural Language Processing. The main purpose is to demonstrate how a sentence is tokenized and then transformed into a binary vector by checking the presence of its words in a predefined vocabulary. It communicates the key ideas of tokenization, word counting, and vector representation as fundamental steps in preparing text data for computational analysis.

**Content Interpretation:**
The image depicts a foundational Natural Language Processing (NLP) technique known as the Bag-of-Words model. It illustrates the transformation of a raw text input into a numerical feature vector. The key processes shown are 'Tokenization', which involves splitting the input string into individual word tokens, and the 'Bag-of-words' creation, where the frequency or presence of these tokens is recorded against a predefined vocabulary. The significance lies in demonstrating how qualitative text data can be quantified into a 'Vector representation' (a sequence of 0s and 1s in this specific example), making it suitable for machine learning algorithms. All extracted text elements, such as 'Input: My cat is cute', 'Tokenization: Split input by a whitespace', 'Bag-of-words: Count individual words', and 'Vector representation', directly support this interpretation by detailing each step and its outcome.

**Key Insights:**
The main takeaway from this image is the step-by-step methodology for converting a sentence into a Bag-of-Words vector representation. Key insights include: 1. Text processing often begins with 'Tokenization', where an input string is broken down into its constituent words, as shown by 'Split input by a whitespace'. 2. The 'Bag-of-words' model simplifies text by focusing on word presence or count, disregarding word order, as indicated by 'Count individual words'. 3. The final output is a 'Vector representation', typically numerical (0s and 1s in this case), which enables machine learning algorithms to process linguistic information. The specific word-to-number mapping (e.g., 'that 0', 'is 1', 'cute 1') provides concrete evidence of how individual words from the input are mapped to positions in the vector, signifying their presence or absence in a predefined vocabulary.

**Document Context:**
This image directly supports the document section titled 'Representing Language as a Bag-of-Words' by visually detailing the process. It serves as a concrete example of how the abstract concept of a bag-of-words is created from a sentence, thereby enhancing the reader's comprehension. The text after the image, 'Figure 1-5. A bag-of-words is created by counting individual words. These values are referred to as vector representations,' perfectly aligns with and summarizes the visual information, reinforcing the purpose and outcome of the depicted process.

**Summary:**
The image illustrates the process of converting a natural language input sentence into a numerical vector representation using the Bag-of-Words model. It begins with an 'Input' sentence, which is then subjected to 'Tokenization' to split it into individual words. Following this, a 'Bag-of-words' approach counts the presence of these words against a predefined vocabulary, resulting in a 'Vector representation' of 0s and 1s. The process is clear and progresses sequentially, showing how text data is transformed for computational analysis. The explicit labels for each step and the resulting numerical values provide a comprehensive understanding of how a simple sentence is numerically encoded.](images/1c4a71cce28f9d871cf81a14d6eb667f5d5bbd6cffb8690d472ce23f31ddd45c.jpg)
Figure 1-5. A bag-of-words is created by counting individual words. These values are referred to as vector representations.

Although bag-of-words is a classic method, it is by no means completely obsolete. In Chapter 5, we will explore how it can still be used to complement more recent language models.

# Better Representations with Dense Vector Embeddings

Bag-of-words, although an elegant approach, has a flaw. It considers language to be nothing more than an almost literal bag of words and ignores the semantic nature, or meaning, of text.

Released in 2013, word2vec was one of the first successful attempts at capturing the meaning of text in embeddings.3 Embeddings are vector representations of data that attempt to capture its meaning. To do so, word2vec learns semantic representations of words by training on vast amounts of textual data, like the entirety of Wikipedia.

To generate these semantic representations, word2vec leverages neural networks. These networks consist of interconnected layers of nodes that process information. As illustrated in Figure 1-6, neural networks can have many layers where each connection has a certain weight depending on the input. These weights are often referred to as the parameters of the model.

![## Image Analysis: da8cf7c8c75474b6ff272fb6f631b6a1fcdc8b9a2af6b87243fb0afadc34d744.jpg

**Conceptual Understanding:**
This image represents a simplified diagram of a feedforward artificial neural network. Conceptually, it illustrates how information flows and is processed through a series of interconnected computational units, or 'nodes,' organized into distinct layers. The main purpose of the image is to visually explain the fundamental architecture of a neural network, showing how input features are transformed through weighted connections and calculations in hidden layers to produce a specific output, such as a classification decision. It communicates the core idea that neural networks learn to identify patterns and relationships in data by adjusting the 'weights' of the connections between nodes to arrive at a desired output.

**Content Interpretation:**
The image illustrates the fundamental architecture and operational flow of a basic neural network. It visually represents how input data is transformed through successive layers to produce an output. The key concepts being shown are: neural network layers (Input, Hidden, Output), individual processing units (Nodes), and the influence of connections between nodes (Weights). The diagram specifically depicts a classification task, evidenced by the final outputs "Spam" and "Not spam", suggesting the network's purpose is to categorize an input based on its features. The varying thickness of the lines connecting the layers visually implies that different "Weights" have different "Strength and direction of the influence one node has on another", which is a critical aspect of how neural networks learn and make decisions. The description of a "Node" as something that "(Takes weights, performs calculations, and produces output)" clearly defines the basic computational unit of the network.

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Layered Architecture:** Neural networks are organized into distinct layers (Input, Hidden, Output) for sequential data processing.
2.  **Feature Input:** Raw data, referred to as "Features" (e.g., "Feature 1", "Feature 2", "Feature 3"), is fed into the network via the Input layer.
3.  **Weighted Connections:** The influence of one node on another is quantified by "Weights", which represent the "(Strength and direction of the influence one node has on another)". These weights are adjusted during the learning process.
4.  **Node Functionality:** Each "Node" acts as a computational unit that "(Takes weights, performs calculations, and produces output)", transforming the incoming signals.
5.  **Output for Decision Making:** The Output layer produces the final results (e.g., "Spam", "Not spam"), often representing classification categories or predictions.

**Conclusions/Insights:**
*   The diagram visually confirms that neural networks process information in a directed flow from input features through intermediate calculations to a final output decision.
*   The concept of "Weights" is central to how information is propagated and transformed across layers, indicating the learned relationships between features and outcomes.
*   The example of "Spam" and "Not spam" outputs suggests that a common application of such networks is binary classification, demonstrating their utility in practical problems.

**Textual Evidence:**
*   The labels "Input layer", "Hidden layer", "Output layer" directly support the layered architecture.
*   "Feature 1", "Feature 2", "Feature 3" clearly show the network's input type.
*   The explicit label "Weights" and its definition "(Strength and direction of the influence one node has on another)" detail the nature of inter-node connections.
*   The label "Node" and its definition "(Takes weights, performs calculations, and produces output)" define the basic unit's function.
*   The output labels "Spam" and "Not spam" provide evidence for the network's classification capability.

**Document Context:**
This image serves as a foundational visual explanation within a document section titled "Better Representations with Dense Vector Embeddings", directly supporting the subsequent text: "Figure 1-6. A neural network consists of interconnected layers of nodes where each connection is a linear equation." It provides a clear, simplified visual model of a neural network's structure, which is crucial for understanding how dense vector embeddings, a concept likely discussed in the surrounding text, are generated or utilized within such a network. By illustrating the input, hidden, and output layers, as well as the roles of nodes and weights, the image lays the groundwork for comprehending more complex concepts related to neural network architecture and data representation, making abstract ideas more concrete for the reader. The specific output labels "Spam" and "Not spam" provide a concrete example of a common application of neural networks.

**Summary:**
The image displays a conceptual diagram of a simple feedforward neural network, illustrating its core components and the flow of information. It consists of three main layers: an Input layer, a Hidden layer, and an Output layer, arranged from left to right. The Input layer receives initial data, specified as "Feature 1", "Feature 2", and "Feature 3", which are fed into three distinct nodes within this layer. These input nodes are connected to four nodes in the central Hidden layer. The connections between the Input and Hidden layers are represented by lines of varying thickness, collectively labeled as "Weights", with a detailed explanation that these represent the "(Strength and direction of the influence one node has on another)". The Hidden layer's nodes, after processing the weighted inputs, pass their information to two nodes in the final Output layer. Each node, particularly highlighted by the label pointing to the bottom output node, is defined as an entity that "(Takes weights, performs calculations, and produces output)". The Output layer then generates the final results, which are explicitly labeled as "Spam" and "Not spam", indicating a binary classification task. The entire diagram illustrates the sequential processing of information from raw features through intermediate computations to a final classification decision, emphasizing the roles of layers, nodes, and weights in this process.](images/da8cf7c8c75474b6ff272fb6f631b6a1fcdc8b9a2af6b87243fb0afadc34d744.jpg)
Figure 1-6. A neural network consists of interconnected layers of nodes where each connection is a linear equation.

Using these neural networks, word2vec generates word embeddings by looking at which other words they tend to appear next to in a given sentence. We start by assigning every word in our vocabulary with a vector embedding, say of 50 values for each word initialized with random values. Then in every training step, as illustrated in Figure 1-7, we take pairs of words from the training data and a model attempts to predict whether or not they are likely to be neighbors in a sentence.

During this training process, word2vec learns the relationship between words and distills that information into the embedding. If the two words tend to have the same neighbors, their embeddings will be closer to one another and vice versa. In Chapter 2, we will look closer at word2vec’s training procedure.

![## Image Analysis: 6d4e9f5f423bcc52ce1e36ffc94970ebcf6ecfde8b605502d46afa1a0f9b5f59.jpg

**Conceptual Understanding:**
This image conceptually represents the process of using dense vector embeddings and a neural network to determine the semantic proximity or 'neighborliness' of two words. The main purpose is to illustrate the mechanism by which word representations are leveraged in a machine learning model to make a prediction about their relationship, specifically focusing on how words are input, transformed into embeddings, processed by a neural network, and then yield a numerical prediction of their relatedness.

**Content Interpretation:**
The image depicts a simplified pipeline for a neural network model designed to assess the semantic relationship or 'neighborliness' between two input words. It shows the transformation of lexical inputs into numerical vector representations (embeddings) and their subsequent processing by a neural network to yield a predictive score. This score quantifies the likelihood or degree to which the two words are considered 'neighbors' based on the network's training.

**Key Insights:**
The main takeaway is that words can be transformed into numerical 'Embeddings' for computational processing. A 'Neural network' can then be trained with a 'Task:' to evaluate relationships between these word embeddings, such as determining if 'Are the two words neighbors?'. The output is a 'Model prediction' which is a quantifiable score, for instance, '0.74', representing the network's assessment of that relationship. This illustrates how machine learning models, specifically neural networks, learn and predict semantic properties of words by operating on their dense vector representations.

**Document Context:**
This image directly supports the document section 'Better Representations with Dense Vector Embeddings' by visually explaining how dense vector embeddings are utilized. It demonstrates their role as the input representation for a neural network that is trained to understand and predict relationships between words, specifically if they are 'neighbors'. The text accompanying the figure further clarifies that during this process, the embeddings themselves are updated, highlighting the learning aspect inherent in creating 'better representations'.

**Summary:**
The image illustrates a process where a neural network is used to predict if two words are semantically 'neighbors'. The process begins with two input words, 'Cat' and 'Cute'. Each word is then converted into a dense vector 'Embeddings', represented by colored blocks (red shades for 'Cat' and blue shades for 'Cute'), with varying shades indicating different values within the vector. These two embedding vectors are then fed into a central processing unit labeled 'Neural network'. The 'Neural network' has a specific 'Task: Are the two words neighbors?'. After processing these embeddings, the neural network generates a 'Model prediction', which is a numerical value, in this case, '0.74', indicating the network's confidence or probability that the two words are neighbors.](images/6d4e9f5f423bcc52ce1e36ffc94970ebcf6ecfde8b605502d46afa1a0f9b5f59.jpg)
Figure 1-7. A neural network is trained to predict if two words are neighbors. During this process, the embeddings are updated to be in line with the ground truth.

The resulting embeddings capture the meaning of words but what exactly does that mean? To illustrate this phenomenon, let’s somewhat oversimplify and imagine we have embeddings of several words, namely “apple” and “baby.” Embeddings attempt to capture meaning by representing the properties of words. For instance, the word “baby” might score high on the properties “newborn” and “human” while the word “apple” scores low on these properties.

As illustrated in Figure 1-8, embeddings can have many properties to represent the meaning of a word. Since the size of embeddings is fixed, their properties are chosen to create a mental representation of the word.

![## Image Analysis: 6c867208a0b31e397d1d43704b46ba5c756c35afb5da7387f2a5a6e4b8f92e3f.jpg

**Conceptual Understanding:**
This image conceptually represents **dense vector embeddings** for words. It illustrates how individual words are transformed from discrete linguistic units into continuous, multi-dimensional numerical vectors. The main purpose is to show that words are not just labels but possess various underlying semantic and grammatical properties, and these properties can be quantified numerically. Each word (e.g., "cats", "puppy") is associated with a vector of numbers, where each number corresponds to the degree or extent to which that word exhibits a particular property or dimension (e.g., "animal", "newborn", "human", "plural", "fruit"). The core idea communicated is that the meaning of a word can be encoded by a collection of its features, expressed as a vector of real-valued numbers. This approach allows for capturing intricate semantic relationships between words, as words with similar meanings will have similar vectors.

**Content Interpretation:**
The image meticulously displays the concept of word embeddings by representing five distinct words—"cats," "puppy," "houses," "apple," and "baby"—as dense numerical vectors. Each vector is a column of values, where each value corresponds to a specific semantic or grammatical property listed on the left: "animal," "newborn," "human," "plural," and "fruit." The ellipses indicate that a full embedding would typically include many more such properties or dimensions. The numerical values within each cell (e.g., ".91", ".71", "-.56", ".94", ".89") quantify the degree of association between the word and its respective property, with positive values indicating a strong presence of the property, negative values indicating a strong absence or inverse relationship, and values near zero suggesting neutrality. The color-coding (red, blue, orange, purple, green) visually highlights the strongest positive associations for certain properties, enhancing the clarity of the representation.

The system being shown is a conceptual model of how words can be mapped into a high-dimensional vector space, where their positions and relationships are determined by their semantic features. This dense representation allows for capturing nuanced meanings and relationships that are not possible with simpler token-based representations. The image effectively demonstrates how properties like 'animal', 'newborn', 'human', 'plural', and 'fruit' contribute to forming the unique numerical signature of each word. For example, 'puppy' scores high on 'animal' (.93) and 'newborn' (.71), clearly distinguishing it from 'apple' which scores high on 'fruit' (.89) and low on 'animal' (-.67). Similarly, 'cats' and 'houses' both score high on 'plural' (.94), but their other property scores differ significantly, reflecting their distinct meanings.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Words as Multi-dimensional Vectors:** The image clearly demonstrates that words are not treated as isolated units but as points in a multi-dimensional space, represented by vectors of numerical values. Each word (e.g., "cats", "puppy", "apple") corresponds to a unique vector.
    *   **Textual Evidence:** The horizontal labels "cats", "puppy", "houses", "apple", "baby" each head a vertical column of numbers, which constitute their respective vectors. The label "Number of dimensions (properties)" explicitly states the multi-dimensional nature.

2.  **Quantification of Semantic Properties:** Each dimension or property (e.g., "animal", "newborn", "human", "plural", "fruit") is assigned a numerical score for each word. These scores quantify the degree to which a word embodies that property, allowing for a nuanced representation of meaning.
    *   **Textual Evidence:** The left-hand column lists specific properties, and each cell at their intersection with a word contains a precise numerical value (e.g., "cats" has ".91" for "animal", "apple" has ".89" for "fruit").

3.  **Capturing Semantic Relationships:** Words with similar meanings or shared characteristics tend to have similar numerical values for relevant properties. This allows computational models to infer semantic relationships between words.
    *   **Textual Evidence:** "cats" and "puppy" both show high positive values for "animal" (.91 and .93 respectively). "puppy" and "baby" both show high positive values for "newborn" (.71 and .90). "cats," "puppy," and "baby" all show positive values for "human" (.19, .36, .87), indicating varying degrees of association, with "baby" being the highest. "cats" and "houses" both have high values for "plural" (.94), despite being semantically very different otherwise.

4.  **Density and Richness of Representation:** The numerical nature of these embeddings allows for a much richer and more nuanced representation of word meaning compared to discrete, symbolic representations. The ellipses indicate that a vast number of properties can contribute to this richness.
    *   **Textual Evidence:** The numerical values themselves (e.g., .91, -.11, .19) provide a continuous spectrum of association, rather than a binary yes/no. The presence of "..." for omitted dimensions and the label "Number of dimensions (properties)" imply a high-dimensional space.

5.  **Distinction Between Words:** The unique combination of property scores for each word enables clear differentiation between them. Even words with some shared properties will have distinct overall vectors.
    *   **Textual Evidence:** While "cats" and "puppy" are both animals, "puppy" has a significantly higher "newborn" score (.71 vs -.11), highlighting their difference in age. "apple" is clearly distinguished by its high ".89" for "fruit" and negative scores for animal-related properties.

In essence, the image effectively demonstrates that word embeddings translate linguistic meaning into a mathematical format, making words computationally understandable through their quantified features.

**Document Context:**
This image directly supports the document's section titled "Better Representations with Dense Vector Embeddings." It serves as a crucial visual aid, concretely illustrating the abstract concept of dense vector embeddings mentioned in the text. By providing a simplified yet clear example of how words (like "cats," "puppy," "houses," "apple," "baby") can be represented by numerical vectors based on their properties ("animal," "newborn," "human," "plural," "fruit"), the image enhances the reader's understanding of what these embeddings are and how they function. The accompanying text after the image, "The values of embeddings represent properties that are used to represent words. We may oversimplify by imagining that dimensions represent concepts (which they don’t), but it helps express the idea," perfectly aligns with the visual, reinforcing that the dimensions (properties) are the underlying features captured by the numerical values. Thus, the image is central to explaining the foundational data structure discussed in the section, making the concept of dense vector embeddings tangible and comprehensible.

**Summary:**
The image illustrates the concept of "dense vector embeddings," a fundamental method in natural language processing (NLP) for representing words as numerical vectors. At the top, five distinct words are presented horizontally: "cats," "puppy," "houses," "apple," and "baby." Vertically on the left, a set of semantic "properties" or "dimensions" is listed: "animal," "newborn," "human," followed by an ellipsis ("..."), then "plural," and "fruit." The ellipses, both within the property list and vertically within each word's column, signify that a real word embedding involves many more properties and dimensions than explicitly shown in this simplified diagram. 

Each of the five words corresponds to a vertical column of numerical values, which together form its vector embedding. Each numerical value within a word's vector quantifies the degree to which that word possesses the property listed in the corresponding row. The values can be positive, indicating a strong association with the property; negative, suggesting a strong absence or inverse relationship; or near zero, implying neutrality. Certain values are highlighted with color to visually emphasize strong positive associations.

For example:
*   The word **"cats"** is represented by values such as ".91" for "animal" (highlighted in red, indicating a strong positive association), "-.11" for "newborn," ".19" for "human," ".94" for "plural" (also red, strong positive), and "-.51" for "fruit."
*   The word **"puppy"** shows values like ".93" for "animal" (blue, strong positive) and ".71" for "newborn" (blue, strong positive), ".36" for "human," "-.82" for "plural," and "-.91" for "fruit."
*   **"houses"** is associated with values such as "-.56" for "animal," "-.32" for "newborn," ".31" for "human," ".94" for "plural" (orange, strong positive), and "-.5" for "fruit."
*   **"apple"** features "-.67" for "animal," "-.1" for "newborn," ".29" for "human," "-.51" for "plural," and ".89" for "fruit" (purple, strong positive).
*   **"baby"** is characterized by values like ".01" for "animal," ".90" for "newborn" (green, strong positive), ".87" for "human" (green, strong positive), "-.11" for "plural," and "-.51" for "fruit."

A vertical dotted arrow on the right side of the image, explicitly labeled "Number of dimensions (properties)," reinforces that these embeddings are multi-dimensional representations. This numerical vector approach allows computational models to capture the rich semantic and grammatical characteristics of words, enabling them to understand word relationships and meanings in a continuous, quantitative manner.](images/6c867208a0b31e397d1d43704b46ba5c756c35afb5da7387f2a5a6e4b8f92e3f.jpg)
Figure 1-8. The values of embeddings represent properties that are used to represent words. We may oversimplify by imagining that dimensions represent concepts (which they don’t), but it helps express the idea.

In practice, these properties are often quite obscure and seldom relate to a single entity or humanly identifiable concept. However, together, these properties make sense to a computer and serve as a good way to translate human language into computer language.

Embeddings are tremendously helpful as they allow us to measure the semantic similarity between two words. Using various distance metrics, we can judge how close one word is to another. As illustrated in Figure 1-9, if we were to compress these embeddings into a two-dimensional representation, you would notice that words with similar meaning tend to be closer. In Chapter 5, we will explore how to compress these embeddings into $n$ -dimensional space.

![## Image Analysis: e4f420cf6bc2ed6312e0a73fef21f57d6047d63a3ac0ad7371e1d0c1f0edb3ca.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified two-dimensional visualization of word embeddings, where each word is mapped to a point in a vector space. The main purpose or message being conveyed is that words which are semantically similar or contextually related are positioned closer to each other in this embedding space, while unrelated words are further apart. The key idea communicated is the spatial representation of semantic similarity through dense vector embeddings.

**Content Interpretation:**
The image demonstrates the concept of word embeddings, specifically how semantic relationships between words are represented by their spatial proximity in a multi-dimensional (here, 2D) space. It illustrates the idea that words with similar meanings are mapped to points that are close to each other.

Supporting evidence from extracted text elements:
*   The cluster of "cats" (red dot), "dog" (grey dot), and "puppy" (blue dot) in the upper-left clearly shows related animal terms grouped together.
*   The grouping of "apple" (purple dot) and "banana" (grey dot) in the upper-right demonstrates the proximity of fruit-related terms.
*   The cluster of "building" (grey dot) and "houses" (orange dot) in the lower-left illustrates the closeness of related structural terms.
*   The pairing of "adult" (grey dot) and "baby" (green dot) in the lower-right indicates the proximity of terms related to human age.

The significance of this arrangement is that it visually confirms the effectiveness of dense vector embeddings in capturing and representing the semantic meaning and relationships between words. The distinct clustering of words by category, and the separation between categories, highlights the model's ability to differentiate and associate meanings.

**Key Insights:**
The main takeaways from this image are:
1.  Word embeddings represent words as points (vectors) in a continuous, multi-dimensional space.
2.  The fundamental principle of word embeddings is that the spatial proximity of these word vectors corresponds to their semantic similarity. Words that are similar in meaning or context are located closer together in this embedding space.
3.  This visualization provides an intuitive understanding of how machine learning models can capture and represent complex linguistic relationships by mapping them into a geometric space.

These insights are directly supported by the textual evidence:
*   The words "cats", "dog", and "puppy" are grouped together, demonstrating that semantically related animal terms are spatially close.
*   The words "apple" and "banana" are clustered, showing the proximity of related fruit terms.
*   Similarly, "building" and "houses" are close, as are "adult" and "baby", further reinforcing the concept that related terms occupy nearby positions in the embedding space.

**Document Context:**
This image directly supports the document's section on "Better Representations with Dense Vector Embeddings" by providing a clear visual example of the core principle. It concretely illustrates the concept that words with similar meanings are represented by vectors that are close to each other in an embedding space. The image, labeled as Figure 1-9, is further clarified by the accompanying text: "Figure 1-9. Embeddings of words that are similar will be close to each other in dimen‐ sional space." This direct correspondence makes the abstract concept of vector embeddings tangible and understandable for the reader, showing how semantic similarity is translated into spatial proximity.

**Summary:**
This image is a two-dimensional grid illustrating the concept of word embeddings. Each cell in the grid is square-shaped, forming a uniform coordinate system. Several words are plotted on this grid, each marked by a small, colored circular dot. The placement of these words visually represents their semantic relationships.

In the upper-left area of the grid, three animal-related words are clustered closely together: "cats" (marked with a red dot), "dog" (marked with a grey dot), and "puppy" (marked with a blue dot). "cats" and "dog" are particularly close, with "puppy" positioned slightly below and to the right of them but still within the same general vicinity.

Moving to the upper-right section, two fruit-related words are grouped: "apple" (marked with a purple dot) and "banana" (marked with a grey dot). These words are positioned near each other, indicating their similar category.

In the lower-left part of the grid, two words related to structures are shown close together: "building" (marked with a grey dot) and "houses" (marked with an orange dot).

Finally, in the lower-right area, two words representing human age stages are clustered: "adult" (marked with a grey dot) and "baby" (marked with a green dot).

The overall layout demonstrates that words sharing similar meanings or belonging to the same semantic category are positioned in close proximity to each other within this embedding space, while words from different categories are separated by greater distances. This visualization effectively illustrates how dense vector embeddings map semantic similarity to spatial closeness.](images/e4f420cf6bc2ed6312e0a73fef21f57d6047d63a3ac0ad7371e1d0c1f0edb3ca.jpg)
Figure 1-9. Embeddings of words that are similar will be close to each other in dimen‐ sional space.

# Types of Embeddings

There are many types of embeddings, like word embeddings and sentence embed‐ dings that are used to indicate different levels of abstractions (word versus sentence), as illustrated in Figure 1-10.

Bag-of-words, for instance, creates embeddings at a document level since it repre‐ sents the entire document. In contrast, word2vec generates embeddings for words only.

Throughout the book, embeddings will take on a central role as they are utilized in many use cases, such as classification (see Chapter 4), clustering (see Chapter 5), and semantic search and retrieval-augmented generation (see Chapter 8). In Chapter 2, we will take our first deep dive into token embeddings.

![## Image Analysis: 57e990ffd4aff594696292bb523fceb29d393916990026448bc1f4acd67bbf97.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of generating text embeddings, which are numerical representations of textual data. It demonstrates how different forms of text input, ranging from entire documents to individual sub-word tokens, can be transformed into these embeddings using a central "Representation model."

The main purpose or message conveyed is to show the versatility and different granularities at which text can be embedded. It highlights that embeddings are not a single type of representation but can vary based on the scope of the input (document, sentence, word, token) and that a common model can handle these different levels of input.

Key ideas communicated include: the concept of text embeddings as numerical representations; the role of a "Representation model" in this transformation; the importance of tokenization as a pre-processing step for fine-grained analysis; and the existence of distinct embedding types corresponding to different linguistic granularities (document, sentence, word, and token).

**Content Interpretation:**
The image explicitly shows two main processes:
1.  **Tokenization:** The step "Split input up into tokens" which breaks down a sentence ("Her vocalization was melodic") into smaller units like "Her", "vocal", "##ization", "was", "melodic". This indicates that text can be pre-processed before embedding.
2.  **Embedding Generation:** The "Representation model" with the internal text "Embed the input text" is the core process responsible for converting the various forms of text (raw document, raw sentence, individual tokens) into numerical embeddings.

**Concepts shown:**
*   **Input:** Demonstrates that the "Representation model" can accept different lengths and types of textual "Input" ("The dominant sequence transduction models are based on complex...", "My cat is cute.", "Her vocalization was melodic").
*   **Tokens:** Illustrates that a sentence can be broken down into "tokens," some of which may be sub-word units (e.g., "vocal", "##ization" together forming "vocalization").
*   **Embeddings:** The primary concept is "embeddings," which are the numerical representations. The image shows four distinct types:
    *   "Document embeddings" (red blocks): Generated from longer, document-level text.
    *   "Sentence embeddings" (orange blocks): Generated from full sentences.
    *   "Token embeddings" (purple blocks): Generated from individual tokens.
    *   "Word embeddings" (green blocks): Generated from tokens, potentially by combining sub-word tokens into full word representations.

**Relationships:**
*   **Input-to-Model:** All forms of textual input (document, sentence, token) flow into the "Representation model."
*   **Model-to-Embeddings:** The "Representation model" outputs different types of embeddings.
*   **Tokenization-to-Tokens:** The "Split input up into tokens" process directly leads to the individual tokens.
*   **Token-to-Word Embedding:** The flow from multiple token embeddings (like "vocal" and "##ization") to a single "Word embeddings" block implies a hierarchical relationship where word embeddings can be constructed from token embeddings.

**Significance:** The different colored blocks (red, orange, green, purple) visually signify distinct types of embeddings. The multiple small rectangles within each block suggest that embeddings are typically multi-dimensional vectors. The varying number of small rectangles (e.g., 5 for sentence, 4 for word, 3 for token) might imply different vector dimensions, although this is not explicitly stated. The clear separation and distinct names for each embedding type underscore the importance of granularity in text representation. All extracted text elements from the transcription directly name and describe these inputs, processes, and outputs, providing explicit evidence for the interpretation.

**Key Insights:**
**Main Takeaway 1: Text can be represented numerically in various ways, known as embeddings, to capture its meaning.**
*   *Evidence:* The image explicitly labels "Document embeddings," "Sentence embeddings," "Word embeddings," and "Token embeddings" as the outputs of processing text, demonstrating these different forms of representation.

**Main Takeaway 2: The granularity of the input text largely determines the type of embedding generated.**
*   *Evidence:* A long input text ("The dominant sequence...") leads to "Document embeddings"; a sentence input ("My cat is cute.") leads to "Sentence embeddings"; and a tokenized sentence input ("Her vocalization was melodic" -> tokens) leads to "Token embeddings" and "Word embeddings." The arrows clearly link specific input types to specific embedding outputs, illustrating this dependency.

**Main Takeaway 3: A "Representation model" is the core component that performs the embedding process for all input granularities.**
*   *Evidence:* The central, large box is explicitly labeled "Representation model" with the action "Embed the input text," and all diverse inputs converge to it, highlighting its universal role in creating embeddings.

**Main Takeaway 4: Tokenization is a common and often necessary pre-processing step for generating finer-grained embeddings (specifically word and token embeddings).**
*   *Evidence:* The arrow originating from the input sentence "Her vocalization was melodic" explicitly states "Split input up into tokens," which is a crucial step before individual tokens feed into the model to produce "Token embeddings" and subsequently "Word embeddings."

**Main Takeaway 5: Word embeddings can sometimes be composed from sub-word token embeddings, indicating a hierarchical relationship in textual representation.**
*   *Evidence:* The visual connection from the "vocal" and "##ization" token embeddings to the singular "Word embeddings" block suggests that a word (like "vocalization") broken into sub-word tokens can be re-assembled or represented at the word level from its constituent token embeddings.

**Document Context:**
This image directly supports the document section "Types of Embeddings" by visually illustrating *how* different types of embeddings are created and *what* different granularities of embeddings exist. It provides concrete examples of input text and their corresponding embedding outputs, making the abstract concept of embeddings more tangible. It clarifies that "embeddings can be created for different types of input," as stated in the text immediately following the image.

**Summary:**
This diagram illustrates how various forms of textual input are processed to generate different types of numerical representations called "embeddings." These embeddings can capture the meaning of text at different levels of granularity.

The process begins with "Input" text, shown in three examples:
1.  A long text snippet: "The dominant sequence transduction models are based on complex..." This represents a document-level input.
2.  A complete sentence: "My cat is cute." This represents a sentence-level input.
3.  Another complete sentence: "Her vocalization was melodic." This sentence demonstrates a more granular embedding process.

All these inputs are fed into a central component called the "Representation model," whose core function is to "Embed the input text." There is also a small graphic symbol in the top right corner of the "Representation model" box, which looks like a folded page corner or an arrow.

Let's trace each path:

*   **Path 1: Document Embeddings**
    *   When the long document-level input ("The dominant sequence transduction models are based on complex...") enters the "Representation model," it is processed to produce "Document embeddings," visually represented by a block of red rectangles.

*   **Path 2: Sentence Embeddings**
    *   When the sentence-level input ("My cat is cute.") enters the "Representation model," it is processed to generate "Sentence embeddings," depicted as a block of orange rectangles.

*   **Path 3: Token and Word Embeddings**
    *   For the sentence "Her vocalization was melodic," an additional pre-processing step occurs first: the input is explicitly labeled as being "Split input up into tokens."
    *   This tokenization breaks the sentence into individual tokens, each housed in a separate rounded rectangular box: "Her," "vocal," "##ization," "was," and "melodic." The "##ization" token with the "##" prefix indicates it is a sub-word unit, a continuation of a previous word.
    *   Each of these individual tokens then enters the "Representation model." Each token is processed to "Embed the input text."
    *   The direct output from processing these individual tokens are "Token embeddings," shown as multiple distinct blocks of purple rectangles.
    *   Furthermore, a specific connection from some of the purple "token embeddings" (specifically those corresponding to "vocal" and "##ization") leads to "Word embeddings," represented by a block of green rectangles. This indicates that "word embeddings" can sometimes be derived or composed from multiple "token embeddings" when a word is broken into sub-word units.

In summary, the diagram clearly shows that a single "Representation model" can effectively transform various textual inputs—from lengthy documents to individual sub-word tokens—into different types of numerical embeddings, enabling a flexible representation of text meaning.](images/57e990ffd4aff594696292bb523fceb29d393916990026448bc1f4acd67bbf97.jpg)
Figure 1-10. Embeddings can be created for different types of input.

# Encoding and Decoding Context with Attention

The training process of word2vec creates static, downloadable representations of words. For instance, the word “bank” will always have the same embedding regardless of the context in which it is used. However, “bank” can refer to both a financial bank as well as the bank of a river. Its meaning, and therefore its embeddings, should change depending on the context.

A step in encoding this text was achieved through recurrent neural networks (RNNs). These are variants of neural networks that can model sequences as an additional input.

To do so, these RNNs are used for two tasks, encoding or representing an input sentence and decoding or generating an output sentence. Figure 1-11 illustrates this concept by showing how a sentence like $^ { \mathfrak { c } } \mathrm { I }$ love llamas” gets translated to the Dutch “Ik hou van lama’s.”

![## Image Analysis: 6ec142c6658af6aeddafdd21c0e1f4010268032b7e0cdeb1fe8c94e46dd8a9be.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a sequence-to-sequence model used in neural machine translation. Its main purpose is to illustrate how an input sequence of words in one language is processed by an 'Encoder (RNN)' to create a hidden representation, and then how a 'Decoder (RNN)' uses this representation to generate an output sequence of words in a different target language. The key idea communicated is the separation of concerns: one neural network (Encoder) for understanding and representing the source language context, and another (Decoder) for generating text based on that context in the target language.

**Content Interpretation:**
This image illustrates a neural machine translation system, specifically an encoder-decoder architecture that leverages Recurrent Neural Networks (RNNs). The system processes an input sequence, transforms it into an internal representation, and then generates an output sequence in a different language. The central components are the 'Encoder (RNN)', responsible for 'representing language' by capturing the meaning of the input, and the 'Decoder (RNN)', responsible for 'generating language' by producing the translated output. The relationship between these components is sequential and hierarchical, where the encoder's output feeds the decoder's input, enabling the translation process.

**Key Insights:**
The main takeaway from this image is the fundamental two-part structure of a neural machine translation system: an encoder and a decoder. The 'Encoder (RNN)' serves to understand and condense the meaning of the input sequence, as indicated by its 'Task: representing language'. Conversely, the 'Decoder (RNN)' uses this condensed meaning to construct the output sequence in the target language, as shown by its 'Task: generating language'. The specific example, translating 'I love llamas' to 'Ik hou van lama's', clearly demonstrates the capability of such a system to handle language translation. This illustrates that machine translation involves not just word-for-word replacement but a process of encoding the original language's semantic content and then decoding it into the target language.

**Document Context:**
This image directly supports the document's section on 'Encoding and Decoding Context with Attention' by visually demonstrating a core concept of sequence-to-sequence models in neural machine translation. It provides a clear, high-level overview of how two recurrent neural networks, an encoder and a decoder, work in tandem to translate an input sequence from one language (English, 'I love llamas') to another (Dutch, 'Ik hou van lama's'). This visual example helps readers understand the foundational mechanism before potentially delving into more complex topics like 'Attention' which often builds upon this basic encoder-decoder structure. The accompanying text after the image, 'Figure 1-11. Two recurrent neural networks (decoder and encoder) translating an input sequence from English to Dutch,' further confirms and reinforces this context.

**Summary:**
The image illustrates the fundamental architecture of a neural machine translation system, specifically an encoder-decoder model utilizing Recurrent Neural Networks (RNNs). The process begins with an 'Input sequence' of words: 'I', 'love', and 'llamas'. Each word from this input sequence is fed sequentially into the 'Encoder (RNN)'. The primary 'Task' of this 'Encoder (RNN)' is 'representing language', meaning it processes the input words to create a meaningful internal representation or context vector. This representation then serves as the input for the 'Decoder (RNN)'. The 'Decoder (RNN)' has the 'Task: generating language'. It takes the encoded representation and sequentially produces the 'Output sequence' of translated words: 'Ik', 'hou', 'van', 'lama's'. The overall system is labeled 'Neural machine translation', highlighting its function in converting text from a source language (English in this example) to a target language (Dutch).](images/6ec142c6658af6aeddafdd21c0e1f4010268032b7e0cdeb1fe8c94e46dd8a9be.jpg)
Figure 1-11. Two recurrent neural networks (decoder and encoder) translating an input sequence from English to Dutch.

Each step in this architecture is autoregressive. When generating the next word, this architecture needs to consume all previously generated words, as shown in Figure 1-12.

![## Image Analysis: bc98f2d5410ab7b5a98bfc6bd307e7221a8df8e2f70a2aa376a13a87224f1a18.jpg

**Conceptual Understanding:**
This image conceptually illustrates an auto-regressive sequence generation process, commonly used in natural language processing (NLP) models for tasks like machine translation or text generation. The main purpose of the image is to demonstrate how an output sequence is built iteratively, where each new token generated becomes part of the input context for predicting the subsequent token. It conveys the idea of a step-by-step decoding mechanism that uses its own past outputs to inform future predictions, thus maintaining a coherent and contextually relevant sequence.

**Content Interpretation:**
The image depicts an auto-regressive sequence generation process, most commonly found in Natural Language Processing (NLP) models like sequence-to-sequence or Transformer decoders. It shows how an initial input sequence is processed to generate an output sequence, one token at a time. The key mechanism highlighted is that each new output token is generated by taking the original input sequence and appending all previously generated output tokens as part of the current input context.

Specifically, the blue boxes represent the fixed initial input sequence, "I love llamas". The green boxes represent the tokens that are being generated. The arrows indicate the flow of information from the current set of input tokens to the next predicted output token. The process illustrates a form of conditional generation where the probability of generating the next token is conditioned on both the original source input and the tokens already generated in the target sequence.

This system allows the model to maintain coherence and context as it builds a complete output sequence, ensuring that each new word or sub-word unit fits logically with what has already been produced.

**Key Insights:**
The main takeaway from this image is the concept of auto-regressive decoding in sequence generation. Key insights include:

1.  **Iterative Generation:** Output sequences are built one token at a time, not all at once. This is evident as each 'Step' produces a single new output token.

2.  **Contextual Dependence:** Each new output token's generation is dependent on not just the initial input but also on the sequence of tokens already generated. For example, in 'Step 2', 'lk' is added to the input, influencing the generation of 'hou'.

3.  **Accumulating Context:** The input context for generating the next token grows with each step, incorporating all previously generated tokens. This is clearly shown by the increasing number of green boxes in the 'Input' section across the steps.

4.  **Forward Dependency:** There is a clear forward dependency where the output of one step becomes part of the input for the next, forming a chain of predictions.

The specific text elements like 'Step 1' generating 'lk', and 'Step 2' using 'lk' to generate 'hou', provide concrete evidence for these insights, demonstrating the sequential and context-dependent nature of the generation process.

**Document Context:**
This image directly illustrates the mechanism described in the accompanying document context: "Each previous output token is used as input to generate the next token." It visually clarifies how a sequence generation model iteratively produces output, making the abstract concept of auto-regressive decoding concrete. It serves as a foundational example for understanding how models generate text sequentially, which is critical for tasks like machine translation, text summarization, or dialogue generation, which would likely be discussed in the broader section on "Encoding and Decoding Context with Attention". The image specifically shows how the 'context' for decoding grows with each step as previously generated tokens are incorporated into the input for the subsequent prediction.

**Summary:**
This image illustrates a sequential process of token generation, likely within a natural language processing model, where each new output token is generated using the initial input sequence combined with all previously generated output tokens. The process unfolds in four distinct steps.

In Step 1, the initial input sequence "I love llamas" is used to generate the first output token, "lk".

For Step 2, the input expands to include the original sequence "I love llamas" plus the newly generated token from Step 1, "lk". This combined input then produces the next output token, "hou".

Step 3 follows the same pattern: the input consists of the original sequence "I love llamas" and the two previously generated tokens, "lk" and "hou". From this, the output token "van" is generated.

Finally, in Step 4, the input comprises "I love llamas" along with the accumulated generated tokens "lk", "hou", and "van". This comprehensive input leads to the generation of the final output token, "lama's".

This demonstrates a common auto-regressive decoding mechanism where the model iteratively builds a target sequence, using its own past predictions as context for future predictions.](images/bc98f2d5410ab7b5a98bfc6bd307e7221a8df8e2f70a2aa376a13a87224f1a18.jpg)
Figure 1-12. Each previous output token is used as input to generate the next token.

The encoding step aims to represent the input as well as possible, generating the context in the form of an embedding, which serves as the input for the decoder. To generate this representation, it takes embeddings as its inputs for words, which means we can use word2vec for the initial representations. In Figure 1-13, we can observe this process. Note how the inputs are processed sequentially, one at a time, as well as the output.

![## Image Analysis: f549a5155a751b84e4daa94ab82979215ec41d5cb450f31e2ec021adc9bd2905.jpg

**Conceptual Understanding:**
Conceptually, this image represents the architecture of a fundamental sequence-to-sequence neural network model, specifically an Encoder-Decoder model utilizing Recurrent Neural Networks (RNNs).

The main purpose of the image is to visually illustrate how an input sequence of words is processed and transformed into an output sequence. It demonstrates the complete flow from converting individual words into numerical embeddings, encoding the entire input sequence into a single 'context' representation, and then decoding that context into a new, semantically related output sequence.

The key ideas being communicated are:
*   The use of word embeddings (e.g., word2vec) to represent words numerically.
*   The sequential processing of inputs by an Encoder (RNN) to capture the overall meaning or context of a phrase or sentence.
*   The generation of a 'Context embedding' that summarizes the entire input sequence.
*   The sequential generation of an output sequence by a Decoder (RNN), based on the provided context.
*   The application of this architecture to tasks like machine translation, exemplified by the input 'I love llamas' and the likely Dutch translation 'lk hou van lama's'.

**Content Interpretation:**
The image depicts an Encoder-Decoder Recurrent Neural Network (RNN) architecture. It illustrates the following processes:

1.  **Word Embedding Generation:** Input words ("I", "love", "llamas") are first transformed into "word2vec embeddings," which are numerical vector representations of words.
2.  **Sequential Encoding:** These word embeddings are fed in sequence (Order of processing: 1, 2, 3) into the "Encoder (RNN)". The Encoder processes the input sequence, building an internal representation of the sentence.
3.  **Context Vector Creation:** The Encoder's final state or a summary of its internal states results in a single, fixed-size "Context embedding." This vector encapsulates the semantic meaning of the entire input sequence.
4.  **Sequential Decoding:** The "Context embedding" is then passed to the "Decoder (RNN)". The Decoder uses this context to generate an output sequence word by word ("lk" (4), "hou" (5), "van" (6), "lama's" (7)).

The significance of the information presented is to demonstrate how a sequence-to-sequence model can map an input sequence (like a sentence in one language) to an output sequence (like the same sentence in another language, in this case, Dutch). The use of "word2vec embeddings" highlights the importance of semantic word representations, while the RNNs in both encoder and decoder emphasize their capability in handling sequential data and capturing dependencies. The "Context embedding" is critical as it serves as the complete representation of the input's meaning, enabling the decoder to generate a coherent and relevant output.

**Key Insights:**
The image offers several key takeaways and insights:

1.  **Sequence-to-Sequence Modeling:** The core concept demonstrated is the ability of neural networks to transform one sequence into another (e.g., a sentence in one language to a sentence in another). This is evident from the input "I love llamas" and the corresponding output "lk hou van lama's".
2.  **Importance of Word Embeddings:** The use of "word2vec embeddings" for input words highlights that numerical vector representations are crucial for capturing semantic meaning and enabling neural networks to process linguistic information effectively.
3.  **Encoder-Decoder Paradigm:** The image clearly shows the two distinct phases: an "Encoder (RNN)" that processes the input sequence and a "Decoder (RNN)" that generates the output sequence, mediated by a "Context embedding". This paradigm is fundamental to many modern NLP applications.
4.  **Context Vector as Information Bottleneck:** The "Context embedding" is a crucial concept. It acts as a summary of the entire input sequence, forcing the Encoder to condense all essential information into a fixed-size vector, which the Decoder then uses as its starting point for generation. This emphasizes the challenge and solution for representing the full meaning of a variable-length input.
5.  **Sequential Processing by RNNs:** The numbered arrows (1-3 for encoding, 4-7 for decoding) and the "Order of processing" label illustrate the step-by-step nature of how RNNs handle and generate sequences, leveraging information from previous steps.

These insights are directly supported by all the transcribed text: the labels "word2vec embeddings", "Encoder (RNN)", "Context embedding", "Decoder (RNN)", the input and output words, and the sequential numbering, all combine to illustrate these key principles of sequence modeling.

**Document Context:**
This image, titled "Figure 1-13. Using word2vec embeddings, a context embedding is generated that repre‐ sents the entire sequence," is highly relevant to a document section on "Encoding and Decoding Context with Attention." It serves as a foundational visual explanation for how the "context" of an entire sequence is captured and utilized. The diagram directly illustrates the concepts mentioned in the surrounding text, such as the use of "word2vec embeddings" and the generation of a "context embedding that represents the entire sequence." It provides a clear visual blueprint of the Encoder-Decoder architecture, which is a prerequisite for understanding more advanced concepts like attention mechanisms in sequence-to-sequence models. It educates the reader on the fundamental steps of converting an input sequence into a meaningful context vector and then generating an output sequence from it.

**Summary:**
This diagram illustrates a fundamental neural network architecture known as an Encoder-Decoder model, specifically implemented using Recurrent Neural Networks (RNNs), which is commonly used in natural language processing tasks like machine translation.

The process begins with an input sequence of words: "I", "love", and "llamas". Each of these words is first converted into a numerical representation called a "word2vec embedding". These embeddings are vector representations that capture the semantic meaning of words, shown as small shaded bars in the diagram.

Next, these word2vec embeddings are fed sequentially into the "Encoder (RNN)". The "Order of processing" is indicated by the numbered arrows: the embedding for "I" enters first (1), followed by "love" (2), and then "llamas" (3). The Encoder (RNN) processes these inputs one by one, continuously updating its internal state to build a summary of the entire input sentence.

Once all input words have been processed by the Encoder, it generates a single fixed-size vector called the "Context embedding". This context embedding, depicted as a blue-shaded bar, effectively encapsulates the comprehensive meaning and information of the entire input sequence ("I love llamas"). It acts as a bridge between the Encoder and the Decoder.

Finally, the "Context embedding" is passed to the "Decoder (RNN)". The Decoder then uses this context to generate an output sequence, word by word, in a sequential manner. The output words are "lk" (4), "hou" (5), "van" (6), and "lama's" (7). Notably, "lk hou van lama's" is the Dutch translation of "I love llamas", demonstrating the model's ability to translate sentences.

In essence, this figure visually explains how a machine learning model can understand an entire input sentence, summarize its meaning, and then generate a corresponding output sentence, making it a powerful tool for tasks like language translation.](images/f549a5155a751b84e4daa94ab82979215ec41d5cb450f31e2ec021adc9bd2905.jpg)
Figure 1-13. Using word2vec embeddings, a context embedding is generated that repre‐ sents the entire sequence.

This context embedding, however, makes it difficult to deal with longer sentences since it is merely a single embedding representing the entire input. In 2014, a solution called attention was introduced that highly improved upon the original architecture.4 Attention allows a model to focus on parts of the input sequence that are relevant to one another (“attend” to each other) and amplify their signal, as shown in Fig‐ ure 1-14. Attention selectively determines which words are most important in a given sentence.

For instance, the output word “lama’s” is Dutch for “llamas,” which is why the atten‐ tion between both is high. Similarly, the words “lama’s” and “I” have lower attention since they aren’t as related. In Chapter 3, we will go more in depth on the attention mechanism.

![## Image Analysis: b69b051fdefe0464c5e4c8d4f47c8ab3d1c32b1850b836069b439eecd65cbbcd.jpg

**Conceptual Understanding:**
This image conceptually represents an attention matrix, a component within a neural network model, likely in the context of natural language processing or machine translation. The main purpose of the image is to visually illustrate how an 'attention mechanism' identifies and quantifies the relationships between words in two different sequences. It demonstrates that words with similar meanings or direct translations receive higher 'attention weights', indicating a stronger connection or relevance between them. Essentially, it shows which words 'pay attention' to which other words, allowing the model to focus on the most relevant parts of the input sequence when processing an output sequence.

**Content Interpretation:**
The image displays an attention matrix, a core component in natural language processing (NLP) models, particularly for tasks like machine translation. It shows the relationship between words of two different sentences, "Ik hou van lama's" (Dutch) and "I love llamas" (English), which are direct translations of each other. The intensity of the blue color in each cell of the matrix represents the 'attention weight' or the degree of relevance a word from the Dutch sentence has to a word from the English sentence. This illustrates how the attention mechanism maps semantically related words, even across different languages.

Specifically:
*   **'Ik' (Dutch)** shows high attention with **'I' (English)**, as indicated by the dark blue cell at their intersection.
*   **'hou' (Dutch)** shows high attention with **'love' (English)** (dark blue cell), and also some attention with **'llamas' (English)** (light blue cell).
*   **'van' (Dutch)** shows medium attention with **'love' (English)** (medium blue cell).
*   **'lama's' (Dutch)** shows high attention with **'llamas' (English)** (dark blue cell).

The overarching concept is that the model 'attends' more to words that are semantically similar or direct translations, assigning them higher attention weights, as explicitly stated by the text "Words with similar meaning have higher attention weights since they are highly related."

**Key Insights:**
The main takeaway from this image is that attention mechanisms in NLP models are designed to identify and quantify the semantic relationships between words, even across different languages. Specifically:
1.  **Semantic Correlation:** The model assigns higher attention weights (darker blue cells) to words that are direct translations or have similar meanings, as seen with 'Ik' and 'I', 'hou' and 'love', and 'lama's' and 'llamas'.
2.  **Graded Relationships:** Attention is not binary; it's a spectrum. Words can have high, medium, or low attention weights to other words, indicating varying degrees of relatedness (represented by different shades of blue).
3.  **Contextual Linkage:** The mechanism highlights which words in one sequence are most relevant to understanding individual words in another sequence, thereby helping the model build a richer contextual understanding. The text "Words with similar meaning have higher attention weights since they are highly related" directly supports this insight by providing the rationale behind the visual representation of attention weights.

**Document Context:**
This image directly supports the document's section on "Encoding and Decoding Context with Attention" and the subsequent text: "Figure 1-14. Attention allows a model to “attend” to certain parts of sequences that might relate more or less to one another." It visually demonstrates how attention mechanisms work by showing a matrix of attention weights between words of two related sentences. This helps to concretize the abstract concept of 'attention' in a neural network model, particularly in the context of understanding and translating natural language. The example of Dutch and English sentences illustrates how the model identifies semantic correspondences across different languages, which is crucial for tasks like machine translation and cross-lingual understanding. It provides a foundational visual explanation of how context is encoded and decoded by highlighting relevant word pairings.

**Summary:**
The image illustrates how an attention mechanism assigns varying 'attention weights' between words in two sequences, one English and one Dutch, which are translations of each other. The visualization uses a matrix where rows represent words from the Dutch phrase "Ik hou van lama's" and columns represent words from the English phrase "I love llamas". The intensity of the blue color in each cell of the matrix indicates the level of attention: darker blue signifies 'High attention,' while lighter blue and white represent 'Low attention.'

Specifically, the top-left cell, corresponding to 'Ik' and 'I', is dark blue, indicating high attention. The cell at the intersection of 'hou' and 'love' is also dark blue, showing high attention. Similarly, the cell at 'van' and 'love' is a lighter shade of blue, suggesting a medium level of attention. The cell for 'lama's' and 'llamas' is dark blue, representing high attention. This visually demonstrates that words with similar meanings or direct translations across the two phrases receive higher attention weights, reinforcing the idea that attention mechanisms identify strong relationships between relevant parts of different sequences.](images/b69b051fdefe0464c5e4c8d4f47c8ab3d1c32b1850b836069b439eecd65cbbcd.jpg)
Figure 1-14. Attention allows a model to “attend” to certain parts of sequences that might relate more or less to one another.

By adding these attention mechanisms to the decoder step, the RNN can generate signals for each input word in the sequence related to the potential output. Instead of passing only a context embedding to the decoder, the hidden states of all input words are passed. This process is demonstrated in Figure 1-15.

![## Image Analysis: 656f282dd4254190db4561e7d9ad3a8e89c4386136512e08466998ea96cf6f8e.jpg

**Conceptual Understanding:**
This image conceptually represents a neural network architecture designed for sequence-to-sequence tasks, specifically focusing on how an 'attention mechanism' enhances the decoding process. The main purpose is to illustrate that when translating a sentence, the decoder does not simply produce output based on a general understanding of the entire input. Instead, it can dynamically 'attend to' or focus on specific, relevant parts of the original input sentence as it generates each word of the output translation. This selective focus, demonstrated by the arrow pointing back to 'llamas,' helps maintain context and improve translation accuracy, particularly for longer sentences or when specific word-level alignment is crucial.

**Content Interpretation:**
This image depicts the architecture of an encoder-decoder model enhanced with an attention mechanism, commonly used in sequence-to-sequence tasks like neural machine translation. It shows how an input sentence ("I love llamas") is processed and translated into an output sentence ("Ik hou van..."), highlighting the crucial role of attention in focusing on relevant input parts during decoding. The "Encoder (RNN)" processes the input sequence and generates an intermediate representation. The "Attention decoder (RNN)" then uses this representation, along with an attention mechanism, to generate the output sequence. The feedback loop explicitly shows the attention mechanism in action, directing the decoder's focus to specific input words (e.g., "llamas") to inform the generation of the corresponding output word.

**Key Insights:**
The main takeaway from this image is the critical role of the attention mechanism in improving the performance of sequence-to-sequence models, particularly in translation. It illustrates that instead of processing input strictly sequentially and relying on a single, compressed representation from the encoder, the "Attention decoder (RNN)" can dynamically "attend to most relevant input" when generating each output word. This allows for better handling of long sequences and ensures that the decoder has access to the most pertinent information from the original input at each step of the translation. The highlighted input word "llamas" and the feedback arrow explicitly show this selective focus, which is essential for accurate translation, especially for words that are direct translations or have strong contextual links.

**Document Context:**
This image is highly relevant to the "Encoding and Decoding Context with Attention" section of the document. It visually explains how the attention mechanism facilitates the decoding process by allowing the model to focus on specific input words, rather than relying solely on a fixed-size context vector from the encoder. The accompanying text, "After generating the words “Ik,” “hou,” and “van,” the attention mechanism of the decoder enables it to focus on the word “llamas” before it generates the Dutch translation (“lama’s”)," directly describes the process illustrated in the figure. It demonstrates a core concept of modern neural sequence models, explaining how context is maintained and utilized more effectively during generation.

**Summary:**
The image illustrates a neural machine translation process using an encoder-decoder architecture with an attention mechanism. Initially, the input English words "I," "love," and "llamas" are fed into the system. These words are first converted into "word2vec embeddings," which are numerical representations. These embeddings are then processed by an "Encoder (RNN)," which is a Recurrent Neural Network responsible for encoding the input sequence into a context vector. The output of the encoder is then passed to an "Attention decoder (RNN)," also a Recurrent Neural Network, which is responsible for generating the output translation. During the generation of the output words, such as "Ik," "hou," and "van," the attention mechanism within the decoder allows it to dynamically "attend to most relevant input." This is indicated by an arrow looping back from the output generation process to the input word "llamas," signifying that the decoder specifically focuses on the word "llamas" from the original input when generating its corresponding translation (which, based on the document context, would be "lama's"). This mechanism enables the decoder to prioritize specific parts of the input sentence that are most relevant to the word it is currently translating, thereby improving the accuracy and coherence of the translation.](images/656f282dd4254190db4561e7d9ad3a8e89c4386136512e08466998ea96cf6f8e.jpg)
Figure 1-15. After generating the words “Ik,” “hou,” and “van,” the attention mechanism of the decoder enables it to focus on the word “llamas” before it generates the Dutch translation (“lama’s”).

As a result, during the generation of “Ik hou van lama’s,” the RNN keeps track of the words it mostly attends to perform the translation. Compared to word2vec, this architecture allows for representing the sequential nature of text and the context in which it appears by “attending” to the entire sentence. This sequential nature, however, precludes parallelization during training of the model.

# Attention Is All You Need

The true power of attention, and what drives the amazing abilities of large lan‐ guage models, was first explored in the well-known “Attention is all you need” paper released in 2017.5 The authors proposed a network architecture called the Transformer, which was solely based on the attention mechanism and removed the recurrence network that we saw previously. Compared to the recurrence network, the Transformer could be trained in parallel, which tremendously sped up training.

In the Transformer, encoding and decoder components are stacked on top of each other, as illustrated in Figure 1-16. This architecture remains autoregressive, needing to consume each generated word before creating a new word.

![## Image Analysis: 9582283a3adc69fb053cbb4b25b5b3e3e2acc46f5dd3632b37d8e9a174d5b648.jpg

**Conceptual Understanding:**
This image conceptually represents the high-level architecture of a Transformer neural network, a model primarily used for sequence-to-sequence tasks. Its main purpose is to illustrate how an input sequence (e.g., a sentence in one language) is processed through a 'Transformer encoder' and then a 'Transformer decoder' to generate an output sequence (e.g., the same sentence translated into another language). The key idea communicated is the modular, stacked encoder-decoder design that forms the backbone of the Transformer, enabling it to perform complex tasks like language translation by sequentially processing and generating tokens.

**Content Interpretation:**
The image illustrates the fundamental encoder-decoder architecture of a Transformer model. It depicts a sequence-to-sequence process where an input sequence of tokens (words) is transformed into an output sequence of tokens, exemplified by an English to Dutch translation. The 'Transformer encoder' is responsible for processing the input sequence to generate rich, contextualized representations, which are then passed to the 'Transformer decoder'. The 'Transformer decoder' subsequently uses these representations, along with previously generated output tokens, to generate the final output sequence. The stacked nature of both the encoder and decoder blocks signifies the multi-layered processing common in deep learning architectures, where each layer refines the representations. The icons within the topmost encoder and decoder blocks (an arrow and a speech bubble, respectively) are visual cues that could represent the flow of information or the generative aspect of the decoder, though their specific functional meaning isn't explicitly textual.

**Key Insights:**
The main takeaway is that the Transformer model employs a two-part, stacked encoder-decoder architecture for sequence-to-sequence tasks. The 'Input sequence' 'I love llamas' is processed by the 'Transformer encoder' which converts it into an internal representation. This representation is then used by the 'Transformer decoder' to generate the 'Output sequence' 'Ik hou van lama's'. This demonstrates the Transformer's utility in machine translation, where an input in one language (English) is transformed into an output in another (Dutch). The visual representation emphasizes the modularity and sequential processing nature of the model, where information flows from input through the encoder and decoder to the output.

**Document Context:**
This image directly supports the document's narrative by visually representing the core architecture of the Transformer model. The text after the image, stating 'The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder,' perfectly aligns with and is explained by this diagram. It provides a foundational understanding of how an input sequence is processed through the distinct encoder and decoder components to yield an output sequence, which is crucial for comprehending the subsequent detailed explanations of the Transformer's mechanisms in the 'Attention Is All You Need' context.

**Summary:**
The image illustrates the high-level architecture of a Transformer model, a neural network designed for sequence-to-sequence tasks like machine translation. It begins with an 'Input sequence' represented by three individual words: 'I', 'love', and 'llamas'. These words are fed into a central processing unit, outlined by a dashed rectangle, which contains the core Transformer components. Inside this unit, there are two main stacked sections. The upper section, labeled 'Transformer encoder', consists of multiple stacked light blue blocks, visually suggesting layers of processing. The topmost 'Transformer encoder' block also contains a small graphic resembling an arrow pointing down and to the left. Below the encoder is the 'Transformer decoder', composed of multiple stacked pink blocks, also indicating layered operations. The topmost 'Transformer decoder' block includes a small icon resembling a speech bubble. After processing through these stacked encoder and decoder blocks, the model produces an 'Output sequence', which in this example consists of four words: 'Ik', 'hou', 'van', and 'lama's'. This output sequence is a Dutch translation of the English input sequence, demonstrating the model's capability in language translation.](images/9582283a3adc69fb053cbb4b25b5b3e3e2acc46f5dd3632b37d8e9a174d5b648.jpg)
Figure 1-16. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder.

Now, both the encoder and decoder blocks would revolve around attention instead of leveraging an RNN with attention features. The encoder block in the Transformer consists of two parts, self-attention and a feedforward neural network, which are shown in Figure 1-17.

![## Image Analysis: ed2a35607fe98e41bf1dcf244df130916a43f8eb6599a4c233954da6646302e5.jpg

**Conceptual Understanding:**
This image conceptually represents a single encoder block of a Transformer model, a core component in many modern natural language processing architectures. Its main purpose is to illustrate how an input sequence of words is processed to generate enriched, contextualized intermediate representations. The key ideas communicated are the hierarchical processing of input tokens, the critical role of self-attention in understanding word relationships within a sequence, and the subsequent transformation by a feedforward neural network to produce more abstract and informative representations.

**Content Interpretation:**
The image details the internal architecture and operational flow of a single Transformer encoder block. It shows how a sequence of input words, represented as 'I love llamas', is transformed through distinct computational layers. The grayscale blocks initially represent the raw or initial embeddings of these words. These are fed into the 'Self-attention' layer, a critical component that allows the model to weigh the importance of other words in the sequence when processing each word. The yellow blocks represent the outputs after the self-attention mechanism, which are contextualized representations that have incorporated information from other words. Subsequently, these contextualized representations are passed through a 'Feedforward neural network' layer, which further processes them independently. The final blue blocks represent the output of the encoder block, which are refined, high-dimensional intermediate representations of the input words, ready for further processing in subsequent encoder blocks or the decoder.

**Key Insights:**
The main takeaway from this image is the sequential processing within a Transformer encoder block. It highlights that raw input words are first converted into numerical representations (embeddings). These embeddings then undergo two primary transformations: first, through a 'Self-attention' mechanism, which is vital for understanding contextual relationships between words in a sequence; and second, through a 'Feedforward neural network', which further refines these contextual representations. The image clearly demonstrates that each word's representation is progressively enriched as it passes through these layers, resulting in more sophisticated intermediate representations. The textual evidence 'I', 'love', 'llamas' shows the input, 'Self-attention' and 'Feedforward neural network' indicate the processing steps, and the progression from grayscale to yellow to blue blocks illustrates the transformation of representations.

**Document Context:**
This image directly supports the document's narrative regarding the Transformer architecture, specifically focusing on its encoder component. It visually explains the statement 'An encoder block revolves around self-attention to generate intermediate representations' by demonstrating the flow of information from raw word inputs through a self-attention mechanism and a feedforward neural network. This visual representation is crucial for understanding how the Transformer processes sequences and builds contextual understanding of words, making the abstract concept of an 'encoder block' tangible and illustrating the central role of self-attention.

**Summary:**
The image illustrates the internal structure and processing flow of a Transformer encoder block. It begins with three input tokens, 'I', 'love', and 'llamas', each represented by a rounded rectangular box containing the word. Each word token is then associated with a four-segment grayscale block, which visually represents its initial embedding or input vector. These three initial input vectors feed into the main 'Transformer encoder' block. 

Inside the 'Transformer encoder' block, the process is sequential through two main layers. First, the input vectors enter the 'Self-attention' layer, which is depicted as a rounded rectangular box. After processing through the 'Self-attention' layer, three new four-segment blocks, colored in shades of yellow, emerge, representing the intermediate contextualized representations for each word. These yellow blocks then serve as input to the second layer within the encoder, the 'Feedforward neural network', also depicted as a rounded rectangular box. Finally, after passing through the 'Feedforward neural network', the encoder outputs three four-segment blocks, colored in shades of blue, which represent the final, enriched intermediate representations for each of the original input words.](images/ed2a35607fe98e41bf1dcf244df130916a43f8eb6599a4c233954da6646302e5.jpg)
Figure 1-17. An encoder block revolves around self-attention to generate intermediate representations.

Compared to previous methods of attention, self-attention can attend to different positions within a single sequence, thereby more easily and accurately representing the input sequence as illustrated in Figure 1-18. Instead of processing one token at a time, it can be used to look at the entire sequence in one go.

![## Image Analysis: a92160e8cdedf7984266d79b61fe6a94808c0341ea0327d727d7cda1612664f5.jpg

**Conceptual Understanding:**
This image conceptually represents the self-attention mechanism as applied to a simple sentence: "I love llamas." Its main purpose is to visually demonstrate how each word in a sequence attends to all other words in that same sequence, assigning varying levels of importance or 'attention' to them.

The key idea being communicated is that when a neural network processes a word using self-attention, it doesn't just look at that word in isolation. Instead, it computes a score of relevance (attention) between the current word and every other word in the input sequence, allowing it to gather contextual information. The varying shades of blue illustrate these attention scores, from "High attention" (dark blue) to "Low attention" (light blue to white).

**Content Interpretation:**
The image illustrates the concept of self-attention, a mechanism used in neural networks, particularly in Transformer models. It visualizes how each word in an input sequence ("I love llamas") assigns attention scores to all other words (and itself) within the same sequence. The matrix effectively functions as an attention heatmap.

The processes shown are:
1.  **Self-referential Attention:** Each word attends strongly to itself, as indicated by the dark blue diagonal cells for "I"-to-"I", "love"-to-"love", and "llamas"-to-"llamas". This suggests that the model primarily focuses on the word itself when processing it.
2.  **Contextual Attention:** Words also attend to other words in the sequence to derive contextual meaning. For example, "I" pays some attention to "love", and "love" pays some attention to both "I" and "llamas". This demonstrates how the self-attention mechanism 'looks' at other parts of the sentence to understand the meaning of a given word.
3.  **Varying Attention Strengths:** The different shades of blue (light blue vs. white vs. dark blue) signify the varying degrees of importance or relevance that one word assigns to another. For instance, "I" paying light attention to "love" and no attention to "llamas" suggests a specific grammatical or semantic relationship being learned.

The significance of the information presented lies in visually demonstrating the core mechanism described in the "Attention Is All You Need" paper. It shows that words don't just process themselves in isolation but dynamically weigh the importance of all other words in the sequence to build a richer, context-aware representation.

**Key Insights:**
The main takeaways and insights from this image, supported by the extracted textual evidence, are:

1.  **Self-attention is a fully connected mechanism within a sequence:** Every word in the input sequence, "I love llamas", considers every other word (and itself) in that same sequence to derive its meaning. This is evident from the 3x3 matrix where each row (representing a word) has attention scores for all three columns (representing all words in the sequence).
2.  **Words attend strongly to themselves:** The darkest blue cells on the diagonal (e.g., "I" attending to "I", "love" attending to "love", "llamas" attending to "llamas") indicate "High attention". This suggests that a word's most important contextual information often comes from itself.
3.  **Attention is directional and weighted:** While a word attends to all others, the strength of this attention varies. For example, "I" pays "light attention" to "love" but "no attention" (white cell) to "llamas". This weighted attention allows the model to prioritize relevant words for contextual understanding. The legend explicitly defines "High attention" (dark blue) and "Low attention" (light blue/white).
4.  **Self-attention provides contextual awareness:** By distributing attention across the entire sequence, the mechanism allows each word's representation to be enriched with information from its surrounding context, both preceding and succeeding words. This capability to "look" both forward and back is a key feature of self-attention, as stated in the accompanying text.

**Document Context:**
This image is directly relevant to the "Attention Is All You Need" document section, serving as a foundational visual explanation for the self-attention mechanism. The text immediately following the image, "Figure 1-18. Self-attention attends to all parts of the input sequence so that it can “look” both forward and back in a single sequence," explicitly clarifies the image's purpose.

The image helps readers understand:
- **The core concept of self-attention:** How each element (word) in a sequence is made aware of all other elements in the same sequence.
- **Bidirectional context:** By showing attention across all pairs of words, it visually confirms that the mechanism can "look" both forward (e.g., "I" to "love", "love" to "llamas") and backward (e.g., "love" to "I", "llamas" to "love") within the sentence.
- **The varying importance of context:** The different shades of blue illustrate that not all parts of the sequence are equally important for understanding a given word, allowing the model to focus on the most relevant contextual cues.

It provides a concrete example ("I love llamas") that makes the abstract concept of attention scores and their application in understanding sentence structure more tangible for the reader, reinforcing the theoretical discussion in the document.

**Summary:**
The image displays a conceptual visualization of self-attention for the sentence "I love llamas". It consists of a 3x3 grid (matrix) where both the rows and columns are labeled with the words from the sentence: "I", "love", and "llamas". The cells within the grid are colored with varying shades of blue, indicating different levels of attention. A legend to the right of the matrix explains this color coding. The darkest blue represents "High attention", while the lightest blue (indicated by a dashed outline) represents "Low attention".

Specifically, the diagonal cells from top-left to bottom-right (where a word attends to itself) are colored in the darkest blue, indicating "High attention". These cells correspond to "I" attending to "I", "love" attending to "love", and "llamas" attending to "llamas".

Other cells show different levels of attention:
- For the row labeled "I": The cell intersecting with "I" (column) is dark blue (High attention). The cell intersecting with "love" (column) is light blue. The cell intersecting with "llamas" (column) is white (no attention).
- For the row labeled "love": The cell intersecting with "I" (column) is light blue. The cell intersecting with "love" (column) is dark blue (High attention). The cell intersecting with "llamas" (column) is light blue.
- For the row labeled "llamas": The cell intersecting with "I" (column) is white (no attention). The cell intersecting with "love" (column) is light blue. The cell intersecting with "llamas" (column) is dark blue (High attention).

The overall layout clearly shows how each word in the sequence "I love llamas" pays varying degrees of attention to every other word in the same sequence, including itself, with the attention levels visually distinguished by color intensity.](images/a92160e8cdedf7984266d79b61fe6a94808c0341ea0327d727d7cda1612664f5.jpg)
Figure 1-18. Self-attention attends to all parts of the input sequence so that it can “look” both forward and back in a single sequence.

Compared to the encoder, the decoder has an additional layer that pays attention to the output of the encoder (to find the relevant parts of the input). As demonstrated in Figure 1-19, this process is similar to the RNN attention decoder that we discussed previously.

![## Image Analysis: bbb6aebb2c924e4dea92005b1cb73414d3318ba121b0d6bb2e356611c977d84c.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and operational flow of a **Transformer decoder block**, a fundamental component in sequence-to-sequence models like the original Transformer model, particularly for tasks such as machine translation or text generation.

The main purpose of this diagram is to illustrate how the decoder processes information from two distinct sources – the output of an encoder (representing the input sequence) and previously generated words (representing the partial output sequence) – to predict the subsequent word in the output sequence. It highlights the role of its internal attention mechanisms: "Masked self-attention" for processing the target sequence and "Encoder attention" for interacting with the source sequence.

Key ideas being communicated include:
*   The modular structure of the Transformer decoder.
*   The dual input nature of the decoder (source context + partial target).
*   The sequential application of different attention mechanisms and a feedforward network.
*   The objective of generating one word at a time in an autoregressive fashion.

**Content Interpretation:**
The image clearly depicts the **internal mechanism of a single layer within a Transformer decoder**, showcasing the flow of data and the sequence of operations.

*   **Processes and Systems:** The primary system being shown is a **Transformer decoder**. The processes include:
    *   **Receiving Encoder Output:** The "Transformer encoder Output" labeled as "I", "am", "a", "student" (blue shaded rectangles) represents the contextual embeddings generated by the Transformer encoder from an input sequence. This provides the decoder with an understanding of the source text.
    *   **Receiving Previously Generated Words:** The "Previously generated words" labeled "I", "love" (grey shaded rectangles) represent the partial sequence that the decoder has already generated. This input is crucial for autoregressive generation, where each new word depends on the preceding words in the output sequence.
    *   **Masked Self-Attention:** This layer (first rectangular block inside the decoder) processes the "Previously generated words". The term "Masked" is significant because it prevents the decoder from "looking ahead" at future words in the target sequence during training, ensuring that the prediction for a given word only relies on past words. The yellow shaded rectangles represent the output of this attention mechanism, which is a refined contextual understanding of the partial target sequence.
    *   **Encoder Attention:** This layer (second rectangular block) is where the decoder "attends" to the source sequence (the "Transformer encoder Output"). It takes the output from the "Masked self-attention" layer and the encoder's output, allowing the decoder to selectively focus on relevant parts of the input sentence when generating the next word. The yellow shaded rectangles again show the output, which now incorporates context from both the partial target and the full source.
    *   **Feedforward Neural Network:** This layer (third rectangular block) further processes the output from the "Encoder attention" layer. It applies a position-wise fully connected feedforward network, independently to each position, enhancing the model's capacity to learn complex relationships.
    *   **Generating the Next Word:** The final output from the "Feedforward neural network" (red shaded rectangles) is then transformed into a probability distribution over the vocabulary, from which the "Next generated word" ("llamas" in this example) is selected.

*   **Significance of Information:**
    *   The distinct visual representation of blue (encoder output) and grey (previously generated words) shaded rectangles highlights the **two distinct types of input** that the decoder leverages.
    *   The sequence of "Masked self-attention", "Encoder attention", and "Feedforward neural network" demonstrates the **hierarchical and multi-faceted processing** within the decoder, where context is iteratively refined.
    *   The explicit labels like "Previously generated words" and "Next generated word" clearly indicate the **autoregressive nature of sequence generation** in this architecture.
    *   The use of words like "I am a student" and "I love" with "llamas" as the predicted word provides a **concrete example** of how the model might operate in a natural language processing context, e.g., generating "I love llamas" from an input like "I am a student".

All extracted text elements ("Transformer encoder Output", "I am a student", "Previously generated words", "I love", "Transformer decoder", "Masked self-attention", "Encoder attention", "Feedforward neural network", "Next generated word", "llamas") are critical in precisely defining the inputs, the processing steps, and the output of this neural network component, directly supporting the interpretation of it as a generative sequence model.

**Key Insights:**
The main takeaways and lessons from this image are:

*   **Autoregressive Generation:** The Transformer decoder generates text word by word, using "Previously generated words" as part of its input to predict the "Next generated word". This is evident from the explicit labels for these inputs and outputs.
*   **Dual Attention Mechanism:** The decoder employs two distinct attention mechanisms, "Masked self-attention" and "Encoder attention", which are crucial for its functionality.
    *   "Masked self-attention" focuses on the words already generated in the target sequence, ensuring that the prediction of a word only depends on preceding words in that sequence. This prevents "data leakage" from future words.
    *   "Encoder attention" allows the decoder to integrate information from the entire source sequence (provided by the "Transformer encoder Output") into its generation process, enabling it to align target words with relevant source words.
*   **Sequential Processing within Decoder:** The architecture demonstrates a clear sequential flow within the decoder block: "Masked self-attention" output feeds into "Encoder attention", which then feeds into a "Feedforward neural network" to produce the final representation for word prediction.
*   **Modular Architecture:** The image shows that the Transformer decoder is a modular block that combines different types of layers (attention and feedforward networks) to achieve its goal.

The specific text elements provide direct evidence for these insights:
*   "Previously generated words" and "Next generated word" directly illustrate the autoregressive nature.
*   "Masked self-attention" explicitly names the mechanism for handling target-side dependencies while masking future information.
*   "Encoder attention" explicitly names the mechanism for attending to the source sequence (from "Transformer encoder Output").
*   The ordered blocks of "Masked self-attention", "Encoder attention", and "Feedforward neural network" within the "Transformer decoder" visually and textually demonstrate the sequential processing and modularity.

**Document Context:**
This image, "Figure 1-19", is highly relevant to the "Attention Is All You Need" document (the foundational paper for Transformer models) and the surrounding text which explicitly states: "The decoder has an additional attention layer that attends to the output of the encoder." The diagram visually elaborates on this sentence, demonstrating precisely *how* the decoder incorporates the encoder's output through the "Encoder attention" layer. It explains a core component of the Transformer architecture, which revolutionized sequence processing by relying entirely on attention mechanisms rather than recurrent or convolutional networks.

**Summary:**
This diagram illustrates a single layer of a **Transformer decoder**, a critical component of the Transformer neural network architecture used in advanced natural language processing tasks like machine translation or text generation. Its primary function is to predict the "Next generated word" by considering both an input sequence (processed by an encoder) and the words it has already generated in the output sequence.

Let's break down the process:

1.  **Inputs to the Decoder:**
    *   **Transformer encoder Output:** On the left, we see the "Transformer encoder Output," which represents the contextualized understanding of an input sentence. In this example, the words "I am a student" are shown with associated blue shaded rectangles, symbolizing their numerical representations (embeddings). This is the information derived from the original sentence the model is processing.
    *   **Previously generated words:** At the top right, the decoder also takes as input "Previously generated words." In this illustration, "I" and "love" are shown with grey shaded rectangles, representing words that the decoder has already successfully generated in the output sequence. These words provide the current context for predicting the next word.

2.  **Processing within the Transformer Decoder (Pink Block):**
The main processing happens within the large pink block labeled "Transformer decoder," which contains three key sub-layers:

    *   **Masked self-attention:** The first layer is "Masked self-attention." This layer takes the "Previously generated words" as input. It allows the decoder to assess the relationships and dependencies *among the words it has already generated*. The "masked" part is crucial: it prevents the model from "cheating" by looking at future words in the output sequence during training. The output of this layer is represented by yellow shaded rectangles, which carry a rich contextual understanding of the partial output sequence.

    *   **Encoder attention:** The next layer is "Encoder attention." This is a crucial step where the decoder connects with the original input. It takes two inputs:
        1.  The output from the "Masked self-attention" layer (the contextual representation of previously generated words).
        2.  The "Transformer encoder Output" ("I am a student" embeddings).
        This layer allows the decoder to intelligently "attend" to, or focus on, specific parts of the *input sequence* (from the encoder) that are most relevant for predicting the *next word in the output sequence*. This mechanism is central to understanding how the input and output sequences are aligned. Again, yellow shaded rectangles represent the output, now enriched with information from both source and target contexts.

    *   **Feedforward neural network:** The final layer within the decoder block is a "Feedforward neural network." This network further processes the combined information from the attention layers, applying non-linear transformations to refine the representation.

3.  **Output Generation:**
    *   The output from the "Feedforward neural network" (represented by red shaded rectangles) is then used to determine the "Next generated word." In this specific example, the predicted word is "llamas."

In essence, the Transformer decoder acts as a sophisticated prediction engine. It continuously processes the words it has already produced, combined with the comprehensive understanding of the original input provided by the encoder, to intelligently guess and output the next most probable word in the sequence.](images/bbb6aebb2c924e4dea92005b1cb73414d3318ba121b0d6bb2e356611c977d84c.jpg)
Figure 1-19. The decoder has an additional attention layer that attends to the output of the encoder.

As shown in Figure 1-20, the self-attention layer in the decoder masks future posi‐ tions so it only attends to earlier positions to prevent leaking information when generating the output.

![## Image Analysis: b25f25af01ac6b1d57c685151b0632c129249f52727e65b3e78a2010aa56492a.jpg

**Conceptual Understanding:**
The image conceptually represents a 'masked self-attention mechanism' used in transformer models, specifically for sequence processing tasks like language modeling. Its main purpose is to visually explain how an attention mechanism can be constrained to ensure that when a model processes a particular token, it only has access to information from that token itself and all preceding tokens in the sequence, and not to any subsequent or future tokens. This prevents the model from 'cheating' by looking ahead at information that would not be available in a real-world predictive scenario.

**Content Interpretation:**
The image illustrates an attention matrix for a sequence of words, specifically demonstrating a masked self-attention mechanism. The processes being shown are: 1. **Token Representation:** Each word in the sentence ('Ik', 'houd', 'van', 'lama's') is represented as both a query and a key token in the matrix axes. 2. **Attention Scoring:** The shaded cells represent the computed attention scores between a query token (row label) and a key token (column label). 3. **Masking:** The white cells above the main diagonal signify that attention is 'masked' or prevented from occurring between a query token and future key tokens. For example, 'houd' (row 2) cannot attend to 'van' or 'lama's' (columns 3 and 4). This ensures that when processing a token, the model only uses information from that token itself and preceding tokens. The significance of the varying blue shades, supported by the 'High attention' and 'Low attention' labels, indicates the relative importance or focus a query token places on a key token. For instance, 'Ik' attends strongly to 'Ik' (dark blue). 'houd' attends strongly to 'houd' and moderately to 'Ik'. 'van' attends strongly to 'van' and moderately to 'houd' and lightly to 'Ik'. 'lama's' attends strongly to 'lama's' and moderately to 'van', 'houd', and lightly to 'Ik'. This suggests a decreasing attention to tokens further in the past.

**Key Insights:**
The main takeaways from this image are: 1. **Causal Masking:** The image vividly demonstrates causal masking in self-attention, where attention is restricted to only present and past tokens, never future ones. This is evidenced by the entirely white (masked) upper-right triangular section of the matrix. 2. **Token-to-Token Relationships:** Each cell's shading indicates the learned or computed 'attention' a word (row) gives to another word (column), revealing how contextual information is weighted. For example, 'lama's' gives attention to 'van', 'houd', and 'Ik', indicating its reliance on the preceding words for its context. 3. **Diagonal Dominance:** The darkest blue cells are predominantly along the main diagonal, showing that words often have high self-attention, meaning they attend strongly to themselves. This is seen in the 'Ik' to 'Ik', 'houd' to 'houd', 'van' to 'van', and 'lama's' to 'lama's' cells. 4. **Decreasing Past Attention:** There's a general trend of decreasing attention (lighter blue) as a word looks further back into the past, suggesting that more recent preceding words often hold more direct relevance, though this is not a strict rule and depends on the specific attention weights learned. For example, 'lama's' attends more to 'van' and 'houd' than to 'Ik'.

**Document Context:**
This image directly supports the document's broader narrative on attention mechanisms, particularly within the context of the 'Attention Is All You Need' paper. The text after the image, 'Figure 1-20. Only attend to previous tokens to prevent “looking into the future.”', explicitly states its purpose. The image visually explains the crucial concept of masking in self-attention, which is fundamental for tasks like language modeling where predicting the next word requires knowledge only of previous words, not future ones. By illustrating how a word like 'van' only attends to 'Ik', 'houd', and 'van' itself, and not to 'lama's', the image clearly demonstrates the 'prevent looking into the future' constraint. It provides a concrete, accessible example using a short Dutch sentence to demystify a core technical component of transformer models.

**Summary:**
The image displays an attention matrix for the Dutch phrase 'Ik houd van lama's' (I like llamas), visually representing how attention is distributed among tokens in a sequence, specifically demonstrating a masked self-attention mechanism where a token can only attend to itself and prior tokens. The matrix has words from the phrase 'Ik houd van lama's' labeling both its horizontal (top) and vertical (left) axes. Each cell in the matrix represents the attention score from a word on the vertical axis (query token) to a word on the horizontal axis (key token). A color scale to the right of the matrix indicates that darker blue shades correspond to 'High attention' and lighter shades, transitioning to white (represented by a dotted outline), correspond to 'Low attention'. The matrix shows a lower triangular pattern of shaded cells. For instance, the word 'Ik' (first word) attends only to 'Ik'. The word 'houd' attends to 'Ik' and 'houd'. The word 'van' attends to 'Ik', 'houd', and 'van'. The word 'lama's' attends to 'Ik', 'houd', 'van', and 'lama's'. All cells above the main diagonal are white, signifying zero or 'Low attention' (masked out), which means a word does not attend to subsequent words in the sequence. This visual effectively illustrates the concept of preventing 'looking into the future' in sequence processing.](images/b25f25af01ac6b1d57c685151b0632c129249f52727e65b3e78a2010aa56492a.jpg)
Figure 1-20. Only attend to previous tokens to prevent “looking into the future.”

Together, these building blocks create the Transformer architecture and are the foun‐ dation of many impactful models in Language AI, such as BERT and GPT-1, which we cover later in this chapter. Throughout this book, most models that we will use are Transformer-based models.

There is much more to the Transformer architecture than what we explored thus far. In Chapters 2 and 3, we will go through the many reasons why Transformer models work so well, including multi-head attention, positional embeddings, and layer normalization.

# Representation Models: Encoder-Only Models

The original Transformer model is an encoder-decoder architecture that serves trans‐ lation tasks well but cannot easily be used for other tasks, like text classification.

In 2018, a new architecture called Bidirectional Encoder Representations from Trans‐ formers (BERT) was introduced that could be leveraged for a wide variety of tasks and would serve as the foundation of Language AI for years to come.6 BERT is an encoder-only architecture that focuses on representing language, as illustrated in Figure 1-21. This means that it only uses the encoder and removes the decoder entirely.

![## Image Analysis: e48deaa01c1dcb4aa9131bcc30341cc657733765439db583485b5fce88f42a4e.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow of a BERT base model, a type of encoder-only Transformer. Its main purpose is to illustrate how an input sequence of words or tokens is transformed into contextualized numerical representations (embeddings) by passing through a stack of multiple Transformer encoder layers. The key idea being communicated is the layered, sequential processing of input tokens by a fixed number of identical encoder blocks to capture semantic and syntactic context, ultimately yielding rich, context-dependent word embeddings. Specifically, it highlights that a 'BERT base' model uses 12 such 'Transformer encoder' layers.

**Content Interpretation:**
The image depicts the architectural design of a BERT base model. It shows a sequential process where an input text sequence is transformed into contextualized word embeddings. The core components include an 'Input sequence' represented by individual tokens ('[CLS]', 'I', 'love', 'llamas'), a 'BERT base' model which is a stack of 12 'Transformer encoder' layers, and the final 'Contextualized word embeddings' as output. The significance lies in demonstrating how the multi-layer Transformer encoder architecture processes each input token through successive layers to generate rich, context-aware vector representations.

**Key Insights:**
The main takeaways from this image are: 1. BERT base models are encoder-only architectures, as indicated by the 'BERT base' label enclosing only 'Transformer encoder' blocks. 2. They process an 'Input sequence' token by token, including special tokens like '[CLS]'. 3. The core of BERT's processing involves multiple stacked 'Transformer encoder' layers, specifically 12 in the case of the BERT base model, as explicitly shown by the numerical labels '1', '2', and '12'. 4. The output of this multi-layer encoding process is 'Contextualized word embeddings', meaning the final representations of words are informed by their surrounding context in the input sequence. All extracted text elements, from 'Input sequence' to 'Contextualized word embeddings' and the specific enumeration of 'Transformer encoder' layers, provide direct evidence for these insights.

**Document Context:**
This image directly supports the document's section on 'Representation Models: Encoder-Only Models' by providing a visual explanation of the architecture of BERT, a prominent example of such a model. It illustrates how BERT takes an 'Input sequence' and processes it through its 'BERT base' component, composed of multiple 'Transformer encoder' layers, to produce 'Contextualized word embeddings'. The text after the image, 'Figure 1-21. The architecture of a BERT base model with 12 encoders', further confirms its specific role in detailing BERT's structure, particularly its 12 encoder layers, which is crucial for understanding its functionality as an encoder-only model for natural language understanding.

**Summary:**
The image illustrates the architecture of a BERT base model, which is an encoder-only model designed for natural language processing tasks. It begins with an 'Input sequence' which is tokenized into individual units, represented here by '[CLS]', 'I', 'love', and 'llamas'. Each of these tokens is then processed through the 'BERT base' component. The BERT base model consists of a stack of 'Transformer encoder' layers. The diagram explicitly shows the first 'Transformer encoder' labeled '1', followed by a second 'Transformer encoder' labeled '2', an ellipsis indicating many more layers, and finally the twelfth 'Transformer encoder' labeled '12'. This signifies that the BERT base model has 12 such encoder layers. Each token from the input sequence passes through all these stacked Transformer encoders. After processing through all 12 layers, the model outputs 'Contextualized word embeddings' for each of the input tokens. These embeddings are represented as blue segmented blocks at the bottom, corresponding to the initial input tokens, but now enriched with contextual information from the entire sequence.](images/e48deaa01c1dcb4aa9131bcc30341cc657733765439db583485b5fce88f42a4e.jpg)
Figure 1-21. The architecture of a BERT base model with 12 encoders.

These encoder blocks are the same as we saw before: self-attention followed by feedforward neural networks. The input contains an additional token, the [CLS] or classification token, which is used as the representation for the entire input. Often, we use this [CLS] token as the input embedding for fine-tuning the model on specific tasks, like classification.

Training these encoder stacks can be a difficult task that BERT approaches by adopt‐ ing a technique called masked language modeling (see Chapters 2 and 11). As shown in Figure 1-22, this method masks a part of the input for the model to predict. This prediction task is difficult but allows BERT to create more accurate (intermediate) representations of the input.

![## Image Analysis: bb69d94d14e53592bc227a447097ef71c4253d733731b259debe9f3fb56f538a.jpg

**Conceptual Understanding:**
This image conceptually illustrates the Masked Language Modeling (MLM) task, which is a key pre-training objective for the BERT (Bidirectional Encoder Representations from Transformers) model. The main purpose of the image is to visually explain how a BERT model is trained to learn contextual representations of words by predicting randomly masked words within a sequence. It conveys the idea of an encoder-only model taking a sequence with masked tokens, processing it through a Transformer architecture, and then outputting a prediction for the missing words.

**Content Interpretation:**
The image depicts the pre-training process of a BERT (Bidirectional Encoder Representations from Transformers) model using the Masked Language Modeling (MLM) objective. It illustrates how the model learns to predict masked tokens based on their context. The process involves: 1. Input preparation, where words are 'Randomly mask words' in a sequence. For example, a sequence starts with a special '[CLS]' token, followed by the word 'I', a masked token '[MASK]', and the word 'llamas'. Each token is converted into an embedding (represented by stacked horizontal rectangles). 2. These embeddings are then fed into the 'BERT_base' model, specifically a 'Transformer encoder' (represented as a stack of light-blue rectangles, indicating multiple layers). The 'Transformer encoder' processes these input embeddings. 3. Finally, the model's output for the masked position is used to 'Predict the masked words'. In this example, the model predicts 'am' for the '[MASK]' token. The 'BERT_base' label highlights the specific architecture being used.

**Key Insights:**
The main takeaway from this image is the fundamental mechanism of Masked Language Modeling (MLM) as a pre-training objective for BERT. The image teaches that BERT learns language context by attempting to 'Predict the masked words' after 'Randomly mask words' in its input. Key insights include: 1. The use of special tokens like '[CLS]' at the beginning of the sequence. 2. The explicit '[MASK]' token replacing a word that BERT needs to predict. 3. The 'BERT_base' architecture, composed of a 'Transformer encoder', is the core component for processing these sequences bidirectionally. 4. The output is a prediction of the original masked word (e.g., 'am' for '[MASK]'). This illustrates BERT's ability to understand context from both left and right directions, crucial for its effectiveness in various NLP tasks.

**Document Context:**
This image is highly relevant to the document's section on 'Representation Models: Encoder-Only Models', specifically focusing on how BERT models are trained. The text after the image, 'Figure 1-22. Train a BERT model by using masked language modeling.', directly confirms its purpose. It visually explains the core pre-training task for BERT, which is essential for understanding how these encoder-only models learn language representations, thereby enhancing the reader's comprehension of the technical details discussed in the document.

**Summary:**
This diagram illustrates the process of training a BERT model using masked language modeling. It begins with a sentence where some words are randomly masked. These masked words, along with the unmasked words and a special classification token, are then fed into the BERT_base model, which primarily consists of multiple layers of a Transformer encoder. The encoder processes these inputs to learn contextual representations. Finally, the model outputs a prediction for each masked word based on its learned context. In the specific example shown, the input tokens are '[CLS]', 'I', '[MASK]', and 'llamas'. These tokens, represented as embeddings, enter the 'Transformer encoder' within the 'BERT_base' architecture. The 'Transformer encoder' then processes these inputs. For the position corresponding to the '[MASK]' token, the model predicts the word 'am'. This entire process is summarized by the labels 'Randomly mask words' at the input stage and 'Predict the masked words' at the output stage, clearly demonstrating how BERT learns to fill in missing information in a sequence.](images/bb69d94d14e53592bc227a447097ef71c4253d733731b259debe9f3fb56f538a.jpg)
Figure 1-22. Train a BERT model by using masked language modeling.

This architecture and training procedure makes BERT and related architectures incredible at representing contextual language. BERT-like models are commonly used for transfer learning, which involves first pretraining it for language modeling and then fine-tuning it for a specific task. For instance, by training BERT on the entirety of Wikipedia, it learns to understand the semantic and contextual nature of text. Then, as shown in Figure 1-23, we can use that pretrained model to fine-tune it for a specific task, like text classification.

![## Image Analysis: 7818123a929ecf880c56c204b09b1f1ef31ad4fdacab211e9e7e6c0e61399e08.jpg

**Conceptual Understanding:**
This image conceptually represents the lifecycle and application methodology of the BERT (Bidirectional Encoder Representations from Transformers) model in natural language processing. The main purpose conveyed is to illustrate how a powerful language model like BERT is first trained on a massive generic text corpus to learn fundamental language patterns and then adapted or specialized for a wide array of specific, real-world NLP tasks. It encapsulates the 'pre-train then fine-tune' transfer learning paradigm.

**Content Interpretation:**
The image illustrates the standard two-stage training methodology for the BERT model, consisting of pre-training and fine-tuning. The first stage, 'Pretrain on large dataset,' demonstrates that the BERT model undergoes an initial, extensive training phase. This pre-training leverages a vast dataset, specifically identified as 'Wikipedia,' and has the core 'Objective: masked language modeling.' Masked language modeling is a self-supervised task where the model learns to predict masked tokens in a sequence, thereby developing a rich understanding of language context and grammar. The second stage, 'Fine-tune for downstream task,' shows that the pre-trained BERT model is then adapted for specific, application-oriented tasks. This 'BERT' model, having already learned general language representations, can be efficiently specialized for various 'downstream tasks' such as 'Classification' (e.g., sentiment analysis), 'Named entity recognition' (identifying proper nouns like people, organizations, locations), and 'Paraphrase identification' (determining if two sentences have the same meaning). Each of these tasks represents a different application of the pre-trained model's acquired knowledge, requiring a smaller, task-specific dataset for further training. The flow demonstrates a powerful transfer learning approach.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Two-Stage Learning Paradigm:** The image clearly demonstrates the widely adopted two-stage learning paradigm for large language models like BERT: pre-training followed by fine-tuning. This highlights the concept of transfer learning in NLP.
2.  **General Pre-training for Language Understanding:** The pre-training phase, explicitly labeled '1 Pretrain on large dataset' with 'Wikipedia' as a data source and 'Objective: masked language modeling,' shows that BERT acquires a general understanding of language structure and semantics before being applied to specific problems. Masked language modeling is a key self-supervised task for this generalized learning.
3.  **Versatility of Fine-tuning:** The '2 Fine-tune for downstream task' phase illustrates BERT's adaptability. A single pre-trained BERT model can be fine-tuned for a multitude of specific NLP tasks, including 'Classification,' 'Named entity recognition,' and 'Paraphrase identification,' underscoring its utility as a foundational model for various applications.
4.  **Efficiency of Transfer Learning:** The implied knowledge is that fine-tuning requires significantly less data and computational resources compared to training a model from scratch for each specific task, thanks to the robust representations learned during pre-training. The progression from a general model to specialized applications is the core insight.

**Document Context:**
This image directly supports the document's section on 'Representation Models: Encoder-Only Models' by visually explaining the operational lifecycle of BERT, a prominent encoder-only model. As stated in the text after the image, 'After pretraining BERT on masked language model, we fine-tune it for specific tasks,' the diagram serves as a clear visual aid for understanding this exact methodology. It delineates how a foundational language model like BERT is first given a broad understanding of language through self-supervised pre-training and then specialized for various practical natural language processing applications, which is central to the concept of pre-trained representation models.

**Summary:**
This image illustrates the two-phase process for utilizing the BERT (Bidirectional Encoder Representations from Transformers) model: pretraining and fine-tuning. The process begins with '1 Pretrain on large dataset,' where the BERT model is trained with the 'Objective: masked language modeling' using data from 'Wikipedia.' After this extensive pretraining, indicated by an arrow, the model transitions to the second phase: '2 Fine-tune for downstream task.' In this phase, the pre-trained BERT model is adapted for various specific natural language processing applications. From the 'BERT' model in this fine-tuning stage, three distinct downstream tasks are shown as outputs: 'Classification,' 'Named entity recognition,' and 'Paraphrase identification.' The diagram clearly separates these two major stages with a vertical dotted line, showcasing a common transfer learning paradigm in natural language processing.](images/7818123a929ecf880c56c204b09b1f1ef31ad4fdacab211e9e7e6c0e61399e08.jpg)
Figure 1-23. After pretraining BERT on masked language model, we fine-tune it for specific tasks.

A huge benefit of pretrained models is that most of the training is already done for us. Fine-tuning on specific tasks is generally less compute-intensive and requires less data. Moreover, BERT-like models generate embeddings at almost every step in their architecture. This also makes BERT models feature extraction machines without the need to fine-tune them on a specific task.

Encoder-only models, like BERT, will be used in many parts of the book. For years, they have been and are still used for common tasks, including classification tasks (see Chapter 4), clustering tasks (see Chapter 5), and semantic search (see Chapter 8).

Throughout the book, we will refer to encoder-only models as representation models to differentiate them from decoder-only, which we refer to as generative models. Note that the main distinction does not lie between the underlying architecture and the way these models work. Representation models mainly focus on representing language, for instance, by creating embeddings, and typically do not generate text. In contrast, generative models focus primarily on generating text and typically are not trained to generate embeddings.

The distinction between representation and generative models and components will also be shown in most images. Representation models are teal with a small vector icon (to indicate its focus on vectors and embeddings) whilst generative models are pink with a small chat icon (to indicate its generative capabilities).

# Generative Models: Decoder-Only Models

Similar to the encoder-only architecture of BERT, a decoder-only architecture was proposed in 2018 to target generative tasks.7 This architecture was called a Generative Pre-trained Transformer (GPT) for its generative capabilities (it’s now known as GPT-1 to distinguish it from later versions). As shown in Figure 1-24, it stacks decoder blocks similar to the encoder-stacked architecture of BERT.

GPT-1 was trained on a corpus of 7,000 books and Common Crawl, a large dataset of web pages. The resulting model consisted of 117 million parameters. Each parameter is a numerical value that represents the model’s understanding of language.

If everything remains the same, we expect more parameters to greatly influence the capabilities and performance of language models. Keeping this in mind, we saw larger and larger models being released at a steady pace. As illustrated in Figure 1-25, GPT-2 had 1.5 billion parameters8 and GPT-3 used 175 billion parameters9 quickly followed.

![## Image Analysis: b5c32087cb63d999f866a528b2bcea837a1ba5f90428119dc1fa3f4d2015e7ac.jpg

**Conceptual Understanding:**
The image conceptually represents the architectural design of a GPT-1 language model. Its main purpose is to illustrate how a sequence of input words is processed through a series of 'Transformer decoder' blocks to generate the next word in the sequence. It communicates the key idea that GPT-1 is a 'decoder-only' model, meaning it focuses solely on generating output based on previous context, and highlights the fundamental components within each decoder block: 'Masked self-attention' and a 'Feedforward neural network'.

**Content Interpretation:**
The image details the internal workings and components of a GPT-1 model, specifically highlighting its decoder-only architecture for text generation. It shows the sequential processing of an input word sequence through multiple stacked transformer decoder layers. Each decoder layer performs masked self-attention and then passes the output through a feedforward neural network. The significance of this design is that it allows the model to predict the next word in a sequence based on all preceding words, a fundamental capability for generative language tasks. The numerical labels (1, 2, ..., 12) indicate the depth of the network, suggesting a substantial number of processing layers.

**Key Insights:**
The main takeaways from this image are: 1. GPT-1 is built upon a stack of multiple 'Transformer decoder' layers, specifically 12 layers in this depiction. 2. Each 'Transformer decoder' layer primarily consists of a 'Masked self-attention' mechanism followed by a 'Feedforward neural network'. 3. The 'Masked self-attention' component is crucial for allowing the model to attend to preceding words in the input sequence while predicting the next word, without 'seeing' future words. 4. The model's function is to take an 'Input sequence' (e.g., 'I love') and predict the 'Next generated word' (e.g., 'llamas'), demonstrating its capability in text generation.

**Document Context:**
This image directly supports the document's section on 'Generative Models: Decoder-Only Models' by providing a visual representation of a GPT-1, a seminal example of such a model. It clarifies the statement 'It uses a decoder-only architecture and removes the encoder-attention block' by explicitly showing only 'Transformer decoder' blocks and their internal components, without any encoder structure. The diagram is crucial for understanding the foundational architecture of early generative pre-trained transformers.

**Summary:**
The image illustrates the architecture of a GPT-1 model, a type of generative model that employs a decoder-only architecture. It begins by taking an 'Input sequence' of words, such as 'I' and 'love'. These input tokens are then processed sequentially through a stack of 'Transformer decoder' layers. The diagram shows the first 'Transformer decoder' labeled '1', followed by a second 'Transformer decoder' labeled '2', and then an ellipsis indicating multiple intermediate layers. The final 'Transformer decoder' is labeled '12'. Within this final decoder block, there are two main sub-components: a 'Masked self-attention' layer, followed by a 'Feedforward neural network'. After processing through all the decoder layers, the model outputs the 'Next generated word', which is exemplified as 'llamas'. The overall structure is enclosed within a dotted line boundary labeled 'GPT-1', signifying that this entire system constitutes the GPT-1 model. The process demonstrates how the model takes an input sequence and predicts the subsequent word in a generative task.](images/b5c32087cb63d999f866a528b2bcea837a1ba5f90428119dc1fa3f4d2015e7ac.jpg)
Figure 1-24. The architecture of a GPT-1. It uses a decoder-only architecture and removes the encoder-attention block.

![## Image Analysis: 086aca0ca5c28d176eb1674b09a518ba7d6212ad3a4ab9d4b5f1f5fee130a0f1.jpg

**Conceptual Understanding:**
This image conceptually represents the evolution and scaling of Generative Pre-trained Transformer (GPT) models. Its main purpose is to visually demonstrate the dramatic increase in the number of parameters from GPT-1 to GPT-3, thereby illustrating the rapid growth in model complexity and computational scale of these prominent decoder-only generative models. The key idea communicated is the exponential scaling of AI models, particularly in the realm of natural language processing.

**Content Interpretation:**
The image illustrates the exponential growth in the number of parameters across successive versions of Generative Pre-trained Transformer (GPT) models. It visually represents how each new iteration, from GPT-1 to GPT-3, has dramatically increased in complexity and scale, as measured by the number of trainable parameters. The increasing size of the circles, coupled with the corresponding parameter counts, directly demonstrates this trend, highlighting a rapid expansion in the computational capacity and potential capabilities of these decoder-only generative models.

**Key Insights:**
The main takeaway from this image is the staggering and rapid increase in the size of GPT models across generations. GPT-1 started with 117 Million parameters, GPT-2 scaled up significantly to 1.5 Billion, and GPT-3 represents an enormous leap to 175 Billion parameters. This highlights a clear trend in the development of generative models, where increasing the number of parameters is a primary strategy for enhancing model performance and capabilities. The visual representation emphasizes that the growth is not linear but exponential, indicating a deliberate and aggressive scaling approach in the pursuit of more powerful AI.

**Document Context:**
This image directly supports the document's section on "Generative Models: Decoder-Only Models" by providing a visual and quantitative representation of the growth in GPT models. The accompanying text, "Figure 1-25. GPT models quickly grew in size with each iteration," is perfectly illustrated by this diagram. It demonstrates the trend of increasing model complexity in the field of large language models, particularly focusing on the rapid scaling of parameter counts as a key characteristic of these generative AI advancements.

**Summary:**
The image is a horizontal bar chart that illustrates the dramatic increase in the number of parameters across three generations of GPT models: GPT-1, GPT-2, and GPT-3. The horizontal axis, indicated by an arrow pointing to the right, is labeled "Number of parameters", signifying a progression from fewer to more parameters. Each GPT model is represented by a vertical line extending upwards from the horizontal axis to a labeled circle, with the size of the circle visually correlating to the number of parameters. GPT-1 is shown with 117 Million parameters, represented by a small circle. Following this, GPT-2 is positioned further along the axis with 1.5 Billion parameters, depicted by a larger circle than GPT-1. Finally, GPT-3 is located much further to the right on the axis, boasting a massive 175 Billion parameters, represented by a significantly larger circle that dominates the visual space, clearly indicating the substantial scale-up in model size.](images/086aca0ca5c28d176eb1674b09a518ba7d6212ad3a4ab9d4b5f1f5fee130a0f1.jpg)
Figure 1-25. GPT models quickly grew in size with each iteration.

These generative decoder-only models, especially the “larger” models, are commonly referred to as large language models (LLMs). As we will discuss later in this chapter, the term LLM is not only reserved for generative models (decoder-only) but also representation models (encoder-only).

Generative LLMs, as sequence-to-sequence machines, take in some text and attempt to autocomplete it. Although a handy feature, their true power shone from being trained as a chatbot. Instead of completing a text, what if they could be trained to answer questions? By fine-tuning these models, we can create instruct or chat models that can follow directions.

As illustrated in Figure 1-26, the resulting model could take in a user query (prompt) and output a response that would most likely follow that prompt. As such, you will often hear that generative models are completion models.

![## Image Analysis: 7753b7595e3b41cf99e08d95960b3cd7816d01cb299455201d4eebcad5bc3330.jpg

**Conceptual Understanding:**
The image conceptually represents the basic inference process of a Generative Large Language Model (LLM). Its main purpose is to visually illustrate how an LLM receives a text prompt, processes it with the task of 'complete the input', and subsequently generates a relevant and extended text output. It conveys the idea that generative models act upon an input to produce a coherent continuation or response.

**Content Interpretation:**
This image illustrates the core operational mechanism of a Generative Large Language Model (LLM). It demonstrates how such a model takes a natural language 'User query (prompt)' as input and then, based on its designated 'Task: complete the input', processes this prompt to generate a coherent and relevant 'Output (completion)'. The example specifically shows the LLM generating descriptive information about 'llamas' in response to a direct question.

**Key Insights:**
The main takeaway from this image is that Generative LLMs operate by taking an initial user 'prompt' and then generating a 'completion' based on that input. The specific textual evidence, 'User query (prompt)' feeding into 'Generative LLM' with the 'Task: complete the input', and resulting in a descriptive 'Output (completion)' about llamas, clearly demonstrates this input-output mechanism. It highlights the LLM's capability to expand on an initial query with relevant, generated content.

**Document Context:**
This image directly supports the document's section on 'Generative Models: Decoder-Only Models' by visually explaining how generative LLMs function. It aligns perfectly with the subsequent text, 'Generative LLMs take in some input and try to complete it,' by providing a concrete example of this input-processing-output cycle. The diagram differentiates the basic 'autocomplete' function from answering a question, even though the LLM's explicit task is to 'complete the input,' it generates an informative response akin to answering a question about llamas, which ties into the document's explanation that 'with instruct models, this is more than just autocomplete and attempts to answer the question.'

**Summary:**
The image illustrates the fundamental process of how a Generative Large Language Model (LLM) responds to a user query. It begins with a 'User query (prompt)' which is the input provided by the user, specifically: 'Tell me something about llamas'. This input is then processed by a 'Generative LLM', whose explicit 'Task:' is to 'complete the input'. Following this processing, the LLM produces an 'Output (completion)' which is a generated response providing information about llamas. The output text begins with: 'Llamas are domesticated South American camelids, widely used as pack animals by Andean cultures since pre-Hispanic times. With their fluffy coat, long neck, and distinctive facial features...' The process flow is linear: user query feeds into the Generative LLM, which then produces the output.](images/7753b7595e3b41cf99e08d95960b3cd7816d01cb299455201d4eebcad5bc3330.jpg)
Figure 1-26. Generative LLMs take in some input and try to complete it. With instruct models, this is more than just autocomplete and attempts to answer the question.

A vital part of these completion models is something called the context length or context window. The context length represents the maximum number of tokens the model can process, as shown in Figure 1-27. A large context window allows entire documents to be passed to the LLM. Note that due to the autoregressive nature of these models, the current context length will increase as new tokens are generated.

![## Image Analysis: 53df94a10a0bf9bd286d8143c49b811e2fda72449627c0e8b0bea89f8639dfa7.jpg

**Conceptual Understanding:**
The image conceptually represents the token-by-token text generation process of a Generative Large Language Model (LLM), emphasizing the role and management of context length. Its main purpose is to illustrate how an LLM iteratively produces output by feeding previously generated tokens back into its input, thereby extending its 'current context' up to a 'maximum context length'. The key idea communicated is the iterative, autoregressive nature of LLM generation, where the model's understanding of the ongoing conversation or text is dynamically updated with each new token it creates, while operating under a predefined context window limit.

**Content Interpretation:**
The image depicts the operational mechanism of a Generative Large Language Model (LLM) in producing text. It illustrates an iterative generation process where the LLM's input consists of both an initial prompt and any tokens it has previously generated. This combined input forms the 'current context' for the model. The diagram highlights the concept of 'tokenization', where input is broken down into discrete units, and demonstrates how a new token is generated one at a time. A critical aspect shown is the 'Maximum context length' of the LLM, indicating a finite capacity for the amount of text it can consider at any given moment. Furthermore, the image clarifies that each new token generated by the LLM is subsequently incorporated back into the input, effectively extending the 'current context length' for the generation of the next token. The significance lies in showing how LLMs build responses sequentially, relying on an ever-growing, yet bounded, historical context.

**Key Insights:**
The image conveys several key takeaways regarding Generative Large Language Models: 1. LLMs operate by generating text iteratively, producing 'one token at a time'. 2. The 'Input' to an LLM for any given generation step comprises both the 'prompt' and all 'previously generated tokens', which collectively form the 'current context'. 3. LLMs possess a 'Maximum context length' (e.g., 512 tokens), defining the upper limit of information they can consider at once. 4. Each newly generated token is subsequently integrated back into the input sequence through an 'Update input (increases current context length)' mechanism, allowing the model to build upon its own output to generate further text. 5. This iterative process, coupled with the context window, is fundamental to how LLMs maintain conversational flow and generate coherent sequences.

**Document Context:**
This image directly supports the document's section on 'Generative Models: Decoder-Only Models' by providing a visual explanation of how these models function, particularly regarding their input, output, and the crucial concept of 'context length'. The text after the image, 'Figure 1-27. The context length is the maximum context an LLM can handle.', directly references and is explained by the visual flow shown. The diagram elucidates the iterative nature of text generation in decoder-only LLMs, where previous outputs become part of the subsequent input context. This is fundamental to understanding the operational limitations and capabilities of such models, specifically how they maintain coherence and continuity in generated text while being constrained by a maximum context window.

**Summary:**
The image illustrates the process of a Generative Large Language Model (LLM) creating output text, specifically focusing on how context length is managed during token generation. The process begins with an 'Input' which consists of 'both prompt and generated tokens'. An example prompt 'Tell me something about llamas.' is shown, broken down into six individual tokens: 'Tell' (1), 'me' (2), 'something' (3), 'about' (4), 'llamas' (5), and '.' (6). Alongside the prompt, 'Previously generated tokens' are included as part of the input, such as 'Llamas' (7) and 'are' (8). Together, these form the 'Current context length = 8' tokens. This combined input is fed into the 'Generative LLM', which is specified to have a 'Maximum context length: 512'. The LLM then produces 'Output (generate one token at a time)', yielding a single new token, in this example, 'domesticated' (9). This newly generated token is then fed back into the system via an 'Update input (increases current context length)' mechanism, becoming part of the 'previously generated tokens' for the next iteration, thereby extending the context for subsequent token generation. The diagram clearly shows the iterative, token-by-token generation process and the dynamic increase of the context, while also highlighting the fixed maximum context length of the LLM.](images/53df94a10a0bf9bd286d8143c49b811e2fda72449627c0e8b0bea89f8639dfa7.jpg)
Figure 1-27. The context length is the maximum context an LLM can handle.

# The Year of Generative AI

LLMs had a tremendous impact on the field and led some to call 2023 The Year of Generative AI with the release, adoption, and media coverage of ChatGPT (GPT-3.5). When we refer to ChatGPT, we are actually talking about the product and not the underlying model. When it was first released, it was powered by the GPT-3.5 LLM and has since then grown to include several more performant variants, such as GPT-4.10

GPT-3.5 was not the only model that made its impact in the Year of Generative AI. As illustrated in Figure 1-28, both open source and proprietary LLMs have made their way to the people at an incredible pace. These open source base models are often referred to as foundation models and can be fine-tuned for specific tasks, like following instructions.

# Proprietary models

![## Image Analysis: 89c4348b4591c275044513787ec9a7906c24a259f6e712fe8aa6d6881e8f5c2b.jpg

**Conceptual Understanding:**
This image conceptually represents a **timeline illustrating the rapid evolution and expansion of generative AI models during the years 2023 and early 2024**. It visually charts the introduction of prominent large language models (LLMs) and other generative AI models, making a clear distinction between those developed and maintained by private entities ("Proprietary models") and those that are open-source or publicly available ("Open models").

The main purpose of the image is to **provide a comprehensive overview of the generative AI landscape's development trajectory**, highlighting the key players and their contributions within this period. It aims to demonstrate the accelerating pace of innovation, the increasing number of models becoming available, and the diversity in their scale (indicated by parameter counts for open models).

Key ideas being communicated include:
*   The **fast-paced development** in generative AI.
*   The **duality of the ecosystem**, with significant contributions from both proprietary and open-source communities.
*   The **scale and complexity** of open models, often indicated by their parameter counts, which range from small (e.g., 2.7B for Phi-2) to very large (e.g., 70B for Llama 2, or 65B for Llama).
*   The **continual emergence of new models and updates**, making the field highly dynamic.

**Content Interpretation:**
The image primarily shows the release timeline of generative AI models, specifically large language models.

**Processes/Concepts Shown:**
*   **Chronological Development:** The horizontal line with dots and year markers ("2023", "2024") explicitly represents the progression of time and the sequential release of models.
*   **Categorization of Models:** The blue upper section labeled "Proprietary models" and the beige lower section labeled "Open models" clearly illustrate the two main approaches to AI model development and distribution. This highlights a fundamental distinction in the AI ecosystem.
*   **Model Scale (for Open Models):** The parameter counts associated with the open models (e.g., "7B/13B/33B/65B" for Llama, "7B/30B" for MPT, "7B/13B/70B" for Llama 2, "8 × 7B" for Mixtral) indicate the varying computational complexity and capacity of these models. This signifies that the open-source community is developing a range of models, from relatively smaller and more accessible ones to very large and capable ones.

**Significance of Information Presented:**
*   **Explosive Growth:** The sheer number of models listed within a relatively short timeframe (2023-2024) signifies an explosive growth and intense innovation period in generative AI, supporting the "Year of Generative AI" context.
*   **Competitive Landscape:** The presence of many proprietary models (ChatGPT, GPT-4, BARD, PaLM 2, Claude 2, Grok, Gemini) from different major tech companies indicates a highly competitive landscape.
*   **Vibrant Open-Source Community:** The numerous open models (Llama, MPT, Falcon, Llama 2, Qwen, Mistral, Yi, Mixtral, Phi-2, DeciLM, Command R) with diverse parameter sizes demonstrate a very active and significant open-source community contributing to the field, offering alternatives to proprietary solutions.
*   **Trend Towards Larger Models (and also specialized ones):** While some open models are smaller (e.g., Phi-2 at 2.7B, Mistral at 7B), others are quite large (Llama 2 70B, Llama 65B, Command R 35B, Yi 34B), showing a trend towards increasing model complexity and capabilities in the open-source domain. The Mixtral "8 × 7B" also points to more complex architectural approaches like Mixtures of Experts.

**Supporting Textual Evidence from Section 1 (Transcription Summary):**
*   **Timeline Markers:** "2023" and "2024" explicitly anchor events in time.
*   **Model Sequence:** The left-to-right arrangement of models (e.g., ChatGPT -> GPT-4; Llama -> MPT) illustrates chronological development.
*   **Categorization Labels:** "Proprietary models" and "Open models" directly evidence the two categories.
*   **Specific Model Names:** "ChatGPT", "GPT-4", "Gemini", "Llama", "Mixtral", etc., identify individual entities.
*   **Parameter Counts:** "7B/13B/33B/65B" for Llama, "70B" for Llama 2, "8 × 7B" for Mixtral, "2.7B" for Phi-2, etc., provide data on model scale and variety.

**Key Insights:**
**Main Takeaways/Lessons:**

1.  **Generative AI experienced a concentrated period of intense innovation and release activity between 2023 and early 2024.** The sheer number of models appearing on the timeline, particularly from late 2023 into 2024, underscores the "Year of Generative AI" concept mentioned in the document context.
    *   *Evidence:* The presence of over 18 distinct models (7 proprietary, 11 open) packed into a timeline spanning roughly 18-24 months, with explicit "2023" and "2024" markers.
2.  **The generative AI landscape is fundamentally bifurcated into proprietary and open-source ecosystems, both actively developing advanced models.** Both categories show significant contributions from various developers.
    *   *Evidence:* The clear visual and textual distinction with "Proprietary models" in the blue upper section and "Open models" in the beige lower section, each containing numerous distinct model names.
3.  **The open-source community is rapidly catching up and offering a wide range of model sizes, providing strong alternatives to proprietary solutions.** The variety in parameter counts for open models indicates flexibility and specialized uses.
    *   *Evidence:* The presence of 11 open models compared to 7 proprietary ones, with specific parameter counts ranging from "2.7B" (Phi-2) to "70B" (Llama 2), and even complex architectures like "8 × 7B" (Mixtral). This shows a robust and diverse open-source contribution.
4.  **Major technology companies are actively investing in and releasing their own proprietary foundational models.**
    *   *Evidence:* The presence of well-known models like "ChatGPT", "GPT-4" (OpenAI), "BARD", "PaLM 2", "Gemini" (Google), "Claude 2" (Anthropic), and "Grok" (xAI) as proprietary offerings.
5.  **Model development is not slowing down, with multiple new releases continuing into 2024.**
    *   *Evidence:* The "2024" marker and models like Gemini, Mixtral, Phi-2, DeciLM, and Command R appearing around or after it, demonstrating ongoing activity.

**Conclusions/Insights:**

*   The generative AI field is dynamic, with continuous advancement driven by both corporate and open-source efforts.
*   The availability of a diverse set of open models empowers broader research, development, and application in the AI community, fostering innovation beyond a few proprietary leaders.
*   Understanding the distinction between proprietary and open models is crucial for navigating the generative AI ecosystem, as they come with different access, licensing, and usage implications.
*   The scale of models, as indicated by parameter counts, is a key differentiator, and both small, efficient models and very large, general-purpose models are being developed in the open-source space.

**Document Context:**
This image, titled "Figure 1-28. A comprehensive view into the Year of Generative AI," directly supports the document's narrative by providing a visual timeline of key generative AI model releases. Given that it's placed under a section titled "Proprietary models" in the document, it effectively expands upon this by also including "Open models," thereby offering a holistic view of the market's evolution. It serves to illustrate the rapid pace of development and the proliferation of models, both commercially driven and open-source, that characterized the period from 2023 into 2024. The note that "many models are still missing from this overview!" further emphasizes the sheer volume of activity in this domain, making the depicted timeline a significant, though not exhaustive, representation.

**Summary:**
This image displays a timeline illustrating the release of prominent generative AI models from early 2023 extending into 2024. The timeline is designed to categorize models into two distinct groups: "Proprietary models" and "Open models," indicated by the blue upper section and the beige lower section, respectively.

The timeline progresses horizontally, with key year markers "2023" and "2024" highlighted in gray boxes. The leftmost ellipsis and initial dot suggest the timeline begins prior to the visible start, and the arrow on the right indicates continuation beyond what is shown.

**Proprietary Models (Upper Blue Section):**
These models are typically developed and controlled by commercial entities. They appear in approximate chronological order from left to right:
1.  **ChatGPT:** Positioned early on the timeline, representing a foundational release around the beginning of 2023.
2.  **GPT-4:** Follows shortly after ChatGPT in 2023, indicating a subsequent and significant release.
3.  **BARD:** Appears around the same time as GPT-4 or slightly later in 2023.
4.  **PaLM 2:** Released after BARD, during the mid-to-late 2023 period.
5.  **Claude 2:** Emerges later in 2023, following PaLM 2.
6.  **Grok:** Appears towards the end of 2023 or very early 2024.
7.  **Gemini:** Positioned around the "2024" marker, indicating a prominent release early in that year.

**Open Models (Lower Beige Section):**
These models are generally open-source or have publicly available components, often accompanied by their parameter counts (in billions, denoted as 'B'). They are also arranged chronologically from left to right:
1.  **Llama:** Released early in 2023, with various parameter sizes available: "7B/13B/33B/65B".
2.  **MPT:** Follows Llama in early to mid-2023, with parameter options "7B/30B".
3.  **Falcon:** Appears after MPT, with "7B/40B" parameters.
4.  **Llama 2:** A successor or significant update to Llama, released later in 2023, offering "7B/13B/70B" parameter sizes.
5.  **Qwen:** Appears in the latter half of 2023, available in "7B" parameters.
6.  **Mistral:** Released towards the end of 2023, with a "7B" parameter model.
7.  **Yi:** Appears late in 2023 or early 2024, at "34B" parameters.
8.  **Mixtral:** Released around the same period as Yi, featuring an "8 × 7B" parameter configuration (likely indicating a Mixture of Experts architecture).
9.  **Phi-2:** Positioned around the "2024" marker, a smaller model at "2.7B" parameters.
10. **DeciLM:** Also appearing around the "2024" marker, with "7B" parameters.
11. **Command R:** The latest open model depicted on this timeline, appearing after the "2024" marker, at "35B" parameters.

In summary, this timeline vividly illustrates the "Year of Generative AI" by showcasing the rapid and continuous emergence of diverse AI models. It highlights the simultaneous innovation occurring in both proprietary sectors, led by major tech companies, and a robust open-source community that provides a wide array of models with varying scales and capabilities, ultimately shaping the current landscape of generative AI.](images/89c4348b4591c275044513787ec9a7906c24a259f6e712fe8aa6d6881e8f5c2b.jpg)
Figure 1-28. A comprehensive view into the Year of Generative AI. Note that many models are still missing from this overview!

Apart from the widely popular Transformer architecture, new promising architec‐ tures have emerged such as Mamba11,12 and RWKV.13 These novel architectures attempt to reach Transformer-level performance with additional advantages, like larger context windows or faster inference.

These developments exemplify the evolution of the field and showcase 2023 as a truly hectic year for AI. It took all we had to just keep up with the many developments, both within and outside of Language AI.

As such, this book explores more than just the latest LLMs. We will explore how other models, such as embedding models, encoder-only models, and even bag-ofwords can be used to empower LLMs.

# The Moving Definition of a “Large Language Model”

In our travels through the recent history of Language AI, we observed that primarily generative decoder-only (Transformer) models are commonly referred to as large language models. Especially if they are considered to be “large.” In practice, this seems like a rather constrained description!

What if we create a model with the same capabilities as GPT-3 but 10 times smaller? Would such a model fall outside the “large” language model categorization?

Similarly, what if we released a model as big as GPT-4 that can perform accurate text classification but does not have any generative capabilities? Would it still qualify as a large “language model” if its primary function is not language generation, even though it still represents text?

The problem with these kinds of definitions is that we exclude capable models. What name we give one model or the other does not change how it behaves.

Since the definition of the term “large language model” tends to evolve with the release of new models, we want to be explicit in what it means for this book. “Large” is arbitrary and what might be considered a large model today could be small tomor‐ row. There are currently many names for the same thing and to us, “large language models” are also models that do not generate text and can be run on consumer hardware.

As such, aside from covering generative models, this book will also cover models with fewer than 1 billion parameters that do not generate text. We will explore how other models, such as embedding models, representation models, and even bag-of-words can be used to empower LLMs.

# The Training Paradigm of Large Language Models

Traditional machine learning generally involves training a model for a specific task, like classification. As shown in Figure 1-29, we consider this to be a one-step process.

![## Image Analysis: 57e62396ee56555eed5ddcccfebbde257437301b9a97628da56a8683ace6da8d.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental, traditional machine learning process. It illustrates the basic flow of how raw data is utilized to create a functional, trained model.

The main purpose of the image is to visually explain the core components and a singular, task-specific training step involved in traditional machine learning. It clearly communicates the input (Data), the process (Training), and the output (Trained Model) in a simplified, linear fashion. The key message being conveyed is the direct relationship between data, a single training effort, and the resulting model, emphasizing its purpose-built nature for a 'specific task'.

**Content Interpretation:**
This image illustrates the core components and a single-step process of traditional machine learning. It shows the transformation from input data to a trained model.

- **Processes shown:** The central process is 'Training', explicitly labeled as '1 Training (specific task)'. This indicates a singular, focused operation.
- **Concepts and Systems:**
    - **Data (often structured):** This represents the initial input to the machine learning process, typically organized in a tabular or structured format, as suggested by the grid icon. The parenthetical '(often structured)' provides a key characteristic of data used in traditional ML.
    - **Training (specific task):** This is the fundamental step where the machine learning algorithm learns patterns and relationships from the input data. The number '1' highlights that this is a single, distinct step. The phrase '(specific task)' is crucial, indicating that in traditional machine learning, models are developed and optimized for a narrowly defined objective, such as classification or regression, rather than a broad range of tasks.
    - **Trained Model:** This is the output of the training process. It represents the machine learning model that has successfully learned from the data and is now capable of performing the 'specific task' it was trained for.

- **Significance of Information:** The sequential flow from 'Data' to 'Trained Model' via a single 'Training' step underscores the direct and task-specific nature of traditional machine learning workflows. The explicit numbering of '1' for the training step is particularly significant in contrasting it with more complex, multi-stage, or multi-task training paradigms, such as those often found in Large Language Models.

**Key Insights:**
The main takeaway from this image is that traditional machine learning is characterized by a single, focused training step where structured data is used to develop a model for a highly specific task.

- **Key Insight 1: Singular Training Process:** The explicit numerical label '1' associated with 'Training (specific task)' highlights that the learning process is a distinct, single phase. This contrasts with more elaborate, multi-stage, or iterative training paradigms seen in advanced models.
- **Key Insight 2: Task-Specificity:** The phrase '(specific task)' is a critical detail, emphasizing that traditional models are designed and optimized to perform one particular function. This implies that if a new task is required, a new model often needs to be trained from scratch.
- **Key Insight 3: Structured Data Reliance:** The input 'Data (often structured)', depicted as a table, reinforces the common requirement for well-organized and labeled data in traditional machine learning. This structure is often a prerequisite for effective model training in this paradigm.

These insights collectively indicate a relatively straightforward and purpose-built approach to model development in traditional machine learning, setting the stage for discussions on paradigms that might deviate from this simplicity.

**Document Context:**
This image serves as a foundational reference point or a contrast for the document's broader narrative, which is situated in a section titled 'The Training Paradigm of Large Language Models'. By illustrating the 'traditional machine learning' process, which 'involves a single step: training a model for a specific target task, like classification or regression' (as stated in the text after the image), it sets up a clear distinction. The document is likely to subsequently discuss how Large Language Models differ from this traditional paradigm, perhaps involving more complex, multi-stage, or general-purpose training approaches. The image provides the baseline understanding against which the complexities of LLM training will be compared, enabling readers to appreciate the evolution and unique characteristics of modern language models.

**Summary:**
The image displays a simplified, linear process flow illustrating the traditional machine learning training paradigm. The process begins with 'Data (often structured)', which is visually represented by a grid icon with three columns and three rows, where the top row of cells is highlighted in blue, signifying headers or structured data. An arrow points from this data to a blue rounded rectangular box labeled 'Model'. Above this 'Model' box, the word 'Trained' is prominently displayed. Over the connecting arrow, a grey circle containing the number '1' is present, immediately followed by the text 'Training (specific task)'. This entire diagram clearly and concisely communicates that structured data undergoes a singular 'Training' step, specifically tailored for a 'specific task', which then yields a 'Trained Model'. The visual elements and extracted text emphasize a direct and single-purpose transformation from raw data to a functional model.](images/57e62396ee56555eed5ddcccfebbde257437301b9a97628da56a8683ace6da8d.jpg)
Figure 1-29. Traditional machine learning involves a single step: training a model for a specific target task, like classification or regression.

Creating LLMs, in contrast, typically consists of at least two steps:

# Language modeling

The first step, called pretraining, takes the majority of computation and training time. An LLM is trained on a vast corpus of internet text allowing the model to learn grammar, context, and language patterns. This broad training phase is not yet directed toward specific tasks or applications beyond predicting the next word. The resulting model is often referred to as a foundation model or base model. These models generally do not follow instructions.

# Fine-tuning

The second step, fine-tuning or sometimes post-training, involves using the previ‐ ously trained model and further training it on a narrower task. This allows the LLM to adapt to specific tasks or to exhibit desired behavior. For example, we could fine-tune a base model to perform well on a classification task or to follow instructions. It saves massive amounts of resources because the pretraining phase is quite costly and generally requires data and computing resources that are out of the reach of most people and organizations. For instance, Llama 2 has been trained on a dataset containing 2 trillion tokens.14 Imagine the compute necessary to create that model! In Chapter 12, we will go over several methods for fine-tuning foundation models on your dataset.

Any model that goes through the first step, pretraining, we consider a pretrained model, which also includes fine-tuned models. This two-step approach of training is visualized in Figure 1-30.

![## Image Analysis: 5671cea3e05231a6aff4c3d51abb8d733e7e83faf11e458339e446cd05a8bb21.jpg

**Conceptual Understanding:**
This image conceptually represents the standard pipeline for developing Large Language Models (LLMs). Its main purpose is to illustrate the sequential, two-phase training methodology: an initial 'Pretraining' phase to establish a foundational model, followed by a 'Fine-tuning' phase to adapt that model for specialized applications. It communicates the key idea that LLMs undergo distinct stages of learning, each utilizing different types of data (unsupervised vs. supervised) and serving different objectives (general language modeling vs. specific task performance).

**Content Interpretation:**
The image depicts the lifecycle of training a Large Language Model (LLM) through a two-stage process: pretraining and fine-tuning. It illustrates how different types of data are utilized at each stage to develop a general-purpose base model and then adapt it for specific tasks. The process flow explicitly shows an initial input of 'Data (unsupervised)' leading to a 'Base LLM' via '① Pretraining (language modeling)'. This 'Base LLM' then proceeds to '② Fine-tuning (specific task)', where it is further refined using 'Data (supervised)', ultimately yielding a 'Fine-tuned LLM'. The distinction between unsupervised and supervised data, and the different objectives of pretraining (language modeling) and fine-tuning (specific task), are central to understanding the evolution of an LLM's capabilities.

**Key Insights:**
The main takeaways from this image are: 1. LLM training is a two-stage process: '① Pretraining (language modeling)' followed by '② Fine-tuning (specific task)'. 2. 'Pretraining' primarily uses 'Data (unsupervised)' to create a 'Base LLM' with general language understanding capabilities. 3. 'Fine-tuning' refines the 'Base LLM' for a 'specific task' using 'Data (supervised)'. 4. The result of this entire process is a specialized 'Fine-tuned LLM'. These insights are directly evidenced by the sequential flow and explicit labels: 'Data (unsupervised)' -> '① Pretraining (language modeling)' -> 'Base LLM' -> '② Fine-tuning (specific task)' (with input 'Data (supervised)') -> 'Fine-tuned LLM'. The specific annotations '(language modeling)' and '(specific task)' clearly define the purpose of each stage, and '(unsupervised)' and '(supervised)' delineate the nature of the data required.

**Document Context:**
This image directly supports the document's section on 'Fine-tuning' by visually explaining the multi-step approach of LLM training, especially in contrast to traditional machine learning as mentioned in the text after the image. It provides a foundational understanding of how Large Language Models are developed, from a broadly trained 'Base LLM' to a specialized 'Fine-tuned LLM'. By detailing the inputs and processes, it sets the stage for discussions about the 'Fine-tuning' stage and its importance in adapting LLMs for particular applications.

**Summary:**
This image illustrates the two-step process involved in training a Large Language Model (LLM), starting from initial pretraining to a final fine-tuned model. The process begins with 'Data (unsupervised)', represented by a stack of documents, which is fed into the '① Pretraining (language modeling)' phase. This pretraining phase develops a 'Base LLM'. Subsequently, this 'Base LLM' undergoes '② Fine-tuning (specific task)'. The fine-tuning process is supported by additional 'Data (supervised)', shown as a single document icon with an upward-pointing arrow indicating its input into the fine-tuning stage. The ultimate outcome of this entire process is a 'Fine-tuned LLM'. Each step is clearly delineated, showing the type of data used and the resulting model at each stage, highlighting the iterative and data-dependent nature of LLM development. The numbered steps (① and ②) indicate a sequential progression.](images/5671cea3e05231a6aff4c3d51abb8d733e7e83faf11e458339e446cd05a8bb21.jpg)
Figure 1-30. Compared to traditional machine learning, LLM training takes a multistep approach.

Additional fine-tuning steps can be added to further align the model with the user’s preferences, as we will explore in Chapter 12.

# Large Language Model Applications: What Makes Them So Useful?

The nature of LLMs makes them suitable for a wide range of tasks. With text genera‐ tion and prompting, it almost seems as if your imagination is the limit. To illustrate, let’s explore some common tasks and techniques:

Detecting whether a review left by a customer is positive or negative This is (supervised) classification and can be handled with both encoder- and decoder-only models either with pretrained models (see Chapter 4) or by finetuning models (see Chapter 11).

Developing a system for finding common topics in ticket issues This is (unsupervised) classification for which we have no predefined labels. We can leverage encoder-only models to perform the classification itself and decoder-only models for labeling the topics (see Chapter 5).

Building a system for retrieval and inspection of relevant documents

A major component of language model systems is their ability to add external resources of information. Using semantic search, we can build systems that allow us to easily access and find information for an LLM to use (see Chapter 8). Improve your system by creating or fine-tuning a custom embedding model (see Chapter 12).

Constructing an LLM chatbot that can leverage external resources, such as tools and documents

This is a combination of techniques that demonstrates how the true power of LLMs can be found through additional components. Methods such as prompt engineering (see Chapter 6), retrieval-augmented generation (see Chapter 8), and fine-tuning an LLM (see Chapter 12) are all pieces of the LLM puzzle.

Constructing an LLM capable of writing recipes based on a picture showing the products in your fridge

This is a multimodal task where the LLM takes in an image and reasons about what it sees (see Chapter 9). LLMs are being adapted to other modalities, such as Vision, which opens a wide variety of interesting use cases.

LLM applications are incredibly satisfying to create since they are partially bounded by the things you can imagine. As these models grow more accurate, using them in practice for creative use cases such as role-playing and writing children’s books simply becomes more and more fun.

# Responsible LLM Development and Usage

The impact of LLMs has been and likely continues to be significant due to their wide‐ spread adoption. As we explore the incredible capabilities of LLMs it is important to keep their societal and ethical implications in mind. Several key points to consider:

# Bias and fairness

LLMs are trained on large amounts of data that might contain biases. LLMs might learn from these biases, start to reproduce them, and potentially amplify them. Since the data on which LLMs are trained are seldom shared, it remains unclear what potential biases they might contain unless you try them out.

# Transparency and accountability

Due to LLMs’ incredible capabilities, it is not always clear when you are talking with a human or an LLM. As such, the usage of LLMs when interacting with humans can have unintended consequences when there is no human in the loop. For instance, LLM-based applications used in the medical field might be regulated as medical devices since they could affect a patient’s well-being.

# Generating harmful content

An LLM does not necessarily generate ground-truth content and might confi‐ dently output incorrect text. Moreover, they can be used to generate fake news, articles, and other misleading sources of information.

Intellectual property

Is the output of an LLM your intellectual property or that of the LLM’s creator? When the output is similar to a phrase in the training data, does the intellectual property belong to the author of that phrase? Without access to the training data it remains unclear when copyrighted material is being used by the LLM.

# Regulation

Due to the enormous impact of LLMs, governments are starting to regulate commercial applications. An example is the European AI Act, which regulates the development and deployment of foundation models including LLMs.

As you develop and use LLMs, we want to stress the importance of ethical considera‐ tions and urge you to learn more about the safe and responsible use of LLMs and AI systems in general.

# Limited Resources Are All You Need

The compute resources that we have referenced several times thus far generally relate to the GPU(s) you have available on your system. A powerful GPU (graphics card) will make both training and using LLMs much more efficient and faster.

In choosing a GPU, an important component is the amount of VRAM (video random-access memory) you have available. This refers to the amount of memory you have available on your GPU. In practice, the more VRAM you have the better. The reason for this is that some models simply cannot be used at all if you do not have sufficient VRAM.

Because training and fine-tuning LLMs can be an expensive process, GPU-wise, those without a powerful GPU have often been referred to as the GPU-poor. This illustrates the battle for computing resources to train these huge models. To create the Llama 2 family of models, for example, Meta used A100-80 GB GPUs. Assuming renting such a GPU would cost $\$ 1.50/\mathrm { h r }$ the total costs of creating these models would exceed $\$ 5,000,000$ 5

Unfortunately, there is no single rule to determine exactly how much VRAM you need for a specific model. It depends on the model’s architecture and size, compres‐ sion technique, context size, backend for running the model, etc.

This book is for the GPU-poor! We will use models that users can run without the most expensive GPU(s) available or a big budget. To do so, we will make all the code available in Google Colab instances. At the time of writing, a free instance of Google Colab will net you a T4 GPU with 16 GB VRAM, which is the minimum amount of VRAM that we suggest.

# Interfacing with Large Language Models

Interfacing with LLMs is a vital component of not only using them but also develop‐ ing an understanding of their inner workings. Due to the many developments in the field, there has been an abundance of techniques, methods, and packages for communicating with LLMs. Throughout the book, we intend to explore the most common techniques for doing so, including using both proprietary (closed source) and publicly available open models.

# Proprietary, Private Models

Closed source LLMs are models that do not have their weights and architecture shared with the public. They are developed by specific organizations with their underlying code being kept secret. Examples of such models include OpenAI’s GPT-4 and Anthropic’s Claude. These proprietary models are generally backed by significant commercial support and have been developed and integrated within their services.

You can access these models through an interface that communicates with the LLM, called an API (application programming interface), as illustrated in Figure 1-31. For instance, to use ChatGPT in Python you can use OpenAI’s package to interface with the service without directly accessing it.

![## Image Analysis: b8a3787fdf3f5bae26705f128201cbaa943314e0fe56f54ad5280cdc44b3eb91.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture for accessing a closed-source, proprietary Large Language Model. Its main purpose is to illustrate the interaction mechanism where a 'User' communicates with the 'Proprietary LLM (closed source)' exclusively through an 'API (interface)'. The diagram emphasizes the separation of the user's environment from the organization's hosted environment, highlighting that the LLM's core details are not directly exposed to the user but mediated by the API.

**Content Interpretation:**
The image depicts a simplified system architecture for accessing a proprietary Large Language Model (LLM). It illustrates the interaction flow between a 'User' and a 'Proprietary LLM (closed source)' system. The central concept is the role of the 'API (interface)' as the exclusive gateway for user access to the LLM. This highlights a client-server or consumer-provider relationship where the user's application acts as the client and the organization's hosted LLM provides the service via the API. The diagram also clearly delineates the ownership and hosting responsibilities, showing components hosted by the user versus those hosted by the organization.

**Key Insights:**
The main takeaway from this image is that proprietary, closed-source Large Language Models are not directly accessible by users. Instead, all interaction must occur through a well-defined 'API (interface)'. This mechanism ensures that the internal workings, code, and architecture of the 'Proprietary LLM (closed source)' remain hidden from the user, as the model is 'Hosted by organization' while the user's component is 'Hosted by user'. The API acts as a controlled exposure point, allowing functionality to be utilized without revealing the underlying proprietary intellectual property. This also implies a clear division of responsibilities and hosting environments.

**Document Context:**
This image directly supports the document's section on 'Proprietary, Private Models' by visually explaining how these models are accessed by users. The accompanying text states, 'Closed source LLMs are accessed by an interface (API). As a result, details of the LLM itself, including its code and architecture are not shared with the user.' The diagram precisely illustrates this concept by showing the 'User' connecting via 'API (interface)' to the 'Proprietary LLM (closed source)', thus demonstrating the mechanism of interaction and implicitly the separation of user from the LLM's internal details.

**Summary:**
This diagram illustrates how a user interacts with a proprietary, closed-source Large Language Model (LLM) through an Application Programming Interface (API). The process begins with the 'User' component, which is 'Hosted by user'. This user interacts with an 'API (interface)', which serves as the intermediary. The API, along with the 'Proprietary LLM (closed source)', is 'Hosted by organization'. A solid arrow connects the 'User' to the 'API (interface)', indicating data or request flow. Another solid arrow connects the 'API (interface)' to the 'Proprietary LLM (closed source)', showing that the API facilitates access to the LLM. A vertical dotted line clearly separates the components 'Hosted by user' from those 'Hosted by organization', visually delineating the boundary between user-controlled and organization-controlled environments. The diagram clearly depicts a sequential flow where the user's requests pass through the API to reach the proprietary LLM.](images/b8a3787fdf3f5bae26705f128201cbaa943314e0fe56f54ad5280cdc44b3eb91.jpg)
Figure 1-31. Closed source LLMs are accessed by an interface (API). As a result, details of the LLM itself, including its code and architecture are not shared with the user.

A huge benefit of proprietary models is that the user does not need to have a strong GPU to use the LLM. The provider takes care of hosting and running the model and generally has more computing available. There is no expertise necessary concerning hosting and using the model, which lowers the barrier to entry significantly. More‐ over, these models tend to be more performant than their open source counterparts due to the significant investment from these organizations.

A downside to this is that it can be a costly service. The provider manages the risk and costs of hosting the LLM, which often translates to a paid service. Moreover, since there is no direct access to the model, there is no method to fine-tune it yourself. Lastly, your data is shared with the provider, which is not desirable in many common use cases, such as sharing patient data.

# Open Models

Open LLMs are models that share their weights and architecture with the public to use. They are still developed by specific organizations but often share their code for creating or running the model locally—with varying levels of licensing that may or may not allow commercial usage of the model. Cohere’s Command R, the Mistral models, Microsoft’s Phi, and Meta’s Llama models are all examples of open models.

There are ongoing discussions as to what truly represents an open source model. For instance, some publicly shared models have a permissive commercial license, which means that the model cannot be used for commercial purposes. For many, this is not the true definition of open source, which states that using these models should not have any restrictions. Similarly, the data on which a model is trained as well as its source code are seldom shared.

You can download these models and use them on your device as long as you have a powerful GPU that can handle these kinds of models, as shown in Figure 1-32.

![## Image Analysis: e97ad625074d9c3510b8ae3b60a8bce801a9e6b5395e72d04fdf3ba26216f24e.jpg

**Conceptual Understanding:**
This image conceptually represents a self-hosted deployment model for open-source Large Language Models (LLMs). Its main purpose is to illustrate how a user can directly host and interact with an open-source LLM using their own computational resources. The key idea communicated is the direct, user-controlled nature of open-source LLM deployment, where the user manages the necessary hardware to run the model.

**Content Interpretation:**
The image depicts a direct hosting model for open-source Large Language Models (LLMs). The user directly employs their own hardware (such as a personal computer or cloud infrastructure) to host and interact with an open-source LLM. This implies a scenario where the user has full control over the environment and the LLM's deployment. The diagram emphasizes a direct, user-centric interaction and hosting mechanism, differentiating it from models where LLMs are hosted by third-party providers.

**Key Insights:**
The main takeaway from this image is that open-source LLMs can be directly hosted and operated by the end-user, providing them with full control over the environment. This directly contrasts with proprietary or externally hosted models where the user might not have access to the underlying hardware or code. The phrase "Hosted by user" highlights the responsibility and direct involvement of the user in managing the LLM. The inclusion of "(PC, cloud, etc.)" for "User hardware" indicates flexibility in the user's chosen hosting infrastructure. The specific labeling "LLM (open source)" reinforces the core concept of accessible and modifiable models, aligning with the document's "Open Models" section.

**Document Context:**
This image is placed in the "Open Models" section of the document and is explicitly referred to as "Figure 1-32. Open source LLMs are directly by the user. As a result, details of the LLM itself including its code and architecture are shared with the user." It visually complements this surrounding text by illustrating the direct operational relationship between a user, their hardware, and an open-source LLM. The diagram provides a clear visual representation of what it means for an open-source LLM to be "directly by the user," showing that the user's hardware serves as the hosting environment, thereby facilitating direct access to the LLM's code and architecture as mentioned in the text.

**Summary:**
The image illustrates a conceptual model for how open-source Large Language Models (LLMs) are hosted and accessed by a user. The entire process is labeled as "Hosted by user," indicating that the user is responsible for the operational environment. The flow begins with the "User," who interacts with "User hardware" (which can be a "PC, cloud, etc."). This hardware then directly interacts with or hosts the "LLM," which is explicitly identified as an "(open source)" model. The diagram uses a left-to-right flow, starting from the user, moving through their hardware, and concluding with the open-source LLM. This clearly shows a self-contained, user-managed setup for an open-source LLM.](images/e97ad625074d9c3510b8ae3b60a8bce801a9e6b5395e72d04fdf3ba26216f24e.jpg)
Figure 1-32. Open source LLMs are directly by the user. As a result, details of the LLM itself including its code and architecture are shared with the user.

A major advantage of these local models is that you, the user, have complete control over the model. You can use the model without depending on the API connection, fine-tune it, and run sensitive data through it. You are not dependent on any service and have complete transparency of the processes that lead to the output of the model. This benefit is enhanced by the large communities that enable these processes, such as Hugging Face, demonstrating the possibilities of collaborative efforts.

A downside is that you need powerful hardware to run these models and even more when training or fine-tuning them. Moreover, it requires specific knowledge to set up and use these models (which we will cover throughout this book).

We generally prefer using open source models wherever we can. The freedom this gives to play around with options, explore the inner workings, and use the model locally arguably provides more benefits than using proprietary LLMs.

# Open Source Frameworks

Compared to closed source LLMs, open source LLMs require you to use certain pack‐ ages to run them. In 2023, many different packages and frameworks were released that, each in their own way, interact with and make use of LLMs. Wading through hundreds upon hundreds of potentially worthwhile frameworks is not the most enjoyable experience.

As a result, you might even miss your favorite framework in this book!

Instead of attempting to cover every LLM framework in existence (there are too many, and they continue to grow in number), we aim to provide you with a solid foundation for leveraging LLMs. The idea is that after reading this book, you can easily pick up most other frameworks as they all work in a very similar manner.

The intuition that we attempt to realize is an important component of this. If you have an intuitive understanding of not only LLMs but also using them in practice with common frameworks, branching out to others should be a straightforward task.

More specifically, we focus on backend packages. These are packages without a GUI (graphical user interface) that are created for efficiently loading and running any LLM on your device, such as llama.cpp, LangChain, and the core of many frameworks, Hugging Face Transformers.

We will mostly cover frameworks for interacting with large lan‐ guage models through code. Although it helps you learn the fundamentals of these frameworks, sometimes you just want a ChatGPT-like interface with a local LLM. Fortunately, there are many incredible frameworks that allow for this. A few examples include text-generation-webui, KoboldCpp, and LM Studio.

# Generating Your First Text

An important component of using language models is selecting them. The main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general. As the name implies, the package was built on top of the transformers framework that we discussed in “A Recent History of Language AI” on page 5.

At the time of writing, you will find more than 800,000 models on Hugging Face’s platform for many different purposes, from LLMs and computer vision models to models that work with audio and tabular data. Here, you can find almost any open source LLM.

Although we will explore all kinds of models throughout this book, let’s start our first lines of code with a generative model. The main generative model we use throughout the book is Phi-3-mini, which is a relatively small (3.8 billion parameters) but quite performant model.16 Due to its small size, the model can be run on devices with less than 8 GB of VRAM. If you perform quantization, a type of compression that we will further discuss in Chapters 7 and 12, you can use even less than 6 GB of VRAM. Moreover, the model is licensed under the MIT license, which allows the model to be used for commercial purposes without constraints!

Keep in mind that new and improved LLMs are frequently released. To ensure this book remains current, most examples are designed to work with any LLM. We’ll also highlight different models in the repository associated with this book for you to try out.

Let’s get started! When you use an LLM, two models are loaded:

• The generative model itself • Its underlying tokenizer

The tokenizer is in charge of splitting the input text into tokens before feeding it to the generative model. You can find the tokenizer and model on the Hugging Face site and only need the corresponding IDs to be passed. In this case, we use “microsoft/ Phi-3-mini-4k-instruct” as the main path to the model.

We can use transformers to load both the tokenizer and model. Note that we assume you have an NVIDIA GPU (device_map $| =$ "cuda") but you can choose a different device instead. If you do not have access to a GPU you can use the free Google Colab notebooks we made available in the repository of this book:

from transformers import AutoModelForCausalLM, AutoTokenizer   
# Load model and tokenizer   
model $=$ AutoModelForCausalLM.from_pretrained( "microsoft/Phi-3-mini-4k-instruct", device_map="cuda", torch_dtype $=$ "auto", trust_remote_code=True,   
)   
tokenizer $=$ AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

Running the code will start downloading the model and depending on your internet connection can take a couple of minutes.

Although we now have enough to start generating text, there is a nice trick in trans‐ formers that simplifies the process, namely transformers.pipeline. It encapsulates the model, tokenizer, and text generation process into a single function:

from transformers import pipeline

# Create a pipeline   
generator $=$ pipeline( "text-generation", model=model, tokenizer $=$ tokenizer, return_full_text=False, max_new_tokens $\begin{array} { r l } { \mathbf { \Psi } } & { { } = } \end{array} .$ , do_sample=False   
)

The following parameters are worth mentioning:

return_full_text By setting this to False, the prompt will not be returned but merely the output of the model.

max_new_tokens

The maximum number of tokens the model will generate. By setting a limit, we prevent long and unwieldy output as some models might continue generating output until they reach their context window.

do_sample

Whether the model uses a sampling strategy to choose the next token. By setting this to False, the model will always select the next most probable token. In Chapter 6, we explore several sampling parameters that invoke some creativity in the model’s output.

To generate our first text, let’s instruct the model to tell a joke about chickens. To do so, we format the prompt in a list of dictionaries where each dictionary relates to an entity in the conversation. Our role is that of “user” and we use the “content” key to define our prompt:

# The prompt (user input / query)   
messages $= [$ {"role": "user", "content": "Create a funny joke about chickens."}   
]

# Generate output output $=$ generator(messages) print(output[0]["generated_text"])

Why don't chickens like to go to the gym? Because they can't crack the eggsistence of it!

And that is it! The first text generated in this book was a decent joke about chickens.

# Summary

In this first chapter of the book, we delved into the revolutionary impact LLMs have had on the Language AI field. It has significantly changed our approach to tasks such as translation, classification, summarization, and more. Through a recent history of Language AI, we explored the fundamentals of several types of LLMs, from a simple bag-of-words representation to more complex representations using neural networks.

We discussed the attention mechanism as a step toward encoding context within models, a vital component of what makes LLMs so capable. We touched on two main categories of models that use this incredible mechanism: representation models (encoder-only) like BERT and generative models (decoder-only) like the GPT family of models. Both categories are considered large language models throughout this book.

Overall, the chapter provided an overview of the landscape of Language AI, including its applications, societal and ethical implications, and the resources needed to run such models. We ended by generating our first text using Phi-3, a model that will be used throughout the book.

In the next two chapters, you will learn about some underlying processes. We start by exploring tokenization and embeddings in Chapter 2, two often underestimated but vital components of the Language AI field. What follows in Chapter 3 is an in-depth look into language models where you will discover the precise methods used for generating text.

# Tokens and Embeddings

Tokens and embeddings are two of the central concepts of using large language models (LLMs). As we’ve seen in the first chapter, they’re not only important to understanding the history of Language AI, but we cannot have a clear sense of how LLMs work, how they’re built, and where they will go in the future without a good sense of tokens and embeddings, as we can see in Figure 2-1.

![## Image Analysis: ea7aeb828f6dfe1bdd68262c540876e8e46bbbbbcd51e01b0217fed1ec7f6d51.jpg

**Conceptual Understanding:**
This image conceptually illustrates the foundational process of preparing human language for computation by a language model. Its main purpose is to demystify how a continuous stream of text is transformed into discrete, machine-understandable numerical data. The image conveys two key ideas: first, that text is broken down into 'tokens' (smaller, fundamental units), and second, that these tokens are then converted into 'embeddings' (numeric representations that encapsulate their meaning), enabling the language model to perform computations on language.

**Content Interpretation:**
The image depicts the initial processing steps for natural language within a language model: 'Tokenization' and 'Embedding.' The 'Input' stage begins with a sample phrase, 'Have the bards who preceded...'. This input is then subjected to 'Tokenization,' a process explicitly described as to 'Break down the text into smaller pieces (words or parts of words).' This results in a sequence of individual 'Tokens': 'Have', 'the', 'bards', 'who', 'preceded'. Following this, the 'Embedding' process occurs, which is defined as to 'Turn tokens into numeric representations capturing their meaning.' This step converts each token into a set of four-cell grids, representing numerical data, thereby transforming linguistic units into a format comprehensible by computational models. The image clearly shows the transformation of a continuous text string into discrete, semantically rich numerical vectors.

**Key Insights:**
The main takeaways from this image are the two essential preparatory steps for language models to process text: Tokenization and Embedding. Tokenization is the process of breaking down raw input text into smaller, manageable units called tokens, which can be individual words or parts of words, as exemplified by 'Have the bards who preceded...' becoming distinct tokens 'Have', 'the', 'bards', 'who', 'preceded'. The subsequent process, Embedding, is critical for converting these textual tokens into 'numeric representations capturing their meaning,' as explicitly stated. This highlights that language models do not directly operate on text but rather on numerical representations derived from the text, where the semantic content is encoded. The visual representation of abstract 'Embeddings' as grids underscores their numerical, non-textual nature, providing a concrete understanding of how meaning is quantitatively captured for machine processing.

**Document Context:**
This image serves as a crucial visual aid within the document's 'Tokens and Embeddings' section. It directly illustrates the core concept explained in the surrounding text, which states that 'Language models deal with text in small chunks called tokens. For the language model to compute language, it needs to turn tokens into numeric representations called embeddings.' The diagram precisely maps this conceptual explanation into a step-by-step visual process, enhancing the reader's understanding of how raw text is prepared for processing by a language model, thereby laying the groundwork for more advanced topics discussed in the document.

**Summary:**
This image illustrates the fundamental two-step process by which language models convert raw text input into a numerical format they can compute. Starting with an 'Input' text, the process first undergoes 'Tokenization,' where the continuous text is broken down into discrete 'Tokens.' Subsequently, these 'Tokens' are transformed into 'Embeddings,' which are numeric representations designed to capture the meaning of each token. The visual flow clearly delineates these stages, showing the transformation from human-readable text to machine-interpretable data, crucial for any natural language processing task within a language model. The comprehensive transcription ensures that all definitional text and process labels are explicitly stated, leaving no ambiguity in the explanation of each step.](images/ea7aeb828f6dfe1bdd68262c540876e8e46bbbbbcd51e01b0217fed1ec7f6d51.jpg)
Figure 2-1. Language models deal with text in small chunks called tokens. For the lan‐ guage model to compute language, it needs to turn tokens into numeric representations called embeddings.

In this chapter, we look more closely at what tokens are and the tokenization meth‐ ods used to power LLMs. We will then dive into the famous word2vec embedding method that preceded modern-day LLMs and see how it’s extending the concept of token embeddings to build commercial recommendation systems that power a lot of the apps you use. Finally, we go from token embeddings into sentence or text embeddings, where a whole sentence or document can have one vector that represents it—enabling applications like semantic search and topic modeling that we see in Part II of this book.

# LLM Tokenization

The way the majority of people interact with language models, at the time of this writing, is through a web playground that presents a chat interface between the user and a language model. You may notice that a model does not produce its output response all at once; it actually generates one token at a time.

But tokens aren’t only the output of a model, they’re also the way in which the model sees its inputs. A text prompt sent to the model is first broken down into tokens, as we’ll now see.

# How Tokenizers Prepare the Inputs to the Language Model

Viewed from the outside, generative LLMs take an input prompt and generate a response, as we can see in Figure 2-2.

![## Image Analysis: 1448a94c81f8a98443ca26958b65a46ea6ebeefc472d2de3635f922b598c23d5.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental input-process-output mechanism of a language model. Its main purpose is to convey how a language model receives a textual input, performs a core operation on it, and subsequently generates an output. The key ideas communicated are the role of a user-provided 'Input prompt' (exemplified by the phrase 'Have the bards who preceded me left any theme unsung?'), the internal processing function of a 'Language model' (defined as 'Process the text and predict the next token'), and the resulting 'Output'.

**Content Interpretation:**
The image illustrates the fundamental workflow of a language model. It depicts how an input, specifically a textual prompt, is received by the language model, which then performs a defined operation on that text to produce an output. The process shown is the core function of a generative language model: taking an existing sequence of text and predicting the subsequent element (token). The example prompt provides a concrete instance of the type of textual input a model might receive, while the 'Language model' box clarifies its primary task. The flow demonstrates a basic, sequential operation from user input to model processing and ultimately to an output, representing a single inference step.

**Key Insights:**
The main takeaway from this image is the simplified, yet crucial, operational loop of a language model: it accepts a textual prompt as input, processes this text, and then predicts the next token as its output. The specific example prompt, "Have the bards who preceded me left any theme unsung?", highlights that the input is natural language. The text within the 'Language model' box, "Process the text and predict the next token," clearly defines the core computational task performed. This illustrates that language models operate by sequentially predicting elements based on the preceding context, forming the basis for generating longer text sequences.

**Document Context:**
This image serves as a foundational diagram within a section titled "How Tokenizers Prepare the Inputs to the Language Model" and is explicitly referenced as "Figure 2-2. High-level view of a language model and its input prompt." Its placement before discussions on tokenization is crucial as it establishes the overall context of a language model's operation. It first shows the end-to-end process at a high level (input -> model -> output) before the document presumably delves into the specific internal step of tokenization, which is part of how the 'Language model' processes the input text. This diagram ensures readers understand the overall system before focusing on a particular preprocessing step.

**Summary:**
The image provides a high-level conceptual diagram illustrating the fundamental process of a language model. It shows a clear sequence starting with an "Input prompt" provided by a user, which in this example is the phrase "Have the bards who preceded me left any theme unsung?". This prompt is fed into a central component labeled "Language model". The function of this language model is explicitly described as "Process the text and predict the next token". Following this processing, an "Output" is generated, although the specific output content is not shown in this diagram. The diagram visually represents the core input-process-output mechanism of a language model, setting the stage for understanding how such models handle textual data.](images/1448a94c81f8a98443ca26958b65a46ea6ebeefc472d2de3635f922b598c23d5.jpg)
Figure 2-2. High-level view of a language model and its input prompt.

Before the prompt is presented to the language model, however, it first has to go through a tokenizer that breaks it into pieces. You can find an example showing the tokenizer of GPT-4 on the OpenAI Platform. If we feed it the input text, it shows the output in Figure 2-3, where each token is shown in a different color.

![## Image Analysis: 7a5939372ff6b4e9c4c5f53cd99caf221c7f7c458e20fb0de666854bdd37a000.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental process of text tokenization, a critical preprocessing step for large language models (LLMs). Its main purpose is to demonstrate how a natural language sentence is segmented into smaller units called tokens, which are the actual inputs consumed by the language model. The image conveys the key ideas that tokens are often sub-word units (not always full words), that the number of tokens typically differs from the raw character count, and that tokenization is a model-specific procedure (implied by the different GPT model options).

**Content Interpretation:**
The image shows a web-based tool or interface for demonstrating text tokenization. It depicts the process of converting a natural language sentence into a sequence of tokens. The key concepts illustrated are tokenization, the notion of sub-word units, and the distinction between character count versus token count. The relationships shown include the input text being processed to yield 13 tokens from 53 characters. The visual segmentation clearly maps parts of the original text to their resulting tokens, for example, 'unsung?' is split into two tokens: 'unsung' and '?'. The system demonstrated is an interface for tokenizers used by GPT-3.5 & GPT-4 models. The presence of 'Tokens: 13' and 'Characters: 53' signifies that tokens are not directly equivalent to characters or even human-perceived words, as the sentence has fewer words but results in 13 tokens. The color-coded segmentation of the sentence 'Have the bards who preceded me left any theme unsung?' provides visual proof of how the text is split into sub-word units and punctuation, explaining the discrepancy between word count and token count. The 'GPT-3.5 & GPT-4' and 'GPT-3 (Legacy)' tabs indicate that tokenization methods can differ or evolve across different language model versions.

**Key Insights:**
The image teaches several key lessons about tokenization for language models:
1.  **Tokenization is Granular and Sub-Word:** Text is often broken down into units called tokens that are smaller than whole words. For example, the punctuation mark '?' is shown as a separate token, and the total of 13 tokens for a sentence with fewer intuitive words demonstrates this granularity.
2.  **Tokens are Distinct from Characters and Words:** There is no direct one-to-one correspondence between characters, words, and tokens. The input text of 53 characters results in 13 tokens, clearly illustrating this non-equivalence.
3.  **Tokenization Methods Can Be Model-Specific:** The presence of 'GPT-3.5 & GPT-4' and 'GPT-3 (Legacy)' tabs implies that different language models or their versions might utilize distinct tokenization schemes, potentially leading to varying token counts for the same input.
4.  **Tokens Have Textual and Numerical Representations:** The toggle options 'Text' and 'Token IDs' indicate that tokens have both human-readable text forms and machine-processable numerical identifiers.

These insights are supported by:
*   The input text 'Have the bards who preceded me left any theme unsung?' and its subsequent color-coded, segmented display, which visually breaks down words and punctuation into discrete tokens.
*   The explicit counts 'Tokens: 13' and 'Characters: 53', providing quantitative evidence for the difference between these units.
*   The distinct tabs 'GPT-3.5 & GPT-4' and 'GPT-3 (Legacy)', which suggest that tokenization is not a universal process but can depend on the specific model or its version.

**Document Context:**
The image directly supports the document's section titled 'How Tokenizers Prepare the Inputs to the Language Model.' It serves as a concrete, visual example, translating the abstract concept of tokenization into a tangible demonstration of how a specific sentence is broken down into tokens for large language models like GPT. This visual aid makes the explanation of the preprocessing step more accessible and understandable for the reader. The text immediately following the image, 'Figure 2-3. A tokenizer breaks down text into words or parts of words before the model processes the text. It does so according to a specific method and training procedure (from https://oreil.ly/ovUWO),' directly correlates with and is elucidated by the content presented in the image.

**Summary:**
This image displays a user interface demonstrating how a tokenizer, a core component in Natural Language Processing (NLP), prepares text for a large language model. It specifically shows the tokenization process for models like "GPT-3.5 & GPT-4," with an option to compare against "GPT-3 (Legacy)".

At the top, two tabs indicate the chosen language model's tokenizer: "GPT-3.5 & GPT-4" is currently active (highlighted in green), while "GPT-3 (Legacy)" is an alternative. Below these tabs, a large text input box contains the sample sentence: "Have the bards who preceded me left any theme unsung?". This is the raw text that the tokenizer will process.

Below the input box, two action buttons are present: "Clear," presumably to erase the input text, and "Show example," likely to populate the input box with a predefined sample.

After the text has been processed, the interface provides key statistics: the text "Tokens" followed by the number "13," and the text "Characters" followed by the number "53." These numbers indicate that the input sentence, consisting of 53 individual characters (including spaces and punctuation), has been segmented into 13 discrete tokens by the selected tokenizer. This highlights that tokens are not simply equivalent to characters or even whole words.

Further down, the original sentence, "Have the bards who preceded me left any theme unsung?," is displayed again. This time, however, it is visually broken down into its constituent tokens using distinct color-coded segments. For instance, "Have" is one token (purple), "the" is another (light green), "bards" (orange), "who" (red), "preceded" (light blue), "me" (green), "left" (light orange), "any" (red), "theme" (blue), "unsung" (purple), and importantly, the question mark "?" is treated as a separate, eleventh token (green). This visual segmentation vividly demonstrates that tokens can be whole words, parts of words, or punctuation marks.

Finally, at the bottom, there are two toggle buttons to change the display mode of the tokenized output. "Text" is currently selected, showing the textual representation of each token. The alternative, "Token IDs," would likely display the unique numerical identifier assigned to each token by the language model's vocabulary, which is what the model internally processes.

In summary, this image clearly illustrates the tokenizer's role in converting human-readable text into a sequence of machine-understandable tokens, emphasizing that this process involves segmenting text into often smaller, non-standard units (sub-word tokens) and demonstrating how the number of tokens can differ from the character count of the original input.](images/7a5939372ff6b4e9c4c5f53cd99caf221c7f7c458e20fb0de666854bdd37a000.jpg)
Figure 2-3. A tokenizer breaks down text into words or parts of words before the model processes the text. It does so according to a specific method and training procedure (from https://oreil.ly/ovUWO).

Let’s look at a code example and interact with these tokens ourselves. Here we’ll be downloading an LLM and seeing how to tokenize the input before generating text with the LLM.

# Downloading and Running an LLM

Let’s start by loading our model and its tokenizer as we’ve done in Chapter 1:

from transformers import AutoModelForCausalLM, AutoTokenizer

# Load model and tokenizer   
model $=$ AutoModelForCausalLM.from_pretrained( "microsoft/Phi-3-mini-4k-instruct", device_map="cuda", torch_dtype $=$ "auto", trust_remote_code=True,   
)   
tokenizer $=$ AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")

We can then proceed to the actual generation. We first declare our prompt, then tokenize it, then pass those tokens to the model, which generates its output. In this case, we’re asking the model to only generate 20 new tokens:

prompt $=$ "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>"

# Tokenize the input prompt input_ids $=$ tokenizer(prompt, return_tensors $=$ "pt").input_ids.to("cuda")

# Generate the text   
generation_output $=$ model.generate( input_ids $\mathbf { \Psi } =$ input_ids, max_new_tokens $\scriptstyle = 2 \Theta$   
)

# Print the output print(tokenizer.decode(generation_output[0]))

# Output:

<s> Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant $| >$ Subject: My Sincere Apologies for the Gardening Mishap   
Dear

The text in bold is the 20 tokens generated by the model.

Looking at the code, we can see that the model does not in fact receive the text prompt. Instead, the tokenizers processed the input prompt, and returned the infor‐ mation the model needed in the variable input_ids, which the model used as its input.

Let’s print input_ids to see what it holds inside:

tensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420, 920, 372, 9559, 29889, 32001]], device='cuda:0')

This reveals the inputs that LLMs respond to, a series of integers as shown in Figure 2-4. Each one is the unique ID for a specific token (character, word, or part of a word). These IDs reference a table inside the tokenizer containing all the tokens it knows.

![## Image Analysis: cab60c394682bd7e5c9c4bf1b3b33134f4dd3493f7321c930d859671b3b3a15b.jpg

**Conceptual Understanding:**
This image conceptually represents the tokenization pipeline, a critical pre-processing step for natural language understanding and generation tasks performed by language models. Its main purpose is to visualize how an input text string is broken down into smaller units, called tokens, and subsequently converted into a numerical format (token IDs) that a language model can process. The key idea communicated is the transformation of human language into a machine-readable numerical sequence, highlighting the role of the tokenizer as an 'encoder' in this process.

**Content Interpretation:**
The image depicts the pre-processing workflow for natural language text before it is fed into a language model. It illustrates the role of a tokenizer in converting human-readable text into a sequence of numerical identifiers (token IDs) that a machine learning model can understand. The process shown involves segmenting the input text into discrete units called 'Tokens' and then mapping each token to a unique numerical 'Token ID'. This transformation is crucial for bridging the gap between human language and computational processing. The specific example of 'Have the bards who preceded...' highlights how a phrase is broken down and encoded.

**Key Insights:**
The main takeaway from this image is the fundamental process of tokenization in natural language processing (NLP). It teaches that raw text input is not directly consumed by language models but first undergoes a conversion into 'Tokens' and then into numerical 'Token IDs'. This process is mediated by a 'Tokenizer' which 'encodes' the text. The image demonstrates that a single word or part of a word can be a token (e.g., 'Have', 'the', 'b', 'ards', 'who'), and each token corresponds to a specific numerical 'Token ID' (e.g., 'Have' -> '6,975', 'the' -> '278', 'b' -> '278'). The numerical token IDs are the actual input to the 'Language model'. This illustrates the necessity of representing linguistic data in a discrete, quantitative format for machine processing.

**Document Context:**
This image directly supports the document's discussion on the 'Output' section, specifically explaining how a tokenizer prepares input prompts for language models. The text after the image, stating that 'A tokenizer processes the input prompt and prepares the actual input into the language model: a list of token IDs,' directly corresponds to the visual representation. The diagram serves as a concrete illustration of the abstract concept of tokenization, making the technical explanation more accessible and understandable for the reader. It visually clarifies the phrase 'a list of token IDs' by showing how an English sentence is converted into such a list.

**Summary:**
This diagram illustrates the process by which a textual input is prepared for a language model. Initially, the 'Input' text, "Have the bards who preceded...", is fed into a 'Tokenizer'. The tokenizer performs an 'encode' operation, breaking down the input text into individual 'Tokens'. These tokens are then converted into numerical 'Token IDs'. Specifically, the token "Have" is assigned the ID "6,975", "the" is assigned "278", "b" is assigned "278", "ards" is assigned "3,163", and "who" is assigned "1,058". These token IDs represent the actual numerical input that is then passed into the 'Language model'. The process shows a clear, sequential transformation from human-readable text to machine-understandable numerical representations, which are essential for the language model's operation. The diagram visually separates the input, the tokenization process, the resulting tokens and their IDs, and the final destination of this processed data, which is the language model itself.](images/cab60c394682bd7e5c9c4bf1b3b33134f4dd3493f7321c930d859671b3b3a15b.jpg)
Figure 2-4. A tokenizer processes the input prompt and prepares the actual input into the language model: a list of token IDs. The specific token IDs in the figure are just demonstrative.

If we want to inspect those IDs, we can use the tokenizer’s decode method to translate the IDs back into text that we can read:

for id in input_ids[0]: print(tokenizer.decode(id))

This prints (each token is on a separate line):

<table><tr><td>&lt;S&gt;</td></tr><tr><td>Write</td></tr><tr><td>an</td></tr><tr><td>email</td></tr><tr><td>apolog</td></tr><tr><td>izing</td></tr><tr><td>to</td></tr><tr><td> Sarah</td></tr><tr><td>for the</td></tr><tr><td>trag</td></tr><tr><td>ic</td></tr><tr><td> garden</td></tr><tr><td>ing</td></tr><tr><td>m</td></tr><tr><td>ish</td></tr></table>

<table><tr><td></td></tr><tr><td>ap</td></tr><tr><td></td></tr><tr><td>.</td></tr><tr><td>Exp</td></tr><tr><td>lain</td></tr><tr><td>how</td></tr><tr><td>it</td></tr><tr><td>happened</td></tr><tr><td>. &lt;|assistant|&gt;</td></tr></table>

This is how the tokenizer broke down our input prompt. Notice the following:

• The first token is ID 1 $( < s > )$ , a special token indicating the beginning of the text.   
• Some tokens are complete words (e.g., Write, an, email).   
• Some tokens are parts of words (e.g., apolog, izing, trag, ic).   
• Punctuation characters are their own token.

Notice how the space character does not have its own token. Instead, partial tokens (like “izing” and “ic”) have a special hidden character at their beginning that indicates that they’re connected with the token that precedes them in the text. Tokens without that special character are assumed to have a space before them.

On the output side, we can also inspect the tokens generated by the model by printing the generation_output variable. This shows the input tokens as well as the output tokens (we’ll highlight the new tokens in bold):

tensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278,   
25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420,   
920, 372, 9559, 29889, 32001, 3323, 622, 29901, 1619, 317,   
3742, 406, 6225, 11763, 363, 278, 19906, 292, 341, 728,   
481, 13, 13, 29928, 799]], device $\mathbf { = }$ 'cuda:0')

This shows us the model generated the token 3323, 'Sub', followed by token 622, 'ject'. Together they formed the word 'Subject'. They were then followed by token 29901, which is the colon ':'...and so on. Just like on the input side, we need the tokenizer on the output side to translate the token ID into the actual text. We do that using the tokenizer’s decode method. We can pass it an individual token ID or a list of them:

print(tokenizer.decode(3323)) print(tokenizer.decode(622)) print(tokenizer.decode([3323, 622])) print(tokenizer.decode(29901))

This outputs:

<table><tr><td>Sub</td></tr><tr><td>ject</td></tr><tr><td> Subject</td></tr><tr><td>：</td></tr></table>

# How Does the Tokenizer Break Down Text?

There are three major factors that dictate how a tokenizer breaks down an input prompt.

First, at model design time, the creator of the model chooses a tokenization method. Popular methods include byte pair encoding (BPE) (widely used by GPT models) and WordPiece (used by BERT). These methods are similar in that they aim to optimize an efficient set of tokens to represent a text dataset, but they arrive at it in different ways.

Second, after choosing the method, we need to make a number of tokenizer design choices like vocabulary size and what special tokens to use. More on this in “Compar‐ ing Trained LLM Tokenizers” on page 46.

Third, the tokenizer needs to be trained on a specific dataset to establish the best vocabulary it can use to represent that dataset. Even if we set the same methods and parameters, a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset.

In addition to being used to process the input text into a language model, tokenizers are used on the output of the language model to turn the resulting token ID into the output word or token associated with it, as Figure 2-5 shows.

![## Image Analysis: 927c59356ecc0c6986dda851a6f8fd0a0fd063048004bd235fbcc1b8cf882e11.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental pipeline of natural language processing (NLP) for text input and output in the context of a language model. The main purpose of the image is to visually demonstrate how a tokenizer acts as an interface between human-readable text and a machine-processable numerical format, both for preparing input to a language model and interpreting its output. The key ideas communicated are the two primary functions of a tokenizer: encoding (converting text into numerical token IDs) and decoding (converting numerical token IDs back into text), and how these functions facilitate the operation of a language model.

**Content Interpretation:**
The image illustrates the fundamental process of natural language processing (NLP) involving tokenization and a language model. It shows how raw text is first encoded into numerical 'Token IDs' by a tokenizer, processed by a 'Language model', and then how the model's output 'Output token ID' is decoded back into a human-readable token. The specific processes depicted are: 1. **Text Encoding (Tokenization):** The initial input text "Have the bards who preceded..." is broken down into individual 'Tokens' such as "Have", "the", "b", "ards", and "who". Each of these tokens is assigned a unique numerical 'Token ID' (e.g., "Have" -> 6,975, "the" -> 278, "b" -> 278, "ards" -> 3,163, "who" -> 1,058). The splitting of "bards" into "b" and "ards" highlights a common subword tokenization strategy. 2. **Language Model Processing:** The sequence of 'Token IDs' generated from the encoding step (6,975, 278, 278, 3,163, 1,058) is fed into the 'Language model' for processing. 3. **Output Generation:** The 'Language model' produces a numerical 'Output token ID: 1,394'. 4. **Text Decoding:** This 'Output token ID: 1,394' is then passed back to the tokenizer for 'decode'ing, which converts the numerical ID back into its corresponding textual token, which is "Or". The significance of the image lies in demystifying the internal workings of NLP systems, showing the conversion between human-readable text and machine-processable numbers. The example demonstrates that tokenizers can operate at a subword level to handle words efficiently, and that language models perform their computations on these numerical representations.

**Key Insights:**
The main takeaways and lessons from this image are: 1. **Tokenization is a bidirectional process:** It involves both converting text to numerical IDs ("Tokenizer: encode") and converting numerical IDs back to text ("Tokenizer: decode"). 2. **Language models process numerical data:** Language models do not directly handle human language but operate on numerical 'Token IDs', which are the outputs of the encoding phase. This is evident from the 'Token IDs' flowing into the 'Language model'. 3. **Subword tokenization is a common strategy:** Words can be split into smaller, more manageable units (e.g., "bards" being tokenized as "b" and "ards"). This technique helps language models handle out-of-vocabulary words and reduces the overall vocabulary size. The distinct tokens "b" and "ards" with their respective IDs demonstrate this. 4. **Each token has a unique numerical identifier:** The diagram shows specific numerical 'Token IDs' (e.g., 6,975, 278, 3,163, 1,058) associated with individual tokens, highlighting how text is mapped to numbers for machine processing. These insights are directly supported by the explicit labels "Tokenizer: encode" and "Tokenizer: decode", the flow of "Token IDs" into the "Language model", the breakdown of "bards" into smaller tokens, and the unique numerical values assigned to each token.

**Document Context:**
This image directly supports the document's section titled "How Does the Tokenizer Break Down Text?". It provides a clear visual example and step-by-step illustration of the tokenization process, both for encoding input text for a language model and decoding the model's output. The text immediately following the image, "Figure 2-5. Tokenizers are also used to process the output of the model by converting the output token ID into the word or token associated with that ID," precisely describes the latter half of the diagram (Tokenizer: decode), reinforcing its relevance by explaining how tokenizers complete the cycle of converting model output back into comprehensible text.

**Summary:**
This diagram illustrates the end-to-end process of how text is handled by a tokenizer and a language model, from initial input to final output. The process begins with an **Input** text, which in this example is "Have the bards who preceded...". This input text is then sent to a **Tokenizer** for **encoding**. The tokenizer's role is to break down the continuous text into smaller, manageable units called **Tokens**, and assign a unique **Token ID** (a numerical representation) to each. In this specific example: The word "Have" becomes the token "Have" with **Token ID 6,975**. The word "the" becomes the token "the" with **Token ID 278**. The word "bards" is split into two tokens: "b" and "ards". Both "b" and "ards" are assigned the **Token ID 278** and **3,163** respectively. This demonstrates a common practice called subword tokenization, where words can be broken into smaller parts, helping language models handle complex words and variations. Finally, the word "who" becomes the token "who" with **Token ID 1,058**. These generated **Token IDs** (6,975, 278, 278, 3,163, 1,058) are then fed into the **Language model**. The language model processes these numerical inputs to perform its task, such as predicting the next word or generating a response. After processing, the **Language model** outputs a single **Output token ID: 1,394**. This numerical output ID is then sent back to the **Tokenizer** for **decoding**. The tokenizer's decoding function converts the numerical Token ID back into its corresponding human-readable word or token. In this instance, the **Output token ID 1,394** is decoded to the token **Or**. This completes the cycle, showing how a machine-understandable numerical output is translated back into text for human comprehension or further processing.](images/927c59356ecc0c6986dda851a6f8fd0a0fd063048004bd235fbcc1b8cf882e11.jpg)
Figure 2-5. Tokenizers are also used to process the output of the model by converting the output token ID into the word or token associated with that ID.

# Word Versus Subword Versus Character Versus Byte Tokens

The tokenization scheme we just discussed is called subword tokenization. It’s the most commonly used tokenization scheme but not the only one. The four notable ways to tokenize are shown in Figure 2-6. Let’s go over them:

# Word tokens

This approach was common with earlier methods like word2vec but is being used less and less in NLP. Its usefulness, however, led it to be used outside of NLP for use cases such as recommendation systems, as we’ll see later in the chapter.

One challenge with word tokenization is that the tokenizer may be unable to deal with new words that enter the dataset after the tokenizer was trained. This also results in a vocabulary that has a lot of tokens with minimal differences between them (e.g., apology, apologize, apologetic, apologist). This latter chal‐ lenge is resolved by subword tokenization as it has a token for apolog, and then suffix tokens (e.g., -y, -ize, -etic, -ist) that are common with many other tokens, resulting in a more expressive vocabulary.

# Subword tokens

This method contains full and partial words. In addition to the vocabulary expressivity mentioned earlier, another benefit of the approach is its ability to represent new words by breaking down the new token into smaller characters, which tend to be a part of the vocabulary.

![## Image Analysis: a7a1b2c125cd9a7a1c17542d713ec78f3421a0fe27b95d9df52a52c1217cea4e.jpg

**Conceptual Understanding:**
The image conceptually represents the process of **text tokenization** and the varying **granularity levels** at which text can be broken down for computational processing. The main purpose of the image is to visually demonstrate and compare four common tokenization methods: word, subword, character, and byte tokenization, using a single example sentence. It communicates the key idea that the same input text can yield very different token sequences depending on the chosen method, with implications for how natural language is understood and processed by algorithms.

**Content Interpretation:**
The image illustrates various tokenization methods used in natural language processing (NLP). It visually compares how an identical piece of input text, "Have the 🎵 bards who preceded...", is segmented into different components based on the chosen tokenization granularity.

-   **Word tokens** demonstrate a straightforward segmentation by natural language words and symbols, which is the most intuitive level for human understanding.
-   **Subword tokens** show a more advanced segmentation that can break words into smaller, linguistically meaningful units (like prefixes, suffixes, or stems) while keeping common words intact. This method helps manage vocabulary size and handle rare or unseen words effectively.
-   **Character tokens** represent the text as a sequence of individual characters, including spaces. This provides the most atomic unit of linguistic information visible to a human reader.
-   **Byte tokens** delve into the lowest level of representation, showing the binary encoding of each character. This highlights the digital nature of text and the fact that characters, especially complex or non-ASCII ones like "🎵", can be represented by multiple bytes.

The significance of the data presented lies in demonstrating the spectrum of tokenization choices available and their implications for how text is processed. For example, the "🎵" symbol's multi-byte representation in byte tokens emphasizes the universal applicability of byte-level processing for any character set.

**Key Insights:**
The main takeaways from this image are:

1.  **Varying Granularity of Tokenization:** Text can be broken down into tokens at different levels: words, subwords, characters, and bytes. Each level offers a different trade-off in terms of vocabulary size, handling of unknown words, and computational efficiency.
    *   **Textual Evidence:** The distinct rows labeled "Word tokens", "Subword tokens", "Character tokens", and "Byte tokens" clearly illustrate these different levels of granularity, showing the same input text segmented in various ways.

2.  **Subword Tokenization Benefits:** Subword tokenization (e.g., splitting "bards" into "bard" and "s") helps manage the vocabulary size and provides a way to represent out-of-vocabulary words by combining known subword units.
    *   **Textual Evidence:** The explicit splitting of "bards" into "bard" and "s", and "preceded" into "preced" and "ed" under "Subword tokens" provides direct evidence for this concept.

3.  **Universal Text Representation via Bytes:** Byte tokenization offers the most granular and universal method for representing text, capable of encoding any character, including complex or multi-byte symbols.
    *   **Textual Evidence:** The representation of the musical note symbol "🎵" as four separate byte columns under "Byte tokens" explicitly demonstrates that characters can occupy multiple bytes, highlighting the universality and necessity of byte-level processing for diverse character sets.

**Document Context:**
This image directly supports the document's section on "Subword tokens" by providing a clear visual comparison of subword tokenization with other fundamental text tokenization methods. It helps to concretize the abstract concept of breaking down text into different components, as stated in the surrounding text: "There are multiple methods of tokenization that break down the text to different sizes of components (words, subwords, characters, and bytes)." The image serves as an explanatory figure, allowing the reader to visually grasp the output of each tokenization type and understand the granularity differences, especially how subword tokens fit within this hierarchy.

**Summary:**
The image illustrates four different methods of tokenization for the example text "Have the 🎵 bards who preceded...". Each method breaks down the text into progressively smaller units. 

1.  **Word tokens:** The text is segmented into complete words and a symbol: "Have", "the", "🎵", "bards", "who", "preceded", and then "...". This method treats each distinct word and common symbol as an individual token.

2.  **Subword tokens:** This method breaks down the text into meaningful sub-units, which can be parts of words or entire words. For the same input text, the tokens are: "Have", "the", "🎵", "bard", "s", "who", "preced", "ed", and "...". This demonstrates that words like "bards" are split into "bard" and "s", and "preceded" is split into "preced" and "ed", often to handle vocabulary size and out-of-vocabulary words more efficiently.

3.  **Character tokens:** The text is broken down into its individual characters, including spaces. The tokens are: "H", "a", "v", "e", "<space>", "t", "h", "e", "<space>", "🎵", "<space>", "b", "a", "r", "d", "s", and "...". Each character, punctuation, and space is treated as a separate token.

4.  **Byte tokens:** This is the most granular level, where each character (including spaces and symbols) is represented by its underlying byte sequence (binary digits). For the characters "H a v e <space> t h e <space> 🎵 <space>", each is shown as a vertical column of eight binary digits (0s and 1s). For instance, 'H' is represented by the byte sequence 011010010. Notably, the musical note symbol "🎵" is represented by four distinct columns of bytes (111100001, 100111111, 100111001, 100010001), indicating it is a multi-byte character (e.g., in UTF-8 encoding). The arrow above this section visually links the "🎵" and the subsequent "<space>" from the character token level to their respective byte token representations. The "..." in all tokenization rows signifies that the original text continues beyond the shown example.](images/a7a1b2c125cd9a7a1c17542d713ec78f3421a0fe27b95d9df52a52c1217cea4e.jpg)
Figure 2-6. There are multiple methods of tokenization that break down the text to different sizes of components (words, subwords, characters, and bytes).

# Character tokens

This is another method that can deal successfully with new words because it has the raw letters to fall back on. While that makes the representation easier to tokenize, it makes the modeling more difficult. Where a model with subword tokenization can represent “play” as one token, a model using character-level tokens needs to model the information to spell out “p-l-a-y” in addition to modeling the rest of the sequence.

Subword tokens present an advantage over character tokens in the ability to fit more text within the limited context length of a Transformer model. So with a model with a context length of 1,024, you may be able to fit about three times as much text using subword tokenization than using character tokens (subword tokens often average three characters per token).

# Byte tokens

One additional tokenization method breaks down tokens into the individual bytes that are used to represent unicode characters. Papers like “CANINE: Pretraining an efficient tokenization-free encoder for language representation” out‐ line methods like this, which are also called “tokenization-free encoding.” Other works like “ByT5: Towards a token-free future with pre-trained byte-to-byte models” show that this can be a competitive method, especially in multilingual scenarios.

One distinction to highlight here: some subword tokenizers also include bytes as tokens in their vocabulary as the final building block to fall back to when they encounter characters they can’t otherwise represent. The GPT-2 and RoBERTa token‐ izers do this, for example. This doesn’t make them tokenization-free byte-level token‐ izers, because they don’t use these bytes to represent everything, only a subset, as we’ll see in the next section.

If you want to go deeper into tokenizers, they are discussed in more detail in Design‐ ing Large Language Model Applications.

# Comparing Trained LLM Tokenizers

We’ve pointed out earlier three major factors that dictate the tokens that appear within a tokenizer: the tokenization method, the parameters and special tokens we use to initialize the tokenizer, and the dataset the tokenizer is trained on. Let’s compare and contrast a number of actual, trained tokenizers to see how these choices change their behavior. This comparison will show us that newer tokenizers have changed their behavior to improve model performance, and we’ll also see how spe‐ cialized models (like code generation models, for example) often need specialized tokenizers.

We’ll use a number of tokenizers to encode the following text:

text $=$ """   
English and CAPITALIZATION   
??鸟   
show_tokens False None elif $= = > =$ else: two tabs:" " Three tabs:   
$1 2 . 0 ^ { \ast } 5 0 = 6 0 0$

This will allow us to see how each tokenizer deals with a number of different kinds of tokens:

• Capitalization.   
• Languages other than English.   
• Emojis.   
• Programming code with keywords and whitespaces often used for indentation (in languages like Python for example).   
• Numbers and digits.   
• Special tokens. These are unique tokens that have a role other than representing text. They include tokens that indicate the beginning of the text, or the end of the text (which is the way the model signals to the system that it has completed this generation), or other functions as we’ll see.

Let’s go from older to newer tokenizers to see how they tokenize this text and what that might say about the language model. We’ll tokenize the text, and then print each token with a color background color using this function:

colors_list $=$ [ '102;194;165', '252;141;98', '141;160;203', '231;138;195', '166;216;84', '255;217;47'   
]

def show_tokens(sentence, tokenizer_name): tokenizer $=$ AutoTokenizer.from_pretrained(tokenizer_name) token_ids $=$ tokenizer(sentence).input_ids for idx, t in enumerate(token_ids): print( f'\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + tokenizer.decode(t) + '\x1b[0m', end=' )

# BERT base model (uncased) (2018)

Link to the model on the HuggingFace model hub

Tokenization method: WordPiece, introduced in “Japanese and Korean voice search”:

Vocabulary size: 30,522

Special tokens:

unk_token [UNK] An unknown token that the tokenizer has no specific encoding for.

sep_token [SEP]

A separator that enables certain tasks that require giving the model two texts (in these cases, the model is called a cross-encoder). One example is reranking, as we’ll see in Chapter 8.

pad_token [PAD]

A padding token used to pad unused positions in the model’s input (as the model expects a certain length of input, its context-size).

cls_token [CLS] A special classification token for classification tasks, as we’ll see in Chapter 4.

mask_token [MASK]

A masking token used to hide tokens during the training process.

Tokenized text:

[CLS] english and capital ##ization [UNK] [UNK] show _ token ##s false none eli ##f = = > = else : two tab ##s : " " three tab ##s : " " 12 . 0 \* 50 = 600 [SEP]

BERT was released in two major flavors: cased (where the capitalization is kept) and uncased (where all capital letters are first turned into small cap letters). With the uncased (and more popular) version of the BERT tokenizer, we notice the following:

• The newline breaks are gone, which makes the model blind to information encoded in newlines (e.g., a chat log when each turn is in a new line).   
• All the text is in lowercase.   
• The word “capitalization” is encoded as two subtokens: capital ##ization. The ## characters are used to indicate this token is a partial token connected to the token that precedes it. This is also a method to indicate where the spaces are, as it is assumed tokens without ## in front have a space before them.   
• The emoji and Chinese characters are gone and replaced with the [UNK] special token indicating an “unknown token.”

# BERT base model (cased) (2018)

Link to the model on the HuggingFace model hub

Tokenization method: WordPiece

Vocabulary size: 28,996

Special tokens: Same as the uncased version

Tokenized text:

![## Image Analysis: ce8618a64547f1d3755b2a9cb719fafd4aa1f81389d52a5c796bd1be036eafaa.jpg

**Conceptual Understanding:**
This image conceptually represents an input sequence that has undergone tokenization, specifically using a WordPiece-like algorithm, for processing by a BERT-style language model. Its main purpose is to illustrate how raw textual data, encompassing natural language, programming constructs, and mathematical expressions, is segmented into a series of discrete tokens, including special control tokens and subword units. The image highlights the detailed process of preparing text for a neural network, showing the granular components that form the input to a powerful language understanding model like BERT. The presence of '##' prefixes signifies subword tokenization, while `[CLS]`, `[UNK]`, and `[SEP]` are special tokens used for model input formatting and task-specific signaling.

**Content Interpretation:**
The image illustrates the tokenization process for a BERT model, specifically demonstrating how diverse textual input (natural language, programming-like constructs, mathematical expressions) is broken down into a sequence of tokens. It showcases WordPiece tokenization through the use of '##' prefixes for subword units (e.g., 'CA ##PI ##TA ##L ##I ##Z ##AT ##ION' forming 'CAPITALIZATION'). Special tokens like '[CLS]' (for classification/start of sequence), '[UNK]' (for unknown tokens), and '[SEP]' (for separation/end of sequence) are also prominently displayed.

The significance of the data presented lies in showing the granular level at which a BERT model processes text. The presence of '##' prefixed tokens indicates subword tokenization, which is crucial for handling out-of-vocabulary words and reducing vocabulary size while maintaining semantic richness. The inclusion of programming-like snippets ('##if == > = else : two ta ##bs : " " Three ta ##bs : ') and mathematical expressions ('12 . 0 * 50 = 600') highlights the model's capability to process varied content types. The specific text elements, such as '[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK] [UNK] show _ token ##s F ##als ##e None el ##if == > = else : two ta ##bs : " " Three ta ##bs : " 12 . 0 * 50 = 600 [SEP]', directly evidence this comprehensive tokenization, the use of special tokens, and the model's ability to handle different data formats within a single input sequence.

**Key Insights:**
1.  **Subword Tokenization Handles OOV Words and Reduces Vocabulary:** The breakdown of complex words like "CAPITALIZATION" into "CA", "##PI", "##TA", "##L", "##I", "##Z", "##AT", "##ION" and potentially "False" from "F", "##als", "##e" (as evidenced by the '##' prefixes) demonstrates that WordPiece tokenization enables BERT to process words not explicitly in its vocabulary by decomposing them into known subword units. This effectively manages the problem of out-of-vocabulary (OOV) words and keeps the vocabulary size manageable.
2.  **BERT Processes Diverse Input Types:** The sequence includes natural language ("English and"), programming-like syntax ("##if == > = else : two ta ##bs : " " Three ta ##bs : "), and a mathematical expression ("12 . 0 * 50 = 600"). This broad range of token types shows BERT's versatility in handling various forms of textual data, not just standard prose.
3.  **Special Tokens Provide Structural Context for the Model:** The presence of `[CLS]` at the beginning and `[SEP]` at the end (or between segments) provides critical structural information to the model, indicating the boundaries of the input sequence. `[UNK]` tokens alert the model to words outside its known vocabulary. These tokens are integral to how BERT understands and interprets the input's structure for different NLP tasks.
4.  **Cased Models Preserve Capitalization:** The use of tokens like "English" and "CA" (with original capitalization) confirms that the example is consistent with a "BERT base model (cased)", which maintains case information, unlike "uncased" models that convert all text to lowercase.

**Document Context:**
This image is highly relevant to the document's "BERT base model (cased) (2018)" section as it directly illustrates a fundamental step in how BERT processes text: tokenization. Understanding this tokenization process is crucial for comprehending the architecture and functioning of the BERT model. It provides a concrete example of the input format and pre-processing required before text can be fed into the neural network, demonstrating how human-readable text is converted into a machine-understandable sequence of tokens. The "cased" nature of the model is also reinforced by the preserved capitalization in tokens like "English" and "CA".

**Summary:**
This image visually represents a sequence of tokens, which is the output of a tokenization process, likely WordPiece, used to prepare text for a BERT (Bidirectional Encoder Representations from Transformers) model. This specific example demonstrates the tokenization for a "cased" BERT model, meaning it preserves the original capitalization of words.

The sequence begins with [CLS], a special token that signifies the start of the input sequence and is often used by BERT for classification tasks.

Following [CLS], we see natural language words like "English" and "and". Then, the word "CAPITALIZATION" has been broken down into multiple subword tokens: "CA", "##PI", "##TA", "##L", "##I", "##Z", "##AT", "##ION". The "##" prefix indicates that these are continuations of a word rather than standalone words. This subword tokenization is a key feature of BERT, allowing it to handle complex words and out-of-vocabulary (OOV) words by breaking them into smaller, known units.

The sequence then shows two [UNK] tokens, which stand for "unknown". These appear when the tokenization vocabulary does not contain a particular word or subword, and they represent tokens that the model has not encountered before. This is followed by "show", an underscore _, and "token".

The second line presents a series of tokens that resemble programming language elements: "##s", "F", "##als", "##e" (likely forming "False" or "falses"), "None", "el", "##if", "==", ">", "=", "else", and a colon :. This suggests that BERT can process and potentially understand code-like structures. This is further illustrated by "two ta ##bs : " "" and "Three ta ##bs : ", including a string literal "".

Finally, the sequence includes a mathematical expression: "12 . 0 * 50 = 600". This demonstrates that numerical values and mathematical operators are also tokenized and treated as part of the input sequence.

The entire sequence concludes with [SEP], another special token used to separate different segments of text or to mark the end of a single input sequence.

In summary, this image provides a detailed look into how text of various forms—natural language, specialized terminology, code-like fragments, and mathematical expressions—is transformed into a token sequence, complete with special tokens for structural context and subword units for vocabulary coverage, before being processed by a BERT-like language model.](images/ce8618a64547f1d3755b2a9cb719fafd4aa1f81389d52a5c796bd1be036eafaa.jpg)

The cased version of the BERT tokenizer differs mainly in including uppercase tokens.

• Notice how “CAPITALIZATION” is now represented as eight tokens: CA ##PI ##TA ##L ##I ##Z ##AT ##ION.

• Both BERT tokenizers wrap the input within a starting [CLS] token and a closing [SEP] token. [CLS] and [SEP] are utility tokens used to wrap the input text and they serve their own purposes. [CLS] stands for classification as it’s a token used at times for sentence classification. [SEP] stands for separator, as it’s used to separate sentences in some applications that require passing two sentences to a model (For example, in Chapter 8, we will use a [SEP] token to separate the text of the query and a candidate result.)

# GPT-2 (2019)

Link to the model on the HuggingFace model hub

Tokenization method: Byte pair encoding (BPE), introduced in “Neural machine translation of rare words with subword units”.

Vocabulary size: 50,257

Special tokens: <|endoftext|>

![## Image Analysis: ae0e85f9efce8d1a29dcf5a347effc1d2d51148aebc3c1554db1cdcf877a8cb4.jpg

**Conceptual Understanding:**
This image represents the conceptual relationship between the English language and the linguistic rule of capitalization. Its main purpose is to draw attention to the term "CAPITALIZATION" and potentially to the way it can be deconstructed or analyzed. The segmentation of the word "CAPITALIZATION" visually communicates the idea of breaking down a complex linguistic feature into its constituent parts for deeper understanding or processing.

**Content Interpretation:**
The image conceptually represents the phrase "English and CAPITALIZATION," with a particular focus on the word "CAPITALIZATION" by visually segmenting it into distinct parts using different background colors. This segmentation highlights the components of the word, possibly for linguistic analysis, educational purposes, or to draw attention to how such a term might be processed or understood in a computational linguistic context. It primarily illustrates the concept of capitalization as a feature within the English language.

**Key Insights:**
The main takeaway from this image is the explicit highlighting and segmentation of the term "CAPITALIZATION" in the context of the "English" language. The visual breakdown into "CAP," "ITAL," "IZ," and "ATION" suggests an analytical approach to the word, possibly emphasizing its morphological structure, phonetic components, or the distinct elements a language model would need to recognize and process. This reinforces the idea that linguistic features, even at the sub-word level, are important considerations when dealing with natural language processing models.

**Document Context:**
Within the document's broader narrative about GPT-2, a large language model, this image is highly relevant. Language models like GPT-2 are designed to process and generate human language, and a fundamental aspect of English orthography and grammar is capitalization. Understanding the rules and implications of capitalization is crucial for a language model to produce contextually appropriate and grammatically correct text. The visual segmentation of "CAPITALIZATION" could be used to illustrate how such a concept might be broken down for analysis or training in an AI system.

**Summary:**
This image presents the phrase "English and CAPITALIZATION" with a distinct visual emphasis on the word "CAPITALIZATION." The phrase begins with "English" on an orange background, followed by "and" on a blue background. The word "CAPITALIZATION" is then broken down into four segments, each on a different colored background: "CAP" is on a pink background, "ITAL" is on a light green background, "IZ" is on a yellow background, and "ATION" is on a teal background. The segmentation and color-coding visually highlight the individual components of the word "CAPITALIZATION," making the concept stand out and suggesting a focus on its structure or features. The text is clear and readable, with no other annotations, arrows, or detailed graphics present.](images/ae0e85f9efce8d1a29dcf5a347effc1d2d51148aebc3c1554db1cdcf877a8cb4.jpg)

# � �

![## Image Analysis: 4b0c87aaf92321b8df01c6c5a81eaea0d03925b0a65981217744e90fe36813dc.jpg

**Conceptual Understanding:**
This image represents a sequence of lexical tokens or a snippet of programming code. Conceptually, it illustrates the decomposition of a line of code into its fundamental components. The main purpose is to display a series of keywords, literals, operators, and potential identifiers, likely to demonstrate how a piece of code might be tokenized or highlighted by a parser or editor. The key ideas communicated are related to programming language syntax and the essential building blocks (tokens) that form program instructions.

**Content Interpretation:**
This image conceptually represents a sequence of lexical tokens or a fragment of programming code. It shows various programming language elements including identifiers (`show`, `_t ok ens`), boolean literals (`False`), a null literal (`None`), conditional keywords (`elif`, `else`), comparison operators (`==`, `>=`), a syntax delimiter (`:`), and string literals with descriptive labels (`two tabs : " "`, `Three tabs : " "`). The distinct coloring of each element suggests a visual representation of syntax highlighting or the output of a lexical analysis (tokenization). The significance is to demonstrate the fundamental components that comprise programming instructions and how they might be individually recognized and categorized.

**Key Insights:**
The main takeaways from this image are an illustration of how a line of code or a sequence of programming constructs might be represented as individual tokens. The image demonstrates various types of tokens found in programming languages, including: identifiers (`show`, `_t ok ens`), boolean literals (`False`), null literals (`None`), conditional keywords (`elif`, `else`), comparison operators (`==`, `>=`), a syntax delimiter (`:`), string literals (`" "`), and descriptive text related to string content (`two tabs`, `Three tabs`). These specific text elements, particularly "False", "None", "elif", "==", ">=", "else", and the structure "keyword : "literal"", provide direct evidence for these insights, showcasing a diversity of programming language constructs and their distinct roles.

**Document Context:**
This image, likely originating from a technical or academic document, fits into sections discussing programming language fundamentals, syntax, parsing, lexing, or code representation. It could serve to illustrate the concept of tokenization, where a line of code is broken down into its smallest meaningful units. Furthermore, it demonstrates examples of different data types (boolean, null), control flow keywords (conditional statements), and operators. It could also visually represent syntax highlighting, where distinct types of tokens are displayed in different colors, or explain string literals and the representation of whitespace characters (like tabs) within strings. It provides a concrete visual example of programming language elements to support theoretical explanations.

**Summary:**
This image displays a sequence of lexical tokens or a fragment of programming code, presented as a single line of text broken into distinct, colored blocks. It is not a traditional flowchart or process diagram. The sequence of elements from left to right is: "show", "_t ok ens", "False", "None", "elif", "==", ">=", "else", ":", "two tabs : ", """, "Three tabs : ", """ . This sequence illustrates various programming language constructs such as identifiers, boolean and null literals, conditional keywords, comparison operators, and string literals, visually segmented by color. It could be used to explain concepts like tokenization, syntax highlighting, or basic programming language syntax to a reader, demonstrating the building blocks of code.](images/4b0c87aaf92321b8df01c6c5a81eaea0d03925b0a65981217744e90fe36813dc.jpg)

12 . 0 \* 50 = 600

With the GPT-2 tokenizer, we notice the following:

• The newline breaks are represented in the tokenizer.   
• Capitalization is preserved, and the word “CAPITALIZATION” is represented in four tokens.   
• The $\pmb { \triangleright }$ 鸟 characters are now represented by multiple tokens each. While we see these tokens printed as the $\spadesuit$ character, they actually stand for different tokens. For example, the $\pmb { \triangleright }$ emoji is broken down into the tokens with token IDs 8582, 236, and 113. The tokenizer is successful in reconstructing the original character from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]), which prints out $\pmb { \triangleright }$ .   
• The two tabs are represented as two tokens (token number 197 in that vocabu‐ lary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.   
• The two tabs are represented as two tokens (token number 197 in that vocabu‐ lary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.

What is the significance of whitespace characters? These are important for models to understand or generate code. A model that uses a single token to represent four consecutive whitespace characters is more tuned to a Python code dataset. While a model can live with representing it as four different tokens, it does make the modeling more difficult as the model needs to keep track of the indentation level, which often leads to worse performance. This is an example of where tokenization choices can help the model improve on a certain task.

# Flan-T5 (2022)

Tokenization method: Flan-T5 uses a tokenizer implementation called SentencePiece, introduced in “SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing”, which supports BPE and the unigram language model (described in “Subword regularization: Improving neural network translation models with multiple subword candidates”).

Vocabulary size: 32,100

Special tokens:

• unk_token <unk> • pad_token <pad>

Tokenized text:

![## Image Analysis: 6e6f7317991ab3ea96d71075b33bb0d735eefbaa09fe051975b719ed42d20ed6.jpg

**Conceptual Understanding:**
This image illustrates a sequence of text, likely the output or an internal representation from a natural language processing (NLP) model, specifically a tokenized string. The distinct colored blocks suggest individual tokens or sub-word units generated by a tokenizer. It combines natural language segments, special control tokens, conditional programming constructs, and a numerical operation. The main purpose is to demonstrate the fine-grained tokenization of a string, showing how a sequence is broken down into constituent parts, including words, sub-words (for capitalized terms), special 'unknown' tokens ('<unk>'), and control flow-like elements ('show_', 'elif', 'else', '</s>'). It provides insight into the internal workings or output format of an NLP system, likely Flan-T5 as per the document context. Key ideas communicated include tokenization, sub-word tokenization (for 'CAPITALIZATION'), special tokens, conditional logic representation, and the structured output/representation within an NLP pipeline.

**Content Interpretation:**
The image displays the output of a text tokenization process, which is a fundamental step in many NLP systems. It shows how an input string (implicitly 'English and CAPITALIZATION' followed by other conceptual elements) is transformed into a sequence of discrete tokens. The inclusion of '<unk>' tokens suggests that parts of the input might have been outside the model's vocabulary or were placeholder tokens. The segments like 'show_', 'elif', 'else', and the numerical calculation '12.0 * 50 = 600' indicate that the tokenized sequence might represent a complex input that includes both natural language and programmatic/conditional instructions, or it might be a structured output from a task that involves such elements. The '</s>' token is a common end-of-sequence marker in many transformer models. The breakdown of 'CAPITALIZATION' into 'CA', 'PI', 'TAL', 'IZ', 'ATION' highlights sub-word tokenization, a common strategy to handle out-of-vocabulary words and manage vocabulary size efficiently. The programmatic tokens ('show_', 'elif', 'else') suggest that the model can interpret or generate structured queries, commands, or conditional logic, which is critical for tasks like code generation, structured data extraction, or complex instruction following. The numerical operation might represent a generated calculation or a part of a problem-solving sequence.

**Key Insights:**
The main takeaways are: 1. Granular Tokenization: Text is broken down into very fine-grained units, including sub-words (e.g., 'CA', 'PI', 'TAL', 'IZ', 'ATION'), crucial for handling complex vocabulary and morphological variations in NLP models. 2. Handling of Special Tokens: NLP models utilize specific tokens (like '<unk>' for unknown/out-of-vocabulary elements and '</s>' for sequence boundaries) for control purposes. 3. Representation of Programmatic Constructs: The model can represent or generate structures that resemble programming logic (e.g., 'show_tokens False', 'None elif ==>', 'else', numerical operations '12.0 * 50 = 600'), indicating capabilities beyond simple natural language understanding, hinting at the model's ability to engage in code-related tasks or instruction following that involves logical conditions. This image supports the insight that advanced NLP models like Flan-T5 employ sophisticated tokenization schemes and are capable of processing and generating highly structured text that can embed both natural language and programmatic or logical elements. It provides a concrete example of the tokens an NLP model might 'see' or produce when processing a complex input, illustrating its underlying representation power.

**Document Context:**
Given that the image is in a section titled "Flan-T5 (2022)", it is highly relevant as an illustrative example of the Flan-T5 model's tokenization scheme or its internal processing/output format. It visually supports discussions about how such large language models represent and process diverse forms of information—from natural language to programmatic instructions and numerical facts—by breaking them down into a sequence of discrete tokens. This image serves as a concrete demonstration of the input/output interface at a fundamental level for the model.

**Summary:**
This image provides a detailed, tokenized representation of a sequence of text, likely generated by or fed into a natural language processing model such as Flan-T5. Each distinct colored block represents an individual "token" or sub-word unit that the model processes. The sequence begins with common English words: "English" and "and". Following this, the word "CAPITALIZATION" is broken down into its constituent sub-word tokens: "CA", "PI", "TAL", "IZ", "ATION". This sub-word tokenization strategy is common in NLP models to efficiently handle variations of words and reduce the overall vocabulary size. Next, there are two instances of the special token "<unk>", which typically stands for "unknown" or "out-of-vocabulary" tokens. These might represent characters or words not recognized by the model's vocabulary, or they could be placeholders for specific control signals. The sequence then includes tokens that appear to form a programmatic variable or flag: "show_", "to", "ken", "s", "Fal", "s", "e", which combine to suggest "show_tokens False". This indicates a possible configuration setting or a boolean state being represented within the token stream. Following this, tokens "Non", "e", "l", "if", "=", "=>" together form "None elif ==>", resembling an "else if" conditional statement in programming. This is immediately followed by "=" and "else", completing a common conditional logic structure. The conditional logic then presents possible outcomes or labels: "two", "tab", "s", ":" which form "two tabs:", and then "Three", "tab", "s", ":" which form "Three tabs:". These are enclosed by double quotes (" "), suggesting they are string literals or labels associated with the conditional branches. Finally, the sequence includes a numerical operation and its result: "12.", "0", "*", "50", "=", "600", representing the calculation "12.0 * 50 = 600". This demonstrates the model's ability to process or generate numerical expressions. The entire sequence concludes with the token "</s>", a standard end-of-sequence marker used in many transformer-based language models to signify the end of a generated output or an input segment. In summary, this image offers a highly granular view of how a complex input, encompassing natural language, special symbols, programmatic conditions, and numerical operations, is tokenized into discrete units for processing by a model like Flan-T5, highlighting the model's versatility in understanding and generating diverse forms of information.](images/6e6f7317991ab3ea96d71075b33bb0d735eefbaa09fe051975b719ed42d20ed6.jpg)

The Flan-T5 family of models use the SentencePiece method. We notice the following:

• No newline or whitespace tokens; this would make it challenging for the model to work with code.   
• The emoji and Chinese characters are both replaced by the <unk> token, making the model completely blind to them.

# GPT-4 (2023)

Tokenization method: BPE

Vocabulary size: A little over 100,000

Special tokens:

• <|endoftext|>

• Fill in the middle tokens. These three tokens enable the LLM to generate a completion given not only the text before it but also considering the text after it. This method is explained in more detail in the paper “Efficient training of language models to fill in the middle”; its exact details are beyond the scope of this book. These special tokens are: — <|fim_prefix|> — <|fim_middle|> — <|fim_suffix|>

Tokenized text:

English and CAPITAL IZATION

![## Image Analysis: f52c45a26de5ca908ba71eb36f221250fd5b51e8b7e2b4e68a5c354a0c60eb30.jpg

**Conceptual Understanding:**
This image conceptually represents a sequence of individual lexical tokens, likely derived from a programming language (like Python, given `elif`, `False`, `None`, `else`) or a text processing task. Each colored block visually isolates a distinct token, such as keywords, operators, literals, identifiers, and punctuation.

The main purpose of this image is to illustrate how a continuous string of code or text can be broken down into its fundamental, meaningful components (tokens). This process is known as "tokenization," a crucial first step in many language processing tasks, including compilers, interpreters, and natural language processing (NLP) systems. The varying colors might visually categorize different types of tokens (e.g., keywords, literals, operators, identifiers, strings).

Key ideas being communicated include:
*   **Tokenization:** The act of splitting text into a sequence of meaningful units (tokens).
*   **Lexical Analysis:** The initial phase of a compiler where source code is broken down into a stream of tokens.
*   **Syntactic Components:** How individual words, symbols, and operators form the building blocks of a larger statement or expression.
*   **Visual Representation of Tokens:** A method to display tokens, potentially highlighting their distinctiveness or categories through color.

**Content Interpretation:**
The image clearly shows the **tokenization** of a hypothetical code or text snippet. Each colored block represents a single token, which is a fundamental unit of meaning in programming languages or natural language processing.

*   **Keywords and Identifiers:** Tokens like `show`, `_tokens`, `elif`, `else`, `False`, `None`, `two`, `tabs`, `Three` are identified. `elif`, `else`, `False`, `None` are keywords or boolean literals commonly found in Python. `show`, `_tokens`, `two`, `tabs`, `Three` could be identifiers, variables, or parts of string literals.
*   **Operators:** The tokens `==` (equality) and `>=` (greater than or equal to) are clear examples of comparison operators.
*   **Punctuation and Separators:** Tokens such as `:` (colon), `;` (semicolon), `"` (double quote), and ` ` (space) demonstrate how even these seemingly small characters are treated as distinct tokens, especially within string literals or code structure.
*   **String Literals:** The sequence `"` ` ` `"` and `"` ` ` `"` strongly suggests the presence of empty string literals or strings containing only a space. The phrases `two tabs` and `Three tabs` followed by punctuation and quotes indicate potential string content or labels.

**How extracted text elements support these interpretations:**
The verbatim transcription provides direct evidence for each of these interpretations:
*   The presence of `elif`, `else`, `False`, `None` (extracted as tokens 3, 4, 5, 8) confirms the lexical elements of a structured language, likely conditional statements.
*   The operators `==` and `>=` (extracted as tokens 6, 7) are explicitly recognized.
*   The punctuation marks `[:]`, `[;]`, `["]` (extracted as tokens 9, 12, 13, 15, 18, 19, 21) are clearly distinguished as individual tokens.
*   The sequential appearance of `"` ` ` `"` (tokens 13-15 and 19-21) explicitly shows how even spaces within quotes or empty quotes are tokenized.
*   The words `two`, `tabs`, `Three` (tokens 10, 11, 16, 17) demonstrate the tokenization of descriptive text components, which could be part of string literals or comments.

The segmentation into colored blocks visually reinforces the idea that each transcribed text element is a distinct, atomic unit resulting from a tokenization process.

**Key Insights:**
**Main takeaways and lessons from this image:**

1.  **Code/Text is Decomposed into Tokens:** The primary insight is that complex text, whether code or natural language, is first broken down into a series of elementary units called tokens. This is fundamental for any computational processing of text.
    *   **Evidence:** The entire visual representation, where each word, operator, and punctuation mark is isolated into its own colored block (e.g., `show`, `_tokens`, `False`, `None`, `elif`, `==`, `>=`, `else`, `:`, `two`, `tabs`, `;`, `"`, ` `, `"`, `Three`, `tabs`, `;`, `"`, ` `, `"`).

2.  **Tokens Vary in Type and Function:** The sequence includes keywords, operators, literals, and punctuation, demonstrating that tokenization categorizes different parts of the input based on their syntactic or semantic role.
    *   **Evidence:** `False` and `None` are literals (tokens 3, 4). `elif` and `else` are control flow keywords (tokens 5, 8). `==` and `>=` are comparison operators (tokens 6, 7). `:` and `;` are punctuation (tokens 9, 12, 18). `"` indicates string delimiters (tokens 13, 15, 19, 21). `show` and `_tokens` could be identifiers or parts of a function call (tokens 1, 2).

3.  **Whitespace and Quotes are Significant Tokens:** Even whitespace (like a single space) and quotation marks are treated as distinct tokens, particularly when they carry structural or literal meaning.
    *   **Evidence:** The explicit ` ` (space character) token appearing between double quotes `"` ` ` `"` (tokens 13-15 and 19-21) highlights that whitespace, especially within string contexts, is not always ignored but can be tokenized. The quotes themselves are also individual tokens.

4.  **Foundation for Further Processing:** Tokenization is the prerequisite for syntax analysis (parsing) and semantic analysis, as it provides the structured input that subsequent stages of language processing rely upon.
    *   **Evidence:** While not explicitly a "process flow," the linear arrangement of discrete, meaningful units implies that this is an initial stage in a larger pipeline. The specific tokens (e.g., `elif`, `else`, operators) suggest they would then be used to build an abstract syntax tree or similar structure to understand the logic.

**Document Context:**
Given the document context "GPT-4 (2023)," this image most likely illustrates a fundamental concept in large language models or programming language processing: **tokenization**. GPT-4, like other transformer models, operates not directly on raw text characters, but on a sequence of tokens. These tokens are numerical representations of words, sub-words, or characters. This image visually demonstrates how a snippet of input (potentially related to code or a specific linguistic construct) is segmented into these discrete units that an AI model or compiler would then process. It highlights the granular level at which these advanced models "see" and interpret text. The example provided seems to be a blend of programming language constructs (like `elif`, `else`, operators, boolean literals) and potentially string content.

**Summary:**
This image presents a visual breakdown of a segment of text into its individual, fundamental components, known as "tokens." Imagine you have a sentence or a line of computer code; tokenization is the process of splitting that continuous text into a list of discrete words, symbols, or punctuation marks that carry specific meaning. Each colored rectangular block in the image represents one such token.

Reading from left to right, the tokens are:
1.  `show`: Likely an identifier or part of a function name.
2.  `_tokens`: A continuation of an identifier, possibly forming `show_tokens`.
3.  `False`: A boolean literal, representing the value 'false'.
4.  `None`: A special literal, often signifying the absence of a value.
5.  `elif`: A keyword, typically used in conditional statements (short for "else if").
6.  `==`: An operator, specifically for checking equality.
7.  `>=`: An operator, meaning "greater than or equal to."
8.  `else`: Another keyword for conditional statements.
9.  `:`: A colon, often used to introduce a block of code or a statement.
10. `two`: The word "two."
11. `tabs`: The word "tabs."
12. `;`: A semicolon, a punctuation mark.
13. `"`: A double quotation mark, typically marking the beginning or end of a string.
14. ` `: A single space character, treated as a distinct token here.
15. `"`: Another double quotation mark.
16. `Three`: The word "Three."
17. `tabs`: The word "tabs."
18. `;`: A semicolon.
19. `"`: A double quotation mark.
20. ` `: A single space character.
21. `"`: Another double quotation mark.

The different colors of the blocks might visually categorize the types of tokens, such as keywords (e.g., `elif`, `else`), literals (`False`, `None`), operators (`==`, `>=`), or parts of strings. This granular segmentation is a crucial preliminary step in how computers, including advanced AI models like GPT-4, process and understand human language or programming code. By breaking down text into these individual tokens, the system can then assign numerical IDs to each token and process them further for tasks like interpreting code, generating text, or performing analysis. It illustrates the very first layer of understanding a machine gains from raw input text.](images/f52c45a26de5ca908ba71eb36f221250fd5b51e8b7e2b4e68a5c354a0c60eb30.jpg)

# $1 2 \cdot \odot ^ { \star } 5 \Theta = 6 \odot \Theta$

The GPT-4 tokenizer behaves similarly to its ancestor, the GPT-2 tokenizer. Some differences are:

• The GPT-4 tokenizer represents the four spaces as a single token. In fact, it has a specific token for every sequence of whitespaces up to a list of 83 whitespaces.   
• The Python keyword elif has its own token in GPT-4. Both this and the previ‐ ous point stem from the model’s focus on code in addition to natural language.   
• The GPT-4 tokenizer uses fewer tokens to represent most words. Examples here include “CAPITALIZATION” (two tokens versus four) and “tokens” (one token versus three).   
• Refer back to what we said about the GPT-2 tokenizer with regards to the Ł tokens.

# StarCoder2 (2024)

StarCoder2 is a 15-billion parameter model focused on generating code described in the paper “StarCoder 2 and the stack v2: The next generation”, which continues the work from the original StarCoder described in “StarCoder: May the source be with you!”.

Tokenization method: Byte pair encoding (BPE)

Vocabulary size: 49,152

Example special tokens:

• <|endoftext|> • Fill in the middle tokens:

— <fim_prefix> — <fim_middle> — <fim_suffix> — <fim_pad>

• When representing code, managing the context is important. One file might make a function call to a function that is defined in a different file. So the model needs some way of being able to identify code that is in different files in the same code repository, while making a distinction between code in different repos. That’s why StarCoder2 uses special tokens for the name of the repository and the filename:

— <filename> — <reponame> — <gh_stars>

Tokenized text:

![## Image Analysis: 38197bac9c6c23e69869f0550be82dda34aface86b53ed6e0a9937b3d31dea9a.jpg

**Conceptual Understanding:**
Conceptually, the image represents a thematic highlighting of two fundamental linguistic concepts: the English language and the grammatical rule of capitalization. The main purpose of the image is to draw explicit attention to these specific terms, suggesting their importance within the surrounding document's discussion. The visual presentation implies that these concepts are either a subject of analysis, a key feature, or a critical consideration, particularly given the full capitalization of 'CAPITALIZATION' and its segmentation, which further emphasizes its significance.

**Content Interpretation:**
The image explicitly displays the phrase 'English and CAPITALIZATION'. This visually emphasizes two key linguistic components: the English language itself and the grammatical rule of capitalization. The full capitalization of 'CAPITALIZATION' and its visual separation into two distinct colored boxes ('CAPITAL' and 'IZATION') serve to highlight this concept with particular importance. This suggests that capitalization is a significant feature or a specific focus point within the context where this image is presented. The image does not depict any processes, relationships, or systems beyond the textual emphasis on these terms.

**Key Insights:**
The main takeaway from this image is the explicit and visually emphasized focus on 'English' and 'CAPITALIZATION'. This implies that these are key areas of consideration within the document's context, likely related to the capabilities, challenges, or specifications of the StarCoder2 model. The specific textual evidence includes the words 'English', 'and', 'CAPITAL', and 'IZATION', with 'CAPITALIZATION' being fully capitalized and segmented across two distinct colored boxes, signaling its enhanced importance. For a large language model like StarCoder2, understanding and correctly applying capitalization rules in English text (or code, which often uses English keywords and identifiers with specific casing) is a fundamental capability. This image underscores that this aspect is a direct point of attention or a studied feature of the model.

**Document Context:**
Given the document context 'StarCoder2 (2024)', this image likely serves to highlight critical aspects that a language model, such as StarCoder2, must comprehend and process. The English language is fundamental to such models, as it represents a primary domain for training data and output generation. Capitalization is a crucial grammatical and stylistic element in English, with significant implications for meaning, proper noun identification, sentence structure, and even code syntax (where case sensitivity is common). The explicit visual emphasis on 'English' and 'CAPITALIZATION' suggests that these are areas of particular focus, difficulty, or importance for the StarCoder2 model's development, evaluation, or application, especially in tasks related to natural language understanding, generation, or code generation where correct casing is vital.

**Summary:**
The image displays the phrase 'English and CAPITALIZATION', with each word or part of a word enclosed in a distinct colored rectangular box. The word 'English' is in an orange box, 'and' is in a blue box, 'CAPITAL' is in a pink box, and 'IZATION' is in a light green box. The capitalization of 'CAPITAL' and 'IZATION' suggests a strong emphasis on the concept of capitalization. The phrase is presented horizontally, with equal spacing between the boxed words. There are no other graphical elements, arrows, or detailed process flow diagrams present in the image. The simple, segmented presentation highlights these specific linguistic aspects.](images/38197bac9c6c23e69869f0550be82dda34aface86b53ed6e0a9937b3d31dea9a.jpg)

# � �

![## Image Analysis: ad7f19be6e1b77b94d396d7f894713a6226de45f39f1be103acc87c62f013d48.jpg

**Conceptual Understanding:**
The image conceptually represents a tokenized view of a programming language statement or code snippet. Each distinct colored block contains a lexical token (a word, symbol, or character sequence) that would be recognized by a programming language parser or interpreter. This visual segmentation likely serves to illustrate the process of lexical analysis, where raw code is broken down into its fundamental meaningful units.

The main purpose of the image is to convey the structure and components of a piece of code, focusing on aspects of conditional logic, basic data types, and potentially code formatting. It highlights how a complex line or block of code can be dissected into individual, interpretable elements.

Key ideas and concepts being communicated include:
*   **Lexical Tokenization:** The primary concept is the breakdown of text into tokens, which are the building blocks of a programming language.
*   **Conditional Statements:** The keywords `elif` and `else`, along with comparison operators `==` and `>=`, signify control flow through conditional logic.
*   **Data Types/Literals:** `False` (Boolean) and `None` (null/NoneType) represent fundamental data values.
*   **String Literals:** The text enclosed in double quotes suggests string data.
*   **Code Indentation/Structure:** The references to `two tabs` and `Three tabs` hint at the importance of indentation in defining code blocks, particularly relevant in languages like Python.

**Content Interpretation:**
The image displays a tokenized representation of a programming code snippet, likely from Python. It illustrates key programming concepts and elements:

*   **Programming Language Syntax:** The arrangement and combination of `elif`, `==`, `>=`, `else`, `False`, and `None` are characteristic of high-level programming languages, strongly pointing to Python syntax for conditional statements.
*   **Conditional Statement Structure:** The tokens `elif`, `==`, `>=`, and `else :` collectively form a multi-branch conditional statement, where different code paths are executed based on conditions. For instance, `elif` introduces an 

**Key Insights:**
The image offers several key takeaways and insights into programming language concepts and code structure:

1.  **Code Tokenization is Fundamental:** It visually demonstrates the process of lexical analysis, where source code is systematically broken down into discrete, meaningful units (tokens). Each colored block represents one such token, highlighting the granular level at which code is processed by compilers or interpreters. The exact transcription of elements like `show_`, `tokens`, `False`, `None`, `elif`, `==`, `>=`, `else`, `:`, `two`, `tabs`, `:"`, `"`, `"`, `Three`, `tabs`, `"`, `"` provides direct evidence for the specific tokens identified.
2.  **Conditional Logic is a Core Programming Construct:** The presence of `elif` (else if) and `else` keywords, combined with comparison operators (`==`, `>=`), clearly illustrates the essential concept of conditional programming. This allows for diverse execution paths based on specific criteria, a cornerstone of algorithmic design. The verbatim text confirms these control flow elements.
3.  **Basic Data Types and Operators are Building Blocks:** The inclusion of `False` (a Boolean literal) and `None` (a null-type value) along with equality (`==`) and relational (`>=`) operators showcases fundamental data types and operations common in programming. These elements are explicitly transcribed from the image.
4.  **Indentation Significance in Code Structure:** The phrases `two tabs` and `Three tabs`, appearing alongside code elements, strongly suggest that indentation is not merely a stylistic choice but a critical structural component, particularly in languages like Python where it defines code blocks and scope. This textual evidence points to a discussion about code readability and syntax rules.
5.  **Precision in Lexical Analysis:** The highly specific and sometimes unusual grouping of characters into single tokens (e.g., `:"` as one token) underscores the precise and literal nature of how a lexical analyzer processes input. It captures every character and its context, demonstrating the exact output of such a system. The meticulous transcription reflects this precision.

**Document Context:**
The image, which presents a tokenized sequence of keywords, operators, and literals, most likely fits within a technical or academic document related to computer science, programming languages, or software development. Without additional document context (as indicated by "Section: � �"), its specific placement within a larger narrative cannot be definitively stated. However, its content suggests it would be highly relevant in sections discussing:

*   **Lexical Analysis and Tokenization:** It serves as a prime example of how source code is broken down into its fundamental building blocks (tokens) by a compiler or interpreter. The distinct colored blocks visually enhance this concept.
*   **Programming Language Syntax and Semantics:** It could illustrate the syntax of conditional statements (if-elif-else constructs), the use of Boolean and null values, and comparison operators within a specific language, likely Python.
*   **Code Structure and Indentation:** The explicit mention of "two tabs" and "Three tabs" suggests a discussion about code formatting, style guides, or the functional significance of indentation (e.g., in Python where it defines code blocks).
*   **Compiler/Interpreter Design:** It might be used to demonstrate the output of a lexical analyzer component of a compiler or interpreter.

In essence, the image's role is to provide a concrete visual example of a code segment's underlying structure as perceived by a computational system, making abstract parsing concepts more tangible for the reader.

**Summary:**
This image presents a single line of text, broken down into a series of distinct, color-coded blocks. Each block contains a word, symbol, or combination of characters, effectively illustrating a tokenized representation of a code snippet or a structured statement.

Starting from the left, the complete and verbatim sequence of tokens, block by block, is:
1.  **`show_`** (orange block)
2.  **`tokens`** (purple block)
3.  **`False`** (light green block)
4.  **`None`** (yellow block)
5.  **`elif`** (teal block)
6.  **`==`** (orange block)
7.  **`>=`** (light blue block)
8.  **`else`** (purple block)
9.  **`:`** (light green block)
10. **`two`** (yellow block)
11. **`tabs`** (teal block)
12. **`:`** (orange block - a colon)
13. **`"`** (purple block - a single double quote)
14. **`"`** (light green block - another single double quote)
15. **`Three`** (yellow block)
16. **`tabs`** (teal block)
17. **`"`** (orange block - a single double quote)
18. **`"`** (light blue block - another single double quote)

Combining these tokens, the complete verbatim text is: `show_tokens False None elif == >= else : two tabs :"" Three tabs ""`.

This sequence closely resembles a segment of code from a programming language, particularly Python, due to the presence of keywords like `elif`, `False`, `None`, and `else`. The individual colored blocks represent the discrete lexical units or](images/ad7f19be6e1b77b94d396d7f894713a6226de45f39f1be103acc87c62f013d48.jpg)

# 1 2 . 0 \* 5 0 = 6 0 0

This is an encoder that focuses on code generation:

• Similar to GPT-4, it encodes the list of whitespaces as a single token. • A major difference here to everything we’ve seen so far is that each digit is assigned its own token (so 600 becomes $6 \odot \Theta$ ). The hypothesis here is that this would lead to better representation of numbers and mathematics. In GPT-2, for example, the number 870 is represented as a single token. But 871 is represented as two tokens (8 and 71). You can intuitively see how that might be confusing to the model and how it represents numbers.

# Galactica

The Galactica model described in “Galactica: A large language model for science” is focused on scientific knowledge and is trained on many scientific papers, reference materials, and knowledge bases. It pays extra attention to tokenization that makes it more sensitive to the nuances of the dataset it’s representing. For example, it includes special tokens for citations, reasoning, mathematics, amino acid sequences, and DNA sequences.

Tokenization method: Byte pair encoding (BPE)

Vocabulary size: 50,000

Special tokens:

• <s>   
• <pad>   
• </s>   
• <unk>   
• References: Citations are wrapped within the two special tokens: — [START_REF] — [END_REF] — One example of usage from the paper is: Recurrent neural net works, long short-term memory [START_REF]Long Short-Term Memory, Hochreiter[END_REF]   
• Step-by-step reasoning: — <work> is an interesting token that the model uses for chain-of-thought rea‐ soning.

Tokenized text:

![## Image Analysis: c01181d3a1ad27307bbed0678341b04467c74335d13f82becc39b969b2b7dc90.jpg

**Conceptual Understanding:**
The image conceptually represents a specific topic within linguistics or natural language processing: the rules and application of capitalization in the English language. Its main purpose is to serve as a title, heading, or a key identifier for a section or discussion that will elaborate on this particular subject matter. The key ideas communicated are the English language itself and the grammatical concept of capitalization.

**Content Interpretation:**
The image presents the compound concept of "English and CAPITALIZATION". This represents a specific linguistic and grammatical topic. The individual words/syllables are color-coded, suggesting an emphasis on each part, or a visual separation of the components for clarity or stylistic reasons. It is not a process flow but rather a direct statement of a topic.

**Key Insights:**
The main takeaway from this image is the explicit identification of the topic: "English and CAPITALIZATION". It signals that the subsequent content will delve into these two interlinked concepts. The presentation, though simple, emphasizes these terms as key elements of discussion. The textual evidence is the phrase itself, "English and CAPITALIZATION", clearly stating the subject matter.

**Document Context:**
Given the document context "Section: Galactica", this image most likely serves as a sub-heading, a prominent label, or an emphasized keyword within a discussion related to the Galactica project. It suggests that the document will address aspects of the English language, specifically focusing on capitalization rules, potentially in the context of how a system like Galactica handles or understands these linguistic nuances, or how it applies them in text generation or analysis.

**Summary:**
The image displays the phrase "English and CAPITALIZATION". Each word or part of the word is presented in a distinct color block: "English" is in an orange block, "and" is in a blue block, "CAP" is in a pink block, "ITAL" is in a green block, and "IZATION" is in a yellow block. This visual presentation likely serves to highlight the individual components of the phrase or to provide a stylistic heading for a section discussing these concepts. The image acts as a direct label for a topic concerning the rules of capitalization within the English language, presumably as it pertains to the broader context of "Galactica".](images/c01181d3a1ad27307bbed0678341b04467c74335d13f82becc39b969b2b7dc90.jpg)

#

![## Image Analysis: e8b24d311fbb798fd2edd9f6eda243a22a295098c20af38ce465bcdba63b6dc1.jpg

**Conceptual Understanding:**
This image represents a visual output of a tokenization process. Conceptually, it illustrates how a continuous string of text or code is decomposed into a sequence of meaningful units, or 'tokens.' The main purpose is to demonstrate the granular components that a system, such as a large language model or a compiler, recognizes when processing structured input. It communicates the key idea that language processing begins by segmenting input into discrete, categorized elements rather than processing it as an undifferentiated stream of characters. The presence of programming keywords and operators further suggests its context within code analysis.

**Content Interpretation:**
The image conceptually illustrates the process and output of tokenization, likely applied to a segment of programming code. It shows how a continuous string of characters is broken down into a sequence of fundamental, meaningful units called tokens. The content itself is a fragment containing elements such as conditional keywords (`elif`, `else`), boolean and null literals (`False`, `None`), comparison and assignment operators (`==`, `>`, `=`), identifiers (`show_`, `tokens`, `two`, `t`, `abs`, `Three`), punctuation (`:`, `;`), and empty string literals (`""`). The varying background colors for each token visually emphasize their individual identity and potential categorization by a lexical analyzer.

**Key Insights:**
The main takeaways from this image are: 1. **Tokenization is a fundamental process:** It demonstrates the breaking down of raw text/code into elementary units, essential for compilers, interpreters, and large language models. The verbatim transcription of all individual words and symbols as separate blocks ('show_', 'tokens', 'False', 'None', 'elif', etc.) provides direct evidence of this. 2. **Diverse Syntactic Elements:** The image showcases a variety of syntactic elements common in programming languages: keywords (`elif`, `else`), literals (`False`, `None`, `""`), identifiers (`show_tokens`, `two`, `t`, `abs`, `Three`), and operators (`==`, `>`, `=`). The explicit presence of these terms confirms the code-like nature of the tokenized input. 3. **Role of Lexical Analysis:** The distinct coloring for each token implies that a lexical analyzer categorizes these units, a crucial step before parsing. This categorization allows for structural understanding. 4. **Foundation for Language Models:** For systems like Galactica, this image illustrates the initial form in which information is ingested, highlighting the importance of understanding discrete tokens for processing and generating code or structured text.

**Document Context:**
This image is highly relevant to the 'Galactica' section because Galactica is a large language model, particularly known for its capabilities with scientific text and code. Tokenization is an essential precursor to language understanding and generation in such models. The image demonstrates how input, specifically code-like fragments, is processed at a granular level – being split into discrete, manageable tokens. This foundational step allows the language model to interpret the syntax and semantics of the code/text, directly supporting its ability to generate, analyze, or complete similar structures. The detailed breakdown of text into 'tokens' is a core mechanism that underpins the operations of models like Galactica, enabling them to comprehend the underlying structure and meaning of complex textual data.

**Summary:**
The image displays a single line of tokenized text, where each word or symbol is presented in a distinct colored rectangular block, signifying the output of a lexical analysis or tokenization process. From left to right, the tokens are: 'show_', 'tokens', 'False', 'None', 'elif', '==', '>', '=', 'else', ':', 'two', 't', 'abs', ';', '""', 'Three', 't', 'abs', ';', '""'. This comprehensive sequence illustrates various elements typically found in programming languages, including identifiers, boolean and null literals, keywords for conditional statements, comparison and assignment operators, punctuation, and empty string literals. The clear segmentation and color-coding highlight that each of these components is treated as a discrete unit by a tokenizer, which is fundamental for language models to parse and understand structured input like code or specialized text.](images/e8b24d311fbb798fd2edd9f6eda243a22a295098c20af38ce465bcdba63b6dc1.jpg)

# 1 2 . 0 \* 5 0 = 6 0 0

The Galactica tokenizer behaves similar to StarCoder2 in that it has code in mind. It also encodes whitespaces in the same way: assigning a single token to sequences of whitespace of different lengths. It differs in that it also does that for tabs, though. So from all the tokenizers we’ve seen so far, it’s the only one that assigns a single token to the string made up of two tabs $( " \backslash \ t \backslash \ t ^ { \prime } )$ .

# Phi-3 (and Llama 2)

The Phi-3 model we look at in this book reuses the tokenizer of Llama 2 yet adds a number of special tokens.

Tokenization method: Byte pair encoding (BPE)

Vocabulary size: 32,000

Special tokens:

• <|endoftext|>

• Chat tokens: As chat LLMs rose to popularity in 2023, the conversational nature of LLMs started to be a leading use case. Tokenizers have been adapted to this direction by the addition of tokens that indicate the turns in a conversation and the roles of each speaker. These special tokens include:

$$
\begin{array} { l } { - < | \mathsf { u s e r } | > } \\ { - < | \mathsf { a s s i s t a n t } | > } \\ { - < | \mathsf { s y s t e m } | > } \end{array}
$$

We can now recap our tour by looking at all these examples side by side:

<table><tr><td>BERT base model (uncased)</td><td>[CLS]english and capital ##ization [UNk][UNk] showtoken ##s false none eli ##f==&gt;=else:twotab ##s:&quot;&quot;three tab##s:&quot;12.0*50=600[SEP]</td></tr><tr><td>BERT base model (cased)</td><td>[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK][UNK] shoWtoken ##sF##als ##e Noneel##if =&gt;=else :two ta ##bs :&quot; Three ta ##bs:12. *50=600[SEP]</td></tr><tr><td>GPT-2</td><td>Englishand CAP ITAL IZATION showtok ens False Noneelif==&gt;=else:two tabs:&quot;&quot; Three tabs : 12.0*50=600</td></tr><tr><td>FLAN-T5 GPT-4</td><td>English and CA PI TAL IZ ATION &lt;unk&gt;&lt;unk&gt; showto ken SFal s e NoneeLif ==&gt; =else:twotabs:&quot; Three tabs:&quot;&quot;12.0*50=600&lt;/s&gt; EnglishandCAPITALIZATION</td></tr><tr><td></td><td>日日日 show_tokens FalseNoneelif==&gt;=else:twotabs:&quot;&quot;Threetabs:&quot; 12.0*50=600</td></tr><tr><td>StarCoder</td><td>English and CAPITAL IZATION 自0000 showtokens FalseNoneelif==&gt;=else:two tabs :&quot;&quot; Three tabs:&quot; 12.050=600</td></tr></table>

![## Image Analysis: 91a1aa85a754170ae8c4b7c64a80ae0a82e62f2bc7ff1cc306c8607db5799ffa.jpg

**Conceptual Understanding:**
This image visually represents and compares the tokenization outputs of two distinct large language models: Galactica and Phi-3 and Llama 2. Conceptually, it illustrates how raw human-readable text is broken down into smaller, discrete units called tokens, which are the fundamental input for these neural network models. The main purpose of the image is to highlight similarities and, more importantly, differences in the tokenization strategies employed by these models across various types of text inputs, including natural language phrases, sequences of special characters, code-like snippets, and numerical expressions. It conveys the key idea that tokenization is not uniform across all language models and that the granularity and specific choices of token boundaries can vary, which has implications for how each model processes and interprets text.

**Content Interpretation:**
The image compares the tokenization outputs of "Galactica" and "Phi-3 and Llama 2" across several examples. It specifically shows how these large language models segment raw text into discrete tokens, which are the fundamental units of processing.

**Processes Shown:**
- **Text Tokenization:** The core process depicted is the conversion of continuous text strings into a sequence of tokens. This is illustrated by the color-coded blocks, where each block represents a single token.

**Concepts and Systems Shown:**
- **Language Model Tokenization:** The image provides a direct visual representation of the internal tokenization mechanisms of two different language models, Galactica and Phi-3 with Llama 2.
- **Sub-word Tokenization:** The splitting of words like "CAPITALIZATION" into smaller units (e.g., "CAP", "ITAL", "IZATION" or "CAP", "IT", "AL", "IZ", "ATION") demonstrates the use of sub-word tokenization, which allows models to handle out-of-vocabulary words and common prefixes/suffixes.
- **Special Tokens:** The presence of diamond question mark tokens and the `<s >` token highlights the use of special tokens for various purposes, such as representing unknown characters or marking the start of a sequence.
- **Consistency vs. Variation:** The image demonstrates that tokenization can be consistent across models for certain types of input (e.g., code snippets, numerical expressions) but can vary significantly for others (e.g., complex words).

**Significance of Information:**
- The differing granularity in tokenizing "CAPITALIZATION" (three tokens for Galactica vs. five for Phi-3 and Llama 2) signifies different approaches to handling morphology and word structure. A more granular tokenization might allow for better understanding of sub-word units but could also lead to longer input sequences.
- The identical tokenization of code-like snippets and numerical expressions suggests that common patterns for these data types are often handled similarly across different model architectures or their underlying tokenizers.
- The explicit `<s >` token for Phi-3 and Llama 2 (and its absence for Galactica) indicates architectural differences or design choices regarding sequence delimiters, which are crucial for model input formatting and understanding sequence boundaries.

**Supporting Evidence (from transcription):**
- **Galactica - Segment 1: "English | and | CAP | ITAL | IZATION"** clearly shows the tokenization of "CAPITALIZATION" into three parts.
- **Phi-3 and Llama 2 - Segment 2: "English | and | CAP | IT | AL | IZ | ATION"** clearly shows the tokenization of "CAPITALIZATION" into five parts, supporting the idea of finer granularity.
- **Both models - Segment [2/3]: "? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ? | ?"** (13 diamond tokens) indicates shared handling of unknown/special tokens.
- **Both models - Segment [3/4]: "show_tokens | False | None | elif | == | => | =else | : | two | t | abs | : | " | " | Three | t | abs | : | " | ""** demonstrates identical segmentation for code-like text, including the split of "tabs" into "t" and "abs" and explicit empty string tokens.
- **Both models - Segment [4/5]: "12.0 | * | 50 | = | 600"** shows identical segmentation for numerical expressions.
- **Phi-3 and Llama 2 - Segment 1: "<s | >"** specifically identifies the start-of-sequence token unique to this model in the provided examples.

**Key Insights:**
The image provides several key takeaways and insights regarding the tokenization behavior of large language models:

**1. Tokenization Granularity Varies Across Models:**
- **Evidence:** The word "CAPITALIZATION" is tokenized as `CAP | ITAL | IZATION` by Galactica (3 tokens) but as `CAP | IT | AL | IZ | ATION` by Phi-3 and Llama 2 (5 tokens). This clearly demonstrates that different models can have varying levels of granularity in their sub-word tokenization, even for common words.
- **Insight:** This difference in granularity can impact how models understand morphology, handle out-of-vocabulary words, and potentially influence sequence length and computational cost.

**2. Consistency for Structured Text (Code, Numbers):**
- **Evidence:** Both models tokenize the code-like snippet `show_tokens False None elif == => =else: two tabs: "" Three tabs : ""` and the numerical expression `12.0*50=600` identically. For instance, `show_tokens | False | None | elif | == | => | =else | : | two | t | abs | : | " | " | Three | t | abs | : | " | "` and `12.0 | * | 50 | = | 600` are the same for both.
- **Insight:** This suggests that there might be widely adopted or robust tokenization strategies for highly structured data like programming keywords, operators, and numerical values, leading to similar internal representations across different models.

**3. Use of Special/Control Tokens:**
- **Evidence:** Both models show a sequence of thirteen `?` (diamond with question mark) tokens. Phi-3 and Llama 2 additionally uses an explicit `<s | >` token at the beginning of its input.
- **Insight:** Language models use various special tokens for different purposes, such as representing unknown characters, control signals (like start/end of sequence), or padding. These tokens are crucial for the model's internal processing and understanding of the text's structure.

**4. Architectural Differences Impact Tokenization:**
- **Evidence:** The presence of the `<s | >` (start-of-sequence) token for Phi-3 and Llama 2, which is absent in the Galactica examples, points to potential architectural or tokenizer design differences between the models regarding how input sequences are framed.
- **Insight:** Tokenization is not just a pre-processing step but is deeply integrated with the model's architecture. Choices in tokenizer design reflect the underlying model's requirements and training objectives.

**Document Context:**
This image is directly relevant to a section discussing "Phi-3 (and Llama 2)" as it provides a concrete, visual example of their tokenization behavior compared to another model, "Galactica." In the broader document context, it likely serves to illustrate how these specific models process raw text, which is a foundational step in natural language processing. Understanding tokenization differences is crucial for comprehending a model's performance characteristics, its ability to handle various linguistic inputs (e.g., complex words, code snippets, special characters), and potential implications for tasks like text generation, comprehension, or fine-tuning.

The detailed tokenization breakdown helps readers grasp the nuances of sub-word tokenization strategies employed by different large language models. It visually answers questions about how specific inputs are broken down and highlights architectural choices, such as the use of a start-of-sequence token, which are essential for technical understanding of these models.

**Summary:**
This image presents a side-by-side comparison illustrating the tokenization behavior of two different language models: "Galactica" and "Phi-3 and Llama 2". Each model is shown processing several distinct input text strings, with colored blocks indicating the boundaries of individual tokens. The comparison highlights both the commonalities and differences in their sub-word segmentation strategies.

For the input phrase "English and CAPITALIZATION":
- **Galactica** tokenizes this as five distinct tokens: "English", "and", "CAP", "ITAL", "IZATION".
- **Phi-3 and Llama 2** tokenizes the same phrase into seven tokens: "English", "and", "CAP", "IT", "AL", "IZ", "ATION". This indicates a more granular segmentation of the word "CAPITALIZATION" by Phi-3 and Llama 2.

Both models then display a sequence of thirteen identical tokens, each represented by a diamond shape containing a question mark (?). These likely serve as placeholders for special, unknown, or control tokens.

Next, for a code-like snippet `show_tokens False None elif == => =else: two tabs: "" Three tabs : ""`:
- Both **Galactica** and **Phi-3 and Llama 2** tokenize this string identically. The individual tokens are: "show_tokens", "False", "None", "elif", "==", "=>", "=else", ":", "two", "t", "abs", ":", "" (an empty string token), "" (an empty string token), "Three", "t", "abs", ":", "" (an empty string token), "" (an empty string token). It's notable that the word "tabs" is consistently split into "t" and "abs" as separate tokens.

Finally, for the numerical expression `12.0*50=600`:
- Both **Galactica** and **Phi-3 and Llama 2** show identical tokenization: "12.0", "*", "50", "=", "600". This suggests a consistent approach for numerical values and arithmetic symbols.

A key distinguishing feature is that "Phi-3 and Llama 2" explicitly includes an initial "<s" token followed by a ">" token (segmented as "<s" | ">") at the beginning of its first example, which is a common representation for a "start of sequence" or "beginning of text" marker in transformer models. This token is not present in the Galactica examples shown. The detailed comparison allows readers to understand the specific tokenization choices made by each model for different types of linguistic inputs.](images/91a1aa85a754170ae8c4b7c64a80ae0a82e62f2bc7ff1cc306c8607db5799ffa.jpg)

# Tokenizer Properties

The preceding guided tour of trained tokenizers showed a number of ways in which actual tokenizers differ from each other. But what determines their tokenization behavior? There are three major groups of design choices that determine how the tokenizer will break down text: the tokenization method, the initialization parame‐ ters, and the domain of the data the tokenizer targets.

# Tokenization methods

As we’ve seen, there are a number of tokenization methods with byte pair encoding (BPE) being the more popular one. Each of these methods outlines an algorithm for how to choose an appropriate set of tokens to represent a dataset. You can find a great overview of all these methods on the Hugging Face page that summarizes tokenizers.

# Tokenizer parameters

After choosing a tokenization method, an LLM designer needs to make some deci‐ sions about the parameters of the tokenizer. These include:

Vocabulary size

How many tokens to keep in the tokenizer’s vocabulary? (30K and 50K are often used as vocabulary size values, but more and more we’re seeing larger sizes like 100K.)

Special tokens

What special tokens do we want the model to keep track of? We can add as many of these as we want, especially if we want to build an LLM for special use cases. Common choices include:

• Beginning of text token (e.g., $\tt { < S > }$ )   
• End of text token   
• Padding token

• Unknown token • CLS token • Masking token

Aside from these, the LLM designer can add tokens that help better model the domain of the problem they’re trying to focus on, as we’ve seen with Galactica’s <work> and [START_REF] tokens.

# Capitalization

In languages such as English, how do we want to deal with capitalization? Should we convert everything to lowercase? (Name capitalization often carries useful information, but do we want to waste token vocabulary space on all-caps versions of words?)

# The domain of the data

Even if we select the same method and parameters, tokenizer behavior will be differ‐ ent based on the dataset it was trained on (before we even start model training). The tokenization methods mentioned previously work by optimizing the vocabulary to represent a specific dataset. From our guided tour we’ve seen how that has an impact on datasets like code and multilingual text.

For code, for example, we’ve seen that a text-focused tokenizer may tokenize the indentation spaces like this (we’ll highlight some tokens in color):

def add_numbers(a, b):

...."""Add the two numbers \`a\` and \`b\`."""

....return a $^ +$ b

This may be suboptimal for a code-focused model. Code-focused models are often improved by making different tokenization choices:

def add_numbers(a, b):

"""Add the two numbers \`a\` and \`b\`."""

.return a + b

These tokenization choices make the model’s job easier and thus its performance has a higher probability of improving.

You can find a more detailed tutorial on training tokenizers in the Tokenizers section of the Hugging Face course and in Natural Language Processing with Transformers, Revised Edition.

# Token Embeddings

Now that we understand tokenization, we have solved one part of the problem of representing language to a language model. In this sense, language is a sequence of tokens. And if we train a good-enough model on a large-enough set of tokens, it starts to capture the complex patterns that appear in its training dataset:

• If the training data contains a lot of English text, that pattern reveals itself as a model capable of representing and generating the English language. • If the training data contains factual information (Wikipedia, for example), the model would have the ability to generate some factual information (see the following note).

The next piece of the puzzle is finding the best numerical representation for these tokens that the model can use to calculate and properly model the patterns in the text. These patterns reveal themselves to us as a model’s coherence in a specific language, or capability to code, or any of the growing list of capabilities we expect from language models.

As we’ve seen in Chapter 1, that is what embeddings are. They are the numeric representation space utilized to capture the meanings and patterns in language.

Oops: Achieving a good threshold of language coherence and better-than-average factual generation, however, starts to present a new problem. Some users start to trust the model’s fact generation ability (e.g., at the beginning of 2023 some language models were being dubbed “Google killers”). It didn’t take long for advanced users to recognize that generation models alone aren’t reliable search engines. This led to the rise of retrieval-augmented genera‐ tion (RAG), which combines search and LLMs. We cover RAG in more detail in Chapter 8.

# A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer

After a tokenizer is initialized and trained, it is then used in the training process of its associated language model. This is why a pretrained language model is linked with its tokenizer and can’t use a different tokenizer without training.

The language model holds an embedding vector for each token in the tokenizer’s vocabulary, as we can see in Figure 2-7. When we download a pretrained language model, a portion of the model is this embeddings matrix holding all of these vectors.

Before the beginning of the training process, these vectors are randomly initialized like the rest of the model’s weights, but the training process assigns them the values that enable the useful behavior they’re trained to perform.

![## Image Analysis: bf8d8dff490607a8a8870775c773622166b6f2c008cf85c052e283259c01d9fd.jpg

**Conceptual Understanding:**
This image conceptually represents the foundational mechanism by which a language model processes textual input. Its main purpose is to illustrate the relationship between a tokenizer's vocabulary and the corresponding embedding vectors stored within a language model. It communicates the key idea that each discrete token, identified by a unique `Token ID` assigned by a tokenizer, has an associated continuous vector representation (an embedding) that a language model uses for its internal computations and understanding of language. The image clarifies how textual units are mapped to numerical, semantically rich representations for machine processing.

**Content Interpretation:**
The image shows two core components in natural language processing: a `Trained tokenizer` and a `Language model`. 

**1. Trained tokenizer:** This component is depicted as a lookup table named "Tokens" that maps a unique numerical `Token ID` to its corresponding `Token` (a piece of text or character). For example, `Token ID` `0` is mapped to the `Token` `!` and `Token ID` `1` is mapped to the `Token` `"` (double quote). The table shows a range of `Token ID`s from `0` to `50,257`, implying a vocabulary size of 50,258 distinct tokens.

**2. Language model:** This component is shown storing "Token embeddings." For each `Token ID` from the tokenizer's vocabulary (e.g., `0`, `1`, ..., `50,257`), the language model holds a corresponding embedding vector, represented graphically as a small blue grid of cells. These grids signify dense numerical vectors that capture the semantic and syntactic properties of the tokens in a high-dimensional space.

The relationship between them is that the `Language model` relies on the `Token ID`s established by the `Trained tokenizer` to store and retrieve the appropriate `Token embeddings`. Each discrete token from the tokenizer's vocabulary has a unique ID, and this ID is used by the language model to index and access its continuous vector representation.

**Key Insights:**
The main takeaways from this image are:

1.  **Tokenizer's Role:** A tokenizer (specifically a `Trained tokenizer`) is responsible for converting raw text into discrete units called `Tokens` and assigning each a unique `Token ID`. The image explicitly shows this with the "Tokens" table mapping `Token ID` `0` to `!` and `1` to `"`, extending to `50,257` unique `Token ID`s.
2.  **Language Model's Role:** A language model stores numerical representations, known as `Token embeddings`, for each `Token ID` in the tokenizer's vocabulary. The diagram illustrates `Token embeddings` as grid-like vectors associated with `Token ID`s `0`, `1`, and `50,257`.
3.  **Direct Correspondence:** There is a one-to-one correspondence between a `Token ID` generated by the tokenizer and its `Token embedding` within the language model. The shared `Token ID`s (`0`, `1`, ..., `50,257`) across both sections explicitly demonstrate this direct link.
4.  **Vocabulary Size:** The vocabulary size, determined by the tokenizer, dictates the number of embeddings a language model must maintain. The range `0` to `50,257` indicates a vocabulary of `50,258` items.

These insights are directly supported by the verbatim textual elements: the headers "Trained tokenizer" and "Language model," the internal labels "Tokens," "Token ID," "Token," and "Token embeddings," and the specific numerical (`0`, `1`, `50,257`) and symbolic (`!`, `"`) entries.

**Document Context:**
The image is directly relevant to the section title "A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer" and the subsequent caption "Figure 2-7. A language model holds an embedding vector associated with each token in its tokenizer." It visually explains this core concept, illustrating how a tokenizer creates a mapping from numerical IDs to textual tokens, and how a language model then uses these same IDs to associate each token with a unique numerical embedding vector. This fundamental concept underpins how language models process and understand human language by converting discrete words/subwords into a continuous numerical space.

**Summary:**
The image illustrates the fundamental relationship between a `Trained tokenizer` and a `Language model` in the context of natural language processing, specifically how a language model maintains embeddings for the vocabulary defined by its tokenizer. 

The diagram is split into two main conceptual components, presented side-by-side:

On the left, under the header "Trained tokenizer" (colored red), a table labeled "Tokens" is shown. This table has two columns: "Token ID" and "Token". The "Token ID" column lists numerical identifiers starting from 0, then 1, followed by an ellipsis (indicating continuation), and ending with 50,257. Correspondingly, the "Token" column shows the actual linguistic units: "!" for Token ID 0, """ (double quote) for Token ID 1, followed by an ellipsis, and presumably the token associated with ID 50,257 (though its specific token is not shown, only the ID).

On the right, under the header "Language model" (colored blue), a structure labeled "Token embeddings" is depicted. This section also uses the same numerical identifiers for tokens. For Token ID 0, a blue grid-like structure is shown, representing a vector embedding. Similarly, for Token ID 1, another blue grid-like structure represents its embedding. An ellipsis indicates continuation, and finally, for Token ID 50,257, a blue grid-like structure again represents its corresponding embedding vector.

In essence, the `Trained tokenizer` maps unique numerical IDs to discrete textual tokens, forming the vocabulary. The `Language model` then stores a continuous, dense numerical representation (an embedding vector) for each of these tokens, indexed by the same `Token ID`s. The consistent use of `Token ID`s from 0 to 50,257 across both components highlights a direct one-to-one correspondence between a specific token, its unique ID, and its learned embedding vector within the language model.](images/bf8d8dff490607a8a8870775c773622166b6f2c008cf85c052e283259c01d9fd.jpg)
Figure 2-7. A language model holds an embedding vector associated with each token in its tokenizer.

# Creating Contextualized Word Embeddings with Language Models

Now that we’ve covered token embeddings as the input to a language model, let’s look at how language models can create better token embeddings. This is one of the primary ways to use language models for text representation. This empowers applications like named-entity recognition or extractive text summarization (which summarizes a long text by highlighting the most important parts of it, instead of generating new text as a summary).

Instead of representing each token or word with a static vector, language models create contextualized word embeddings (shown in Figure 2-8) that represent a word with a different token based on its context. These vectors can then be used by other systems for a variety of tasks. In addition to the text applications we mentioned in the previous paragraph, these contextualized vectors, for example, are what powers AI image generation systems like DALL·E, Midjourney, and Stable Diffusion, for example.

![## Image Analysis: 19e30f05984b3df5cbf5d0cbd394839da6a3040fe4af012f006d16296262d05c.jpg

**Conceptual Understanding:**
This image conceptually represents the function of a language model in generating contextualized word embeddings. Its main purpose is to illustrate the process by which a language model takes a piece of text, processes it to understand the context, and then outputs rich, context-dependent numerical representations (embeddings) for each word or token in that text. The image communicates the key idea that language models enhance token embeddings by incorporating the surrounding linguistic context, leading to more meaningful and useful representations compared to static embeddings.

**Content Interpretation:**
The image shows a conceptual process of how a language model generates contextualized word embeddings. The input is a sentence, which is then processed by a 'Language model'. This model's function is to 'Process the text and incorporate additional context'. The output consists of 'Contextualized token embedding vectors', which are explicitly described as 'Better token embedding vectors that incorporate more context'. The five blue squares visually represent these individual token embeddings. The entire process illustrates the transformation of raw textual input into enriched, context-sensitive numerical representations, which are fundamental for various natural language processing tasks. The significance lies in demonstrating that language models move beyond static embeddings by integrating surrounding text to create more informative token representations.

**Key Insights:**
The main takeaway from this image is that modern language models are capable of generating 'Contextualized token embedding vectors' which are superior to raw, static token embeddings. The image teaches that the key to this improvement is the language model's ability to 'Process the text and incorporate additional context'. This means that the meaning and representation of a word (token) are not fixed but are dynamically adjusted based on the surrounding words in a given sentence. The specific text 'Better token embedding vectors that incorporate more context' directly provides evidence for the improved quality and contextual nature of the output embeddings. The input sentence 'Have the bards who preceded me left any theme unsung?' serves as a concrete example of text that would undergo this contextualization process.

**Document Context:**
This image directly supports the document's section 'Creating Contextualized Word Embeddings with Language Models' and specifically relates to the text after the image, which states: 'Figure 2-8. Language models produce contextualized token embeddings that improve on raw, static token embeddings.' The diagram visually explains the mechanism by which language models achieve this improvement. It clarifies the 'how' – by taking input text, processing it to add context, and then outputting 'better' embeddings. This illustration is crucial for understanding the core concept of contextualization in modern NLP, differentiating it from earlier methods that might have used static embeddings.

**Summary:**
This image illustrates how a language model processes input text to produce contextualized token embeddings. The process begins with an input text, 'Have the bards who preceded me left any theme unsung?'. This text is then fed into a 'Language model', which is described as an entity that 'Process the text and incorporate additional context'. The output of this language model is a series of five blue square shapes, representing individual tokens. Below these shapes, the output is explicitly labeled as 'Contextualized token embedding vectors', with an accompanying explanation: 'Better token embedding vectors that incorporate more context'. The overall flow demonstrates the transformation of raw input text into more meaningful, context-aware numerical representations through the application of a language model. Each component contributes to understanding how modern language models generate richer representations of words.](images/19e30f05984b3df5cbf5d0cbd394839da6a3040fe4af012f006d16296262d05c.jpg)
Figure 2-8. Language models produce contextualized token embeddings that improve on raw, static token embeddings.

Let’s look at how we can generate contextualized word embeddings; the majority of this code should be familiar to you by now:

from transformers import AutoModel, AutoTokenizer   
# Load a tokenizer   
tokenizer $=$ AutoTokenizer.from_pretrained("microsoft/deberta-base")   
# Load a language model   
model $=$ AutoModel.from_pretrained("microsoft/deberta-v3-xsmall")   
# Tokenize the sentence   
tokens $=$ tokenizer('Hello world', return_tensors $: =$ 'pt')   
# Process the tokens   
output $=$ model(\*\*tokens)[0]

The model we’re using here is called DeBERTa v3, which at the time of writing is one of the best-performing language models for token embeddings while being small and highly efficient. It is described in the paper “DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training gradient-disentangled embedding sharing”.

This code downloads a pretrained tokenizer and model, then uses them to process the string “Hello world”. The output of the model is then saved in the output variable. Let’s inspect that variable by first printing its dimensions (we expect it to be a multidimensional array):

output.shape

This prints out:

<table><tr><td>torch.Size([1,4,384])</td></tr></table>

Skipping the first dimension, we can read this as four tokens, each one embedded in a vector of 384 values. The first dimension is the batch dimension used in cases (like training) when we want to send multiple input sentences to the model at the same time (they’re processed at the same time, which speeds up the process).

But what are these four vectors? Did the tokenizer break the two words into four tokens, or is something else happening here? We can use what we’ve learned about tokenizers to inspect them:

for token in tokens['input_ids'][0]: print(tokenizer.decode(token))

This prints out:

<table><tr><td>[CLs]</td><td></td></tr><tr><td>Hello</td><td></td></tr><tr><td>world</td><td></td></tr><tr><td>[SEP]</td><td></td></tr></table>

This particular tokenizer and model operate by adding the [CLS] and [SEP] tokens to the beginning and end of a string.

Our language model has now processed the text input. The result of its output is the following:

<table><tr><td colspan="2">tensor([[</td></tr><tr><td>[-3.3060，-0.0507，-0.1098，...， -0.1704，-0.1618,0.6932],</td><td></td></tr><tr><td>[0.8918，0.0740，-0.1583，...，0.1869，1.4760,0.0751],</td><td></td></tr><tr><td></td><td>0.0871，0.6364，-0.3050，...，0.4729，-0.1829，1.0157]，</td></tr><tr><td></td><td>[-3.1624，-0.1436，-0.0941， ..., -0.0290，-0.1265，0.7954]</td></tr></table>

This is the raw output of a language model. The applications of large language models build on top of outputs like this.

We recap the input tokenization and resulting outputs of a language model in Fig‐ ure 2-9. Technically, the switch from token IDs into raw embeddings is the first step that occurs inside a language model.

![## Image Analysis: 8b794c0fb2a149a5c15d06dbcd9be1bcdccded8b7de6e0a78921c7ea11196ab6.jpg

**Conceptual Understanding:**
This image conceptually illustrates the pipeline for processing textual data through a language model. Its main purpose is to demonstrate the transformation of raw, human-readable text into a more meaningful and context-aware numerical format suitable for machine learning tasks. The image conveys the key idea that language models enhance basic word representations by integrating contextual information, leading to a richer understanding of the text's meaning beyond individual words.

**Content Interpretation:**
The image depicts a sequential information processing pipeline for natural language. It illustrates the transformation of a raw text input into more semantically rich, contextual embeddings. The process starts with a sentence, which undergoes "Tokenization" to be broken down into individual words or sub-word units. These tokens are then converted into initial "Token embedding vectors"—numeric representations that capture the individual meaning of each token. Subsequently, a "Language model" processes these initial embeddings, incorporating additional context from the surrounding words. This results in the generation of "Contextual token embedding vectors", which are explicitly stated to be "Better token embedding vectors that incorporate more context", highlighting the added value of the language model in understanding the nuances of language.

**Key Insights:**
1. Textual input is initially broken down into discrete units called tokens through a process known as "Tokenization" (e.g., "Have the bards who preceded me left any theme unsung?" becomes "Have", "The", "bards", "who", "preceded").
2. These individual tokens are then converted into "Token embedding vectors", which are "Numeric representations of the tokens capturing their meaning". This is the first step in translating human language into a machine-understandable format while retaining semantic information.
3. A "Language model" plays a crucial role in processing these initial token embeddings by "incorporat[ing] additional context". This means the model considers the relationships and meanings of words within their surrounding text.
4. The output of the language model is "Contextual token embedding vectors", which are superior to basic token embeddings because they "incorporate more context". This indicates that language models produce a richer, more nuanced numerical representation of text that accounts for its surrounding linguistic environment.

**Document Context:**
This image directly illustrates the process described in the document's section "Process the tokens" and specifically supports the accompanying text: "Figure 2-9. A language model operates on raw, static embeddings as its input and produces contextual text embeddings." It provides a clear, step-by-step visual explanation of how a language model takes raw linguistic input, tokenizes it, generates initial embeddings, and then refines these into context-aware embeddings. This helps the reader understand the fundamental input-output mechanism and the internal transformation steps of a language model, serving as a foundational concept for subsequent discussions in the document.

**Summary:**
The image illustrates the conceptual process of transforming a natural language sentence into contextual text embeddings using a language model. The process begins with an input sentence, "Have the bards who preceded me left any theme unsung?". The first stage is "Tokenization", which breaks down the text into smaller pieces, specifically words or parts of words. This results in individual tokens such as "Have", "The", "bards", "who", and "preceded", each represented in separate pink-bordered boxes. Next, these tokens are converted into "Token embedding vectors", which are described as "Numeric representations of the tokens capturing their meaning". These are visually represented by orange segmented boxes, one for each token. These token embedding vectors are then fed into a "Language model", whose function is to "Process the text and incorporate additional context". The final output of the language model is "Contextual token embedding vectors". These are highlighted as "Better token embedding vectors that incorporate more context", signifying that the language model has enriched the initial embeddings with an understanding of the surrounding textual context. These final contextual embeddings are shown as blue segmented boxes.](images/8b794c0fb2a149a5c15d06dbcd9be1bcdccded8b7de6e0a78921c7ea11196ab6.jpg)
Figure 2-9. A language model operates on raw, static embeddings as its input and produces contextual text embeddings.

A visual like this is essential for the next chapter when we start to look at how Transformer-based LLMs work.

# Text Embeddings (for Sentences and Whole Documents)

While token embeddings are key to how LLMs operate, a number of LLM applica‐ tions require operating on entire sentences, paragraphs, or even text documents. This has led to special language models that produce text embeddings—a single vector that represents a piece of text longer than just one token.

We can think of text embedding models as taking a piece of text and ultimately producing a single vector that represents that text and captures its meaning in some useful form. Figure 2-10 shows that process.

![## Image Analysis: 8bb0fa202f350b81897ee9b3edb1d43347d5e496d888f59982069cbb2e517c40.jpg

**Conceptual Understanding:**
This image conceptually represents the process of generating text embeddings from an input text. Its main purpose is to visually explain how a piece of text, like a sentence, is transformed into a numerical vector or sequence of vectors, known as embeddings. The core idea communicated is that an 'Embedding model' takes natural language 'Input' and processes it to 'create embeddings', which are a machine-readable representation of the text's meaning.

**Content Interpretation:**
The image depicts a fundamental process in Natural Language Processing (NLP): the conversion of human-readable text into numerical embeddings. It shows a single input, 'Best movie ever!', being fed into an 'Embedding model'. The model's explicit 'Objective' is to 'create embeddings'. The output is visually represented by a sequence of three red rectangular blocks, followed by an ellipsis, which signifies a numerical or vector representation of the input text. The small icon in the top right of the 'Embedding model' box, showing four arrows fanning out, suggests the creation of a multi-dimensional representation.

**Key Insights:**
The main takeaway is that text embedding models serve as a crucial interface between human language and machine understanding. They transform qualitative, semantic text into quantitative, machine-processable data. The image highlights that the objective of such a model is explicitly to 'create embeddings', which are abstract numerical representations of text. The visual representation of the output as distinct blocks implies that embeddings are typically discrete, possibly vector-based, data structures that capture the meaning of the input text. This process is fundamental for tasks like sentiment analysis, semantic search, and recommendation systems.

**Document Context:**
This image directly illustrates the first step mentioned in the accompanying text: 'In step 1, we use the embedding model to extract the features and convert the input text to embeddings.' It visually grounds the abstract concept of text embeddings within the broader document context of 'Text Embeddings (for Sentences and Whole Documents)'. The diagram serves as a clear, initial explanation of how textual input is processed to yield numerical embeddings, setting the foundation for understanding subsequent concepts discussed in the section.

**Summary:**
This image illustrates the initial step in generating text embeddings, converting a natural language input into a numerical representation. The process begins with an 'Input' text, specifically the phrase 'Best movie ever!'. This input is then processed by an 'Embedding model'. The clear 'Objective' of this model is to 'create embeddings'. The output of this model is depicted as a series of three small red blocks, followed by an ellipsis, signifying a continuous or multi-dimensional numerical output, which represents the generated embeddings. This visual explanation directly supports the concept of transforming textual data into a format suitable for machine learning, as described in the document's section on 'Text Embeddings (for Sentences and Whole Documents)'.](images/8bb0fa202f350b81897ee9b3edb1d43347d5e496d888f59982069cbb2e517c40.jpg)
Figure 2-10. In step 1, we use the embedding model to extract the features and convert the input text to embeddings.

There are multiple ways of producing a text embedding vector. One of the most common ways is to average the values of all the token embeddings produced by the model. Yet high-quality text embedding models tend to be trained specifically for text embedding tasks.

We can produce text embeddings with sentence-transformers, a popular package for leveraging pretrained embedding models.1 The package, like transformers in the previous chapter, can be used to load publicly available models. To illustrate creating embeddings, we use the all-mpnet-base-v2 model. Note that in Chapter 4, we will further explore how you can choose an embedding model for your task.

from sentence_transformers import SentenceTransformer # Load model model $=$ SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# Convert text to text embeddings vector $=$ model.encode("Best movie ever!")

The number of values, or the dimensions, of the embedding vector depends on the underlying embedding model. Let’s explore that for our model:

vector.shape

This sentence is now encoded in this one vector with a dimension of 768 numerical values. In Part II of this book, once we start looking at applications, we’ll start to see the immense usefulness of these text embeddings vectors in powering everything from categorization to semantic search to RAG.

# Word Embeddings Beyond LLMs

Embeddings are useful even outside of text and language generation. Embeddings, or assigning meaningful vector representations to objects, turns out to be useful in many domains, including recommender engines and robotics. In this section, we’ll look at how to use pretrained word2vec embeddings and touch on how the method creates word embeddings. Seeing how word2vec is trained will prime you to learn about contrastive training in Chapter 10. Then in the following section, we’ll see how those embeddings can be used for recommendation systems.

# Using pretrained Word Embeddings

Let’s look at how we can download pretrained word embeddings (like word2vec or GloVe) using the Gensim library:

import gensim.downloader as api

# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)   
# Other options include "word2vec-google-news-300"   
# More options at https://github.com/RaRe-Technologies/gensim-data   
model $=$ api.load("glove-wiki-gigaword-50")

Here, we’ve downloaded the embeddings of a large number of words trained on Wikipedia. We can then explore the embedding space by seeing the nearest neighbors of a specific word, “king” for example:

model.most_similar([model['king']], topn $\scriptstyle 1 = 1 1$ )

This outputs:

[('king', 1.0000001192092896), ('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721), ('uncle', 0.7627150416374207), ('kingdom', 0.7542161345481873), ('throne', 0.7539914846420288), ('brother', 0.7492411136627197), ('ruler', 0.7434253692626953)]

# The Word2vec Algorithm and Contrastive Training

The word2vec algorithm described in the paper “Efficient estimation of word repre‐ sentations in vector space” is described in detail in The Illustrated Word2vec. The central ideas are condensed here as we build on them when discussing one method for creating embeddings for recommendation engines in the following section.

Just like LLMs, word2vec is trained on examples generated from text. Let’s say, for example, we have the text “Thou shalt not make a machine in the likeness of a human mind” from the Dune novels by Frank Herbert. The algorithm uses a sliding window to generate training examples. We can, for example, have a window size two, meaning that we consider two neighbors on each side of a central word.

The embeddings are generated from a classification task. This task is used to train a neural network to predict if words commonly appear in the same context or not (context here means in many sentences in the training dataset we’re modeling). We can think of this as a neural network that takes two words and outputs 1 if they tend to appear in the same context, and 0 if they do not.

In the first position for the sliding window, we can generate four training examples, as we can see in Figure 2-11.

![## Image Analysis: 8367a80f3698c85e4eb390b56bfd2be7c619e868af6d2d5f77896144fc060591.jpg

**Conceptual Understanding:**
This image conceptually represents the initial data preparation phase for natural language processing models, specifically illustrating how raw text is transformed into training examples for algorithms like Word2vec. Its main purpose is to demonstrate the mechanics of using a "sliding window" to define a local context within a sentence, followed by the "tokenization" of words, and the identification of a "center word" and its surrounding "context words." The core message conveyed is the systematic method of extracting structured word-pair data from unstructured text to facilitate machine learning in understanding word relationships and generating embeddings.

**Content Interpretation:**
This image depicts the fundamental preprocessing steps involved in preparing textual data for the Word2vec algorithm. It illustrates two key processes: the application of a "sliding window" to a continuous text and the "tokenization" of that text into individual words, followed by the identification of a "Center word" within a window. The red highlight around "Thou shalt not make a" visually represents the operation of a sliding window, which selects a segment of the original sentence. Below this, the sentence is broken down into discrete "Tokenized words" in separate boxes, demonstrating how text is converted into a sequence of individual linguistic units. The specific labeling of "not" as the "Center word" within the tokenized sequence, alongside its distinct green color, indicates its role as the target word for which embeddings will be learned. The surrounding words ("thou", "shalt", "make", "a"), depicted in shades of red/pink, are context words that provide information about the "Center word." The different shades of red/pink could imply varying distances or weights in relation to the "Center word." This process generates the (center word, context word) pairs necessary for training word embedding models.

**Key Insights:**
The main takeaway is that for algorithms like Word2vec, raw text undergoes a preprocessing step involving a sliding window to identify relevant word relationships. This process generates training examples by pairing a "Center word" with its "context words." The image clearly shows that the continuous text, "Thou shalt not make a machine in the likeness of a human mind," is first considered in segments by a "sliding window" (e.g., "Thou shalt not make a"). Within this window, the text is then broken down into "Tokenized words" (e.g., "thou", "shalt", "not", "make", "a"). From these tokens, a specific word is designated as the "Center word" (explicitly shown as "not"), with the surrounding words serving as its context. The visual differentiation of the center word by color (green for "not") and context words by other colors (shades of red/pink) highlights their distinct roles in the training data generation. This step is essential for contrastive training, where the model learns to predict if two words are "neighbors" based on these generated pairs.

**Document Context:**
This image fits directly into the document's section on "The Word2vec Algorithm and Contrastive Training" by visually explaining the initial data preparation step. As stated in the text after the image, "A sliding window is used to generate training examples for the word2vec algorithm to later predict if two words are neighbors or not." The illustration perfectly clarifies how this "sliding window" mechanism works to define context and center words from raw text, which are then tokenized. This is a crucial conceptual foundation for understanding how Word2vec processes language to learn word relationships and create word embeddings.

**Summary:**
This image illustrates the initial steps of preparing text for the Word2vec algorithm, specifically demonstrating how training examples are generated using a sliding window and tokenization. It begins with a line labeled "Text and sliding window," showing the full sentence "Thou shalt not make a machine in the likeness of a human mind." A red rectangular box, representing the "sliding window," is highlighted over the phrase "Thou shalt not make a." Below this, a section labeled "Tokenized words" shows the same sentence broken down into individual word tokens, each enclosed in a separate rectangular box: "thou," "shalt," "not," "make," "a," "machine," "in," "the," and an ellipsis "...". Below this sequence of tokenized words, the label "Center word" points directly to the box containing the word "not." The tokenized words are color-coded: "thou" and "a" are in a lighter pink/red, "shalt" and "make" are in a darker pink/red, and "not" is in green. The words "machine," "in," "the," and "..." are in white boxes. The overall process shows how a continuous text is segmented by a window, and then tokenized into discrete words, identifying a central word and its surrounding context words for machine learning model training.](images/8367a80f3698c85e4eb390b56bfd2be7c619e868af6d2d5f77896144fc060591.jpg)
Figure 2-11. A sliding window is used to generate training examples for the word2vec algorithm to later predict if two words are neighbors or not.

In each of the produced training examples, the word in the center is used as one input, and each of its neighbors is a distinct second input in each training example. We expect the final trained model to be able to classify this neighbor relationship and output 1 if the two input words it receives are indeed neighbors. These training examples are visualized in Figure 2-12.

![## Image Analysis: e2cf1a30129ea7ba3d5814a312c0e3450e235bc615fdaea69df5ac3daac2d3a8.jpg

**Conceptual Understanding:**
This image represents a set of 'positive' training examples used in the context of word embedding models, likely Word2vec, which employ contrastive training. Conceptually, it illustrates how pairs of words that are genuine neighbors in a text corpus are presented to a model. The main purpose is to show the data format for 'positive' samples, where a target value of '1' explicitly indicates that the two words in a pair are indeed contextually related or immediate neighbors, thereby serving as ground truth for the model to learn from.

**Content Interpretation:**
The image illustrates the structure of positive training examples for a word embedding algorithm, specifically for contrastive training as mentioned in the document context. It shows pairs of words, where the first word is consistently 'Not' and the second word varies ('thou', 'shalt', 'make', 'a'). The 'Target' value of '1' for all entries signifies that these are 'positive' examples, meaning these word pairs are considered to be actual neighbors or contextually related within the training data. This setup is crucial for models that learn word relationships by distinguishing between real neighboring words and non-neighboring words.

**Key Insights:**
The main takeaway is the explicit structure of positive training examples for word embedding models. It shows that: 1. Training examples are structured as word pairs. 2. A 'Target' value of '1' denotes a positive example, meaning the words in the pair are considered neighbors. 3. These positive examples are generated from the corpus, representing actual co-occurrences of words. This setup is fundamental for algorithms that aim to learn word representations by maximizing the similarity of positive pairs and minimizing the similarity of negative pairs (which are not shown in this specific image but are implied by the 'Target' column and the surrounding text). The consistency of 'Not' as 'Word 1' suggests it is a focus word for which its context words are being identified.

**Document Context:**
The image directly supports the section "The Word2vec Algorithm and Contrastive Training" by providing a concrete visual example of what a 'positive' training example looks like. It clarifies the statement "Each generated training example shows a pair of neighboring words" from the text after the image, demonstrating how 'Word 1' and 'Word 2' form such a pair and how the 'Target' value of '1' labels them as true neighbors. This table is a foundational element in understanding how Word2vec, particularly in a contrastive learning setting, is trained to identify co-occurring words.

**Summary:**
This image displays a table labeled "Training examples" which contains four rows of data, each representing a positive training example for a word embedding model like Word2vec. The table has three columns: "Word 1", "Word 2", and "Target". The first column, "Word 1", consistently shows the word "Not" across all rows. The second column, "Word 2", lists various words that are presented as neighbors to "Not" in these training examples: "thou", "shalt", "make", and "a". The third column, "Target", uniformly shows the value "1" for all rows, indicating that these pairs are considered positive examples (i.e., the two words are actual neighbors or contextually related in the source text). The first column cells are colored light green, while the second column cells are light red, and the third column cells have a light blue '1' on a white background, adding visual distinction to the data.](images/e2cf1a30129ea7ba3d5814a312c0e3450e235bc615fdaea69df5ac3daac2d3a8.jpg)
Figure 2-12. Each generated training example shows a pair of neighboring words.   
Figure 2-13. We need to present our models with negative examples: words that are not usually neighbors. A better model is able to better distinguish between the positive and negative examples.

If, however, we have a dataset of only a target value of 1, then a model can cheat and ace it by outputting 1 all the time. To get around this, we need to enrich our training dataset with examples of words that are not typically neighbors. These are called negative examples and are shown in Figure 2-13.

<table><tr><td>Word 1</td><td>Word 2</td><td>Target</td><td rowspan="6">Positive examples</td></tr><tr><td>not</td><td>thou</td><td>1</td></tr><tr><td>not</td><td>shalt</td><td>1</td></tr><tr><td>not</td><td>make</td><td>1</td></tr><tr><td>not</td><td>a</td><td>1</td></tr><tr><td></td><td>apothecary</td><td>0</td></tr><tr><td>thou</td><td>sublime</td><td>0</td></tr><tr><td>not make</td><td>def</td><td>0</td></tr><tr><td>a</td><td>playback</td><td>0</td></tr></table>

It turns out that we don’t have to be too scientific in how we choose the negative examples. A lot of useful models result from the simple ability to detect positive examples from randomly generated examples (inspired by an important idea called noise-contrastive estimation and described in “Noise-contrastive estimation: A new estimation principle for unnormalized statistical models”). So in this case, we get random words and add them to the dataset and indicate that they are not neighbors (and thus the model should output 0 when it sees them).

With this, we’ve seen two of the main concepts of word2vec (Figure 2-14): skip-gram, the method of selecting neighboring words, and negative sampling, adding negative examples by random sampling from the dataset.

![## Image Analysis: 95bf00bf86c65b9710d203f02ff0cf09627c35f0de581920d1338a0dc17681f5.jpg

**Conceptual Understanding:**
This image conceptually represents the data generation process for the Skip-gram model with Negative Sampling, fundamental to the word2vec algorithm. The main purpose is to illustrate how positive and negative training examples are extracted or constructed from a sequence of text (a sentence) to train word embeddings. The 'Skip-gram' part focuses on generating positive (input, context) word pairs, while the 'Negative sampling' part shows how to create a balanced dataset of positive and negative examples for a given input word, which is essential for efficient model training.

**Content Interpretation:**
The image illustrates two fundamental data generation processes for training word embedding models like word2vec: Skip-gram and Negative Sampling. The 'Skip-gram' table (with headers 'input' and 'output') demonstrates how an input word (e.g., 'make') is paired with its surrounding context words (e.g., 'shalt', 'not', 'a', 'machine') from the sentence 'shalt not make a machine'. This process generates positive word pairs used for training. The 'Negative sampling' table (with headers 'Input word', 'Output word', 'Target') further elaborates on how training data is structured. For a given 'Input word' ('make'), a true 'Output word' ('shalt') is assigned a 'Target' of '1' (positive example), while randomly chosen irrelevant words ('aaron', 'taco') are designated as 'Output word's with a 'Target' of '0' (negative examples). This combination of positive and negative examples is crucial for efficient and effective training of word embeddings, allowing the model to distinguish between words that are likely to appear together and those that are not.

**Key Insights:**
The main takeaways from this image are: 1. **Skip-gram generates positive word pairs:** The 'Skip-gram' section shows how a target word ('make') is paired with its context words ('shalt', 'not', 'a', 'machine') to form positive training examples. This is evidenced by the 'input' 'make' being paired with each 'output' context word. 2. **Negative sampling creates both positive and negative training examples:** The 'Negative sampling' table clearly shows how for an 'Input word' ('make'), a genuine 'Output word' ('shalt') is labeled with a 'Target' of '1', while non-contextual words ('aaron', 'taco') are sampled as 'Output word's and labeled with a 'Target' of '0'. This demonstrates the contrastive aspect of training, where the model learns to differentiate between relevant and irrelevant word pairs. 3. **The 'Target' column indicates the validity of a word pair:** A 'Target' of '1' signifies a true contextual relationship, and '0' signifies a non-contextual or negative relationship, which is critical for the loss function in training word embedding models.

**Document Context:**
This image directly supports the document's section on "The Word2vec Algorithm and Contrastive Training" by visually explaining two core components: Skip-gram and Negative sampling. The text after the image, "Figure 2-14. Skip-gram and negative sampling are two of the main ideas behind the word2vec algorithm and are useful in many other problems that can be formulated as token sequence problems," reinforces the image's role in illustrating these fundamental concepts. It provides a concrete example of how training data is prepared for word embedding models, which is essential for understanding how word2vec learns word representations.

**Summary:**
The image displays two key components of the word2vec algorithm: Skip-gram and Negative sampling, presented as data transformation or generation examples. The 'Skip-gram' section illustrates how word pairs are generated from a given sentence. It begins with the example sentence represented as a sequence of words: "shalt | not | make | a | machine". Below this, a table is provided with two columns, 'input' and 'output'. The 'input' column consistently shows the word "make", while the 'output' column lists the words "shalt", "not", "a", and "machine", indicating the context words around "make" that are paired with it. The 'Negative sampling' section, presented alongside, shows a different table with three columns: 'Input word', 'Output word', and 'Target'. This table demonstrates how positive and negative training examples are created. For the 'Input word' "make", the 'Output word' "shalt" is paired with a 'Target' of '1', signifying a true or positive relationship (as derived from the Skip-gram example). Conversely, 'Output word' "aaron" and 'Output word' "taco" are both paired with a 'Target' of '0', indicating false or negative relationships, meaning these words are not in the context of "make" for this specific example. The overall visual content illustrates the fundamental data generation processes for training word embeddings using the Skip-gram model with Negative sampling.](images/95bf00bf86c65b9710d203f02ff0cf09627c35f0de581920d1338a0dc17681f5.jpg)
Figure 2-14. Skip-gram and negative sampling are two of the main ideas behind the word2vec algorithm and are useful in many other problems that can be formulated as token sequence problems.

We can generate millions and even billions of training examples like this from running text. Before proceeding to train a neural network on this dataset, we need to make a couple of tokenization decisions, which, just like we’ve seen with LLM tokenizers, include how to deal with capitalization and punctuation and how many tokens we want in our vocabulary.

We then create an embedding vector for each token, and randomly initialize them, as can be seen in Figure 2-15. In practice, this is a matrix of dimensions vocab_size x embedding_dimensions.

![## Image Analysis: 05a46f18c7828bc51940761de5de7988d40d8feafececf2c32b01f25e1ebe256.jpg

**Conceptual Understanding:**
This image conceptually represents the initial state of word embeddings within a natural language processing system. It illustrates a small vocabulary of words (tokens) and their corresponding "token embeddings," which are shown as uninitialized or randomly assigned vector spaces. The main purpose is to demonstrate the one-to-one correspondence between a distinct word and its unique, albeit currently untrained, numerical representation (vector) before any machine learning algorithm, such as Word2vec, begins to learn semantic meaning for these embeddings.

**Content Interpretation:**
The image displays a structured list, effectively a table, with two primary columns. The first column, explicitly labeled "Token," contains a list of nine distinct English words: "thou," "shalt," "make," "a," "not," "apothecary," "sublime," "def," and "playback." These words represent individual units of text, or tokens, in a defined vocabulary. The second column, labeled "Token embedding," visually correlates with each word in the "Token" column. For each listed word, this column presents a set of three small, blank, connected squares. The significance of this representation is that it visually conveys the concept of a vector (a sequence of numerical values) being associated with each word. The "blank" nature of the squares, supported by the external document context describing "starting, random, uninitialized embedding vectors," indicates that these are initial, arbitrary numerical assignments for the words. There are no learned relationships or semantic meanings encoded in these embeddings yet. The visual representation suggests that each embedding is a vector of a specific, fixed dimension (here, conceptually represented by three dimensions). This demonstrates the fundamental input structure for word embedding models, where each unique word is mapped to a unique vector placeholder that will later be optimized during a training process.

**Key Insights:**
The main takeaway from this image is that in natural language processing, words (tokens) are represented as "Token embedding" vectors, and these vectors have an initial state, which is often "uninitialized" or random before any learning occurs. The explicit text "Token" and "Token embedding" clearly define these two core components. The list of words ("thou", "shalt", "make", "a", "not", "apothecary", "sublime", "def", "playback") provides concrete examples of linguistic tokens that are subject to this embedding process. The visual representation of three blank squares for each embedding, combined with the surrounding document context, strongly implies that these are placeholders for numerical values that are yet to be determined through training. This provides insight into the foundational data structure used in word embedding models, highlighting that words are converted into a machine-readable numerical format for computational processing.

**Document Context:**
This image is highly relevant within the "The Word2vec Algorithm and Contrastive Training" section of the document, as indicated by the accompanying text: "Figure 2-15. A vocabulary of words and their starting, random, uninitialized embedding vectors." It serves as a visual illustration of the very first conceptual step in preparing data for word embedding models like Word2vec, by showing how a vocabulary of words is assigned initial, arbitrary numerical representations before the learning process begins.

**Summary:**
This image presents a straightforward, two-column table structure illustrating the initial state of word embeddings. The left column, clearly labeled "Token", lists a vocabulary of nine distinct words: "thou", "shalt", "make", "a", "not", "apothecary", "sublime", "def", and "playback". Each word represents a unique linguistic unit. The right column, labeled "Token embedding", corresponds directly to each word in the left column. For every token, this column displays a visual representation of three small, blank, interconnected square shapes. These blank squares symbolize an uninitialized or randomly assigned numerical vector, which is the embedding for that specific word. The empty nature of these visual representations signifies that no semantic meaning has yet been learned or encoded into these vectors. Conceptually, this image demonstrates the fundamental step in natural language processing where individual words are mapped to a unique, but initially arbitrary, numerical vector space before any training algorithm, such as Word2vec, begins to learn and assign meaningful values to these embeddings based on their contextual usage.](images/05a46f18c7828bc51940761de5de7988d40d8feafececf2c32b01f25e1ebe256.jpg)
Figure 2-15. A vocabulary of words and their starting, random, uninitialized embedding vectors.

A model is then trained on each example to take in two embedding vectors and predict if they’re related or not. We can see what this looks like in Figure 2-16.

![## Image Analysis: bfd26649c94f256a10262b431f5e0281610cc03618b0989e32b2d0a9e88a11c0.jpg

**Conceptual Understanding:**
This image conceptually illustrates the forward pass of a neural network in the context of natural language processing, specifically for a word relationship prediction task. The main purpose is to demonstrate how individual words, or 'tokens,' are represented numerically as 'embeddings,' processed by a 'neural network' to perform a specific 'task' (determining if two words are 'neighbors'), and ultimately yield a 'model prediction' as an output. It communicates the key ideas of word tokenization, vector embeddings, neural network processing, and probabilistic prediction for semantic relationships between words.

**Content Interpretation:**
The image shows a process where two word tokens, "not" and "thou," are first converted into their respective "Embeddings." These embeddings are then input into a "Neural network." The task of this neural network is specified as determining: "are the two words neighbors." The output of this neural network is a "Model prediction" which is given as a numerical value of "0.90." The overall system illustrates how a neural network processes word inputs to make a prediction about their semantic or contextual relationship, specifically focusing on whether they can be considered 'neighbors'. The numerical prediction of 0.90 suggests a high probability or strong association between the words 'not' and 'thou' according to the model.

**Key Insights:**
The main takeaways from this image are: 1. Words (Tokens) are transformed into numerical vector representations called Embeddings for computational processing, as shown by "not" and "thou" leading to their respective "Embeddings." 2. Neural networks are employed to learn and predict relationships between words, with the "Neural network" explicitly tasked to determine "are the two words neighbors." 3. The output of such a predictive model is a numerical score or probability (the "Model prediction" of "0.90"), which quantifies the likelihood of the specified relationship. 4. The process represents a fundamental step in training word embedding models like Word2vec, where the network's ability to correctly classify word relationships helps refine the quality of the embeddings themselves.

**Document Context:**
This image directly supports the document's discussion on "The Word2vec Algorithm and Contrastive Training." The accompanying text, "Figure 2-16. A neural network is trained to predict if two words are neighbors. It updates the embeddings in the training process to produce the final, trained embeddings," explicitly describes the function depicted. The diagram visually explains the core mechanism of how a neural network processes word inputs (tokens), converts them to numerical representations (embeddings), and then performs a classification task (predicting if they are neighbors) to learn and refine those embeddings. It exemplifies the practical application of contrastive training where the model distinguishes between related and unrelated word pairs.

**Summary:**
The image illustrates a simplified neural network model designed to predict if two given words are "neighbors." The process begins with two input "Tokens": the words "not" and "thou," each encased in a rounded rectangle. These tokens are then transformed into their respective "Embeddings," which are numerical vector representations of the words' meanings, visually depicted as abstract sets of three gray squares. These two sets of embeddings are fed as input into a central processing unit labeled "Neural network." The explicit "Task" for this neural network is stated as "are the two words neighbors." After processing, the neural network outputs a "Model prediction," which is a numerical value of "0.90," displayed in red. This value represents the model's confidence or probability that the two input words, "not" and "thou," are indeed considered neighbors. This diagram visually outlines the forward pass of such a network, showcasing the flow from raw word inputs to their numerical representations, through a processing model, and finally to a predictive output regarding their relationship.](images/bfd26649c94f256a10262b431f5e0281610cc03618b0989e32b2d0a9e88a11c0.jpg)
Figure 2-16. A neural network is trained to predict if two words are neighbors. It updates the embeddings in the training process to produce the final, trained embeddings.

Based on whether its prediction was correct or not, the typical machine learning training step updates the embeddings so that the next time the model is presented with those two vectors, it has a better chance of being more correct. And by the end of the training process, we have better embeddings for all the tokens in our vocabulary.

This idea of a model that takes two vectors and predicts if they have a certain relation is one of the most powerful ideas in machine learning, and time after time has proven to work very well with language models. This is why we’re dedicating Chapter 10 to this concept and how it optimizes language models for specific tasks (like sentence embeddings and retrieval).

The same idea is also central to bridging modalities like text and images, which is key to AI image generation models, as we’ll see in Chapter 9 on multimodal models. In that formulation, a model is presented with an image and a caption, and it should predict whether that caption describes the image or not.

# Embeddings for Recommendation Systems

As we’ve mentioned, the concept of embeddings is useful in so many other domains.   
In industry, it’s widely used for recommendation systems, for example.

# Recommending Songs by Embeddings

In this section we’ll use the word2vec algorithm to embed songs using human-made music playlists. Imagine if we treated each song as we would a word or token, and we treated each playlist like a sentence. These embeddings can then be used to recommend similar songs that often appear together in playlists.

The dataset we’ll use was collected by Shuo Chen from Cornell University. It contains playlists from hundreds of radio stations around the US. Figure 2-17 demonstrates this dataset.

![## Image Analysis: a43c14d4c7692996fc743ff255aadaef1a31a36b94342bebc63a568596a9ce57.jpg

**Conceptual Understanding:**
The image conceptually represents a simplified dataset of music playlists, illustrating how individual songs are grouped within these collections. Its main purpose is to demonstrate the basic structure of input data (playlists containing songs) that would be utilized in a song recommendation system, particularly one that uses song embeddings to capture similarity. The key ideas communicated are that playlists are fundamental units of song collection, and songs can belong to multiple playlists, implying potential similarity or shared context that is crucial for building recommendation models based on co-occurrence.

**Content Interpretation:**
The image illustrates a data structure foundational for building a collaborative filtering or content-based recommendation system for music. It shows the grouping of individual songs into distinct playlists. The labels "Playlist #1:", "Playlist #2:", "Playlist #3:" signify distinct collections of songs. The labels "Song 1", "Song 13", "Song 2", "Song 400", "Song 81", "Song 82", "Song 77" represent individual musical tracks, with numbers suggesting unique identifiers. The most significant aspect is the presence of "Song 13" and "Song 2" in all three playlists, indicating a strong co-occurrence. This co-occurrence suggests a strong relationship or similarity between these songs that could be leveraged by embedding models. The varying lengths of the playlists (four songs in Playlist #1, five in Playlist #2, two in Playlist #3) demonstrate typical diversity in playlist creation.

**Key Insights:**
The main takeaways from this image are that playlist data serves as a valuable source for understanding song relationships, and songs that frequently co-occur in multiple playlists are likely to be similar or complementary. This co-occurrence is a key signal for learning effective song embeddings. The diversity in playlist length and content also provides rich data for training recommendation models. For instance, the explicit commonality of "Song 13" and "Song 2" across "Playlist #1", "Playlist #2", and "Playlist #3" highlights how implicit connections can be derived from user behavior (playlist creation), providing direct evidence for their perceived similarity.

**Document Context:**
This image directly supports the "Recommending Songs by Embeddings" section of the document. The accompanying text, "Figure 2-17. For song embeddings that capture song similarity we’ll use a dataset made up of a collection of playlists, each containing a list of songs," is perfectly exemplified by this visual. The image provides a concrete example of the dataset structure being discussed, showing a 'collection of playlists' (Playlist #1, #2, #3) with 'each containing a list of songs' (e.g., Song 1, Song 13, Song 2, Song 400 for Playlist #1). It serves as a visual aid to understand the input data for the embedding process.

**Summary:**
The image presents a simplified illustration of a dataset comprising three distinct playlists, each populated with a unique collection of songs. This structure is intended to serve as input for systems that recommend songs by understanding their similarity through embeddings. The dataset is organized as follows: Playlist #1 contains four songs: "Song 1", "Song 13", "Song 2", and "Song 400". Playlist #2 contains five songs: "Song 2", "Song 81", "Song 13", "Song 82", and "Song 77". Playlist #3 contains two songs: "Song 13" and "Song 2". A key observation from this data is the co-occurrence of certain songs across multiple playlists. Specifically, "Song 13" and "Song 2" are present in all three displayed playlists. This repetition indicates a potential strong relationship or perceived similarity between these songs, a crucial signal that song embedding models would leverage to learn meaningful representations. For instance, if users frequently include "Song 13" and "Song 2" in their diverse playlists, it suggests these songs share a common attribute or appeal to similar listening contexts. The varying lengths of the playlists also demonstrate the natural variety found in real-world music consumption data. This visual representation concretely exemplifies the "collection of playlists, each containing a list of songs" that the accompanying text describes as the foundation for learning song embeddings to capture similarity.](images/a43c14d4c7692996fc743ff255aadaef1a31a36b94342bebc63a568596a9ce57.jpg)
Figure 2-17. For song embeddings that capture song similarity we’ll use a dataset made up of a collection of playlists, each containing a list of songs.

Let’s demonstrate the end product before we look at how it’s built. So let’s give it a few songs and see what it recommends in response.

Let’s start by giving it Michael Jackson’s “Billie Jean,” the song with ID 3822:

# We will define and explore this function in detail below print_recommendations(3822)

<table><tr><td>id Title</td><td>artist</td></tr><tr><td>4181 Kiss</td><td>Prince &amp; The Revolution</td></tr><tr><td>12749 Wanna Be Startin&#x27; Somethin&#x27;</td><td>Michael Jackson</td></tr><tr><td>1506</td><td>The Way You Make Me Feel Michael Jackson</td></tr><tr><td>3396 Holiday</td><td>Madonna</td></tr><tr><td>500</td><td>Don&#x27;t Stop&#x27;Til You Get Enough Michael Jackson</td></tr></table>

That looks reasonable. Madonna, Prince, and other Michael Jackson songs are the nearest neighbors.

Let’s step away from pop and into rap, and see the neighbors of 2Pac’s “California Love”:

print_recommendations(842)

<table><tr><td>id Title</td><td>artist</td></tr><tr><td>413 IfIRuledthe World (ImagineThat)(wVLauryHill)</td><td>Nas</td></tr><tr><td>196 I&#x27;ll Be Missing You</td><td> Puff Daddy &amp; The Family</td></tr><tr><td>330 Hate It or Love It (wV 50 Cent)</td><td>The Game</td></tr><tr><td>211 Hypnotize</td><td>The Notorious B.I.G.</td></tr><tr><td>5788 Drop It Like It&#x27;s Hot (wV/ Pharrell)</td><td>Snoop Dogg</td></tr></table>

Another quite reasonable list! Now that we know it works, let’s see how to build such a system.

# Training a Song Embedding Model

We’ll start by loading the dataset containing the song playlists as well as each song’s metadata, such as its title and artist:

import pandas as pd from urllib import request

# Get the playlist dataset file   
data $=$ request.urlopen('https://storage.googleapis.com/maps-premium/data   
set/yes_complete/train.txt')

# Parse the playlist dataset file. Skip the first two lines as # they only contain metadata lines $=$ data.read().decode("utf-8").split('\n')[2:]

# Remove playlists with only one song playlists $=$ [s.rstrip().split() for s in lines if len(s.split()) > 1]

# Load song metadata   
songs_file $=$ request.urlopen('https://storage.googleapis.com/maps-premium/data   
set/yes_complete/song_hash.txt')   
songs_file $=$ songs_file.read().decode("utf-8").split( $" \mathrm { \Delta } \mathrm { \backslash n " }$ )   
songs $=$ [s.rstrip().split('\t') for s in songs_file]   
songs_df $=$ pd.DataFrame(data $=$ songs, columns $=$ ['id', 'title', 'artist'])   
songs_df $=$ songs_df.set_index('id')

Now that we’ve saved them, let’s inspect the playlists list. Each element inside it is a playlist containing a list of song IDs:

print( 'Playlist #1:\n ', playlists[0], '\n') print( 'Playlist #2:\n ', playlists[1])

Playlist #1: Playlist #2: $\begin{array} { r l } & { [ { ^ { \dag } \Theta ^ { \dag } } , { ^ { \dag } \Theta ^ { \dag } } , { ^ { \dag } \Theta ^ { \dag } } , { ^ { \dag } 2 ^ { \dag } } , { ^ { \dag } 3 ^ { \dag } } , { ^ { \dag } 4 ^ { \dag } } , { ^ { \dag } 5 ^ { \dag } } , { ^ { \dag } \dots } , { ^ { \dag } 4 3 ^ { \dag } } ] } \\ & { [ { ^ { \dag } 7 8 ^ { \dag } } , { ^ { \dag } 7 9 ^ { \dag } } , { ^ { \dag } 8 \Theta ^ { \dag } } , { ^ { \dag } 3 ^ { \dag } } , { ^ { \dag } } , { ^ { \dag } 6 2 ^ { \dag } } , { ^ { \dag } \dots } , { ^ { \dag } 2 1 \Theta ^ { \dag } } ] } \end{array}$

Let’s train the model:

from gensim.models import Word2Vec   
# Train our Word2Vec model   
model $=$ Word2Vec( playlists, vector_size $= 3 2$ , windo $\scriptstyle 1 = 2 \Theta$ , negative $\scriptstyle \mathbf { \varepsilon = } 5 \Theta$ , min_count $\mathbf { \Psi } = \mathbf { \Psi }$ , workers=4   
)

That takes a minute or two to train and results in embeddings being calculated for each song that we have. Now we can use those embeddings to find similar songs exactly as we did earlier with words:

song_id $=$ 2172

# Ask the model for songs similar to song #2172 model.wv.most_similar(positive $\mathrel { \mathop : } =$ str(song_id))

This outputs:

[('2976', 0.9977465271949768), ('3167', 0.9977430701255798), ('3094', 0.9975950717926025), ('2640', 0.9966474175453186), ('2849', 0.9963167905807495)]

That is the list of the songs whose embeddings are most similar to song 2172. In this case, the song is:

print(songs_df.iloc[2172])

title Fade To Black artist Metallica Name: 2172 , dtype: object

This results in recommendations that are all in the same heavy metal and hard rock genre:

import numpy as np   
def print_recommendations(song_id): similar_songs $=$ np.array( model.wv.most_similar(positive $\iota =$ str(song_id),topn $\scriptstyle 1 = \atop \left. \begin{array} { r l } \end{array} \right.$ ) )[:,0] return songs_df.iloc[similar_songs]

# Extract recommendations print_recommendations(2172)

<table><tr><td>id</td><td>Title</td><td>artist</td></tr><tr><td>11473</td><td>Little Guitars</td><td>Van Halen</td></tr><tr><td>3167</td><td> Unchained</td><td>Van Halen</td></tr><tr><td>5586</td><td> The Last in Line</td><td>Dio</td></tr><tr><td>5634</td><td>Mr. Brownstone</td><td> Guns N&#x27; Roses</td></tr><tr><td>3094</td><td> Breaking the Law</td><td> Judas Priest</td></tr></table>

# Summary

In this chapter, we have covered LLM tokens, tokenizers, and useful approaches to using token embeddings. This prepares us to start looking closer at language models in the next chapter, and also opens the door to learn about how embeddings are used beyond language models.

We explored how tokenizers are the first step in processing input to an LLM, trans‐ forming raw textual input into token IDs. Common tokenization schemes include breaking text down into words, subword tokens, characters, or bytes, depending on the specific requirements of a given application.

A tour of real-world pretrained tokenizers (from BERT to GPT-2, GPT-4, and other models) showed us areas where some tokenizers are better (e.g., preserving informa‐ tion like capitalization, newlines, or tokens in other languages) and other areas where tokenizers are just different from each other (e.g., how they break down certain words).

Three of the major tokenizer design decisions are the tokenizer algorithm (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on.

Language models are also creators of high-quality contextualized token embeddings that improve on raw static embeddings. Those contextualized token embeddings are what’s used for tasks including named-entity recognition (NER), extractive text summarization, and text classification. In addition to producing token embeddings, language models can produce text embeddings that cover entire sentences or even documents. This empowers plenty of applications that will be shown in Part II of this book covering language model applications

Before LLMs, word embedding methods like word2vec, GloVe, and fastText were popular. In language processing, this has largely been replaced with contextualized word embeddings produced by language models. The word2vec algorithm relies on two main ideas: skip-gram and negative sampling. It also uses contrastive training similar to the type we’ll see in Chapter 10.

Embeddings are useful for creating and improving recommender systems as we discussed in the music recommender we built from curated song playlists.

In the next chapter, we will take a deep dive into the process after tokenization: how does an LLM process these tokens and generate text? We will look at some of the main intuitions of how LLMs that use the Transformer architecture work.

# Looking Inside Large Language Models

Now that we have a sense of tokenization and embeddings, we’re ready to dive deeper into the language model and see how it works. In this chapter, we’ll look at some of the main intuitions of how Transformer language models work. Our focus will be on text generation models so we get a deeper sense for generative LLMs in particular.

We’ll be looking at both the concepts and some code examples that demonstrate them. Let’s start by loading a language model and getting it ready for generation by declaring a pipeline. In your first read, feel free to skip the code and focus on grasping the concepts involved. Then in a second read, the code will get you to start applying these concepts.

import torch   
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline   
# Load model and tokenizer   
tokenizer $=$ AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")   
model $=$ AutoModelForCausalLM.from_pretrained( "microsoft/Phi-3-mini-4k-instruct", device_map="cuda", torch_dtype $=$ "auto", trust_remote_code=True,   
)   
# Create a pipeline   
generator $=$ pipeline( "text-generation", model=model, tokenizer $=$ tokenizer, return_full_text $=$ False, max_new_tokens $= 5 \Theta$ , do_sample=False,   
)

# An Overview of Transformer Models

Let’s begin our exploration with a high-level overview of the model, and then we’ll see how later work has improved upon the Transformer model since its introduction in 2017.

# The Inputs and Outputs of a Trained Transformer LLM

The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response. Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs. Figure 3-1 shows one such model used to author an email.

![## Image Analysis: a219dbef7dda246e0c9e56e71849cd59e84ca1a98e1975a40e42623d3dbe0562.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental operational flow of a Transformer Large Language Model (LLM). Its main purpose is to clearly illustrate the process by which an LLM takes a textual input, known as a 'Prompt,' and generates a corresponding textual output, referred to as 'Generation.' The key ideas being communicated are the direct relationship between a user's instruction and the AI's response, emphasizing the role of the LLM as a text-to-text transformation engine. It simplifies the complex internal workings of the LLM into a straightforward, understandable input-process-output model.

**Content Interpretation:**
This image illustrates the core function of a Transformer Large Language Model (LLM): taking a text prompt as input and producing generated text as output. The process involves a user providing a specific instruction or scenario (the prompt) to the LLM, which then processes this input to create relevant and coherent text (the generation). The text extracted from the image explicitly shows this relationship:

*   **Input Prompt:** The text 'Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.' specifies a clear task and context for the LLM.
*   **Processing Unit:** The 'Transformer LLM' box, featuring a flame/spark and speech bubble icon, represents the artificial intelligence model responsible for natural language processing and generation.
*   **Output Generation:** The text 'Dear Sarah,
I'm writing to apologize for the incident last week. [...]' demonstrates the LLM's capability to generate a coherent email response, starting the apology as requested, and hinting at further explanatory content (indicated by '[...]').

This demonstrates the system's ability to understand the intent of the prompt (apology, explanation) and produce text that aligns with that intent.

**Key Insights:**
The main takeaway from this image is a clear understanding of the fundamental input-output mechanism of a Transformer LLM. It teaches that these models are designed to:

1.  **Receive specific instructions or questions (prompts):** Evidenced by the prompt 'Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.'
2.  **Process these prompts using advanced AI capabilities:** Represented by the 'Transformer LLM' box and its associated icons, signifying the computational and generative power.
3.  **Produce relevant, contextually appropriate text as an output:** Demonstrated by the generated email draft 'Dear Sarah,
I'm writing to apologize for the incident last week. [...]'

This simple flow highlights the LLM's ability to interpret human language and generate creative, functional text based on the provided input, illustrating its core utility in natural language generation tasks.

**Document Context:**
This image serves as a foundational visual aid for the document's section titled 'The Inputs and Outputs of a Trained Transformer LLM'. It directly supports and elaborates on the concept mentioned in the subsequent text: 'Figure 3-1. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text.' The diagram visually breaks down this high-level abstraction into concrete examples of a prompt and the corresponding generated output, making the explanation of LLM functionality more tangible and easier to grasp for the reader. It sets the stage for understanding the basic operational flow before delving into more complex aspects of Transformer LLMs.

**Summary:**
This image depicts a straightforward, high-level abstraction of how a Transformer Large Language Model (LLM) processes a text prompt to generate text. It illustrates the input-output relationship, starting with a user-provided prompt, passing it through the LLM, and concluding with the generated textual output. The process begins with a 'Prompt', which is a specific instruction or request given to the LLM. This prompt is then fed into the 'Transformer LLM', which is represented by a pink rounded rectangle containing its name along with a flame/spark icon and a speech bubble icon, symbolizing its generative and conversational capabilities. Following the processing by the LLM, the output is labeled 'Generation', which is the text produced by the model in response to the initial prompt. The example shows a practical application of an LLM for text generation.](images/a219dbef7dda246e0c9e56e71849cd59e84ca1a98e1975a40e42623d3dbe0562.jpg)
Figure 3-1. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text.

The model does not generate the text all in one operation; it actually generates one token at a time. Figure 3-2 shows four steps of token generation in response to the input prompt. Each token generation step is one forward pass through the model (that’s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph).

![## Image Analysis: 2d163b099067903081df6e2c0ec5859c449d3c80abed783e4ea74eeeec53ec9b.jpg

**Conceptual Understanding:**
The image conceptually represents the input-output mechanism and the generation process of a Transformer Large Language Model (LLM). Its main purpose is to demonstrate that LLMs generate text in an iterative fashion, producing one token (e.g., a word, a punctuation mark, or a special character) at a time, building up the complete output sequentially from an initial prompt.

**Content Interpretation:**
This image illustrates the operational mechanism of a Transformer LLM during text generation. It visually represents the process where a textual prompt serves as input, and the LLM iteratively produces output text one 'token' at a time. The core concept shown is the sequential nature of LLM output, emphasizing that each word, punctuation mark, or special character (like a newline) is generated as a discrete step, building the complete response over time. This highlights the auto-regressive property commonly found in such models.

**Key Insights:**
The main takeaway from this image is that Transformer LLMs operate by generating text in a discrete, sequential manner, one token at a time, rather than producing an entire text block simultaneously. This iterative process is initiated by a given prompt. The evidence for this insight includes the explicit numbering of generated tokens (#1, #2, #3, #4) and the presentation of each output component ('Dear', 'Sarah', ',', '\n') within separate, sequential boxes, clearly demonstrating the step-by-step nature of the generation process.

**Document Context:**
This image directly supports the document's section titled "The Inputs and Outputs of a Trained Transformer LLM" and specifically elaborates on the concept introduced in "Figure 3-2. Transformer LLMs generate one token at a time, not the entire text at once." It provides a visual explanation of how a prompt is processed and how the output text is constructed sequentially, token by token, thereby clarifying a fundamental aspect of LLM operation.

**Summary:**
The diagram illustrates the sequential, token-by-token text generation process of a Transformer Large Language Model (LLM) in response to a given prompt. It begins with a textual prompt, which is fed into the Transformer LLM. The LLM then generates the output text one token at a time, with each token representing a distinct step in the generation sequence. For instance, in this example, the LLM first generates the token "Dear" (labeled #1), then "Sarah" (labeled #2), followed by "," (labeled #3), and finally "\n" (labeled #4), which is explicitly clarified as a "<newline>" character. This step-by-step generation highlights that LLMs do not produce the entire text at once, but rather build it incrementally.](images/2d163b099067903081df6e2c0ec5859c449d3c80abed783e4ea74eeeec53ec9b.jpg)
Figure 3-2. Transformer LLMs generate one token at a time, not the entire text at once.

After each token generation, we tweak the input prompt for the next generation step by appending the output token to the end of the input prompt. We can see this in Figure 3-3.

![## Image Analysis: e2c555804ce8bb4a1c7f5d7d7d7fd2980bcbcdfbf0f7a8614c2dd8379a8ad6c7.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental process of how a Transformer-based Large Language Model (LLM) generates text. The main purpose is to demonstrate the iterative and sequential nature of text generation, where the model produces one output token at a time, and each new token is then used as an additional input context for generating the subsequent token. It visually explains the auto-regressive property of these models in generating continuous text based on a given prompt.

**Content Interpretation:**
The image depicts the auto-regressive nature of text generation by a Transformer Large Language Model (LLM). It shows how an LLM processes an input prompt and generates output text token by token, where each newly generated token is appended to the input sequence for the generation of the next token. This iterative process is crucial for producing coherent and contextually relevant text sequences. The flame icon associated with 'Transformer LLM' likely symbolizes the power or advanced capability of the model.

**Key Insights:**
1. Transformer LLMs generate text in an auto-regressive manner: Each word or 'token' is generated sequentially. This is evident from the distinct output tokens 'Dear' and 'Sarah' being produced in separate, consecutive steps. 2. The output of one generation step becomes part of the input for the next: To generate 'Sarah', the model receives the original prompt combined with the previously generated token 'Dear'. This shows that the context for generation continuously expands with each new token. 3. Each token generation involves a 'forward pass' through the Transformer LLM: The image shows the 'Transformer LLM' box being invoked twice, once for 'Dear' and again for 'Sarah', indicating a distinct processing step for each output token.

**Document Context:**
This image serves as a direct visual explanation for the document's section titled 'The Inputs and Outputs of a Trained Transformer LLM'. It vividly demonstrates the mechanism by which a Transformer LLM processes input to generate output, particularly highlighting the iterative, token-by-token generation. The text after the image, 'Figure 3-3. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token,' explicitly describes the process shown in the figure, making the image a central explanatory component of the document's discussion on LLM mechanics.

**Summary:**
This image illustrates the iterative, token-by-token text generation process of a Transformer Large Language Model (LLM). It depicts two sequential steps in generating an email apology. Initially, a prompt is fed into the Transformer LLM, which then generates the first word, 'Dear'. In the next step, this newly generated word, 'Dear', is appended to the original prompt, forming an updated input. This updated input is then fed back into the Transformer LLM to generate the subsequent word, 'Sarah'. This demonstrates how the model builds a response incrementally, where each generated token becomes part of the context for predicting the next token, ensuring coherence and continuity in the output.](images/e2c555804ce8bb4a1c7f5d7d7d7fd2980bcbcdfbf0f7a8614c2dd8379a8ad6c7.jpg)
Figure 3-3. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token.

This gives us a more accurate picture of the model as it is simply predicting the next token based on an input prompt. Software around the neural network basically runs it in a loop to sequentially expand the generated text until completion.

There’s a specific word used in machine learning to describe models that consume their earlier predictions to make later predictions (e.g., the model’s first generated token is used to generate the second token). They’re called autoregressive models. That is why you’ll hear text generation LLMs being called autoregressive models. This is often used to differentiate text generation models from text representation models like BERT, which are not autoregressive.

This autoregressive, token-by-token generation is what happens under the hood when we generate text with the LLM like we see here:

prompt $=$ "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened."

output $=$ generator(prompt)

print(output[0]['generated_text'])

This generates the text:

Solution 1:   
Subject: My Sincere Apologies for the Gardening Mishap   
Dear Sarah,   
I hope this message finds you well. I am writing to express my deep

We can see the model begin to write the email starting with the subject. It stopped abruptly because it reached the token limit we established by setting max_new_tokens to 50 tokens. If we increase that, it will continue until concluding the email.

# The Components of the Forward Pass

In addition to the loop, two key internal components are the tokenizer and the language modeling head (LM head). Figure 3-4 shows where these components lie in the system. We saw in the previous chapter how tokenizers break down the text into a sequence of token IDs that then become the input to the model.

The tokenizer is followed by the neural network: a stack of Transformer blocks that do all of the processing. That stack is then followed by the LM head, which translates the output of the stack into probability scores for what the most likely next token is.

![## Image Analysis: 4a57dedec0a651884f6232ba1e56e2cb5e277e3e7d6fbee7415a4782492c2c2a.jpg

**Conceptual Understanding:**
This image conceptually represents the internal architecture and a simplified operational flow (the 'forward pass') of a Transformer Large Language Model (LLM). Its main purpose is to demystify how an LLM processes a given prompt to initiate text generation by breaking down the complex system into its core, sequential components: the tokenizer, the stack of transformer blocks, and the language modeling head. The image communicates the key idea that LLMs are modular systems that sequentially transform input text to predict subsequent words or tokens.

**Content Interpretation:**
The image depicts the architecture and operational flow of a Transformer Large Language Model (LLM). It illustrates how a natural language prompt is processed by the LLM to generate an initial token of a response. The 'Tokenizer' is responsible for converting the input text into a format suitable for the model. The 'Stack of Transformer blocks' represents the core computational engine where the actual processing, learning, and transformation of the input sequence occur, typically involving attention mechanisms and feed-forward networks across multiple layers (blocks 1 through N). Finally, the 'LM head' (Language Modeling head) is responsible for predicting the next token in the sequence based on the processed information from the Transformer blocks. The significance of this setup is to show the sequential, modular nature of an LLM's forward pass from input prompt to output generation, emphasizing that the 'Transformer LLM' is not a monolithic entity but rather a system composed of distinct, specialized components working in concert.

**Key Insights:**
1.  **LLM Input Processing:** Transformer LLMs take a natural language prompt (e.g., "Write an email apologizing...") as input. This is evident from the first box. 
2.  **Modular Architecture:** A Transformer LLM is not a single component but a system comprised of distinct modules: a 'Tokenizer', a 'Stack of Transformer blocks', and an 'LM head'. This is explicitly shown in the detailed breakdown. 
3.  **Sequential Block Processing:** The 'Stack of Transformer blocks' consists of multiple sequential 'Transformer block' units (from 1 to N), indicating a layered processing approach. 
4.  **Token-by-Token Generation:** LLMs generate output token by token, as demonstrated by the output 'Dear' following the LLM processing, which represents the first token of a generated email. 
5.  **Forward Pass Components:** The diagram effectively illustrates the key components involved in the forward pass of an LLM, aligning with the document's section title.

**Document Context:**
The image directly supports the document section "The Components of the Forward Pass" by visually breaking down a Transformer LLM into its constituent parts. It explains the initial steps of how such a model takes an input (a prompt for an email) and begins to generate an output ('Dear'), illustrating the internal mechanisms that facilitate this process. The accompanying text after the image, "Figure 3-4. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head," confirms and reinforces the visual representation, serving as a direct caption and summary of the diagram's content. This image is crucial for understanding the foundational architecture and processing steps of a Transformer LLM.

**Summary:**
This image illustrates the functional components of a Transformer Large Language Model (LLM) in the context of generating a response to a user prompt. The overall process begins with a user input: "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened." This input is fed into a 'Transformer LLM'. The LLM then begins to produce output, indicated by the first generated token: 'Dear'.

A detailed breakdown of the 'Transformer LLM' reveals its internal architecture. It is composed of three main stages: a 'Tokenizer', a 'Stack of Transformer blocks', and an 'LM head'. The 'Stack of Transformer blocks' is further elaborated to show that it consists of individual units labeled 'Transformer block 1', 'Transformer block 2', and extending up to 'Transformer block N', implying multiple such blocks arranged sequentially. This diagram visually explains how an LLM processes an input prompt to generate a textual output, highlighting the modular nature of the Transformer architecture.](images/4a57dedec0a651884f6232ba1e56e2cb5e277e3e7d6fbee7415a4782492c2c2a.jpg)
Figure 3-4. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head.

Recall from Chapter 2 that the tokenizer contains a table of tokens—the tokenizer’s vocabulary. The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings). Figure 3-5 shows both the vocabulary and associated token embeddings for a model with a vocabulary of 50,000 tokens.

![## Image Analysis: 30fbc386165244294f83d86d9873153b21f7fa809c4d0f569741bd16a17ef8ca.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental input processing pipeline of a Transformer Large Language Model (LLM). Its main purpose is to show how raw input (implied, by being processed by the 'Tokenizer') is transformed into a numerical representation that the core neural network (the 'Stack of Transformer blocks') can understand and process. It conveys the key idea that textual input is first broken down into discrete 'Tokens' using a defined 'Token vocabulary', and then these tokens are converted into dense numerical 'Token embeddings' before being fed into the multi-layered 'Transformer blocks' for advanced language understanding and generation, ultimately leading to an output via the 'LM head'.

**Content Interpretation:**
The image depicts the architectural components and data flow of a Transformer Large Language Model (LLM). It shows the internal structure of the LLM, including its tokenizer, a stack of transformer blocks, and a language model head. Crucially, it illustrates the relationship between the internal tokenizer and an external 'Token vocabulary' table, as well as the connection between the transformer blocks and 'Token embeddings'. The diagram clarifies how tokens are identified and then converted into numerical embeddings for processing within the neural network, specifically highlighting the scale of the vocabulary at 50,000 unique tokens and corresponding embeddings.

**Key Insights:**
1.  **Transformer LLM Architecture:** The image provides a high-level architectural view of a Transformer LLM, comprising an internal Tokenizer, a Stack of Transformer blocks, and an LM head. These are the fundamental logical units of such a model.
2.  **Tokenization Process:** Input to the LLM first goes through an internal 'Tokenizer'. This tokenizer refers to an external 'Token vocabulary' which maps a 'Token ID' to a specific 'Token'. This demonstrates the discrete nature of token representation.
3.  **Vocabulary Size:** The 'Token vocabulary' explicitly shows a range from 0 to '50,000', with 'Zyzzyva' as an example token at the 50,000th index. This indicates a vocabulary size of 50,001 unique tokens (if starting from 0) or 50,000 if 0 is considered the first of 50,000, confirming the textual information that the tokenizer has a vocabulary of 50,000 tokens.
4.  **Token Embeddings:** Each token ID has a corresponding 'Token embedding'. The image shows 'Token embeddings' for IDs 0, 1, ..., 50,000, represented as small grid structures. This highlights that tokens are converted into dense vector representations suitable for neural network processing.
5.  **Sequential Processing:** The overall flow implies a sequence: input text is tokenized (using the vocabulary), then converted to embeddings, which are then fed into the 'Stack of Transformer blocks' for further processing, before reaching the 'LM head' for final output.
6.  **Modular Design:** The 'Stack of Transformer blocks' contains 'Transformer block 1', 'Transformer block 2', up to 'Transformer block N', indicating a modular and layered architecture common in Transformer models.
7.  **Data Flow:** The red line from the internal Tokenizer points to the 'Token vocabulary', signifying the tokenizer's reliance on this vocabulary. The blue line from the 'Stack of Transformer blocks' points to 'Token embeddings', indicating that these embeddings are the input or are utilized by the transformer blocks.

**Document Context:**
This image directly supports the document's section titled 'The Components of the Forward Pass' by visually explaining the initial stages of data processing within a Transformer LLM. It shows how raw input (implicitly, via the Tokenizer) is converted into a numerical format (tokens and embeddings) that the main Transformer blocks can process. The explicit mention of a 50,000-token vocabulary and associated embeddings directly relates to the text after the image, which states, 'The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings.' Thus, the image provides a crucial visual aid for understanding the foundational input mechanisms before the deeper processing by the Transformer architecture itself.

**Summary:**
This diagram illustrates the core components and data flow within a Transformer Large Language Model (LLM) during its forward pass, as described in the document context. It comprehensively details how textual input is processed from tokenization through embedding and into the Transformer blocks for language modeling. The diagram is organized into a main 'Transformer LLM' unit and external components for token management. The 'Transformer LLM' internally consists of a 'Tokenizer' (pink box), a 'Stack of Transformer blocks' (blue box containing 'Transformer block 1', 'Transformer block 2', and 'Transformer block N'), and an 'LM head'. The internal 'Tokenizer' connects via a red line to an external 'Token vocabulary', which is a table mapping 'Token ID's (0, 1, ..., 50,000) to their corresponding 'Token's ('!', '"', ..., 'Zyzzyva'). Simultaneously, the 'Stack of Transformer blocks' connects via a blue line to 'Token embeddings', which are numerical representations for each token ID (0, 1, ..., 50,000), shown as small blue grid structures. This arrangement clearly depicts the transformation of input text into a numerical format suitable for deep learning processing, highlighting the 50,000-token vocabulary size as a key parameter for both token identification and embedding generation. The information is organized from the overarching model to its granular sub-components and their associated data structures.](images/30fbc386165244294f83d86d9873153b21f7fa809c4d0f569741bd16a17ef8ca.jpg)
Figure 3-5. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings.

The flow of the computation follows the direction of the arrow from top to bottom. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probabil‐ ity distribution for the next token, seen in Figure 3-6.

![## Image Analysis: 00dced44467d08bcbb4da1a0e2fd77cd3556da55d0863510c33be8031eea6eb4.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow and the forward pass mechanism of a Transformer Large Language Model (LLM). The main purpose of the image is to illustrate the sequential components an input traverses within such a model, from initial token processing through complex transformations, culminating in the generation of a probabilistic prediction for the next possible token in a sequence. It communicates the key idea that an LLM's output is not a single word, but a distribution of probabilities across its entire vocabulary, indicating the likelihood of each token being the most appropriate continuation based on the input it processed.

**Content Interpretation:**
The image details the forward pass within a Transformer Large Language Model (LLM). It illustrates the sequential processes and components involved in taking an input (implicitly after an initial embedding) and generating a probability distribution over the vocabulary. The key processes shown are: 1. Tokenization: Represented by the 'Tokenizer' component, responsible for converting input into discrete tokens. 2. Contextual Processing: Handled by the 'Stack of Transformer blocks' which comprises multiple individual 'Transformer block' units (Transformer block 1, Transformer block 2, ..., Transformer block N), performing self-attention and feed-forward operations to enrich token representations. 3. Prediction Head: The 'LM head' (Language Model head) takes the final processed representations and projects them to the size of the vocabulary. 4. Probability Output: The 'Output' section titled 'Token probability' showcases the final outcome, a table with 'Token ID' and 'Token' columns, alongside their assigned probabilities (e.g., '0' for '!', '1' for '"', '1,102' for 'Dear', '50,000' for 'Zyzzyva' with probabilities 0.01%, 0.03%, 40%, 1.00% respectively). The significance of the information lies in demonstrating the model's ability to not just predict a single next word, but to assign a likelihood to every possible word in its vocabulary. The high probability for 'Dear' (40%) compared to others illustrates the model's learned contextual understanding and predictive power.

**Key Insights:**
The image provides several key insights into the operation of a Transformer LLM: 1. Modular Architecture: The 'Stack of Transformer blocks' (including 'Transformer block 1', 'Transformer block 2', 'Transformer block N') highlights the modular and layered design characteristic of Transformer models, enabling deep processing of input. 2. Sequential Processing Pipeline: The flow from 'Tokenizer' to 'Stack of Transformer blocks' to 'LM head' and finally to 'Output: Token probability' demonstrates the clear, sequential steps involved in the forward pass of an LLM. 3. Final Probability Distribution: The 'Token probability' output, with specific examples like 'Dear' having 40% probability, clearly illustrates that the model's final output is a probability distribution over all possible tokens in its vocabulary, rather than a single deterministic prediction. This indicates the model's capacity to assess the likelihood of various potential next tokens. 4. Token Identifiers: The 'Token ID' column (e.g., '0', '1', '1,102', '50,000') shows that each distinct token (like '!', '"', 'Dear', 'Zyzzyva') is mapped to a unique numerical identifier within the model's vocabulary.

**Document Context:**
This image directly supports the document's section 'The Components of the Forward Pass' by visually laying out the internal architecture and operational flow of a Transformer LLM. It precisely illustrates the textual description provided immediately after the figure, which states, 'Figure 3-6. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary.' The diagram graphically elaborates on how this 'probability score for each token' is derived by showing the sequential steps from the tokenizer through the transformer blocks and the LM head, culminating in the 'Token probability' output table. It serves as a foundational visual aid to understand the mechanics behind how these models generate their predictions.

**Summary:**
This diagram illustrates the forward pass of a Transformer Large Language Model (LLM), showing its key architectural components and how it generates token probabilities. The process begins with the overarching system labeled "Transformer LLM". The first internal component is the "Tokenizer", which implicitly handles the conversion of raw text into numerical tokens. Following the "Tokenizer", the data proceeds into a "Stack of Transformer blocks". This stack represents the core deep learning component of the model and consists of multiple individual processing units: "Transformer block 1", "Transformer block 2", and continuing through to "Transformer block N". These blocks are responsible for understanding the context and relationships between tokens. After passing through all transformer blocks, the processed information is fed into the "LM head" (Language Model head). This component is the final layer that translates the internal representations into a distribution over the model's entire vocabulary. The output from the "LM head" is then shown as the "Output" of the entire system, specifically titled "Token probability". This output is presented as a table with two columns: "Token ID" (a numerical identifier for each token) and "Token" (the actual word or punctuation mark). Alongside these, a bar graph visually represents the "Token probability" for each entry. For example, the table lists: "Token ID" 0 corresponds to the "Token" "!" with a "Token probability" of 0.01%; "Token ID" 1 corresponds to the "Token" """ with a "Token probability" of 0.03%; "Token ID" 1,102 corresponds to the "Token" "Dear" with a significantly higher "Token probability" of 40%; and "Token ID" 50,000 corresponds to the "Token" "Zyzzyva" with a "Token probability" of 1.00%. This comprehensive output demonstrates how the Transformer LLM, at the end of its forward pass, calculates and assigns a probability score to every possible token in its vocabulary, indicating its likelihood of being the next token in a sequence. The varying probabilities highlight the model's ability to predict the most probable continuations.](images/00dced44467d08bcbb4da1a0e2fd77cd3556da55d0863510c33be8031eea6eb4.jpg)
Figure 3-6. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary.

The LM head is a simple neural network layer itself. It is one of multiple possible “heads” to attach to a stack of Transformer blocks to build different kinds of systems. Other kinds of Transformer heads include sequence classification heads and token classification heads.

We can display the order of the layers by simply printing out the model variable. For this model, we have:

Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_id ${ \it \Omega } = 3 2 0 0 0$ ) (embed_dropout): Dropout $\cdot P = 0 . 0$ , inplace $\iota =$ False) (layers): ModuleList( (0-31): $3 2 \ \times$ Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear(in_features $\ B { = } 3 \Theta 7 2$ , out_features $\mathtt { \_ 3 0 7 2 }$ , bias=False) (qkv_proj): Linear(in_features $\mathtt { \_ 3 0 7 2 }$ , out_features ${ \tt = } 9 2 1 6$ , bias $=$ False) (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): Linear(in_feature $\mathord {  } 3 \Theta 7 2$ , out_feature $\scriptstyle \sum 1 6 3 8 4$ ,   
bias=False) (down_proj): Linear(in_feature ${ \tt s } = 8 1 9 2$ , out_feature ${ \tt s } = 3 0 7 2$ , bias $\mathbf { \Psi } _ { 1 } =$ False) (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm() (resid_attn_dropout): Dropout( $\mathsf { p } { = } \Theta . \Theta$ , inplace=False) (resid_mlp_dropout): Dropout $\mathsf { p } { = } \Theta . \Theta$ , inplace=False) (post_attention_layernorm): Phi3RMSNorm() ) ) (norm): Phi3RMSNorm() (lm_head): Linear(in_feature ${ \tt S } = 3 \Theta 7 2$ , out_feature $s { = } 3 2 \Theta 6 4$ , bias=False)   
)

Looking at this structure, we can notice the following highlights:

• This shows us the various nested layers of the model. The majority of the model is labeled model, followed by lm_head.   
• Inside the Phi3Model model, we see the embeddings matrix embed_tokens and its dimensions. It has 32,064 tokens each with a vector size of 3,072.   
• Skipping the dropout layer for now, we can see the next major component is the stack of Transformer decoder layers. It contains 32 blocks of type Phi3Deco derLayer.   
• Each of these Transformer blocks includes an attention layer and a feedforward neural network (also known as an mlp or multilevel perceptron). We’ll cover these in more detail later in the chapter.   
• Finally, we see the lm_head taking a vector of size 3,072 and outputting a vector equivalent to the number of tokens the model knows. That output is the proba‐ bility score for each token that helps us select the output token.

# Choosing a Single Token from the Probability Distribution (Sampling/ Decoding)

At the end of processing, the output of the model is a probability score for each token in the vocabulary, as we saw previously in Figure 3-6. The method of choosing a sin‐ gle token from the probability distribution is called the decoding strategy. Figure 3-7 shows how this leads to picking the token “Dear” in one example.

The easiest decoding strategy would be to always pick the token with the highest probability score. In practice, this doesn’t tend to lead to the best outputs for most use cases. A better approach is to add some randomness and sometimes choose the second or third highest probability token. The idea here is to basically sample from the probability distribution based on the probability score, as the statisticians would say.

What this means for the example in Figure 3-7 is that if the token “Dear” has a $4 0 \%$ probability of being the next token, then it has a $4 0 \%$ chance of being picked (instead of greedy search, which would pick it directly for having the highest score). So with this method, all the other tokens have a chance of being picked according to their score.

![## Image Analysis: 39be2d51e721c4cb0c8227a78e14cfa6362cbe6e4c9b07207c52d3bdc5914210.jpg

**Conceptual Understanding:**
The image conceptually represents the token generation process within a Large Language Model (LLM). Its main purpose is to illustrate how an LLM, given a prompt, predicts the probabilities for the next possible tokens and how a subsequent "decoding strategy" then chooses a single token from these probabilities to form the output. The key ideas communicated are the probabilistic nature of LLM outputs and the critical role of a decoding strategy in converting these probabilities into a concrete sequence of tokens.

**Content Interpretation:**
The image details the token generation process within a Large Language Model (LLM). It shows an input "Prompt" ("Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.") being fed into a "Transformer LLM". The LLM processes this prompt and outputs a distribution of "Output token probabilities (highest)" for potential next tokens. Specifically, the probabilities are shown for "Dear" (40%), "Title" (13%), "To" (8%), and "Hi" (2%), with an ellipsis (...) indicating other possibilities. A subsequent "Decoding strategy" then selects one token from this probability distribution. The final chosen output token is "Dear". This illustrates the probabilistic nature of LLM text generation and the role of a decoding strategy in actual token selection, likely favoring the highest probability token in this example.

**Key Insights:**
The main takeaways from this image are: 1. Large Language Models (LLMs) do not directly output words but rather generate a probability distribution over potential next tokens. This is evidenced by the "Output token probabilities (highest)" box showing "Dear 40%", "Title 13%", "To 8%", "Hi 2%", and "...". 2. A distinct "Decoding strategy" is essential to select a single token from the probability distribution generated by the LLM. This is shown by the arrow labeled "Decoding strategy" leading from the probabilities to the final output. 3. The chosen decoding strategy, in this illustration, appears to be a greedy approach, selecting the token with the highest probability. This is supported by "Dear" (40%) being the selected output token, as it has the highest probability among the options presented. These insights highlight the two-step nature of LLM text generation: probabilistic prediction followed by deterministic (or semi-deterministic) selection.

**Document Context:**
This image directly supports the document's section titled "Choosing a Single Token from the Probability Distribution (Sampling/ Decoding)." It visually explains the abstract concept discussed in the text, demonstrating the intermediate step where an LLM provides a probability distribution for the next token and how a "decoding strategy" is then used to select a single token from this distribution. The image serves as a concrete example for "Figure 3-7. The tokens with the highest probability after the model’s forward pass. Our decoding strategy decides which of the tokens to output by sampling based on the probabilities," enhancing the reader's understanding of the sampling and decoding mechanisms in LLMs.

**Summary:**
The image illustrates the process of how a Transformer Large Language Model (LLM) generates the next token in a sequence based on a given prompt. First, a "Prompt" is provided: "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened." This prompt is then processed by the "Transformer LLM". The LLM, after its forward pass, does not immediately produce a single word but instead calculates "Output token probabilities (highest)" for several potential next tokens. In this specific example, the highest probabilities are for "Dear" (40%), "Title" (13%), "To" (8%), and "Hi" (2%), along with other less probable tokens indicated by an ellipsis ("..."). Subsequently, a "Decoding strategy" is applied to these probabilities. This strategy makes the decision of which single token to output. In this depicted scenario, the token "Dear" is selected, likely because it had the highest probability among the options, demonstrating a common greedy decoding approach. This entire flow visually explains the mechanism by which LLMs predict and select words to form coherent text.](images/39be2d51e721c4cb0c8227a78e14cfa6362cbe6e4c9b07207c52d3bdc5914210.jpg)
Figure 3-7. The tokens with the highest probability after the model’s forward pass. Our decoding strategy decides which of the tokens to output by sampling based on the probabilities.

Choosing the highest scoring token every time is called greedy decoding. It’s what happens if you set the temperature parameter to zero in an LLM. We cover the concept of temperature in Chapter 6.

Let’s look more closely at the code that demonstrates this process. In this code block, we pass the input tokens through the model, and then lm_head:

prompt $=$ "The capital of France is"   
# Tokenize the input prompt   
input_ids $=$ tokenizer(prompt, return_tensors $=$ "pt").input_ids   
# Tokenize the input prompt   
input_ids $=$ input_ids.to("cuda")   
# Get the output of the model before the lm_head   
model_output $=$ model.model(input_ids)   
# Get the output of the lm_head   
lm_head_output $=$ model.lm_head(model_output[0])

Now, lm_head_output is of the shape [1, 6, 32064]. We can access the token proba‐ bility scores for the last generated token using lm_head_output[0,-1], which uses the index 0 across the batch dimension; the index $^ { - 1 }$ gets us the last token in the sequence. This is now a list of probability scores for all 32,064 tokens. We can get the top scoring token ID, and then decode it to arrive at the text of the generated output token:

In this case this turns out to be:

# Parallel Token Processing and Context Size

One of the most compelling features of Transformers is that they lend themselves better to parallel computing than previous neural network architectures in language processing. In text generation, we get a first glance at this when looking at how each token is processed. We know from the previous chapter that the tokenizer will break down the text into tokens. Each of these input tokens then flows through its own computation path (that’s a good first intuition, at least). We can see these individual processing tracks or streams in Figure 3-8.

![## Image Analysis: ceed25cd93f15e6cc6f8c4f00f6ad787fbe68738bf5c72128a42d3a36dec88ae.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow and processing mechanism of a Transformer-based Large Language Model (LLM). Its main purpose is to visually explain how a natural language input prompt is broken down, processed in parallel through a series of computational layers, and then directed towards generating an output. The key ideas communicated are the modularity of LLM components (Tokenizer, Transformer blocks, LM head), the critical role of tokenization in converting raw text into processable units, and the parallel, layered processing of these tokens through the Transformer architecture, which is fundamental to the efficiency and power of these models. It illustrates a common computational paradigm in modern AI for language understanding and generation tasks.

**Content Interpretation:**
The image illustrates the internal processing flow of a Transformer Large Language Model (LLM) for a given text prompt. It depicts a sequence of operations: first, a natural language input is tokenized into individual units. Then, each of these tokens is processed in parallel through a 'Stack of Transformer blocks,' which consists of multiple layers ('Transformer block 1', 'Transformer block 2', up to 'Transformer block N'). Finally, the processed representations from these blocks are fed into an 'LM head' (Language Model head) to produce the model's output. The diagram shows the specific input text "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened." being broken down into tokens: "Write", "...", "Explain", "how", "it", "happen", "##ed", ".". These individual tokens are then shown with distinct vertical processing paths through the transformer block stack and the LM head, emphasizing the parallel nature of token processing. The '##ed' token signifies subword tokenization, a common technique in LLMs. The overall system is explicitly labeled as 'Transformer LLM'.

**Key Insights:**
The image provides several key takeaways about Transformer LLMs: 1. **Tokenization is the initial step:** Raw text inputs like "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened." are first converted into a sequence of discrete tokens (e.g., "Write", "Explain", "how", "it", "happen", "##ed", ".") by a 'Tokenizer.' This highlights that LLMs operate on these granular units, not raw characters or full words exclusively. 2. **Subword tokenization is utilized:** The presence of '##ed' as a token demonstrates that words can be split into subword units, indicating the use of advanced tokenization strategies to handle morphology and potentially reduce vocabulary size. 3. **Tokens are processed in parallel:** Each individual token follows its own computational stream through the core layers of the model. This is clearly depicted by the distinct vertical lines from each token passing simultaneously through the 'Stack of Transformer blocks,' underscoring the parallel processing capability central to Transformer architecture. 4. **Layered architecture for complex transformations:** Transformer LLMs are composed of multiple, stacked 'Transformer blocks' (labeled 'Transformer block 1' through 'Transformer block N'). This layered design enables iterative and increasingly abstract transformations of token representations, crucial for building deep contextual understanding. 5. **Modular system design:** The overall system is modular, comprising a 'Tokenizer' for input preparation, a 'Stack of Transformer blocks' for core processing, and an 'LM head' for generating the final output, each with a distinct function.

**Document Context:**
This image directly supports the document section 'Parallel Token Processing and Context Size' by visually explaining how tokens are processed in parallel within a Transformer LLM. It illustrates the 'token processing' aspect by showing the input being tokenized and then each token being processed along its own computational stream. The visual representation of distinct vertical lines for each token passing through the 'Stack of Transformer blocks' directly aligns with the concept of parallel token processing. The text after the image, "Each token is processed through its own stream of computation (with some interaction between them in attention steps, as we’ll later see)," perfectly describes the visual flow presented in this diagram, making the image a key visual aid for understanding the foundational architecture and operation of Transformer models in the context of large language model processing.

**Summary:**
This diagram, labeled "Transformer LLM" (indicated by a flame/spark icon), illustrates the step-by-step internal process a Large Language Model undergoes when it receives a textual prompt. The process begins with an initial input: "Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened." 1. **Tokenization:** This full input sentence is first fed into the "Tokenizer" component. The role of the "Tokenizer" is to break down the continuous text into smaller, discrete units called "tokens." For the given example, the "Tokenizer" converts the sentence into the following sequence of tokens: "Write", "...", "Explain", "how", "it", "happen", "##ed", ".". The "..." token indicates omitted text from the original prompt for brevity, while "##ed" is a subword token, common in LLMs to handle different word forms. 2. **Parallel Processing through Transformer Blocks:** Each of these individual tokens then enters a "Stack of Transformer blocks." This stack represents the core computational layers of the Transformer LLM. The diagram clearly shows that each token (e.g., "Write", "Explain", "how", "it", "happen", "##ed", ".") is processed simultaneously and independently through this stack. The stack consists of multiple layers, indicated by "Transformer block 1", "Transformer block 2", and continues up to "Transformer block N," signifying a potentially large number of such layers. As each token's representation passes through these blocks, it undergoes complex transformations, incorporating contextual information from other tokens through mechanisms like attention (though attention interactions are not explicitly drawn, the parallel lines imply their eventual integration). 3. **Language Model Head:** After traversing all the "Transformer blocks" in the stack, the processed representation of each token is passed to the "LM head" (Language Model head). This final component is responsible for interpreting the rich, contextually-aware representations from the transformer blocks and generating the model's final output, such as predicting the next word, completing a sentence, or performing the specific task requested in the initial prompt. The diagram thus vividly depicts how a natural language input is systematically deconstructed, processed in parallel through deep learning layers, and then re-synthesized to produce a meaningful output from the "Transformer LLM."](images/ceed25cd93f15e6cc6f8c4f00f6ad787fbe68738bf5c72128a42d3a36dec88ae.jpg)
Figure 3-8. Each token is processed through its own stream of computation (with some interaction between them in attention steps, as we’ll later see).

Current Transformer models have a limit for how many tokens they can process at once. That limit is called the model’s context length. A model with 4K context length can only process 4K tokens and would only have 4K of these streams.

Each of the token streams starts with an input vector (the embedding vector and some positional information; we’ll discuss positional embeddings later in the chap‐ ter). At the end of the stream, another vector emerges as the result of the model’s processing, as shown in Figure 3-9.

![## Image Analysis: 83880c51e73af413158219042f6b9ab259b5f4a3d5b78e5c7280ce9c97d72dc2.jpg

**Conceptual Understanding:**
This image conceptually represents the forward pass of a Transformer Large Language Model (LLM). Its main purpose is to illustrate the sequential and parallel processing of an input text sequence through the different architectural components of a Transformer model. The key ideas being communicated are tokenization, the generation of embeddings, the multi-layered processing within transformer blocks, and the production of final output vectors for language tasks. It visually explains how an LLM converts human-readable text into a format it can process, computes relationships between parts of that text, and generates meaningful numerical representations as an output.

**Content Interpretation:**
The image depicts the architectural flow of a Transformer Large Language Model (LLM). It illustrates the sequence of operations involved in processing an input text query or sentence. The core components shown are the Tokenizer, the process of generating Embeddings, the sequential processing through a Stack of Transformer blocks, and the final generation of Output vectors via an LM head. The significance lies in showing how raw text is transformed into numerical representations (embeddings), processed through multiple layers to capture contextual relationships (transformer blocks), and then prepared for specific language tasks (LM head producing output vectors). The parallel processing of tokens through the transformer blocks is a key aspect, indicating that each token's embedding is processed concurrently through the network layers.

**Key Insights:**
The main takeaways from this image are: 1.  **Modular Architecture:** A Transformer LLM is composed of distinct, sequential modules: Tokenizer, Embeddings, Stack of Transformer blocks, and LM head. 2.  **Tokenization:** Input text is broken down into smaller units (tokens) by the 'Tokenizer', as evidenced by 'Write', '...', 'Explain', 'how', 'it', 'happen', '##ed', '.'. 3.  **Embedding Generation:** These tokens are converted into numerical representations called 'Embeddings', indicating a transformation from discrete words to continuous vectors. 4.  **Layered Processing:** The core of the model is a 'Stack of Transformer blocks' ('Transformer block 1', 'Transformer block 2', 'Transformer block N'), which processes the embeddings in multiple layers, suggesting iterative refinement of contextual information. 5.  **Parallel Processing:** The vertical lines from each embedding into the stack of transformer blocks illustrate that each token's embedding undergoes processing through the blocks concurrently. 6.  **Output Vectors:** The final output from the transformer blocks is 'Output vectors', which are then used by the 'LM head' for downstream tasks like generating text or predicting the next word. The image visually confirms the conceptual framework of how Transformer LLMs handle and process input sequences in a parallel and layered fashion.

**Document Context:**
This image directly follows and supports the document section titled 'Parallel Token Processing and Context Size'. It visually demonstrates how tokens are processed in parallel within a Transformer LLM architecture, which is a key concept for understanding the efficiency and capabilities of such models. The diagram explains how an input sequence like 'Write ... Explain how it happen ##ed .' is broken down, embedded, and then simultaneously fed into a stack of transformer blocks, ultimately leading to output vectors. This visualization is crucial for understanding how the model handles the 'context size' by processing multiple tokens concurrently.

**Summary:**
The image illustrates the internal architecture of a Transformer Large Language Model (LLM), detailing how an input sequence of text is processed through tokenization, embedding, a stack of transformer blocks, and finally an LM head to produce output vectors. The process begins with an input text phrase, exemplified as 'Write ... Explain how it happen ##ed .'. This text is first fed into the 'Tokenizer' component. The tokenizer breaks down the input text into individual tokens, represented by separate white boxes labeled with parts of the phrase: 'Write', '...', 'Explain', 'how', 'it', 'happen', '##ed', and '.'. Each of these tokens is then converted into numerical representations called 'Embeddings', which are depicted as small blue rectangular blocks. These embeddings are then passed in parallel into a 'Stack of Transformer blocks'. This stack is a larger blue-outlined box containing multiple individual transformer layers. Within this stack, three distinct blocks are explicitly labeled: 'Transformer block 1', 'Transformer block 2', and 'Transformer block N', indicating a sequential or layered processing where 'N' represents the total number of blocks in the stack. Each transformer block processes the embeddings, refining their contextual understanding. After passing through the entire 'Stack of Transformer blocks', the processed embeddings emerge as 'Output vectors', shown as small orange rectangular blocks. These output vectors are then fed into the 'LM head' (Language Model head), which is the final component responsible for converting these vectors into predictions, such as the probability distribution over the next possible token in a sequence. The overall diagram showcases the modular and parallel nature of a Transformer LLM's processing pipeline, from input text to contextualized output representations.](images/83880c51e73af413158219042f6b9ab259b5f4a3d5b78e5c7280ce9c97d72dc2.jpg)
Figure 3-9. Each processing stream takes a vector as input and produces a final resulting vector of the same size (often referred to as the model dimension).

For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token.

You may wonder why we go through the trouble of calculating all the token streams if we’re discarding the outputs of all but the last token. The answer is that the calculations of the previous streams are required and used in calculating the final stream. Yes, we’re not using their final output vector, but we use earlier outputs (in each Transformer block) in the Transformer block’s attention mechanism.

If you’re following along with the code examples, recall that the output of lm_head was of the shape [1, 6, 32064]. That was because the input to it was of the shape [1, 6, 3072], which is a batch of one input string, containing six tokens, each of them represented by a vector of size 3,072 corresponding to the output vectors after the stack of Transformer blocks.

We can access these matrices and view their dimensions by printing:

model_output[0].shape

This outputs:

torch.Size([1, 6, 3072])

Similarly, we can print the output of the LM head:

lm_head_output.shape

This outputs:

# Speeding Up Generation by Caching Keys and Values

Recall that when generating the second token, we simply append the output token to the input and do another forward pass through the model. If we give the model the ability to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), we no longer need to repeat the calculations of the previous streams. This time the only needed calculation is for the last stream. This is an optimization technique called the keys and values (kv) cache and it provides a significant speedup of the generation process. Keys and values are some of the central components of the attention mechanism, as we’ll see later in this chapter.

Figure 3-10 shows how when generating the second token, only one processing stream is active as we cache the results of the previous streams.

![## Image Analysis: 89afe8e619172242dc9fe03b2e0e914315f11ccab30b67143bc0ec529b306bc3.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow and operational principle of a Transformer Large Language Model (LLM) during the text generation phase, specifically emphasizing a mechanism for computational efficiency. The main purpose of the image is to illustrate *how* caching of intermediate computation results for previously processed tokens helps to speed up the generation of new text, thereby avoiding redundant calculations. It communicates the core idea that by storing past computations in a 'Cached calculation' module, the LLM can generate 'Output vectors' for new tokens more rapidly, leading to faster text generation.

**Content Interpretation:**
The image depicts the architecture and operational flow of a Transformer Large Language Model (LLM) during text generation, with a specific focus on optimizing efficiency through caching. It illustrates the steps from tokenizing input text, converting tokens into embeddings, processing these embeddings in a 'Cached calculation' unit, and finally generating 'Output vectors' for an 'LM head' to predict the next token. The key concept shown is that intermediate computations for previously processed tokens are stored and reused, preventing redundant calculations and speeding up the generation of subsequent tokens.

Specifically:
*   **Tokenization:** The 'Tokenizer' box and the sequence of words/subwords ('Write', '...', 'Explain', 'how', 'it', 'happen', '##ed', '.', 'Dear') demonstrate how input text is segmented.
*   **Embeddings:** The vertical label 'Embeddings' and the light blue blocks signify the conversion of tokens into numerical vector representations, which serve as input to the model's core layers.
*   **Cached Calculation:** The central 'Cached calculation' box, receiving inputs from multiple embeddings, illustrates that computations for preceding tokens are stored. This is crucial for avoiding recalculations.
*   **Sequential Processing and Generation:** The left-to-right flow of tokens and the focus on 'Dear' as a new token imply an autoregressive text generation process.
*   **Output Vectors:** The 'Output vectors' label and the arrows emerging from 'Cached calculation' represent the processed information passed to the final prediction layer.
*   **LM Head:** The 'LM head' component is shown receiving these output vectors, indicating its role in predicting the next token in the sequence.
*   **Caching Mechanism Visualization:** The blue vertical line on the far right, including the orange block, visually represents how cached information is utilized to efficiently generate the next token's output.

**Key Insights:**
The main takeaways and insights from this image are:

*   **Enhanced LLM Generation Efficiency:** The primary insight is that Transformer LLMs can significantly "Speed Up Generation" by utilizing a "Cached calculation" mechanism. This prevents the model from redundantly computing the internal states for all previously generated tokens each time a new token is predicted.
*   **Core Components of a Transformer LLM:** The diagram provides a clear visual breakdown of essential components involved in text generation within a Transformer architecture: the "Tokenizer" for input processing, the generation of "Embeddings" for numerical representation, a central processing unit (here focused on "Cached calculation"), and the "LM head" for final token prediction.
*   **Autoregressive Generation with Optimization:** It highlights the sequential, autoregressive nature of text generation (processing tokens like "Write"... then predicting "Dear") and demonstrates an explicit optimization strategy within this process through caching.
*   **Mechanism of Caching:** The "Cached calculation" box, with arrows from multiple token embeddings converging into it, visually confirms that intermediate computational results are stored and leveraged. The blue line and the orange block on the right further illustrate the efficient utilization of these cached values to produce the next token's output. This confirms the principle of reusing past computations to reduce computational load for subsequent steps in the generation sequence.

**Document Context:**
This image directly supports the document's section titled "Speeding Up Generation by Caching Keys and Values" and the accompanying text: "Figure 3-10. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again." It provides a visual explanation of *how* a Transformer LLM implements this caching mechanism. By illustrating the 'Tokenizer', 'Embeddings', 'Cached calculation', 'Output vectors', and 'LM head' components, it clarifies the data flow and the specific point where caching is applied to improve efficiency during text generation. The diagram serves as a concrete example of the abstract concept discussed in the text, helping readers understand the internal workings and optimization strategies of Transformer LLMs.

**Summary:**
This diagram illustrates a simplified view of how a Transformer Large Language Model (LLM) generates text efficiently by utilizing a caching mechanism.

At the top, we see the overall system labeled "Transformer LLM," indicating the type of artificial intelligence model being depicted.

The process begins with an input sequence of text, such as "Write ... Explain how it happen ##ed . Dear". This text first goes into a "Tokenizer" component, which breaks it down into individual units called tokens. Each word or sub-word (like "##ed," which indicates a part of a word) becomes a distinct token.

Next, these tokens are converted into numerical representations known as "Embeddings." The "Embeddings" label on the left points to several light blue rectangular blocks, each representing the embedding for a corresponding token from the input sequence. For example, there's an embedding for "Write," one for "Explain," and so on, up to "Dear."

These embeddings are then fed into a central processing unit, the large light blue box labeled "Cached calculation." This is the core of the efficiency improvement. For all previously processed tokens (e.g., from "Write" up to the period "."), the results of their internal computations are stored or "cached" within this module. This means that when the model needs to process subsequent tokens or generate the next word (like "Dear"), it doesn't need to recalculate everything from scratch for the earlier parts of the sequence. Instead, it leverages the "Cached calculation" to retrieve the necessary intermediate results.

From the "Cached calculation" module, "Output vectors" are generated for each token. These are numerical representations that contain the model's understanding and predictions based on the input and the cached information. These "Output vectors" are then directed downwards to the "LM head" component at the bottom of the diagram.

The "LM head" (Language Model head) is the final part of the model responsible for taking these "Output vectors" and converting them into actual probabilities for the next word or token to be generated. The vertical blue line on the far right, which passes through a small orange rectangular block before reaching the "LM head," visually emphasizes the flow of cached information being used to generate or predict the next token efficiently. The orange block likely represents the output or embedding of the *new* token being generated, directly benefiting from the cached historical data.

In essence, the diagram shows a Transformer LLM systematically processing tokens, creating their embeddings, and then using a "Cached calculation" module to store and reuse past computation results. This caching prevents redundant work, especially important during sequential text generation, allowing the "LM head" to quickly generate new "Output vectors" and predict the next token, thus speeding up the overall generation process.](images/89afe8e619172242dc9fe03b2e0e914315f11ccab30b67143bc0ec529b306bc3.jpg)
Figure 3-10. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again.

In Hugging Face Transformers, cache is enabled by default. We can disable it by setting use_cache to False. We can see the difference in speed by asking for a long generation, and timing the generation with and without caching:

prompt $=$ "Write a very long email apologizing to Sarah for the tragic gardening   
mishap. Explain how it happened."   
# Tokenize the input prompt   
input_ids $=$ tokenizer(prompt, return_tensors $=$ "pt").input_ids   
input_ids $=$ input_ids.to("cuda")

Then we time how long it takes to generate 100 tokens with caching. We can use the %%timeit magic command in Jupyter or Colab to time how long the execution takes (it runs the command several times and gets the average):

%%timeit -n 1   
# Generate the text   
generation_output $=$ model.generate( input_ids $\mathbf { \Psi } =$ input_ids, max_new_tokens $\begin{array} { r l } { \mathrm { ~  ~ \tau ~ } } & { { } = } \\ { \mathrm { ~  ~ \tau ~ } } & { { } = } \end{array}$ , use_cache=True   
)

On a Colab with a T4 GPU, this comes to 4.5 seconds. How long would that take if we disable the cache, however?

%%timeit -n 1   
# Generate the text   
generation_output $=$ model.generate( input_ids $\cdot = \cdot$ input_ids, max_new_tokens $\begin{array} { r l } { \mathrm { ~  ~ \tau ~ } } & { { } = } \\ { \mathrm { ~  ~ \tau ~ } } & { { } = } \end{array}$ , use_cache=False   
)

This comes out to 21.8 seconds. A dramatic difference. In fact, from a user experience standpoint, even the four-second generation time tends to be a long time to wait for a user that’s staring at a screen and waiting for an output from the model. This is one reason why LLM APIs stream the output tokens as the model generates them instead of waiting for the entire generation to be completed.

# Inside the Transformer Block

We can now talk about where the vast majority of processing happens: the Trans‐ former blocks. As Figure 3-11 shows, Transformer LLMs are composed of a series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs). Each block processes its inputs, then passes the results of its processing to the next block.

![## Image Analysis: fea40327e414a5b1a63ae445e4a7e2571e4dcb1358713e366cb36c0bd29c9c96.jpg

**Conceptual Understanding:**
This image conceptually represents the high-level architecture and data flow within a Transformer Large Language Model (LLM). Its main purpose is to illustrate the sequential steps an input text goes through, emphasizing the role and interconnectedness of the 'Transformer blocks' as the core processing units. It communicates the key idea that complex language understanding and generation in LLMs are achieved through a layered transformation of input data, starting from tokenization and embeddings, passing through multiple computational blocks, and culminating in output vectors for a language model head.

**Content Interpretation:**
The image illustrates the core processing architecture of a Transformer Large Language Model (LLM). It demonstrates the sequential stages involved in transforming raw text input into a format suitable for language model tasks. The key processes shown include tokenization, embedding generation, sequential processing through multiple Transformer blocks (highlighting the iterative and layered nature of the Transformer architecture), and the final generation of output vectors before passing to the LM head. The red labels indicate the flow of data, particularly how the output of one block feeds into the next, emphasizing the 'stacking' mechanism. The specific text "Say something smart" is used as a concrete example of input text, making the process tangible. The presence of 'N' for the last Transformer block signifies that the number of such blocks can vary, a common feature in Transformer models.

**Key Insights:**
1.  **Sequential Processing:** Transformer LLMs process input sequentially through multiple layers, specifically a 'Stack of Transformer blocks', where the output of one block becomes the input for the next. This highlights the deep, layered nature of these models. 
2.  **Modular Architecture:** The model is composed of distinct, identifiable modules: Tokenizer, Embeddings, Transformer blocks (repeated 'N' times), and an LM head. Each module performs a specific, crucial step in the overall processing.
3.  **Data Transformation:** Input text undergoes several transformations: from raw text to tokens, then to numerical embeddings, and finally through complex computations within Transformer blocks to produce 'Output vectors'.
4.  **Scalability:** The notation 'Transformer block N' implies that the number of processing layers is configurable and can be scaled, which is a characteristic of modern LLMs.
5.  **Core of LLM:** The 'Stack of Transformer blocks' represents the central computational engine where the most significant processing and learning occur within the LLM. The extracted text "Transformer block 1", "Transformer block 2", "Transformer block N", and "Stack of Transformer blocks" directly illustrates these points.

**Document Context:**
This image directly supports the document's narrative regarding the internal workings of a Transformer Block, specifically within the section "Inside the Transformer Block". It visually explains the statement that "The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block." The diagram provides a clear visual representation of this sequential processing, showing how input data flows through the tokenizer, embeddings, and a stack of Transformer blocks before reaching the LM head, thereby enhancing the reader's comprehension of the LLM's architecture and its core computational steps.

**Summary:**
This diagram illustrates the internal processing flow within a Transformer Large Language Model (LLM), detailing how input text is transformed into output vectors. The process begins with the input text, exemplified by "Say something smart", which first passes through a Tokenizer. The output of the Tokenizer then generates Embeddings, represented by blue stacked blocks, for each token. These Embeddings are fed into a series of Transformer blocks, which are depicted as a "Stack of Transformer blocks". Specifically, the input first enters "Transformer block 1". The output from "Transformer block 1" then serves as the input to "Transformer block 2", and this sequential processing continues through "Transformer block N", indicating an arbitrary number of such blocks. The outputs from the final Transformer block (N) are then presented as "Output vectors" (represented by blue stacked blocks), which are subsequently passed to an "LM head" for final processing.](images/fea40327e414a5b1a63ae445e4a7e2571e4dcb1358713e366cb36c0bd29c9c96.jpg)
Figure 3-11. The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block.

A Transformer block (Figure 3-12) is made up of two successive components:

1. The attention layer is mainly concerned with incorporating relevant information from other input tokens and positions 2. The feedforward layer houses the majority of the model’s processing capacity

![## Image Analysis: 700350d7b2dba288d0012aa998158165b5a0d298ddd2182736749183f4b93427.jpg

**Conceptual Understanding:**
The image conceptually represents the fundamental architecture of a Transformer network. Its main purpose is to visually illustrate the modularity and hierarchical structure of these models. Specifically, it conveys that a Transformer model is built by sequentially layering multiple 'Transformer block' units. Furthermore, it details the internal composition of a single 'Transformer block', showing that each block itself comprises a 'Self-attention' layer and a 'Feedforward neural network'. The key ideas communicated are the reusability of the 'Transformer block' module and the sequential data flow through these stacked components.

**Content Interpretation:**
The image shows the architectural breakdown of a Transformer model, specifically focusing on its modular components. It illustrates that a Transformer model is built by stacking multiple identical "Transformer block" units. The left side depicts the macroscopic view of this stacking, with inputs flowing sequentially from one "Transformer block" to the next. The right side provides a microscopic view, detailing the internal structure of a single "Transformer block". This detailed view reveals that each block fundamentally consists of two key layers: a "Self-attention" mechanism, which is crucial for understanding relationships between different parts of the input, and a "Feedforward neural network", which processes the output of the self-attention layer. The connection between the stacked blocks and the detailed single block highlights the recursive and modular nature of the Transformer architecture, where the output of one block serves as the input for the subsequent block.

**Key Insights:**
The main takeaways from this image are: 1.  Transformer models are constructed by stacking multiple identical processing units, referred to as "Transformer block"s. This is evident from the sequential arrangement of "Transformer block 1", "Transformer block 2", and "Transformer block N". 2.  Each individual "Transformer block" has a consistent internal architecture, comprising two primary layers: a "Self-attention" layer and a "Feedforward neural network". This is explicitly shown in the detailed view on the right. 3.  The processing within a Transformer model is sequential, where the output of one "Transformer block" serves as the input for the next block in the stack. This is indicated by the flow from "Transformer block 1" to "Output of Transformer block 1" (red squares) and then to "Transformer block 2". These insights confirm the modular and layered design principle that underpins the power and scalability of Transformer architectures in machine learning.

**Document Context:**
This image is directly relevant to the document's section titled "Inside the Transformer Block". It visually elaborates on the textual description provided, which states: "Figure 3-12. A Transformer block is made up of a self-attention layer and a feedforward neural network." The image complements this by showing not only the components of a single Transformer block but also how multiple such blocks are arranged sequentially to form a larger Transformer architecture. It visually confirms that the 'self-attention layer' and 'feedforward neural network' are indeed the core constituents of each 'Transformer block' and illustrates the concept of stacking these blocks (e.g., 'Transformer block 1', 'Transformer block 2', 'Transformer block N') to build the entire model, with the 'Output of Transformer block 1' flowing into the next block. Thus, it serves as a foundational visual aid for understanding the structural basis of Transformer models.

**Summary:**
The image illustrates the architecture of a Transformer block and how multiple such blocks are stacked in a Transformer model. On the left side, a sequence of stacked Transformer blocks is shown, encased within a blue-bordered rectangle. It begins with an input represented by two light blue squares at the top. This input feeds into "Transformer block 1". The output of "Transformer block 1" is explicitly labeled as "Output of Transformer block 1" in red text, pointing to two red squares, which then serve as the input for "Transformer block 2". This sequential processing continues, leading to "Transformer block N", indicating that an arbitrary number of blocks can be stacked. Finally, the output of the last block, "Transformer block N", is represented by two downward-pointing arrows. On the right side, a detailed view of a single "Transformer block" is presented, also within a blue-bordered rectangle labeled "Transformer block". This detailed view shows that each Transformer block is composed of two main layers: a top layer labeled "Self-attention" (in a light orange box) and a bottom layer labeled "Feedforward neural network" (in a blue box). Dotted lines connect the overall stack of Transformer blocks to the detailed view of a single "Transformer block", indicating that each block in the sequence (e.g., Transformer block 1, 2, N) has the internal structure shown on the right. The image effectively demonstrates the modular and layered design characteristic of Transformer models, where identical blocks are stacked, and each block itself contains key neural network components.](images/700350d7b2dba288d0012aa998158165b5a0d298ddd2182736749183f4b93427.jpg)
Figure 3-12. A Transformer block is made up of a self-attention layer and a feedforward neural network.

# The feedforward neural network at a glance

A simple example giving the intuition of the feedforward neural network would be if we pass the simple input “The Shawshank” to a language model, with the expectation that it will generate “Redemption” as the most probable next word (in reference to the film from 1994).

The feedforward neural network (collectively in all the model layers) is the source of this information, as Figure 3-13 shows. When the model was successfully trained to model a massive text archive (which included many mentions of “The Shawshank Redemption”), it learned and stored the information (and behaviors) that make it succeed at this task.

![## Image Analysis: 1b58ad45b968389ea057eec5206259cc78c3ae63f7be473feae3d3e944d9b8e0.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow of a Transformer neural network, particularly highlighting the role of stacked Transformer blocks and detailing a key internal component: the feedforward neural network. Its main purpose is to illustrate how an input (e.g., "The Shawshank") is progressively transformed through multiple layers of a Transformer model, eventually leading to a specific output (e.g., "Redemption"). The diagram emphasizes the modular and sequential nature of Transformer models in processing information.

**Content Interpretation:**
The image depicts a simplified, conceptual pipeline of a Transformer-based language model. It illustrates how an input text, exemplified by "The Shawshank", is processed sequentially through multiple "Transformer block" layers. Each Transformer block, as shown in the detailed view, contains a "Feedforward neural network". After passing through the stacked Transformer blocks, the information is processed by an "LM head" to produce a final output, in this case, "Redemption". This suggests the model's task might involve transforming or completing a phrase or concept. The diagram highlights the layered nature of Transformer models and the compositional elements within each layer.

**Key Insights:**
1.  **Sequential Processing in Transformers:** The input "The Shawshank" flows sequentially through "Transformer block 1", "Transformer block 2", up to "Transformer block N", demonstrating the layered processing characteristic of Transformer architectures. 2.  **Composition of a Transformer Block:** Each "Transformer block" (as labeled and detailed externally) contains a "Feedforward neural network", indicating that this network is a fundamental component of each processing layer. 3.  **Language Model Output:** The process culminates in an "LM head" and a final output of "Redemption", suggesting that the model is performing a language-related task, possibly text generation, completion, or transformation, where an input phrase leads to a meaningful related output. 4.  **Conceptual Mapping:** The use of "The Shawshank" as input and "Redemption" as output suggests a conceptual transformation or an understanding of the relationship between these terms, reflecting the model's ability to 'understand' and process information. The associated text indicates the feedforward network's role in memorization and interpolation, crucial for achieving such conceptual links.

**Document Context:**
This image directly supports the document's section titled "The feedforward neural network at a glance" by visually representing the role and context of the feedforward neural network within a Transformer block. The accompanying text, "Figure 3-13. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation," further clarifies that this specific component, highlighted within the Transformer block, is crucial for the model's learning capabilities. The diagram provides a high-level overview of the Transformer's architecture and then zooms in to show a key internal component.

**Summary:**
This diagram illustrates the conceptual flow of a language model (LM) based on a Transformer architecture, using the phrase "The Shawshank Redemption" as an example input and output. The process begins with the input phrase "The Shawshank", where "The" and "Shawshank" are shown as individual rectangular boxes, each connected to smaller, light blue boxes, which likely represent tokenized or embedded input. These inputs then proceed into a series of stacked Transformer blocks. The first block is labeled "Transformer block 1", followed by "Transformer block 2", and finally "Transformer block N", indicating that an arbitrary number of such blocks can be stacked sequentially. These stacked Transformer blocks are enclosed within a larger blue-bordered conceptual grouping. Following the processing through these Transformer blocks, the output feeds into an "LM head" (Language Model head), represented by a rounded rectangular box. The final output of this entire process is "Redemption", depicted in a purple rounded rectangular box. To the right of the main processing flow, there is a detailed view of a single "Transformer block". This external "Transformer block" is labeled at the top of a larger gray-bordered box and contains one internal component: a blue rounded rectangular box labeled "Feedforward neural network". A dashed line connects the outline of the main blue-bordered grouping (containing the stacked Transformer blocks) to the external "Transformer block" label, signifying that each "Transformer block 1", "Transformer block 2", up to "Transformer block N" is an instance of this detailed "Transformer block", which includes a "Feedforward neural network". The overall diagram shows a sequential progression of information from input to final output through multiple layers of Transformer processing.](images/1b58ad45b968389ea057eec5206259cc78c3ae63f7be473feae3d3e944d9b8e0.jpg)
Figure 3-13. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation.

For an LLM to be successfully trained, it needs to memorize a lot of information. But it is not simply a large database. Memorization is only one ingredient in the recipe of impressive text generation. The model is able to use this same machinery to interpolate between data points and more complex patterns to be able to generalize— which means doing well on inputs it hadn’t seen in the past and were not in its training dataset.

When you use a modern commercial LLM, the outputs you get are not the ones mentioned earlier in the strict meaning of a “lan‐ guage model.” Passing “The Shawshank” to a chat LLM like GPT-4 produces an output:

"The Shawshank Redemption" is a 1994 film directed by Frank Darabont and is based on the novella "Rita Hayworth and Shawshank Redemption" written by Stephen King. ...etc.

This is because raw language models (like GPT-3) are difficult for people to properly utilize. This is why the language model is then trained on instruction-tuning and human preference and feedback fine-tuning to match people’s expectations of what the model should output.

# The attention layer at a glance

Context is vital in order to properly model language. Simple memorization and interpolation based on the previous token can only take us so far. We know that because this was one of the leading approaches to build language models before neural networks (see Chapter 3, “N-gram Language Models” of Speech and Language Processing by Daniel Jurafsky and James H. Martin).

Attention is a mechanism that helps the model incorporate context as it’s processing a specific token. Think of the following prompt:

“The dog chased the squirrel because it”

For the model to predict what comes after “it,” it needs to know what “it” refers to. Does it refer to the dog or the squirrel?

In a trained Transformer LLM, the attention mechanism makes that determination.   
Attention adds information from the context into the representation of the “it” token.   
We can see a simple version of that in Figure 3-14.

![## Image Analysis: 59e2d0f7b480a5f91b1a7d285d50024302bbb2d9992830d6a5011af440718c2b.jpg

**Conceptual Understanding:**
Conceptually, this image illustrates the self-attention mechanism within a Transformer architecture. Its main purpose is to demonstrate how a model, when processing a particular word (token), can dynamically weigh and incorporate information from other words in the same input sequence to better understand its meaning or context. This is exemplified by the pronoun "it" attending to "squirrel" in the given sentence. The image communicates the idea that processing occurs in stacked 'Transformer blocks,' each containing a 'Self-attention' layer and a 'Feedforward neural network' to refine token representations contextually.

**Content Interpretation:**
This image illustrates the core mechanism of a Transformer model in processing a natural language sentence, specifically focusing on how the self-attention layer allows the model to understand contextual relationships between words. It shows a sentence, "The dog chased the squirrel because it", being processed token by token. The key concept is the "Attention" mechanism, depicted by the orange arrow, where the pronoun "it" looks back to "squirrel" to resolve its reference. This highlights the ability of Transformer models to capture long-range dependencies and contextual meanings within a sequence. The image also breaks down the internal structure of a "Transformer block" into its fundamental components: a "Self-attention" layer and a "Feedforward neural network", indicating a multi-layered processing architecture.

**Key Insights:**
The main takeaway from this image is that the self-attention mechanism within a Transformer model enables a token to incorporate relevant contextual information from other tokens in the sequence. Specifically, for the sentence "The dog chased the squirrel because it", the model can determine that "it" refers to "squirrel" by attending to it, which is a fundamental aspect of natural language understanding. This process occurs iteratively through multiple "Transformer block" layers. Each "Transformer block" itself is composed of a "Self-attention" layer followed by a "Feedforward neural network" layer, indicating a two-stage processing step within each block. The explicit labeling of "Attention" on the curved arrow from "squirrel" to "it" provides direct evidence for how contextual connections are formed.

**Document Context:**
This image directly supports the document's narrative regarding "The attention layer at a glance" and the explanation that "The self-attention layer incorporates relevant information from previous positions that help process the current token." It visually demonstrates this textual description by showing how the token "it" attends to the token "squirrel" within the first Transformer block. This visual explanation is crucial for understanding how Transformer models, particularly their self-attention mechanism, establish contextual connections between words in a sentence to derive meaning.

**Summary:**
The image illustrates how a Transformer block processes a sequence of words, specifically highlighting the self-attention mechanism. At the top, a sentence, "The dog chased the squirrel because it", is presented as input. Below this, each significant word ("dog", "squirrel", "because", "it") is represented as a token entering a stack of Transformer blocks. The main processing occurs within these blocks, labeled sequentially as "Transformer block 1", "Transformer block 2", and continuing up to "Transformer block N", indicating multiple layers of processing. An orange curved arrow, labeled "Attention", demonstrates that when processing the token "it" within "Transformer block 1", the model draws information from the token "squirrel". This indicates that the "it" in the sentence is referring to the "squirrel". To the right, a detailed view of a single "Transformer block" is shown, revealing its two core components: a "Self-attention" layer (peach-colored box) and a "Feedforward neural network" layer (blue box). This structure demonstrates that each token's representation is refined by considering its context through self-attention, and then further processed by a feedforward network, with this process repeating across multiple transformer blocks.](images/59e2d0f7b480a5f91b1a7d285d50024302bbb2d9992830d6a5011af440718c2b.jpg)
Figure 3-14. The self-attention layer incorporates relevant information from previous positions that help process the current token.

The model does that based on the patterns seen and learned from the training dataset. Perhaps previous sentences also give more clues, like, for example, referring to the dog as “she” thus making it clear that “it” refers to the squirrel.

# Attention is all you need

It is worth diving deeper into the attention mechanism. The most stripped-down version of the mechanism is shown in Figure 3-15. It shows multiple token positions going into the attention layer; the final one is the one being currently processed (the pink arrow). The attention mechanism operates on the input vector at that position. It incorporates relevant information from the context into the vector it produces as the output for that position.

![## Image Analysis: 70f0c775463a46c3cdd599293c95a6ff88a2e9c35503060a10e074dc618f7368.jpg

**Conceptual Understanding:**
This image conceptually represents the 'Self-attention' mechanism, a key component in transformer architectures used in deep learning. Its main purpose is to illustrate how the representation of a specific element (or 'position') in an input sequence is enriched by dynamically integrating contextual information from all other elements within that same sequence. The image communicates the idea that instead of processing each position in isolation, self-attention allows a model to weigh the importance of different parts of the input when processing a particular position, thereby creating a more informed and contextually rich representation for that position.

**Content Interpretation:**
The image illustrates the core mechanism of self-attention. It visually explains how the information for a specific, 'Position currently being processed', is not only derived from its own 'Current position information' but is significantly 'Enriched with context information from other positions' within the same input sequence. The multiple gray lines originating from 'Other positions in the sequence' and feeding into the main sequence representation, alongside the red line specifically for the 'Position currently being processed', demonstrate the attention mechanism's role in gathering relevant contextual data. The transformation from blue 'Current position information' blocks to red 'Enriched with context information from other positions' blocks highlights the outcome of this process: a richer, contextually aware representation of the current position.

**Key Insights:**
The main takeaway is that self-attention mechanisms enhance the representation of a 'Position currently being processed' by incorporating 'context information from other positions' within the input sequence. This process transforms simple 'Current position information' into a more 'Enriched' representation. The diagram implies that each position in a sequence attends to all other positions (including itself) to compute its new representation, thereby capturing long-range dependencies and contextual relationships without relying solely on sequential processing. This mechanism is crucial for understanding how models like transformers can process sequences effectively by weighting the importance of different parts of the input relative to the current position.

**Document Context:**
This image directly supports the document's explanation of the self-attention mechanism, particularly in the context of the section 'Attention is all you need'. It provides a simplified, yet clear, visual framing of how attention works. The accompanying text states, 'A simplified framing of attention: an input sequence and a current position being processed. As we’re mainly concerned with this position, the figure shows an input vector and an output vector that incorporates information from the previous elements in the sequence according to the attention mechanism.' The image precisely depicts this, showing an input (blue blocks for 'Current position information') and an output (red blocks for 'Enriched with context information from other positions') that integrates information from other sequence elements, thereby enhancing the reader's comprehension of this fundamental concept in transformer models.

**Summary:**
The image illustrates the self-attention mechanism by showing how a 'Position currently being processed' integrates 'information from other positions' within a sequence. It depicts a long horizontal beige rectangle, symbolizing an input sequence. Above this rectangle, there are text labels: 'Self-attention' positioned to the left, 'Other positions in the sequence' centrally, and 'Position currently being processed' to the right. Multiple vertical gray lines extend downwards from 'Other positions in the sequence' into the beige rectangle, each ending with a gray downward-pointing arrow, indicating that information from these positions is considered. A distinct vertical red line extends downwards from 'Position currently being processed' into the beige rectangle, also ending with a red downward-pointing arrow. To the right of this red line, two sets of square blocks are shown: three light blue square blocks labeled 'Current position information', representing the initial state of the position being processed. Below these, three red square blocks are labeled 'Enriched with context information from other positions', indicating the output of the self-attention mechanism where the current position's information has been augmented by considering other parts of the sequence. The overall visual arrangement suggests a process where the 'Self-attention' mechanism takes 'Current position information' and enhances it by drawing 'context information from other positions' in the sequence, resulting in 'Enriched with context information from other positions'.](images/70f0c775463a46c3cdd599293c95a6ff88a2e9c35503060a10e074dc618f7368.jpg)
Figure 3-15. A simplified framing of attention: an input sequence and a current position being processed. As we’re mainly concerned with this position, the figure shows an input vector and an output vector that incorporates information from the previous elements in the sequence according to the attention mechanism.

Two main steps are involved in the attention mechanism:

1. A way to score how relevant each of the previous input tokens are to the current token being processed (in the pink arrow).   
2. Using those scores, we combine the information from the various positions into a single output vector.

![## Image Analysis: b449d378851b5121f78d3c19b007f18e1ba0ad3fe4547e9e62a0121a4d4903e1.jpg

**Conceptual Understanding:**
The image conceptually represents the inner workings of a 'Self-attention' mechanism, which is a key component in transformer models for natural language processing. Its main purpose is to illustrate how a single 'Position currently being processed' within a sequence is dynamically contextualized by considering its relevance to all 'Other positions in the sequence'. The core idea communicated is that attention allows a model to weigh the importance of different parts of the input sequence when processing each element, leading to an enriched representation.

**Content Interpretation:**
The image depicts the two fundamental steps involved in a self-attention mechanism, as described by the textual elements 'Relevance scoring' and 'Combining information'. It illustrates how a 'Position currently being processed' is enriched by information from 'Other positions in the sequence'. The 'Current position information' (blue squares) is an input that goes through a 'Relevance scoring' phase with all other positions. This scoring then informs the 'Combining information' step, which integrates the weighted information from all positions. The output, indicated by red squares labeled 'Enriched with context information from other positions', signifies the result of this contextualization.

**Key Insights:**
The main takeaway is that self-attention is a two-step process: first, determining how relevant other positions are to the current one ('Relevance scoring'), and second, integrating that contextually weighted information back into the current position ('Combining information'). This mechanism allows each position in a sequence to be understood in the context of all other positions, leading to an 'Enriched with context information from other positions' output. The clear labels 'Other positions in the sequence', 'Position currently being processed', 'Relevance scoring', and 'Combining information' provide explicit evidence for this sequential and contextual processing.

**Document Context:**
This image, Figure 3-16, directly supports the document's discussion on the self-attention mechanism, specifically by visually representing the two major steps mentioned in the accompanying text: 'relevance scoring for each position, then a step where we combine the information based on those scores.' It clarifies how an individual 'Position currently being processed' interacts with 'Other positions in the sequence' to become 'Enriched with context information from other positions', thereby illustrating the core concept of attention as described in the 'Attention is all you need' section.

**Summary:**
The diagram illustrates the self-attention mechanism, detailing its two core steps: relevance scoring and combining information. It starts with two types of input: 'Other positions in the sequence' and the 'Position currently being processed'. The 'Position currently being processed' is represented by blue squares labeled 'Current position information'. These inputs collectively feed into the 'Relevance scoring' step. Following this, the outputs from relevance scoring, along with the initial inputs, proceed to the 'Combining information' step. The final output, represented by red squares, is labeled 'Enriched with context information from other positions', indicating that the original current position information has been enhanced by integrating context from other parts of the sequence through the self-attention process.](images/b449d378851b5121f78d3c19b007f18e1ba0ad3fe4547e9e62a0121a4d4903e1.jpg)
Figure 3-16 shows these two steps.   
Figure 3-16. Attention is made up of two major steps: relevance scoring for each posi‐ tion, then a step where we combine the information based on those scores.

To give the Transformer more extensive attention capability, the attention mecha‐ nism is duplicated and executed multiple times in parallel. Each of these parallel applications of attention is conducted into an attention head. This increases the model’s capacity to model complex patterns in the input sequence that require paying attention to different patterns at once.

Figure 3-17 shows the intuition of how attention heads run in parallel with a preced‐ ing step of splitting information and a later step of combining the results of all the heads.

![## Image Analysis: 4551b14641d1997930f5bdf18c4c470dfef8aefd525ff85de8219733b4dae5a3.jpg

**Conceptual Understanding:**
This image conceptually represents a single "Self-attention" mechanism, specifically an "Attention head," a fundamental component within transformer models used in natural language processing and other sequence modeling tasks. It illustrates how a model processes a specific element (position) within a sequence by considering its relationship and relevance to all other elements in that same sequence.

The main purpose of this diagram is to demonstrate how an "Attention head" enriches the information of a "Position currently being processed" by scoring its relevance against "Other positions in the sequence" and then combining that contextual information. The core message is that the meaning or representation of a single token/position is not isolated but is deeply influenced and enhanced by its context within the entire sequence.

Key ideas and concepts being communicated include:
*   **Self-attention:** The mechanism that allows elements of a sequence to be weighted and combined based on their interdependencies.
*   **Contextualization:** The process of enhancing an element's representation by incorporating information from its surrounding context.
*   **Relevance Scoring:** A crucial step to dynamically determine the importance of other sequence elements to the current one.
*   **Information Combination:** The subsequent process of integrating the original and contextually relevant information.
*   **Attention head:** A specific computational unit that performs these self-attention operations.

**Content Interpretation:**
The image details the internal processes of a single "Attention head" within a "Self-attention" mechanism. It illustrates how information from a "Position currently being processed" is combined with information from "Other positions in the sequence" to create a contextually richer representation. The key processes shown are:

*   **Input Processing:** The system takes as input "Current position information" for the "Position currently being processed" and information from "Other positions in the sequence."
*   **Relevance scoring:** This is an explicit step where the attention head evaluates the importance or relatedness of each "Other position in the sequence" to the "Position currently being processed." This is indicated by the text "Relevance scoring."
*   **Combining information:** Following relevance scoring, the attention head integrates the original information of the current position with the contextually relevant information from other positions. This step is labeled "Combining information."

The significance of the information presented lies in demonstrating the mechanism by which a single token's representation is enhanced. The output for the "Position currently being processed" is specifically labeled "Enriched with context information from other positions" and is visually distinct (red squares), emphasizing that the attention process successfully incorporates sequential context, thereby improving the understanding of the current position. The distinction between "Current position information" (blue) and the "Enriched with context information" (red) highlights the transformation and value addition.

**Key Insights:**
**Main Takeaways/Lessons:**
*   **Self-attention is a contextualization mechanism:** The diagram explicitly shows how a "Position currently being processed" interacts with "Other positions in the sequence" to become "Enriched with context information from other positions," underscoring self-attention's role in generating context-aware representations.
*   **Attention involves two core, sequential steps:** An "Attention head" systematically performs "Relevance scoring" followed by "Combining information." This reveals the fundamental operations for achieving contextualization, where relevance must first be determined before information is integrated.
*   **Each position's representation is improved by considering its peers:** The output for the "Position currently being processed" is not just its original information but an "Enriched" version, directly demonstrating the benefit and goal of the self-attention mechanism.

**Conclusions/Insights:**
*   The "Attention head" serves as a dedicated computational unit that efficiently integrates global sequence information for each individual element.
*   The concept of "relevance" is central to how context is incorporated, suggesting a learned or calculated importance for each contextual element within the sequence.
*   The process implies that self-attention produces a new, more informed representation for each position, which is critical for subsequent tasks within advanced models like Large Language Models (LLMs).

**Textual Evidence for Insights:**
*   The main labels "Self-attention" and "Attention head" clearly identify the subject and the executor of the process.
*   The input labels "Other positions in the sequence" and "Position currently being processed" establish the scope of interaction among elements.
*   The explicit process steps "Relevance scoring" and "Combining information" provide direct evidence for the two core computational stages.
*   The output label "Enriched with context information from other positions" serves as definitive evidence of the outcome and the value added by the self-attention mechanism, confirming that the representation is enhanced through contextual understanding.
*   The visual distinction between "Current position information" (blue) and "Enriched with context information from other positions" (red) visually reinforces the transformation and added value.

**Document Context:**
This image fits perfectly within the document's narrative, especially given the section title "Attention is all you need" and the subsequent text: "Figure 3-17. We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information." The diagram provides a foundational illustration of how a *single* "Attention head" within a self-attention mechanism works. It is a building block for understanding the broader concept of multi-head attention, which is crucial for the performance of Large Language Models (LLMs) like the Transformer architecture. By detailing the internal workings of one attention head, the image sets the stage for comprehending how parallel attention mechanisms ("multiple times in parallel") can enhance an LLM's ability to process and understand complex sequences by focusing on various aspects of context.

**Summary:**
The diagram illustrates the core process of a "Self-attention" mechanism, specifically detailing the operations performed by an "Attention head." This mechanism is fundamental to how models, especially Large Language Models, understand the context of words or tokens within a sequence.

The process begins with two types of inputs to the "Attention head":
1.  Information from "Other positions in the sequence": These are all the other elements or tokens in the input sequence apart from the one currently being focused on.
2.  Information for the "Position currently being processed": This is the specific element or token whose representation we want to enrich. This "Current position information" is visually represented by light blue squares.

Inside the "Attention head," two primary operations occur:
1.  **Relevance scoring:** The "Attention head" takes the information from both the "Position currently being processed" and "Other positions in the sequence." In this step, it calculates how relevant or important each of the "Other positions" is to the "Position currently being processed." This allows the model to selectively focus on the most pertinent contextual elements.
2.  **Combining information:** After determining the relevance scores, this step integrates the information. It takes the original input from the "Position currently being processed," along with the contextual information from "Other positions in the sequence" (weighted by their relevance scores), and merges them.

The output for the "Position currently being processed" is then "Enriched with context information from other positions." This means the original information about the current position has been enhanced and deepened by incorporating insights and relationships from its surrounding context within the sequence. This "enriched" information is visually represented by red squares and a red arrow, highlighting its transformed nature.

In essence, this diagram explains how self-attention allows a model to understand a word or token by simultaneously looking at all other words in the sentence, weighing their importance (relevance scoring), and then creating a richer, context-aware representation for that word. This ability to integrate context from an entire sequence is a key factor in the superior performance of modern neural networks, particularly in natural language processing.](images/4551b14641d1997930f5bdf18c4c470dfef8aefd525ff85de8219733b4dae5a3.jpg)
Figure 3-17. We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information.

# How attention is calculated

Let’s look at how attention is calculated inside a single attention head. Before we start the calculation, let’s observe the following as the starting position:

• The attention layer (of a generative LLM) is processing attention for a single position.   
• The inputs to the layer are: — The vector representation of the current position or token — The vector representations of the previous tokens   
• The goal is to produce a new representation of the current position that incorpo‐ rates relevant information from the previous tokens: — For example, if we’re processing the last position in the sentence “Sarah fed the cat because it,” we want “it” to represent the cat—so attention bakes in “cat information” from the cat token.

• The training process produces three projection matrices that produce the com‐ ponents that interact in this calculation:

— A query projection matrix — A key projection matrix — A value projection matrix

![## Image Analysis: f5a36e31bd25981bcfc64232a1646a93ec4f979198acf86d62f06ee7055589c9.jpg

**Conceptual Understanding:**
This image conceptually illustrates the input structure and initial components of a single attention head within a self-attention mechanism, a core concept in transformer models for natural language processing. Its main purpose is to show the 'Position currently being processed' and the 'Projection matrices' for query, key, and value that are used to transform this input, ultimately leading to an output 'Enriched with context information from other positions'. The key idea communicated is the preparatory stage of self-attention, where input position information is prepared via specific projection matrices before contextual relationships are computed. It visually defines the elements that will be used in subsequent attention calculations, establishing the 'starting position' for these components.

**Content Interpretation:**
This image displays the foundational components and their relationships within a single attention head for a self-attention mechanism. It shows the input 'Position currently being processed' entering the attention head. Inside 'Attention head #1', three distinct 'Projection matrices' are presented: 'Query projection' (purple grid), 'Key projection' (orange grid), and 'Value projection' (dark blue grid). These matrices are crucial for transforming the input into different representations. The output of this attention head, represented by a red grid, is described as 'Enriched with context information from other positions'. This indicates that the single position being processed gains understanding by considering its relationship with other positions in the sequence, which is the core function of self-attention. The legend further distinguishes between 'Other positions in the sequence' and 'Current position information', using light blue and dark blue grid patterns respectively, though only the dark blue grid is explicitly shown as a 'Value projection' component.

**Key Insights:**
1. The self-attention mechanism operates using individual 'Attention head' components, here specifically 'Attention head #1'.
2. A single position, labeled 'Position currently being processed', serves as input to the attention head.
3. The attention head utilizes three distinct 'Projection matrices' for transforming inputs: 'Query projection', 'Key projection', and 'Value projection'. These are visually represented by different colored grids (purple, orange, dark blue respectively).
4. The output of an attention head is 'Enriched with context information from other positions', highlighting the mechanism's purpose of integrating global context into each position's representation.
5. The legend differentiates between 'Other positions in the sequence' (light blue grid) and 'Current position information' (dark blue grid), indicating different types of positional data considered within the sequence processing.

**Document Context:**
This image directly supports the document's section 'How attention is calculated' by visually detailing the initial state of components. It clarifies that multiple attention heads exist but focuses on one for simplicity, as stated in the text. The diagram precedes the actual calculation steps, setting the stage by showing the inputs (position information) and the essential 'Projection matrices' (for queries, keys, and values) that are prerequisite to the attention mechanism. It explains Figure 3-18 mentioned in the document context, which shows the starting position for all components before the attention calculations start, thus providing a concrete visual foundation for the subsequent discussion on attention computation.

**Summary:**
This image illustrates the initial setup for a single 'Attention head #1' within a 'Self-attention' mechanism, specifically showing the inputs and projection matrices before attention calculations begin. It details how a 'Position currently being processed' interacts with 'Projection matrices' for 'Query', 'Key', and 'Value'. The diagram also clarifies that the output of this head is 'Enriched with context information from other positions'. A legend distinguishes 'Other positions in the sequence' (light blue grid) from 'Current position information' (dark blue grid).](images/f5a36e31bd25981bcfc64232a1646a93ec4f979198acf86d62f06ee7055589c9.jpg)
Figure 3-18 shows the starting position for all of these components before the atten‐ tion calculations start. For simplicity, let’s look at only one attention head because the other heads have identical calculations but with their individual projection matrices.   
Figure 3-18. Before starting the self-attention calculation, we have the inputs to the layer and projection matrices for queries, keys, and values.

Attention starts by multiplying the inputs by the projection matrices to create three new matrices. These are called the queries, keys, and values matrices. These matrices contain the information of the input tokens projected to three different spaces that help carry out the two steps of attention:

1. Relevance scoring   
2. Combining information

![## Image Analysis: 44aa82f0ccfb3769585e478ed200de68c70da90ea8baff999a1496a87e4831bc.jpg

**Conceptual Understanding:**
The image conceptually represents a single attention head within a self-attention mechanism, a core component in Transformer models used in natural language processing and other sequence modeling tasks. Its main purpose is to illustrate how an input position is processed to generate Query, Key, and Value vectors (or matrices) and how these are structured to incorporate information from both the current position and preceding positions in a sequence. The diagram visually explains the initial steps where the input is projected and segmented into its constituent parts that will later interact to compute attention scores and produce contextually enriched representations.

**Content Interpretation:**
The image illustrates the process within a single "Attention head #1" as part of a larger "Self-attention" mechanism. It shows how an input, specifically a "Position currently being processed," is transformed into Query, Key, and Value representations. These representations, derived through "Projection matrices," are then used to integrate information from both "Previous tokens" and the "Current token" within the sequence. The core concept is the generation of context-aware representations by combining the current position's information with information from other positions in the sequence, resulting in an output that is "Enriched with context information from other positions." The diagram signifies the initial steps of how attention computes relevance between different parts of an input sequence.

**Key Insights:**
The main takeaways are: 1. Self-attention mechanisms, specifically within an attention head, process an input "Position currently being processed." 2. This processing involves "Projection matrices" (Query, Key, Value projection) that transform the input into distinct "Queries," "Keys," and "Values" representations. 3. The "Queries," "Keys," and "Values" matrices explicitly separate information from "Previous tokens" and the "Current token" within a sequence. This is evident by the distinct labeling and shading of the rows in these matrices. 4. The output of this attention head is a representation that is "Enriched with context information from other positions," indicating the mechanism's role in incorporating broader sequence context. 5. The legend clarifies the visual encoding: light blue grids represent "Other positions in the sequence," and a dark blue row signifies "Current position information," reinforcing the concept of positional encoding within the matrices.

**Document Context:**
This image is directly relevant to the section titled "How attention is calculated," as mentioned in the document context. It visually depicts the creation of the "queries, keys, and values matrices" which the accompanying text states "are produced by multiplying the layer’s inputs with the projection matrices." Specifically, it shows how the bottom row of the generated Query, Key, and Value matrices is associated with the 'current position' (the 'Current token'), and the rows above it are associated with 'previous positions' (the 'Previous tokens'), aligning perfectly with the textual description of Figure 3-19. This image provides a foundational understanding of the matrix operations and data flow involved in generating these essential components for attention mechanisms.

**Summary:**
The image titled "Self-attention" illustrates the internal workings of an "Attention head #1" within a self-attention mechanism, specifically showing how Query, Key, and Value matrices are generated and used to enrich information. It details the process of creating these matrices from the input, distinguishing between current and previous token information, and highlights how the output is enriched with contextual information. The diagram begins with an input labeled "Position currently being processed" entering the "Attention head #1". Inside this head, there are "Projection matrices" consisting of "Query projection" (purple grid), "Key projection" (orange grid), and "Value projection" (blue grid). These projection matrices are applied to the input to generate the "Queries" (purple grid), "Keys" (orange grid), and "Values" (blue grid). The "Queries" matrix is explicitly segmented into "Previous tokens" (the upper two rows of lighter purple squares) and "Current token" (the bottom row of darker purple squares). Similarly, the "Keys" and "Values" matrices also show a darker bottom row, representing the "Current token" information, and lighter upper rows for "Previous tokens". The overall output from this attention head is an "Enriched with context information from other positions" matrix, represented by red squares. A legend in the top right clarifies that light blue grids represent "Other positions in the sequence", while a single dark blue row represents "Current position information". This visual explanation breaks down how attention mechanisms process an input position by projecting it into different spaces (Queries, Keys, Values) and combining it with information from other positions to create a context-rich representation.](images/44aa82f0ccfb3769585e478ed200de68c70da90ea8baff999a1496a87e4831bc.jpg)
Figure 3-19 shows these three new matrices, and how the bottom row of all three matrices is associated with the current position while the rows above it are associated with the previous positions.   
Figure 3-19. Attention is carried out by the interaction of the queries, keys, and val‐ ues matrices. Those are produced by multiplying the layer’s inputs with the projection matrices.

# Self-attention: Relevance scoring

In a generative Transformer, we’re generating one token at a time. This means we’re processing one position at a time. So the attention mechanism here is only concerned with this one position, and how information from other positions can be pulled in to inform this position.

The relevance scoring step of attention is conducted by multiplying the query vector of the current position with the keys matrix. This produces a score stating how relevant each previous token is. Passing that by a softmax operation normalizes these scores so they sum up to 1. Figure 3-20 shows the relevance score resulting from this calculation.

![## Image Analysis: 293a6aac8341cc87de98135e66bfbc74e09d88d54fc2d45ce27b15745a1fe43c.jpg

**Conceptual Understanding:**
This image conceptually represents a single attention head within a self-attention mechanism, a fundamental component in transformer models used in natural language processing. Its main purpose is to illustrate how a 'Position currently being processed' (or 'Current token') determines its contextual relevance to 'Other positions in the sequence'. This is achieved by calculating 'Relevance scores', which essentially indicate how much 'attention' the current token should pay to each of the other tokens. The diagram shows the inputs ('Queries' derived from the 'Current token') and how they interact with 'Keys' (derived from other tokens) to produce a weighted distribution of importance.

**Content Interpretation:**
The image illustrates the core mechanism of a single attention head within a self-attention layer, specifically focusing on how relevance scores are computed. It demonstrates the conceptual steps of generating queries from a current token, multiplying them with keys from other tokens in the sequence, and then outputting a distribution of relevance scores. This process essentially quantifies how much attention the 'Current token' should pay to each of the other tokens in the sequence, thereby enriching its representation with contextual information. The 

**Key Insights:**
The main takeaway is that self-attention mechanisms quantify the importance of other words in a sequence to a specific 'Current token' by calculating 'Relevance scores'. This is achieved through a multiplication operation between 'Queries' generated by the current token and 'Keys' representing other tokens. The diagram demonstrates that different tokens can have varying degrees of relevance (e.g., 'cat' 50% vs. 'Sarah' 3%), implying a focused attention mechanism. The process culminates in the 'Current token' being 'Enriched with context information from other positions', highlighting how self-attention integrates context. The text 'Projection matrices' also indicates that input representations are transformed before queries and keys are formed.

**Document Context:**
This image directly follows and explains the preceding text, "Figure 3-20. Scoring the relevance of previous tokens is accomplished by multiplying the query associated with the current position with the keys matrix." It visually details the mathematical operation mentioned, showing how the 'Queries' from the 'Current token I' interact with the 'Keys' (representing other tokens) to produce 'Relevance scores'. This reinforces the document's explanation of self-attention, providing a clear visual breakdown of the scoring process and how contextual information is derived. The image serves as a concrete example of the abstract concept of relevance scoring in self-attention mechanisms.

**Summary:**
The image illustrates the process of how a single 'Attention head #1' within a self-attention mechanism calculates relevance scores for other tokens in a sequence relative to a 'Current token'. The overall process begins with a 'Position currently being processed'. This information enters the 'Attention head #1', which contains 'Projection matrices' (depicted as purple, orange, and blue grids). To score relevance, the 'Current token I' (represented by a purple block) generates 'Queries'. These 'Queries' are then multiplied by 'Keys' (represented by an orange grid block). The result of this multiplication is a set of 'Relevance scores' for various words in the sequence. For example, 'Sarah' receives 3%, 'fed' 5%, 'the' 35%, 'cat' 50%, 'because' 2%, and 'it' 5%. These scores indicate the importance or relevance of each of these words to the 'Current token I'. Finally, the output of this head is 'Enriched with context information from other positions', which is visually represented by a red grid, signifying the integration of these relevance scores to create a more informed representation of the current token.](images/293a6aac8341cc87de98135e66bfbc74e09d88d54fc2d45ce27b15745a1fe43c.jpg)
Figure 3-20. Scoring the relevance of previous tokens is accomplished by multiplying the query associated with the current position with the keys matrix.

# Self-attention: Combining information

Now that we have the relevance scores, we multiply the value vector associated with each token by that token’s score. Summing up those resulting vectors produces the output of this attention step, as we see in Figure 3-21.

![## Image Analysis: 2de435cc46defbf70190806dd33841a35a9c73d10fe89ba63b9ba47077951154.jpg

**Conceptual Understanding:**
This image conceptually illustrates the core operation of a single attention head within a self-attention mechanism. Its main purpose is to demonstrate how a particular position (e.g., a word in a sentence) is understood in the context of other positions in the sequence. It achieves this by calculating how 'relevant' other positions are, and then using these relevance scores to create a weighted average of their 'values' (their inherent information). The key idea communicated is that the final representation of a word is not just its isolated meaning, but a rich blend of its meaning combined with information from the most relevant words around it, dynamically determined by the attention mechanism.

**Content Interpretation:**
The image depicts the computational process within a single attention head of a self-attention mechanism. It shows how the representation of a 'Position currently being processed' is enriched by combining information from other positions in the sequence. This involves calculating 'Relevance scores' for each other position relative to the current one, multiplying these scores by the 'Values' (contextual information) of those positions, and then summing these weighted values. The significance lies in the dynamic weighting, where highly relevant words (e.g., 'cat' at 50% and 'the' at 35%) contribute more to the final contextual representation than less relevant words (e.g., 'Sarah' at 3% or 'because' at 2%). The visual representation of 'Values' as 'Current position information' and the final 'Sum' as 'Enriched with context information from other positions' clearly illustrates the transformation and aggregation of data to produce a context-aware output.

**Key Insights:**
The main takeaways from this image are: 1. Self-attention mechanisms calculate 'Relevance scores' to quantify the importance of other words in a sequence to a word currently being processed. 2. These relevance scores are used to perform a weighted sum of the 'Values' (contextual information) associated with each word in the sequence. 3. The output of this process is a contextualized representation for the processed word, which has been 'Enriched with context information from other positions,' highlighting how information from the entire sequence is distilled and focused. The specific percentages for 'Relevance scores' (e.g., 'cat: 50%', 'the: 35%') provide concrete evidence for how varying degrees of importance are assigned, and the 'x' and 'Sum' operations explicitly show the mathematical aggregation of these weighted values.

**Document Context:**
This image directly supports the document's section on "Self-attention: Combining information" and the subsequent text, "Attention combines the relevant information of previous positions by multiplying their relevance scores by their respective value vectors." It visually breaks down this abstract concept into a clear, step-by-step process, illustrating how an attention head specifically processes an input to produce a contextually enriched output. The diagram explains the mechanism by which information from different parts of a sequence is weighted and integrated, which is crucial for understanding how Transformer models process sequences.

**Summary:**
The image illustrates the inner workings of an "Attention head #1" within a "Self-attention" mechanism, which is a core component of Transformer models used in natural language processing. The primary purpose is to demonstrate how a "Position currently being processed" gathers and integrates contextual information from other positions in a sequence. The process begins with an input labeled "Position currently being processed." Inside the "Attention head #1" frame, two main components are shown: "Relevance scores" and "Values." For the "Position currently being processed," the mechanism determines "Relevance scores" for other words in the sequence: "Sarah" (3%), "fed" (5%), "the" (35%), "cat" (50%), "because" (2%), and "it" (5%). These scores indicate how much each word contributes to understanding the current position. Simultaneously, "Values," represented by six solid blue blocks (each signifying "Current position information"), are associated with these words. The diagram shows a multiplication symbol "x" between the "Relevance scores" and "Values," indicating that each relevance score is multiplied by its corresponding value. The result of these multiplications is represented by six lighter blue, partially filled blocks. Subsequently, these weighted values are aggregated via a "Sum" operation. The final output of this summation is represented by a red solid block, labeled as "Enriched with context information from other positions." This output is a contextualized representation of the initially processed position, incorporating relevant information from the entire sequence. The legend clarifies the visual elements: a light blue grid represents "Other positions in the sequence," solid blue blocks represent "Current position information," and solid red blocks represent "Enriched with context information from other positions."](images/2de435cc46defbf70190806dd33841a35a9c73d10fe89ba63b9ba47077951154.jpg)
Figure 3-21. Attention combines the relevant information of previous positions by multi‐ plying their relevance scores by their respective value vectors.

# Recent Improvements to the Transformer Architecture

Since the release of the Transformer architecture, much work has been done to improve it and create better models. This spans training on larger datasets and opti‐ mizations for the training process and learning rates to use, but it also extends to the architecture itself. At the time of writing, a lot of the ideas of the original Transformer stand unchanged. There are a few architectural ideas that have proved to be valuable. They contribute to the performance of more recent Transformer models like Llama 2. In this final section of the chapter, we go over a number of the important recent developments of the Transformer architecture.

# More Efficient Attention

The area that gets the most focus from the research community is the attention layer of the Transformer. This is because the attention calculation is the most computation‐ ally expensive part of the process.

# Local/sparse attention

As Transformers started getting larger, ideas like sparse attention (“Generating long sequences with sparse transformers”) and sliding window attention (“Longformer: The long-document transformer”) provided improvements for the efficiency of the attention calculation. Sparse attention limits the context of previous tokens that the model can attend to, as we can see in Figure 3-22.

![## Image Analysis: 2ad8d0eb903c340914ab315fa960646d034e2ba12f38bfb42277fc8e0601690c.jpg

**Conceptual Understanding:**
This image represents and illustrates the conceptual difference between two methods of implementing autoregressive self-attention within a Transformer's self-attention layer: global and local. The main purpose is to visually demonstrate how the scope of attention to 'Input tokens' differs between these two approaches. The key ideas communicated are that global attention considers all prior context, while local attention constrains this context to a limited, recent window of tokens, impacting computational requirements and the range of dependencies captured.

**Content Interpretation:**
The image illustrates the operational scope of two distinct autoregressive self-attention mechanisms within a Transformer model. The 'Global autoregressive self-attention' (left side) demonstrates a mechanism where the self-attention layer for a given output token (indicated by the vertical dotted line at token 7) computes its attention over *all* preceding input tokens (from 1 to 7). This is visually represented by all 'Input tokens' 1-7 being active (blue-filled) and the dark blue attention region covering the entire historical context for the current position.

In contrast, the 'Local autoregressive self-attention' (right side) depicts a mechanism where the self-attention layer, for the same output token (indicated by the vertical dotted line at token 7), only considers a *limited, local window* of preceding input tokens (specifically, tokens 4 through 7, as tokens 1-3 are grayed out). This is visually represented by only tokens 4-7 being active (blue-filled) and the dark blue attention region starting its base at the position corresponding to token 4, extending to token 7. The grayed-out tokens 1-3 signify that these earlier positions are excluded from the attention calculation for the current token.

**Key Insights:**
1.  **Global Attention Scope:** Global autoregressive self-attention considers all preceding tokens in the input sequence (1 to 7) when computing attention for the current token (evidenced by all 'Input tokens' 1-7 being active and the attention region covering the entire sequence history). This implies a high computational cost but potentially captures long-range dependencies.
2.  **Local Attention Scope:** Local autoregressive self-attention restricts its attention to a defined, recent window of preceding tokens (e.g., 4 to 7) for the current token, effectively ignoring earlier tokens (evidenced by 'Input tokens' 1-3 being gray-outlined and the attention region starting from token 4). This significantly reduces computational complexity and memory usage.
3.  **Efficiency Implication:** The visual representation of local attention highlights its efficiency by showing a smaller active set of tokens and a reduced attention scope, which aligns with the stated benefit of 'boosting performance by only paying attention to a small number of previous positions'.
4.  **Architectural Choice:** The diagram demonstrates a fundamental design choice in Transformer architectures regarding how much context (how many previous tokens) the self-attention mechanism is allowed to access. Global attention provides a full context, while local attention trades off full context for computational efficiency.

**Document Context:**
This image directly supports the document's section on 'Local/sparse attention' by providing a clear visual comparison between global and local autoregressive self-attention. It visually explains the core concept that 'Local attention boosts performance by only paying attention to a small number of previous positions.' The diagram uses the specific textual elements 'Input tokens' and 'Transformer self-attention layer' to ground the visual representation in the context of Transformer models. The labels 'Global autoregressive self-attention' and 'Local autoregressive self-attention' explicitly name the two concepts being compared, directly aligning with the section's focus. The visual differentiation of active (blue) vs. inactive (gray) tokens and the differing start points of the dark blue attention triangles directly illustrate the 'small number of previous positions' concept.

**Summary:**
The image visually compares two types of self-attention mechanisms in a Transformer model: Global autoregressive self-attention and Local autoregressive self-attention. The diagram is divided into two main sections, one for each attention type, arranged horizontally. Both sections feature two rows: 'Input tokens' at the top and 'Transformer self-attention layer' below it. 

For 'Global autoregressive self-attention' (left side):
- 'Input tokens' are depicted as seven blue-filled, rounded rectangular blocks, labeled sequentially from left to right with numbers '1', '2', '3', '4', '5', '6', '7'. All tokens are active and considered.
- The 'Transformer self-attention layer' below shows a large light blue rectangle. Within this, a dark blue triangular shaded region starts from the bottom-left corner and extends diagonally upwards to the right, covering the entire lower-left portion of the rectangle. A vertical dotted line extends from the top of token '7' down to the bottom of the self-attention layer, indicating the current position being attended to. The dark blue triangle's apex touches this dotted line, demonstrating that attention is paid to all preceding tokens up to the current position.

For 'Local autoregressive self-attention' (right side):
- 'Input tokens' are depicted as seven rounded rectangular blocks. The first three tokens (labeled implicitly as '1', '2', '3' by their position relative to the numbers '4', '5', '6', '7' that are explicitly shown) are gray-outlined and unfilled, indicating they are not actively considered. The subsequent four tokens are blue-filled and labeled '4', '5', '6', '7'. This illustrates that only a subset of recent tokens is active.
- The 'Transformer self-attention layer' below also shows a light blue rectangle. Within this, a dark blue triangular shaded region starts horizontally aligned with token '4' (specifically, aligned with the left edge of token '4's position in the 'Input tokens' row, but within the self-attention layer rectangle) and extends diagonally upwards to the right. A vertical dotted line extends from the top of token '7' down to the bottom of the self-attention layer, again indicating the current position being attended to. The dark blue triangle's apex touches this dotted line, but its base only starts at the position corresponding to token '4', showing that attention is restricted to tokens within a local window (e.g., from token 4 to 7).

Both diagrams include the labels 'Global autoregressive self-attention' and 'Local autoregressive self-attention' at their respective bases.](images/2ad8d0eb903c340914ab315fa960646d034e2ba12f38bfb42277fc8e0601690c.jpg)
Figure 3-22. Local attention boosts performance by only paying attention to a small number of previous positions.

One model that incorporates such a mechanism is GPT-3. But it does not use that for all the Transformer blocks—the quality of the generation would vastly degrade if the model could only see a small number of previous tokens. The GPT-3 architec‐ ture interweaved full-attention and efficient-attention Transformer blocks. So the Transformer blocks alternate between full attention (e.g., blocks 1 and 3) and sparse attention (e.g., blocks 2 and 4).

To demonstrate different kinds of attention, review Figure 3-23, which shows how different attention mechanisms work. Each figure shows which previous tokens (light blue) can be attended to when processing the current token (in dark blue).

![## Image Analysis: cc688c0bfd489ffca33e6e87c748f251ab10b29228841989186447421350258f.jpg

**Conceptual Understanding:**
This image conceptually illustrates different attention patterns used in Transformer neural network architectures. Its main purpose is to visually compare the 'full attention' mechanism of a standard Transformer with two specific 'sparse attention' mechanisms: 'strided' and 'fixed' patterns. The key idea being communicated is how sparsity can be introduced into attention computations to potentially reduce computational complexity and enable the processing of longer sequences, by selectively attending to only a subset of previous tokens instead of all of them.

**Content Interpretation:**
The image illustrates three distinct patterns of attention mechanisms, specifically within the context of Transformer models. (a) represents a 'Transformer' using full attention, where every position in a sequence attends to all previous positions (and itself), resulting in a dense lower triangular matrix of attention. The darker blue diagonal signifies self-attention, and the lighter blue fill indicates attention to all preceding elements. (b) depicts a 'Sparse Transformer (strided)' where attention is limited to the current position (dark blue diagonal) and specific preceding positions that are 'strided' or regularly spaced. This is visualized by columns of light blue squares appearing at regular intervals below the diagonal, leaving many other positions (gray squares) unattended. (c) shows a 'Sparse Transformer (fixed)', where attention is also sparse but follows a 'fixed' pattern, meaning attention is directed to specific, predefined, non-strided blocks or clusters of preceding positions (light blue squares), along with self-attention (dark blue diagonal). The gray squares in (b) and (c) consistently represent positions where no attention is computed, demonstrating the reduction in computational load compared to full attention. The different shades of blue likely differentiate between self-attention (dark blue) and attention to other preceding elements (light blue), while gray denotes no attention.

**Key Insights:**
The main takeaway from this image is that while a standard Transformer utilizes a computationally intensive 'full attention' mechanism where every element attends to all preceding elements, 'Sparse Transformers' offer more efficient alternatives. Specifically, two types of sparse attention are demonstrated: 'strided' attention, which involves attending to regularly spaced previous elements, and 'fixed' attention, which focuses on predefined blocks of previous elements. Both sparse methods significantly reduce the number of attention connections compared to full attention. The visual evidence from the images—the dense lower triangle in (a) versus the scattered and patterned blue squares in (b) and (c)—clearly illustrates this reduction in connectivity, which implies reduced computational cost and memory usage for processing long sequences. The labels (a) 'Transformer', (b) 'Sparse Transformer (strided)', and (c) 'Sparse Transformer (fixed)' explicitly name and differentiate these attention strategies, providing direct textual evidence for the distinct approaches to managing attention sparsity.

**Document Context:**
This image directly supports the document's section on 'Local/sparse attention' by providing clear visual examples of full attention versus two specific types of sparse attention. It helps readers understand the conceptual differences between these mechanisms. The image's source, 'Generating long sequences with sparse transformers', further indicates its relevance to the discussion of how sparse attention can be used to handle longer sequences more efficiently than traditional full attention, which incurs a quadratic computational cost. By visually contrasting the dense attention matrix with the sparse ones, the image effectively illustrates the core principle of reducing computational complexity by selectively attending to fewer input elements.

**Summary:**
The image displays three grid-like matrices, each representing a different attention mechanism in Transformer models: (a) Transformer (full attention), (b) Sparse Transformer (strided attention), and (c) Sparse Transformer (fixed attention). Each matrix is composed of small squares, where lighter blue, dark blue, and gray squares indicate different states of attention or lack thereof. The matrices are 12x12 squares. (a) 'Transformer' shows a dense lower triangular matrix in light blue, with a dark blue diagonal. This visually represents full attention where each element attends to all preceding elements in the sequence, including itself. (b) 'Sparse Transformer (strided)' shows a sparse pattern of attention. The dark blue diagonal is present, indicating self-attention. Additionally, there are specific columns of light blue squares appearing at regular, strided intervals extending downwards from the diagonal, indicating that elements attend to distant but regularly spaced prior elements. Many squares in the lower triangle are gray, signifying no attention. (c) 'Sparse Transformer (fixed)' also displays a sparse attention pattern. Similar to (b), it has a dark blue diagonal for self-attention. The lighter blue squares here are concentrated in specific, fixed blocks or clusters below the diagonal, indicating that attention is focused on a predefined, non-strided set of prior elements. As in (b), many squares are gray, denoting positions where no attention is computed. The overarching theme is to visually compare the full, dense attention of a standard Transformer with two different strategies of sparse attention, highlighting how sparsity reduces the number of attended positions.](images/cc688c0bfd489ffca33e6e87c748f251ab10b29228841989186447421350258f.jpg)
Figure 3-23. Full attention versus sparse attention. Figure 3-24 explains the coloring. (Source: “Generating long sequences with sparse transformers”.)

Each row corresponds to a token being processed. The color coding indicates which tokens the model is able to pay attention to while it’s processing the token in the dark blue cell. Figure 3-24 describes this with more clarity.

![## Image Analysis: dc17de364261ccd0a453c6a02e81b2d6060c41e44195d11bc7b9663a8e5e46d6.jpg

**Conceptual Understanding:**
This image conceptually represents an attention mechanism within the field of natural language processing, specifically illustrating how a model processes a sequence of tokens (words) and determines which prior tokens it can "attend" to for contextual understanding. The main purpose is to visualize a causal or masked attention pattern, where each token being processed can only leverage information from itself and the tokens that have already occurred in the sequence, not future tokens. The key idea communicated is the sequential and cumulative nature of context building in such an attention model, showing how the "attention window" expands with each processed token, but always looking backward.

**Content Interpretation:**
The image presents a visual representation of a "masked" or "causal" attention mechanism commonly used in sequential data processing, particularly in Transformer models for Natural Language Processing. It depicts how an attention mechanism allows a model to consider a limited context (past tokens and the current token) when processing each element in a sequence.

Specifically, it shows the processing of a three-word sentence: "The dog chased".

- **For Token 1 ("The")**: The system focuses only on "The" (represented by the dark blue cell). The preceding tokens column shows no prior context.
- **For Token 2 ("dog")**: The system focuses on "dog" (dark blue cell) and can also attend to the preceding token "The" (light blue cell).
- **For Token 3 ("chased")**: The system focuses on "chased" (dark blue cell) and can attend to the preceding tokens "The" and "dog" (light blue cells).

The varying shades of blue likely indicate the current token being processed (dark blue) and the tokens it can attend to (lighter blue), highlighting the accumulating context. The grey cells indicate tokens that are not yet processed or are outside the attention window (future tokens). The labels "1) The token being processed" and "2) Tokens it can pay attention to" clarify the function of the columns and the mechanism being illustrated.

**Key Insights:**
The main takeaway from this image is the clear illustration of a causal or masked attention mechanism. Key insights include:

1.  **Sequential Context Building**: As each token in a sequence is processed, its available context (the tokens it can "attend" to) expands to include all previously processed tokens and itself, but never future tokens. This is evidenced by the growing number of blue cells from Token 1 to Token 3, always remaining within the upper-left triangular section of the matrix.
2.  **Focus on Current Token**: The dark blue cells consistently highlight "1) The token being processed", indicating the current focus of the attention mechanism.
3.  **Limited Attention Window**: The image explicitly shows that tokens can only "pay attention to" past tokens and themselves, as indicated by the labels and the pattern of shaded cells. The grey cells signify that future tokens are not considered.
4.  **Application in Language Models**: This mechanism is fundamental in models that process sequences left-to-right, such as autoregressive language models, where predicting the next word depends only on the words that have already been generated. The exact text "Token 1 The", "Token 2 The dog", "Token 3 The dog chased" combined with the labels "2) Tokens it can pay attention to" and "1) The token being processed" clearly demonstrate these principles.

**Document Context:**
This image directly relates to the document's section on "Local/sparse attention". It serves as a foundational example of a specific type of local attention mechanism – causal attention. By visually demonstrating how attention is constrained to only preceding tokens (and the current token) in a sequence, it illustrates a core concept of local attention where the context window is limited rather than global. The image provides a concrete example that helps readers understand how an attention mechanism allows a model to build context for each token while adhering to a sequential processing order, crucial for tasks like language generation where future tokens are not yet known.

**Summary:**
This image illustrates a causal attention mechanism in natural language processing, where each token being processed can only attend to itself and the preceding tokens in the sequence. The visual is a 3x3 grid, representing three processing steps for the sentence "The dog chased".

- When processing "Token 1", which is the word "The", it can only attend to itself. This is shown by the top-left cell containing "The" being dark blue, while the cells to its right are light grey, indicating no attention.

- When processing "Token 2", which is the word "dog", it can attend to "The" (Token 1) and "dog" (itself). This is represented by the cells in the second row, up to and including "dog", being blue. Specifically, the cell with "The" is light blue, and the cell with "dog" is dark blue.

- When processing "Token 3", which is the word "chased", it can attend to "The" (Token 1), "dog" (Token 2), and "chased" (itself). This is shown by the entire third row, containing "The", "dog", and "chased", being blue, with "chased" being dark blue, and "The" and "dog" being light blue.

The arrows at the bottom clarify the columns: the left arrow points to the column for "The" and is labeled "2) Tokens it can pay attention to", indicating the preceding tokens. The right arrow points to the column for the token currently being processed (e.g., "chased") and is labeled "1) The token being processed". This visually demonstrates how the context for a token grows as the sequence is processed, but always remains limited to past tokens.](images/dc17de364261ccd0a453c6a02e81b2d6060c41e44195d11bc7b9663a8e5e46d6.jpg)
Figure 3-24. Attention figures show which token is being processed, and which previous tokens an attention mechanism allows it to attend to.

This figure also shows the autoregressive nature of decoder Transformer blocks (which make up most text generation models); they can only pay attention to previ‐ ous tokens. Contrast this to BERT, which can pay attention to both sides (hence the B in BERT stands for bidirectional).

# Multi-query and grouped-query attention

A more recent efficient attention tweak to the Transformer is grouped-query atten‐ tion (“GQA: Training generalized multi-query transformer models from multi-head checkpoints”), which is used by models like Llama 2 and 3. Figure 3-25 shows these different types of attention, and the next section continues to explain them.

![## Image Analysis: b630ffa4d3a130cdd018c661c87c2a04ebc79c2eb211e89cab7a496acfad0d0d.jpg

**Conceptual Understanding:**
This image conceptually represents the structural differences between Multi-head, Grouped-query, and Multi-query attention mechanisms in neural networks, particularly transformers. The main purpose is to visually compare how 'Values', 'Keys', and 'Queries' interact and are scaled across these three distinct approaches, illustrating the efficiency improvements achieved by sharing 'Keys' and 'Values' among 'Queries'. It communicates the idea that by altering the relationship between the number of 'Keys'/'Values' and 'Queries', one can optimize computational resources, especially for inference (decoding).

**Content Interpretation:**
The image illustrates the architectural differences in how 'Values', 'Keys', and 'Queries' are structured and associated across three attention mechanisms: Multi-head, Grouped-query, and Multi-query. This depicts a progression from individual attention heads with dedicated key-value pairs (Multi-head) to an increased sharing of key-value pairs among queries (Grouped-query) and, finally, to a single shared key-value pair for all queries (Multi-query). The number of 'Values' and 'Keys' indicates the computational overhead and memory footprint, with a clear reduction from Multi-head to Multi-query attention for a consistent number of 'Queries'.

**Key Insights:**
The main takeaway from this image is the visual comparison of the architectural strategies for handling 'Keys' and 'Values' relative to 'Queries' in different attention mechanisms, aimed at improving efficiency. 'Multi-head' attention involves a one-to-one mapping for each of the ten 'Keys', 'Values', and 'Queries', suggesting high parallelism and potentially higher computational cost and memory. 'Grouped-query' attention introduces a level of sharing, where four 'Keys' and 'Values' serve ten 'Queries', reducing resources while maintaining some distinct key-value processing. 'Multi-query' attention represents the most efficient approach in terms of key-value memory, with a single 'Key' and 'Value' serving all ten 'Queries'. This illustrates a direct trade-off between the distinctness of attention operations and the computational/memory efficiency. The consistent number of 'Queries' (ten) across all three types highlights how the mechanisms optimize the 'Keys' and 'Values' components.

**Document Context:**
This image serves as a critical visual aid in a document section discussing 'Multi-query and grouped-query attention'. It directly supports the textual explanation by offering a clear, comparative visual representation of the three attention types, which are fundamental to understanding transformer architectures. The comparison of how 'Values', 'Keys', and 'Queries' are managed across these methods is essential for grasping the trade-offs in performance and efficiency, particularly in the context of 'Fast transformer decoding' as mentioned in the accompanying text. It helps readers visualize the memory and computation reduction strategies employed in grouped-query and multi-query attention compared to the standard multi-head attention.

**Summary:**
The image is a conceptual diagram that visually compares three different types of attention mechanisms: Multi-head, Grouped-query, and Multi-query. It illustrates the relationships between 'Values', 'Keys', and 'Queries' for each mechanism. Each component (Values, Keys, Queries) is represented by capsule-like shapes. The connections between Keys and Queries are shown using dashed lines to depict their mapping. The diagram progresses from left to right, showcasing decreasing parallelism in Keys and Values relative to Queries. For 'Multi-head' attention, there are ten distinct 'Values' and ten distinct 'Keys', each of which connects to its corresponding one of the ten 'Queries'. In 'Grouped-query' attention, the number of 'Values' and 'Keys' is reduced to four each, while there are still ten 'Queries'. Each of the four 'Keys' connects to a group of 'Queries'. Specifically, the first Key connects to the first three Queries, the second Key connects to the next three Queries, the third Key connects to the subsequent two Queries, and the fourth Key connects to the final two Queries. Finally, for 'Multi-query' attention, there is only one 'Value' and one 'Key', which connects to all ten 'Queries'. This visually demonstrates the sharing of Key-Value pairs across multiple queries, which has implications for computational efficiency and memory usage in transformer architectures.](images/b630ffa4d3a130cdd018c661c87c2a04ebc79c2eb211e89cab7a496acfad0d0d.jpg)
Figure 3-25. A comparison of different kinds of attention: the original multi-head, grouped-query attention, and multi-query attention (source: “Fast transformer decod‐ ing: One write-head is all you need”).

Grouped-query attention builds on multi-query attention (“Fast transformer decod‐ ing: One write-head is all you need”). These methods improve inference scalability of larger models by reducing the size of the matrices involved.

# Optimizing attention: From multi-head to multi-query to grouped query

Earlier in the chapter we showed how the Transformer paper described multi-headed attention. The Illustrated Transformer discusses in detail how the queries, keys, and values matrices are used to conduct the attention operation. Figure 3-26 shows how each “attention head” has its own distinct query, key, and value matrices calculated for a given input.

The way that multi-query attention optimizes this is to share the keys and values matrices between all the heads. So the only unique matrices for each head would be the queries matrices, as we can see in Figure 3-27.

![## Image Analysis: d79e2d7fbcde597e6c08eb784667514ebd5cedc2278c8d9f93ba9986a9cb4636.jpg

**Conceptual Understanding:**
The image conceptually represents the 'Self-attention' mechanism, a key component in modern neural network architectures, particularly transformer models. Its main purpose is to illustrate how an input, specifically a single 'Position currently being processed,' is transformed by attending to itself and other positions within a sequence. The diagram visually breaks down the process into stages: first, splitting the input into multiple parallel 'Attention heads,' then processing 'Queries, Keys, and Values' independently within each head, and finally combining the results. The core idea communicated is that by performing attention with multiple 'heads' and then consolidating their outputs, the model can capture a more comprehensive and 'Enriched with context information from other positions' representation of the input, enabling it to weigh the relevance of different parts of the input sequence to the current position.

**Content Interpretation:**
The image illustrates the 'Self-attention' mechanism, a core component in neural networks like transformers. It details how input information, specifically a 'Position currently being processed,' is transformed. The process begins by 'Split[ting] into heads,' signifying the creation of multiple independent attention mechanisms (Attention head #1, #2, #3). Each 'Attention head' concurrently processes the input to generate 'Queries,' 'Keys,' and 'Values.' While 'Attention head #1' explicitly shows separate 'Queries,' 'Keys,' and 'Values' matrices, 'Attention head #2' and 'Attention head #3' indicate they each utilize 'Queries, keys, values' collectively, suggesting each head performs the full QKV computation. This parallel processing allows the model to attend to different aspects of the input simultaneously. Finally, the information generated by all these independent heads is gathered and integrated in the 'Combine information from all heads' step, leading to an output that is 'Enriched with context information from other positions.' This enrichment implies that the current position's representation has been enhanced by incorporating relevant information from other parts of the input sequence.

**Key Insights:**
The main takeaway from this image is a detailed understanding of the 'Self-attention' mechanism, specifically illustrating the 'multi-head attention' paradigm. It shows that processing a 'Position currently being processed' involves first a 'Split into heads' operation, allowing for parallel computations across multiple independent 'Attention head' instances. Each head independently generates 'Queries, Keys, and Values,' as evidenced by the distinct labels for 'Attention head #1' and the combined labels for 'Attention head #2' and '#3.' This parallel processing is critical for capturing diverse contextual relationships. Finally, the outputs of these individual heads are aggregated through the 'Combine information from all heads' step to produce a richer, more comprehensive representation of the original position, which is 'Enriched with context information from other positions.' This emphasizes the role of self-attention in incorporating global context into local representations.

**Document Context:**
The image is situated within a document section titled 'Optimizing attention: From multi-head to multi-query to grouped query' and is specifically referenced as 'Figure 3-26. Attention is conducted using matrices of queries, keys, and values. In multi-head attention, each head has a distinct version of each of these matrices.' This context clearly establishes the image's role in explaining the foundational concept of multi-head attention as a precursor or component of more optimized attention mechanisms. The image precisely illustrates how multi-head attention works by showing the 'Split into heads' and the subsequent processing of 'Queries, Keys, and Values' within each head, directly supporting the textual explanation that 'each head has a distinct version of each of these matrices.' It visually clarifies the parallel processing and eventual combination of information, which is central to understanding the improvements offered by multi-query or grouped query attention later in the document.

**Summary:**
The image illustrates the process of 'Self-attention,' a mechanism fundamental to transformer models, particularly in the context of processing a 'Position currently being processed.' The input, which is 'Current position information' (represented by blue squares in the legend), first undergoes a step to 'Split into heads.' This splitting action generates multiple 'Attention head' instances. The diagram specifically shows 'Attention head #1,' 'Attention head #2,' and 'Attention head #3.' Each head processes the input differently using 'Queries,' 'Keys,' and 'Values' matrices. For 'Attention head #1,' these are depicted separately as distinct 'Queries' (purple grid), 'Keys' (orange grid), and 'Values' (blue grid). However, for 'Attention head #2' and 'Attention head #3,' the combined set of 'Queries, keys, values' (represented by overlapping purple, orange, and blue grids) is shown, indicating that each head works with all three components. After these parallel processing steps, the information from all heads is then integrated in a step labeled 'Combine information from all heads.' The final output of this self-attention process is 'Enriched with context information from other positions,' visually represented by red squares, which the legend confirms as 'Enriched with context information from other positions.' The entire process flow moves from an input 'Position currently being processed' at the top, through the internal 'Split into heads' and attention heads, to 'Combine information from all heads,' resulting in the contextually enriched output at the bottom.](images/d79e2d7fbcde597e6c08eb784667514ebd5cedc2278c8d9f93ba9986a9cb4636.jpg)
Figure 3-26. Attention is conducted using matrices of queries, keys, and values. In multi-head attention, each head has a distinct version of each of these matrices.

![## Image Analysis: 252017fce5a41978b890befec909ece0093f74bf520c9f0999c026e59fd746a5.jpg

**Conceptual Understanding:**
This image conceptually illustrates the architecture and data flow of a Multi-query attention mechanism, a variant of the self-attention layer commonly found in Transformer models. Its main purpose is to demonstrate how this specific attention mechanism processes input to generate contextually enriched output more efficiently than standard multi-head attention. The core idea conveyed is the sharing of 'Keys' and 'Values' matrices among different 'Attention head' units while maintaining separate 'Queries' for each head, which is a key optimization strategy.

**Content Interpretation:**
The image depicts the architecture of a Multi-query attention mechanism within a 'Self-attention' module. It illustrates how an input, labeled as 'Current position information', is processed. The key components shown are the initial 'Split into heads' operation, followed by the sharing of 'Keys' and 'Values' matrices. This shared resource is then used by multiple independent 'Attention head #1', 'Attention head #2', and 'Attention head #3', each of which processes its own 'Queries'. The final step is to 'Combine information from all heads', leading to an output that is 'Enriched with context information from other positions'. The significance of this design is the explicit sharing of 'Keys' and 'Values' across heads, which is highlighted by the 'Shared:' label, differentiating it from traditional multi-head attention where keys and values are typically separate for each head. This sharing aims to improve efficiency. The blue squares represent 'Current position information' and the red squares represent 'Enriched with context information from other positions', indicating the transformation of input into contextually rich output.

**Key Insights:**
The main takeaway from this image is that Multi-query attention achieves efficiency by sharing the 'Keys' and 'Values' matrices across multiple 'Attention head' instances. This contrasts with traditional multi-head attention, where each head typically has its own set of Keys and Values. The explicit labeling of 'Shared: Keys' and 'Shared: Values' (along with the corresponding orange and blue square matrices) directly supports this insight. While the 'Queries' (purple squares) are distinct for each head ('Attention head #1 Queries', 'Attention head #2 Queries', 'Attention head #3 Queries'), the critical shared components reduce computational overhead. The process flow from 'Current position information' to 'Enriched with context information from other positions' demonstrates that this optimized attention mechanism still successfully extracts and integrates contextual data.

**Document Context:**
This image is presented in the document section titled 'Optimizing attention: From multi-head to multi-query to grouped query' and is followed by the caption 'Figure 3-27. Multi-query attention presents a more efficient attention mechanism by sharing the keys and values matrices across all the attention heads.' This contextual information directly confirms that the diagram explains a specific optimization technique for attention mechanisms. It illustrates the 'multi-query' aspect by showing shared 'Keys' and 'Values' compared to the separate 'Queries' for each head. The image is crucial for understanding how this optimization works, particularly the mechanism of sharing parameters to achieve greater efficiency in processing attention, which is a core concept in transformer models and natural language processing.

**Summary:**
This diagram illustrates the Multi-query attention mechanism, which is a method for optimizing attention in neural networks. The process begins with 'Current position information' (represented by blue squares) as input. Inside the 'Self-attention' block, the first step is 'Split into heads'. This stage prepares the input for parallel processing across multiple attention heads. Crucially, the 'Keys' (orange squares) and 'Values' (blue squares) matrices are 'Shared:' across all subsequent attention heads. Following this, there are distinct processing paths for 'Attention head #1', 'Attention head #2', and 'Attention head #3'. Each of these heads independently receives 'Queries' (purple squares). After each attention head has processed its queries using the shared keys and values, the next step is to 'Combine information from all heads'. This final combination step integrates the outputs from the individual attention heads into a unified representation. The output of this entire process is 'Enriched with context information from other positions' (represented by red squares), indicating that the initial position information has been enhanced with contextual understanding derived from the attention mechanism.](images/252017fce5a41978b890befec909ece0093f74bf520c9f0999c026e59fd746a5.jpg)
Figure 3-27. Multi-query attention presents a more efficient attention mechanism by sharing the keys and values matrices across all the attention heads.

As model sizes grow, however, this optimization can be too punishing and we can afford to use a little more memory to improve the quality of the models. This is where grouped-query attention comes in. Instead of cutting the number of keys and values matrices to one of each, it allows us to use more (but less than the number of heads). Figure 3-28 shows these groups and how each group of attention heads shares keys and values matrices.

![## Image Analysis: e2089894abfedbd869c76960112cf7433de6b6eb1ee2e85f754d08cf67ec2ff4.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural design of a 'Grouped-query attention' mechanism, which is a component of a Self-attention layer in neural networks, often used in Transformer models. The main purpose of this diagram is to illustrate how input information, specifically 'Current position information,' is processed through a hierarchical structure of attention heads that are organized into groups. Each group shares certain resources (implied key/value matrices, denoted by 'Shared' and the overlapping grid icons) among its constituent attention heads. The process aims to transform the input into an output 'Enriched with context information from other positions,' thereby capturing relationships and dependencies within the input data.

**Content Interpretation:**
This image illustrates the architecture and data flow of a Grouped-query attention mechanism, which is a variant of the self-attention process. It depicts how input information is processed through multiple attention heads, organized into groups with shared key/value matrices. The core concepts shown are the splitting of input into heads, the grouping of these heads with shared resources, and the subsequent combination of information to produce a context-enriched output. The purpose is to demonstrate a computational model designed to process sequential data, likely for tasks in natural language processing or other deep learning applications requiring contextual understanding.

**Key Insights:**
The main takeaway from this image is the structural organization of Grouped-query attention. It highlights that instead of each attention head having its own set of key/value matrices (as in multi-head) or all heads sharing one set (as in multi-query), Grouped-query attention introduces an intermediate level: 'n_groups,' where each group shares a set of key/value matrices among its 'n_attention_heads.'

**Specific evidence from the image:**
- The overall process is labeled "Self-attention."
- The input is "Current position information."
- The first processing step is "Split into heads."
- The presence of "n_groups" indicates a configurable number of groups.
- Within the groups, labels like "Group 1 Shared:" and "Group 2 Shared:" (with the overlapping grid icons) explicitly show that key/value matrices are shared within a group.
- The presence of "n_attention_heads" indicates a configurable number of attention heads.
- Each "Attention head #[Number]" processes "Queries."
- The final step, "Combine information from all heads," signifies the aggregation of processed information.
- The output, "Enriched with context information from other positions," indicates the purpose of the self-attention mechanism: to integrate contextual understanding.

This architecture is crucial for understanding the trade-off mentioned in the surrounding text (efficiency vs. quality) by explicitly showing the grouped sharing of key/value matrices, which offers more flexibility than a single shared set while being more efficient than fully independent sets.

**Document Context:**
This image directly supports the document's section titled "Optimizing attention: From multi-head to multi-query to grouped query." It visually explains how 'Grouped-query attention' operates as an optimization, building upon the concepts of multi-head and multi-query attention. The text accompanying the figure further clarifies that Grouped-query attention "sacrifices a little bit of the efficiency of multiquery attention in return for a large improvement in quality by allowing multiple groups of shared key/value matrices; each group has its respective set of attention heads." This diagram visually represents that specific architectural choice, showing how input information is divided among 'n_groups,' each with 'n_attention_heads' and 'Shared' key/value matrices (represented by the overlapping grids), ultimately leading to context-enriched output. Thus, it is a key visual aid for understanding the specific optimization being discussed.

**Summary:**
This image illustrates the Grouped-query attention mechanism within a Self-attention process. The flow begins with "Current position information" entering the overall "Self-attention" block. Inside this block, the first step is "Split into heads." This splitting leads to multiple groups, indicated by "n_groups" on the left. The diagram shows two such groups: "Group 1 Shared:" and "Group 2 Shared:", each associated with overlapping orange and blue grid icons, signifying shared key/value matrices. Within each group, there are multiple "Attention head" units, denoted by "n_attention_heads." For instance, "Group 1" contains "Attention head #1" and "Attention head #2," while "Group 2" contains "Attention head #3" and "Attention head #4." Each attention head processes "Queries," represented by purple grid icons. Following the processing by individual attention heads, the outputs are then sent to the step labeled "Combine information from all heads." The final output of this combined information is described as "Enriched with context information from other positions," associated with a red grid icon. The blue grid icon at the input signifies "Current position information," and the red grid icon at the output signifies "Enriched with context information from other positions."](images/e2089894abfedbd869c76960112cf7433de6b6eb1ee2e85f754d08cf67ec2ff4.jpg)
Figure 3-28. Grouped-query attention sacrifices a little bit of the efficiency of multiquery attention in return for a large improvement in quality by allowing multiple groups of shared key/value matrices; each group has its respective set of attention heads.

# Flash Attention

Flash Attention is a popular method and implementation that provides significant speedups for both training and inference of Transformer LLMs on GPUs. It speeds up the attention calculation by optimizing what values are loaded and moved between a GPU’s shared memory (SRAM) and high bandwidth memory (HBM). It is described in detail in the papers “FlashAttention: Fast and memory-efficient exact attention with IO-awareness” and the subsequent “FlashAttention-2: Faster attention with bet‐ ter parallelism and work partitioning”.

# The Transformer Block

Recall that the two major components of a Transformer block are an attention layer and a feedforward neural network. A more detailed view of the block would also reveal the residual connections and layer-normalization operations that we can see in Figure 3-29.

![## Image Analysis: d26cd2e93bd9399e9a2a4b6e13ca6a0227999d1c625a381fa7a3ecb5787de649.jpg

**Conceptual Understanding:**
This image conceptually represents the internal architecture of a single Transformer block, a foundational component in the Transformer neural network model. Its main purpose is to illustrate the sequential processing steps and key sub-layers involved in transforming input embeddings into output embeddings while preserving and capturing contextual information and dependencies within a sequence. It communicates how self-attention, residual connections, layer normalization, and feedforward networks are integrated to form this powerful building block for sequence processing tasks like natural language understanding.

**Content Interpretation:**
The image details the architectural components and data flow within a Transformer block. It shows two primary processing stages: a self-attention mechanism and a feedforward network, each coupled with a residual connection and layer normalization. The inputs, labeled as 'Thinking' and 'Machines' (with 'x1' and 'x2' as respective tokens), are first combined with 'Positional encoding' before entering the main 'Transformer block'.

Inside the 'Transformer block', the sequence flows through:
1.  **Self-attention:** This layer processes the input to weigh the importance of different parts of the input sequence relative to each other, capturing contextual relationships. The text 'Self-attention' directly labels this orange rectangular box.
2.  **Add and normalize (first instance):** Following self-attention, the output of the self-attention layer is combined with its input (a residual connection, indicated by the dashed line) and then normalized. The text 'Add and normalize' labels this green rounded rectangular box.
3.  **Feedforward:** The output from the first 'Add and normalize' step then enters two parallel 'Feedforward' sub-layers, labeled 'Feedforward' (blue rounded rectangular boxes). These are typically position-wise fully connected layers that process each position independently and identically.
4.  **Add and normalize (second instance):** Similar to the first instance, the outputs of the 'Feedforward' layers are combined with their input (another residual connection) and normalized. The text 'Add and normalize' labels this final green rounded rectangular box within the main block.

The arrows clearly indicate the flow of information through these sequential and parallel steps. The initial '+' symbol for 'Positional encoding' signifies the addition of positional information to the input embeddings.

**Key Insights:**
The main takeaways from this image are:
1.  **Core Components:** A Transformer block fundamentally consists of two main sub-layers: a 'Self-attention' mechanism and 'Feedforward' neural networks.
2.  **Positional Encoding:** Input tokens (like 'Thinking' and 'Machines', represented by 'x1' and 'x2') are enhanced with 'Positional encoding' before entering the Transformer block, which is critical for the model to understand the order of elements in a sequence.
3.  **Residual Connections and Normalization:** Both the self-attention and feedforward sub-layers are followed by an 'Add and normalize' step. This signifies the use of residual connections (skip connections, indicated by dashed lines) where the input of the sub-layer is added to its output, followed by layer normalization. These techniques are vital for enabling the training of very deep networks by alleviating the vanishing gradient problem and speeding up convergence.
4.  **Modular Design:** The Transformer block has a clear, modular design, allowing for stacking multiple such blocks to form deep Transformer models. The 'Feedforward' layers operate in parallel, typically independently on each position.

These insights are directly supported by the verbatim text labels 'Positional encoding', 'Transformer block', 'Self-attention', 'Add and normalize', and 'Feedforward', as well as the visual representation of the data flow and the dashed residual connection lines.

**Document Context:**
This image directly supports the document's narrative on "The Transformer Block" by visually presenting its internal architecture. It serves as Figure 3-29, explicitly identified as "A Transformer block from the original Transformer paper." The diagram breaks down the complex concept of a Transformer block into its core, understandable components, illustrating the flow of data through self-attention, residual connections, layer normalization, and feedforward networks. This visual representation is crucial for understanding how Transformers process sequences and establish dependencies, laying the groundwork for explaining more advanced Transformer-based models.

**Summary:**
The image illustrates the internal structure and data flow within a single Transformer block, a fundamental component of the Transformer architecture. It depicts how input embeddings are processed through self-attention, feedforward networks, and residual connections with normalization. The process begins with input tokens, represented here as "Thinking" and "Machines," which are first augmented with positional encoding to provide sequence order information. These enhanced embeddings then enter the Transformer block, where they sequentially pass through a self-attention layer, followed by an "Add and normalize" step. This is then followed by two parallel "Feedforward" sub-layers, each independently processing the data. Finally, another "Add and normalize" step integrates the output of the feedforward layers before exiting the block. The dashed lines indicate residual connections, where the input to a sub-layer is added to its output before normalization, facilitating training of deep networks.](images/d26cd2e93bd9399e9a2a4b6e13ca6a0227999d1c625a381fa7a3ecb5787de649.jpg)
Figure 3-29. A Transformer block from the original Transformer paper.

The latest Transformer models at the time of this writing still retain the major components, yet make a number of tweaks as we can see in Figure 3-30.

One of the differences we see in this version of the Transformer block is that nor‐ malization happens prior to attention and the feedforward layers. This has been reported to reduce the required training time (read: “On layer normalization in the Transformer architecture”). Another improvement in normalization here is using RMSNorm, which is simpler and more efficient than the LayerNorm used in the original Transformer (read: “Root mean square layer normalization”). Lastly, instead of the original Transformer’s ReLU activation function, newer variants like SwiGLU (described in “GLU Variants Improve Transformer”) are now more common.

![## Image Analysis: 332630301ed2a8de0b27f04cae8794a04ad230035429f872ae9cbc4c8577564f.jpg

**Conceptual Understanding:**
This image conceptually represents the internal architecture and data flow within a single "2024-era Transformer block," which is a fundamental building block of modern Transformer models like Llama 3. The main purpose is to illustrate how two distinct input sequences ("Thinking" and "Machines") are processed in parallel through normalization, a shared self-attention mechanism, and feedforward networks, incorporating pre-normalization and residual connections. The key ideas communicated are the specific components and their arrangement within a contemporary Transformer block, highlighting advancements such as RMSNorm, grouped-query attention, and rotary embeddings.

**Content Interpretation:**
The image depicts a specific, modern implementation of a Transformer block, designed to process two input streams.

*   **Processes Shown:**
    *   **Pre-normalization:** Evidenced by "Normalize" blocks (labeled "RMSNorm") appearing *before* the self-attention and feedforward layers. This is a common tweak in modern Transformers to improve training stability. The verbatim text "Normalize" and "RMSNorm" explicitly detail this.
    *   **Self-attention:** A core mechanism in Transformers for weighting the importance of different parts of the input sequence. The block is labeled "Self-attention" with additional optimizations: "Grouped-query attention" and "rotary embeddings." "Grouped-query attention" suggests an efficiency improvement by sharing queries among multiple attention heads, while "rotary embeddings" are a type of positional encoding. The placement of this shared block after initial normalization for both inputs shows its central role in integrating information from both streams.
    *   **Feedforward Networks:** Standard fully connected layers that process each position independently and in parallel. The "Feedforward" blocks are shown separately for the x₁ and x₂ paths, indicating they operate on their respective processed streams.
    *   **Residual Connections (Skip Connections):** Represented by the dashed lines bypassing a layer and then adding (⊕) its output to the layer's result. These connections ("Add" (⊕) symbols) help mitigate the vanishing gradient problem and allow the network to learn identity functions, facilitating deeper models. These are present after the self-attention and after the feedforward layers for both paths.

*   **Concepts and Relationships:**
    *   **Parallel Processing:** The x₁ ("Thinking") and x₂ ("Machines") inputs are processed in parallel through separate initial normalization layers, then converge for shared self-attention, and diverge again for separate subsequent normalization and feedforward layers. This suggests the block can handle multi-modal inputs or two distinct sequences simultaneously.
    *   **Modular Design:** The entire architecture is encapsulated within a "2024-era Transformer block," reinforcing the idea that this is a reusable, fundamental unit in larger Transformer models.
    *   **Architectural Evolution:** The label "2024-era Transformer block" along with specific optimizations like "RMSNorm," "Grouped-query attention," and "rotary embeddings" (all extracted verbatim) indicates that this is not a vanilla Transformer block but incorporates recent advancements for improved performance, efficiency, or stability, as mentioned in the document context.
    *   **Pre-normalization vs. Post-normalization:** The placement of "Normalize (RMSNorm)" *before* the main computational layers (Self-attention, Feedforward) explicitly illustrates the pre-normalization technique, which is a key tweak mentioned in the document context.

*   **Information Presented and its Significance:**
    *   The "x₁" and "x₂" labels indicate distinct input embeddings or token sequences. The small green blocks after them represent the sequence of tokens.
    *   "RMSNorm" signifies a specific type of normalization layer (Root Mean Square Normalization) which is known for its computational efficiency and good performance in modern large language models, contrasting with LayerNorm often found in older Transformers.
    *   "Grouped-query attention, rotary embeddings" are specific techniques to enhance the self-attention mechanism, likely improving efficiency (grouped-query) and handling positional information more effectively (rotary embeddings) for longer sequences.

All extracted text elements ("Normalize," "RMSNorm," "Self-attention," "Grouped-query attention, rotary embeddings," "Feedforward," "⊕," "x₁," "x₂," "Thinking," "Machines," "2024-era Transformer block") are crucial in detailing these processes, concepts, and architectural choices, directly supporting the interpretation of a modern, optimized Transformer block.

**Key Insights:**
**Main Takeaways/Lessons:**

*   **Modern Transformer Blocks are Optimized:** The presence of "2024-era Transformer block" and specific component labels like "RMSNorm," "Grouped-query attention," and "rotary embeddings" highlights that contemporary Transformer architectures continuously evolve with performance and efficiency enhancements. This shows a shift from older, simpler designs.
*   **Pre-normalization is a Standard Practice:** The diagram clearly shows "Normalize" (RMSNorm) layers applied *before* the self-attention and feedforward layers. This indicates that pre-normalization has become a preferred architectural choice in modern Transformer designs for reasons such as improved training stability, as hinted by the document context.
*   **Attention Mechanisms are Refined:** The "Self-attention" layer is not just basic attention but explicitly enhanced with "Grouped-query attention" and "rotary embeddings." This implies that standard self-attention has been further specialized to handle specific challenges like computational cost or encoding positional information more robustly, especially for larger models.
*   **Residual Connections Remain Critical:** The consistent use of "Add" (⊕) symbols with dashed skip connections throughout the block confirms that residual connections are still fundamental for enabling the training of very deep neural networks by facilitating gradient flow.
*   **Parallel Input Processing:** The distinct "Thinking" (x₁) and "Machines" (x₂) input paths, processed in parallel through most of the block (except for the shared Self-attention), demonstrate the flexibility of Transformer blocks to potentially handle multiple input sequences or modalities.

**Conclusions/Insights Supported by Textual Evidence:**

*   **Insight:** The depicted Transformer block is a highly specialized and updated version, reflecting best practices for models like Llama 3.
    *   **Evidence:** The title "2024-era Transformer block" explicitly states its contemporaneity. The annotations "RMSNorm," "Grouped-query attention, rotary embeddings" are specific, advanced techniques not present in foundational Transformer papers, thus indicating an evolved design.
*   **Insight:** Data flows through a sequence of normalization, attention, and feedforward operations, with skip connections at key stages.
    *   **Evidence:** The sequential arrows connecting "Normalize" -> "Self-attention" -> "Normalize" -> "Feedforward" describe the primary data path. The dashed arrows leading to "⊕" symbols unequivocally show the skip connections, and the "Add" labels clarify their function.
*   **Insight:** Efficiency and improved representational capacity are key drivers for the architectural choices.
    *   **Evidence:** "Grouped-query attention" is an optimization for efficiency. "Rotary embeddings" and "RMSNorm" contribute to better positional encoding and stable training respectively, enhancing representational capacity and model performance.
*   **Insight:** The block supports processing of distinct parallel inputs, integrating them at the attention stage.
    *   **Evidence:** The presence of two separate input streams labeled "Thinking x₁" and "Machines x₂" that remain somewhat distinct throughout the block, only fully converging at the "Self-attention" layer before diverging again, clearly supports this.

**Document Context:**
This image directly supports the document's narrative in the "The Transformer Block" section by visually detailing the "tweaks" mentioned in the accompanying text. Specifically, it illustrates the "pre-normalization" (explicitly shown as "Normalize RMSNorm" layers before attention and feedforward) and an "attention optimized with grouped-query attention and rotary embeddings" (explicitly labeled within the "Self-attention" block). This diagram serves as a concrete, visual explanation of the architectural advancements defining a "2024-era Transformer like Llama 3," as stated in the text after the image. It bridges the conceptual description of these features with their actual structural implementation within the block.

**Summary:**
The image displays a detailed diagram of a "2024-era Transformer block," a core component in advanced AI models like Llama 3. This block is designed to process two distinct input streams, labeled "Thinking" (represented by 'x₁') and "Machines" (represented by 'x₂'), in a parallel yet integrated manner.

The processing for each input stream begins with a **"Normalize" layer, specifically implementing "RMSNorm"**. This pre-normalization step, applied before the main computational layers, is a key modern tweak aimed at enhancing training stability.

Following this initial normalization, the processed outputs from both "Thinking" and "Machines" streams converge into a **shared "Self-attention" layer**. This critical layer is where the model weighs the importance of different parts of the input. Crucially, this "Self-attention" mechanism is optimized with two modern techniques: **"Grouped-query attention"** (likely for efficiency improvements) and **"rotary embeddings"** (a method for handling positional information in sequences).

After the "Self-attention" layer, the output for each stream branches into two paths. One path is the direct output from "Self-attention," and the other is a **skip connection (indicated by a dashed line)** from the output of the *initial* "Normalize (RMSNorm)" layer. These two paths are combined through an **"Add" (⊕) operation**, forming the first residual connection in the block.

The combined output then enters a **second "Normalize" layer, again using "RMSNorm"**. This continues the pre-normalization pattern within the block.

The output of this second "Normalize" layer also branches. One path feeds into a **"Feedforward" layer**, which applies further transformations independently to each position in the sequence. The other path is another **skip connection (dashed line)**, originating from the output of this second "Normalize (RMSNorm)" layer.

Finally, the output of the "Feedforward" layer for each stream is combined with its corresponding skip connection using a **second "Add" (⊕) operation**. The results of these final additions represent the processed outputs of the "2024-era Transformer block" for both the "Thinking" and "Machines" inputs, ready for subsequent layers in the larger Transformer model.

In essence, this diagram comprehensively illustrates a sophisticated Transformer block featuring pre-normalization (RMSNorm), advanced self-attention (grouped-query attention, rotary embeddings), feedforward networks, and two layers of residual connections, all working in concert to process and integrate information from parallel input streams.](images/332630301ed2a8de0b27f04cae8794a04ad230035429f872ae9cbc4c8577564f.jpg)
Figure 3-30. The Transformer block of a 2024-era Transformer like Llama 3 features some tweaks like pre-normalization and an attention optimized with grouped-query attention and rotary embeddings.

# Positional Embeddings (RoPE)

Positional embeddings have been a key component since the original Transformer. They enable the model to keep track of the order of tokens/words in a sequence/ sentence, which is an indispensable source of information in language. From the many positional encoding schemes proposed in the past years, rotary positional embeddings (or “RoPE,” introduced in “RoFormer: Enhanced Transformer with rotary position embedding”) is especially important to point out.

The original Transformer paper and some of the early variants had absolute posi‐ tional embeddings that, in essence, marked the first token as position 1, the second as position 2...etc. These could either be static methods (where the positional vectors are generated using geometric functions) or learned (where the model training assigns them their values during the learning process). Some challenges arise from such methods when we scale up models, which requires us to find ways to improve their efficiency.

For example, one challenge in efficiently training models with large context is that a lot of documents in the training set are much shorter than that context. It would be inefficient to allocate the entire, say, 4K context to a short 10-word sentence. So during model training, documents are packed together into each context in the training batch, as Figure 3-31 shows.

![## Image Analysis: 2c9abbab95a3089c5762402d2c5d56f2433d653ae453689dbd8d2324b5408261.jpg

**Conceptual Understanding:**
This image conceptually illustrates the challenge and solution for efficiently organizing training data, particularly short documents, within a fixed processing 'Context size.' The main purpose of the image is to visually compare an 'inefficient' approach that results in significant wasted space ('Padding') with an 'efficient' method, termed 'packing,' which optimizes the use of the context by grouping multiple documents together. The key ideas being communicated are the importance of maximizing the utility of a fixed context window and minimizing the overhead introduced by padding during model training, especially for natural language processing tasks where documents can vary greatly in length.

**Content Interpretation:**
The image conceptually represents the process of organizing short training documents within a fixed 'Context size' for machine learning models, particularly in the context of sequence processing. It visually compares two distinct strategies: an inefficient method and an efficient 'packing' method. The inefficient method shows individual documents consuming their own context, leading to substantial 'Padding' at the end of each document's assigned space. This implies wasted computational resources and reduced throughput as a large portion of the 'Context size' is filled with non-meaningful data. In contrast, the efficient packing method demonstrates concatenating multiple short documents, separated by 'Sep' tokens, into a single 'Context size'. This strategy effectively minimizes 'Padding', thereby maximizing the utilization of the available context for actual document content. The image highlights the importance of data preparation techniques to optimize the training process, especially when dealing with documents shorter than the maximum context length.

**Key Insights:**
The main takeaways from this image are: 1.  **Inefficiency of Naive Data Organization:** The 'Naive, inefficient organization of training data' section clearly demonstrates that processing each 'Document 1' or 'Document 2' individually within a fixed 'Context size' leads to significant 'Padding.' This indicates a substantial waste of computational resources, as a large portion of the context is filled with non-informative tokens. 2.  **Benefits of Efficient Data Packing:** The 'Efficient training data packing' section shows that by concatenating multiple documents ('Document 1', 'Document 2', 'Document 3', 'Document 4', 'Document 5') and separating them with 'Sep' tokens, the amount of 'Padding' required at the end of the 'Context size' is drastically reduced. This strategy maximizes the utilization of the available context window. 3.  **Optimization for Short Documents:** The explicit use of multiple documents per 'Batch' in the efficient example (e.g., 'Document 1 Sep Document 2 Sep Document 3') suggests that this packing method is particularly beneficial for short training documents that would otherwise leave a lot of empty space if processed individually. 4.  **Improved Resource Utilization:** The core insight is that 'Efficient training data packing' directly translates to better utilization of a fixed 'Context size,' leading to more effective and potentially faster model training by reducing redundant 'Padding' and processing more actual data per training step.

**Document Context:**
This image directly relates to the 'Positional Embeddings (RoPE)' section of the document by illustrating a crucial data pre-processing step that impacts how models, especially those using positional embeddings, process sequences. Efficient organization of training data, or 'packing,' is vital because models with fixed context windows (like those using RoPE) benefit from having as much meaningful data as possible within each context. The accompanying text explicitly defines 'packing' as 'the process of efficiently organizing short training documents into the context' and mentions 'grouping multiple documents in a single context while minimizing the padding at the end of the context.' This image serves as a visual explanation of why packing is necessary – to prevent the inefficient use of context size with excessive padding, which would waste computational resources and reduce the effective training data throughput for models that rely on the entire context window, such as those employing positional embeddings like RoPE.

**Summary:**
The image illustrates two methods of organizing training data for machine learning models, specifically comparing a 'Naive, inefficient organization' with an 'Efficient training data packing' approach. Both sections display data within a fixed 'Context size' and are processed in 'Batch' units. The top section, labeled 'Naive, inefficient organization of training data', shows two separate batches. The first batch contains 'Document 1' followed by a large 'Padding' block. The second batch contains 'Document 2' also followed by a large 'Padding' block. This visually represents underutilization of the available 'Context size' due to substantial padding. The bottom section, titled 'Efficient training data packing', demonstrates a more optimized approach. The first batch in this section contains 'Document 1', followed by a 'Sep' (separator), then 'Document 2', another 'Sep', 'Document 3', another 'Sep', and finally a significantly smaller 'Padding' block. The second batch in this efficient packing method includes 'Document 4', a 'Sep', 'Document 5', a 'Sep', and then a small 'Padding' block. This arrangement shows that multiple documents can be grouped into a single 'Context size' by using separators, thereby drastically reducing the amount of 'Padding' and making more efficient use of the training context. The overall message is that proper data organization, like packing, can significantly improve the efficiency of utilizing computational resources during model training.](images/2c9abbab95a3089c5762402d2c5d56f2433d653ae453689dbd8d2324b5408261.jpg)
Figure 3-31. Packing is the process of efficiently organizing short training documents into the context. It includes grouping multiple documents in a single context while minimizing the padding at the end of the context.

Learn more about packing by reading “Efficient sequence packing without crosscontamination: Accelerating large language models without impacting performance” and watching the great visuals in “Introducing packed BERT for 2X training speed-up in natural language processing”.

Positional embedding methods have to adapt to this and other practical considera‐ tions. If Document 50, for example, starts at position 50, then we’d be misinforming the model if we tell it that that first token is number 50 and that would affect its performance (because it would assume there’s previous context while in reality the earlier tokens belong to a different and unrelated document the model should ignore).

Instead of the static, absolute embeddings that are added in the beginning of the forward pass, rotary embeddings are a method to encode positional information in a way that captures absolute and relative token position information. It is based on the idea of rotating vectors in their embeddings space. In the forward pass, they are added in the attention step, as Figure 3-32 shows.

![## Image Analysis: 1141d2d1ec78f3985b067bc2447721b379d0ea4c480a8b3b1aa61efaffa27ca0.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a Transformer Large Language Model (LLM), focusing specifically on where "Rotary embeddings" (a type of positional encoding) are integrated into its processing pipeline.

The main purpose of the diagram is to illustrate that Rotary embeddings are not added to the initial token embeddings at the very beginning of the model's forward pass. Instead, they are applied *within* the "Self-attention" mechanism of each "Transformer block". This highlights a specific design choice for incorporating positional information in some Transformer variants, distinguishing it from traditional additive positional encodings. The key idea communicated is the precise point of interaction of RoPE within the attention calculation.

**Content Interpretation:**
The image depicts the sequential processing stages within a Transformer LLM, from input tokenization to final output generation, with a specific focus on the integration of Rotary Positional Embeddings (RoPE).

*   **Tokenizer and Embeddings:** The initial stages, "Tokenizer" and "Embeddings", show how raw text is converted into numerical vector representations ("Write", "...", "happen", "##ed", "." becoming small blue blocks). This is standard for NLP models, establishing the input format for the deeper layers.
*   **Stack of Transformer Blocks:** This core component ("Stack of Transformer blocks") signifies the iterative, multi-layered nature of Transformers. Each "Transformer block" is identical in structure, processing information sequentially or in parallel through its layers.
*   **Transformer Block Components:** Each "Transformer block" contains "Self-attention" and a "Feedforward neural network". The "Self-attention" mechanism is critical for understanding contextual relationships between tokens, while the "Feedforward neural network" further refines these representations.
*   **Rotary Embeddings Application (Key Point):** The most significant detail is the "Rotary embeddings" arrow pointing directly to "Self-attention" in both "Transformer block 1" and "Transformer block 2". This visually confirms that rotary positional information is *integrated into the self-attention calculation itself*, rather than being added to the embeddings before they enter the transformer stack. This design is crucial for how the model interprets the position of tokens relative to each other during attention computation.
*   **LM Head:** The final "LM head" indicates the output layer responsible for generating predictions (e.g., next word probabilities), completing the forward pass.

The extracted text "Rotary embeddings" and its arrows directly supporting the "Self-attention" blocks provide concrete evidence for the interpretation that these embeddings are applied at the attention step, not at the start of the forward pass, as also stated in the document context.

**Key Insights:**
The main takeaway from this image is the specific mechanism and integration point of Rotary Positional Embeddings (RoPE) within a Transformer Large Language Model.

*   **Key Insight 1: RoPE are applied during Self-Attention.** The arrows explicitly labeled "Rotary embeddings" pointing to "Self-attention" within "Transformer block 1" and "Transformer block 2" directly show that these positional embeddings are incorporated into the attention calculation. This contrasts with other positional encoding methods that might be added to token embeddings earlier in the process.
*   **Key Insight 2: RoPE integration is layer-wise within the Transformer stack.** The repeated application of "Rotary embeddings" to "Self-attention" in both "Transformer block 1" and "Transformer block 2" demonstrates that this positional encoding is a feature of each processing layer within the "Stack of Transformer blocks", rather than a one-time initial injection.
*   **Key Insight 3: Differentiating Positional Embeddings from Token Embeddings.** The diagram distinguishes between the initial "Embeddings" (the blue blocks derived from "Write", "...", "happen", "##ed", ".") and "Rotary embeddings". This implies that while token embeddings provide semantic meaning, rotary embeddings specifically provide positional context, and they do so at a distinct stage of processing.

The verbatim transcription of "Rotary embeddings", "Self-attention", "Transformer block 1", "Transformer block 2", "Tokenizer", "Embeddings", "Stack of Transformer blocks", and "LM head" provides all the necessary textual evidence to support these insights, detailing the complete flow and the specific interaction points.

**Document Context:**
This image fits perfectly within a section discussing "Positional Embeddings (RoPE)". It visually confirms and elaborates on the statement provided after the image: "Rotary embeddings are applied in the attention step, not at the start of the forward pass." The diagram is a direct illustration of this specific architectural detail, showing *how* and *where* RoPE is implemented within the Transformer's structure. It clarifies that RoPE is not an initial additive embedding but an intrinsic part of the attention mechanism across multiple layers.

**Summary:**
This diagram illustrates the forward pass of a Transformer Large Language Model (LLM), with a particular focus on how "Rotary embeddings" are integrated. The overall process begins at the top and flows downwards.

First, an input text (represented by example tokens "Write", "...", "happen", "##ed", ".") is processed by a "Tokenizer". The Tokenizer breaks down the input into these individual units. These tokens are then converted into numerical "Embeddings", which are dense vector representations.

These "Embeddings" then enter the core of the model: a "Stack of Transformer blocks". This stack comprises multiple layers, two of which are explicitly shown as "Transformer block 1" and "Transformer block 2". Each Transformer block has two main sub-components:

1.  **Self-attention:** This component is crucial for allowing the model to weigh the importance of different words in the input sequence relative to each other, understanding context.
2.  **Feedforward neural network:** This is a standard neural network that processes the output of the self-attention layer, refining the representations.

A critical aspect highlighted in this diagram is the application of "Rotary embeddings". Instead of being added to the initial "Embeddings" at the very beginning, "Rotary embeddings" are specifically applied to the "Self-attention" mechanism *within* each "Transformer block". This is clearly indicated by the purple arrows labeled "Rotary embeddings" pointing directly to the "Self-attention" boxes in both "Transformer block 1" and "Transformer block 2". This design ensures that positional information is infused directly into how the attention mechanism computes relationships between tokens, rather than being a separate initial input.

Finally, after passing through all the layers in the "Stack of Transformer blocks", the processed information is fed into the "LM head" (Language Model head). The LM head is responsible for generating the model's final output, such as predicting the next token in a sequence or performing other language understanding tasks. This visual representation clearly articulates that rotary positional embeddings are an integral, layer-wise component of the attention mechanism, providing essential positional context throughout the Transformer's processing.](images/1141d2d1ec78f3985b067bc2447721b379d0ea4c480a8b3b1aa61efaffa27ca0.jpg)
Figure 3-32. Rotary embeddings are applied in the attention step, not at the start of the forward pass.

During the attention process, the positional information is mixed in specifically to the queries and keys matrices just before we multiply them for relevance scoring, as we can see in Figure 3-33.

![## Image Analysis: d20be1d048e576d1b5aae1dc537558a9ec41c1a3b09998844cf8289c1c972be1.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified view of a self-attention mechanism's "Attention head #1" within a neural network architecture, specifically highlighting the integration of Rotary Positional Embeddings (RoPE). The main purpose of the image is to illustrate the process by which positional information is introduced into the "Queries" and "Keys" before they are used to compute attention scores, thereby making the self-attention mechanism sensitive to the order of tokens in a sequence. The key idea being communicated is that positional embeddings, specifically rotary embeddings, are applied to the representations of tokens to imbue them with positional awareness, which is essential for understanding sequential data like text. The distinct colors for "Other positions in the sequence" and "Current position information" in the legend further emphasize the role of relative positioning.

**Content Interpretation:**
The image illustrates the process of incorporating Rotary Positional Embeddings (RoPE) into the self-attention mechanism within a Transformer model, specifically focusing on a single "Attention head #1." It shows how "Queries" and "Keys" are processed. The "Projection matrices" likely represent the linear transformations applied to input tokens to obtain queries, keys, and values. The central action is the application of "rotary positional embeddings" to the "Queries" and "Keys," which enriches them with positional information. This is critical because self-attention itself is permutation-invariant and lacks inherent knowledge of token order. The output of this attention head, after the embeddings are applied, is described as "Enriched with context information from other positions," indicating that the positional information has facilitated the contextual understanding. The significance of the data presented is that it visually breaks down a complex operation in neural networks, showing how sequence order is preserved and utilized, a common challenge in models like Transformers. All extracted text elements, such as "Self-attention," "Attention head #1," "Projection matrices," "Queries," "Keys," "Apply rotary positional embeddings," "(with positional information)," and "Enriched with context information from other positions," directly support this interpretation by labeling the components and steps of this positional encoding process within self-attention.

**Key Insights:**
The main takeaway from this image is the specific integration point of Rotary Positional Embeddings (RoPE) within a self-attention layer. It clearly demonstrates that RoPE are applied directly to the "Queries" and "Keys" components. The textual evidence "Apply rotary positional embeddings" on the arrow, and the subsequent "Queries" and "Keys (with positional information)" explicitly highlight this crucial step. This process occurs within an "Attention head #1" (and by extension, other attention heads in a multi-head attention mechanism) after the initial projection matrices, but before the final context enrichment. The diagram illustrates that without this step, the "Queries" and "Keys" would lack positional information, thus emphasizing the necessity of RoPE for sequence-aware processing. The output, "Enriched with context information from other positions," further underlines the contribution of positional embeddings to a more informed representation.

**Document Context:**
This image is highly relevant to the "Positional Embeddings (RoPE)" section of the document. It visually explains the abstract concept of RoPE by showing precisely where and how these embeddings are applied within the self-attention mechanism of a neural network. The text after the image, "Figure 3-33. Rotary positional embeddings are added to the representation of tokens just before the relevance scoring step in self-attention," perfectly aligns with the diagram, as the application of RoPE to Queries and Keys (which are used for relevance scoring) is depicted. This diagram serves to concretely illustrate the architectural detail being discussed in the surrounding text, enhancing the reader's comprehension of how positional information is encoded and utilized in advanced natural language processing models.

**Summary:**
This image illustrates how Rotary Positional Embeddings (RoPE) are integrated into a self-attention mechanism, specifically within "Attention head #1." The process begins with a "Position currently being processed," which feeds into the attention head. Inside, "Projection matrices" are shown as a preparatory component. The core input consists of "Queries" and "Keys," represented by distinct grid-like shapes. An arrow labeled "Apply rotary positional embeddings" indicates a crucial step where these embeddings are applied to both "Queries" and "Keys." This transformation results in modified "Queries" and "Keys," now explicitly described as "(with positional information)." Following this, the information is processed further within the attention head (implied by the outgoing arrow) and emerges as "Enriched with context information from other positions," depicted by a red grid-like shape. A legend in the top right clarifies that light blue grid-like shapes represent "Other positions in the sequence," while dark blue grid-like shapes signify "Current position information." The entire diagram details the flow of data through a single attention head, highlighting the precise point where positional information is introduced.](images/d20be1d048e576d1b5aae1dc537558a9ec41c1a3b09998844cf8289c1c972be1.jpg)
Figure 3-33. Rotary positional embeddings are added to the representation of tokens just before the relevance scoring step in self-attention.

# Other Architectural Experiments and Improvements

Many tweaks of the Transformer are proposed and researched on a continuous basis. “A Survey of Transformers” highlights a few of the main directions. Transformer architectures are also constantly adapted to domains beyond LLMs. Computer vision is an area where a lot of Transformer architecture research is happening (see: “Trans‐ formers in vision: A survey” and “A survey on vision transformer”). Other domains include robotics (see “Open X-Embodiment: Robotic learning datasets and RT-X models”) and time series (see “Transformers in time series: A survey”).

# Summary

In this chapter we discussed the main intuitions of Transformers and recent develop‐ ments that enable the latest Transformer LLMs. We went over many new concepts, so let’s break down the key concepts that we discussed in this chapter:

• A Transformer LLM generates one token at a time.   
• That output token is appended to the prompt, then this updated prompt is presen‐ ted to the model again for another forward pass to generate the next token.   
• The three major components of the Transformer LLM are the tokenizer, a stack of Transformer blocks, and a language modeling head.   
• The tokenizer contains the token vocabulary for the model. The model has token embeddings associated with those tokens. Breaking the text into tokens and then using the embeddings of these tokens is the first step in the token generation process.   
• The forward pass flows through all the stages once, one by one.   
• Near the end of the process, the LM head scores the probabilities of the next possible token. Decoding strategies inform which actual token to pick as the output for this generation step (sometimes it’s the most probable next token, but not always).   
• One reason the Transformer excels is its ability to process tokens in parallel. Each of the input tokens flow into their individual tracks or streams of processing. The number of streams is the model’s “context size” and this represents the max number of tokens the model can operate on.   
• Because Transformer LLMs loop to generate the text one token at a time, it’s a good idea to cache the processing results of each step so we don’t duplicate the processing effort (these results are stored as various matrices within the layers).   
• The majority of processing happens within Transformer blocks. These are made up of two components. One of them is the feedforward neural network, which is able to store information and make predictions and interpolations from data it was trained on.   
• The second major component of a Transformer block is the attention layer. Attention incorporates contextual information to allow the model to better cap‐ ture the nuance of language.   
• Attention happens in two major steps: (1) scoring relevance and (2) combining information.   
• A Transformer attention layer conducts several attention operations in parallel, each occurring inside an attention head, and their outputs are aggregated to make up the output of the attention layer.   
• Attention can be accelerated via sharing the keys and values matrices between all heads, or groups of heads (grouped-query attention).   
• Methods like Flash Attention speed up the attention calculation by optimizing how the operation is done on the different memory systems of a GPU.

Transformers continue to see new developments and proposed tweaks to improve them in different scenarios, including language models and other domains and applications.

In Part II of the book, we will cover some of these practical applications of LLMs. In Chapter 4, we start with text classification, a common task in Language AI. This next chapter serves as an introduction to applying both generative and representation models.

# Using Pretrained Language Models

# Text Classification

A common task in natural language processing is classification. The goal of the task is to train a model to assign a label or class to some input text (see Figure 4-1). Classifying text is used across the world for a wide range of applications, from sentiment analysis and intent detection to extracting entities and detecting language. The impact of language models, both representative and generative, on classification cannot be understated.

![## Image Analysis: affaa0c44bf69e6a44aef23721e2ce5bc3e8e2cd49ccb92e89bc0fbff4c75954.jpg

**Conceptual Understanding:**
This image conceptually represents an automated text classification system. Its main purpose is to visually explain how an input text is processed by a language model to determine its most relevant category. The core message is that language models can be used to efficiently and automatically sort or label text based on its content, making it easier to organize and understand large volumes of textual data. Key ideas communicated are the input-process-output paradigm in machine learning, specifically applied to natural language processing for classification tasks.

**Content Interpretation:**
This image illustrates the process of text classification using a language model. It shows how raw textual input is processed by an AI model to assign it to one of several predefined categories. The core system is a 'Language model' configured for 'classification'. The process begins with an 'Input' (unstructured text), which is then fed into the 'Language model'. The output of this model is a classification, specifically '(The best category for the text)', chosen from a set of discrete categories: 'Customer service', 'Returns', and 'Shipping'. The highlighting of 'Returns' indicates the result of a specific classification task.

**Key Insights:**
The main takeaways from this image are: 1. Language models are a key technology for performing text classification. 2. Text classification involves transforming an unstructured 'Input' text into a structured 'Output' category. 3. The 'Objective: classification' of the language model is to determine '(The best category for the text)'. 4. The output is a selection from a predefined set of categories, such as 'Customer service', 'Returns', or 'Shipping', with one category being identified as the most suitable. These insights are directly evidenced by the labels 'Language model', 'Objective: classification', 'Input', 'Output (The best category for the text)', and the specific category labels.

**Document Context:**
This image directly supports the document's 'Section: Text Classification' by visually explaining the fundamental mechanism described by 'Figure 4-1. Using a language model to classify text'. It serves as a clear, concise visual aid to understand how a language model performs the task of categorizing text, making the abstract concept of text classification concrete. It sets the stage for further discussion on text classification methodologies and applications.

**Summary:**
This diagram illustrates the process of classifying text using a language model, moving from an initial input through a processing step to a categorized output. The process begins with an 'Input', represented by a document icon. This input text is fed into a rectangular box labeled 'Language model' with the sub-label 'Objective: classification', indicating that the language model's purpose is to classify the input. From the 'Language model', an arrow points to the 'Output' section. The 'Output' is further described as '(The best category for the text)'. This output is presented as a list of three potential categories, represented by radio buttons: 'Customer service', 'Returns', and 'Shipping'. In this illustration, 'Returns' is highlighted with a yellow background and a filled radio button, signifying that it has been identified as the best category for the given input text. The overall flow shows a clear, unidirectional process from raw text input to its categorized output.](images/affaa0c44bf69e6a44aef23721e2ce5bc3e8e2cd49ccb92e89bc0fbff4c75954.jpg)
Figure 4-1. Using a language model to classify text.

In this chapter, we will discuss several ways to use language models for classifying text. It will serve as an accessible introduction to using language models that already have been trained. Due to the broad field of text classification, we will discuss several techniques and use them to explore the field of language models:

“Text Classification with Representation Models” on page 113 demonstrates the flexibility of nongenerative models for classification. We will cover both taskspecific models and embedding models. “Text Classification with Generative Models” on page 127 is an introduction to generative language models as most of them can be used for classification. We will cover both an open source as well as a closed source language model.

In this chapter, we will focus on leveraging pretrained language models, models that already have been trained on large amounts of data that can be used for classifying text. As illustrated in Figure 4-2, we will examine both representation and language models and explore their differences.

![## Image Analysis: 8905badb807d8e635b7b0ae2fd86a5dea0f3fdc51f9cd8396b773d67ba95239b.jpg

**Conceptual Understanding:**
This image conceptually represents two distinct architectural patterns for performing text classification using different types of language models. The main purpose is to visually compare and contrast the 'Representation language model' and the 'Generative language model' in the context of classification. It highlights that while both models achieve the same 'Objective: classification', their mechanisms for input processing and the format of their classification 'Output' are fundamentally different. Key ideas communicated include the distinct nature of input for each model type (raw text versus prompted text) and the varying interpretability of their outputs (a numerical class ID versus a natural language explanation).

**Content Interpretation:**
The image presents two distinct methodologies for text classification using different types of language models. The first process demonstrates a 'Representation language model' which is fed raw 'Input' text. Its purpose, explicitly stated as 'Objective: classification', involves assigning the input to one of the predefined 'Classes: 0, 1, 2'. The output is a singular, discrete class identifier, exemplified by '1', indicating 'The best category for the text'. The second process outlines a 'Generative language model' approach. This model requires a more structured 'Input', specifically a 'Classification prompt containing the input text', suggesting that the input is framed as a question or instruction for the model. Similar to the first model, its 'Objective: classification'. However, its 'Output' is significantly different; instead of a numerical class, it provides a natural language explanation: 'The input class is of the type: 1: Returns'. This implies that generative models not only classify but can also articulate the classification in a human-readable format, leveraging their text generation capabilities. The key distinction lies in the input format and the verbosity and interpretability of the output.

**Key Insights:**
The main takeaways from this image are: 1. Both representation and generative language models are capable of performing text classification. 2. Their operational approaches differ significantly in how they handle input and formulate output. The 'Representation language model' takes raw text as 'Input' and assigns a discrete class ID (e.g., '1') from predefined 'Classes: 0, 1, 2' as 'Output' ('The best category for the text'). 3. The 'Generative language model' requires a 'Classification prompt containing the input text' as 'Input', indicating a need for more contextualized input. 4. Critically, the 'Generative language model' produces a more descriptive, natural language 'Output' (e.g., 'The input class is of the type: 1: Returns') rather than a simple class ID. This highlights the ability of generative models to provide explanations or descriptive labels for their classifications, which is a key insight into their application and advantages.

**Document Context:**
This image serves as a direct illustration for a section on 'Text Classification', specifically demonstrating how two different types of language models—representation and generative—approach the task. The text following the image, 'Figure 4-2. Although both representation and generative models can be used for classification, their approaches differ,' perfectly aligns with and is visually supported by the diagram. The diagram visually elaborates on these differing approaches by showing variations in input preparation, model processing, and most notably, the format and nature of the classification output, thus enhancing the reader's understanding of the theoretical concepts discussed in the surrounding text.

**Summary:**
This image illustrates two distinct approaches to text classification: one using a Representation language model and the other using a Generative language model. Both models aim to classify text, but they differ in how they process input and present output. The top process shows a 'Representation language model' which takes 'Input' (a document icon representing text) and processes it with the 'Objective: classification' across 'Classes: 0, 1, 2'. The 'Output' is described as '(The best category for the text)' and is numerically represented as '1'. The bottom process depicts a 'Generative language model'. Its 'Input' is a document icon accompanied by the text 'Classification prompt containing the input text', indicating that the input includes both the text and a prompt for classification. This model also has the 'Objective: classification'. The 'Output' for the generative model is a descriptive phrase: 'The input class is of the type: 1: Returns'. This comparison highlights that while both models achieve classification, the generative model provides a more interpretive, natural language-based output, contrasting with the discrete class ID output of the representation model. The overall flow is left-to-right, demonstrating the transformation from input to classified output for each model type.](images/8905badb807d8e635b7b0ae2fd86a5dea0f3fdc51f9cd8396b773d67ba95239b.jpg)
Figure 4-2. Although both representation and generative models can be used for classifi‐ cation, their approaches differ.

This chapter serves as an introduction to a variety of language models, both genera‐ tive and nongenerative. We will encounter common packages for loading and using these models.

![## Image Analysis: 54e19cbc15e9e17d6a31d2df862701d9226acd37ea8b43714fba9341e3172809.jpg

**Conceptual Understanding:**
The image represents a stylized graphic or logo featuring the silhouette of an animal, specifically a squirrel. Its main purpose is likely branding, visual identification, or as a decorative element within a document. It conceptually communicates an identity rather than informational content.

**Content Interpretation:**
The image displays a green silhouette of an animal, which appears to be a squirrel or similar rodent, depicted in a four-legged stance with its tail curled upwards and forming a prominent loop above its back. This silhouette is contained within a simple green square outline. The significance is purely aesthetic and likely serves as a visual identifier or branding element for an organization, publication, or project. As no text is present, no specific processes, concepts, relationships, or systems beyond a visual identifier are directly shown.

**Key Insights:**
The image primarily conveys a brand identity or a symbolic representation rather than specific data or a process. The key takeaway is the visual association with a green squirrel in a square frame, which would serve as a recognition element for an associated entity. Without any accompanying text or further context, no specific insights or conclusions related to the document's technical content can be extracted from this image alone.

**Document Context:**
Given its appearance as a standalone logo, if placed within a document, it would likely serve as a branding element for the author, institution, or project related to the document context of "Text Classification." It does not directly contribute to the technical content or information within the "Text Classification" section but rather to the document's overall presentation, branding, or identification of its source.

**Summary:**
The image is a logo featuring a vibrant green silhouette of a squirrel or similar small mammal. The animal is depicted in profile, standing on all fours, with its head facing towards the left. Its prominent bushy tail curls upwards and then forward, forming a distinctive spiral loop above its back. The entire green silhouette is enclosed within a simple, thin green square outline, creating a framed appearance. There is no text visible within or around the logo.](images/54e19cbc15e9e17d6a31d2df862701d9226acd37ea8b43714fba9341e3172809.jpg)

Although this book focuses on LLMs, it is highly advised to com‐ pare these examples against classic, but strong baselines such as representing text with TF-IDF and training a logistic regression classifier on top of that.

# The Sentiment of Movie Reviews

You can find the data we use to explore techniques for classifying text on the Hugging Face Hub, a platform for hosting models but also data. We will use the well-known “rotten_tomatoes” dataset to train and evaluate our models.1 It contains 5,331 positive and 5,331 negative movie reviews from Rotten Tomatoes.

To load this data, we make use of the datasets package, which will be used through‐ out the book:

# from datasets import load_dataset

# Load our data   
data $=$ load_dataset("rotten_tomatoes")   
data   
DatasetDict({ train: Dataset({ features: ['text', 'label'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label'], num_rows: 1066 })   
})

The data is split up into train, test, and validation splits. Throughout this chapter, we will use the train split when we train a model and the test split for validating the results. Note that the additional validation split can be used to further validate generalization if you used the train and test splits to perform hyperparameter tuning.

Let’s take a look at some examples in our train split:

{'text': ['the rock is destined to be the 21st century\'s new " conan " and that he\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',   
'things really get weird , though not particularly scary : the movie is all portent and no content .'],   
'label': [1, 0]}

These short reviews are either labeled as positive (1) or negative (0). This means that we will focus on binary sentiment classification.

# Text Classification with Representation Models

Classification with pretrained representation models generally comes in two flavors, either using a task-specific model or an embedding model. As we explored in the previous chapter, these models are created by fine-tuning a foundation model, like BERT, on a specific downstream task as illustrated in Figure 4-3.

![## Image Analysis: e9c3fefb2276cf31e09c71c648bff47d2dd858d52a5afd0b830ae49e5c4f6c2e.jpg

**Conceptual Understanding:**
This image conceptually represents the process of adapting a generalized pre-trained language model, specifically BERT, to perform specialized functions. The main purpose is to illustrate the concept of 'fine-tuning' a 'foundation model' for specific downstream tasks. It communicates the key idea that a broad base model can be specialized through supervised learning into either a 'Task-specific model' for 'classification' or an 'Embedding model' for generating 'embeddings', which are both crucial in text representation and analysis.

**Content Interpretation:**
The image depicts a workflow for adapting a pre-trained natural language processing model, BERT, for specialized applications. It illustrates that a 'Base (foundation model)' (BERT) can be refined through 'Fine-tuning (supervised)' to create models tailored for specific tasks. Two key tasks are highlighted: 'classification' and 'embeddings'. The fine-tuning for 'Task: classification' results in a 'Task-specific model' whose 'Objective: Perform classification'. Similarly, fine-tuning for 'Task: embeddings' yields an 'Embedding model' with the 'Objective: Create embeddings'. The phrase 'Fine-tuned (used in this chapter)' signifies that these task-specific models are the practical outcomes relevant to the discussion in the current chapter.

**Key Insights:**
The main takeaway from this image is that large, pre-trained 'foundation models' like 'BERT' are not used directly for specific tasks but rather serve as a 'Base' that can be specialized. This specialization occurs through a process called 'Fine-tuning (supervised)'. This fine-tuning process allows the base model to be adapted for different 'Task:' objectives, such as 'classification' or 'embeddings'. The outcome of fine-tuning is either a 'Task-specific model' aimed at performing 'classification' or an 'Embedding model' aimed at creating 'embeddings'. This demonstrates a fundamental paradigm in modern NLP where powerful general models are customized for practical applications, as indicated by the 'Fine-tuned (used in this chapter)' label.

**Document Context:**
This image directly supports the document's section on 'Text Classification with Representation Models' by visually explaining the core concept of how a foundation model, such as BERT, is adapted or 'fine-tuned' for specific downstream applications. The text after the image, 'Figure 4-3. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings,' explicitly states its purpose. The diagram elucidates how general-purpose models can be specialized to perform classification or generate embeddings, which are fundamental operations in text representation and classification.

**Summary:**
The diagram illustrates the process of fine-tuning a base or foundation model, specifically BERT, for two distinct purposes: text classification and generating general-purpose embeddings. The process begins with a 'Base (foundation model)' labeled 'BERT'. This model then undergoes 'Fine-tuning (supervised)' for two parallel tasks. The first task is 'Task: classification', which leads to a 'Task-specific model' with the 'Objective: Perform classification'. The second task is 'Task: embeddings', which leads to an 'Embedding model' with the 'Objective: Create embeddings'. Both resulting models are categorized under 'Fine-tuned (used in this chapter)', indicating their application within the current document's context.](images/e9c3fefb2276cf31e09c71c648bff47d2dd858d52a5afd0b830ae49e5c4f6c2e.jpg)
Figure 4-3. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings.

A task-specific model is a representation model, such as BERT, trained for a specific task, like sentiment analysis. As we explored in Chapter 1, an embedding model generates general-purpose embeddings that can be used for a variety of tasks not limited to classification, like semantic search (see Chapter 8).

The process of fine-tuning a BERT model for classification is covered in Chapter 11 while creating an embedding model is covered in Chapter 10. In this chapter, we keep both models frozen (nontrainable) and only use their output as shown in Figure 4-4.

![## Image Analysis: b77dfa0e94159e1e44d55850b3b0d686e3d6f74ef7bd450d52093a946125f8ae.jpg

**Conceptual Understanding:**
This image conceptually represents two fundamental architectures for text classification. The main purpose is to illustrate and contrast the direct application of a task-specific model with an indirect approach that first generates general-purpose text embeddings, which are then used by a separate, trainable classifier. The image communicates the key idea that text can be understood and categorized either by a model end-to-end trained for the specific task or by using a pre-processed numerical representation (embeddings) as input to a simpler classification algorithm.

**Content Interpretation:**
The image depicts two primary strategies for performing text classification. The first strategy involves a direct approach where a 'Task-specific model' is utilized, whose explicit objective is to 'Perform classification' on the input text. The second, indirect strategy, first employs an 'Embedding model' to 'Create embeddings' from the input text. This embedding model is indicated as 'Nontrainable' and "Frozen", meaning its internal parameters are static. The embeddings generated are then passed to a separate 'Train classifier', exemplified by a 'logistic regression' model, which performs the final classification. Both pathways ultimately yield a 'Positive' classification output.

**Key Insights:**
The main takeaways from this image are: 
1. Text classification can be performed directly by a model designed and trained specifically for the classification task, as shown by the 'Task-specific model' whose 'Objective: Perform classification'.
2. Alternatively, text classification can be performed indirectly by first transforming the text into numerical 'embeddings' using an 'Embedding model', as indicated by 'Objective: Create embeddings'.
3. When using an 'Embedding model' for indirect classification, the embedding model itself may be 'Nontrainable' and "Frozen", meaning its parameters are fixed and it acts as a feature extractor. This is a crucial detail for understanding the role of pre-trained embeddings.
4. The embeddings generated by a frozen embedding model are then typically fed into a separate, trainable 'Train classifier' (e.g., 'logistic regression') to produce the final classification output. This highlights the modularity of the embedding-based approach.
5. Both direct and indirect methods can lead to the same classification outcome, as evidenced by both pathways yielding 'Output: 1 Positive'.

**Document Context:**
This image serves as a clear visual explanation for the section 'Text Classification with Representation Models' and directly supports the subsequent text: 'Figure 4-4. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings.' It visually differentiates between using a dedicated model for classification versus leveraging pre-computed or general-purpose embeddings as features for a subsequent, separate classifier. This helps the reader understand the architectural differences and implications of each approach in the context of text analysis.

**Summary:**
This image illustrates two distinct methods for text classification, originating from a single input. The process begins with an 'Input' text, specifically the phrase "Best movie ever!". This input feeds into two parallel classification pathways.

Pathway 1, at the top, demonstrates direct classification using a 'Task-specific model'. The 'Objective' of this model is to 'Perform classification'. The output of this model is labeled 'Output' and shows '1 Positive', indicating a successful classification.

Pathway 2, at the bottom, illustrates indirect classification using an 'Embedding model'. The 'Objective' of this model is to 'Create embeddings'. This embedding model is explicitly labeled as 'Nontrainable' and "Frozen," represented visually by an iceberg icon, signifying that its internal parameters are fixed and not updated during the classification process. The output from the embedding model then proceeds to a 'Train classifier' step, which is visually represented by a sigmoid function with input and output nodes (circles). This classifier is further specified as being, for example, a 'logistic regression' model. The final output of this pathway is also labeled 'Output' and shows '1 Positive', identical to the first pathway, signifying a similar successful classification outcome.

In essence, the diagram comprehensively details how text can be classified either by a model specifically trained for the classification task or by transforming the text into general-purpose numerical embeddings which are then fed into a separate, trained classifier.](images/b77dfa0e94159e1e44d55850b3b0d686e3d6f74ef7bd450d52093a946125f8ae.jpg)
Figure 4-4. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings.

We will leverage pretrained models that others have already fine-tuned for us and explore how they can be used to classify our selected movie reviews.

# Model Selection

Choosing the right models is not as straightforward as you might think with over 60,000 models on the Hugging Face Hub for text classification and more than 8,000 models that generate embeddings at the moment of writing. Moreover, it’s crucial to select a model that fits your use case and consider its language compatibility, the underlying architecture, size, and performance.

Let’s start with the underlying architecture. As we explored in Chapter 1, BERT, a well-known encoder-only architecture, is a popular choice for creating task-specific and embedding models. While generative models, like the GPT family, are incredible models, encoder-only models similarly excel in task-specific use cases and tend to be significantly smaller in size.

Over the years, many variations of BERT have been developed, including RoBERTa,2 DistilBERT,3 ALBERT,4 and DeBERTa,5 each trained in various contexts. You can find an overview of some well-known BERT-like models in Figure 4-5.

![## Image Analysis: 3fe335633ef31f65c040b1ea635967717fcc3275f84f258bcf61a57846b5af59.jpg

**Conceptual Understanding:**
This image represents a chronological timeline illustrating the release and associated specifications (likely parameter counts) of various 'BERT-like' natural language processing models. Its main purpose is to show the evolution and diversity of these foundation models over a specific period (2019-2021). The key ideas being communicated are the rapid advancement in large language model architectures, the emergence of different variants with varying sizes or complexities, and the temporal sequence of their introduction into the field.

**Content Interpretation:**
This image is a timeline that graphically represents the release dates and associated key metrics (likely parameter counts) for several BERT-like transformer models. The models shown are BERT, RoBERTa, ALBERT, DistilBERT, and DeBERTa. Each model is positioned chronologically from 2019 to 2021. The numerical values accompanying each model, such as '110/340M' for BERT or '12/18/60/235M' for ALBERT, are crucial pieces of information that differentiate these models, likely indicating different configurations or sizes in millions of parameters. The timeline clearly shows the rapid development and introduction of various BERT-like architectures within a short span, indicating active research and advancement in the field of natural language processing.

**Key Insights:**
The main takeaways from this image are: 1. **Rapid Evolution of BERT-like Models:** Several advanced BERT-like models were released in quick succession between 2019 and 2021, including BERT, RoBERTa, ALBERT, DistilBERT, and DeBERTa. 2. **Variations in Model Size/Complexity:** The numerical data associated with each model (e.g., '110/340M' for BERT, '356M' for RoBERTa, '12/18/60/235M' for ALBERT, '66M' for DistilBERT, '134/384/750M' for DeBERTa) indicates different scales or parameter counts, suggesting a range of model sizes and complexities available. 3. **DistilBERT's Smaller Footprint:** DistilBERT stands out with a significantly smaller parameter count ('66M') compared to others like RoBERTa ('356M') or DeBERTa ('134/384/750M'), implying it's a more lightweight or 'distilled' version. These insights are directly supported by the verbatim transcription of model names, associated numbers, and the chronological arrangement on the timeline.

**Document Context:**
This image is presented in the 'Model Selection' section of a document and is directly preceded by the text: 'Figure 4-5. A timeline of common BERT-like model releases. These are considered foundation models and are mostly intended to be fine-tuned on a downstream task.' This context clarifies that the timeline is crucial for understanding the historical development and options available when selecting foundation models for specific tasks. The image visually supports the discussion on model selection by providing a concise, chronological overview of significant BERT-like models and their characteristics, aiding readers in grasping the landscape of available models for fine-tuning.

**Summary:**
The image displays a horizontal timeline illustrating the release of several BERT-like models from approximately 2019 to 2021. It highlights five distinct models: BERT, RoBERTa, ALBERT, DistilBERT, and DeBERTa. Each model is positioned on the timeline according to its release period, with specific numerical data (likely representing parameter counts or sizes in millions) associated with each. The timeline starts with an ellipsis (...) indicating earlier periods and progresses with distinct markers for the years 2019, 2020, and 2021, ending with an arrow indicating continuation. The clear labeling of each model and its associated numerical data, combined with the chronological arrangement, provides a comprehensive overview of the evolution of these foundation models.](images/3fe335633ef31f65c040b1ea635967717fcc3275f84f258bcf61a57846b5af59.jpg)
Figure 4-5. A timeline of common BERT-like model releases. These are considered foundation models and are mostly intended to be fine-tuned on a downstream task.

Selecting the right model for the job can be a form of art in itself. Trying thousands of pretrained models that can be found on Hugging Face’s Hub is not feasible so we need to be efficient with the models that we choose. Having said that, several models are great starting points and give you an idea of the base performance of these kinds of models. Consider them solid baselines:

• BERT base model (uncased)   
• RoBERTa base model   
• DistilBERT base model (uncased)   
• DeBERTa base model   
• bert-tiny   
• ALBERT base v2

For the task-specific model, we are choosing the Twitter-RoBERTa-base for Senti‐ ment Analysis model. This is a RoBERTa model fine-tuned on tweets for sentiment analysis. Although this was not trained specifically for movie reviews, it is interesting to explore how this model generalizes.

When selecting models to generate embeddings from, the MTEB leaderboard is a great place to start. It contains open and closed source models benchmarked across several tasks. Make sure to not only take performance into account. The importance of inference speed should not be underestimated in real-life solutions. As such, we will use sentence-transformers/all-mpnet-base-v2 as the embedding throughout this section. It is a small but performant model.

# Using a Task-Specific Model

Now that we have selected our task-specific representation model, let’s start by load‐ ing our model:

from transformers import pipeline   
# Path to our HF model   
model_path $=$ "cardiffnlp/twitter-roberta-base-sentiment-latest"   
# Load model into pipeline   
pipe $=$ pipeline( model=model_path, tokenizer=model_path, return_all_scores ${ } = { }$ True, device="cuda:0"   
)

As we load our model, we also load the tokenizer, which is responsible for converting input text into individual tokens, as illustrated in Figure 4-6. Although that parameter is not needed as it is loaded automatically, it illustrates what is happening under the hood.

![## Image Analysis: c472dec498dfb0544fee126ffc1ed695270a4835c2ece95ca7e833abe30d34cc.jpg

**Conceptual Understanding:**
This image conceptually illustrates a simplified pipeline for text processing in Natural Language Processing (NLP), specifically focusing on tokenization followed by a classification task. The main purpose is to demonstrate how a raw input sentence is transformed into a format consumable by a machine learning model, and then how that model performs a designated task. It communicates the fundamental idea that textual data undergoes a structured conversion process before it can be semantically interpreted and categorized by an AI system.

**Content Interpretation:**
The image illustrates a two-stage process within an NLP pipeline: tokenization and text classification. The first stage involves taking a raw text "Input" (the sentence "Her vocalization was melodic") and processing it through a "Tokenizer." The role of the Tokenizer, as indicated by the annotation "Split input up into tokens," is to break down the continuous text into discrete units. The specific tokens generated ("Her," "vocal," "##ization," "was," "melodic") demonstrate how a tokenizer operates, including the use of subword units (like "##ization" for part of "vocalization") to represent words efficiently. The second stage shows these tokens being fed into a "Task-specific model" whose "Objective" is to "Perform classification." This highlights that NLP models are designed for particular tasks. The final "Output" of "1" and "Positive" signifies the result of this classification, indicating that the model has assigned a 'Positive' sentiment or category to the input sentence.

**Key Insights:**
The image conveys several key pieces of knowledge: 1.  **Mandatory Preprocessing:** Raw text cannot be directly processed by most NLP models; it requires a preprocessing step like tokenization, as shown by the flow from "Input" to "Tokenizer." 2.  **Function of Tokenization:** Tokenization breaks down continuous text into a sequence of discrete units (tokens), which can be words or subword units (evidenced by "Split input up into tokens" and the resulting tokens "Her," "vocal," "##ization," etc.). 3.  **Task-Specific Nature of Models:** NLP models are trained for specific tasks, and their "Objective" is clearly defined, such as "Perform classification." 4.  **Classification Output:** The outcome of a classification task is typically a label or category, as demonstrated by the "Output" being "1" and "Positive." These insights illustrate the foundational steps in preparing and processing linguistic data for machine learning tasks.

**Document Context:**
This image serves as a clear visual aid for understanding the initial steps of processing text in an NLP system, aligning with the document's broader narrative on loading models into pipelines. The text after the image, "Figure 4-6. An input sentence is first fed to a tokenizer before it can be processed by the task-specific model," explicitly links the diagram's content to the document's explanation. It provides a concrete example of how raw linguistic data is prepared (tokenized) before being passed to a specialized model for tasks like classification, which is a fundamental concept in building NLP applications.

**Summary:**
This diagram illustrates a fundamental Natural Language Processing (NLP) pipeline, detailing the journey of an input sentence through tokenization and classification. The process begins with an "Input" sentence: "Her vocalization was melodic." This raw text is then fed into a component labeled "Tokenizer." The primary function of the Tokenizer is to "Split input up into tokens," transforming the single sentence into a sequence of individual words or subword units. In this specific example, "Her vocalization was melodic" is broken down into five distinct tokens: "Her," "vocal," "##ization," "was," and "melodic." The inclusion of "##ization" suggests the use of a subword tokenization strategy, common in modern NLP to handle morphology and reduce vocabulary size. These tokenized units are then passed to a "Task-specific model." The explicit "Objective" of this model is to "Perform classification," meaning it processes the tokens to assign a category or label to the original input. The final stage is the "Output," which for this pipeline is presented as "1" and "Positive." This indicates that the model has successfully classified the input sentence into the "Positive" category, represented numerically by '1'. The diagram clearly shows the sequential flow from raw text to a structured, classified output.](images/c472dec498dfb0544fee126ffc1ed695270a4835c2ece95ca7e833abe30d34cc.jpg)
Figure 4-6. An input sentence is first fed to a tokenizer before it can be processed by the task-specific model.

These tokens are at the core of most language models, as explored in depth in Chapter 2. A major benefit of these tokens is that they can be combined to generate representations even if they were not in the training data, as shown in Figure 4-7.

![## Image Analysis: 662f4a53dfd6944e42c4314c1110c4853e24d28c73c94d18dde3bef34ff6a5e8.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process by which a natural language processing (NLP) model, specifically a BERT Foundation model, handles an input sentence to generate word embeddings. The main purpose is to demonstrate the crucial role of tokenization, particularly sub-word tokenization, in breaking down an input sentence into manageable units that the model can process, ultimately leading to a numerical representation (word embedding). It highlights how models can still generate representations for complex or potentially unknown words by dissecting them into smaller, known sub-word units.

**Content Interpretation:**
The image depicts a workflow for natural language understanding, focusing on the generation of "Word Embedding" from an input text.The process starts with a human-readable "Input" phrase: "Her vocalization was melodic". This is the raw textual data that needs to be understood by a machine.A "Tokenizer" is the first system shown. Its function is explicitly stated as "Split input up into tokens". This is a fundamental step in NLP, breaking down a continuous text into discrete units.The diagram provides significant insight into how the tokenizer handles complex words. The word "vocalization" is not treated as a single token but is broken down into two sub-word units: "vocal" and "##ization". The "##" prefix on "##ization" indicates that it is a continuation of a previous token, a common convention in sub-word tokenization strategies (like WordPiece or BPE) used by models like BERT. This strategy allows the model to handle words it hasn't seen during training by composing them from known sub-word units.The "BERT Foundation model" is the core processing unit. The arrows show that each token ("Her", "vocal", "##ization", "was", "melodic") individually enters this model.Within the BERT model, each token is converted into a numerical representation called a "Token Embedding". These are visually represented by sequences of colored blocks (light and dark purple), where each block represents a dimension in the embedding vector. The diagram shows that "vocal" and "##ization" both receive their own token embeddings, which are then combined (implied by the converging lines) within the BERT model.The final "Output" is a "Word Embedding", visually represented by a sequence of green blocks. This signifies a comprehensive numerical representation of the entire input, or possibly the word "vocalization" in this context (given the connection lines from 'vocal' and '##ization' converging). All extracted text elements, from the "Input" sentence to the "Tokenizer" description and the individual "tokens" (including sub-word tokens), the "BERT Foundation model" label, and the "Token Embedding" and "Word Embedding" outputs, collectively support the interpretation of a structured process for converting human language into a machine-understandable numerical format, emphasizing the role of sub-word tokenization for robust vocabulary handling.

**Key Insights:**
The image provides several key takeaways and insights regarding how modern NLP models process text:Sub-word Tokenization is Crucial for Robustness: The most significant insight is that words are not always treated as atomic units. The example of "vocalization" being split into "vocal" and "##ization" explicitly demonstrates sub-word tokenization. This technique, as the document context suggests ("By breaking down an unknown word into tokens, word embeddings can still be generated."), is vital for handling out-of-vocabulary (OOV) words. If "vocalization" wasn't in the model's vocabulary, by breaking it into "vocal" (likely a common word) and "##ization" (a common suffix), the model can still assign meaningful embeddings.BERT's Role in Contextual Embeddings: The "BERT Foundation model" is identified as the component responsible for generating "Token Embedding" and subsequently "Word Embedding." BERT (Bidirectional Encoder Representations from Transformers) is known for producing contextualized embeddings, meaning the numerical representation of a word changes based on its surrounding words in a sentence. While not explicitly detailed in the diagram, the presence of BERT implies this advanced capability.Multi-step Process for Text Representation: The diagram illustrates a clear, sequential process: raw text input -> tokenization -> individual token processing within a model -> embedding generation. This shows that understanding natural language for a machine involves multiple transformations.Numerical Representation of Language: Ultimately, the output is "Word Embedding," a numerical representation (depicted as sequences of blocks). This reinforces the concept that for machines to "understand" language, it must be converted into a mathematical format that algorithms can process.The specific text elements like "Tokenizer: Split input up into tokens", the breakdown of "vocalization" into "vocal" and "##ization", and the labels "BERT Foundation model", "Token Embedding", and "Word Embedding" directly provide the evidence for these insights. They show the mechanism, the specific example of a difficult word being handled, and the ultimate goal of numerical representation.

**Document Context:**
This image is highly relevant to the document's section "Load model into pipeline" and the accompanying text "Figure 4-7. By breaking down an unknown word into tokens, word embeddings can still be generated." It serves as a concrete visual explanation of a fundamental mechanism within natural language processing pipelines, particularly when using advanced models like BERT.

**Summary:**
This diagram illustrates the process of how a sentence is transformed into a "Word Embedding" using a "BERT Foundation model," emphasizing how even complex or unfamiliar words are handled.The process begins with an **Input** sentence: "Her vocalization was melodic".Next, this sentence is fed into a component called the **Tokenizer**. The role of this Tokenizer is to "Split input up into tokens." This means it breaks down the sentence into its constituent words or sub-word units. For instance, the simple words "Her", "was", and "melodic" are treated as individual tokens.Crucially, for a more complex word like "vocalization," the Tokenizer employs a technique called sub-word tokenization. It breaks "vocalization" into two smaller units: "vocal" and "##ization." The "##" prefix indicates that "##ization" is a part of a larger word and not a standalone word. This method is vital because it allows the model to understand and generate embeddings for words it might not have encountered during its training by combining embeddings of known sub-word units.These individual tokens ("Her", "vocal", "##ization", "was", "melodic") are then passed into the **BERT Foundation model**. Inside this model, each token is converted into a numerical representation known as a **Token Embedding**, which is visually depicted as sequences of colored blocks, representing different dimensions of the numerical vector. The embeddings for "vocal" and "##ization" are processed and combined within BERT.Finally, the **Output** of this entire process is a **Word Embedding**, represented as a sequence of green blocks. This final embedding is a dense numerical vector that captures the semantic meaning and contextual information of the original input sentence or a specific word within it, making the language understandable to machine learning algorithms. This mechanism ensures that even if a word is "unknown" to the model, breaking it into smaller, recognizable tokens allows for the successful generation of meaningful numerical representations.](images/662f4a53dfd6944e42c4314c1110c4853e24d28c73c94d18dde3bef34ff6a5e8.jpg)
Figure 4-7. By breaking down an unknown word into tokens, word embeddings can still be generated.

After loading all the necessary components, we can go ahead and use our model on the test split of our data:

import numpy as np   
from tqdm import tqdm   
from transformers.pipelines.pt_utils import KeyDataset   
# Run inference   
y_pred $= \ [ ]$   
for output in tqdm(pipe(KeyDataset(data["test"], "text")),   
total $\cdot ^ { = }$ len(data["test"])): negative_score $=$ output[0]["score"] positive_score $=$ output[2]["score"] assignment $=$ np.argmax([negative_score, positive_score]) y_pred.append(assignment)

Now that we have generated our predictions, all that is left is evaluation. We create a small function that we can easily use throughout this chapter:

from sklearn.metrics import classification_report   
def evaluate_performance(y_true, y_pred): """Create and print the classification report""" performance $=$ classification_report( y_true, y_pred, target_names $\mathbf { \Psi } _ { 1 } =$ ["Negative Review", "Positive Review"]

) print(performance)

Next, let’s create our classification report:

evaluate_performance(data["test"]["label"], y_pred)

<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.76</td><td>0.88</td><td>0.81</td><td>533</td></tr><tr><td>Positive Review</td><td>0.86</td><td>0.72</td><td>0.78</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.80</td><td>1066</td></tr><tr><td>macro avg</td><td>0.81</td><td>0.80</td><td>0.80</td><td>1066</td></tr><tr><td> weighted avg</td><td>0.81</td><td>0.80</td><td>0.80</td><td>1066</td></tr></table>

To read the resulting classification report, let’s first start by exploring how we can identify correct and incorrect predictions. There are four combinations depending on whether we predict something correctly (True) versus incorrectly (False) and whether we predict the correct class (Positive) versus incorrect class (Negative). We can illustrate these combinations as a matrix, commonly referred to as a confusion matrix, in Figure 4-8.

![## Image Analysis: a4dd0c74760d3bb047455a47f89b7662abb519c126b11d6f844f208d2188e2a6.jpg

**Conceptual Understanding:**
This image represents a **confusion matrix**, which is a core conceptual tool in machine learning for evaluating the performance of classification algorithms, particularly in binary classification problems. Its main purpose is to comprehensively illustrate the types of predictions a model makes by comparing the model's outputs (Predicted values) against the true, ground-truth labels (Actual values). The image clearly communicates the key idea that classification outcomes are not simply 'right' or 'wrong,' but can be categorized into four distinct types: True Positives (correctly identified positive instances), False Positives (incorrectly identified positive instances), False Negatives (incorrectly identified negative instances), and True Negatives (correctly identified negative instances). This breakdown is crucial for understanding the specific strengths and weaknesses of a model, highlighting which types of errors it is prone to making.

**Content Interpretation:**
The image displays a confusion matrix, a standard tool for evaluating the performance of a binary classification machine learning model. It illustrates how a model's predictions align with actual outcomes across four categories: True positive (TP), False positive (FP), False negative (FN), and True negative (TN).

*   **True positive (TP):** Represents instances where the 'Predicted values' are 'Positive' and 'Actual values' are 'Positive'. The supporting text is "Positive review correctly classified as positive."
*   **False positive (FP):** Represents instances where the 'Predicted values' are 'Positive' but 'Actual values' are 'Negative'. The supporting text is "Negative review incorrectly classified as positive."
*   **False negative (FN):** Represents instances where the 'Predicted values' are 'Negative' but 'Actual values' are 'Positive'. The supporting text is "Positive review incorrectly classified as negative."
*   **True negative (TN):** Represents instances where the 'Predicted values' are 'Negative' and 'Actual values' are 'Negative'. The supporting text is "Negative review correctly classified as negative."

The significance of this data lies in providing a granular understanding of model performance. Beyond a simple accuracy score, it distinguishes between different types of correct predictions (TP, TN) and, critically, between different types of errors (FP, FN). For example, knowing the count of 'False positive (FP)' indicates how often the model incorrectly identifies a negative case as positive, while 'False negative (FN)' indicates how often it misses a true positive case. The explicit annotations like "Negative review incorrectly classified as positive" for FP provide clear, real-world examples, enhancing the understanding of each classification type.

**Key Insights:**
The image provides several key takeaways regarding the evaluation of binary classification models:

1.  **Four Prediction Outcomes:** There are precisely four possible outcomes when a binary classification model makes a prediction, defined by the intersection of 'Actual values' (Positive, Negative) and 'Predicted values' (Positive, Negative). These are 'True positive (TP)', 'False positive (FP)', 'False negative (FN)', and 'True negative (TN)'.
2.  **Distinction Between Correct Predictions:** A model can be correct in two ways: correctly identifying a positive case ('True positive (TP)', e.g., "Positive review correctly classified as positive") and correctly identifying a negative case ('True negative (TN)', e.g., "Negative review correctly classified as negative").
3.  **Two Types of Errors:** A model can be incorrect in two distinct ways, each with different implications: 'False positive (FP)' (e.g., "Negative review incorrectly classified as positive") where a negative is wrongly called positive, and 'False negative (FN)' (e.g., "Positive review incorrectly classified as negative") where a positive is wrongly called negative.
4.  **Foundation for Advanced Metrics:** These four values (TP, FP, FN, TN) are the fundamental components used to calculate more advanced model performance metrics such as precision, recall, F1-score, and specificity, which offer a more nuanced understanding than overall accuracy alone.

The specific textual evidence, such as the full phrases like "Positive review correctly classified as positive" and "Negative review incorrectly classified as positive," clearly illustrates the practical meaning of each quadrant, making the abstract concepts of TP, FP, FN, and TN concrete and understandable.

**Document Context:**
The image, a confusion matrix, is highly relevant within the "Run inference" section of a document. After a machine learning model has been developed and deployed to make predictions (i.e., 'run inference'), it is imperative to assess its performance systematically. The confusion matrix serves precisely this purpose by providing a detailed, quantitative, and qualitative breakdown of the model's prediction outcomes. It moves beyond a simple 'correct' or 'incorrect' assessment to categorize the specific nature of both accurate and erroneous predictions. This allows for a deep understanding of the model's reliability and error patterns, which is critical for model refinement, deployment decisions, and interpreting its results in real-world applications.

**Summary:**
This image displays a confusion matrix, which is a fundamental table used to describe the performance of a classification model, particularly in binary classification scenarios. It visually organizes the results of a model's predictions by comparing the model's "Predicted values" against the "Actual values" (the true, ground-truth labels). The matrix is structured with "Actual values" labeled across the top for columns ("Positive" and "Negative") and "Predicted values" labeled down the left side for rows ("Positive" and "Negative"). This arrangement creates four distinct quadrants, each representing a different outcome of the model's prediction:

1.  **True positive (TP):** Located in the top-left cell, this outcome occurs when the model predicts "Positive" and the "Actual value" is also "Positive." The associated annotation, "Positive review correctly classified as positive," clarifies that these are instances where the model made a correct positive prediction.
2.  **False positive (FP):** Found in the top-right cell, this outcome occurs when the model predicts "Positive" but the "Actual value" is "Negative." The associated annotation, "Negative review incorrectly classified as positive," explains this as a Type I error, where a negative instance is mistakenly identified as positive.
3.  **False negative (FN):** Located in the bottom-left cell, this outcome occurs when the model predicts "Negative" but the "Actual value" is "Positive." The associated annotation, "Positive review incorrectly classified as negative," defines this as a Type II error, where a positive instance is mistakenly identified as negative.
4.  **True negative (TN):** In the bottom-right cell, this outcome occurs when the model predicts "Negative" and the "Actual value" is also "Negative." The associated annotation, "Negative review correctly classified as negative," indicates these are instances where the model made a correct negative prediction.

In essence, the confusion matrix provides a comprehensive breakdown of all four types of predictions a model can make: two types of correct predictions (True Positives and True Negatives) and two types of incorrect predictions (False Positives and False Negatives). This detailed view is critical for understanding the strengths and weaknesses of a model's inference capabilities, helping to identify biases or specific types of errors it frequently commits.](images/a4dd0c74760d3bb047455a47f89b7662abb519c126b11d6f844f208d2188e2a6.jpg)
Figure 4-8. The confusion matrix describes four types of predictions we can make.

Using the confusion matrix, we can derive several formulas to describe the quality of the model. In the previously generated classification report we can see four such methods, namely precision, recall, accuracy, and the F1 score:

• Precision measures how many of the items found are relevant, which indicates the accuracy of the relevant results.   
• Recall refers to how many relevant classes were found, which indicates its ability to find all relevant results.

• Accuracy refers to how many correct predictions the model makes out of all predictions, which indicates the overall correctness of the model. • The F1 score balances both precision and recall to create a model’s overall performance.

These four metrics are illustrated in Figure 4-9, which describes them using the aforementioned classification report.

![## Image Analysis: f7e331d4de3b35bc6015f78d796f1cd777d7d23145b684ddfe040e362c2bbd24.jpg

**Conceptual Understanding:**
The image conceptually represents a 'classification report' for a machine learning model. Its main purpose is to comprehensively evaluate and present the performance of a classifier, particularly in distinguishing between two categories labeled 'Negative review' and 'Positive review'. It communicates the effectiveness of the model by providing standard metrics (precision, recall, f1-score, accuracy) both per class and as overall averages, along with the underlying sample counts (support) and the formulas used for calculation. The report allows users to understand not just how accurate the model is overall, but also its specific strengths and weaknesses for each class it attempts to predict.

**Content Interpretation:**
The image illustrates the performance evaluation of a classification model, likely a sentiment analysis model given the 'Negative review' and 'Positive review' classes. It presents the raw values for precision, recall, and f1-score for each class, along with the 'support' (number of samples per class). The 'accuracy' metric provides an overall performance score. The 'macro avg' and 'weighted avg' offer different ways to aggregate the per-class metrics, with 'weighted avg' considering the class imbalance indicated by 'support'. The presence of formulas for precision, recall, f1-score, and accuracy explicitly details how these metrics are derived from True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN). The significance lies in understanding the model's strengths and weaknesses for each class; for example, 'Negative review' has higher recall (0.88) but lower precision (0.76) compared to 'Positive review' (0.72 recall, 0.86 precision). This suggests the model is better at identifying negative reviews but might incorrectly label some positive reviews as negative, and vice-versa for positive reviews.

**Key Insights:**
1.  **Model Performance Evaluation:** The image provides a detailed view of how to evaluate a classification model using standard metrics. The metrics presented (precision, recall, f1-score, accuracy, support) are fundamental for assessing classification model quality.
2.  **Class-Specific Performance:** The model's performance varies significantly between the 'Negative review' and 'Positive review' classes. For 'Negative review', precision is 0.76 and recall is 0.88. For 'Positive review', precision is 0.86 and recall is 0.72. This indicates a trade-off: the model is better at recalling negative reviews (identifying most of them) but with more false positives, while it is more precise for positive reviews but misses more of them.
3.  **F1-Score as a Balanced Metric:** The f1-score, defined as '2 × (Precision × Recall) / (Precision + Recall)', combines precision and recall. The f1-scores are 0.81 for 'Negative review' and 0.78 for 'Positive review', suggesting slightly better balanced performance for negative reviews.
4.  **Sample Distribution (Support):** The 'support' values of 533 for both 'Negative review' and 'Positive review' indicate an equal distribution of samples across the two classes in the dataset used for evaluation, totaling '1066' samples.
5.  **Overall Accuracy:** The model achieves an 'accuracy' of '0.80', calculated as '(TP + TN) / (TP + TN + FP + FP)'. This provides a general measure of correct predictions across all classes.
6.  **Averaging Methods:** 'macro avg' (0.81 precision, 0.80 recall, 0.80 f1-score) and 'weighted avg' (0.81 precision, 0.80 recall, 0.80 f1-score) are provided. Since 'support' is equal for both classes, 'macro avg' and 'weighted avg' yield the same results in this specific instance, indicating an average performance across classes, 'Averaged across all classes'.

**Document Context:**
This image directly relates to the 'Run inference' section of a document and is explicitly described as 'Figure 4-9. The classification report describes several metrics for evaluating a model’s performance.' It serves to demonstrate the practical output and interpretation of model evaluation metrics after an inference run. It provides concrete examples of precision, recall, f1-score, support, and accuracy values for a specific model's performance on 'Negative review' and 'Positive review' classifications, which is crucial for understanding how the model performs in a real-world context.

**Summary:**
The image displays a classification report, a common tool for evaluating the performance of a machine learning model, particularly for classification tasks. It presents key metrics such as precision, recall, f1-score, and support for two distinct classes: 'Negative review' and 'Positive review'. Additionally, it includes overall accuracy and averaged metrics ('macro avg' and 'weighted avg') across all classes. The report quantifies how well the model predicts each class and its overall performance using specific formulas and numerical results.](images/f7e331d4de3b35bc6015f78d796f1cd777d7d23145b684ddfe040e362c2bbd24.jpg)
Figure 4-9. The classification report describes several metrics for evaluating a model’s performance.

We will consider the weighted average of the F1 score throughout the examples in this book to make sure each class is treated equally. Our pretrained BERT model gives us an F1 score of 0.80 (we are reading this from the weighted avg row and the $f { \cal I }$ -score column), which is great for a model not trained specifically on our domain data!

To improve the performance of our selected model, we could do a few different things including selecting a model trained on our domain data, movie reviews in this case, like DistilBERT base uncased finetuned SST-2. We could also shift our focus to another flavor of representation models, namely embedding models.

# Classification Tasks That Leverage Embeddings

In the previous example, we used a pretrained task-specific model for sentiment analysis. However, what if we cannot find a model that was pretrained for this specific task? Do we need to fine-tune a representation model ourselves? The answer is no!

There might be times when you want to fine-tune the model yourself if you have sufficient computing available (see Chapter 11). However, not everyone has access to extensive computing. This is where general-purpose embedding models come in.

# Supervised Classification

Unlike the previous example, we can perform part of the training process ourselves by approaching it from a more classical perspective. Instead of directly using the rep‐ resentation model for classification, we will use an embedding model for generating features. Those features can then be fed into a classifier, thereby creating a two-step approach as shown in Figure 4-10.

![## Image Analysis: f267c56ab8b160fd5dcb7d1234c4337729b477ed1cc61ea2b4885153211fc69a.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified two-stage machine learning pipeline for supervised classification. Its main purpose is to illustrate the clear separation and sequential relationship between the process of 'Feature extraction' and the subsequent 'Classification' step. The image conveys the idea that raw input data is first transformed into a more suitable, meaningful representation (embeddings) before being fed into a model designed to categorize or predict a class.

**Content Interpretation:**
The image depicts a sequential two-stage pipeline for supervised classification. The first stage is the 'Feature extractor,' which employs an 'Embedding model' with the explicit 'Objective: Create embeddings.' This process involves transforming raw input data into a dense vector representation (embeddings) that captures semantic information. The second stage is the 'Classifier,' which takes these generated embeddings as input. The visual representation within the classifier box, an S-shaped curve, along with the textual annotation '(e.g., logistic regression)', signifies a classification algorithm that makes predictions based on the input embeddings. The vertical dashed line between the 'Feature extractor' and 'Classifier' explicitly indicates that these are separate and distinct steps in the overall classification process.

**Key Insights:**
The main takeaways from this image are: 1. Supervised classification pipelines typically involve distinct, sequential stages for data preparation and prediction. 2. 'Feature extraction' is a critical initial step, often implemented using an 'Embedding model' to 'Create embeddings.' 3. 'Embeddings' serve as the refined input for the subsequent 'Classifier' stage. 4. The 'Classifier' performs the task of categorizing the processed data, with 'logistic regression' provided as a common example. 5. The explicit separation (indicated by the dashed line) between feature extraction and classification highlights their functional independence and allows for specialized development and optimization of each component. These insights are directly supported by the verbatim labels '1 Feature extractor', 'Embedding model', 'Objective: Create embeddings', '2 Classifier', and '(e.g., logistic regression)', as well as the visual separation.

**Document Context:**
This image directly supports the document's section on 'Supervised Classification' by visually detailing a foundational architectural pattern. The accompanying text 'Figure 4-10. The feature extraction step and classification steps are separated.' reinforces the image's core message. The diagram clarifies how, in a supervised classification workflow, an initial data preprocessing step involving feature extraction and the creation of embeddings is performed as a distinct and separate phase before the actual classification algorithm is applied. This separation is crucial for understanding the modularity and functional independence of these two key components in machine learning model development.

**Summary:**
The image illustrates a two-stage process fundamental to supervised classification, clearly separating the "Feature extractor" and "Classifier" components. The first stage, labeled "1 Feature extractor," consists of an "Embedding model" whose explicit "Objective" is to "Create embeddings." This means raw input data is processed and transformed into a meaningful, usually lower-dimensional, numerical representation called embeddings. An arrow indicates the flow of these embeddings from the feature extractor to the next stage. A vertical dashed line visually emphasizes the distinct separation between these two stages. The second stage, labeled "2 Classifier," is represented by a grey rounded rectangular box containing a purple S-shaped curve and small black dots, which is a common visual representation for a sigmoid function, often used in classification models. Below this visual representation, the text "(e.g., logistic regression)" provides a specific example of a classification algorithm that would operate at this stage. This entire diagram highlights a common machine learning pipeline where data transformation (feature extraction) is a prerequisite for and distinct from the predictive modeling (classification) task.](images/f267c56ab8b160fd5dcb7d1234c4337729b477ed1cc61ea2b4885153211fc69a.jpg)
Figure 4-10. The feature extraction step and classification steps are separated.

A major benefit of this separation is that we do not need to fine-tune our embedding model, which can be costly. In contrast, we can train a classifier, like a logistic regression, on the CPU instead.

In the first step, we convert our textual input to embeddings using the embedding model as shown in Figure 4-11. Note that this model is similarly kept frozen and is not updated during the training process.

![## Image Analysis: 8e457f93ab024a5bd264725e3e53c74ceae577356b0b573990e28b39e750931c.jpg

**Conceptual Understanding:**
This image conceptually represents the initial feature extraction phase for text data in a machine learning pipeline. Its main purpose is to illustrate how raw textual input is converted into numerical vector embeddings using a pre-trained, fixed 'Embedding model'. The image conveys the key idea that textual information needs to be transformed into a numerical representation to be processed by algorithms, and that this transformation can be achieved using a 'Frozen' or 'Nontrainable' model, implying the use of pre-computed or externally trained embeddings.

**Content Interpretation:**
The image depicts a foundational data preprocessing step in natural language processing (NLP) for machine learning. It specifically shows the transformation of raw textual input into numerical vector representations, known as embeddings. The process involves an 'Embedding model' whose 'Objective' is to 'Create embeddings'. The text 'Nontrainable' and the term '"Frozen"' (within an iceberg graphic) indicate that this embedding model is pre-trained and its internal parameters are fixed, meaning it will not be updated or learned further during subsequent model training phases. The input 'Best movie ever!' serves as a concrete example of text that undergoes this transformation. The red squares symbolize the resulting numerical embeddings, which are the feature representations of the input text. This process is crucial for converting unstructured text into a structured format that machine learning models can process.

**Key Insights:**
The main takeaway from this image is that text data must be transformed into a numerical format, specifically embeddings, before it can be used by most machine learning models for tasks like supervised classification. The image highlights that 'Embedding model' has the 'Objective: Create embeddings'. A crucial insight is that these embedding models can be 'Nontrainable' or '"Frozen"', meaning they are pre-trained and used as-is for feature extraction, providing a stable and consistent representation of input text without being modified during the training of the downstream classification model. This practice ensures that the feature extraction process is consistent and often leverages robust, general-purpose representations learned from large text corpora.

**Document Context:**
This image directly supports the 'Supervised Classification' section of the document, specifically detailing 'step 1' as mentioned in the accompanying text: 'Figure 4-11. In step 1, we use the embedding model to extract the features and convert the input text to embeddings.' It provides a clear visual explanation of how the initial textual data is prepared for a supervised learning task. By showing the text being converted into embeddings using a 'nontrainable' model, it sets the stage for how features are extracted before they are fed into a classifier, which would typically be the next step in a supervised classification pipeline.

**Summary:**
The image illustrates the initial step in a supervised classification process, specifically how raw text input is converted into numerical embeddings. It begins with an 'Input' text, exemplified by the phrase 'Best movie ever!'. This input is fed into an 'Embedding model'. The explicit 'Objective' of this model is to 'Create embeddings'. The embedding model itself is highlighted as 'Nontrainable' and '"Frozen"', visually represented by an iceberg, indicating that its parameters are static and do not undergo further training. The output of this model is a series of red squares, which visually represent the generated embeddings. The process flow moves sequentially from the text input, through the embedding model, to the output of embeddings, signifying a transformation from textual data to a numerical representation suitable for machine learning algorithms.](images/8e457f93ab024a5bd264725e3e53c74ceae577356b0b573990e28b39e750931c.jpg)
Figure 4-11. In step 1, we use the embedding model to extract the features and convert the input text to embeddings.

We can perform this step with sentence-transformer, a popular package for lever‐ aging pretrained embedding models.6 Creating the embeddings is straightforward:

from sentence_transformers import SentenceTransformer # Load model model $=$ SentenceTransformer("sentence-transformers/all-mpnet-base-v2")

# Convert text to embeddings train_embeddings $=$ model.encode(data["train"]["text"], show_progress_bar=True) test_embeddings $=$ model.encode(data["test"]["text"], show_progress_bar $=$ True)

As we covered in Chapter 1, these embeddings are numerical representations of the input text. The number of values, or dimension, of the embedding depends on the underlying embedding model. Let’s explore that for our model:

train_embeddings.shape (8530, 768)

This shows that each of our 8,530 input documents has an embedding dimension of 768 and therefore each embedding contains 768 numerical values.

In the second step, these embeddings serve as the input features to the classifier illus‐ trated in Figure 4-12. The classifier is trainable and not limited to logistic regression and can take on any form as long as it performs classification.

![## Image Analysis: 75e79fbb7394f92632023fa7298659c53def1154fadebf8a300d8cac3005880a.jpg

**Conceptual Understanding:**
This image conceptually represents a single step in a machine learning classification process. Its main purpose is to visualize how an input feature vector is passed into a trainable classifier, which then processes this input to yield a binary output. The core message is that an input (likely pre-processed, such as embeddings) goes through a learning component to make a categorical decision, in this case, classifying it as 'Positive' or 'Negative (implied if not positive).

**Content Interpretation:**
The image illustrates a core component of a machine learning classification pipeline. It shows how prepared input features are fed into a trainable model (likely a single neuron or a logistic regression model) that processes these features and outputs a binary classification decision. The sigmoid-like curve within the trainable component suggests the use of an activation function commonly found in neural networks or logistic regression. The overall system depicted is performing a binary classification task, categorizing the input into one of two classes, specifically showing a 'Positive' outcome.

**Key Insights:**
The main takeaways from this image are: 1. Machine learning models require specific, prepared inputs for classification, referred to as "Input for classifier." 2. Classification tasks involve a "Trainable" component, which is a model or layer whose parameters are learned from data (e.g., a logistic regression model or a neural network layer). 3. The output of such a model in a binary classification scenario is a distinct prediction, exemplified here as "1 Positive." This diagram provides a visual understanding of the data flow and the role of a trainable component in generating a classification result. The textual evidence "Input for classifier", "Trainable", and "1 Positive" directly support these insights by clearly labeling the function of each part of the process.

**Document Context:**
This image directly illustrates the subsequent step after text has been converted into embeddings, as described in the document context: "Convert text to embeddings train_embeddings = model.encode(data["train"]["text"], show_progress_bar=True) test_embeddings = model.encode(data["test"]["text"], show_progress_bar = True)". The text following the image, "Figure 4-12. Using the embeddings as our features, we train a logistic regression model on our training data," explicitly links the input shown in the diagram to these embeddings and the trainable component to the logistic regression model being trained. The diagram visualizes how these pre-computed embeddings are used as features to train and then classify data, resulting in a binary prediction.

**Summary:**
The image depicts a simplified, high-level process of a classification model, likely a single layer of a neural network or a logistic regression model, as it processes an input to produce a binary classification outcome. The process starts with an undefined preceding input, indicated by a series of three vertical dots, representing a continuous flow from previous steps. This input then feeds into a segmented red rectangular block, which is explicitly labeled as the "Input for classifier". This signifies that the preceding data has been prepared and is now ready to be consumed by a classification algorithm. Following this, the flow proceeds to a gray rounded rectangular shape. This shape contains a purple S-shaped curve, characteristic of a sigmoid activation function, and is annotated with small circles at the top and dots at the bottom, suggesting input and output nodes of a computational unit. Crucially, this component is labeled "Trainable" with an accompanying flame icon, indicating that its internal parameters can be adjusted during a training process to learn from data. Finally, an arrow leads from the trainable component to a green rounded rectangular box on the left, which is labeled "Output" and contains the text "1 Positive". This signifies the final prediction of the classifier, indicating that the input has been classified into the 'Positive' class, represented numerically by '1'.](images/75e79fbb7394f92632023fa7298659c53def1154fadebf8a300d8cac3005880a.jpg)
Figure 4-12. Using the embeddings as our features, we train a logistic regression model on our training data.

We will keep this step straightforward and use a logistic regression as the classifier. To train it, we only need to use the generated embeddings together with our labels:

from sklearn.linear_model import LogisticRegression # Train a logistic regression on our train embeddings clf $=$ LogisticRegression(random_state $= 4 2$ ) clf.fit(train_embeddings, data["train"]["label"])

Next, let’s evaluate our model:

# Predict previously unseen instances y_pred $=$ clf.predict(test_embeddings) evaluate_performance(data["test"]["label"], y_pred)

<table><tr><td> precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.85</td><td>0.86</td><td>0.85 533</td></tr><tr><td>Positive Review</td><td>0.86</td><td>0.85</td><td>0.85 533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.85 1066</td></tr><tr><td>.macro avg</td><td>0.85</td><td>0.85</td><td>0.85 1066</td></tr><tr><td> weighted avg</td><td>0.85</td><td>0.85</td><td>0.85 1066</td></tr></table>

By training a classifier on top of our embeddings, we managed to get an F1 score of 0.85! This demonstrates the possibilities of training a lightweight classifier while keeping the underlying embedding model frozen.

![## Image Analysis: 7f9f2b5ea5af1fa8ff1ce5fbe949db02e5d73bf21c1e2bbd877ef25a5b1dcafe.jpg

**Conceptual Understanding:**
This image conceptually represents a stylized animal, most likely a lemur or a similar agile creature, contained within a square outline. Its main purpose is to function as a logo or an emblem, serving as a visual identifier or brand mark for an organization, project, or author. It communicates an identity rather than a specific technical concept or data-driven message. The green color might evoke themes of nature, growth, or vitality, while the animal's agile posture could imply adaptability or dynamic qualities associated with the entity it represents.

**Content Interpretation:**
The image is a logo featuring a stylized green silhouette of an animal, possibly a lemur, characterized by its long, curly tail and agile posture within a square frame. This logo does not depict any processes, concepts, relationships, or systems related to the document's technical content. Instead, it serves as a visual identifier or branding element. There is no data, trends, or specific information presented beyond the logo itself. As there is no text present within the image, all interpretations are based solely on its visual composition.

**Key Insights:**
The primary takeaway from this image is that it serves as a visual identifier or a brand logo. It conveys a sense of identity for the associated document or organization. No specific technical insights, data-driven conclusions, or lessons related to machine learning or document content can be extracted from this image, as it is purely an emblem. As there is no textual content within the image, no specific text elements provide evidence for any insights beyond its visual role as a logo.

**Document Context:**
Given the document context, which discusses 'Predict previously unseen instances y_pred = clf.predict(test_embeddings) evaluate_performance(data["test"]["label"], y_pred)', the image, a logo, does not directly contribute to the technical explanation of machine learning predictions or performance evaluation. Its relevance is primarily as a branding element, likely indicating the author, institution, or project associated with the technical document. It provides an identity mark rather than substantive technical information, fitting into the document's broader narrative by establishing its source or affiliation.

**Summary:**
The image displays a green silhouette of an animal, likely a lemur or a similar primate, characterized by a long, distinctive, and curly tail that extends upwards and loops over its back. The animal is depicted in a walking or pouncing posture, with its body facing left and its head turned towards the viewer. It has four legs, and its body is sleek and stylized. The entire silhouette is enclosed within a simple, unadorned square outline. There is no text, labels, annotations, or any other textual elements present within this image. The image functions as a logo or an emblem, conveying a brand identity or representing an organization, rather than illustrating a process or providing data. Given the document context regarding machine learning predictions and evaluation, this logo likely serves as a branding element for the document's author, publisher, or a specific project, identifying the source of the content rather than contributing directly to the technical explanation.](images/7f9f2b5ea5af1fa8ff1ce5fbe949db02e5d73bf21c1e2bbd877ef25a5b1dcafe.jpg)

In this example, we used sentence-transformers to extract our embeddings, which benefits from a GPU to speed up inference. However, we can remove this GPU dependency by using an exter‐ nal API to create the embeddings. Popular choices for generating embeddings are Cohere’s and OpenAI’s offerings. As a result, this would allow the pipeline to run entirely on the CPU.

# What If We Do Not Have Labeled Data?

In our previous example, we had labeled data that we could leverage, but this might not always be the case in practice. Getting labeled data is a resource-intensive task that can require significant human labor. Moreover, is it actually worthwhile to collect these labels?

To test this, we can perform zero-shot classification, where we have no labeled data to explore whether the task seems feasible. Although we know the definition of the labels (their names), we do not have labeled data to support them. Zero-shot classification attempts to predict the labels of input text even though it was not trained on them, as shown in Figure 4-13.

![## Image Analysis: 265bd0d259380b761bd1a21bd7d9342b925527b78320c1853d92e2e4fa49c63a.jpg

**Conceptual Understanding:**
The image conceptually represents the workflow of a zero-shot classification system. Its main purpose is to illustrate how a machine learning model can classify an input into categories it has not been explicitly trained on, relying instead on its general understanding of language and the semantic meaning of the provided labels. The core idea communicated is the ability to predict 'unseen labels' by relating the input content to candidate labels without requiring labeled examples for those specific labels.

**Content Interpretation:**
The image demonstrates a zero-shot classification process. The process takes a textual input and a set of candidate labels (which the model has not been previously trained on with specific examples) and outputs a probabilistic classification across these labels. The input "Explore the world's flavors through global culinary adventures." is analyzed against candidate labels "travel", "cooking", and "sports". The zero-shot model then predicts the likelihood of the input belonging to each of these unseen categories. The output shows that "Cooking" is the most probable category for the given input with a score of .60, followed by "Travel" at .35, and "Sports" at .05. This illustrates the model's ability to generalize and classify new, previously unencountered categories based on its understanding of language and relationships between concepts.

**Key Insights:**
The main takeaway is that zero-shot classification allows a model to categorize input data into 'unseen' labels without prior direct training on those specific labels. The image clearly shows that the model's 'Objective' is to 'Predict unseen labels'. The input text is evaluated against the semantic meaning of the candidate labels, and the model outputs a probability distribution over these labels, demonstrating its ability to infer relationships. For instance, the input about 'culinary adventures' is correctly associated most strongly with 'cooking' (0.60), showing the model's semantic understanding even for new categories. This highlights the power of zero-shot learning in scenarios where labeled data for all possible categories is unavailable.

**Document Context:**
This image directly supports the document's section titled "What If We Do Not Have Labeled Data?" by visually explaining the concept of zero-shot classification. The text immediately following the image, "Figure 4-13. In zero-shot classification, we have no labeled data, only the labels themselves. The zero-shot model decides how the input is related to the candidate labels," perfectly aligns with the diagram. The image provides a concrete example of how a model can perform classification tasks when direct labeled training data for specific categories is absent, using only the input and the descriptive labels themselves. This is crucial for understanding how machine learning models can handle novel categories without extensive retraining.

**Summary:**
The image illustrates the process of zero-shot classification, where an input text is categorized into previously unseen labels. The process begins with an 'Input' text: "Explore the world's flavors through global culinary adventures." Simultaneously, 'Candidate labels', explicitly noted as '(Previously unseen labels)', are provided, consisting of "travel", "cooking", and "sports". Both the 'Input' text and the 'Candidate labels' are fed into a 'Zero-shot model'. The objective of this model is clearly stated as "Objective: Predict unseen labels". Following processing by the zero-shot model, the 'Output' is generated. This output is presented as a set of probabilities for each candidate label: "Cooking" has a probability of ".60", "Travel" has a probability of ".35", and "Sports" has a probability of ".05". This demonstrates how the model assigns a likelihood of the input text belonging to each of the provided (unseen) categories, with "Cooking" being the most probable classification in this instance. The entire flow illustrates how a model can classify data without having been explicitly trained on examples for those specific categories.](images/265bd0d259380b761bd1a21bd7d9342b925527b78320c1853d92e2e4fa49c63a.jpg)
Figure 4-13. In zero-shot classification, we have no labeled data, only the labels them‐ selves. The zero-shot model decides how the input is related to the candidate labels.

To perform zero-shot classification with embeddings, there is a neat trick that we can use. We can describe our labels based on what they should represent. For example, a negative label for movie reviews can be described as “This is a negative movie review.” By describing and embedding the labels and documents, we have data that we can work with. This process, as illustrated in Figure 4-14, allows us to generate our own target labels without the need to actually have any labeled data.

![## Image Analysis: 920bc9dfeeb24b102e3f920b0068561b50ac97cbf9ff2b189312bd7b85663271.jpg

**Conceptual Understanding:**
This image conceptually represents a method for generating vector embeddings for both a document and its potential classification labels. The main purpose is to show how abstract labels can be converted into descriptive text, which then allows an "Embedding model" to create numerical representations (embeddings) for these labels, similar to how it creates embeddings for the actual document text. This process is critical for understanding and comparing documents with their semantic categories in a shared vector space, especially in contexts lacking direct labeled data.

**Content Interpretation:**
The image depicts a conceptual pipeline for generating numerical vector representations (embeddings) for both a piece of text (a document) and its associated classification labels. It highlights a common technique in natural language processing where labels, initially represented as abstract categories (e.g., 0, 1), are given descriptive textual forms (e.g., "A negative review", "A positive review") so that they can also be processed by the same embedding model. The core concept is that both the document content and the descriptive labels are transformed into a common embedding space, allowing for comparison and analysis, particularly useful in tasks like zero-shot or few-shot learning where explicit labeled data might be scarce.

**Key Insights:**
The main takeaway is that semantic labels can be represented as textual descriptions, which then allows an "Embedding model" to convert them into numerical embeddings. This technique is particularly useful for scenarios where pre-labeled data is scarce or non-existent, enabling the use of descriptive text to generate representations for categories. The process demonstrates that both the original document (e.g., "Best movie ever!") and the descriptive labels ("A negative review," "A positive review") are processed through the same embedding model, resulting in comparable "Document embeddings" and "Label embeddings." This implies that the semantic meaning of the labels can be captured and utilized alongside document semantics.

**Document Context:**
This image directly supports the document's section "What If We Do Not Have Labeled Data?" by illustrating a method to work with labels even when direct labeled data is unavailable. The text after the image, "Figure 4-14. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers," perfectly aligns with the diagram's flow, showing how abstract labels (0, 1) are first converted into descriptive text ("A negative movie review," "A positive movie review") and then processed by an embedding model (like sentence-transformers mentioned in the text) to create usable label embeddings. This method is crucial for leveraging semantic similarity when traditional supervised learning is not feasible due to a lack of pre-labeled examples.

**Summary:**
The diagram illustrates a process for creating embeddings for both a document and its candidate labels using an "Embedding model." The process begins with two main inputs: a "Document" and "Candidate labels." The "Document" is represented by the text "Best movie ever!". The "Candidate labels" are initially provided as "0 (negative)" and "1 (positive)." These labels are then explicitly described as "A negative review" and "A positive review" respectively, indicated by the arrow label "Describe labels." All these inputs – the original document text and the textual descriptions of the labels – are fed into the central "Embedding model," which has the "Objective: create embeddings." This model then outputs corresponding embeddings. The output "Document embeddings" are shown as a blue bar chart representing ""Best movie ever!"". Similarly, the "Label embeddings" are generated for the label descriptions: a red bar chart for ""A negative review"" and a green bar chart for ""A positive review"". This entire process demonstrates how an embedding model can transform both raw text and descriptive labels into a numerical vector space for further machine learning tasks.](images/920bc9dfeeb24b102e3f920b0068561b50ac97cbf9ff2b189312bd7b85663271.jpg)
Figure 4-14. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers.

We can create these label embeddings using the .encode function as we did earlier:

# Create embeddings for our labels label_embeddings $=$ model.encode(["A negative review", "A positive review"])

To assign labels to documents, we can apply cosine similarity to the document label pairs. This is the cosine of the angle between vectors, which is calculated through the dot product of the embeddings and divided by the product of their lengths, as illustrated in Figure 4-15.

![## Image Analysis: 448abda12fb4e8be27942313f3ac3377d6175e583c8f250ad79efb22c7401875.jpg

**Conceptual Understanding:**
The image conceptually represents the use of vector embeddings and cosine similarity for text classification, specifically sentiment analysis. The main purpose is to illustrate how a given document's embedding can be compared to the embeddings of predefined labels (e.g., 'positive' and 'negative') using cosine similarity to determine its closest classification. The key idea communicated is that semantic similarity between texts can be geometrically interpreted as the angular proximity of their vector representations.

**Content Interpretation:**
The image illustrates the process of text classification using vector embeddings and cosine similarity. Each piece of text (the document and the two labels) is represented as a vector in a multidimensional space (simplified to 2D for visualization). The closer two vectors are in terms of angle, the more semantically similar their corresponding texts are. The 'document' here is the phrase "Best movie ever!". The 'labels' are "A positive movie review" and "A negative movie review". The angles θ₁ and θ₂ quantify the cosine similarity, with smaller angles indicating greater similarity. In this depiction, the document "Best movie ever!" is visually much closer (smaller angle θ₁) to "A positive movie review" than it is (larger angle θ₂) to "A negative movie review", implying a positive classification for the document. This method allows for a quantitative comparison of semantic relationships between a target text and predefined categories.

**Key Insights:**
The main takeaway from this image is the practical application of cosine similarity in text classification. Specifically, it demonstrates that: 1. Text can be represented as vectors (embeddings) in a semantic space. 2. The similarity between two texts can be quantified by the cosine of the angle between their corresponding vectors. 3. A smaller angle (like θ₁) indicates higher semantic similarity, while a larger angle (like θ₂) indicates lower semantic similarity. 4. This technique can be used to classify a document by comparing its embedding to the embeddings of various labels and choosing the label with the highest similarity (smallest angle). The explicit text 'θ₁= cosine similarity between document and positive label' and 'θ₂= cosine similarity between document and negative label' are crucial in understanding how these angles directly translate to similarity scores for classification.

**Document Context:**
This image directly supports the document's narrative on creating embeddings for labels and using them for classification. The preceding text mentions 'label_embeddings = model.encode(["A negative review", "A positive review"])', which aligns perfectly with the 'A positive movie review' and 'A negative movie review' labels shown as vectors in the diagram. The subsequent text, 'Figure 4-15. The cosine similarity is the angle between two vectors or embeddings. In this example, we calculate the similarity between a document and the two possible labels, positive and negative.', explicitly describes the content of this figure. The image visually demonstrates the practical application of calculating cosine similarity to classify a document (e.g., a movie review) against predefined sentiment labels.

**Summary:**
This image is a two-dimensional plot illustrating the concept of cosine similarity between text embeddings, specifically for classifying a document's sentiment. It shows three distinct vectors originating from the origin, each representing an embedding in a conceptual space. The blue vector points towards a label described as "Best movie ever!", which represents the document being classified. The green vector points towards the label "A positive movie review". The red vector points towards the label "A negative movie review". The similarity between the document and each label is measured by the angle between their respective vectors. Theta_1 (θ₁) is depicted as the angle between the blue vector (document) and the green vector (positive label), indicated by an orange dotted arc. Theta_2 (θ₂) is depicted as the angle between the blue vector (document) and the red vector (negative label), indicated by a black dotted arc. The textual definitions below the axes clarify these angles: "θ₁= cosine similarity between document and positive label" and "θ₂= cosine similarity between document and negative label". Visually, θ₁ is significantly smaller than θ₂, indicating that the document "Best movie ever!" has a higher cosine similarity (and thus higher semantic similarity) to "A positive movie review" than to "A negative movie review".](images/448abda12fb4e8be27942313f3ac3377d6175e583c8f250ad79efb22c7401875.jpg)
Figure 4-15. The cosine similarity is the angle between two vectors or embeddings. In this example, we calculate the similarity between a document and the two possible labels, positive and negative.

We can use cosine similarity to check how similar a given document is to the descrip‐ tion of the candidate labels. The label with the highest similarity to the document is chosen as illustrated in Figure 4-16.

![## Image Analysis: f3d7f1218d13a2d81dd3b286f11e482cc0b055c1c6e0973aa70e405dd306001c.jpg

**Conceptual Understanding:**
The image conceptually represents the process of sentiment classification using vector embeddings and cosine similarity. Its main purpose is to demonstrate how a given text ("Best movie ever!") is classified as either positive or negative by comparing its numerical representation (embedding) with the embeddings of predefined sentiment labels. The core idea communicated is that the text is assigned the sentiment of the label with which it has the highest cosine similarity score, indicating a stronger semantic resemblance. The image explicitly shows the comparison between a document embedding and two label embeddings, resulting in similarity scores that inform the final classification decision.

**Content Interpretation:**
The image shows the process of classifying a given text, "Best movie ever!", based on its semantic similarity to predefined sentiment labels, "A negative review" and "A positive review", using cosine similarity. The blue boxes represent the embedding of the input document ("Best movie ever!"). The red boxes represent the embedding of the "A negative review" label, and the green boxes represent the embedding of the "A positive review" label. The numerical results (.08 and .92) are the cosine similarity scores, indicating how closely related the input document is to each label's embedding. The higher score (.92) signifies a stronger correlation with the "A positive review" label, leading to the final classification of "Positive" for the input text. This illustrates a method for automated sentiment analysis where a document's vector representation is compared against label vector representations to determine its category.

**Key Insights:**
The main takeaway from this image is that cosine similarity can be effectively used to determine the semantic relationship between a piece of text (like a document or review) and various labels. A higher cosine similarity score indicates a stronger semantic match. In the context of sentiment analysis, this means that if a document's embedding has a higher cosine similarity with the "positive review" label's embedding than with the "negative review" label's embedding, the document is classified as positive. This is evidenced by the text: "Cosine similarity (" [blue boxes for "Best movie ever!"] ", " [red boxes for "A negative review"] ") = .08" and "Cosine similarity (" [blue boxes for "Best movie ever!"] ", " [green boxes for "A positive review"] ") = .92". The significant difference between .08 and .92 clearly indicates a stronger positive sentiment, leading to the "Positive" classification.

**Document Context:**
This image directly illustrates the step described in the accompanying document context: "After embedding the label descriptions and the documents, we can use cosine similarity for each label document pair." It visually demonstrates how the embeddings, which are created as mentioned in the preceding text "label_embeddings = model.encode(["A negative review", "A positive review"]) ", are then used in cosine similarity calculations to classify a document (in this case, "Best movie ever!"). The image shows the practical application of the embedding and similarity concepts for sentiment analysis, where a document's sentiment is determined by comparing its embedding to pre-embedded positive and negative review labels.

**Summary:**
The image illustrates the application of cosine similarity for text classification, specifically sentiment analysis. It presents two calculations to determine the sentiment of the phrase "Best movie ever!". In the first calculation, the cosine similarity between the embedding of "Best movie ever!" (represented by four blue boxes) and the embedding of "A negative review" (represented by four red boxes) is .08. In the second calculation, the cosine similarity between the embedding of "Best movie ever!" (also four blue boxes) and the embedding of "A positive review" (represented by four green boxes) is .92. A vertical line connects the two similarity scores, .08 and .92, and an arrow points from this line to the right, labeling the outcome as "Positive". This demonstrates that because the similarity score with "A positive review" (.92) is significantly higher than with "A negative review" (.08), the original phrase "Best movie ever!" is classified as "Positive". The description is organized from the overall concept of cosine similarity in sentiment analysis down to the specific scores and their implications for classification.](images/f3d7f1218d13a2d81dd3b286f11e482cc0b055c1c6e0973aa70e405dd306001c.jpg)
Figure 4-16. After embedding the label descriptions and the documents, we can use cosine similarity for each label document pair.

To perform cosine similarity on the embeddings, we only need to compare the document embeddings with the label embeddings and get the best matching pairs:

from sklearn.metrics.pairwise import cosine_similarity # Find the best matching label for each document sim_matrix $=$ cosine_similarity(test_embeddings, label_embeddings) y_pred $=$ np.argmax(sim_matrix, axis $^ { , = 1 }$ )

And that is it! We only needed to come up with names for our labels to perform our classification tasks. Let’s see how well this method works:

evaluate_performance(data["test"]["label"], y_pred)

<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.78</td><td>0.77</td><td>0.78</td><td>533</td></tr><tr><td>Positive Review</td><td>0.77</td><td>0.79</td><td>0.78</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.78</td><td>1066</td></tr><tr><td>macro avg</td><td>0.78</td><td>0.78</td><td>0.78</td><td>1066</td></tr><tr><td>weighted avg</td><td>0.78</td><td>0.78</td><td>0.78</td><td>1066</td></tr></table>

If you are familiar with zero-shot classification with Transformerbased models, you might wonder why we choose to illustrate this with embeddings instead. Although natural language inference models are amazing for zero-shot classification, the example here demonstrates the flexibility of embeddings for a variety of tasks. As you will see throughout the book, embeddings can be found in most Language AI use cases and are often an underestimated but incredibly vital component.

An F1 score of 0.78 is quite impressive considering we did not use any labeled data at all! This just shows how versatile and useful embeddings are, especially if you are a bit creative with how they are used.

Let’s put that creativity to the test. We decided upon “A nega‐ tive/positive review” as the name of our labels but that can be improved. Instead, we can make them a bit more concrete and specific toward our data by using “A very negative/positive movie review” instead. This way, the embedding will capture that it is a movie review and will focus a bit more on the extremes of the two labels. Try it out and explore how it affects the results.

# Text Classification with Generative Models

Classification with generative language models, such as OpenAI’s GPT models, works a bit differently from what we have done thus far. These models take as input some text and generative text and are thereby aptly named sequence-to-sequence models. This is in stark contrast to our task-specific model, which outputs a class instead, as illustrated in Figure 4-17.

![## Image Analysis: 01e0ed2ef86c77b6f4267f8f67e29dcaa15578d5da6b6973adec85d95f2d1307.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental differences in input/output mechanisms and objectives between two primary types of Natural Language Processing (NLP) models: task-specific models (exemplified by classification) and generative models. Its main purpose is to clearly differentiate between "sequence-to-value" models (task-specific models for classification) and "sequence-to-sequence" models (generative models). It highlights that while both take sequences of tokens as input, their objectives and the nature of their outputs differ significantly. Key ideas communicated include: text representation as "Sequence (of tokens)", the distinction between "Task-specific model" and "Generative model", their respective objectives ("perform classification" vs. "generate text"), the different output types ("Numerical (value)" vs. "Sequence (of tokens)"), and the overarching model paradigms of "Sequence-to-value model" and "Sequence-to-sequence model".

**Content Interpretation:**
The image displays two distinct Artificial Intelligence (AI) processes: 1. A Text Classification Process where an input "Sequence (of tokens)" ("Best movie ever !") is processed by a "Task-specific model" with the "Objective: perform classification", resulting in a "Numerical (value)" output ("1"). This demonstrates a many-to-one transformation from text to a single label. 2. A Text Generation Process where an input "Sequence (of tokens)" ("What is 1 + 1 ?") is processed by a "Generative model" with the "Objective: generate text", resulting in an output "Sequence (of tokens)" ("The answer is 2"). This shows a many-to-many transformation from one text sequence to another. The relationship highlighted is a contrast between these two paradigms: Task-specific models (like classifiers) are designed for discriminative tasks resulting in a single value, explicitly labeled as a "Sequence-to-value model"; while Generative models create new content, explicitly labeled as a "Sequence-to-sequence model". The extracted text elements such as "Input" and "Output" headers, specific example token sequences like "Best movie ever !" and "What is 1 + 1 ?", model names ("Task-specific model", "Generative model"), their stated objectives ("Objective: perform classification", "Objective: generate text"), and the nature of their outputs ("1" / "Numerical (value)" and "The answer is 2" / "Sequence (of tokens)") all provide direct evidence for these distinct processes and their transformations, reinforcing the labels "Sequence-to-value model" and "Sequence-to-sequence model".

**Key Insights:**
The main takeaways from this image are: 1. A fundamental distinction exists between AI models designed for specific tasks (e.g., classification) and those for generating content. This is evidenced by the contrasting labels "Task-specific model" vs. "Generative model" and their distinct objectives and outputs. 2. Both model types process "Sequence (of tokens)" as "Input", but their "Output" varies significantly: a single "Numerical (value)" for classification, or another "Sequence (of tokens)" for generation. This is supported by the consistent "Sequence (of tokens)" for inputs and the divergent outputs like "1" (Numerical) and "The answer is 2" (Sequence of tokens). 3. The paradigms "Sequence-to-value model" and "Sequence-to-sequence model" precisely describe the data transformation. Task-specific classification is a "Sequence-to-value model", while generative text models are "Sequence-to-sequence models". These labels are explicitly present as dotted line annotations. 4. The model choice is driven by the specific objective, whether it's to "perform classification" or to "generate text". The explicit "Objective" text within each model box directly supports this insight, linking the task goal to the model's design and output format. The image supports the conclusion that understanding the desired output and objective is crucial for selecting an appropriate AI model: "Sequence-to-value models" for discrete judgments (like sentiment analysis), and "Sequence-to-sequence models" for creative or conversational responses (like chatbots).

**Document Context:**
Given the document context "Section: Text Classification with Generative Models" and the accompanying text "Figure 4-17. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens.", this image serves as a foundational visual explanation. It disambiguates and illustrates the core differences between task-specific models (specifically for classification) and generative models. The diagram visually clarifies the concepts of a "task-specific model generates numerical values from sequences of tokens" and a "generative model generates sequences of tokens from sequences of tokens", which are central to understanding text classification using generative models. It sets the stage for more detailed discussions by establishing these basic distinctions between model functionalities and their input/output behaviors.

**Summary:**
This diagram illustrates the fundamental differences between two types of Artificial Intelligence models that process textual data: task-specific models and generative models, by presenting two parallel workflows. The first workflow, representing a "Task-specific model", begins with an "Input" as a "Sequence (of tokens)" ("Best movie ever !"). This sequence is fed into a "Task-specific model" whose "Objective: perform classification". The output is a single "Numerical (value)", shown as "1". This entire process is labeled as a "Sequence-to-value model", indicating that a sequence of tokens is reduced to a single numerical output. The second workflow, representing a "Generative model", also starts with an "Input" as a "Sequence (of tokens)" ("What is 1 + 1 ?"). This input is processed by a "Generative model" with the "Objective: generate text". The output is another "Sequence (of tokens)" ("The answer is 2"). This process is labeled as a "Sequence-to-sequence model", signifying that an input sequence of tokens is transformed into an output sequence of tokens. In essence, the diagram highlights that while both models handle tokenized text, their objectives and the nature of their outputs differ significantly: classification models yield a single value, whereas generative models produce new text sequences.](images/01e0ed2ef86c77b6f4267f8f67e29dcaa15578d5da6b6973adec85d95f2d1307.jpg)
Figure 4-17. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens.

These generative models are generally trained on a wide variety of tasks and usually do not perform your use case out of the box. For instance, if we give a generative model a movie review without any context, it has no idea what to do with it.

Instead, we need to help it understand the context and guide it toward the answers that we are looking for. As demonstrated in Figure 4-18, this guiding process is done mainly through the instruction, or prompt, that you give such a model. Iteratively improving your prompt to get your preferred output is called prompt engineering.

![## Image Analysis: 8b816ac90a8a3cb0f362bd74b9d7722ecb9d8d633e4f22703079f6ca76264bdf.jpg

**Conceptual Understanding:**
The image conceptually illustrates the process of **prompt engineering** within the context of **text classification using generative models**. Its main purpose is to demonstrate how different phrasings of an 'Input (prompt)' for a 'Generative model' (which has the 'Objective: generate text') can lead to significantly varied 'Output (completion)' for the same underlying piece of text being analyzed, in this case, a movie review with the content "Best movie ever!". The diagram specifically highlights the iterative nature of finding the 'Best prompt for the preferred output' to achieve a desired classification format, moving from verbose explanations to a concise, direct label like 'Positive'.

**Content Interpretation:**
The image displays a workflow for text classification using a 'Generative model' with the 'Objective: generate text'. It illustrates three different prompting strategies for sentiment analysis of the same movie review, "Best movie ever!", and shows the varied outputs or 'completion' generated by the model for each prompt.

1.  **Prompt 1:** "Rate the sentiment." + "Best movie ever!" leads to a descriptive output that rates the sentiment numerically and provides a rationale: "Given a typical sentiment rating scale it would be rated as 5 due to its highly positive connotation."
2.  **Prompt 2:** "What sentiment does this review have?" + "Best movie ever!" leads to a more direct, sentence-based sentiment statement: "The review expresses a positive sentiment."
3.  **Prompt 3:** "Is this movie review negative or positive?" + "Best movie ever!" leads to the most concise, single-word classification: "Positive." This specific input-output pair is highlighted as the "Best prompt for the preferred output," signifying that for a binary classification task, this prompt yields the most desirable direct answer.

The significance of this illustration is to demonstrate the critical role of prompt engineering in shaping the output of generative models. By subtly altering the phrasing of the input prompt, the desired format, detail, and specificity of the model's completion can be controlled and optimized for a particular task.

**Key Insights:**
The main takeaways from this image are:

1.  **Prompt Engineering is Crucial:** The choice of prompt significantly influences the output generated by a generative model for the same underlying task. This is evident from the three distinct outputs for the same movie review, each driven by a different prompt.
2.  **Output Customization:** Generative models can produce outputs with varying levels of detail and specificity, from detailed explanations to concise labels, depending on how they are prompted. For example, 'Given a typical sentiment rating scale it would be rated as 5 due to its highly positive connotation.' vs. 'Positive'.
3.  **Optimization for Preferred Output:** There exists an 'optimal' prompt for a 'preferred output'. The image highlights 'Is this movie review negative or positive?' leading to 'Positive' as the 'Best prompt for the preferred output', indicating the process of refining prompts to achieve a specific, desired outcome (e.g., a simple binary classification).
4.  **Generative Model's Role:** The central 'Generative model' with the 'Objective: generate text' acts as the processing engine that interprets the prompt and the input text to produce a completion.

**Document Context:**
This image directly supports the document's section on "Text Classification with Generative Models" and specifically the statement: "Figure 4-18. Prompt engineering allows prompts to be updated to improve the output generated by the model." It serves as a visual example of how varying the 'Input (prompt)' for a generative model can drastically change the 'Output (completion)' for the same core task of sentiment analysis. The diagram explicitly shows that through iterative refinement of prompts, a 'Best prompt for the preferred output' can be identified, thereby improving the model's utility for specific text classification objectives.

**Summary:**
The image illustrates the concept of prompt engineering for text classification using a generative model. It demonstrates how different input prompts, when applied to the same core review text ("Best movie ever!"), lead to varying outputs or completions from a generative model. The diagram presents three distinct prompting strategies and their respective results.

The first prompt, "Rate the sentiment." followed by "Best movie ever!", leads to a detailed output: "Given a typical sentiment rating scale it would be rated as 5 due to its highly positive connotation." This output provides both a rating and a justification.

The second prompt, "What sentiment does this review have?" followed by "Best movie ever!", results in a more concise output: "The review expresses a positive sentiment." This output directly states the sentiment.

The third prompt, "Is this movie review negative or positive?" followed by "Best movie ever!", yields the most direct and simplest output: "Positive." This third input-output pair is highlighted by a surrounding dotted line, with the annotation "Best prompt for the preferred output" positioned below it. This indicates that for a task requiring a simple positive/negative classification, this prompt is the most effective in achieving the desired, concise output.

All three prompts are fed into a central component labeled "Generative model" with the objective "Objective: generate text." The process clearly shows that by refining the prompt, one can guide the generative model to produce the desired format and level of detail in its output, a core principle of prompt engineering.](images/8b816ac90a8a3cb0f362bd74b9d7722ecb9d8d633e4f22703079f6ca76264bdf.jpg)
Figure 4-18. Prompt engineering allows prompts to be updated to improve the output generated by the model.

In this section, we will demonstrate how we can leverage different types of generative models to perform classification without our Rotten Tomatoes dataset.

# Using the Text-to-Text Transfer Transformer

Throughout this book, we will explore mostly encoder-only (representation) mod‐ els like BERT and decoder-only (generative) models like ChatGPT. However, as discussed in Chapter 1, the original Transformer architecture actually consists of an encoder-decoder architecture. Like the decoder-only models, these encoder-decoder models are sequence-to-sequence models and generally fall in the category of genera‐ tive models.

An interesting family of models that leverage this architecture is the Text-to-Text Transfer Transformer or T5 model. Illustrated in Figure 4-19, its architecture is similar to the original Transformer where 12 decoders and 12 encoders are stacked together.7

![## Image Analysis: 5b87a0bb7c5b457a86bc54a66ecb21989643c31729160d4a03a7a27a0466df67.jpg

**Conceptual Understanding:**
This image conceptually represents the T5 (Text-to-Text Transfer Transformer) model's architecture. Its main purpose is to illustrate how T5 processes an input sequence of text tokens through an Encoder-Decoder structure to produce an output sequence of text tokens. It conveys the idea that text processing in T5 involves taking an input query or statement and transforming it into a desired text output, effectively treating all tasks as a text-to-text problem.

**Content Interpretation:**
The image depicts the high-level architecture of the T5 (Text-to-Text Transfer Transformer) model. It illustrates a sequence-to-sequence processing pipeline, specifically an Encoder-Decoder architecture, which is a common pattern in natural language processing models for tasks like question answering or translation. The input is a sequence of tokens representing a question, which is processed by a multi-layered Encoder. The output of the Encoder is then fed into a multi-layered Decoder, which generates an output sequence representing the answer. The specific example shows a mathematical question being posed and correctly answered, demonstrating the model's capability to understand and generate coherent text. The notation '×12' indicates that both the Encoder and Decoder consist of 12 stacked layers, highlighting the depth of the model's processing capabilities.

**Key Insights:**
The main takeaway from this image is that the T5 model operates on a text-to-text paradigm, treating all NLP tasks as text transformation problems. The image clearly demonstrates an Encoder-Decoder architecture. The input "What is 1 + 1 ?" is processed, and the output "The answer is 2" is generated, illustrating its capability for natural language understanding and generation, even for simple arithmetic. The '×12' annotations for both Encoder and Decoder layers indicate that T5 is a deep learning model with multiple stacked layers, crucial for learning complex patterns in text. This setup suggests that T5 can handle various tasks by framing them as generating a target text sequence from a source text sequence.

**Document Context:**
This image is highly relevant within the section "Using the Text-to-Text Transfer Transformer" as it visually explains the fundamental architecture of the T5 model. By showing a clear example of input and output, it helps readers understand how the T5 model takes a text sequence as input, processes it through its Encoder-Decoder components, and produces another text sequence as output. The accompanying text, "Figure 4-19. The T5 architecture is similar to the original Transformer model, a decoderencoder architecture," reinforces the visual representation by linking T5 to the established Transformer architecture and specifying its core components.

**Summary:**
The image illustrates the architecture and operation of the T5 (Text-to-Text Transfer Transformer) model, showing how it processes an input sequence to generate an output sequence. The input, a question "What is 1 + 1 ?", is broken down into individual tokens ("What", "is", "1", "+", "1", "?"). These tokens are fed into the Encoder component of the T5 model. The Encoder consists of 12 layers, as indicated by "×12". The output from the Encoder then passes to the Decoder component. The Decoder also consists of 12 layers ("×12"). The Decoder then processes this information to produce an output sequence, which in this example is the answer: "The answer is 2", similarly broken into individual tokens ("The", "answer", "is", "2"). The overall flow demonstrates a sequence-to-sequence transformation, where an input text string is transformed into an output text string through the Encoder-Decoder architecture of T5.](images/5b87a0bb7c5b457a86bc54a66ecb21989643c31729160d4a03a7a27a0466df67.jpg)
Figure 4-19. The T5 architecture is similar to the original Transformer model, a decoderencoder architecture.

With this architecture, these models were first pretrained using masked language modeling. In the first step of training, illustrated in Figure 4-20, instead of masking individual tokens, sets of tokens (or token spans) were masked during pretraining.

![## Image Analysis: 87099f7601ca611279f0ee88345374c662c7619ae4b4c61448f758b0c4c8d12f.jpg

**Conceptual Understanding:**
This image conceptually represents the pretraining objective and mechanism of the T5 (Text-to-Text Transfer Transformer) model. Its main purpose is to illustrate how the T5 model learns to understand context and generate text by predicting masked tokens within an input sequence. The core idea communicated is that during pretraining, the model is trained to complete incomplete sentences or phrases, where parts are intentionally hidden (masked), thereby developing its ability to infer and generate contextually appropriate text.

**Content Interpretation:**
The image depicts the pretraining phase of the T5 model. It showcases how the model learns to predict masked tokens within an input sequence. The process involves taking a sentence with strategically placed '[MASK]' tokens, feeding it into the T5 model, whose objective is to predict these masks, and then outputting the predicted tokens. Specifically, the input sentence 'LLMs can be used for [MASK], a form of [MASK].' is processed. The T5 model successfully predicts 'text generation' for the first mask and 'generative ai' for the second mask, demonstrating its ability to infer missing information based on context. All extracted text elements from Section 1, including the input tokens, the '1 Pretraining' label, 'T5 model', 'Objective: predict [MASK]', and the 'Output (predicted masks)' with 'text generation' and 'generative ai', collectively illustrate this core pretraining mechanism.

**Key Insights:**
The main takeaway from this image is that the T5 model undergoes a pretraining phase where its primary objective is to predict masked tokens within an input sequence. This process is fundamental for the model to learn contextual relationships and generate coherent text. The image demonstrates that a single mask can correspond to multiple output tokens (e.g., '[MASK]' predicting 'text generation' or 'generative ai'), highlighting the model's capability to predict multi-token sequences. The specific text elements '1 Pretraining', 'T5 model', 'Objective: predict [MASK]', and the precise 'Input (masked sequence)' tokens like 'LLMs', 'can', 'be', 'used', 'for', '[MASK]', ',', 'a', 'form', 'of', '[MASK]', '.' along with the 'Output (predicted masks)' 'text generation' and 'generative ai' provide clear evidence for these insights, showing the exact mechanism and the model's successful predictions.

**Document Context:**
This image directly supports the document's section on "Using the Text-to-Text Transfer Transformer" by visually explaining the initial "pretraining" step. It clarifies the fundamental mechanism by which the T5 model learns, which is by predicting masked tokens within a sequence. This specific pretraining task, where the model's objective is to fill in the blanks, is crucial for developing its contextual understanding and generative capabilities, enabling it to perform various text-to-text tasks discussed in the broader narrative. It specifically aligns with the text after the image, which states, 'In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens.'

**Summary:**
This diagram illustrates the "Pretraining" process for the "T5 model" as part of its text-to-text transfer capabilities. The process begins with an "Input (masked sequence)". This input is a sentence broken into individual tokens, with some words intentionally replaced by "[MASK]" tokens. For example, the input sequence shown is: "LLMs", "can", "be", "used", "for", "[MASK]" (in light purple), ",", "a", "form", "of", "[MASK]" (in light green), ".".

During the "Pretraining" step, identified as "1 Pretraining", the "T5 model" processes this masked sequence. The explicit "Objective" of the T5 model at this stage is to "predict [MASK]", meaning it must infer the original words or phrases that were hidden.

The outcome of this process is the "Output (predicted masks)". For the first "[MASK]" (which was purple in the input), the model predicts "text generation" (shown as two light purple rounded rectangles). For the second "[MASK]" (which was green in the input), the model predicts "generative ai" (shown as two light green rounded rectangles). This demonstrates how the T5 model learns to fill in missing information based on the surrounding context during its pretraining phase, a crucial step for its subsequent performance in various natural language processing tasks.](images/87099f7601ca611279f0ee88345374c662c7619ae4b4c61448f758b0c4c8d12f.jpg)
Figure 4-20. In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens.

The second step of training, namely fine-tuning the base model, is where the real magic happens. Instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. As illustrated in Figure 4-21, this allows the model to be trained on a wide variety of tasks.

![## Image Analysis: 5ccca55ec9c3bf6b9b918141effc47f7d6c25ee88a14780fbd5a3f36f76e3e3e.jpg

**Conceptual Understanding:**
The image conceptually represents the fine-tuning process of the T5 model for multiple text-to-text tasks. Its main purpose is to illustrate how the T5 model can handle various Natural Language Processing (NLP) challenges—like translation, grammar correction, and summarization—by treating each as a text-to-text conversion problem. The core idea is that a single model architecture, when fine-tuned with specific task instructions embedded in the input, can generate appropriate outputs for different tasks, emphasizing the model's versatility and unified approach to NLP.

**Content Interpretation:**
The image displays a conceptual model of how the T5 (Text-to-Text Transfer Transformer) model is fine-tuned to perform various natural language processing tasks. It shows that different tasks, such as translation, grammar checking, and summarization, are all framed as text-to-text problems. The T5 model, with the objective to 'predict multiple tasks', takes these textual task instructions as input and produces task-specific text outputs. This illustrates the versatility and unified approach of the T5 architecture.

**Key Insights:**
The main takeaway is that the T5 model can be effectively fine-tuned to perform a wide array of natural language processing tasks by converting each task into a text-to-text format. The 'Objective: predict multiple tasks' highlights its multi-task learning capability. Specific textual evidence demonstrates this: 'Translate into Dutch: My name is Maarten.' (input) leads to 'Mijn naam is Maarten' (output); 'Grammar: The building is tall and wide.' (input) leads to 'Acceptable' (output); and 'Summarize: Reading books has a myriad of advantages...' (input) leads to 'Reading improves mental health and broadens knowledge.' (output). This illustrates the power of a unified framework for diverse NLP problems.

**Document Context:**
This image directly supports the document's narrative on 'Using the Text-to-Text Transfer Transformer' by providing a visual explanation of how the T5 model can be trained on a variety of tasks during fine-tuning. It concretely demonstrates the concept of converting specific tasks into textual instructions for the T5 model, as described in the accompanying text (Figure 4-21). It serves as a visual aid to understand the model's architecture and its application to diverse NLP challenges.

**Summary:**
The image illustrates the fine-tuning process of the T5 model for multiple text-to-text tasks. It begins with 'Input (multiple tasks)', which are presented as textual instructions. Three distinct input tasks are shown: 'Translate into Dutch: My name is Maarten.', 'Grammar: The building is tall and wide.', and 'Summarize: Reading books has a myriad of advantages that contribute to both mental and emotional health. Engaging with written material...'. These inputs are fed into the 'T5 model' during a '2 Fine-tuning' phase. The 'T5 model' has the 'Objective: predict multiple tasks'. Following the processing by the T5 model, corresponding 'Output' examples are generated. For the translation task, the output is 'Mijn naam is Maarten'. For the grammar task, the output is 'Acceptable'. For the summarization task, the output is 'Reading improves mental health and broadens knowledge.'. The diagram effectively demonstrates how the T5 model, through fine-tuning, can handle diverse natural language processing tasks by converting them into a unified text-to-text format.](images/5ccca55ec9c3bf6b9b918141effc47f7d6c25ee88a14780fbd5a3f36f76e3e3e.jpg)
Figure 4-21. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks during fine-tuning.

This method of fine-tuning was extended in the paper “Scaling instruction-finetuned language models”, which introduced more than a thousand tasks during fine-tuning that more closely follow instructions as we know them from GPT models.8 This resulted in the Flan-T5 family of models that benefit from this large variety of tasks.

To use this pretrained Flan-T5 model for classification, we will start by loading it through the "text2text-generation" task, which is generally reserved for these encoder-decoder models:

# Load our model   
pipe $=$ pipeline( "text2text-generation", model "google/flan-t5-small", device="cuda:0"   
)

The Flan-T5 model comes in various sizes (flan-t5-small/base/large/xl/xxl) and we will use the smallest to speed things up a bit. However, feel free to play around with larger models to see if you can improve the results.

Compared to our task-specific model, we cannot just give the model some text and hope it will output the sentiment. Instead, we will have to instruct the model to do so.

Thus, we prefix each document with the prompt “Is the following sentence positive or negative?”:

# Prepare our data   
prompt $=$ "Is the following sentence positive or negative? "   
data $=$ data.map(lambda example: {"t5": prompt $^ +$ example['text']})   
data   
DatasetDict({ train: Dataset({ features: ['text', 'label', 't5'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label', 't5'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label', 't5'], num_rows: 1066 })   
})

After creating our updated data, we can run the pipeline similar to the task-specific example:

# Run inference   
y_pred $= \ [ ]$   
for output in tqdm(pipe(KeyDataset(data["test"], "t5")),   
total $\cdot ^ { = }$ len(data["test"])): text $=$ output[0]["generated_text"] y_pred.append(0 if text $= =$ "negative" else 1)

Since this model generates text, we did need to convert the textual output to numer‐ ical values. The output word “negative” was mapped to 0 whereas “positive” was mapped to 1.

These numerical values now allow us to test the quality of the model in the same way we have done before:

evaluate_performance(data["test"]["label"], y_pred)   

<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td> support</td></tr><tr><td>Negative Review</td><td>0.83</td><td>0.85</td><td>0.84</td><td>533</td></tr><tr><td>Positive Review</td><td>0.85</td><td>0.83</td><td>0.84</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.84</td><td>1066</td></tr><tr><td>macro avg</td><td>0.84</td><td>0.84</td><td>0.84</td><td>1066</td></tr><tr><td> weighted avg</td><td>0.84</td><td>0.84</td><td>0.84</td><td>1066</td></tr></table>

With an F1 score of 0.84, it is clear this Flan-T5 model is an amazing first look into the capabilities of generative models.

# ChatGPT for Classification

Although we focus throughout the book on open source models, another major com‐ ponent of the Language AI field is closed sourced models; in particular, ChatGPT.

Although the underlying architecture of the original ChatGPT model (GPT-3.5) is not shared, we can assume from its name that it is based on the decoder-only architecture that we have seen in the GPT models thus far.

Fortunately, OpenAI shared an overview of the training procedure that involved an important component, namely preference tuning. As illustrated in Figure 4-22, OpenAI first manually created the desired output to an input prompt (instruction data) and used that data to create a first variant of its model.

![## Image Analysis: 485f4111e4333807e1921a55f0d79ad850c9b924d6b78110b1cf123a94f0e5ec.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of 'instruction-tuning' a 'Generative model.' Its main purpose is to demonstrate how human-labeled data, consisting of input 'prompts' and their corresponding 'desired outputs,' is collected and then used to fine-tune a machine learning model. The core idea conveyed is that by providing explicit examples of instructions (prompts) and the correct responses (outputs), a generative model can be trained to understand and follow those instructions, thereby generating appropriate 'output given prompt.'

**Content Interpretation:**
The image depicts a foundational process in training large language models: instruction-tuning. It shows how human-generated prompt-output pairs are used to teach a generative model to produce specific, desired responses. The core concept is supervised learning, where the model learns from examples of correct behavior. The process involves creating 'instruction data collection' by pairing 'Sample prompts' with 'desired outputs' manually. This collected data then facilitates 'Instruction-tuning,' which is the mechanism to align the 'Generative model' with the 'Objective: generate output given prompt.' The entire flow represents a critical step in making generative models more controllable and useful by aligning their outputs with human expectations and instructions.

**Key Insights:**
The main takeaways from this image are: 1. Instruction-tuning relies on human-labeled data: The text 'Human labelers create desired outputs' explicitly states that human input is crucial for generating the training data. 2. Data consists of prompt-output pairs: The rounded boxes 'What is 1+1?' (prompt) and 'The answer is 2.' (output) demonstrate the structure of the instruction data. 3. Instruction data collection is a distinct phase: The dashed line encompassing the prompt and output, labeled 'Instruction data collection,' highlights this as a specific step. 4. Instruction-tuning is a process that trains a generative model: The arrow labeled 'Instruction-tuning' points from the collected data to the 'Generative model.' 5. The objective of the generative model is to produce outputs based on prompts: The text 'Objective: generate output given prompt' inside the Generative model box clearly defines its function. These points collectively explain how supervised learning with prompt-output pairs enables generative models to learn to follow instructions and produce desired responses.

**Document Context:**
This image directly supports the document's section on 'ChatGPT for Classification' by illustrating the instruction-tuning process. The text after the image, 'Figure 4-22. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning),' confirms that this diagram visually explains how manual data labeling (the 'Instruction data collection' and 'Human labelers create desired outputs' steps) is used for fine-tuning, specifically instruction-tuning, to improve the performance and alignment of models like ChatGPT for classification tasks. It clarifies the data preparation and model training steps involved in achieving classification capabilities.

**Summary:**
This diagram illustrates the process of instruction-tuning for a generative model, which involves collecting manually labeled data to teach the model to generate desired outputs given specific prompts. The process begins with 'Sample prompts,' where a human labeler takes an input prompt, such as 'What is 1+1?'. The human labeler then 'create[s] desired outputs' by providing the correct response, for instance, 'The answer is 2.'. This pair of prompt and desired output forms the 'Instruction data collection.' This collected data is then used in the 'Instruction-tuning' phase, which fine-tunes a 'Generative model.' The objective of this generative model, as stated in the diagram, is to 'generate output given prompt.' The arrow indicating '...' suggests that this process is part of a larger workflow or that the model, once tuned, can then be used to generate further outputs based on new prompts.](images/485f4111e4333807e1921a55f0d79ad850c9b924d6b78110b1cf123a94f0e5ec.jpg)
Figure 4-22. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning).

OpenAI used the resulting model to generate multiple outputs that were manually ranked from best to worst. As shown in Figure 4-23, this ranking demonstrates a preference for certain outputs (preference data) and was used to create its final model, ChatGPT.

![## Image Analysis: 18b619d2b164713fc5381e3d728ddc9e8dba1bdf0cb4277800c0b9266555a037.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of Reinforcement Learning from Human Feedback (RLHF), specifically focusing on the data collection and preference modeling stage. Its main purpose is to show how human preferences are leveraged to train a generative language model to produce higher-quality, 'preferred' outputs. The key idea being communicated is that models are first instruction-tuned to generate varied responses, and then human judgment is used to rank these responses, creating 'preference data' that guides further model refinement.

**Content Interpretation:**
The image depicts a workflow for fine-tuning a generative model using human feedback, specifically through a process known as preference-tuning. It demonstrates how a prompt leads to multiple model outputs, which are then manually ranked by human labelers. This ranking generates preference data used to iteratively improve the generative model's ability to produce more desirable or 'preferred' outputs.

**Key Insights:**
The main takeaway is that generative models, like ChatGPT, are refined not only through initial instruction-tuning but also significantly through human preference data. Key insights include: 1. **Iterative Improvement:** Models generate outputs, which are then evaluated and ranked by humans. 2. **Human-in-the-Loop:** Human labelers play a critical role in defining what constitutes a 'preferred output.' 3. **Preference Data as a Training Signal:** The explicit ranking (e.g., C > B > A) provides valuable 'preference-tuning' data, which guides the model's learning to align with human judgments. 4. **Objective of Generative Models:** The ultimate goal is to produce outputs that are preferred by users when given a specific prompt, moving beyond just grammatically correct or fluent text to semantically appropriate and desired responses. The extracted text elements such as "Generate outputs with instruction-tuned model," "Human labelers rank the output," "C > B > A," "Preference-tuning," and "Objective: generate preferred output given prompt" all provide direct evidence for these insights.

**Document Context:**
This image directly supports the document's section on "ChatGPT for Classification" by illustrating the underlying mechanism, described as manual preference ranking, used to generate the final ChatGPT model. The text after the image, "Manually ranked preference data was used to generate the final model, ChatGPT," explicitly connects this depicted process to the creation of ChatGPT. It explains a crucial step in developing large language models by showing how human feedback is integrated into the model's training loop to refine its output quality.

**Summary:**
This image illustrates a three-stage process for training a Generative model to produce preferred outputs, specifically using human preference data. The process begins with a prompt, in this case, "Explain LLMs." An existing instruction-tuned model generates multiple outputs (A, B, C) in response to this prompt. Output A is "An abbreviation for the Master of Laws." Output B is "I am not familiar with..." Output C is "Large language models are artificial...". Following this, human labelers evaluate and rank these outputs based on preference, resulting in a ranking of "C > B > A," indicating C is preferred over B, and B over A. This human-ranked preference data is then used for "Preference-tuning" the Generative model. The ultimate objective of this Generative model, as stated, is to "generate preferred output given prompt." The entire process of generating outputs and having them ranked by humans is labeled as the mechanism to "Create preference data."](images/18b619d2b164713fc5381e3d728ddc9e8dba1bdf0cb4277800c0b9266555a037.jpg)
Figure 4-23. Manually ranked preference data was used to generate the final model, ChatGPT.

A major benefit of using preference data over instruction data is the nuance it represents. By demonstrating the difference between a good and better output the generative model learns to generate text that resembles human preference. In Chap‐ ter 12, we will explore how these fine-tuning and preference-tuning methodologies work and how you can perform them yourself.

The process of using a closed sourced model is quite different from the open sourced examples we have seen thus far. Instead of loading the model, we can access the model through OpenAI’s API.

Before we go into the classification example, you will first need to create a free account on https://oreil.ly/AEXvA and create an API key here: https://oreil.ly/lrTXl. After doing so, you can use your API to communicate with OpenAI’s servers.

We can use this key to create a client:

import openai

# Create client client $=$ openai.OpenAI(api_key $\mathbf {  }$ "YOUR_KEY_HERE")

Using this client, we create the chatgpt_generation function, which allows us to generate some text based on a specific prompt, input document, and the selected model:

def chatgpt_generation(prompt, document, model $=$ "gpt-3.5-turbo-0125" """Generate an output based on a prompt and an input document."" messages=[ { "role": "system", "content": "You are a helpful assistant." }, { "role": "user", "content": prompt.replace("[DOCUMENT]", document) } ] chat_completion $=$ client.chat.completions.create( messages=messages, model $\cdot =$ model, temperature $\scriptstyle = 0$ ) return chat_completion.choices[0].message.content

Next, we will need to create a template to ask the model to perform the classification:

# Define a prompt template as a base   
prompt $=$ """Predict whether the following document is a positive or negative   
movie review:

[DOCUMENT]

If it is positive return 1 and if it is negative return 0. Do not give any other answers.

# Predict the target using GPT document $=$ "unpretentious , charming , quirky , original" chatgpt_generation(prompt, document)

This template is merely an example and can be changed however you want. For now, we kept it as simple as possible to illustrate how to use such a template.

Before you use this over a potentially large dataset, it is important to always keep track of your usage. External APIs such as OpenAI’s offering can quickly become costly if you perform many requests. At the time of writing, running our test dataset using the “gpt-3.5-turbo- $. 0 1 2 5 ^ { \mathrm { { \circ } } }$ model costs 3 cents, which is covered by the free account, but this might change in the future.

When dealing with external APIs, you might run into rate limit errors. These appear when you call the API too often as some APIs might limit the rate with which you can use it per minute or hour.

To prevent these errors, we can implement several methods for retrying the request, including something referred to as exponential backoff. It performs a short sleep each time we hit a rate limit error and then retries the unsuccessful request. Whenever it is unsuccessful again, the sleep length is increased until the request is successful or we hit a maximum number of retries.

To use it with OpenAI, there is a great guide that can help you get started.

Next, we can run this for all reviews in the test dataset to get its predictions. You can skip this if you want to save your (free) credits for other tasks.

# You can skip this if you want to save your (free) credits   
predictions $=$ [ chatgpt_generation(prompt, doc) for doc in tqdm(data["test"]["text"])   
]

Like the previous example, we need to convert the output from strings to integers to evaluate its performance:

# # Extract predictions y_pred $=$ [int(pred) for pred in predictions]

# Evaluate performance evaluate_performance(data["test"]["label"], y_pred)

<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.87</td><td>0.97</td><td>0.92</td><td>533</td></tr><tr><td>Positive Review</td><td>0.96</td><td>0.86</td><td>0.91</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.91</td><td>1066</td></tr><tr><td>macro avg</td><td>0.92</td><td>0.91</td><td>0.91</td><td>1066</td></tr><tr><td> weighted avg</td><td>0.92</td><td>0.91</td><td>0.91</td><td>1066</td></tr></table>

The F1 score of 0.91 already gives a glimpse into the performance of the model that brought generative AI to the masses. However, since we do not know what data the model was trained on, we cannot easily use these kinds of metrics for evaluating the model. For all we know, it might have actually been trained on our dataset!

In Chapter 12, we will explore how we can evaluate both open source and closed source models on more generalized tasks.

# Summary

In this chapter, we discussed many different techniques for performing a wide variety of classification tasks, from fine-tuning your entire model to no tuning at all! Classi‐ fying textual data is not as straightforward as it may seem on the surface and there is an incredible amount of creative techniques for doing so.

In this chapter, we explored text classification using both generative and representa‐ tion language models. Our goal was to assign a label or class to input text for the classification of a review’s sentiment.

We explored two types of representation models, a task-specific model and an embedding model. The task-specific model was pretrained on a large dataset specif‐ ically for sentiment analysis and showed us that pretrained models are a great technique for classifying documents. The embedding model was used to generate multipurpose embeddings that we used as the input to train a classifier.

Similarly, we explored two types of generative models, an open source encoderdecoder model (Flan-T5) and a closed source decoder-only model (GPT-3.5). We used these generative models in text classification without requiring specific (addi‐ tional) training on domain data or labeled datasets.

In the next chapter, we will continue with classification but focus instead on unsuper‐ vised classification. What can we do if we have textual data without any labels? What information can we extract? We will focus on clustering our data as well as naming the clusters with topic modeling techniques.

# Text Clustering and Topic Modeling

Although supervised techniques, such as classification, have reigned supreme over the last few years in the industry, the potential of unsupervised techniques such as text clustering cannot be understated.

Text clustering aims to group similar texts based on their semantic content, meaning, and relationships. As illustrated in Figure 5-1, the resulting clusters of semantically similar documents not only facilitate efficient categorization of large volumes of unstructured text but also allow for quick exploratory data analysis.

![## Image Analysis: ca165fafe03da19c20cafc8ae5a93bc41f1b1d6193b277ceb21caf009b1b7f18.jpg

**Conceptual Understanding:**
The image conceptually represents the process of text clustering. Its main purpose is to illustrate how a collection of unstructured textual data is analyzed and grouped into meaningful categories based on the semantic relationships between the texts. The key idea communicated is the transformation of disordered text into an organized structure where similar items are grouped together, enabling better understanding and analysis of large text datasets.

**Content Interpretation:**
The image demonstrates a data processing workflow where raw, unstructured textual data is subjected to a 'Clustering' process. This process organizes the text into distinct groups, or 'Clusters', where the texts within each group are 'semantically similar'. The visualization uses a grid to represent a conceptual space where these texts (or their underlying topics/words) are plotted, and then outlines are drawn around groups of related items to form clusters. For example, 'cat' and 'dog' are grouped together, indicating their semantic relationship, distinct from 'soccer' and 'basketball', or 'pasta' and 'pizza'. The specific example of 'Document about cats' pointing to the 'cat' item within its cluster clearly shows how an incoming document is categorized.

**Key Insights:**
The main takeaway is that text clustering is a method for organizing large volumes of unstructured textual data into coherent groups based on their semantic similarity. The image clearly shows that the output of this process is not random, but rather a collection of 'Clusters of semantically similar texts'. The specific examples of grouped words ('cat', 'dog'; 'soccer', 'basketball'; 'pasta', 'pizza') illustrate that words or documents sharing common themes are brought together. The 'Document about cats' annotation further clarifies that individual text pieces are assigned to these semantically defined clusters, demonstrating the practical application of the clustering process in categorizing documents.

**Document Context:**
This image serves as a foundational visual explanation for the concept of text clustering within a document section titled 'Text Clustering and Topic Modeling'. It visually explains the core idea presented in the text, illustrating how unstructured text is transformed into organized, semantically meaningful clusters. It directly supports the narrative by providing a clear 'before-and-after' view of the data, emphasizing the role of clustering in bringing structure to textual data.

**Summary:**
The image illustrates the process of text clustering, transforming unstructured textual data into organized clusters of semantically similar texts. On the left, a stack of three document icons represents the 'Input' which is specified as '(Textual data)'. An arrow labeled 'Clustering' points from these input documents towards a grid on the right, which represents the 'Output'. The output is further specified as '(Clusters of semantically similar texts)'. Within this grid, several words are depicted as points and are grouped into three distinct, colored clusters. A purple cluster contains the words 'cat' and 'dog'. A blue cluster contains the words 'soccer' and 'basketball'. A green cluster contains the words 'pasta' and 'pizza'. An additional annotation, 'Document about cats', with an arrow, specifically points to the 'cat' item within the purple cluster, indicating how an individual document is mapped into the output clusters.](images/ca165fafe03da19c20cafc8ae5a93bc41f1b1d6193b277ceb21caf009b1b7f18.jpg)
Figure 5-1. Clustering unstructured textual data.

The recent evolution of language models, which enable contextual and semantic representations of text, has enhanced the effectiveness of text clustering. Language is more than a bag of words, and recent language models have proved to be quite capable of capturing that notion. Text clustering, unbound by supervision, allows for creative solutions and diverse applications, such as finding outliers, speedup labeling, and finding incorrectly labeled data.

Text clustering has also found itself in the realm of topic modeling, where we want to discover (abstract) topics that appear in large collections of textual data. As shown in Figure 5-2, we generally describe a topic using keywords or keyphrases and, ideally, have a single overarching label.

![## Image Analysis: d4f3598c5badde0a43af58c22cc4afac68733048928eafc5ada838654179a1ee.jpg

**Conceptual Understanding:**
This image conceptually represents the outcome of a topic modeling process. It illustrates how individual words or documents are clustered into distinct, semantically meaningful groups, which are then assigned descriptive 'topics'. The main purpose of the image is to visually explain how topic modeling works by demonstrating the grouping of related terms (keywords) into coherent themes, thereby providing an interpretable structure to unstructured text data. It communicates the key idea that words that frequently appear together or are semantically related are assigned to the same topic, and these topics can then be labeled and understood.

**Content Interpretation:**
The image displays three distinct clusters of words, each representing a 'topic'. These clusters are visually separated on a grid, suggesting a conceptual space where related terms are located close to each other. The three topics are: 1. **Topic 1 - Pets**: This cluster includes the words 'cat' and 'dog' within a purple outlined shape on the grid. The associated keywords provided are 'pet, dog, cat, animal shelter, ...'. 2. **Topic 2 - Food**: This cluster contains the words 'pasta' and 'pizza' within a green outlined shape on the grid. The associated keywords are 'pasta, pizza, rice, ...'. 3. **Topic 3 - Sports**: This cluster shows the words 'soccer' and 'basketball' within a blue outlined shape on the grid. The associated keywords are 'sport, soccer, game, ...'. Each topic description (Topic 1 - Pets, Topic 2 - Food, Topic 3 - Sports) has a curved arrow pointing to its corresponding colored cluster on the grid. The grid itself provides a two-dimensional context for the clustering, emphasizing the spatial grouping of related terms. The core system being shown is the outcome of a topic modeling process, which organizes individual terms into coherent, labeled themes.

**Key Insights:**
The main takeaways from this image are: 1. **Semantic Grouping**: Topic modeling effectively groups words that are semantically similar. For example, 'cat' and 'dog' are grouped under 'Pets', 'pasta' and 'pizza' under 'Food', and 'soccer' and 'basketball' under 'Sports'. 2. **Topic Assignment**: The process assigns a meaningful label (e.g., 'Pets', 'Food', 'Sports') to these clusters of related words. 3. **Keyword Association**: Each topic is characterized by a set of keywords that represent its core theme, demonstrating how the model identifies and links terms to specific topics. For instance, 'Topic 1 - Pets' is explicitly linked to 'pet, dog, cat, animal shelter, ...', providing a clear understanding of its content. 4. **Visual Representation of Closeness**: The grid layout and the clustering shapes visually convey that words within a cluster are more closely related to each other than to words in other clusters, reinforcing the concept of semantic proximity. The distinct colors and boundaries of the clusters for 'Pets' (purple), 'Food' (green), and 'Sports' (blue) clearly illustrate this separation and cohesion.

**Document Context:**
This image directly illustrates the concept of topic modeling, as stated in the document context: 'Figure 5-2. Topic modeling is a way to give meaning to clusters of textual documents.' It serves as a visual example of how this process works, showing how disparate keywords are grouped into semantically meaningful topics. The diagram supports the document's narrative on 'Text Clustering and Topic Modeling' by providing a concrete visual representation of topic assignment and keyword association within identified topics, enhancing the reader's understanding of the textual explanation.

**Summary:**
The image visually represents the concept of topic modeling, where individual keywords or terms are grouped into distinct topics based on their semantic relationships. It uses a grid as a conceptual space to show the proximity of related words. Three main topics are identified: 'Pets', 'Food', and 'Sports', each with a list of associated keywords and a visual cluster of two representative words on the grid. The diagram illustrates how topic modeling assigns a meaningful label to a collection of related terms, thereby giving meaning to clusters of textual documents. The clear visual separation of the clusters on the grid, each labeled with a distinct topic and accompanied by relevant keywords, makes the underlying concept easy to grasp.](images/d4f3598c5badde0a43af58c22cc4afac68733048928eafc5ada838654179a1ee.jpg)
Figure 5-2. Topic modeling is a way to give meaning to clusters of textual documents.

In this chapter, we will first explore how to perform clustering with embedding models and then transition to a text-clustering-inspired method of topic modeling, namely BERTopic.

Text clustering and topic modeling have an important role in this book as they explore creative ways to combine a variety of different language models. We will explore how combining encoder-only (embeddings), decoder-only (generative), and even classical methods (bag-of-words) can result in amazing new techniques and pipelines.

# ArXiv’s Articles: Computation and Language

Throughout this chapter, we will be running clustering and topic modeling algo‐ rithms on ArXiv articles. ArXiv is an open-access platform for scholarly articles, mostly in the fields of computer science, mathematics, and physics. We will explore articles in the field of Computation and Language to keep with the theme of this book. The dataset contains 44,949 abstracts between 1991 and 2024 from ArXiv’s cs.CL (Computation and Language) section.

We load the data and create separate variables for the abstracts, titles, and years of each article:

# # Load data from Hugging Face from datasets import load_dataset dataset $=$ load_dataset("maartengr/arxiv_nlp")["train"]

# Extract metadata abstracts $=$ dataset["Abstracts"] titles $=$ dataset["Titles"]

# A Common Pipeline for Text Clustering

Text clustering allows for discovering patterns in data that you may or may not be familiar with. It allows for getting an intuitive understanding of the task, for example, a classification task, but also of its complexity. As a result, text clustering can become more than just a quick method for exploratory data analysis.

Although there are many methods for text clustering, from graph-based neural net‐ works to centroid-based clustering techniques, a common pipeline that has gained popularity involves three steps and algorithms:

1. Convert the input documents to embeddings with an embedding model.   
2. Reduce the dimensionality of embeddings with a dimensionality reduction model.   
3. Find groups of semantically similar documents with a cluster model.

# Embedding Documents

The first step is to convert our textual data to embeddings, as illustrated in Figure 5-3. Recall from previous chapters that embeddings are numerical representations of text that attempt to capture its meaning.

![## Image Analysis: 60d2282659142a3ce17385f05d00f232ac9ac5fecd64977e96110de0e165d385.jpg

**Conceptual Understanding:**
The image conceptually represents the initial stage of transforming textual information into a machine-readable, numerical format. Its main purpose is to illustrate how a collection of 'n documents' is converted into 'embeddings', which are numerical vectors. This process is labeled as '1. Embed documents'. The outcome is a matrix where each row corresponds to a document, and the columns represent the 'Dimensions' of the embedding (e.g., 512 values), effectively creating a numerical fingerprint for each document. The overall message conveyed is that complex textual data can be simplified and structured into a quantifiable format for further computational processing.

**Content Interpretation:**
The image depicts the fundamental process of converting textual documents into a numerical format known as 'embeddings'. This is a critical step in many natural language processing (NLP) tasks, as it allows machines to understand and process human language by representing words, sentences, or entire documents as dense vectors in a continuous vector space. The process shows that multiple documents ('n documents') are fed into an embedding mechanism ('1. Embed documents') and transformed into a matrix. This matrix organizes the information such that each document becomes a row vector, and the 'Dimensions' (e.g., 512 values) refer to the features or components of that vector, capturing the semantic meaning of the document. The 'Number of documents' represents the total quantity of documents processed, which directly translates to the number of rows in the resulting embedding matrix. This transformation enables documents to be compared, clustered, and searched based on their semantic similarity.

**Key Insights:**
The main takeaway from this image is that documents can be systematically converted into a numerical matrix representation, known as embeddings. This conversion assigns a fixed number of 'Dimensions' (e.g., 512 values) to each document, regardless of its original length, effectively standardizing the data for computational analysis. The process ensures that each of the 'n documents' is represented as a distinct numerical vector, forming a matrix where the number of rows corresponds to the 'Number of documents' and the number of columns corresponds to the 'Dimensions'. This transformation is crucial for enabling machines to understand and work with textual data, facilitating tasks like similarity comparison, clustering, and information retrieval.

**Document Context:**
This image is presented within a document section titled 'Embedding Documents' and is explicitly identified as 'Figure 5-3. Step 1: We convert documents to embeddings using an embedding model.' It serves as the foundational visual explanation for the very first step in a larger process, likely involving document processing or retrieval systems. The diagram directly illustrates the abstract concept of 'embedding' by showing a concrete transformation from textual documents to a quantifiable numerical structure, making the subsequent steps or explanations in the document more understandable.

**Summary:**
The image illustrates the initial step of embedding documents, which is converting a collection of 'n documents' into a numerical matrix representation. This process involves taking multiple individual documents, represented by document icons, and transforming them into a structured data format. The conversion is labeled as '1. Embed documents'. The output is a matrix where each row corresponds to one of the 'Number of documents', and each column represents a 'Dimensions' feature (e.g., 512 values). This means that each document is represented as a vector of numerical values in a fixed-size dimension, enabling computational processing and analysis.](images/60d2282659142a3ce17385f05d00f232ac9ac5fecd64977e96110de0e165d385.jpg)
Figure 5-3. Step 1: We convert documents to embeddings using an embedding model.

Choosing embedding models optimized for semantic similarity tasks is especially important for clustering as we attempt to find groups of semantically similar documents. Fortunately, most embedding models at the time of writing focus on just that, semantic similarity.

As we did in the previous chapter, we will use the MTEB leaderboard to select an embedding model. We will need an embedding model that has a decent score on clustering tasks but also is small enough to run quickly. Instead of using the “sentence-transformers/all-mpnet-base-v2” model we used in the previous chapter, we use the “thenlper/gte-small” model instead. It is a more recent model that outper‐ forms the previous model on clustering tasks and due to its small size is even faster for inference. However, feel free to play around with newer models that have been released since!

from sentence_transformers import SentenceTransformer # Create an embedding for each abstract embedding_model $=$ SentenceTransformer("thenlper/gte-small") embeddings $=$ embedding_model.encode(abstracts, show_progress_bar=True)

Let’s check how many values each document embedding contains:

# Check the dimensions of the resulting embeddings embeddings.shape

(44949, 384)

Each embedding has 384 values that together represent the semantic representation of the document. You can view these embeddings as the features that we want to cluster.

# Reducing the Dimensionality of Embeddings

Before we cluster the embeddings, we will first need to take their high dimensionality into account. As the number of dimensions increases, there is an exponential growth in the number of possible values within each dimension. Finding all subspaces within each dimension becomes increasingly complex.

As a result, high-dimensional data can be troublesome for many clustering tech‐ niques as it gets more difficult to identify meaningful clusters. Instead, we can make use of dimensionality reduction. As illustrated in Figure 5-4, this technique allows us to reduce the size of the dimensional space and represent the same data with fewer dimensions. Dimensionality reduction techniques aim to preserve the global structure of high-dimensional data by finding low-dimensional representations.

![## Image Analysis: 180fbe51fd25a709a2f317ea5f670e0aedf185bfabf5d90cb7ed84fe997d958c.jpg

**Conceptual Understanding:**
This image conceptually represents the process of dimensionality reduction. Its main purpose is to visually explain how data existing in a multi-dimensional space can be transformed and compressed into a space with fewer dimensions. The key idea communicated is that this reduction simplifies the data representation without losing critical information, making it more manageable and interpretable, as demonstrated by the distinct distributions of purple and green data points being preserved in the lower-dimensional space.

**Content Interpretation:**
The image illustrates the transformation of data from a higher-dimensional space to a lower-dimensional space. Specifically, it shows data points initially distributed in a 3-dimensional Cartesian coordinate system (labeled 'x, y, and z') being transformed into a 2-dimensional coordinate system (labeled 'a and b'). The process highlighted is 'Dimensionality reduction'. The significance is that complex data, which might be difficult to visualize or process in many dimensions, can be simplified into fewer dimensions while retaining its essential characteristics, such as the distinct clustering or distribution of the two types of data points (purple and green). The visual evidence for this interpretation comes directly from the titles '3-dimensional space (x, y, and z)' and '2-dimensional space (a and b)', and the explicit arrow label 'Dimensionality reduction' connecting the two representations.

**Key Insights:**
The main takeaway from this image is the fundamental concept of dimensionality reduction: it is a process that compresses data from a higher-dimensional representation to a lower-dimensional one. This process is essential for simplifying complex datasets, making them easier to visualize, analyze, and process, especially in fields like machine learning and data science. The image highlights that even after reduction, the intrinsic patterns or relationships within the data (represented by the two distinct colors of points) are maintained. This is evidenced by the explicit labels '3-dimensional space (x, y, and z)' transitioning to '2-dimensional space (a and b)' via 'Dimensionality reduction', showing a direct visual mapping of points from one space to the other.

**Document Context:**
This image directly supports the document's section 'Reducing the Dimensionality of Embeddings' by providing a clear visual explanation of what dimensionality reduction entails. It concretely demonstrates the concept of compressing data from a high-dimensional state to a lower-dimensional state, which is crucial for understanding how complex data, such as embeddings, can be simplified for analysis or visualization. The image visually confirms the statement after it: 'Figure 5-4. Dimensionality reduction allows data in high-dimensional space to be compressed to a lower-dimensional representation.'

**Summary:**
This image is a conceptual diagram illustrating the process of dimensionality reduction. On the left, data points are depicted within a '3-dimensional space' defined by 'x, y, and z' axes, represented as a grid-like cube. There are numerous data points, colored either purple or green, scattered throughout this 3D grid. An arrow, prominently labeled 'Dimensionality reduction', points from this 3D representation to a '2-dimensional space' on the right. The 2D space is represented by a flat grid with 'a' and 'b' axes. In this 2D space, the same data points (purple and green) are now plotted, but their positions are compressed into a two-dimensional plane. The visual effectively shows how a complex, multi-faceted data representation can be simplified into fewer dimensions while attempting to preserve the relative distribution or relationships among the data points. The transition signifies a compression of data from higher to lower dimensions for easier analysis and visualization.](images/180fbe51fd25a709a2f317ea5f670e0aedf185bfabf5d90cb7ed84fe997d958c.jpg)
Figure 5-4. Dimensionality reduction allows data in high-dimensional space to be com‐ pressed to a lower-dimensional representation.

Note that this is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. To help the cluster model create meaningful clus‐ ters, the second step in our clustering pipeline is therefore dimensionality reduction, as shown in Figure 5-5.

![## Image Analysis: 464dda49be096fa6b350f9d83be9763c57f05da5494ef4789a23a18c27fe894c.jpg

**Conceptual Understanding:**
This image conceptually represents the process of dimensionality reduction. Its main purpose is to visually explain how a high-dimensional data representation, such as an embedding with many features or values, can be transformed into a lower-dimensional, more compact form. It communicates the idea of simplifying data while retaining essential information.

**Content Interpretation:**
The image shows the transformation of data from a higher-dimensional space to a lower-dimensional, compressed space. It specifically demonstrates the concept of 'dimensionality reduction' as applied to 'embeddings'. The grid-like structures visually represent the embeddings, and the change in their horizontal extent illustrates the reduction in the number of dimensions. The exact text "Dimensions (e.g., 512 values)" and "Compressed dimensions (e.g., 3 values)" provide concrete examples of the reduction from a large number of values to a much smaller number.

**Key Insights:**
The main takeaway is that dimensionality reduction is a process that transforms data from a higher number of features or dimensions into a lower number of dimensions. This process is depicted as '2. Reduce dimensionality'. The image effectively conveys that a large number of values (e.g., 512) can be compressed into a significantly smaller number (e.g., 3), which is crucial for managing data complexity and improving computational efficiency. This visual evidence, coupled with the explicit numerical examples, demonstrates the core principle and outcome of dimensionality reduction.

**Document Context:**
This image directly supports the document section "Reducing the Dimensionality of Embeddings" and clarifies "Figure 5-5. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction." It provides a visual explanation of the second step in a multi-step process, focusing on how embeddings are transformed to be more compact and potentially more efficient by reducing their dimensional representation.

**Summary:**
The image illustrates the process of dimensionality reduction for embeddings. On the left, a set of embeddings is depicted as a grid of orange squares, representing high-dimensional data labeled as "Dimensions (e.g., 512 values)". A horizontal arrow points from this initial state to a transformed state on the right. The arrow itself is labeled "2. Reduce dimensionality", indicating the action performed. On the right, the embeddings are again shown as a grid of orange squares, but with a significantly smaller width, representing lower-dimensional data. This reduced state is labeled "Compressed dimensions (e.g., 3 values)". The visual representation, along with the numerical examples (512 values reduced to 3 values), clearly demonstrates the concept of reducing the complexity or number of features in a dataset.](images/464dda49be096fa6b350f9d83be9763c57f05da5494ef4789a23a18c27fe894c.jpg)
Figure 5-5. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction.

Well-known methods for dimensionality reduction are Principal Component Analy‐ sis (PCA)1 and Uniform Manifold Approximation and Projection (UMAP).2 For this pipeline, we are going with UMAP as it tends to handle nonlinear relationships and structures a bit better than PCA.

![## Image Analysis: e56cbb42f47136c33f3355f7aa55ebaeb8e2002de1d787a499936f144d71517e.jpg

**Conceptual Understanding:**
This image represents a stylized silhouette of a bird, specifically resembling a crow or a raven, contained within a simple rectangular border. The main purpose of this image appears to be a logo or an emblem, designed for identification or branding. It doesn't convey an explicit message or complex information directly, but rather serves as a symbolic marker. The key idea communicated is the identity of an entity (e.g., a company, organization, or publication) associated with this specific visual symbol.

**Content Interpretation:**
The image shows a graphic representation of an animal, a bird of the corvid family (crow/raven), standing in profile, facing left. Its form is solid blue against a white background, enclosed by a thin blue rectangular border. There is no data, trends, or specific factual information presented within the image itself. Its significance is purely symbolic. Since no textual elements were extracted from the image, the interpretation is based solely on the visual characteristics of the silhouette and its presentation as a contained emblem.

**Key Insights:**
The main takeaway is that this image functions as a visual identifier or a brand mark. It signifies the presence or association of a particular entity. The image supports the conclusion that it is a logo. Logos are typically used for immediate recognition and to represent a brand's values or identity through a visual metaphor, though the specific meaning of the crow/raven would depend on the entity it represents. Given the absence of text within the image, all insights are derived from its visual form as a logo, not from textual evidence.

**Document Context:**
The document context for this image is "Reducing the Dimensionality of Embeddings." A logo depicting a crow or raven generally has no direct, immediately apparent relevance to the highly technical topic of dimensionality reduction in machine learning or data science. It is highly probable that this image serves as a corporate logo (e.g., publisher, author, software company) or a visual placeholder within the document rather than a direct illustration of the technical content. Without additional context from the surrounding document, its specific relevance to the section on "Reducing the Dimensionality of Embeddings" cannot be determined.

**Summary:**
The image displays a simple, clean logo featuring the dark blue silhouette of a crow or raven. The bird is depicted in profile, facing left, with its head slightly raised and its feet firmly planted. The entire silhouette is filled with a solid, dark blue color. This bird figure is centered within a thin, light blue rectangular border, which defines the boundaries of the logo. The background inside the border and behind the bird is plain white. The logo contains no text, labels, numbers, or any other textual annotations. It functions as a minimalist visual identifier or emblem, consistent with a company or publication mark. Its meaning is primarily symbolic, conveying an association with the entity it represents.](images/e56cbb42f47136c33f3355f7aa55ebaeb8e2002de1d787a499936f144d71517e.jpg)

Dimensionality reduction techniques, however, are not flawless. They do not perfectly capture high-dimensional data in a lowerdimensional representation. Information will always be lost with this procedure. There is a balance between reducing dimensionality and keeping as much information as possible.

To perform dimensionality reduction, we need to instantiate our UMAP class and pass the generated embeddings to it:

from umap import UMAP   
# We reduce the input embeddings from 384 dimensions to 5 dimensions   
umap_model $=$ UMAP( n_components $\scriptstyle \mathbf { \alpha = { \begin{array} { l } { \mathbf { \alpha } } \\ { \mathbf { \beta = { \frac { } { } } } } \end{array} } }$ , min_dist $= 0 . 6$ , metric $= ^ { \prime }$ 'cosine', random_state $= 4 2$   
)   
reduced_embeddings $=$ umap_model.fit_transform(embeddings)

We can use the n_components parameter to decide the shape of the lowerdimensional space, namely 5 dimensions. Generally, values between 5 and 10 work well to capture high-dimensional global structures.

The min_dist parameter is the minimum distance between embedded points. We are setting this to 0 as that generally results in tighter clusters. We set metric to 'cosine' as Euclidean-based methods have issues dealing with high-dimensional data.

Note that setting a random_state in UMAP will make the results reproducible across sessions but will disable parallelism and therefore slow down training.

# Cluster the Reduced Embeddings

The third step is to cluster the reduced embeddings, as illustrated in Figure 5-6.

![## Image Analysis: c111dcf13f7d4ce6d0246f52d2b28d44b44e6268f81fd78c79c26381727202a2.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of clustering data points after their dimensionality has been reduced. The main purpose is to show how data embeddings, once compressed into fewer dimensions (e.g., 3 values), are then grouped into distinct clusters. It conveys the idea that even in a reduced dimensional space, inherent groupings or patterns within the data can be identified and visualized. The image serves as a visual explanation of a specific step in a data analysis or machine learning workflow, focusing on the transformation of reduced-dimension data into a clustered arrangement.

**Content Interpretation:**
The image conceptually illustrates the process of applying clustering algorithms to data that has previously undergone dimensionality reduction. It shows a transformation from a state of compressed data dimensions to a clustered representation on a grid. The left side, labeled 'Compressed dimensions (e.g., 3 values)', represents the input data after a reduction in the number of features or dimensions, suggesting a simplification of the data space. The arrow, explicitly labeled '3. Cluster reduced embeddings', indicates the action of applying a clustering technique to these reduced data points. The right side of the image, a grid with scattered points grouped into distinct colored regions (purple, blue, green), visually depicts the result of this clustering process. Each colored region represents a cluster, where data points (white circles) within a region are considered more similar to each other than to points in other regions. The presence of points outside these clusters suggests outliers or data points that did not fit well into any defined cluster. Overall, the image shows how reduced-dimensionality embeddings are grouped into clusters to identify underlying patterns or structures in the data.

**Key Insights:**
The main takeaway from this image is that clustering is a subsequent step after dimensionality reduction in a data processing pipeline. The image demonstrates that even with 'Compressed dimensions (e.g., 3 values)', meaningful clusters can be identified, suggesting that dimensionality reduction can preserve essential information for downstream tasks like clustering. The visual representation of the clusters (purple, blue, green) and unclustered points highlights that clustering effectively groups similar data points while also identifying potential outliers. The text '3. Cluster reduced embeddings' explicitly states the operation, emphasizing that the input to this step is the output of a prior dimensionality reduction process, which is critical for understanding the overall workflow.

**Document Context:**
This image directly corresponds to and visually explains 'Step 3: We cluster the documents using the embeddings with reduced dimensionality' as mentioned in the document's accompanying text. It visually clarifies the process of taking previously compressed or reduced embeddings and applying a clustering algorithm to them. The explicit numbering '3.' on the arrow label further reinforces its position as the third step in a larger process described in the document. The image provides a concrete visual representation for the abstract concept of clustering reduced embeddings, helping readers understand the transformation and the resulting data structure after this specific step.

**Summary:**
The image illustrates the third step in a data processing pipeline: clustering embeddings that have undergone dimensionality reduction. On the left, a stacked representation of grid-like blocks indicates data with 'Compressed dimensions (e.g., 3 values)', signifying that the original data's dimensionality has been reduced, potentially down to three values. An arrow points from this compressed data representation to a grid on the right, with the label '3. Cluster reduced embeddings'. This arrow explicitly denotes the action performed. The grid on the right depicts the outcome of this clustering. It shows multiple white circles, representing the individual data points or embeddings, distributed across the grid. These points are grouped into three distinct, colored clusters: one purple cluster containing four points, one blue cluster containing four points, and one green cluster containing five points. There are also two outlier points not associated with any cluster: one at the top-middle and one at the bottom-right of the grid. The clustering visually segments the data points into coherent groups based on their reduced dimensional features.](images/c111dcf13f7d4ce6d0246f52d2b28d44b44e6268f81fd78c79c26381727202a2.jpg)
Figure 5-6. Step 3: We cluster the documents using the embeddings with reduced dimensionality.

Although a common choice is a centroid-based algorithm like k-means, which requires a set of clusters to be generated, we do not know the number of clusters beforehand. Instead, a density-based algorithm freely calculates the number of clus‐ ters and does not force all data points to be part of a cluster, as illustrated in Figure 5-7.

![## Image Analysis: 1c16aa3c8bb2447624a11ca732bb1bfa44a162e4d0df87fe97762a5d6f79123e.jpg

**Conceptual Understanding:**
This image conceptually represents and compares two distinct data clustering methodologies: 'Centroid-based' and 'Density-based' clustering. The main purpose is to illustrate how these different approaches define and form clusters from a set of data points, and crucially, how they treat 'outliers'. The image conveys the idea that the choice of clustering algorithm significantly impacts the resulting cluster structure and the identification of anomalous data points.

**Content Interpretation:**
The image illustrates two fundamental data clustering algorithms: 'Centroid-based' and 'Density-based'. The Centroid-based approach groups data points by partitioning the space into regions, with all points within a region belonging to that cluster, even if they are far from the core of the cluster. The Density-based approach identifies clusters as areas of high data point density, with points that do not fall into these dense regions being explicitly classified as 'Outliers' and 'Not assigned to any cluster'. This visualization explicitly shows how each method handles data point assignment and the treatment of points that are distant from main groups.

**Key Insights:**
The main takeaway from this image is the fundamental difference in how 'Centroid-based' and 'Density-based' clustering algorithms define clusters and handle 'Outliers'. The 'Centroid-based' method partitions the entire data space, meaning every data point, regardless of its distance from a cluster's center, will be assigned to a cluster if it falls within that partition. This is evidenced by the single point in the light blue centroid-based region that is spatially isolated but still part of the cluster. In contrast, the 'Density-based' method, as clearly labeled with 'Outlier (Not assigned to any cluster)', identifies data points that do not belong to sufficiently dense regions as outliers, explicitly not assigning them to any cluster. This illustrates that density-based methods are generally more effective at identifying and isolating true outliers, providing a more nuanced view of data structure by not forcing all points into a predefined cluster. The 'vs.' annotation emphasizes this direct comparison.

**Document Context:**
This image directly supports the document's section on 'Cluster the Reduced Embeddings' and the accompanying text: 'Figure 5-7. The clustering algorithm not only impacts how clusters are generated but also how they are viewed.' It provides a clear visual comparison of how two different clustering algorithms—centroid-based and density-based—generate and present clusters, particularly in their handling of outliers. This comparison is crucial for understanding how the choice of algorithm influences the interpretation of the clustered data, especially when dealing with anomalies or sparse data points. The image visually demonstrates the practical implications of selecting one clustering method over another, making the abstract concept of clustering more concrete for the reader.

**Summary:**
The image illustrates two distinct approaches to data clustering: Centroid-based and Density-based. On the left, the 'Centroid-based' method is depicted, where the data points are divided into distinct regions, represented by different colored background squares (purple, light blue, and green). Each region contains a group of white circles, which are data points. One data point is shown in the light blue region that is far from the main cluster of data points in that region, but it is still assigned to the light blue cluster. The 'vs.' label indicates a comparison between the two methods. On the right, the 'Density-based' method is shown. Here, clusters are formed around areas of high data point density, indicated by irregular colored outlines (purple, light blue, and green) enclosing groups of white circles. In this method, there are two distinct data points labeled as 'Outlier' and further clarified by the text '(Not assigned to any cluster)'. One 'Outlier' at the top is explicitly shown by an arrow pointing to a single data point that is outside any of the colored density-based clusters. Another 'Outlier' at the bottom is shown by an upward arrow pointing to a single data point also outside any defined cluster. This comparison highlights how density-based clustering can explicitly identify and isolate data points that do not belong to any dense cluster, whereas centroid-based clustering forces all data points into a cluster partition.](images/1c16aa3c8bb2447624a11ca732bb1bfa44a162e4d0df87fe97762a5d6f79123e.jpg)
Figure 5-7. The clustering algorithm not only impacts how clusters are generated but also how they are viewed.

A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN).3 HDBSCAN is a hierarchical variation of a clustering algorithm called DBSCAN that allows for dense (micro)-clusters to be found without having to explicitly specify the number of clusters.4 As a density-based method, HDBSCAN can also detect outliers in the data, which are data points that do not belong to any cluster. These outliers will not be assigned or forced to belong to any cluster. In other words, they are ignored. Since ArXiv articles might contain some niche papers, using a model that detects outliers could be helpful.

As with the previous packages, using HDBSCAN is straightforward. We only need to instantiate the model and pass our reduced embeddings to it:

# We fit the model and extract the clusters   
hdbscan_model $=$ HDBSCAN( min_cluster_size $\mathtt { \Gamma } = 5 \Theta$ , metric $=$ "euclidean", cluster_selection_method="eom"   
).fit(reduced_embeddings)   
clusters $=$ hdbscan_model.labels_

# How many clusters did we generate? len(set(clusters))

# 156

With HDBSCAN, we generated 156 clusters in our dataset. To create more clusters, we will need to reduce the value of min_cluster_size as it represents the minimum size that a cluster can take.

# Inspecting the Clusters

Now that we have generated our clusters, we can inspect each cluster manually and explore the assigned documents to get an understanding of its content. For example, let us take a few random documents from cluster 0:

import numpy as np   
# Print first three documents in cluster 0   
cluster $\mathit { \Theta } = \mathit { \Theta } \Theta$   
for index in np.where(clusters $= =$ cluster)[0][:3]: print(abstracts[index][:300] $^ +$ 1 \n")

This works aims to design a statistical machine translation from English text to American Sign Language (ASL). The system is based on Moses tool with some modifications and the results are synthesized through a 3D avatar for interpretation. First, we translate the input text to gloss, a written fo...

Researches on signed languages still strongly dissociate lin- guistic issues related on phonological and phonetic aspects, and gesture studies for recognition and synthesis purposes. This paper focuses on the imbrication of motion and meaning for the analysis, synthesis and evaluation of sign lang...

Modern computational linguistic software cannot produce important aspects of sign language translation. Using some researches we deduce that the majority of automatic sign language translation systems ignore many aspects when they generate animation; therefore the interpretation lost the truth inf...

From these documents, it seems that this cluster contains documents mostly about translation from and to sign language, interesting!

We can take this one step further and attempt to visualize our results instead of going through all documents manually. To do so, we will need to reduce our document embeddings to two dimensions, as that allows us to plot the documents on an $\mathrm { x / y }$ plane:

# Reduce 384-dimensional embeddings to two dimensions for easier visualization   
reduced_embeddings $=$ UMAP( n_components $\mathbf { \Psi } = \mathbf { \Psi }$ , min_dist $= 0 . 6$ , metric $=$ "cosine", random_state=42   
).fit_transform(embeddings)   
# Create dataframe   
df $=$ pd.DataFrame(reduced_embeddings, columns $=$ ["x", "y"])   
df["title"] $=$ titles   
df["cluster"] $=$ [str(c) for c in clusters]

# Select outliers and non-outliers (clusters) to_plot $=$ df.loc[df.cluster $\ : : = \ : \ " - 1 \ " \ : $ , :] outliers $=$ df.loc[df.cluster $\ = \begin{array} { r l } { \frac { \ d } { \ d t } = } & { { } \frac { \ d H } { \ d t } - 1 \frac { \ d H } { \ d t } } \end{array}$ , :]

We also created a dataframe for our clusters (clusters_df) and for the outliers (out liers_df) separately since we generally want to focus on the clusters and highlight those.

![## Image Analysis: 89ff4ab06921a661fd470a44d106a2bd0e7c36b5cd844051e5468b741e86eeb6.jpg

**Conceptual Understanding:**
The image conceptually represents a stylized bird, most likely a crow or raven, serving as a logo or symbolic graphic. Its main purpose is to act as a visual identifier or emblem. The key idea communicated is a visual brand mark, often associated with concepts like intelligence, mystery, or knowledge, depending on the context of the entity it represents.

**Content Interpretation:**
The image is a logo or symbolic graphic featuring a stylized silhouette of a crow or raven. It represents a visual identifier. No processes, concepts, relationships, or systems are being shown beyond the direct visual representation of the bird itself. The significance lies solely in its capacity as a recognizable emblem.

**Key Insights:**
The main takeaway from this image is that it is a visual brand identifier. It conveys no data, trends, or specific technical information. As a logo, its purpose is recognition and association with an entity, rather than providing insights or conclusions about the document's content. There is no textual evidence within the image to support any further insights.

**Document Context:**
Given the document context provided, which describes selecting outliers and non-outliers in a dataframe using Python code, this image appears to be a branding element or a decorative visual. It does not contain any direct information related to the technical content of outlier detection, data analysis, or the mathematical expressions mentioned in the surrounding text. Its relevance is likely as a logo for the document's publisher, author, or a related entity, rather than contributing to the specific technical explanation of the section.

**Summary:**
The image displays a clear, solid blue silhouette of a bird, specifically a crow or raven, facing left. The bird is standing with its head held high, and its beak, eye, and legs are distinctly visible as part of the silhouette. Its feathers are not individually detailed but form a smooth, recognizable shape. The entire bird silhouette is contained within a white square frame, with the bird's form filling a significant portion of the frame without touching the edges. There is no text, numbers, or any other symbols present within or around the bird or the frame.](images/89ff4ab06921a661fd470a44d106a2bd0e7c36b5cd844051e5468b741e86eeb6.jpg)

Using any dimensionality reduction technique for visualization purposes creates information loss. It is merely an approximation of what our original embeddings look like. Although it is informa‐ tive, it might push clusters together and drive them further apart than they actually are. Human evaluation, inspecting the clusters ourselves, is therefore a key component of cluster analysis!

To generate a static plot, we will use the well-known plotting library, matplotlib:

import matplotlib.pyplot as plt

# Plot outliers and non-outliers separately   
plt.scatter(outliers_df.x, outliers_df.y, alpha $\scriptstyle 1 = \atop -$ .05, $s = 2$ , $\mathbf { c } { = }$ "grey")   
plt.scatter( clusters_df.x, clusters_df.y, ${ \mathsf { C } } { \mathsf { = } }$ clusters_df.cluster.astype(int), alpha $1 { = } 0 \cdot 6$ , $s = 2$ , cmap $\mid =$ "tab20b"   
)   
plt.axis("off")

As we can see in Figure 5-8, it tends to capture major clusters quite well. Note how clusters of points are colored in the same color, indicating that HDBSCAN put them in a group together. Since we have a large number of clusters, the plotting library cycles the colors between clusters, so don’t think that all green points are one cluster, for example.

![## Image Analysis: 10fd2649719a93e52c0855efea98bdac67a5445d66c6795cc31e102d9f2db734.jpg

**Conceptual Understanding:**
This image represents a 2D data visualization resulting from a data analysis technique, likely clustering and/or anomaly detection. Its main purpose is to visually distinguish between data points that form natural groupings (clusters) and those that deviate significantly from these groups (outliers). The image communicates the idea that complex datasets can be simplified and understood by identifying inherent structures (clusters) and anomalies (outliers) through visual representation.

**Content Interpretation:**
The image vividly displays the output of a clustering and outlier detection process. The different colored regions clearly represent distinct 'generated clusters,' indicating groups of data points that share common characteristics based on the underlying algorithm. The individual, scattered gray dots, labeled as 'outliers' by the document context, signify data points that do not fit well into any of the identified clusters and are considered anomalous or exceptional. The distribution shows that while some outliers are near clusters, many are isolated, further emphasizing their distinction from the main data groups. This visualization technique is crucial for understanding the effectiveness of a clustering model and for identifying abnormal data points within a dataset.

**Key Insights:**
The main takeaway from this image, in conjunction with its context, is the clear visual differentiation between clustered data and outliers in a 2D projection. It demonstrates that data can be effectively grouped into distinct clusters (various colors) and that anomalous data points (gray) can be identified and separated. This visualization provides insight into the structure of the data and the performance of an anomaly detection or clustering algorithm. The visual evidence shows that the 'generated clusters' are typically denser and form coherent regions, while 'outliers' are sparser and more isolated, supporting the concept of data partitioning into typical and atypical observations.

**Document Context:**
This image directly supports the document section titled 'Plot outliers and non-outliers separately.' As stated in the text after the image, 'Figure 5-8. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization.' The image serves as a visual example, illustrating precisely how clusters and outliers, identified through an analytical process, are presented in a 2D space, thereby enhancing the reader's comprehension of the methodology discussed in the surrounding document.

**Summary:**
The image is a 2D scatter plot visualization depicting data points categorized into distinct clusters and outliers. The central area of the plot shows multiple groups of data points, each rendered in a different color (e.g., light green, brown, light yellow, dark red, various shades of purple, light pink, dark green, dark blue). These colored groups represent the 'generated clusters'. Scattered throughout and around these clusters, particularly visible as a hazy, diffuse background and as individual, less concentrated points, are smaller, darker gray dots. These gray dots represent the 'outliers'. The visualization is set against a plain white background, and there are no axes, labels, legends, titles, or any other textual annotations embedded directly within the image itself. The purpose is to visually distinguish between the clustered data and the anomalous, outlying data points.](images/10fd2649719a93e52c0855efea98bdac67a5445d66c6795cc31e102d9f2db734.jpg)
Figure 5-8. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization.

This is visually appealing but does not yet allow us to see what is happening inside the clusters. Instead, we can extend this visualization by going from text clustering to topic modeling.

# From Text Clustering to Topic Modeling

Text clustering is a powerful tool for finding structure among large collections of documents. In our previous example, we could manually inspect each cluster and identify them based on their collection of documents. For instance, we explored a cluster that contained documents about sign language. We could say that the topic of that cluster is “sign language.”

This idea of finding themes or latent topics in a collection of textual data is often referred to as topic modeling. Traditionally, it involves finding a set of keywords or phrases that best represent and capture the meaning of the topic, as we illustrate in Figure 5-9.

![## Image Analysis: e69b4f6ea61df008d80abf7140d90f170eaec9b559c325bb6f5e020e4947cf8a.jpg

**Conceptual Understanding:**
This image conceptually represents the process of "Topic modeling." Its main purpose is to illustrate how unstructured textual data is processed to identify and extract underlying semantic themes or topics. The key idea communicated is that topic modeling groups related words into distinct, interpretable topics, thereby providing a structured representation of the content within a collection of documents. It also highlights the flexibility in how these topics can be represented. The process moves from an input of raw text to an output of categorized topic representations, each defined by a set of characteristic terms.

**Content Interpretation:**
The image illustrates the process of "Topic modeling." This system takes "Input (Textual data)" and transforms it into structured "Output (Topic representations)". The input is conceptually a collection of documents. The process of topic modeling identifies underlying themes within this text. The output is categorized into discrete topics (Topic 1, Topic 2, Topic 3), each represented by a set of semantically related keywords. For instance, "Topic 1" includes "dog, cat, animal shelter, breeds, pet", indicating a theme around pets. "Topic 2" includes "pasta, pizza, rice, recipes, cooking", for a food theme. "Topic 3" consists of "sport, soccer, basketball, game, athletes", representing a sports theme. The significance lies in showing how unstructured text is given structure, and how topics are not just single words but clusters of related terms. The note "Representations can take many forms: keywords, bi-grams, labels, etc." further explains the versatility of topic representation beyond just keywords.

**Key Insights:**
The main takeaways from this image are: 1. Topic modeling is a method for organizing and understanding large amounts of text by discovering abstract topics within the data, as evidenced by the "Topic modeling" arrow transforming "Input (Textual data)" into "Output (Topic representations)". 2. These discovered topics are represented by a collection of semantically related words or phrases, clearly shown by the keyword groupings for "Topic 1" ("dog, cat, animal shelter, breeds, pet"), "Topic 2" ("pasta, pizza, rice, recipes, cooking"), and "Topic 3" ("sport, soccer, basketball, game, athletes"). 3. The representation of topics is flexible and can extend beyond single keywords to include bi-grams or other labels, as explicitly stated in the note: "Representations can take many forms: keywords, bi-grams, labels, etc.". The image provides an effective visual summary of topic modeling's ability to extract coherent themes from text.

**Document Context:**
This image directly supports the document's section titled "From Text Clustering to Topic Modeling" by visually explaining the core concept of topic modeling. It demonstrates how raw textual data is transformed into meaningful, categorized topic representations. This provides a clear understanding of what topic modeling accomplishes, setting the foundation for further discussion in the document regarding how it differs from or builds upon text clustering techniques. It visually clarifies the output and purpose of the method being discussed.

**Summary:**
This diagram illustrates the conceptual process of "Topic modeling," which transforms raw "Input (Textual data)" into organized "Output (Topic representations)". The input is depicted as a stack of three documents, symbolizing a corpus of text. An arrow labeled "Topic modeling" shows the process of analyzing this textual data. The output consists of distinct topics, each identified by a number and a set of associated terms. For example: "Topic 1" is represented by keywords like "dog", "cat", "animal shelter", "breeds", and "pet", suggesting a theme related to animals or pets. "Topic 2" is characterized by terms such as "pasta", "pizza", "rice", "recipes", and "cooking", clearly indicating a food or culinary theme. "Topic 3" is defined by words like "sport", "soccer", "basketball", "game", and "athletes", representing a sports-related theme. A crucial note at the bottom of the diagram clarifies that "Representations can take many forms: keywords, bi-grams, labels, etc." This emphasizes that while single keywords are shown here for simplicity, topic representations can also include multi-word phrases (bi-grams) or more abstract labels, providing flexibility in how topics are defined and presented. The diagram effectively visualizes how topic modeling groups semantically related terms from a body of text into coherent, interpretable subjects.](images/e69b4f6ea61df008d80abf7140d90f170eaec9b559c325bb6f5e020e4947cf8a.jpg)
Figure 5-9. Traditionally, topics are represented by a number of keywords but can take other forms.

Instead of labeling a topic as “sign language,” these techniques use keywords such as “sign,” “language,” and “translation” to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords.

Classic approaches, like latent Dirichlet allocation, assume that each topic is charac‐ terized by a probability distribution of words in a corpus’s vocabulary.5 Figure 5-10 demonstrates how each word in a vocabulary is scored against its relevance to each topic.

![## Image Analysis: 95642e7afcd933a607c85960cf4cf4dc93bb50cb3347f1abedaffd43b6976cf2.jpg

**Conceptual Understanding:**
This image represents a visualization of keyword distributions across different topics, a common output of topic modeling algorithms. Conceptually, it illustrates how words (keywords) are associated with abstract categories (topics), and the strength of these associations. The main purpose is to demonstrate how individual topics are characterized by a unique set of prominent keywords, thereby making the topics interpretable. It conveys the idea that documents or text segments can be broken down into underlying thematic components, each defined by a cluster of highly relevant words. The key concepts being communicated are topic coherence, keyword importance within a topic, and the distinctiveness of different topics based on their keyword profiles.

**Content Interpretation:**
The image displays the output of a topic modeling process, specifically showing the distribution or weight of various keywords across three identified topics. Each horizontal bar represents a keyword's association with a particular topic, with longer bars indicating a stronger relevance or higher probability. Topic 1 is characterized by keywords like 'cat' and 'pet' with long bars, and 'dog' with a medium bar, suggesting it is related to domestic animals or 'pets'. Topic 2 is predominantly defined by 'cooking', 'pasta', and 'recipes' with long bars, indicating a theme centered around 'culinary activities' or 'food preparation'. Topic 3 is strongly associated with 'athletes' and 'game' via long bars, and 'soccer' with a medium bar, clearly pointing to a 'sports' theme. The colors (purple for Topic 1, green for Topic 2, blue for Topic 3) visually differentiate the topics, while the varying shades within each color seem to denote the different keywords within that topic, although the primary indicator of strength is bar length. The extracted text elements – the topic labels and the keyword labels, along with the visual representation of bar lengths – directly support these interpretations by showing which keywords have significant weight in each topic.

**Key Insights:**
The main takeaway from this image is that topic modeling effectively groups related keywords into coherent, interpretable topics. The chart clearly shows distinct distributions: 'Topic 1' is heavily associated with 'pet'-related terms ('cat', 'pet', 'dog'), 'Topic 2' with 'cooking'-related terms ('cooking', 'pasta', 'recipes'), and 'Topic 3' with 'sports'-related terms ('athletes', 'game', 'soccer'). This demonstrates that keywords are not randomly distributed but rather concentrated within specific topics. The insight is that keyword distribution provides strong evidence for the semantic content of each topic. For instance, the long purple bars for 'cat' and 'pet' under 'Topic 1' provide concrete textual evidence that 'Topic 1' is about pets. Similarly, the long green bars for 'cooking', 'pasta', and 'recipes' confirm 'Topic 2' as food-related, and the long blue bars for 'athletes' and 'game' indicate 'Topic 3' is sports-related. This visual representation underscores the interpretability and value of topic modeling in extracting meaningful themes from text data.

**Document Context:**
This image serves as a direct illustration of the output from a topic modeling process, a key concept in the document's section 'From Text Clustering to Topic Modeling'. It visually clarifies how 'Keywords are extracted based on their distribution over a single topic,' as stated in the text after the image (Figure 5-10 caption). The chart demonstrates how a set of keywords can be distinctly allocated to different underlying topics based on their statistical co-occurrence or relevance, allowing for an intuitive understanding of what each abstract topic represents. This visual evidence supports the theoretical discussion of topic modeling by providing a concrete example of keyword-topic relationships.

**Summary:**
This image is a bar chart visualizing the distribution of nine specific keywords across three distinct topics, labeled 'Topic 1', 'Topic 2', and 'Topic 3'. The chart is organized with keywords listed vertically on the left and horizontal bars extending to the right, indicating their relative weight or prominence within each topic. Each topic is represented by a unique color scheme: Topic 1 uses shades of purple, Topic 2 uses shades of green, and Topic 3 uses shades of blue. For 'Topic 1', 'cat' and 'pet' have the longest purple bars, indicating high relevance. 'Dog' has a medium purple bar. 'Athletes', 'cooking', 'game', 'pasta', 'recipes', and 'soccer' have very small, light purple bars, showing minimal relevance. For 'Topic 2', 'cooking', 'pasta', and 'recipes' have the longest green bars, signifying high relevance. 'Athletes', 'cat', 'dog', 'game', 'pet', and 'soccer' have very small, light green bars, showing minimal relevance. For 'Topic 3', 'athletes' and 'game' have the longest blue bars, indicating high relevance. 'Soccer' has a medium blue bar. 'Cat', 'cooking', 'dog', 'pasta', 'pet', and 'recipes' have very small, light blue bars, showing minimal relevance. The distinct bar lengths within each topic allow readers to quickly identify the primary keywords defining each subject, effectively demonstrating how topic modeling allocates keyword importance.](images/95642e7afcd933a607c85960cf4cf4dc93bb50cb3347f1abedaffd43b6976cf2.jpg)
Figure 5-10. Keywords are extracted based on their distribution over a single topic.

These approaches generally use a bag-of-words technique for the main features of the textual data, which does not take the context nor the meaning of words and phrases into account. In contrast, our text clustering example does take both into account as it relies on Transformer-based embeddings that are optimized for semantic similarity and contextual meaning through attention.

In this section, we will extend text clustering into the realm of topic modeling through a highly modular text clustering and topic modeling framework, namely BERTopic.

# BERTopic: A Modular Topic Modeling Framework

BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations.6 The underlying algorithm can be thought of in two steps.

First, as illustrated in Figure 5-11, we follow the same procedure as we did before in our text clustering example. We embed documents, reduce their dimensionality, and finally cluster the reduced embedding to create groups of semantically similar documents.

![## Image Analysis: 0b4470d1361a453f999c70126f721d4df27f33dc90fc03254fbd4da95336eeea.jpg

**Conceptual Understanding:**
The image conceptually represents the first phase of the BERTopic pipeline, which is the process of grouping semantically similar documents. Its main purpose is to visually explain the sequence of operations and the key algorithms involved in transforming raw document data into meaningful clusters. The image communicates the idea of a multi-stage data transformation, starting from high-dimensional embeddings, reducing their complexity, and finally grouping them based on similarity. It illustrates how three distinct components (SBERT, UMAP, HDBSCAN) work in conjunction to achieve document clustering for topic modeling.

**Content Interpretation:**
This image displays a three-step pipeline for processing documents within the BERTopic framework, specifically for creating clusters of semantically similar documents. The process begins with '1. Embed documents' using 'SBERT', which transforms documents into high-dimensional numerical representations, illustrated as 'Dimensions (e.g., 512 values)'. Next, '2. Reduce dimensionality' is performed using 'UMAP' to compress these high-dimensional embeddings into a lower-dimensional space, shown as 'Compressed dimensions (e.g., 3 values)'. Finally, '3. Cluster compressed embeddings' is carried out using 'HDBSCAN', which groups the low-dimensional embeddings into distinct clusters, visually depicted as colored regions containing data points on a grid, representing semantically similar documents. The orange grid-like structures with vertical ellipses signify the transformation of data dimensions at each stage.

**Key Insights:**
The main takeaways from this image are: 1. Document embedding is achieved using SBERT, converting text into a high-dimensional vector space, as indicated by '1. Embed documents' and 'SBERT' leading to 'Dimensions (e.g., 512 values)'. 2. Dimensionality reduction is a crucial intermediate step, performed by UMAP, to simplify the data while retaining structure, evidenced by '2. Reduce dimensionality' and 'UMAP' resulting in 'Compressed dimensions (e.g., 3 values)'. 3. Clustering of documents is the final step in this initial pipeline, executed by HDBSCAN on the compressed embeddings, as shown by '3. Cluster compressed embeddings' and 'HDBSCAN' yielding a visual representation of distinct clusters. These steps together form a complete sub-process for generating semantically similar document clusters within BERTopic.

**Document Context:**
This image directly follows the document text mentioning 'Figure 5-11. The first part of BERTopic’s pipeline is to create clusters of semantically similar documents.' It serves as a foundational diagram illustrating the initial, critical steps of the BERTopic framework. It provides a visual explanation of how documents are processed from raw text to clustered topics, detailing the specific tools (SBERT, UMAP, HDBSCAN) and transformations (embedding, dimensionality reduction, clustering) involved. This visual representation enhances the reader's understanding of the underlying methodology of BERTopic by breaking down a complex process into understandable, sequential stages.

**Summary:**
The image illustrates the initial pipeline of BERTopic, a modular topic modeling framework, focusing on how semantically similar documents are grouped into clusters. The process involves three sequential stages, each using a specific tool and transforming the data's representation. First, documents are embedded using SBERT, resulting in high-dimensional representations, specifically exemplified by 512 dimensions. Second, the dimensionality of these embeddings is reduced using UMAP, compressing the data into a lower-dimensional space, such as 3 dimensions, while preserving essential structural properties. Finally, these compressed embeddings are clustered using HDBSCAN, which identifies dense regions in the lower-dimensional space to form distinct clusters of semantically similar documents, visually represented as groups of points on a grid. This clear, step-by-step approach demonstrates how raw text is converted into meaningful topic clusters.](images/0b4470d1361a453f999c70126f721d4df27f33dc90fc03254fbd4da95336eeea.jpg)
Figure 5-11. The first part of BERTopic’s pipeline is to create clusters of semantically similar documents.

Second, it models a distribution over words in the corpus’s vocabulary by leveraging a classic method, namely bag-of-words. The bag-of-words, as we discussed briefly in Chapter 1 and illustrate in Figure 5-12, does exactly what its name implies, counting the number of times each word appears in a document. The resulting representation could be used to extract the most frequent words inside a document.

![## Image Analysis: bc512871d6b383d4a247118e5a2e3ec3f81acadb65da201e660df5442f048d3f.jpg

**Conceptual Understanding:**
This image conceptually represents the "Bag-of-words" model, a foundational concept in Natural Language Processing (NLP). The main purpose is to illustrate how text data (specifically sentences) can be transformed into a numerical format, in this instance, binary vectors, based on the presence or absence of words from a predefined vocabulary. The key idea being communicated is the conversion of unstructured text into structured, quantifiable data suitable for computational analysis, emphasizing term frequency or presence while disregarding grammatical structure and word order.

**Content Interpretation:**
The image illustrates the process of creating a Bag-of-words representation for two short documents. The two purple rounded rectangles, labeled "My cat is cute" and "That is a cute dog", represent the raw textual input. The yellow rounded rectangles at the bottom, labeled "that", "is", "a", "cute", "dog", "my", "cat", collectively form the "Vocabulary of all input documents", emphasized by a surrounding dotted line. This vocabulary defines the features against which each document is measured. The rows of gray and purple squares with '0's and '1's represent the Bag-of-words vectors for each input document. For "My cat is cute", the vector is 0, 1, 0, 1, 0, 1, 1, indicating the presence (1) of "is", "cute", "my", and "cat" and absence (0) of "that", "a", and "dog" based on the vocabulary order. For "That is a cute dog", the vector is 1, 1, 1, 1, 1, 0, 0, showing the presence (1) of "that", "is", "a", "cute", and "dog" and absence (0) of "my" and "cat". This transformation is significant as it allows unstructured text to be processed by machine learning algorithms that require numerical input. The title "Bag-of-words" and subtitle "(Counting individual words; term frequency)" clarify that the model counts or checks for the presence of words, disregarding their sequence or grammatical role.

**Key Insights:**
**1. Text Vectorization:** The image demonstrates that complex textual information can be simplified and converted into a numerical vector format, as shown by the transformation of "My cat is cute" into `0, 1, 0, 1, 0, 1, 1` and "That is a cute dog" into `1, 1, 1, 1, 1, 0, 0`. This is a fundamental step for enabling computational analysis of text. **2. Vocabulary as a Feature Set:** The "Vocabulary of all input documents" (that, is, a, cute, dog, my, cat) serves as the complete set of features. Each position in the resulting vector corresponds to a word in this vocabulary, demonstrating how a shared lexicon forms the basis for comparison across documents. **3. Disregard for Word Order:** The Bag-of-words model, as implied by the term "Bag-of-words" itself and the vector representation, treats a document as an unordered collection (a "bag") of words. The relative position of words would yield the same vector, as long as the same words are present, simplifying text but losing semantic nuances related to syntax. **4. Basis for Term Frequency:** The subtitle "(Counting individual words; term frequency)" highlights that while this specific example shows binary presence, the Bag-of-words model generally allows for counting word occurrences, providing a measure of "term frequency." This quantitative aspect is crucial for various NLP tasks.

**Document Context:**
This image is highly relevant to the document's section "BERTopic: A Modular Topic Modeling Framework" because the Bag-of-words model is a fundamental concept in Natural Language Processing (NLP), often used as a preliminary step for topic modeling and other text analysis tasks. Before models like BERTopic can analyze topics, text usually needs to be converted into a numerical representation. The Bag-of-words model provides a straightforward yet effective way to achieve this, thus laying the groundwork for more advanced textual analysis. The subsequent text in the document, "Figure 5-12. A bag-of-words counts the number of times each word appears inside a document," directly confirms the purpose and fundamental mechanism illustrated in this figure, connecting it directly to the broader narrative of text processing for topic modeling.

**Summary:**
The image presents a clear and concise illustration of the "Bag-of-words" concept, a fundamental method used in Natural Language Processing (NLP) to convert text into a numerical format suitable for machine learning algorithms. The title "Bag-of-words" is prominently displayed at the top, accompanied by a subtitle "(Counting individual words; term frequency)" which explains its core mechanism. The illustration features two example sentences, each acting as an input document: "My cat is cute" and "That is a cute dog". Below these sentences, a dashed line encloses a set of yellow rounded rectangles, which collectively form the "Vocabulary of all input documents". This vocabulary is an ordered list of all unique words found across the input documents. In this specific example, the vocabulary, from left to right, consists of: "that", "is", "a", "cute", "dog", "my", "cat". The main part of the illustration demonstrates the transformation of each input sentence into a numerical vector based on this shared vocabulary. For each word in the vocabulary, the corresponding position in the vector will contain a '1' if the word is present in the document, and a '0' if it is absent. For the sentence "My cat is cute", an arrow points to a sequence of gray and purple squares representing its vector. When compared to the vocabulary, the resulting vector is 0, 1, 0, 1, 0, 1, 1. This signifies that "that", "a", and "dog" are absent (0), while "is", "cute", "my", and "cat" are present (1). Similarly, for the sentence "That is a cute dog", an arrow points to its corresponding vector: 1, 1, 1, 1, 1, 0, 0. This indicates that "that", "is", "a", "cute", and "dog" are present (1), while "my" and "cat" are absent (0). In summary, the Bag-of-words model represents each document as an unordered collection of words, converting human-readable text into a quantifiable format for computational analysis, primarily focusing on word presence or frequency.](images/bc512871d6b383d4a247118e5a2e3ec3f81acadb65da201e660df5442f048d3f.jpg)
Figure 5-12. A bag-of-words counts the number of times each word appears inside a document.

There are two caveats, however. First, this is a representation on a document level and we are interested in a cluster-level perspective. To address this, the frequency of words is calculated within the entire cluster instead of only the document, as illustrated in Figure 5-13.

![## Image Analysis: 3621062c1a92da5bbfc5cc5b4a1413b7ea4299d7c2158b2ce88fc14c633a291e.jpg

**Conceptual Understanding:**
This image conceptually represents the calculation of "Class-based term frequency" (c-TF), a method distinct from standard term frequency. Its main purpose is to illustrate how word counts are aggregated at the cluster level rather than being calculated for each individual document. This allows for a topic modeling framework, such as BERTopic, to understand the prominence of words within a defined topic (cluster) rather than just within single documents. The key ideas communicated are the grouping of documents into clusters, the definition of a vocabulary, and the resulting matrix representation of word frequencies per cluster. The image demonstrates that multiple documents contribute to the same row in the c-TF matrix, reinforcing the concept of cluster-level aggregation.

**Content Interpretation:**
The image displays a conceptual diagram explaining how Class-based Term Frequency (c-TF) is generated. It illustrates the transformation of a collection of text documents (potentially grouped into clusters) into a matrix where rows represent clusters and columns represent specific words from a predefined vocabulary. The numerical values within this matrix indicate the aggregated frequency of each word within its corresponding cluster. The visual representation clearly shows that individual documents contribute to a collective word count for their respective clusters, rather than maintaining separate document-level word frequencies. The explicit formula "Frequency (tf) of word (x) in cluster (c) = ||tf_{x,c}||" further clarifies the calculation of these term frequencies.

**Key Insights:**
The main takeaway is that c-TF (Class-based Term Frequency) is a method for aggregating word frequencies at the cluster level, not the document level. This is explicitly shown by the title "Class-based term frequency ('c-TF')" and the process flow where multiple documents (e.g., "My cat is cute", "That is a cute dog") are associated with a single cluster row in the frequency matrix. The extracted text elements, such as the example sentences and the resulting frequency counts for each word in the vocabulary ("that", "is", "a", "cute", "dog", "my", "cat") across different clusters, provide concrete evidence of how words are counted collectively within a cluster. For instance, in the first (purple) cluster, words like 'my', 'cat', 'is', 'a', 'cute', 'dog', 'that' show specific frequency counts (1, 2, 1, 2, 1, 1, 1), which are derived from the collective occurrences in the documents belonging to that cluster. The formula "Frequency (tf) of word (x) in cluster (c) = ||tf_{x,c}||" formally defines this aggregated counting mechanism.

**Document Context:**
This image is highly relevant to the document's section on "BERTopic: A Modular Topic Modeling Framework" as it specifically details "Figure 5-13. Generating c-TF by counting the frequency of words per cluster instead of per document." It provides a visual explanation of a core component, c-TF, which is fundamental to how BERTopic operates. The diagram visually clarifies how BERTopic moves beyond traditional term frequency calculations by aggregating word counts at the cluster level, which is crucial for defining topics within the framework. It directly supports the understanding of how words are weighted and associated with topics, thereby enhancing the comprehension of the BERTopic methodology.

**Summary:**
The image illustrates the process of generating "Class-based term frequency" (c-TF), a method for counting the frequency of words within a cluster rather than per individual document. On the left side, there are four rounded rectangles representing documents or clusters of documents. The first two contain example sentences: "My cat is cute" and "That is a cute dog". The next two show ellipses "..." indicating more documents, grouped into different clusters, identified by their background colors (purple, light blue, and light green). Arrows extend from these document/cluster representations to rows in a grid on the right, which represents the c-TF matrix. At the bottom-left, there is a mathematical notation: "Frequency (tf) of word (x) in cluster (c) = ||tf_{x,c}||". Below the c-TF grid on the right, there is a row of seven rounded rectangles containing individual words, which form the vocabulary: "that", "is", "a", "cute", "dog", "my", "cat". The c-TF grid itself is a matrix of numerical values. Each row in this matrix corresponds to a cluster (matching the colors of the document/cluster representations on the left), and each column corresponds to a word in the vocabulary list at the bottom. The numbers within the squares indicate the frequency of a particular word in a particular cluster. For instance, in the first (purple) cluster row, the word "my" has a frequency of 1, "cat" has a frequency of 1, "is" has a frequency of 2, "a" has a frequency of 1, "cute" has a frequency of 2, "dog" has a frequency of 1, "that" has a frequency of 1. The second (light blue) cluster row shows frequencies like 4, 6, 3, 18, 0, 5, 0. The third (light green) cluster row shows frequencies like 4, 1, 3, 2, 0, 3, 0. The overarching concept is to aggregate word counts at the cluster level.](images/3621062c1a92da5bbfc5cc5b4a1413b7ea4299d7c2158b2ce88fc14c633a291e.jpg)
Figure 5-13. Generating $c$ -TF by counting the frequency of words per cluster instead of per document.

Second, stop words like “the” and “I” tend to appear often in documents and provide little meaning to the actual documents. BERTopic uses a class-based variant of term frequency–inverse document frequency (c-TF-IDF) to put more weight on words that are more meaningful to a cluster and put less weight on words that are used across all clusters.

Each word in the bag-of-words, the c-TF in c-TF-IDF, is multiplied by the IDF value of each word. As shown in Figure 5-14, the IDF value is calculated by taking the logarithm of the average frequency of all words across all clusters divided by the total frequency of each word.

![## Image Analysis: 6bc49b7424ea588dda739e0722f1bbd5e9b8d6bb10baebbc2a1fdedc28b8f3a9.jpg

**Conceptual Understanding:**
The image conceptually represents the calculation of Inverse Document Frequency (IDF), a statistical measure used to evaluate how important a word is to a document in a collection or corpus. Its main purpose is to illustrate the underlying mathematical principle behind weighting words in natural language processing tasks, particularly in topic modeling. It conveys the idea that words that are rare across a collection of documents (i.e., have a low document frequency) are more distinctive and therefore receive a higher weight or 'importance' score than words that are very common. The image breaks down this calculation using a concrete example to make the concept understandable.

**Content Interpretation:**
The image illustrates the calculation of Inverse Document Frequency (IDF) using a specific example. It shows a mathematical formula and applies it to a set of words with their corresponding frequencies. The main components are: the overall IDF formula, the constant '29' (likely representing the total number of documents or a similar corpus-level statistic), a series of words ('that', 'is', 'a', 'cute', 'dog', 'my', 'cat'), and their respective document frequencies (9, 9, 7, 22, 1, 9, 1). The 'log' function and the '+1' ensure the values are positive and scaled. The text below the main diagram explicitly defines the general formula: Average frequency (A) divided by the total frequency of each word (cf_x) = log (A / cf_x + 1). This clearly depicts how individual word frequencies (cf_x) are used to determine their inverse document frequency, highlighting that words appearing in fewer documents (like 'cute' or 'dog') will have a higher IDF score, indicating greater importance in distinguishing topics.

**Key Insights:**
The main takeaway from this image is a clear demonstration of how Inverse Document Frequency (IDF) is calculated and its purpose. It shows that words that appear in fewer documents (e.g., 'cute', 'dog', 'my', 'cat' with frequencies 22, 1, 9, 1 respectively, assuming 29 is the total documents) will have a higher IDF value compared to more common words (e.g., 'that', 'is', 'a' with frequencies 9, 9, 7). This highlights the principle that rarer words are considered more informative for distinguishing between documents or topics. The specific text 'log ( A / cf_x + 1 )' explicitly provides the formula, illustrating that IDF increases as the total frequency of a word (cf_x) decreases relative to the average frequency (A). The constant '29' represents a key value in the calculation, likely the total number of documents in the corpus being analyzed.

**Document Context:**
This image is presented in the context of 'BERTopic: A Modular Topic Modeling Framework', specifically following the text 'Figure 5-14. Creating a weighting scheme.' This indicates that the image serves to explain a fundamental concept (Inverse Document Frequency or IDF) that is crucial for understanding how BERTopic, or any topic modeling framework, assigns weights to words. Understanding IDF is essential for comprehending how topics are identified and differentiated based on the distinctiveness of their constituent words. The image visually breaks down the calculation, making the abstract concept of weighting scheme creation more tangible and comprehensible for readers delving into the mechanics of topic modeling.

**Summary:**
The image illustrates the calculation of Inverse Document Frequency (IDF), a core component in many topic modeling and information retrieval tasks. It presents a specific example to demonstrate how the IDF value is derived for individual words. The formula shown is log (29 / [word_frequency] + 1), where 29 appears to represent the total number of documents or a normalization factor. Below this, an array of numbers (9, 9, 7, 22, 1, 9, 1) is associated with specific words ('that', 'is', 'a', 'cute', 'dog', 'my', 'cat'). These numbers likely represent the document frequencies (cf_x) for each respective word. A word like 'cute' appears 22 times, while 'dog', 'my', and 'cat' appear once. The overall calculation combines these frequencies to produce an IDF score. The explanatory text at the bottom clarifies the general formula for Inverse Document Frequency: Average frequency (A) divided by the total frequency of each word (cf_x) = log (A / cf_x + 1). This visual aid provides a concrete example to understand how common and rare words are weighted differently based on their occurrences across a corpus, with rarer words ('cute', 'dog', 'my', 'cat') receiving higher potential IDF values due to lower `cf_x` values, making them more significant for topic identification. The '+1' ensures that terms appearing in all documents do not result in a division by zero or a log of zero.](images/6bc49b7424ea588dda739e0722f1bbd5e9b8d6bb10baebbc2a1fdedc28b8f3a9.jpg)
Figure 5-14. Creating a weighting scheme.

The result is a weight (“IDF”) for each word that we can multiply with their fre‐ quency $( ^ { \alpha } \mathrm { { c - T F } ^ { \alpha } ) }$ to get the weighted values $\overset { \ast } { \boldsymbol { \mathbf { \mathit { c } } } }$ -TF-IDF”).

This second part of the procedure, as shown in Figure 5-15, allows for generating a distribution over words as we have seen before. We can use scikit-learn’s CountVec torizer to generate the bag-of-words (or term frequency) representation. Here, each cluster is considered a topic that has a specific ranking of the corpus’s vocabulary.

![## Image Analysis: 40f9913acba484ab58662b104ee45c858bb88127c0f21b580192a60c0d665aa3.jpg

**Conceptual Understanding:**
The image conceptually represents the topic representation phase within the BERTopic topic modeling framework. Its main purpose is to illustrate the process of converting raw text data into meaningful, weighted topic representations. This involves two core steps: generating term frequencies per class and subsequently applying a modified TF-IDF (c-TF-IDF) formula to assign weights to these terms, thereby defining the unique characteristics of each topic. The image explains how the importance of individual terms within a class (topic) is calculated.

**Content Interpretation:**
The image illustrates the 'Represent topics' stage of the BERTopic pipeline, detailing the two main steps involved: 'Create a class-based bag-of-words' and 'Weigh terms'. It shows how raw term counts are processed into a frequency matrix and then transformed into weighted scores using a custom TF-IDF (Term Frequency-Inverse Document Frequency) approach. The core process is the calculation of a term's weight within a specific class, which is crucial for defining and distinguishing topics in the BERTopic framework.

**Key Insights:**
1. **Topic Representation Mechanism:** The image elucidates that BERTopic represents topics by first creating a class-based bag-of-words and then weighting these terms. This highlights a two-stage process for topic definition. 
2. **Role of CountVectorizer:** The 'CountVectorizer' is identified as the tool or method for generating the initial term frequencies for each class, providing the foundational numerical representation of words within topics. 
3. **Class-based Term Frequency (c-TF):** The matrix '||tf_x,c||' represents the raw count of terms (tf) within specific classes (c). This is explicitly labeled as 'c-TF', emphasizing that term frequency is considered within the context of its assigned class rather than across all documents. 
4. **Class-based TF-IDF (c-TF-IDF):** The process uses a modified TF-IDF calculation ('c-TF-IDF') to weigh terms. This specific adaptation is key for BERTopic to effectively identify terms that are highly representative of a particular topic/class while being less common across other topics/classes. 
5. **Components of c-TF-IDF Formula:** The formula `||tf_x,c|| × log(A/cf_x + 1)` explicitly breaks down the weighting. `||tf_x,c||` (c-TF) measures how often a term appears in a given class, while `log(A/cf_x + 1)` (IDF) measures the rarity of a term across all classes. `A` likely represents the total number of classes or documents, and `cf_x` represents the number of classes/documents term `x` appears in. The multiplication of these two components ensures that terms frequent within a class but rare across classes receive higher weights, making them strong indicators of that topic.

**Document Context:**
This image directly relates to the 'BERTopic: A Modular Topic Modeling Framework' section of the document, specifically addressing the 'representing the topics' part of BERTopic's pipeline. As indicated by the surrounding text, it details 'the calculation of the weight of term x in a class c', which is a fundamental step in how BERTopic quantifies and distinguishes different topics. The diagram, showing steps 4 and 5, suggests it follows previous steps in a larger topic modeling process.

**Summary:**
The image illustrates the second part of BERTopic's pipeline, specifically focusing on the representation and weighting of topics. It details the process of creating a class-based bag-of-words and subsequently weighing terms using a modified TF-IDF calculation. The workflow begins with a 'CountVectorizer' component, which processes input to generate a matrix of term frequencies per class, represented as '||tf_x,c||'. This matrix visually shows counts, such as '1', '2', '1', '2' in the first row; '4', '6', '3', '18' in the second row; and '4', '1', '3', '2' in the third row. Following this, the process moves to 'c-TF-IDF', where terms are weighted. This weighting is calculated by the formula '||tf_x,c|| × log(A/cf_x + 1)', where '||tf_x,c||' is explicitly labeled as 'c-TF' (class-based Term Frequency) and 'log(A/cf_x + 1)' is labeled as 'IDF' (Inverse Document Frequency). This detailed breakdown helps readers understand how BERTopic quantitatively assesses the importance of terms within specific topics, building on the initial creation of a class-based bag-of-words.](images/40f9913acba484ab58662b104ee45c858bb88127c0f21b580192a60c0d665aa3.jpg)
Figure 5-15. The second part of BERTopic’s pipeline is representing the topics: the calcu‐ lation of the weight of term $^ * { x ^ { * } }$ in a class $^ * { c } ^ { * }$ .

Putting the two steps together, clustering and representing topics, results in the full pipeline of BERTopic, as illustrated in Figure 5-16. With this pipeline, we can cluster semantically similar documents and from the clusters generate topics represented by several keywords. The higher a word’s weight in a topic, the more representative it is of that topic.

![## Image Analysis: f5b4b2832e127050156cac2e417147d9260c97996b9b4489867ec5dc76b3f645.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural pipeline of the BERTopic framework, a method for topic modeling. The main purpose of this diagram is to illustrate the sequential flow and specific components involved in generating topics from text data. It conveys the key ideas that BERTopic is a modular system, broadly divided into a 'Clustering' phase and a 'Topic representation' phase, each employing distinct algorithms to achieve its goals. The image clearly shows the progression from initial text processing and grouping to the final stage of defining what those groups (topics) represent.

**Content Interpretation:**
The image depicts the modular pipeline of the BERTopic framework, outlining the specific components used in its two primary stages: 'Clustering' and 'Topic representation'. The 'Clustering' stage involves sequential application of 'SBERT', 'UMAP', and 'HDBSCAN'. 'SBERT' (Sentence-BERT) is likely used for generating semantically meaningful sentence embeddings. 'UMAP' (Uniform Manifold Approximation and Projection) is a dimensionality reduction technique applied to these embeddings. 'HDBSCAN' (Hierarchical Density-Based Spatial Clustering of Applications with Noise) then clusters the reduced-dimension data. The 'Topic representation' stage consists of 'CountVectorizer' and 'c-TF-IDF'. 'CountVectorizer' typically transforms text into a matrix of token counts, which is then fed into 'c-TF-IDF' (class-based Term Frequency-Inverse Document Frequency) to extract and represent the topics from the generated clusters. The significance lies in showing a clear, step-by-step breakdown of how BERTopic leverages these advanced NLP and machine learning techniques to perform topic modeling.

**Key Insights:**
The main takeaway from this image is that the BERTopic framework for topic modeling is structured into two main, sequential phases: 'Clustering' and 'Topic representation'. Each phase comprises specific, well-defined components. For 'Clustering', the process sequentially involves 'SBERT' for embedding, 'UMAP' for dimensionality reduction, and 'HDBSCAN' for clustering. For 'Topic representation', the process involves 'CountVectorizer' for token counting and 'c-TF-IDF' for generating topic representations. The image clearly demonstrates a modular and pipeline-driven approach to topic modeling, combining state-of-the-art embedding models with effective clustering and representation techniques. The textual evidence, specifically the labels 'Clustering', 'Topic representation', 'SBERT', 'UMAP', 'HDBSCAN', 'CountVectorizer', and 'c-TF-IDF' alongside the connecting arrows, provides direct support for these insights, illustrating the complete workflow of the BERTopic framework.

**Document Context:**
This image serves as a direct visual explanation of the BERTopic framework's operational pipeline, as introduced in the document context. The text after the image, stating 'Figure 5-16. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation,' perfectly sets up this diagram. The image then elaborates on these 'two steps' by detailing the specific algorithms and tools ('SBERT', 'UMAP', 'HDBSCAN' for Clustering; 'CountVectorizer', 'c-TF-IDF' for Topic representation) involved in each stage. This detailed breakdown enhances the reader's understanding of the framework's architecture and the sequence of operations, making abstract concepts concrete and illustrating the modular nature of BERTopic.

**Summary:**
The image illustrates the full pipeline of BERTopic, a topic modeling framework, divided into two main stages: Clustering and Topic Representation. The process begins with the 'Clustering' stage, where 'SBERT' is the initial component, likely responsible for generating embeddings. The output from 'SBERT' then flows into 'UMAP', which performs dimensionality reduction. Following 'UMAP', 'HDBSCAN' is used for hierarchical density-based clustering. After the 'Clustering' stage is complete, the process moves to the 'Topic representation' stage. In this stage, 'CountVectorizer' is utilized, likely to convert a collection of text documents to a matrix of token counts. Finally, the output from 'CountVectorizer' is processed by 'c-TF-IDF' (class-based TF-IDF) to generate the topic representations. The entire flow is sequential, moving from left to right through these distinct components within their respective stages.](images/f5b4b2832e127050156cac2e417147d9260c97996b9b4489867ec5dc76b3f645.jpg)
Figure 5-16. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation.

A major advantage of this pipeline is that the two steps, clustering and topic repre‐ sentation, are largely independent of one another. For instance, with c-TF-IDF, we are not dependent on the models used in clustering the documents. This allows for significant modularity throughout every component of the pipeline. And as we will explore later in this chapter, it is a great starting point to fine-tune the topic representations.

As illustrated in Figure 5-17, although sentence-transformers is used as the default embedding model, we can swap it with any other embedding technique. The same applies to all other steps. If you do not want outliers generated with HDBSCAN, you can use k-means instead.

![## Image Analysis: 6a5c0dac9d0afdfa997ee7f0ffc4d051cfa3afdb72564804322c8cbe8c6e21f2.jpg

**Conceptual Understanding:**
The image conceptually represents the modularity and flexibility of the BERTopic framework. Its main purpose is to illustrate how users can 'Build your own topic model' by assembling different components, much like stacking Lego blocks. The key idea communicated is that BERTopic is not a rigid, fixed model but rather a customizable framework where individual processing steps (e.g., embedding, dimensionality reduction, clustering) can be interchanged to create diverse topic models tailored to specific needs.

**Content Interpretation:**
The image visually represents the modular architecture of the BERTopic framework for topic modeling. It depicts different 'stacks' of components, likened to building blocks, to illustrate how a topic model can be constructed from various interchangeable modules. 

Specifically, the image shows three distinct topic model configurations:
1.  **Topic model 1 (default):** This stack consists of the components SBERT (for embeddings), UMAP (for dimensionality reduction), HDBSCAN (for clustering), CountVectorizer (for term frequency counting), and c-TF-IDF (for topic representation). This sequence represents a typical, out-of-the-box BERTopic configuration.
2.  **Topic model 2 (customized):** This stack demonstrates the flexibility to replace default components with alternatives. Here, Bag-of-words (for embeddings/vectorization), PCA (for dimensionality reduction), k-Means (for clustering), CountVectorizer, and c-TF-IDF are used. This shows how users can customize the model by selecting different algorithms at various stages of the topic modeling pipeline.
3.  **Topic model n:** This stack, represented by ellipses, signifies that users can create 'n' number of different topic models by choosing from a variety of available or custom components for each layer. The consistent presence of 'CountVectorizer' and 'c-TF-IDF' in the top two layers of the first two explicit models suggests these might be standard or highly recommended final steps for topic representation within BERTopic, though the third model implies even these can be customized (represented by the ellipses). The use of '...' for the components of 'Topic model n' and between 'Topic model 2' and 'Topic model n' explicitly supports the idea of numerous potential configurations and the extensibility of the framework.

**Key Insights:**
The main takeaway from this image is that BERTopic is designed with a highly modular architecture, allowing for significant customization in building topic models. Users are not restricted to a single predefined approach but can mix and match various algorithms for embedding, dimensionality reduction, and clustering. The image demonstrates that different choices for each 'layer' or step in the topic modeling process lead to different 'topic models.' This flexibility empowers users to tailor the topic modeling approach to their specific data characteristics and analytical goals, moving beyond a 'one-size-fits-all' solution. The explicit labels 'Topic model 1 (default)' and 'Topic model 2 (customized)' with their respective component lists (SBERT, UMAP, HDBSCAN vs. Bag-of-words, PCA, k-Means) provide clear textual evidence for this customizability and the existence of a default configuration. The presence of 'Topic model n' with ellipses further reinforces the idea of boundless possibilities for customization.

**Document Context:**
This image directly supports the document's section titled 'BERTopic: A Modular Topic Modeling Framework' by visually explaining the core concept of modularity. It provides a concrete illustration of how BERTopic allows users to 'build your own topic model' by combining different algorithms for various stages of the topic modeling process. The accompanying text 'Figure 5-17. The modularity of BERTopic is a key component and allows you to build your own topic model however you want.' further reinforces the image's message, positioning it as a fundamental visual aid for understanding BERTopic's design principle and flexibility. The image enhances comprehension by visually demystifying how different components can be interchangeably used to construct diverse topic models.

**Summary:**
The image displays a conceptual diagram illustrating the modularity of BERTopic, showing how different components can be stacked to 'Build your own topic model'. It presents three distinct topic model configurations, resembling Lego-like blocks stacked vertically, each representing a different set of modular components. 

Starting from the bottom, Topic Model 1 (default) consists of five layers: SBERT, UMAP, HDBSCAN, CountVectorizer, and c-TF-IDF. This represents a standard or default configuration of the BERTopic framework. 

Topic Model 2 (customized) also has five layers, but with different components, demonstrating customization: Bag-of-words at the bottom, followed by PCA, k-Means, CountVectorizer, and c-TF-IDF at the top. This model showcases how components can be swapped out to create a tailored topic model. 

A third stack, labeled 'Topic model n', is shown with ellipses '...' in each of its five block positions, indicating that an arbitrary 'n' number of other custom topic models can be built using various combinations of components. This visual metaphor highlights the flexibility and extensibility of BERTopic, where users can mix and match different algorithms for embedding, dimensionality reduction, clustering, and topic representation to construct a topic model suited to their specific needs. The title 'Build your own topic model' clearly communicates this core message of customization.](images/6a5c0dac9d0afdfa997ee7f0ffc4d051cfa3afdb72564804322c8cbe8c6e21f2.jpg)
Figure 5-17. The modularity of BERTopic is a key component and allows you to build your own topic model however you want.

You can think of this modularity as building with Lego blocks; each part of the pipeline is completely replaceable with another, similar algorithm. Through this modularity, newly released models can be integrated within its architecture. As the field of Language AI grows, so does BERTopic!

# The Modularity of BERTopic

The modularity of BERTopic has another advantage: it allows it to be used and adapted to different use cases using the same base model. For instance, BERTopic supports a wide variety of algorithmic variants:

• Guided topic modeling   
• (Semi-)supervised topic modeling   
• Hierarchical topic modeling   
• Dynamic topic modeling   
• Multimodal topic modeling   
• Multi-aspect topic modeling   
• Online and incremental topic modeling • Zero-shot topic modeling   
• Etc.

The modularity and algorithmic flexibility are the foundation of the author’s aim to make BERTopic the one-stop-shop for topic modeling. You can find a full overview of its capabilities in the documentation or the repository.

To run BERTopic with our ArXiv dataset, we can use our previously defined models and embeddings (although it is not mandatory):

from bertopic import BERTopic

# Train our model with our previously defined models   
topic_model $=$ BERTopic( embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True   
).fit(abstracts, embeddings)

Let us start by exploring the topics that were created. The get_topic_info() method is useful to get a quick description of the topics that we found:

topic_model.get_topic_info()

<table><tr><td colspan="2">Topic Count Name</td><td>Representation</td><td></td></tr><tr><td>-1</td><td>14520</td><td>-1_the_of_and_to</td><td>[the, of, and,to, in, we, that, language,fo...</td><td rowspan="5"></td></tr><tr><td>0</td><td>2290</td><td>0_speech_asr_recognition_end</td><td>[speech,asr, recognition,end,acoustic, spea...</td></tr><tr><td>1</td><td>1403</td><td>1_medical_clinical_biomedical_patient</td><td>[medical, clinical, biomedical, patient, healt...</td></tr><tr><td>2</td><td>1156</td><td> 2_sentiment_aspect_analysis_reviews</td><td>[sentiment,aspect,analysis, reviews, opinion...</td></tr><tr><td>3</td><td>986</td><td>3_translation_nmt_machine_neural</td><td>[translation, nmt, machine, neural, bleu, engl...</td></tr><tr><td></td><td>…</td><td>…</td><td></td></tr><tr><td>150</td><td>54</td><td>150_coherence_discourse_paragraph_text</td><td>[coherence,discourse,paragraph, text, cohes...</td></tr><tr><td>151</td><td>54</td><td>151_prompt_prompts_optimization_prompting[prompt, prompts,optimization,prompting. lm..</td><td></td></tr><tr><td>152</td><td>53</td><td>152_sentence_sts_embeddings_similarity</td><td>[sentence,sts,embeddings,similarity,embed...</td></tr><tr><td>153</td><td>53</td><td>153_counseling_mental_health_therapy</td><td>[counseling, mental, health, therapy, psychoth...</td></tr><tr><td>154</td><td>50</td><td>154_backdoor_attacs_attack_triggers</td><td>[backdoor,attacks,attack, triggers, poisoned...</td></tr></table>

Each of these topics is represented by several keywords, which are concatenated with a “_” in the Name column. This Name column allows us to quickly get a feeling of what the topic is about as it shows the four keywords that best represent it.

You might also have noticed that the very first topic is labeled -1. That topic contains all documents that could not be fitted within a topic and are considered outliers. This is a result of the clustering algorithm, HDBSCAN, which does not force all points to be clus‐ tered. To remove outliers, we could either use a non-outlier algo‐ rithm like k-means or use BERTopic’s reduce_outliers() function to reassign the outliers to topics.

We can inspect individual topics and explore which keywords best represent them with the get_topic function. For example, topic 0 contains the following keywords:

topic_model.get_topic(0)

[('speech', 0.028177697715245358), ('asr', 0.018971184497453525), ('recognition', 0.013457745472471012), ('end', 0.00980445092749381), ('acoustic', 0.009452082794507863), ('speaker', 0.0068822647060204885), ('audio', 0.006807649923681604), ('the', 0.0063343444687017645), ('error', 0.006320144717019838), ('automatic', 0.006290216996043161)]

For example, topic 0 contains the keywords “speech,” “asr,” and “recognition.” Based on these keywords, it seems that the topic is about automatic speech recognition (ASR).

We can use the find_topics() function to search for specific topics based on a search term. Let’s search for a topic about topic modeling:

topic_model.find_topics("topic modeling")

([22, -1, 1, 47, 32], [0.95456535, 0.91173744, 0.9074769, 0.9067007, 0.90510106])

This returns that topic 22 has a relatively high similarity (0.95) with our search term. If we then inspect the topic, we can see that it is indeed a topic about topic modeling:

topic_model.get_topic(22)

[('topic', 0.06634619076655907), ('topics', 0.035308535091932707), ('lda', 0.016386314730705634), ('latent', 0.013372311924864435), ('document', 0.012973600191120576), ('documents', 0.012383715497143821), ('modeling', 0.011978375291037142), ('dirichlet', 0.010078277589545706), ('word', 0.008505619415413312), ('allocation', 0.007930890698168108)]

Although we know that this topic is about topic modeling, let’s see if the BERTopic abstract is also assigned to this topic:

topic_model.topics_[titles.index("BERTopic: Neural topic modeling with a classbased TF-IDF procedure")]

It is! These functionalities allow us to quickly find the topics that we are interested in.

![## Image Analysis: fb5a6a627925aeff69f28933610954ed587b3a7282f7ec02218ab2b771bf8825.jpg

**Conceptual Understanding:**
The image conceptually represents a brand identity or a visual marker. Its main purpose is to serve as a logo or a decorative element within the document. It communicates a visual theme through the silhouette of an animal, likely a monkey or lemur, framed by a square, aiming for recognition and visual association rather than conveying complex information directly.

**Content Interpretation:**
The image is a stylized logo or a decorative graphic. It depicts a solid green silhouette of an agile animal, likely a monkey or lemur, framed by a square. This suggests a brand identity, a project emblem, or a thematic visual element intended to be memorable and distinct. The choice of a quick, agile animal could subtly imply efficiency, quick learning, or dynamic processes.

**Key Insights:**
As a standalone logo without accompanying text, the image primarily conveys a sense of identity or branding. It communicates a visual theme rather than specific data or procedural knowledge. The stylized animal suggests attributes like agility, distinctiveness, or perhaps a connection to nature, depending on the brand's intent. No direct conclusions about model training or technical processes can be drawn from the image itself.

**Document Context:**
Given the document context 'Section: Train our model with our previously defined models,' this image likely serves as a brand logo for the project, organization, or specific model being discussed. While the image itself does not directly illustrate model training, its presence in this section suggests it visually identifies the entity or component related to the training process, providing a visual anchor for the reader. It functions as a visual cue rather than a data-rich diagram.

**Summary:**
The image displays a clear, comprehensive logo. It features a solid green silhouette of an animal, which appears to be a monkey or a lemur, positioned within a simple square outline. The animal is depicted in a crouching or pouncing stance, facing towards the right side of the frame. Its body is elongated, with four distinct limbs and a prominent, long tail that curls upwards and then spirals inwards towards the top-right corner of the square. The overall impression is one of agility and a distinct, stylized brand identity. No textual elements are present within the image.](images/fb5a6a627925aeff69f28933610954ed587b3a7282f7ec02218ab2b771bf8825.jpg)

The modularity of BERTopic gives you a lot of choices, which can be overwhelming. For that purpose, the author created a best practices guide that goes through common practices to speed up training, improve representations, and more.

To make exploration of the topics a bit easier, we can look back at our text clustering example. There, we created a static visualization to see the general structure of the created topic. With BERTopic, we can create an interactive variant that allows us to quickly explore which topics exist and which documents they contain.

Doing so requires us to use the two-dimensional embeddings, reduced_embeddings, that we created with UMAP. Moreover, when we hover over documents, we will show the title instead of the abstract to quickly get an understanding of the documents in a topic:

# Visualize topics and documents   
fig $=$ topic_model.visualize_documents( titles, reduced_embeddings $\mathbf { \equiv }$ reduced_embeddings, width $\scriptstyle 1 = 1 2 0 0$ , hide_annotations=True   
)   
# Update fonts of legend for easier visualization   
fig.update_layout(font $\Bumpeq$ dict(size=16))

As we can see in Figure 5-18, this interactive plot quickly gives us a sense of the created topics. You can zoom in to view individual documents or double-click a topic on the righthand side to only view it.

![## Image Analysis: c898502a99eb5b6f57789b06f53cf452f2a20512a5caf8223263ee330b9cbfac.jpg

**Conceptual Understanding:**
This image conceptually represents a topic or document landscape visualization, likely generated through dimensionality reduction techniques applied to text embeddings. Its main purpose is to illustrate the relationships and clustering of various topics or documents related to Large Language Models in a two-dimensional space. The key idea conveyed is that LLMs are involved in a wide array of distinct yet sometimes overlapping research and application areas, and that these areas can be visually grouped based on their semantic similarity. The green annotation highlights a specific capability or finding about LLMs, specifically that they are effective 'Complex Table Parsers'.

**Content Interpretation:**
The image visualizes the intricate relationships and semantic clustering among different topics and documents within the domain of Large Language Models. It utilizes a dimensionality reduction technique to project high-dimensional data into a two-dimensional space (D1 and D2), allowing for visual identification of groupings. Each cluster, distinguished by color, represents a specific topic as detailed in the legend. The primary concept being shown is the landscape of LLM applications and research, demonstrating how various functionalities and challenges, such as 'speech_asr_recognition', 'translation_nmt_machine', 'gender_bias_biases', and 'adversarial_attacks_attack', form distinct yet potentially interconnected areas. The green banner 'Large Language Models are Complex Table Parsers' highlights a specific interpretation or finding related to a particular cluster or the overall capability of LLMs.

**Key Insights:**
The main takeaway from this image is the diverse and extensive application and research landscape surrounding Large Language Models, as evidenced by the 23 distinct topic categories presented in the legend. The visual clustering suggests that despite their broad applications, specific areas of LLM research and development tend to form coherent, semantically related groups. For instance, 'summarization_summaries_summary' and 'translation_nmt_machine' likely represent related capabilities. A specific insight highlighted is that 'Large Language Models are Complex Table Parsers', indicating a particular strength or a significant area of research/application for LLMs, possibly derived from the analysis of the green-colored cluster. The detailed topics, such as 'legal_court_law' and 'medical_clinical_llms', underscore the cross-domain applicability of LLMs, while 'hate_offensive_speech' and 'gender_bias_biases' point to crucial ethical and societal considerations in LLM development.

**Document Context:**
This image directly serves as 'Figure 5-18. The output when we visualize documents and topics.' within the document. It provides a visual representation of the analytical results when documents and topics are processed and organized. It directly supports the preceding text by offering a concrete example of how topic modeling or document embedding can be visualized, enhancing the reader's comprehension of the patterns and relationships discussed regarding LLMs. The detailed legend helps in understanding the specific categories being analyzed, reinforcing the scope and focus of the document's content.

**Summary:**
This image displays a 2D scatter plot, labeled with axes D1 and D2, which visually represents the clustering of various documents and topics related to Large Language Models (LLMs). Each point on the plot corresponds to a document or topic, and its color indicates the specific category it belongs to, as defined by the comprehensive legend provided on the right side of the visualization. The spatial proximity of points and clusters suggests semantic similarity between the associated topics. A prominent green banner with an arrow points towards a cluster, stating the observation: 'Large Language Models are Complex Table Parsers'. The legend lists 23 distinct topics, each numerically prefixed and with a descriptive name, indicating the broad range of applications and research areas involving LLMs. For example, topics range from speech recognition and machine translation to legal document analysis and adversarial attacks. The visualization allows for an intuitive understanding of how these diverse topics interrelate and form distinct groups within the LLM landscape.](images/c898502a99eb5b6f57789b06f53cf452f2a20512a5caf8223263ee330b9cbfac.jpg)
Figure 5-18. The output when we visualize documents and topics.

There is a wide range of visualization options in BERTopic. There are three that are worthwhile to explore to get an idea of the relationships between topics:

# Visualize barchart with ranked keywords topic_model.visualize_barchart()

# Visualize relationships between topics # Visualize the potential hierarchical structure of topics topic_model.visualize_hierarchy()

# Adding a Special Lego Block

The pipeline in BERTopic that we have explored thus far, albeit fast and modular, has a disadvantage: it still represents a topic through a bag-of-words without taking into account semantic structures.

The solution is to leverage the strength of the bag-of-words representation, which is its speed to generate a meaningful representation. We can use this first meaningful representation and tweak it using more powerful but slower techniques, like embed‐ ding models. As shown in Figure 5-19, we can rerank the initial distribution of words to improve the resulting representation. Note that this idea of reranking an initial set of results is a main staple in neural search, a subject that we cover in Chapter 8.

![## Image Analysis: 5ec7d9aa29a98bda87190a24b579d75b601bd76dd48b7fb61c4bdef239c07946.jpg

**Conceptual Understanding:**
The image conceptually represents a topic modeling refinement process. Its main purpose is to visually demonstrate how an initial set of terms defining a topic (derived via c-TF-IDF) can be transformed and improved by a "Reranker (representation)" module. The core message conveyed is that a computational step can refine the relevance and ordering of terms within a topic, leading to a more accurate or useful representation. It illustrates a before-and-after comparison of a topic's constituent words, highlighting the impact of a reranking algorithm on term importance and composition.

**Content Interpretation:**
The image displays a computational process where an initial topic representation, derived using the c-TF-IDF algorithm, undergoes a reranking procedure to enhance its quality. The "Original topic (with c-TF-IDF)" represents a set of terms associated with a topic, where the length and color of the bars next to each term likely indicate their relevance or weight within that topic. Terms like "Summarization" and "Summaries" are highly ranked initially. The central "Reranker (representation)" block signifies a computational module or algorithm designed to re-evaluate and reorder these terms. The output, the "Reranked topic (in improved order)", shows a refined list where terms are reordered and some new terms appear while others are removed or diminished. For example, "Summary", "Document", "Extractive", "Rouge", "Documents", and "Factual" are present in the original list but absent from the reranked list, replaced by terms like "Sentences", "Text", "Metrics", "Datasets", "Neural", and "Model". This suggests the reranker aims to identify more pertinent terms for the topic and present them in a more meaningful sequence. The 'improved order' implies a higher quality, more coherent, or more accurate representation of the underlying topic.

**Key Insights:**
The main takeaway from this image is that topic representations, initially generated using methods like c-TF-IDF, can be significantly improved through a reranking process. This improvement involves not only reordering existing terms based on their relevance but also potentially replacing less relevant terms with more descriptive or accurate ones, leading to a more coherent and focused topic representation. The textual evidence shows a clear shift in the constituent terms and their hierarchical ordering. For instance, the original topic included general terms like "Document" and "Documents," which are removed in the reranked topic, replaced by more specific terms like "Sentences," "Text," "Metrics," "Datasets," "Neural," and "Model." This suggests the reranker learns to identify and prioritize terms that are more semantically central or technically relevant to the 'summarization' theme, which is strongly present in both lists. The change from "(with c-TF-IDF)" to "(in improved order)" explicitly highlights the value-add of the reranker.

**Document Context:**
This image directly illustrates the "Adding a Special Lego Block" section by visually representing the "Reranker" as a Lego-shaped block. It supports the document's narrative by demonstrating a specific step in fine-tuning topic representations, as stated in the text after the image: "Figure 5-19. Fine-tune the topic representations by reranking the original c-TF-IDF distributions." The image shows the input (original c-TF-IDF distribution of terms), the process (the reranker), and the output (the fine-tuned, reranked distribution), making a complex algorithmic step easily understandable. It is crucial for understanding how topic models are refined in the context of the document.

**Summary:**
The image illustrates a process of refining topic representations, specifically showing how an "Original topic" defined "(with c-TF-IDF)" is processed by a "Reranker (representation)" to produce a "Reranked topic" "(in improved order)". On the left, the "Original topic (with c-TF-IDF)" lists ten terms with varying bar lengths, indicating their initial importance or score: "Summarization" (longest blue bar), "Summaries" (second longest blue bar), "Summary" (grey bar), "Abstractive" (blue bar), "Document" (grey bar), "Extractive" (grey bar), "Rouge" (grey bar), "Documents" (grey bar), "Factual" (grey bar), and "Evaluation" (shortest blue bar). A horizontal arrow points from this list to a blue, Lego-block-shaped component labeled "Reranker (representation)". Another horizontal arrow points from the "Reranker" to the right side, where the "Reranked topic (in improved order)" is displayed. This reranked list also contains ten terms with updated bar lengths and positions, reflecting their improved order: "Summarization" (longest blue bar), "Summaries" (second longest blue bar), "Abstractive" (blue bar), "Evaluation" (blue bar), "Sentences" (grey bar), "Text" (grey bar), "Metrics" (grey bar), "Datasets" (grey bar), "Neural" (grey bar), and "Model" (grey bar). The visual comparison highlights a change in the relative importance and presence of terms after the reranking process.](images/5ec7d9aa29a98bda87190a24b579d75b601bd76dd48b7fb61c4bdef239c07946.jpg)
Figure 5-19. Fine-tune the topic representations by reranking the original c-TF-IDF distributions.

As a result, we can design a new Lego block, as shown in Figure 5-20, that takes in this first topic representation and spits out an improved representation.

In BERTopic, such reranker models are referred to as representation models. A major benefit of this approach is that the optimization of topic representations only needs to be done as many times as we have topics. For instance, if we have millions of documents and a hundred topics, the representation block only needs to be applied once for every topic instead of for every document.

As shown in Figure 5-21, a wide variety of representation blocks have been designed for BERTopic that allows you to fine-tune the representations. The representation block can even be stacked multiple times to fine-tune representations using different methodologies.

![## Image Analysis: ddf83756ff5f49549aeb4834e0a71f14a6997abfa15f8bb96402fb8cf41a4302.jpg

**Conceptual Understanding:**
This image conceptually represents a modular, multi-stage pipeline for document analysis, specifically designed for topic modeling and the generation of refined document representations. Its main purpose is to illustrate the sequential steps and the specific tools/methods employed at each stage, culminating in a fine-tuning process. The core message is to demonstrate a structured approach to extracting meaningful topics and representations from documents, highlighting the integration of a 'reranker' component to enhance the final output. Key ideas communicated include the decomposition of a complex task into manageable modules, the utilization of specialized algorithms for different sub-tasks, and the concept of iterative refinement in machine learning workflows.

**Content Interpretation:**
The image displays a multi-stage process for topic modeling or document representation, structured into three main conceptual phases: 'Clustering', 'Representation', and 'Reranker'. Each phase consists of sequential steps paired with specific algorithms or tools.

**Clustering (Topic creation):** This initial phase aims to identify coherent groups (topics) from documents. It starts with '1. Embed documents' using 'SBERT' to convert text into numerical vectors. These high-dimensional vectors are then processed to '2. Reduce dimensionality' using 'UMAP', making them more manageable. Finally, '3. Cluster embeddings' uses 'HDBSCAN' to group similar embeddings into distinct clusters, effectively creating topics.

**Representation (Label topics):** Following the clustering, this phase focuses on defining and labeling the identified topics. It involves '4. Tokenize words' using 'CountVectorizer' to prepare text for analysis and '5. Weight words' using 'c-TF-IDF' to assign importance scores to words within each topic, thereby helping to label them.

**Reranker (Fine-tune representation):** This final phase, indicated by an arrow pointing upwards from the 'Representation' stack, involves '6. Fine-tune representation' using a 'Reranker (representation)' component. The explicit mention of 'fine-tune representation' suggests that this 'Reranker' refines or optimizes the representations generated in the preceding stages, likely to improve topic coherence or document relevance. The bracketed '(representation)' further clarifies its function, indicating it operates directly on the existing representations.

**Key Insights:**
The image conveys several key insights regarding advanced topic modeling and document representation:

1.  **Modular Pipeline Architecture:** The 'Lego block' visual metaphor, coupled with numbered steps, clearly shows that topic modeling can be broken down into a series of distinct, sequential, and modular stages. Each block represents a specific operation and often a dedicated tool.
    *   *Evidence:* The stacking of individual blocks like 'SBERT', 'UMAP', 'HDBSCAN', 'CountVectorizer', 'c-TF-IDF', and 'Reranker (representation)' for '1. Embed documents' through '6. Fine-tune representation'.

2.  **Standard and Advanced NLP Techniques:** The pipeline integrates both foundational and more advanced NLP techniques. It starts with embedding (SBERT), moves to dimensionality reduction (UMAP) and clustering (HDBSCAN), and then to word processing (CountVectorizer, c-TF-IDF).
    *   *Evidence:* The specific names of tools: 'SBERT', 'UMAP', 'HDBSCAN', 'CountVectorizer', 'c-TF-IDF'.

3.  **Clear Stages with Defined Objectives:** Each major vertical segment of the pipeline has a clear overarching goal: 'Clustering: Topic creation', 'Representation: Label topics', and 'Reranker: Fine-tune representation'.
    *   *Evidence:* The labels 'Clustering', 'Topic creation', 'Representation', 'Label topics', 'Reranker', 'Fine-tune representation'.

4.  **Refinement and Enhancement:** The inclusion of a 'Reranker (representation)' as the final step for 'Fine-tune representation' indicates that initial representations can be further optimized. This suggests an iterative or refinement aspect to the overall process, where a specialized component improves upon prior stages.
    *   *Evidence:* The 'Reranker (representation)' block and the associated text '6. Fine-tune representation' and 'Reranker: Fine-tune representation'. The upward arrow from the 'c-TF-IDF' stack to the 'Reranker' also emphasizes this sequential refinement.

**Document Context:**
This image directly supports the document's section 'Adding a Special Lego Block' by visually illustrating the integration of a new component, specifically the 'Reranker (representation)' block, into an existing topic modeling or document processing pipeline. The accompanying text 'Figure 5-20. The reranker (representation) block sits on top of the c-TF-IDF representation.' provides explicit context, confirming that the reranker is an added layer that builds upon and refines the output of the c-TF-IDF representation step. This demonstrates how modular components can be 'stacked' (like Lego blocks) to enhance or extend a system, providing a clear visual explanation for the introduction of a specialized component within a technical workflow.

**Summary:**
The image titled 'Figure 5-20. The reranker (representation) block sits on top of the c-TF-IDF representation.' visually depicts a six-step process for document analysis, likely topic modeling, structured as a series of modular 'Lego blocks' representing different stages and their corresponding tools or methods. The process begins with document preparation and clustering, moves to topic representation, and culminates in a fine-tuning step involving a reranker. 

The workflow proceeds sequentially:

**Clustering (Topic creation):**
1.  **Embed documents:** Documents are first converted into numerical embeddings using the **SBERT** (Sentence-BERT) model.
2.  **Reduce dimensionality:** The dimensionality of these embeddings is then reduced using **UMAP** (Uniform Manifold Approximation and Projection).
3.  **Cluster embeddings:** The reduced-dimensionality embeddings are clustered to form topics using **HDBSCAN** (Hierarchical Density-Based Spatial Clustering of Applications with Noise).

**Representation (Label topics):**
4.  **Tokenize words:** Words from the documents are tokenized using **CountVectorizer**.
5.  **Weight words:** The tokenized words are then weighted using **c-TF-IDF** (class-based Term Frequency-Inverse Document Frequency) to identify key topic words. An upward arrow indicates that the output of this stage feeds into the next step.

**Reranker (Fine-tune representation):**
6.  **Fine-tune representation:** The final stage involves a **Reranker (representation)** block that sits on top of the c-TF-IDF representation to fine-tune the overall representation, enhancing the quality of the generated topics or document representations. 

This diagram illustrates a structured and modular approach to advanced text processing, emphasizing how different techniques and models are combined to achieve a refined output, particularly highlighting the integration of a reranker for further optimization.](images/ddf83756ff5f49549aeb4834e0a71f14a6997abfa15f8bb96402fb8cf41a4302.jpg)
Figure 5-20. The reranker (representation) block sits on top of the c-TF-IDF representation.

![## Image Analysis: c04ae98b4904f231e98f1f805ede28a77db05b039efa79ce47cd8bf7610c3abd.jpg

**Conceptual Understanding:**
This image conceptually represents different strategies or architectures for using “representation blocks” to fine-tune topics. It uses the metaphor of Lego blocks to illustrate modularity and combinability.The main purpose of the image is to demonstrate that topic fine-tuning can be achieved either by selecting a single, specific representation model (block) or by combining multiple such models (stacking blocks) to potentially create more sophisticated or customized topic representations.Key ideas or concepts being communicated are the modular nature of topic fine-tuning components, the flexibility in choosing between a single model approach versus a composite (stacked) model approach, and the examples of specific representation models like KeyBERT, MMR, and spaCy that can serve as these modular blocks. It highlights the extensibility of such a framework, allowing for various combinations or single-block choices for diverse topic modeling needs.

**Content Interpretation:**
The image conceptually represents different strategies or architectures for using “representation blocks” to fine-tune topics. It uses the metaphor of Lego blocks to illustrate modularity and combinability.The main purpose is to show that topic fine-tuning can be achieved either by selecting a single, specific representation model (block) or by combining multiple such models (stacking blocks) to potentially create more sophisticated or customized topic representations.Key ideas or concepts include modularity, combinability, specific representation models (KeyBERT, MMR, spaCy), topic fine-tuning, and flexibility in model selection and composition.The image depicts:1. **Topic Fine-tuning Process:** The overarching process is “Fine-tune topics with representation blocks,” indicating a phase in natural language processing or machine learning where initial topic models are refined.2. **Representation Blocks:** Specific models or techniques used for this fine-tuning are referred to as “representation blocks.” Examples explicitly named are “KeyBERT,” “MMR,” and “spaCy,” which are known tools/methods in NLP for keyword extraction, text summarization, and natural language understanding, respectively. The “...” blocks signify that many other such models or techniques exist and can be utilized.3. **Selection vs. Stacking Relationship:** The image highlights two distinct approaches for using these blocks:    *   **“Choose a single block”:** This path suggests a simpler, direct application of one specific representation model, exemplified by “KeyBERT.” This implies that some tasks might benefit from a single, focused model.    *   **“Or stack multiple blocks”:** This alternative path indicates the possibility of combining different representation models in a layered or sequential manner. The example stack “MMR” on “KeyBERT” suggests that the output or features from one block (KeyBERT) might feed into or be enhanced by another (MMR), allowing for more complex or multi-faceted topic representations. The “...” blocks further illustrate the potential for varied and extensive combinations.4. **Modularity and Flexibility:** The Lego block metaphor strongly conveys the idea of modular components that can be assembled in different ways to achieve desired outcomes. This suggests a flexible and adaptable framework for topic fine-tuning.The significance of the information presented includes:    *   The inclusion of specific names like “KeyBERT,” “MMR,” and “spaCy” provides concrete examples of the types of “representation blocks” that can be used, grounding the abstract concept in actual tools/algorithms.    *   The “...” blocks are significant because they imply a vast ecosystem of other potential representation models, emphasizing the wide applicability and extensibility of the framework.    *   The explicit presentation of “Choose a single block” versus “Or stack multiple blocks” highlights a key design decision point in implementing topic fine-tuning, offering a trade-off between simplicity and complexity/power.All extracted text elements support these interpretations. For instance, “Fine-tune topics with representation blocks” sets the context. “KeyBERT,” “MMR,” “spaCy” provide concrete examples of the modular components. “Choose a single block” and “Or stack multiple blocks,” with their corresponding visual examples, explicitly detail the two methods of application, reinforced by the “...” blocks indicating further options. The overall visual structure, resembling Lego blocks, reinforces the concepts of modularity and stacking, which are explicitly named by the text.

**Key Insights:**
The main takeaways or lessons this image teaches are:1.  **Modularity in Topic Fine-Tuning:** Topic fine-tuning can be approached using modular “representation blocks,” allowing for flexible system design. This is evident from the “Lego block” visual metaphor and the phrases “Fine-tune topics with representation blocks,” “Choose a single block,” and “Or stack multiple blocks.”2.  **Choice Between Simplicity and Complexity:** Users have the option to employ a single representation model for fine-tuning or to combine multiple models in a stack. The distinct columns labeled “Choose a single block” and “Or stack multiple blocks” directly illustrate this choice.3.  **Examples of Representation Models:** Specific examples of these blocks include “KeyBERT,” “MMR,” and “spaCy,” suggesting a practical application of known NLP techniques.4.  **Extensibility:** The “...” (ellipsis) within the stacks signifies that there are many other potential “representation blocks” or combinations beyond the ones explicitly named, implying a highly extensible framework.Conclusions or insights this image supports include:    *   The fine-tuning process is not restricted to a single monolithic approach but can be customized through the selection and combination of specialized components.    *   The choice of using a single block or multiple blocks depends on the complexity of the topic fine-tuning task and the desired level of granularity or integration of different linguistic features.    *   The mention of specific tools (“KeyBERT”, “MMR”, “spaCy”) implies that these “representation blocks” could correspond to specific algorithms, models, or libraries that serve different functions in text analysis.The specific text elements from Section 1 provide evidence for these insights:    *   “Fine-tune topics with representation blocks”: Directly states the context and the nature of the components.    *   “KeyBERT”, “MMR”, “spaCy”: Provide concrete examples of what these “representation blocks” can be, highlighting the diversity of tools.    *   “Choose a single block” vs. “Or stack multiple blocks”: These phrases clearly delineate the two primary strategies for utilizing the blocks, supporting the insight about choice and customization.    *   The repeated “...” (ellipses) in the placeholder blocks: Explicitly indicates that the listed examples are not exhaustive, thus supporting the conclusion of extensibility and a wide variety of available models.

**Document Context:**
The document context “Section: Adding a Special Lego Block” and “Figure 5-21. After applying the $c$ -TF-IDF weighting, topics can be fine-tuned with a wide variety of representation models, many of which are large language models” explicitly links this image to a step in a larger process: fine-tuning topics after an initial weighting (c-TF-IDF). The image visually illustrates *how* this fine-tuning can be performed using various representation models, conceptualized as modular “Lego blocks.” It supports the idea that once initial topics are identified and weighted, they can be further refined using either a single, powerful model or a combination of models, which can include large language models as mentioned in the text. This flexibility in model choice and combination is a key aspect of advanced topic modeling.

**Summary:**
This image illustrates the process of “fine-tuning topics with representation blocks” using a modular, Lego-like metaphor. It presents two primary strategies for achieving this fine-tuning: selecting a single representation block or stacking multiple blocks together.Initially, the image shows a conceptual stack of different “representation blocks” that can be used. These blocks are labeled “KeyBERT,” “MMR,” and “spaCy,” which are examples of tools or methods relevant to natural language processing and topic modeling. The presence of these three distinct blocks in the initial stack signifies a collection of available components.The first approach, titled “Choose a single block,” demonstrates that fine-tuning can be done by selecting just one specific representation model. For instance, “KeyBERT” is shown as a chosen single block at the top of a stack, with several “...” (ellipses) in the blocks below it, indicating that many other individual blocks could also be chosen. This option suggests a straightforward application of a specialized model.The second approach, labeled “Or stack multiple blocks,” highlights a more complex and potentially powerful method: combining several representation models. In this example, “MMR” is stacked on top of “KeyBERT,” implying that these two models work in conjunction. The “...” (ellipses) below these two explicitly named blocks further convey that more blocks can be added to this stack, allowing for highly customized and multi-layered fine-tuning configurations. This strategy suggests that different models might contribute unique features or processing steps that can be combined for enhanced topic representation.In essence, the diagram visually explains that topic fine-tuning, after an initial weighting process (like c-TF-IDF mentioned in the surrounding text), is a flexible procedure where practitioners can either opt for the simplicity and directness of a single, well-suited representation model or leverage the power of multiple models by combining them, potentially including large language models, to achieve a more nuanced and comprehensive topic representation. The Lego metaphor effectively communicates the modular and customizable nature of this process.](images/c04ae98b4904f231e98f1f805ede28a77db05b039efa79ce47cd8bf7610c3abd.jpg)
Figure 5-21. After applying the $c$ -TF-IDF weighting, topics can be fine-tuned with a wide variety of representation models, many of which are large language models.

Before we explore how we can use these representation blocks, we first need to do two things. First, we are going to save our original topic representations so that it will be much easier to compare with and without representation models:

# Save original representations   
from copy import deepcopy   
original_topics $=$ deepcopy(topic_model.topic_representations_)

Second, let’s create a short wrapper that we can use to quickly visualize the differences in topic words to compare with and without representation models:

def topic_differences(model, original_topics, nr_topics $\mathbf { \Psi } = \mathbf { \Psi }$ ): """Show the differences in topic representations between two models """ df $=$ pd.DataFrame(columns=["Topic", "Original", "Updated"]) for topic in range(nr_topics):

# Extract top 5 words per topic per model   
og_words $=$ " | ".join(list(zip(\*original_topics[topic]))[0][:5]) new_words $=$ " | ".join(list(zip(\*model.get_topic(topic)))[0][:5]) df.loc[len(df)] $=$ [topic, og_words, new_words]

# return df

# KeyBERTInspired

The first representation block that we are going to explore is KeyBERTInspired. KeyBERTInspired is, as you might have guessed, a method inspired by the keyword extraction package, KeyBERT.7 KeyBERT extracts keywords from texts by comparing word and document embeddings through cosine similarity.

BERTopic uses a similar approach. KeyBERTInspired uses c-TF-IDF to extract the most representative documents per topic by calculating the similarity between a document’s c-TF-IDF values and those of the topic they correspond to. As shown in Figure 5-22, the average document embedding per topic is calculated and compared to the embeddings of candidate keywords to rerank the keywords.

![## Image Analysis: 0eb33e3125dd0b63ad4d62ff48056aeecce8cf3e15d9b430b9b62fde964d1944.jpg

**Conceptual Understanding:**
This image conceptually represents a keyword extraction and ranking process inspired by the KeyBERT model. Its main purpose is to illustrate how text embeddings are used to determine the semantic similarity between a document's overall topic and individual candidate keywords. The key ideas communicated are: transforming text into numerical embeddings, aggregating document embeddings to form a topic representation, embedding candidate keywords, and then using cosine similarity to rank these keywords by their relevance to the topic.

**Content Interpretation:**
The image depicts a method for extracting and ranking keywords from a set of documents by leveraging semantic embeddings and cosine similarity. It showcases how a 'topic' is represented by averaging the embeddings of multiple representative documents. This averaged topic embedding is then compared against the embeddings of individual candidate keywords. The result is a 'Reranked keywords' list, where the keywords are ordered based on their semantic proximity to the document's topic. This process ensures that the extracted keywords are not just frequent terms but are also contextually relevant to the core message of the documents. The length of the bars for 'cat', 'cute', and 'that' explicitly signifies their calculated similarity scores, demonstrating the ranking.

**Key Insights:**
The main takeaways are: 1. A document's topic can be represented by averaging the vector embeddings of its constituent sentences or phrases ('Average embedding'). 2. Individual keywords can also be converted into vector embeddings ('Embed keywords'). 3. The semantic relevance of a keyword to a document's topic can be quantified using 'Cosine similarity' between their respective embeddings. 4. Keywords can be 'Reranked keywords' based on these similarity scores, providing a mechanism for extracting the most relevant terms. For instance, 'cat' and 'cute' are identified as more relevant to the topic of "My cat is cute", "That is a cute dog", and "I love pets" than the word 'that', as visually represented by their longer bars in the reranked keywords section.

**Document Context:**
This image, titled "Figure 5-22. KeyBERTInspired representation model procedure," directly supports the document's section on "KeyBERTInspired" models. It provides a visual, step-by-step explanation of how such a model operates, specifically detailing the method for generating and ranking keywords. It visually breaks down the abstract concept of a KeyBERT-inspired model into concrete, understandable stages of document embedding, keyword embedding, similarity calculation, and final keyword ranking, which is crucial for readers to grasp the procedural aspects discussed in the accompanying text.

**Summary:**
This image illustrates the KeyBERT-inspired procedure for generating and ranking keywords based on their semantic similarity to a document's topic. The process begins with two parallel streams: embedding representative documents to form a topic embedding, and embedding individual candidate keywords. For the documents, sentences like "My cat is cute," "That is a cute dog," and "I love pets" are first embedded into a purple grid-like representation. These individual document embeddings are then combined through an "Average embedding" step, resulting in a single purple vector (represented as a horizontal bar). Simultaneously, candidate keywords such as "cat," "that," and "cute" are also embedded into a gray grid-like representation. The next crucial step involves calculating the "Cosine similarity" between the averaged topic embedding (the purple bar) and each of the embedded keywords (the gray grid). This similarity calculation determines how relevant each keyword is to the overall document topic. Finally, the keywords are presented as "Reranked keywords," where their visual length (bars) indicates their similarity score. In the example provided, "cat" has the longest bar, indicating the highest similarity, followed by "cute," and then "that" with the shortest bar, signifying the lowest similarity to the averaged document topic. The equals sign (=) visually connects the cosine similarity output to the reranked keywords.](images/0eb33e3125dd0b63ad4d62ff48056aeecce8cf3e15d9b430b9b62fde964d1944.jpg)
Figure 5-22. KeyBERTInspired representation model procedure.

Due to the modular nature of BERTopic, we can update our initial topic representa‐ tions with KeyBERTInspired without needing to perform the dimensionality reduc‐ tion and clustering steps:

from bertopic.representation import KeyBERTInspired

# Update our topic representations using KeyBERTInspired   
representation_model $=$ KeyBERTInspired()   
topic_model.update_topics(abstracts, representation_model $\cdot ^ { = }$ representation_model)

# Show topic differences topic_differences(topic_model, original_topics)

<table><tr><td>Topic Original</td><td></td><td>Updated</td></tr><tr><td>0</td><td>speech |asr| recognition |end |acoustic</td><td>speech |encoder| phonetic|language |trans...</td></tr><tr><td>1</td><td>medical |clinical biomedical | patient | he..</td><td> nlp |ehr | cinical | biomedical | language</td></tr><tr><td>2</td><td>sentiment |aspect |analysis | reviews|opinion</td><td>aspect |sentiment | aspects |sentiments |cl...</td></tr><tr><td>3</td><td>translation |nmt | machine | neural | bleu</td><td>translation |translating|translate |transl...</td></tr><tr><td>4</td><td></td><td> summarization |summaries |summary | abstract.. summarization |summarizers | summaries |sum..</td></tr></table>

The updated model shows that the topics are easier to read compared to the original model. It also demonstrates the downside of using embedding-based techniques. Words in the original model, like nmt (topic 3), which stands for neural machine translation, are removed as the model could not properly represent the entity. For domain experts, these abbreviations are highly informative.

# Maximal marginal relevance

With c-TF-IDF and the previously shown KeyBERTInspired techniques, we still have significant redundancy in the resulting topic representations. For instance, having both the words “summaries” and “summary” in a topic representation introduces redundancy as they are quite similar.

We can use maximal marginal relevance (MMR) to diversify our topic representa‐ tions. The algorithm attempts to find a set of keywords that are diverse from one another but still relate to the documents they are compared to. It does so by embed‐ ding a set of candidate keywords and iteratively calculating the next best keyword to add. Doing so requires setting a diversity parameter, which indicates how diverse keywords need to be.

In BERTopic, we use MMR to go from a set of initial keywords, let’s say 30, to a smaller but more diverse set of keywords, let’s say 10. It filters out redundant words and only keeps words that contribute something new to the topic representation.

Doing so is rather straightforward:

from bertopic.representation import MaximalMarginalRelevance # Update our topic representations to MaximalMarginalRelevance representation_model $=$ MaximalMarginalRelevance(diversity $= 0 . 2$ ) topic_model.update_topics(abstracts, representation_model $\cdot ^ { = }$ representation_model)

# Show topic differences topic_differences(topic_model, original_topics)

<table><tr><td>Topic Original</td><td></td><td>Updated</td></tr><tr><td>0</td><td>speech |asr|recognition| end|acoustic</td><td>speech | asr | error| model | training</td></tr><tr><td>1</td><td>medical | clinical | biomedical | patient |he..</td><td>clinical | biomedical | patient | healthcare |..</td></tr><tr><td>2</td><td> sentiment |aspect |analysis reviews |opinion</td><td> sentiment |analysis reviews |absa | polarity</td></tr><tr><td>3</td><td>translation | nmt |machine| neural |bleu</td><td>translation | nmt|bleu |parallel | multili..</td></tr><tr><td>4</td><td> summarization |summaries | summary |abstract...</td><td> summarization |document |extractive |rouge .</td></tr></table>

The resulting topics demonstrate more diversity in their representations. For instance, topic 4 only shows one “summary”-like word and instead adds other words that might contribute more to the overall representation.

![## Image Analysis: 2ebbcbd5ba6769fe692b9a756d8e217953907f2a54710156479b9768ba49be77.jpg

**Conceptual Understanding:**
This image conceptually represents a stylized logo or emblem. Its main purpose is likely to serve as a visual identifier or branding mark. The image depicts a green silhouette of an animal, resembling a lemur or a similar primate, positioned within a simple green square outline. There are no other elements, text, or complex visual information that would suggest a deeper conceptual meaning beyond that of a distinct graphic mark.

**Content Interpretation:**
The image exclusively presents a graphical logo. It depicts a stylized green silhouette of an animal, which appears to be a lemur or a similar primate, characterized by its distinctive long, curled tail. The animal is positioned within an open square outline, also rendered in green. There are no processes, concepts, relationships, or systems being shown beyond this singular graphic representation. The significance lies in its visual identity as a logo. All interpretations are based solely on the visual elements of the green animal and square; there is no textual evidence to support any further interpretation, as no text exists in the image.

**Key Insights:**
The image is a simple logo and does not contain any data, diagrams, or textual information from which to extract complex knowledge, patterns, relationships, or insights. The main takeaway is the visual identity of the stylized green animal within a square. No conclusions or lessons can be drawn regarding the document's content or any specific topic differences, as there is no relevant information presented in the image beyond its visual form. The absence of textual evidence means no specific insights can be derived or supported by the image's content.

**Document Context:**
Given the document context section 'Show topic differences topic_differences(topic_model, original_topics)' and the absence of any text or detailed graphical information within the image itself, its relevance is limited to potentially serving as a decorative element, a branding logo, or an identifier for a specific project, tool, or entity within the broader document. Without additional context from the surrounding document, its specific role in explaining 'topic differences' is not discernible from the image alone. It does not visually illustrate any processes, data, or concepts related to topic differences. The image does not provide any information that directly supports or explains 'topic_differences(topic_model, original_topics)'. Its presence is likely for branding or aesthetic purposes.

**Summary:**
The image displays a stylized green silhouette of an animal, likely a lemur or a similar primate, with a long, curled tail. The animal is depicted in a somewhat crouching or active posture, looking towards the right. It is contained within a simple, open green square outline. There is no text present within the image, nor any other discernible details beyond the animal and its bounding box. The image functions purely as a visual logo or emblem.](images/2ebbcbd5ba6769fe692b9a756d8e217953907f2a54710156479b9768ba49be77.jpg)

Both KeyBERTInspired and MMR are amazing techniques for improving the first set of topic representations. KeyBERTInspired especially tends to remove nearly all stop words since it focuses on the semantic relationships between words and documents.

# The Text Generation Lego Block

The representation block in BERTopic has been acting as a reranking block in our previous examples. However, as we already explored in the previous chapter, genera‐ tive models have great potential for a wide variety of tasks.

We can use generative models in BERTopic quite efficiently by following a part of the reranking procedure. Instead of using a generative model to identify the topic of all documents, of which there can potentially be millions, we will use the model to generate a label for our topic. As illustrated in Figure 5-23, instead of generating or reranking keywords, we ask the model to generate a short label based on keywords that were previously generated and a small set of representative documents.

![## Image Analysis: 96245ab4502220e340231197a480152fa77eca532706c4b3aa2f9b10d714c001.jpg

**Conceptual Understanding:**
This image conceptually represents a process of automated topic labeling using a Large Language Model (LLM). Its main purpose is to illustrate how an LLM can be prompted with a combination of textual 'Keywords' and 'Documents' to generate a concise, descriptive 'Label' for a topic. The key idea being communicated is the application of 'prompt engineering' to leverage the generative capabilities of LLMs for specific information extraction and summarization tasks, effectively turning complex input into a simple, human-readable output, such as a topic name.

**Content Interpretation:**
The image depicts an LLM-based system designed for automated topic labeling. It showcases how contextual information, specifically 'Keywords' (e.g., 'attention, text, nlp, transformer') and 'Documents', are used as inputs to construct a prompt for a 'LLM'. The '+' symbol between Keywords and Documents signifies their combined use as input. The large rectangular box explicitly details the 'prompt' structure, instructing the LLM to 'give a short label of the topic' based on the provided '[DOCUMENTS]' and '[KEYWORDS]'. The final output, under 'Label', is a concise description like 'Pre-trained transformer models in NLP'. This illustrates the LLM's capability to synthesize complex information (documents and keywords) into a simple, coherent topic label. The stacked 'LLM' block represents the generative model itself, emphasizing its role as a processing unit for text generation.

**Key Insights:**
The main takeaway is that Large Language Models (LLMs) can effectively generate concise and descriptive topic labels when provided with relevant 'Keywords' and 'Documents'. The diagram emphasizes the crucial role of 'prompt engineering' in guiding the LLM's output. By explicitly structuring the prompt with placeholders for 'documents' and 'keywords' and requesting a 'short label', users can direct the LLM to perform a specific task. The example output, 'Pre-trained transformer models in NLP', demonstrates the LLM's ability to synthesize information from various sources into a meaningful summary. This highlights the LLM as a versatile 'text generation lego block' capable of performing complex summarization and classification tasks.

**Document Context:**
This image directly supports the document's section 'The Text Generation Lego Block' and the accompanying text: 'Figure 5-23. Use text generative LLMs and prompt engineering to create labels for topics from keywords and documents related to each topic.' It serves as a visual example of prompt engineering for topic labeling. The diagram illustrates a practical application of LLMs, demonstrating how a generative model, acting as a 'lego block', can be leveraged for a specific text generation task (labeling) by providing structured input (keywords and documents) via a well-defined prompt. This clarifies the concept of using LLMs for generating new text based on given context and instructions.

**Summary:**
The image illustrates a process for generating a short, descriptive label for a given topic using a Large Language Model (LLM) and prompt engineering. The process begins with two main inputs: 'Keywords' (e.g., 'attention, text, nlp, transformer') and 'Documents'. These inputs are combined to form a detailed prompt that is fed into an LLM, represented by a modular, stacked block. The prompt explicitly states: 'I have a topic that contains the following documents: [DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\nBased on the above information, give a short label of the topic.' The LLM processes this prompt and generates a concise 'Label', as exemplified by 'Pre-trained transformer models in NLP'. This setup demonstrates how an LLM can synthesize information from both keywords and document content to produce a high-level summary or classification, which is a key application in text generation and topic modeling. The stacked LLM block with '...' indicates that the LLM itself is a complex system, likely with multiple layers or components, but the focus here is on its function as a 'lego block' for text generation tasks.](images/96245ab4502220e340231197a480152fa77eca532706c4b3aa2f9b10d714c001.jpg)
Figure 5-23. Use text generative LLMs and prompt engineering to create labels for topics from keywords and documents related to each topic.

There are two components to the illustrated prompt. First, the documents that are inserted using the [DOCUMENTS] tag are a small subset of documents, typically four, that best represent the topic. The documents with the highest cosine similarity of their c-TF-IDF values with those of the topic are selected. Second, the keywords that make up a topic are also passed to the prompt and referenced using the [KEYWORDS] tag. The keywords could be generated by c-TF-IDF or any of the other representa‐ tions we discussed thus far.

As a result, we only need to use the generative model once for every topic, of which there could be potentially hundreds, instead of once for each document, of which there could potentially be millions. There are many generative models that we can choose from, both open source and proprietary. Let’s start with a model that we have explored in the previous chapter, the Flan-T5 model.

We create a prompt that works well with the model and use it in BERTopic through the representation_model parameter:

from transformers import pipeline   
from bertopic.representation import TextGeneration   
prompt $=$ """I have a topic that contains the following documents:   
[DOCUMENTS]   
The topic is described by the following keywords: '[KEYWORDS]'.   
Based on the documents and keywords, what is this topic about?"""   
# Update our topic representations using Flan-T5   
generator $=$ pipeline("text2text-generation", model "google/flan-t5-small") representation_model $=$ TextGeneration(

generator, prompt=prompt, doc_length $\begin{array} { r l } { | = { } } & { { } } \end{array}$ , tokenizer $=$ "whitespace" ) topic_model.update_topics(abstracts, representation_model $\cdot ^ { = }$ representation_model) # Show topic differences topic_differences(topic_model, original_topics)

<table><tr><td colspan="2">Topic 0riginal</td><td>Updated</td></tr><tr><td>0</td><td>speech | asr |recognition | end|acoustic</td><td> Speech-to-description</td></tr><tr><td>1</td><td>medical | clinical | biomedical | patient | he...</td><td>Science/Tech</td></tr><tr><td>2</td><td>sentiment | aspect | analysis | reviews | opinion</td><td>Review</td></tr><tr><td>3</td><td>translation | nmt |machine| neural | bleu</td><td>Attention-based neural machine translation</td></tr><tr><td>4</td><td>summarization | summaries |summary|abstract.. Summarization</td><td></td></tr></table>

Some of these labels, like “Summarization” seem to be logical when comparing them to the original representations. Others, however, like “Science/Tech,” seem quite broad and do not do the original topic justice. Let’s explore instead how OpenAI’s GPT-3.5 would perform considering the model is not only larger but expected to have more linguistic capabilities:

import openai   
from bertopic.representation import OpenAI   
prompt = """   
I have a topic that contains the following documents:   
[DOCUMENTS]

The topic is described by the following keywords: [KEYWORDS]

Based on the information above, extract a short topic label in the following   
format:   
topic: <short topic label>   
# Update our topic representations using GPT-3.5   
client $=$ openai.OpenAI(api_key="YOUR_KEY_HERE")   
representation_model $=$ OpenAI( client, model $\ l =$ "gpt-3.5-turbo", exponential_backoff=True, chat=True,   
prompt=prompt   
)   
topic_model.update_topics(abstracts, representation_model $\ l =$ representation_mode

# Show topic differences topic_differences(topic_model, original_topics)

<table><tr><td>Topic 0riginal</td><td></td><td>Updated</td></tr><tr><td>0</td><td> speech |asr recognition |end|acoustic</td><td>Leveraging External Data for Improving Low-Res...</td></tr><tr><td>1</td><td>medical | clinical | biomedical | patient |he...</td><td>Improved Representation Learning for Biomedica...</td></tr><tr><td>2</td><td> sentiment |aspect |analysis |reviews |opinion</td><td>Advancements in Aspect-Based Sentiment Analys..</td></tr><tr><td>3</td><td>translation |nmt | machine|neural | bleu</td><td>Neural Machine Translation Enhancements</td></tr><tr><td>4</td><td>summarization|summaries | summary |abstract.. Document Summarization Techniques</td><td></td></tr></table>

The resulting labels are quite impressive! We are not even using GPT-4 and the resulting labels seem to be more informative than our previous example. Note that BERTopic is not confined to only using OpenAI’s offering but has local backends as well.

![## Image Analysis: 65b686226626e0527345c0a007b3babc5b1eebecd2334c5e2a5772586a084350.jpg

**Conceptual Understanding:**
This image conceptually represents a stylized animal figure, specifically a lemur or similar primate, rendered as a solid green silhouette within a square frame. Its main purpose, without any accompanying text or document context, is likely to serve as a visual identifier, a logo, or a decorative element. It does not convey a specific message, idea, or concept related to data, processes, or information beyond the visual recognition of the animal form and its graphic presentation.

**Content Interpretation:**
The image is a static, graphic representation of an animal silhouette. It does not depict any processes, concepts, relationships, or systems in an explanatory manner. Its significance is purely visual and potentially symbolic, possibly representing a brand, project, or an abstract concept, but without any accompanying text or further context, its specific meaning cannot be definitively interpreted beyond being a stylized animal figure. There is no data, trends, or information presented. As there is no text in the image, no extracted text elements support any interpretations beyond the visual description.

**Key Insights:**
Without any text, data, or contextual elements within the image itself, there are no specific takeaways, lessons, conclusions, or insights that can be extracted directly from this image. The image is a simple visual identifier or decoration. Its primary function, if any, would be to serve as a brand mark, an icon, or an aesthetic element within the document. No specific text elements are present to provide evidence for any insights.

**Document Context:**
Given the document context 'Section: Show topic differences topic_differences(topic_model, original_topics)', the image appears to be a decorative graphic or potentially a logo associated with a tool, organization, or concept related to topic modeling or analysis. It does not visually represent 'topic differences' or any computational process. Its relevance would be as an identifier or a visual break rather than conveying information directly pertinent to the technical analysis described in the section title. It does not contribute to understanding any process or micro-details mentioned in the section heading, as it is a standalone graphic with no intrinsic informational content related to the topic.

**Summary:**
The image displays a stylized, solid green silhouette of an animal, which appears to be a lemur or a similar primate. The animal is depicted in profile, walking or crouching, with its front legs bent and its hind legs extended. Its long, slender tail curls upwards and then spirals into a distinct coil above its back. The entire figure is contained within a simple, unadorned square frame. There is no text, annotations, or any other visual elements within or around the silhouette or the frame. The image serves as a standalone graphic, likely functioning as a logo, icon, or a decorative element.](images/65b686226626e0527345c0a007b3babc5b1eebecd2334c5e2a5772586a084350.jpg)

Although it seems like we do not need the keywords anymore, they are still representative of the input documents. No model is perfect and it is generally advised to generate multiple topic representa‐ tions. BERTopic allows for all topics to be represented by different representations. You could, for example, use KeyBERTInspired, MMR, and GPT-3.5 side by side to get different perspectives on the same topic.

With these GPT-3.5 generated labels, we can create beautiful illustrations using the datamapplot package (Figure 5-24):

# Visualize topics and documents   
fig $=$ topic_model.visualize_document_datamap( titles, topics=list(range(20)), reduced_embeddings $\mathbf { \sigma } =$ reduced_embeddings, width=1200, label_font_size=11, label_wrap_width $\begin{array} { r l } { | = { } } & { { } } \end{array}$ , use_medoids=True,   
)

![## Image Analysis: a319a7aaeb41ed90734be69259234e87bae8505b114e710c055514f011639df2.jpg

**Conceptual Understanding:**
This image conceptually represents a topical landscape or a thematic map derived from a collection of documents. It illustrates the distribution and relationships between twenty identified topics within a specific domain, likely related to Natural Language Processing (NLP) and AI. The main purpose of this visualization is to provide an intuitive overview of the dominant themes present in a corpus of textual data, allowing users to quickly identify key research areas, their individual foci, and their potential connections or disconnections in the thematic space. It aims to enhance document comprehension by making abstract topical information visually accessible and interpretable.

**Content Interpretation:**
The image visualizes twenty distinct topics (labeled Topic-0 through Topic-19) as clusters in a two-dimensional space. Each cluster represents a specific area of research or content, and its position relative to other clusters may indicate thematic similarity or distinction. The grey background points likely represent individual documents or data points from which these topics were derived, demonstrating the underlying data structure. The content reveals a strong focus on Natural Language Processing (NLP) and related fields, including machine translation, named entity recognition, sentiment analysis, language model evaluation, and specific applications like hate speech detection and biomedical text analysis. The emphasis on 'Topic-4: Document Summarization Techniques' suggests it could be a foundational or particularly significant area within the visualized domain. The titles of the topics provide insight into current research trends and challenges in AI and NLP, such as efficiency, robustness, bias, and domain-specific applications. The visualization aims to make the abstract concept of topic distribution tangible, allowing for a quick grasp of the breadth and depth of the analyzed content.

**Key Insights:**
The main takeaway from this image is the diverse and interconnected nature of research topics within the domain being analyzed, primarily focusing on Natural Language Processing (NLP) and Artificial Intelligence (AI). The visualization highlights both broad areas like 'Topic-4: Document Summarization Techniques' and more specific, technical advancements such as 'Topic-12: Efficient Neural Dependency Parsing with Improved Performance' and 'Topic-15: Parameter-efficient fine-tuning methods for downstream tasks in specific domains.' The presence of topics like 'Topic-5: Advances in Hate Speech Detection and Analysis' and 'Topic-6: Gender Bias in AI Language Models' underscores the societal and ethical considerations within AI research. The proximity of certain clusters suggests potential relationships or overlap between research areas, for instance, various aspects of machine translation (Topic-3, Topic-14) or different types of language models (Topic-9, Topic-11, Topic-18). The visualization demonstrates a comprehensive landscape of ongoing research and challenges within the field. The specific textual evidence for these insights comes from the complete transcription of all topic labels, which individually describe the focus of each research cluster.

**Document Context:**
This image directly supports the document's section titled 'Visualize topics and documents' by providing a concrete example of how topics and their underlying documents can be visually represented. As stated in the text after the image, 'Figure 5-24. The top 20 topics visualized,' this image serves as Figure 5-24, showcasing the output of a topic modeling or clustering process. It helps the reader understand the distribution and interrelation of major themes within a corpus of documents, thus enhancing comprehension of the methods used for topic visualization described in the surrounding text.

**Summary:**
The image displays a scatter plot visualization of twenty distinct topics, labeled from Topic-0 to Topic-19, likely representing clusters of research papers or documents. Each topic is depicted as a colored cluster of densely packed points, connected by a line to its descriptive textual label. The background consists of numerous smaller, grey points, representing individual documents that are not specifically highlighted as part of a major topic cluster. The overall layout suggests a spatial distribution of these topics, where proximity might imply thematic relatedness, although no explicit axes or distance metrics are provided. The prominent label 'Topic-4: Document Summarization Techniques' is positioned in the lower-left, appearing slightly larger and bolder than other topic labels, which may indicate a central or overarching theme, or simply a visual emphasis. The image serves to provide a visual overview of various research themes within a specific domain, enhancing the understanding of how different areas interrelate and are distributed.](images/a319a7aaeb41ed90734be69259234e87bae8505b114e710c055514f011639df2.jpg)
Figure 5-24. The top 20 topics visualized.

# Summary

In this chapter, we explored how LLMs, both generative and representative, can be used in the domain of unsupervised learning. Despite supervised methods like classification being prevalent in recent years, unsupervised approaches such as text clustering hold immense potential due to their ability to group texts based on seman‐ tic content without prior labeling.

We covered a common pipeline for clustering textual documents that starts with converting input text into numerical representations, which we call embeddings. Then, dimensionality reduction is applied to these embeddings to simplify highdimensional data for better clustering outcomes. Finally, a clustering algorithm on the dimensionality-reduced embeddings is applied to cluster the input text. Manually inspecting the clusters helped us understand which documents they contained and how to interpret these clusters.

To transition away from this manual inspection, we explored how BERTopic extends this text clustering pipeline with a method for automatically representing the clusters. This methodology is often referred to as topic modeling, which attempts to uncover themes within large amounts of documents. BERTopic generates these topic repre‐ sentations through a bag-of-words approach enhanced with c-TF-IDF, which weighs words based on their cluster relevance and frequency across all clusters.

A major benefit of BERTopic is its modular nature. In BERTopic, you can choose any model in the pipeline, which allows for additional representations of topics that create multiple perspectives of the same topic. We explored maximal marginal rele‐ vance and KeyBERTInspired as methodologies to fine-tune the topic representations generated with c-TF-IDF. Additionally, we used the same generative LLMs as in the previous chapter (Flan-T5 and GPT-3.5) to further improve the interpretability of topics by generating highly interpretable labels.

In the next chapter, we shift focus and explore a common method for improving the output of generative models, namely prompt engineering.

# Prompt Engineering

In the first chapters of this book, we took our first steps into the world of large language models (LLMs). We delved into various applications, such as supervised and unsupervised classification, employing models that focus on representing text, like BERT and its derivatives.

As we progressed, we used models trained primarily for text generation, models that are often referred to as generative pre-trained transformers (GPT). These models have the remarkable ability to generate text in response to prompts from the user. Through prompt engineering, we can design these prompts in a way that enhances the quality of the generated text.

In this chapter, we will explore these generative models in more detail and dive into the realm of prompt engineering, reasoning with generative models, verification, and even evaluating their output.

# Using Text Generation Models

Before we start with the fundamentals of prompt engineering, it is essential to explore the basics of utilizing a text generation model. How do we select the model to use? Do we use a proprietary or open source model? How can we control the generated output? These questions will serve as our stepping stones into using text generation models.

# Choosing a Text Generation Model

Choosing a text generation model starts with choosing between proprietary models or open source models. Although proprietary models are generally more performant, we focus in this book more on open source models as they offer more flexibility and are free to use.

![## Image Analysis: 7381ea2ca06ae0a7813becdd70154f919efe68360dbec3027feef73f3cb72034.jpg

**Conceptual Understanding:**
This image conceptually represents the landscape of significant foundation models, specifically large language models (LLMs), by illustrating their diversity in terms of available parameter sizes and their approximate chronological release order. The main purpose of the image is to demonstrate that these powerful models are not singular entities but are often offered in multiple configurations (sizes), providing flexibility for different application requirements and computational budgets. It communicates the key idea that 'model size,' typically measured in billions of parameters, is a fundamental characteristic differentiating versions of the same model and distinct models from one another, and that new models are continually being developed and released. This also highlights the evolution and breadth of the foundation model ecosystem.

**Content Interpretation:**
The image displays a comparison of several prominent foundation models, specifically large language models (LLMs), by their names, available parameter sizes, and approximate release order. Each model, identified by name (LLama, StableLM, Falcon, LLama2, Mistral), is visually represented by a collection of concentric circles. The varying sizes of these circles directly correspond to the different parameter counts ('Model size') for each respective model. For example, 'Falcon' has 7B, 40B, and 180B parameter versions, illustrated by three green circles of increasing size. The horizontal axis, labeled 'Release date', suggests a chronological progression from left to right, showcasing the introduction of these models over time. The significance is that foundation models are not monolithic but often come in a 'family' of sizes to cater to different computational resources and application needs.

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Variety in Model Sizes:** Foundation models are not single entities but are often released in multiple versions with differing parameter counts (e.g., LLama has 7B, 13B, 33B, 70B parameters). This offers flexibility for developers to choose a model that balances performance and computational resources.
2.  **Chronological Evolution:** The arrangement along the 'Release date' axis suggests an ongoing development and release of new foundation models (LLama -> StableLM -> Falcon -> LLama2 -> Mistral), highlighting the dynamic nature of the field.
3.  **Model Naming Conventions:** Models often have distinct names (e.g., Falcon, Mistral) or versioning (e.g., LLama vs. LLama2) indicating different iterations or families.
4.  **Parameter Count as a Key Metric:** The 'B' suffix (e.g., 7B, 180B) clearly denotes billions of parameters, which is a critical indicator of a model's complexity and capability, and is explicitly visually represented by the 'Model size' annotation.

**Conclusions/Insights:**
*   The explicit text "7B/13B/33B/70B" for LLama, "3B/7B" for StableLM, "7B/40B/180B" for Falcon, "7B/13B70B" for LLama2, and "7B" for Mistral provides direct textual evidence for the variety of model sizes. The annotation "Model size" pointing to the concentric circles further confirms this visual representation.
*   The horizontal arrangement of models from left to right, along with the "Release date" arrow, supports the insight into the chronological introduction of these models.
*   The distinct names like "LLama", "StableLM", "Falcon", "LLama2", and "Mistral" are the textual evidence for different foundation model identities. The repetition of "LLama" and introduction of "LLama2" shows versioning within a model family.
*   The use of "B" after numbers like "7B" or "180B" consistently across all models indicates that parameter count is a standardized and important metric for describing these models.

**Document Context:**
This image directly supports the document's section on "Choosing a Text Generation Model" by illustrating the concept that foundation models, specifically LLMs, are typically available in various sizes. The text after the image, "Figure 6-1 shows a small selection of impactful foundation models, LLMs that have been pretrained on vast amounts of text data and are often fine-tuned for specific applications. Figure 6-1. Foundation models are often released in several different sizes.", explicitly refers to this figure and reinforces its main message. The visual breakdown of model names and their associated parameter counts provides concrete examples of the model diversity a user might encounter when making a selection, making the abstract concept of "different sizes" tangible and aiding in the practical decision-making process for choosing an appropriate text generation model.

**Summary:**
The image is a horizontal chart titled "Foundation models" that illustrates five different large language models (LLMs) and their various available sizes, arranged approximately by their release date. Each model is represented by a set of concentric circles, where the size of the circles visually corresponds to the model's parameter count (Model size). A dotted horizontal line forms a timeline, with an arrow on the far right pointing towards "Release date", indicating that models are ordered from left to right chronologically. 

Starting from the left:
1.  **LLama:** Represented by four concentric purple circles, indicating four different sizes. The text beneath it specifies these sizes as "7B/13B/33B/70B".
2.  **StableLM:** Represented by two concentric blue circles. The text below indicates sizes "3B/7B".
3.  **Falcon:** Represented by three concentric green circles. Its sizes are listed as "7B/40B/180B".
4.  **LLama2:** Represented by three concentric red circles. The sizes are given as "7B/13B70B". An arrow points from the text "Model size" to the largest red circle, explicitly labeling what the circles' varying dimensions signify.
5.  **Mistral:** Represented by a single orange circle. The specified size is "7B".

The visual layout clearly demonstrates that foundation models are often released in multiple versions, differing in their parameter counts, which directly impacts their computational requirements and capabilities.](images/7381ea2ca06ae0a7813becdd70154f919efe68360dbec3027feef73f3cb72034.jpg)
Figure 6-1 shows a small selection of impactful foundation models, LLMs that have been pretrained on vast amounts of text data and are often fine-tuned for specific applications.   
Figure 6-1. Foundation models are often released in several different sizes.

From those foundation models, hundreds if not thousands of models have been fine-tuned, one more suitable for certain tasks than another. Choosing the model to use can be a daunting task!

We advise starting with a small foundation model. So let’s continue using Phi-3-mini, which has 3.8 billion parameters. This makes it suitable for running with devices up to 8 GB of VRAM. Overall, scaling up to larger models tends to be a nicer experience than scaling down. Smaller models provide a great introduction and lay a solid foundation for progressing to larger models.

# Loading a Text Generation Model

The most straightforward method of loading a model, as we have done in previous chapters, is by leveraging the transformers library:

import torch   
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline   
# Load model and tokenizer   
model $=$ AutoModelForCausalLM.from_pretrained( "microsoft/Phi-3-mini-4k-instruct", device_map="cuda", torch_dtype="auto", trust_remote_code=True,   
)   
tokenizer $=$ AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-4k-instruct")   
# Create a pipeline   
pipe $=$ pipeline( "text-generation", model=model,

tokenizer $=$ tokenizer, return_full_text $=$ False, max_new_tokens $\begin{array} { r l } { \mathbf { \Psi } } & { { } = } \end{array} .$ , do_sample=False, )

Compared to previous chapters, we will take a closer look at developing and using the prompt template.

To illustrate, let’s revisit the example from Chapter 1 where we asked the LLM to make a joke about chickens:

# Prompt   
messages $=$ [ {"role": "user" "content": "Create a funny joke about chickens."}   
]

# Generate the output output $=$ pipe(messages) print(output[0]["generated_text"])

Why don't chickens like to go to the gym? Because they can't crack the eggsistence of it!

Under the hood, transformers.pipeline first converts our messages into a specific prompt template. We can explore this process by accessing the underlying tokenizer:

# Apply prompt template   
prompt $=$ pipe.tokenizer.apply_chat_template(messages, tokenize=False)   
print(prompt)   
<s><|user|>   
Create a funny joke about chickens.<|end|>   
<|assistant|>

You may recognize the special tokens <|user $| >$ and $<$ |assistant $| >$ from Chapter 2. This prompt template, further illustrated in Figure 6-2, was used during the training of the model. Not only does it provide information about who said what, but it is also used to indicate when the model should stop generating text (see the $< | \mathsf { e n d } | >$ token). This prompt is passed directly to the LLM and processed all at once.

In the next chapter, we will customize parts of this template ourselves. Throughout this chapter, we can use transformers.pipeline to handle chat template processing for us. Next, let us explore how we can control the output of the model.

![## Image Analysis: b06da65385db7c21e67da812ce8fea23bdf71ceafe8e64fdfdc789b370c379ee.jpg

**Conceptual Understanding:**
This image conceptually represents the structured format or "template" that the Phi-3 language model expects for processing inputs and generating outputs. Its main purpose is to illustrate the specific sequence of special tokens and content that define the boundaries of user prompts and model responses within a conversational context. It communicates the key idea that for effective interaction with Phi-3, inputs (prompts) and outputs (responses) must adhere to a predefined token-based structure.

**Content Interpretation:**
The image demonstrates the "Phi-3 template," which is a specific protocol for structuring conversational turns between a user and the Phi-3 language model. The interaction is initiated by the `<s` token, explicitly labeled as "Beginning of sentence (BOS) token", indicating a system-level start. The user's input begins with the `<|user|>` token ("Start of prompt"), followed by the actual query, "What is 1 + 1" ("Prompt"), and concludes with the `<|end|>` token ("End of prompt"), clearly delineating the user's contribution. Correspondingly, the model's response begins with the `<|assistant|>` token ("Start of output"), provides its answer, "The answer to 1+1 is 2!" ("Output"), and finishes with the `<|end|>` token ("End of output"). Dashed vertical lines explicitly label segments as "User" and "Model," assigning responsibility for each part. The various distinct tokens (`<s`, `<|user|>`, `<|end|>`, `<|assistant|>`) are crucial for the model to correctly parse and interpret the conversation flow, acting as delimiters and role indicators. The example prompt and response provide concrete evidence of how actual conversational content fits within these token wrappers.

**Key Insights:**
Key Takeaway 1: Structured Interaction is Mandatory. The Phi-3 model requires a specific, token-based template for effective interaction, not just raw text, as evidenced by the explicit labeling of each token and its purpose, such as `<|user|>` for "Start of prompt". Key Takeaway 2: Clear Delineation of Roles and Turns. The template clearly distinguishes between user input and model output using distinct tokens like `<|user|>` and `<|assistant|>`, explicitly labeled with "User" and "Model" sections, ensuring the model understands who is speaking. Key Takeaway 3: Importance of Special Tokens. Special tokens such as `<s` (Beginning of sentence (BOS) token), `<|user|>`, `<|end|>`, and `<|assistant|>` are essential for the model to correctly process conversational context and segment, with the diagram explicitly assigning meaning to each. Key Takeaway 4: Example of Conversational Flow. The inclusion of "What is 1 + 1" as a "Prompt" and "The answer to 1+1 is 2!" as an "Output" provides a practical example of how a simple question-answer exchange would be formatted according to the Phi-3 template.

**Document Context:**
This image, titled "Phi-3 template," is highly relevant to the "Apply prompt template" section of the document, as indicated by its placement as "Figure 6-2." It serves as a visual guide to the exact format expected when interacting with the Phi-3 model, detailing how a user's prompt and the model's response should be structured using specific tokens.

**Summary:**
The image displays the "Phi-3 template," which is a structured format for conducting a conversation with the Phi-3 language model. It outlines how both user input (prompts) and model output are encapsulated by special tokens to ensure correct interpretation by the model. The entire template is contained within a gray rectangular boundary. The interaction begins with a white rounded rectangle containing `<s`, which is identified as the "Beginning of sentence (BOS) token". Following this, the "User" segment of the interaction is defined: a yellow rounded rectangle with `<|user|>` signifies the "Start of prompt"; next, a green rounded rectangle contains the actual user's query, "What is 1 + 1", which is labeled as the "Prompt"; the user's prompt concludes with a light orange rounded rectangle containing `<|end|>`, marking the "End of prompt". Subsequently, the "Model" segment of the interaction is defined: a purple rounded rectangle containing `<|assistant|>` indicates the "Start of output" from the model; this is followed by a pink rounded rectangle with the model's generated response, "The answer to 1+1 is 2!", which is labeled as "Output"; the model's output segment terminates with another light orange rounded rectangle containing `<|end|>`, representing the "End of output". In essence, the template requires a sequence of specific tokens to delimit the beginning of a sentence, the start and end of a user's turn, and the start and end of the model's response, guiding the model through the conversational turns.](images/b06da65385db7c21e67da812ce8fea23bdf71ceafe8e64fdfdc789b370c379ee.jpg)
Figure 6-2. The template Phi-3 expects when interacting with the model.

# Controlling Model Output

Other than prompt engineering, we can control the kind of output we want by adjusting the model parameters. In our previous example, you might have noticed that we used several parameters in the pipe function, including temperature and top_p.

These parameters control the randomness of the output. A part of what makes LLMs exciting technology is that it can generate different responses for the exact same prompt. Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token.

As illustrated in Figure 6-3, in the sentence $^ { \mathfrak { c } } \mathrm { I }$ am driving a…” the likelihood of that sentence being followed by tokens like “car” or “truck” is generally higher than a token like “elephant.” However, there is still a possibility of “elephant” being generated but it is much lower.

![## Image Analysis: a41521dc37eafd73056865f9d1d392c5870300f28d3508e28348d3a93827b354.jpg

**Conceptual Understanding:**
This image conceptually represents the core mechanism of **next token prediction** within a language model. It illustrates how a model, given a sequence of input tokens, determines the most probable next token to append to that sequence.

**Main purpose or message:** The primary purpose of this image is to convey that language models generate text by considering multiple possible next words (or tokens) and assigning each a likelihood score. The model then selects the next token based on these probabilities, favoring those with higher likelihoods to ensure coherence and contextual relevance. It demystifies the probabilistic nature of text generation.

**Key ideas or concepts being communicated:**
1.  **Sequential Token Generation:** Text is built up token by token.
2.  **Probabilistic Output:** The model doesn't just guess; it calculates the probability of various possible next tokens.
3.  **Likelihood Scoring:** Each potential next token is assigned a quantitative score reflecting its probability or contextual fit.
4.  **Context Dependency:** The likelihood of a token is highly dependent on the preceding words (the input context).

**Content Interpretation:**
The image illustrates the core mechanism of next token prediction in a language model. It shows that given a partial input, the model generates a set of candidate words, each assigned a probabilistic likelihood score.

**Processes, concepts, relationships, or systems shown:**
*   **Input Context:** The sequence "I am driving a ?" represents the input provided to the language model, with the "?" marking the position for the next token prediction.
*   **Next Token Prediction:** The arrow signifies the computational process where the model takes the input and predicts potential continuations.
*   **Probabilistic Output Distribution:** The pink/red box listing tokens like "... car", "... truck", "... elephant" demonstrates that the model doesn't just produce one word, but a distribution of possible next tokens. The ellipses emphasize that this is a sample from a larger set of possibilities.
*   **Likelihood Scoring:** The vertical red gradient bar, explicitly labeled "High likelihood" and "Low likelihood", directly represents the probability or score assigned to each potential next token. Tokens positioned higher on the list (and visually in a darker red if shown in gradient) have a higher chance of being selected.

**Significance of data (likelihood scores):** The likelihood scores are crucial because they quantify the model's confidence in each possible next word. For the context "I am driving a", words like "car" and "truck" are shown to have a significantly higher likelihood than "elephant", which would typically be an inappropriate continuation. This scoring mechanism is central to the model's ability to generate text that is grammatically correct and semantically coherent within a given context.

**Support from extracted text elements:**
*   The input sequence "I am driving a ?" precisely sets the scene for a single token prediction task.
*   The output list "... car", "... truck", "... elephant" provides concrete examples of the model's candidate predictions.
*   The connecting arrow visually depicts the transition from input to the generated output distribution.
*   The explicit labels "High likelihood" and "Low likelihood" directly define the meaning of the red gradient scale, which applies to the candidate tokens, reinforcing the idea of probabilistic selection. The pink/red coloring of the output box also ties it directly to this likelihood gradient.

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Token-by-Token Generation:** Language models generate text sequentially, token by token, rather than producing entire sentences or paragraphs at once. This is evidenced by the single '?' token awaiting prediction after a sequence of input tokens.
2.  **Probabilistic Prediction:** For each prediction step, the model considers a multitude of possible next tokens and assigns a distinct likelihood score to each. This is shown by the list of diverse words in the output box (e.g., "car", "truck", "elephant") accompanied by the explicit "High likelihood" to "Low likelihood" scale.
3.  **Contextual Influence:** The likelihood scores are heavily influenced by the preceding context. In the example "I am driving a ?", words like "car" and "truck" are given higher likelihood, while "elephant" is given lower likelihood, demonstrating the model's ability to assess contextual relevance.
4.  **Basis for Output Control:** Understanding these likelihood scores is fundamental to controlling model output, as the model's ultimate choice for the next token is derived from these probabilities.

**Conclusions/Insights:**
*   The process depicted is a foundational element of how generative AI systems produce coherent human-like text. It highlights that the apparent creativity of these models is underpinned by a systematic, probabilistic selection process.
*   The image reveals that language models do not simply 'guess' the next word; they perform a sophisticated statistical analysis of numerous possibilities to identify the most probable continuations. This insight is crucial for grasping both the capabilities and limitations of such models.
*   The visual representation makes it clear that influencing the choice of the next token, or steering the model's output, would involve manipulating or biasing these underlying likelihood distributions.

**Evidence from Text Elements:**
*   The input sequence "I am driving a ?" establishes the specific context for a single next-token prediction.
*   The output list (e.g., "... car", "... truck", "... elephant") provides concrete examples of the model's multiple potential predictions.
*   The clear labels "High likelihood" and "Low likelihood" directly demonstrate that the model quantifies the probability of each candidate token.
*   The contrast in expectedness between "car"/"truck" versus "elephant" in the given context implicitly showcases the model's ability to assess contextual fit and assign appropriate probabilities, which is a key lesson in how these systems function.

**Document Context:**
This image is directly relevant to the "Controlling Model Output" section of the document, as it visually explains the fundamental mechanism by which a language model's output is generated: the selection of the next token based on its likelihood. The accompanying text, "Figure 6-3. The model chooses the next token to generate based on their likelihood scores," serves as a precise caption and summary, reinforcing the image's core message. By illustrating how tokens are probabilistically scored, the image provides the necessary groundwork for understanding how one might intervene in or influence this process to 'control' the model's generated text. It demystifies the generative aspect, showing that output isn't arbitrary but a reasoned, likelihood-based choice.

**Summary:**
The image, implicitly titled "Figure 6-3. The model chooses the next token to generate based on their likelihood scores," illustrates the fundamental process of next token prediction by a language model. It shows an input sequence leading to a list of potential next tokens, each associated with a likelihood score.

The process begins with a sequence of five distinct tokens, presented in individual gray boxes from left to right: "I", "am", "driving", "a", and a pink-tinted box containing a question mark "?". This question mark specifically denotes the position where the language model needs to predict the next token.

An arrow points from this input sequence (specifically from the pink "?" box) to a large, pink-tinttinted rectangular box on the right. This box represents the model's output, which is a list of candidate words or tokens that could plausibly follow the input. The visible tokens in this list are:
*   ... car
*   ... truck
*   ...
*   ...
*   ... elephant
*   ...
The ellipses before and after the listed words indicate that this is a truncated representation of a larger, more comprehensive set of possible next tokens generated by the model.

Adjacent to this output list, on its right, is a vertical, red-gradient bar that functions as a likelihood scale. The top of this bar is explicitly labeled "High likelihood" and is colored in a darker, more intense red. The bottom of the bar is labeled "Low likelihood" and is depicted in a lighter red. This scale visually and textually indicates that the tokens at the top of the output list (like "car" and "truck") are assigned a higher probability of being the correct or most fitting next word, while tokens further down the list (like "elephant") have a lower probability.

In summary, the image details how a language model, given a preceding text context, generates a range of possible subsequent words and assigns a likelihood or probability score to each. The model's eventual selection of the next token is governed by these likelihood scores, favoring the options with higher probabilities to ensure coherent and contextually relevant text generation.](images/a41521dc37eafd73056865f9d1d392c5870300f28d3508e28348d3a93827b354.jpg)
Figure 6-3. The model chooses the next token to generate based on their likelihood scores.

When we loaded our model, we purposefully set do_sample $\mathsf { \Pi } = \mathsf { F }$ alse to make sure the output is somewhat consistent. This means that no sampling will be done and only the most probable next token is selected. However, to use the temperature and top_p parameters, we will set do_sample=True in order to make use of them.

# Temperature

The temperature controls the randomness or creativity of the text generated. It defines how likely it is to choose tokens that are less probable. The underlying idea is that a temperature of 0 generates the same response every time because it always chooses the most likely word. As illustrated in Figure 6-4, a higher value allows less probable words to be generated.

![## Image Analysis: e2efe06f62f4af368f26d7831668fd95dad3fda0682fc49d7a9919fab0f47843.jpg

**Conceptual Understanding:**
This image conceptually represents the impact of a parameter, explicitly labeled "temperature," on the probability distribution of potential outcomes, referred to here as "tokens" (e.g., "car," "truck," "elephant").

The main purpose of the image is to visually demonstrate how varying the "temperature" setting influences the likelihood of generating different tokens. Specifically, it illustrates that a "Low" temperature results in a highly concentrated probability distribution, heavily favoring a few most probable tokens, while a "High" temperature leads to a more uniform or flattened distribution, increasing the likelihood of less probable tokens and reducing the dominance of the most probable ones.

The key idea being communicated is the concept of "temperature" as a mechanism to control the randomness or "creativity" in a generative model by adjusting the entropy of the output probability distribution.

**Complete Verbatim Transcription:**

**A. PROCESS FLOW TRANSCRIPTION:**
*   This image is a comparative bar chart illustrating probability distributions, not a process flow or flowchart. Therefore, this section is not applicable in the typical sense of numbered steps and decision diamonds.

**B. ANNOTATIONS AND METADATA TRANSCRIPTION:**
*   **Main Axis Label:** temperature
*   **Axis Point 1:** Low (with an arrow pointing left)
*   **Axis Point 2:** High (with an arrow pointing right)
*   **Left Bar Chart (under 'Low'):**
    *   Label 1: car
    *   Label 2: truck
    *   Label 3: ...
    *   Label 4: ...
    *   Label 5: elephant
    *   Label 6: ...
    *   Bottom Label: probabilities
*   **Right Bar Chart (under 'High'):**
    *   Label 1: car
    *   Label 2: truck
    *   Label 3: ...
    *   Label 4: ...
    *   Label 5: elephant
    *   Label 6: ...
    *   Bottom Label: probabilities
*   **No other annotations, notes, arrow labels (beyond axis arrows), timeline information, headers, or footers are present.**

**Systematic Process Mapping (Based on Transcription):**

As noted in Section 1, this image is a comparative bar chart, not a process flow diagram. Therefore, a "systematic process mapping" of steps, decision points, and branching options is not applicable. Instead, the image presents two distinct scenarios: "Low temperature" and "High temperature," each showing a distribution of probabilities for various tokens.

The comparison is as follows:
1.  **Start with the concept of "temperature."**
2.  **Consider "Low temperature":** This scenario presents a probability distribution where specific tokens, such as "car" and "truck," have relatively high probabilities (represented by longer bars). Many other tokens, including "elephant" and those represented by "...", have significantly lower, almost negligible, probabilities (represented by much shorter bars). The distribution is sharply peaked, heavily favoring a few tokens.
3.  **Consider "High temperature":** This scenario presents a probability distribution where the probabilities are more spread out and less differentiated among tokens. Tokens like "car" and "truck" still have relatively higher probabilities, but their bars are shorter than in the "Low temperature" case, indicating a reduced probability for these most likely tokens. Conversely, tokens like "elephant" and those represented by "..." have noticeably increased probabilities, with their bars being longer than in the "Low temperature" case. The distribution is much flatter, allowing less probable tokens a higher chance of being generated.
4.  **The end result** is a visual demonstration of how "temperature" acts as a parameter to smooth or sharpen the probability distribution of tokens.

**Content Interpretation:**
The image illustrates the concept of sampling temperature, a parameter often used in language models or other generative AI systems. It shows how this parameter affects the probability distribution of "tokens" (discrete outputs like words or concepts).

*   **Processes/Concepts Shown:**
    *   **Probability Distribution:** The set of vertical bars under both "Low" and "High" temperature, explicitly labeled "probabilities," graphically represent the likelihood of generating each specific token.
    *   **Token Generation:** While not explicitly a "process flow," the image implies the output of a system that generates tokens, and its internal mechanism involves assigning probabilities to these tokens.
    *   **Temperature Parameter:** The horizontal axis labeled "temperature" with "Low" and "High" extremes clearly indicates that "temperature" is a controllable parameter that influences the output.

*   **Significance of Data/Trends:**
    *   **Low Temperature Scenario:** Under "Low" temperature, tokens like "car" and "truck" exhibit significantly longer bars, indicating much higher probabilities compared to tokens like "elephant" and the "..." items. This signifies a **sharper, more deterministic distribution** where the model is highly likely to produce the most probable tokens. The distribution is "peaky."
    *   **High Temperature Scenario:** Under "High" temperature, the bars for "car" and "truck" are noticeably shorter than in the "Low" temperature case, meaning their individual probabilities have decreased. Conversely, the bars for "elephant" and the "..." items are longer, indicating their probabilities have increased. This signifies a **flatter, more diverse distribution** where the model is more likely to generate a wider range of tokens, including those that were less probable at a lower temperature. The distribution is "smoother."

*   **Supporting Text Evidence:**
    *   The labels "**Low**" and "**High**" on the "temperature" axis directly establish the two comparative conditions.
    *   The labels "**car**," "**truck**," and "**elephant**," along with the ellipses ("**...**"), represent different tokens or possible outputs.
    *   The label "**probabilities**" under each set of bars explicitly identifies what the lengths of the bars represent.
    *   The visual comparison of the *relative lengths* of the bars for "car," "truck," and "elephant" between the "Low" and "High" temperature settings provides the quantitative evidence for the shift in probability distribution. For instance, "car" is the longest bar under "Low temperature" but is shorter under "High temperature," while "elephant" has a very short bar under "Low temperature" but a noticeably longer bar under "High temperature."

**Key Insights:**
The image offers several key insights regarding the "temperature" parameter in generative models:

*   **Main Takeaway 1: Temperature Controls Determinism vs. Randomness.**
    *   **Evidence:** Under "**Low**" "**temperature**," tokens like "**car**" and "**truck**" have significantly higher "**probabilities**," leading to highly predictable outputs. Under "**High**" "**temperature**," the "**probabilities**" are more evenly distributed across all tokens, including "**elephant**" and the "...", making the outputs more varied and less predictable. This shows that lower temperatures increase determinism by concentrating probability mass on a few tokens, while higher temperatures increase randomness by spreading probability mass more broadly.

*   **Main Takeaway 2: Lower Temperatures Amplify Probable Tokens and Suppress Improbable Ones.**
    *   **Evidence:** Comparing the "**probabilities**" bars, the most probable tokens (e.g., "**car**") have much longer bars at "**Low**" "**temperature**" than at "**High**" "**temperature**." Conversely, less probable tokens (e.g., "**elephant**") have very short bars at "**Low**" "**temperature**" but noticeably longer bars at "**High**" "**temperature**." This demonstrates that a low temperature exaggerates the differences between high and low probability tokens.

*   **Main Takeaway 3: Higher Temperatures Increase the Likelihood of Generating Less Common/Probable Tokens.**
    *   **Evidence:** The increase in bar length for tokens like "**elephant**" and the "..." under "**High**" "**temperature**" compared to "**Low**" "**temperature**" explicitly shows that higher temperatures make these less probable tokens more likely to be selected. This is crucial for generating more diverse, novel, or "creative" outputs.

*   **Main Takeaway 4: Temperature Reshapes the Entire Probability Distribution.**
    *   **Evidence:** The visual transformation of the entire set of "**probabilities**" bars from a sharp, skewed distribution at "**Low**" "**temperature**" to a flatter, more uniform distribution at "**High**" "**temperature**" illustrates that "temperature" globally modifies the likelihood of all possible outputs, not just a select few.

**Document Context:**
This image, Figure 6-4, directly supports and visually explains the accompanying text: "A higher temperature increases the likelihood that less probable tokens are generated and vice versa." It serves as a concrete illustration of how the "temperature" parameter in generative models (like large language models) influences the output diversity. It's a fundamental concept for understanding how these models control their "creativity" or the variability of their responses.

**Summary:**
This image displays a conceptual diagram that illustrates the effect of "temperature" on the probability distribution of different "tokens" or potential outputs in a system, such as a language model. A horizontal axis at the top of the diagram is labeled "temperature," with an arrow pointing left indicating "Low" temperature and an arrow pointing right indicating "High" temperature. Below this axis, two distinct scenarios are presented: one for "Low temperature" on the left and another for "High temperature" on the right.

Under the "Low temperature" scenario on the left, a vertical bar chart shows the "probabilities" of various tokens. The token "car" has the longest bar, indicating it has the highest probability. The token "truck" has the second longest bar, also representing a high probability, though slightly less than "car." Below "truck," several ellipses ("...") and the token "elephant" are shown with very short bars, signifying extremely low probabilities. This distribution is very "peaky," concentrating most of the probability mass on a few highly favored tokens.

In contrast, under the "High temperature" scenario on the right, the probability distribution for the same set of tokens ("car," "truck," "...","...", "elephant," "...") is significantly altered. While "car" and "truck" still have relatively high probabilities, their bars are noticeably shorter than their counterparts in the "Low temperature" scenario. Crucially, the tokens represented by the ellipses ("...") and "elephant" now have longer bars compared to the "Low temperature" case, indicating an increased likelihood of being generated. This distribution is much "flatter" or more uniform, meaning the probabilities are spread more broadly across a wider range of tokens, making less probable tokens more likely to occur and reducing the overwhelming dominance of the most probable ones.

In essence, the image visually demonstrates that a "Low temperature" setting makes a system more deterministic, favoring the most probable outputs, while a "High temperature" setting makes it more exploratory and diverse, increasing the chance of generating less common or "creative" tokens by leveling out their probabilities.](images/e2efe06f62f4af368f26d7831668fd95dad3fda0682fc49d7a9919fab0f47843.jpg)
Figure 6-4. A higher temperature increases the likelihood that less probable tokens are generated and vice versa.

As a result, a higher temperature (e.g., 0.8) generally results in a more diverse output while a lower temperature (e.g., 0.2) creates a more deterministic output.

You can use temperature in your pipeline as follows:

# Using a high temperature   
output $=$ pipe(messages, do_sample $\ast =$ True, temperature $^ { = 1 }$ )   
print(output[0]["generated_text"])

Why don't chickens like to go on a rollercoaster? Because they're afraid they might suddenly become chicken-soup!

Note that every time you rerun this piece of code, the output will change! tempera ture introduces stochastic behavior since the model now randomly selects tokens.

#

top_p, also known as nucleus sampling, is a sampling technique that controls which subset of tokens (the nucleus) the LLM can consider. It will consider tokens until it reaches their cumulative probability. If we set top_p to 0.1, it will consider tokens until it reaches that value. If we set top_p to 1, it will consider all tokens.

As shown in Figure 6-5, by lowering the value, it will consider fewer tokens and generally give less “creative” output, while increasing the value allows the LLM to choose from more tokens.

![## Image Analysis: 71627ad212fb92280bda686652820aeed5e88d22f90f51175020bacd0725dd88.jpg

**Conceptual Understanding:**
The image conceptually illustrates the mechanism of 'top_p' (nucleus sampling), a parameter used in language models to control the diversity of generated text. Its main purpose is to show how 'top_p' prunes the vocabulary from which the next token is sampled. It conveys the idea that a lower 'top_p' restricts token selection to a small, highly probable set, while a higher 'top_p' allows for a broader range of tokens to be considered, increasing the diversity of the output. The diagram uses two comparative bar charts to effectively demonstrate this concept.

**Content Interpretation:**
The image illustrates the conceptual impact of the 'top_p' parameter, commonly used in natural language generation models, on the selection of output tokens. It shows two probability distributions for potential next tokens: one under a 'Low' top_p setting and another under a 'High' top_p setting. The 'car' token consistently has the highest probability in both scenarios. Under the 'Low' top_p, only a few of the highest probability tokens (e.g., 'car', 'truck') would be considered for selection, effectively truncating the distribution to a small set of highly probable words. In contrast, under the 'High' top_p, a wider range of tokens, extending to those with lower probabilities (e.g., 'elephant' and other implied tokens), are included in the cumulative probability mass for selection. The varying lengths and shades of the bars visually represent these probabilities, with longer and darker bars indicating higher probabilities. The '...' indicates a continuation of tokens with decreasing probabilities.

**Key Insights:**
The main takeaway from this image is that the 'top_p' parameter directly controls the number of high-probability tokens considered during text generation. When 'top_p' is 'Low', as shown by the left distribution, the model primarily focuses on a limited set of the most probable tokens ('car', 'truck'). Conversely, when 'top_p' is 'High', as depicted by the right distribution, the selection pool expands to include a greater variety of tokens, encompassing those with moderately lower probabilities ('elephant', '...'). This implies that a higher 'top_p' leads to more diverse and potentially less predictable text generation, while a lower 'top_p' results in more focused and predictable outputs. The contrast between the two bar charts visually evidences this: the 'High' top_p side clearly shows more bars of significant length, indicating more tokens being considered in the cumulative sum up to 'top_p'.

**Document Context:**
This image, titled implicitly as Figure 6-5, is presented in a section discussing 'Using a high temperature' in document generation. It directly supports the subsequent text: 'Figure 6-5. A higher top_p increases the number of tokens that can be selected to generate and vice versa.' The diagram visually explains this principle by showing that as 'top_p' moves from 'Low' to 'High', the pool of considered tokens expands from just the very highest probability options to include more diverse, albeit less probable, tokens. This helps the reader understand how 'top_p' influences the creativity or coherence of generated text by controlling the breadth of the token sampling process.

**Summary:**
The image illustrates the effect of the 'top_p' parameter on token selection probabilities, specifically showing how a higher 'top_p' value includes more tokens in the selection pool. It is structured into two main parts, representing a 'Low' top_p setting on the left and a 'High' top_p setting on the right, connected by a central horizontal arrow labeled 'top_p' with directional labels 'Low' pointing left and 'High' pointing right. 

On the left side, under the 'Low' setting, a vertical bar chart depicts a distribution of 'probabilities' for various tokens. From top to bottom, the visible tokens are 'car', 'truck', followed by two instances of '...', then 'elephant', and another '...'. The 'car' token has the longest bar, indicating the highest probability. The 'truck' bar is shorter than 'car' but longer than 'elephant'. The bars become progressively shorter and lighter in shade moving down the list, suggesting decreasing probabilities. Under the 'Low' setting, a smaller number of tokens (represented by the longer, darker bars) would be considered for generation.

On the right side, under the 'High' setting, a similar vertical bar chart for 'probabilities' is shown. The tokens listed are identical: 'car', 'truck', '...', '...', 'elephant', '...', from top to bottom. Here, the 'car' token again has the longest bar. However, compared to the 'Low' setting, a larger number of bars (tokens) appear to have significant length and darker shading, extending further down the list. This indicates that with a 'High' top_p, more tokens, including those with lower initial probabilities like 'elephant' and the tokens represented by '...', are included in the cumulative probability distribution from which a token is selected. Essentially, a higher 'top_p' makes the selection process consider a broader range of possible tokens, leading to more diverse output, while a lower 'top_p' focuses on only the most probable tokens.](images/71627ad212fb92280bda686652820aeed5e88d22f90f51175020bacd0725dd88.jpg)
Figure 6-5. A higher top_p increases the number of tokens that can be selected to generate and vice versa.

Similarly, the top_k parameter controls exactly how many tokens the LLM can consider. If you change its value to 100, the LLM will only consider the top 100 most probable tokens.

You can use top_p in your pipeline as follows:

# Using a high top_p output $=$ pipe(messages, do_sample=True, top_p $^ { 1 = 1 }$ ) print(output[0]["generated_text"])

Why don't chickens make good comedians? Because their 'jokes' always 'feather' the truth!

As shown in Table 6-1, these parameters allow the user to have a sliding scale between being creative (high temperature and top_p) and being predictable (lower temperature and top_p).

Table 6-1. Use case examples when selecting values for temperature and top_p.   

<table><tr><td>Example use case</td><td>ture</td><td>Tempera top_p Description</td><td></td></tr><tr><td>Brainstorming session</td><td>High</td><td>High</td><td>High randomness with large pol of potential tokens.The results will be highly diverse, often leading to very creative and unexpected results.</td></tr><tr><td>Email generation</td><td>Low</td><td>Low</td><td>Deterministic output with high probable predicted tokens.This results in predictable, focused,and conservative outputs.</td></tr><tr><td>Creative writing</td><td>High</td><td>Low</td><td>High randomness with a small pool of potential tokens.This combination produces creative outputs but still remains coherent.</td></tr><tr><td>Translation</td><td>Low</td><td>High</td><td>Deterministic output with high probable predicted tokens.Produces coherent output with a wider range of vocabulary,leading to outputs with linguistic variety.</td></tr></table>

# Intro to Prompt Engineering

An essential part of working with text-generative LLMs is prompt engineering. By carefully designing our prompts we can guide the LLM to generate desired responses. Whether the prompts are questions, statements, or instructions, the main goal of prompt engineering is to elicit a useful response from the model.

Prompt engineering is more than designing effective prompts. It can be used as a tool to evaluate the output of a model as well as to design safeguards and safety mitigation methods. This is an iterative process of prompt optimization and requires experimentation. There is not and unlikely will ever be a perfect prompt design.

In this section, we will go through common methods for prompt engineering, and small tips and tricks to understand what the effect is of certain prompts. These skills allow us to understand the capabilities of LLMs and lie at the foundation of interfacing with these kinds of models.

We begin by answering the question: what should be in a prompt?

# The Basic Ingredients of a Prompt

An LLM is a prediction machine. Based on a certain input, the prompt, it tries to predict the words that might follow it. At its core (illustrated in Figure 6-6), the prompt does not need to be more than a few words to elicit a response from the LLM.

![## Image Analysis: 46db636099c91689cbc67eb0b310410198e0c399d6c39d1086a2ee7111776d03.jpg

**Conceptual Understanding:**
Conceptually, this image represents the most basic form of interaction with a Large Language Model: text completion. Its main purpose is to demonstrate how an LLM processes an incomplete input sentence (a 'prompt') and generates a logical continuation or completion of that sentence when no specific instruction or task is provided. It communicates the idea that LLMs can infer context and provide relevant next words or phrases to complete a given textual input.

**Content Interpretation:**
The image illustrates a fundamental interaction between a user's input (a 'Basic prompt') and a Large Language Model (LLM). It visually represents how an LLM processes a partial sentence, without explicit instructions, to generate a logical completion. The sequence shows data flow from an initial prompt to the LLM for processing, and then to the generated output.

**Key Insights:**
The main takeaway is the fundamental concept of prompt completion by an LLM. When given an incomplete phrase like 'The sky is' as an 'Input' in a 'Basic prompt', the 'LLM' will infer the most probable continuation, resulting in 'Generated text' such as 'blue.' as the 'Output'. This highlights that LLMs, by default, aim to complete the given text in a coherent manner if no specific task or instruction is provided, showcasing their predictive text generation capability.

**Document Context:**
This image directly supports the document's section 'The Basic Ingredients of a Prompt' by providing a concrete, visual example of a basic prompt. It aligns with the accompanying text: 'Figure 6-6. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence.' The diagram visually demonstrates this exact scenario, showing how a simple, incomplete sentence serves as input and how the LLM completes it without explicit directives.

**Summary:**
This diagram illustrates a basic prompt interaction with a Large Language Model (LLM) to generate text. It begins with a 'Basic prompt' container, which includes an input field containing the partial sentence 'The sky is'. This text is explicitly labeled as 'Input'. This input is then processed by an 'LLM', represented by a pink box with a speech bubble icon. The LLM's processing leads to an 'Output', which is the generated text 'blue.' displayed within a rectangular box. The text 'blue.' is further explicitly labeled as 'Generated text', showing the completion of the initial prompt.](images/46db636099c91689cbc67eb0b310410198e0c399d6c39d1086a2ee7111776d03.jpg)
Figure 6-6. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence.

However, although the illustration works as a basic example, it fails to complete a specific task. Instead, we generally approach prompt engineering by asking a specific question or task the LLM should complete. To elicit the desired response, we need a more structured prompt.

For example, and as shown in Figure 6-7, we could ask the LLM to classify a sentence into either having positive or negative sentiment. This extends the most basic prompt to one consisting of two components—the instruction itself and the data that relates to the instruction.

![## Image Analysis: 07b8b96f26df00e19653a466f0957b630ee905b7d5852dfc46ae40c937d7aa74.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental architecture of a simple prompt for a Large Language Model (LLM) and its subsequent processing. The main purpose is to visually demonstrate that a prompt is typically comprised of two essential ingredients: a clear instruction detailing the task to be performed, and the specific data that the instruction refers to. It then illustrates how an LLM processes these components to generate a relevant output. The key ideas communicated are the decomposition of a prompt into its functional parts, the role of an LLM as a processing unit, and the generation of output text based on the combined input.

**Content Interpretation:**
The image depicts a basic text classification process using an LLM. The input is an "Instruction prompt" composed of two distinct parts: an "Instruction" ("Classify the text into negative or positive.") and "Data" (""This is a great movie!""). This combined prompt is then processed by an "LLM" (Large Language Model). The outcome is an "Output" which is specified as "Generated text" and contains "The text is positive." This illustrates the typical interaction pattern where an LLM takes a task and relevant data, processes it, and generates a specific response.

**Key Insights:**
The main takeaways from this image are: 

1.  **Prompts are structured:** An effective prompt for an LLM consists of distinct components: a clear "Instruction" and specific "Data" for the LLM to operate on. This is evidenced by the separate boxes labeled "Instruction" ("Classify the text into negative or positive.") and "Data" (""This is a great movie!"") within the "Instruction prompt" container. 
2.  **Clarity in instruction is vital:** The explicit instruction "Classify the text into negative or positive." highlights the importance of unambiguous task definition for the LLM. 
3.  **LLMs process instructions and data to generate output:** The diagram shows the "LLM" taking the combined prompt as input and producing a relevant "Output" ("The text is positive."), demonstrating that LLMs execute tasks based on the provided input structure. 
4.  **Basic LLM interaction workflow:** The image illustrates the fundamental interaction cycle: input prompt (instruction + data) -> LLM processing -> generated text output.

**Document Context:**
This image perfectly fits within a document section on "The Basic Ingredients of a Prompt." It visually breaks down a prompt into its fundamental components (instruction and data) and demonstrates their role in interacting with an LLM to produce an output. It serves as a foundational diagram for understanding prompt construction, directly supporting the explanation provided in the surrounding text, "Figure 6-7. Two components of a basic instruction prompt: the instruction itself and the data it refers to."

**Summary:**
This diagram, titled "Instruction prompt," visually explains the two core components of a basic prompt fed to a Large Language Model (LLM) and the resulting output.

At the top, a large gray box labeled "Instruction prompt" encapsulates the input provided to an LLM. Within this main prompt box, there are two distinct, smaller, rounded rectangular boxes, each representing a crucial part of the prompt:

1.  **Instruction:** Pointed to by a green line and labeled "Instruction," this green-colored box contains the text: "Classify the text into negative or positive." This clearly defines the task the LLM is expected to perform.
2.  **Data:** Pointed to by a blue line and labeled "Data," this blue-colored box contains the text: ""This is a great movie!"" This represents the specific content or information that the LLM needs to process according to the given instruction.

Below the "Instruction prompt" box, there's a smaller pink-colored box labeled "LLM," representing the Large Language Model itself. A downward-pointing arrow connects the "Instruction prompt" to the "LLM," indicating that the combined instruction and data are fed into the LLM for processing.

Finally, at the bottom of the diagram, there's another large gray box labeled "Output." This box represents the result generated by the LLM. An arrow connects the "LLM" to the "Output" box, signifying the flow of information. Inside the "Output" box, the text "The text is positive." is displayed. This is further labeled by a black line pointing to it, marked "Generated text," indicating the LLM's response based on the input prompt.

In essence, the diagram shows a complete, simple workflow: a user provides a prompt containing both a clear task (like classifying text sentiment) and the actual text data, the LLM processes this input, and then delivers a specific, relevant text output (the classification result).](images/07b8b96f26df00e19653a466f0957b630ee905b7d5852dfc46ae40c937d7aa74.jpg)
Figure 6-7. Two components of a basic instruction prompt: the instruction itself and the data it refers to.

More complex use cases might require more components in a prompt. For instance, to make sure the model only outputs “negative” or “positive” we can introduce output indicators that help guide the model. In Figure 6-8, we prefix the sentence with “Text:” and add “Sentiment:” to prevent the model from generating a complete sen‐ tence. Instead, this structure indicates that we expect either “negative” or “positive.” Although the model might not have been trained on these components directly, it was fed enough instructions to be able to generalize to this structure.

![## Image Analysis: 0955c18da2f57b01eba1487d7cdcb78f3bd805b787470eab3ee388e1ac6b63cc.jpg

**Conceptual Understanding:**
This image conceptually represents an example of a well-structured prompt for a Large Language Model (LLM) that aims to perform a specific task: sentiment classification. Its main purpose is to demonstrate how to effectively guide an LLM to produce a desired output by explicitly defining the instruction, providing input data, and, crucially, using 'output indicators' to specify the format or type of the expected response. It communicates the idea that detailed prompt engineering can lead to more controlled and predictable LLM outputs.

**Content Interpretation:**
This image illustrates the process of structuring a prompt for a Large Language Model (LLM) to achieve a specific output, in this case, sentiment classification. It demonstrates how explicit instructions, along with clearly defined output indicators and input data, guide the LLM's behavior. The diagram shows the input prompt, the LLM's processing, and the resulting generated text. The significance is in showing a practical application of prompt engineering to achieve a predictable and desired output format from a generative AI model.

**Key Insights:**
The main takeaway is that providing clear 'Instruction' and 'Output indicators' within a prompt significantly helps in guiding an LLM to produce a specific and desired 'Output'. Explicitly labeling the input data ('Text:', '"This is a great movie!"', 'Data') and the expected output field ('Sentiment:') allows the LLM to understand both the task and the desired format of the 'Generated text'. This structured prompting leads to a precise output like 'Positive' for sentiment analysis, rather than a free-form response. The inclusion of 'Output indicators' is a key strategy for controlled generation from LLMs. The textual elements like 'Classify the text into negative or positive.' define the task, 'Output indicators' define the structure, and 'Positive' is the concrete result.

**Document Context:**
This image directly supports the document's section 'The Basic Ingredients of a Prompt' by visually demonstrating how to extend a prompt with an output indicator to specify the desired output format. As indicated by the text after the image, 'Figure 6-8. Extending the prompt with an output indicator that allows for a specific output.', it serves as a concrete example of this technique in action, enhancing the reader's understanding of effective prompt construction for LLMs.

**Summary:**
The image illustrates a structured prompt designed for a Large Language Model (LLM) to perform sentiment classification, explicitly showing how an instruction is combined with output indicators and input data to guide the LLM's output. The process begins with an 'Instruction prompt' titled 'Instruction prompt With output indicator'. This prompt contains two main components: an 'Instruction' and 'Output indicators'. The 'Instruction' is 'Classify the text into negative or positive.'. The 'Output indicators' specify the structure for the input data and the desired output. Specifically, 'Text:' is followed by the input '"This is a great movie!"', which is also labeled as 'Data'. Below this, an 'Output indicator' for the result is provided as 'Sentiment:'. This entire structured prompt is then fed into an 'LLM' (Large Language Model). The 'Output' from the LLM, which is specifically the 'Generated text', is 'Positive', indicating the classified sentiment of the input text based on the provided instruction and output indicators.](images/0955c18da2f57b01eba1487d7cdcb78f3bd805b787470eab3ee388e1ac6b63cc.jpg)
Figure 6-8. Extending the prompt with an output indicator that allows for a specific output.

We can continue adding or updating the elements of a prompt until we elicit the response we are looking for. We could add additional examples, describe the use case in more detail, provide additional context, etc. These components are merely exam‐ ples and not a limited set of possibilities. The creativity that comes with designing these components is key.

Although a prompt is a single piece of text, it is tremendously helpful to think of prompts as pieces of a larger puzzle. Have I described the context of my question? Does the prompt have an example of the output?

# Instruction-Based Prompting

Although prompting comes in many flavors, from discussing philosophy with the LLM to role-playing with your favorite superhero, prompting is often used to have the LLM answer a specific question or resolve a certain task. This is referred to as instruction-based prompting.

Figure 6-9 illustrates a number of use cases in which instruction-based prompting plays an important role. We already did one of these in the previous example, namely supervised classification.

![## Image Analysis: b104fd3874ad04b5e382133e3588a92af735fd962f9d026d7bece1d4de3a4a96.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates various practical applications, or "use cases," of instruction-based prompting. Each of the five sections (Supervised classification, Search, Summarization, Code generation, Named entity recognition) visually and textually describes a different task that can be accomplished through this method. 

The main purpose of the image is to demonstrate the versatility and broad applicability of instruction-based prompting. It conveys the message that by providing appropriate instructions, AI models can perform a wide range of functions, from analytical tasks like classification and entity recognition to generative tasks like summarization and code creation, and information retrieval via search. The image communicates the key idea that a flexible, instruction-driven approach can unlock diverse functionalities from AI systems, highlighting the breadth of problems that can be addressed by guiding models with specific commands or prompts.

**Content Interpretation:**
The image illustrates five specific use cases for instruction-based prompting: supervised classification, search, summarization, code generation, and named entity recognition. Each section demonstrates a distinct application through abstract visual elements and explicit textual labels or examples. 

- **Supervised classification** is shown as assigning an input to one of several predefined categories (Politics, Cars, Music, Religion) with varying degrees of confidence, indicated by the length of the bars. This signifies the model's ability to categorize information based on instructions.
- **Search** is depicted as taking a "Search query" and returning a set of relevant results (the horizontal bars). This represents the system's capacity to retrieve information from a dataset based on a user's prompt.
- **Summarization** is presented as a process of distilling longer content (multiple bars on the left) into shorter, more concise content (fewer, shorter bars on the right), highlighted by an arrow. This emphasizes the model's ability to condense information while retaining key meaning.
- **Code generation** is directly exemplified by a Python `print("Hello World!")` command and its corresponding output. This demonstrates the model's capability to generate functional programming code from instructions.
- **Named entity recognition** is visually represented by highlighting specific words or phrases (blue segments) within larger text units (red segments). This signifies the model's ability to identify and extract specific types of information, such as names of people, organizations, or locations, based on a given instruction.

The significance of the information presented lies in demonstrating the broad applicability and versatility of instruction-based prompting across different computational linguistic and programming tasks. The varying bar lengths in classification imply probabilistic outcomes. The transformation in summarization explicitly shows input-to-output conversion. The code snippet offers a concrete, executable example. The highlighted entities in NER clearly show the targeted extraction.

**Key Insights:**
The main takeaways from this image are: 

1.  **Versatility of Instruction-Based Prompting:** The image clearly demonstrates that instruction-based prompting is a highly versatile technique applicable to a wide array of tasks. This is evidenced by the five distinct categories: "Supervised classification," "Search," "Summarization," "Code generation," and "Named entity recognition." Each title explicitly states a different functionality.
2.  **Diverse NLP and AI Applications:** Instruction-based prompting can be leveraged for various Natural Language Processing (NLP) tasks such as categorizing text ("Politics," "Cars," "Music," "Religion" in supervised classification), extracting key information ("Search query" and results), condensing content (the transformation in "Summarization"), and identifying specific data points within text (blue segments in "Named entity recognition").
3.  **Code Generation Capability:** A significant insight is the ability of instruction-based prompting to generate executable code, as explicitly shown by the Python snippet `>>> print("Hello World!")` and its output `Hello World!` under "Code generation."
4.  **Abstract Visual Representation for Conceptual Understanding:** While the visual elements are abstract (bars, arrows), they effectively convey the core concept of each use case, making complex AI functionalities understandable at a glance. For instance, the length reduction in summarization and the highlighted segments in named entity recognition clearly illustrate the process outcomes. 

These insights collectively support the conclusion that instruction-based prompting is a powerful and flexible paradigm for interacting with and leveraging AI models across a broad spectrum of computational tasks.

**Document Context:**
This image serves as a direct visual explanation for the "Instruction-Based Prompting" section of the document. It effectively translates the theoretical concept of instruction-based prompting into concrete, understandable application scenarios. By showcasing diverse use cases like classification, search, summarization, code generation, and named entity recognition, the image provides practical examples of how this technique can be applied across various domains. It reinforces the document's narrative by demonstrating the versatility and power of instruction-based prompting, helping readers grasp the practical implications and potential of the methodology being discussed.

**Summary:**
The image, titled "Use cases for instruction-based prompting," presents five distinct application scenarios for this technique, each illustrated with a simple visual representation and relevant text. 

1.  **Supervised classification:** This section displays a bar chart with four categories: "Politics," "Cars," "Music," and "Religion." Each category has a horizontal red bar next to it, with varying lengths, visually representing relative scores or probabilities. "Politics" has the longest bar, followed by "Cars," then "Music," and finally "Religion" with the shortest bar. This illustrates how instruction-based prompting can categorize input into predefined classes.

2.  **Search:** This section features a white rectangular input box labeled with a magnifying glass icon and the text "Search query." Below this input, there are four horizontal bar-like shapes. The top three are red, representing potential search results, while the bottom one is gray. This demonstrates the use of prompting for information retrieval.

3.  **Summarization:** This section shows a transformation process. On the left, there are four horizontal bar-like shapes (two red, two gray), representing longer pieces of content. An arrow points from these four bars towards two shorter red bar-like shapes on the right, signifying a reduction in content length, i.e., summarization.

4.  **Code generation:** This section displays a code snippet. The first line is `>>> print("Hello World!")`, which is a Python command to print text. The second line is `Hello World!`, representing the output of the command. The third line is `>>>`, indicating a ready prompt for further input. This illustrates how instruction-based prompting can generate executable code.

5.  **Named entity recognition:** This section shows four horizontal bar-like shapes, each representing a segment of text. Within two of these shapes, specific parts are highlighted in blue. The first bar is entirely red. The second bar has a red segment, then a blue segment, then another red segment. The third bar is entirely red. The fourth bar also has a red segment, then a blue segment, and then another red segment. This visualizes the process of identifying and extracting specific entities (the blue segments) from a larger body of text.

Collectively, the image provides a clear and comprehensive overview of the diverse capabilities of instruction-based prompting across various tasks in natural language processing and beyond.](images/b104fd3874ad04b5e382133e3588a92af735fd962f9d026d7bece1d4de3a4a96.jpg)
Figure 6-9. Use cases for instruction-based prompting.

Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. Asking the LLM to summarize a piece of text will not suddenly result in classification. To illustrate, examples of prompts for some of these use cases can be found in Figure 6-10.

![## Image Analysis: c19a7e44985bf2dd57564e430035276d3cc87a7025ec2370efa0c3bd65b8791e.jpg

**Conceptual Understanding:**
This image conceptually represents various approaches to 'instruction-based prompting' in the context of Natural Language Processing (NLP) tasks. It illustrates how different prompt structures can be designed for common use cases.

The main purpose of the image is to demonstrate the flexibility and variety in how instructions can be formulated and positioned within a prompt to guide a language model. It aims to show that for a given task, there isn't just one 'correct' way to write a prompt, and additional elements like definitions or output indicators can be included to enhance clarity and performance. The key idea communicated is that prompt engineering involves careful consideration of instruction phrasing, placement, and the inclusion of contextual details.

**Content Interpretation:**
The image illustrates various prompt structures for common Natural Language Processing (NLP) tasks: Summarization, Classification, and Named Entity Recognition. It shows how the same core task can be approached with different instructional phrasing and positioning within a prompt.

For **Summarization**, the image demonstrates two prompt structures: one where the instruction 'Summarize the following text:' precedes the data, and another where the data '...' precedes the instruction 'Explain the above in two to three sentences.' This signifies that instructions can be at the beginning or end of a prompt.

For **Classification**, two prompt variations are shown. The first, associated with 'Output indicators,' asks a direct question 'Is the following text neutral, negative, or positive?' and explicitly labels input as 'Text: ...' and desired output as 'Sentiment:'. The second variation provides a direct command: 'Classify the text into neutral, negative, or positive' followed by the data '...'. This highlights how prompts can be phrased as questions or direct commands and can include output formatting.

For **Named entity recognition**, the prompt structure is more elaborate, incorporating definitions and exclusion criteria. The 'Define entities' section provides a detailed 'Definition: an entity is an organization (org), a person (per), or a location (loc). If it does not fit with the above, use (misc).' This establishes the types of entities to be identified. The 'Exclusion criteria' 'Verbs and adjectives are not entities.' further refines the task by specifying what *not* to extract. The core instruction is 'Extract entities from the following text:' followed by the data '...'. This demonstrates the importance of providing clear guidelines, definitions, and constraints for complex NLP tasks.

All extracted text elements, such as specific instructions ('Summarize the following text:', 'Classify the text into neutral, negative, or positive', 'Extract entities from the following text:'), data placeholders ('...'), definitions, and exclusion criteria, directly support the interpretation of different prompt engineering strategies for various use cases.

**Key Insights:**
The main takeaways from this image are:
1.  **Flexibility in Prompt Structure:** Prompts for the same task can have varied structures. The instruction can appear at the beginning or end of the prompt relative to the input data, as seen in the Summarization examples ('Summarize the following text: ...' vs. '... Explain the above in two to three sentences').
2.  **Instruction Location Matters:** While flexible, the location of the instruction can influence how the model interprets the task, though the image primarily shows *that* it can change, not the comparative efficacy.
3.  **Explicit Output Indicators:** For certain tasks like Classification, explicitly labeling input and desired output fields ('Text: ...', 'Sentiment:') can provide clearer guidance to the model about the expected format of its response.
4.  **Detailed Task Definition and Constraints:** For complex tasks like Named Entity Recognition, providing a precise 'Definition: an entity is an organization (org), a person (per), or a location (loc). If it does not fit with the above, use (misc).' and 'Exclusion criteria: Verbs and adjectives are not entities.' is crucial. These explicit rules help refine the task and improve the accuracy of entity extraction.
5.  **Task Specificity:** Different NLP tasks require different types of instructions and contextual information within the prompt. Summarization needs a clear request for a condensed version; Classification needs criteria for categorization; and Named Entity Recognition benefits from definitions and rules for identification.

These insights are directly evidenced by the complete textual content extracted. For instance, the different prompt wordings for Summarization and Classification explicitly show varied instruction placement. The 'Output indicators' for Classification ('Text:', 'Sentiment:') demonstrate the utility of explicit labeling. The detailed 'Definition:' and 'Exclusion criteria' in Named Entity Recognition provide clear evidence of the need for comprehensive task definition for complex tasks.

**Document Context:**
This image is highly relevant to the document section titled "Instruction-Based Prompting" as it visually demonstrates the core concept: how instructions are formulated and presented to a language model. The text after the image, "Figure 6-10. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed," perfectly frames the image's purpose. The image serves as concrete examples, illustrating the flexibility and considerations in crafting effective prompts for tasks like summarization, classification, and named entity recognition. It directly supports the discussion by showing, rather than just telling, the different ways instructions and associated context (like definitions or output indicators) can be structured to guide a model's behavior, reinforcing the idea that prompt design is a crucial aspect of interacting with language models.

**Summary:**
This image, titled "Prompt examples of common use cases," illustrates various ways to structure prompts for three distinct Natural Language Processing (NLP) tasks: Summarization, Classification, and Named Entity Recognition. The diagram emphasizes how the placement and phrasing of instructions can change within a use case while achieving the same underlying goal. Each section presents multiple prompt variations to demonstrate this flexibility.

For **Summarization**, two example prompt structures are shown. The first places the instruction "Summarize the following text:" followed by the "Data" field (...). The second reverses this, starting with the "Data" field (...) and then the instruction "Explain the above in two to three sentences."

For **Classification**, there are also two prompt examples. The first, marked with "Output indicators," asks "Is the following text neutral, negative, or positive?" and then specifies input as "Text: ..." and output as "Sentiment:". The second prompt for classification directly instructs "Classify the text into neutral, negative, or positive" followed by the "Data" field (...).

The **Named entity recognition** section provides a more complex prompt structure. It begins with a "Define entities" component, stating the "Definition: an entity is an organization (org), a person (per), or a location (loc). If it does not fit with the above, use (misc)." This is followed by "Exclusion criteria" which specifies "Verbs and adjectives are not entities." Finally, the instruction is given as "Extract entities from the following text:" with a subsequent "Data" field (...).

Overall, the image clearly and comprehensively illustrates the concept of instruction-based prompting, showing how instructions, data, definitions, and constraints can be arranged in different configurations to guide a model for specific NLP tasks.](images/c19a7e44985bf2dd57564e430035276d3cc87a7025ec2370efa0c3bd65b8791e.jpg)
Figure 6-10. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed.

Although these tasks require different instructions, there is actually a lot of overlap in the prompting techniques used to improve the quality of the output. A nonexhaustive list of these techniques includes:

Specificity

Accurately describe what you want to achieve. Instead of asking the LLM to “Write a description for a product” ask it to “Write a description for a product in less than two sentences and use a formal tone.”

# Hallucination

LLMs may generate incorrect information confidently, which is referred to as hallucination. To reduce its impact, we can ask the LLM to only generate an answer if it knows the answer. If it does not know the answer, it can respond with “I don’t know.”

# Order

Either begin or end your prompt with the instruction. Especially with long prompts, information in the middle is often forgotten.1 LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect).

Here, specificity is arguably the most important aspect. By restricting and specifying what the model should generate, there is a smaller chance of having it generate something not related to your use case. For instance, if we were to skip the instruc‐ tion “in two to three sentences” it might generate complete paragraphs. Like human conversations, without any specific instructions or additional context, it is difficult to derive what the task at hand actually is.

# Advanced Prompt Engineering

On the surface, creating a good prompt might seem straightforward. Ask a specific question, be accurate, add some examples, and you are done! However, prompting can grow complex quite quickly and as a result is an often-underestimated compo‐ nent of leveraging LLMs.

Here, we will go through several advanced techniques for building up your prompts, starting with the iterative workflow of building up complex prompts all the way to using LLMs sequentially to get improved results. Eventually, we will even build up to advanced reasoning techniques.

# The Potential Complexity of a Prompt

As we explored in the intro to prompt engineering, a prompt generally consists of multiple components. In our very first example, our prompt consisted of instruction, data, and output indicators. As we mentioned before, no prompt is limited to just these three components and you can build it up to be as complex as you want.

These advanced components can quickly make a prompt quite complex. Some com‐ mon components are:

Persona

Describe what role the LLM should take on. For example, use “You are an expert in astrophysics” if you want to ask a question about astrophysics.

# Instruction

The task itself. Make sure this is as specific as possible. We do not want to leave much room for interpretation.

# Context

Additional information describing the context of the problem or task. It answers questions like “What is the reason for the instruction?”

# Format

The format the LLM should use to output the generated text. Without it, the LLM will come up with a format itself, which is troublesome in automated systems.

# Audience

The target of the generated text. This also describes the level of the generated output. For education purposes, it is often helpful to use ELI5 (“Explain it like $\Gamma { \bf m } 5 ^ { \mathrm { \scriptscriptstyle ~ . } }$ ).

Tone

The tone of voice the LLM should use in the generated text. If you are writing a formal email to your boss, you might not want to use an informal tone of voice.

Data

The main data related to the task itself.

To illustrate, let us extend the classification prompt we had earlier and use all of the preceding components. This is demonstrated in Figure 6-11.

This complex prompt demonstrates the modular nature of prompting. We can add and remove components freely and judge their effect on the output. As illustrated in Figure 6-12, we can slowly build up our prompt and explore the effect of each change.

The changes are not limited to simply introducing or removing components. Their order, as we saw before with the recency and primacy effects, can affect the quality of the LLM’s output. In other words, experimentation is vital when finding the best prompt for your use case. With prompting, we essentially have ourselves in an iterative cycle of experimentation.

![## Image Analysis: 5c830f81a10aae883ca8b200b50873b6cc9e310a0c4e34819516969841487190.jpg

**Conceptual Understanding:**
This image conceptually represents a methodology for constructing a 'complex prompt' for a large language model, specifically for the task of 'Summarization.' The main purpose being conveyed is to illustrate how a detailed and structured prompt, comprised of multiple, well-defined components, can guide an AI to produce a highly specific and effective output. It highlights that by providing explicit instructions regarding the AI's persona, the task itself, additional context, desired format, target audience, and required tone, one can significantly improve the quality and relevance of the generated summary. The image demonstrates a best practice in prompt engineering, moving beyond simple commands to a more comprehensive and nuanced approach to interacting with advanced AI models.

**Content Interpretation:**
This image illustrates the construction of a comprehensive prompt for a summarization task, broken down into distinct, sequential components. Each component contributes to refining the prompt to achieve a specific, high-quality output. The 'Persona' component ('You are the expert in large language models. You excel at breaking down complex papers into digestible summaries.') establishes the AI's role and expertise. The 'Instruction' ('Summarize the key findings of the paper provided.') defines the core task. The 'Context' ('Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.') provides additional directives, ensuring the summary is targeted. The 'Format' ('Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.') specifies the desired output structure. The 'Audience' ('The summary is designed for busy researchers that quickly need to grasp the newest trends in large language models.') clarifies the target readership, influencing the summary's style and content focus. The 'Tone' ('The tone should be professional and clear.') sets the overall stylistic requirements. Finally, the 'Data' component ('...') indicates where the actual content to be summarized would be inserted. Collectively, these components demonstrate a systematic method for engineering a prompt that leaves little room for ambiguity, aiming for precise and relevant output.

**Key Insights:**
The main takeaway from this image is the importance of a structured and multi-faceted approach to prompt engineering for large language models, particularly for complex tasks like summarization. By breaking down a prompt into distinct components, users can provide highly specific instructions and constraints, leading to more accurate and tailored outputs. The image teaches that a comprehensive prompt should define: 1. **Persona:** As seen in 'You are the expert in large language models. You excel at breaking down complex papers into digestible summaries.' this sets the AI's identity and capabilities. 2. **Instruction:** 'Summarize the key findings of the paper provided.' clearly states the core task. 3. **Context:** 'Your summary should extract the most crucial points that can help researchers quickly understand the most vital information of the paper.' adds guiding information for relevance. 4. **Format:** 'Create a bullet-point summary that outlines the method. Follow this up with a concise paragraph that encapsulates the main results.' dictates the desired output structure. 5. **Audience:** 'The summary is designed for busy researchers that quickly need to grasp the newest trends in large language models.' ensures the output is appropriate for the intended reader. 6. **Tone:** 'The tone should be professional and clear.' specifies the stylistic requirements. 7. **Data:** '...' represents where the input content will be provided. These components collectively ensure that the language model understands not just 'what to do,' but also 'how to do it,' 'for whom,' and 'in what style,' resulting in a higher quality and more usable output.

**Document Context:**
This image directly supports the document's section on 'Audience' by presenting an example of a 'complex prompt with many components,' as explicitly stated in the text after the image. Specifically, the 'Audience' component within the prompt example ('The summary is designed for busy researchers that quickly need to grasp the newest trends in large language models.') highlights how understanding the target audience is crucial for crafting effective prompts, influencing the summary's content and style. The image serves as a detailed illustration of prompt engineering principles, demonstrating how various elements, including audience considerations, are integrated to achieve a desired output from a large language model. It shows how the abstract concept of considering the 'Audience' translates into a concrete instruction within a prompt.

**Summary:**
The image presents a structured approach to constructing a detailed prompt for a large language model to perform a summarization task. It breaks down the prompt into seven distinct components: Persona, Instruction, Context, Format, Audience, Tone, and Data. Each component is clearly labeled and accompanied by example text that would form part of the complete prompt. The 'Persona' defines the role the model should adopt, the 'Instruction' specifies the core task, 'Context' provides additional guiding information, 'Format' dictates the desired output structure, 'Audience' specifies the intended recipient of the summary, 'Tone' sets the required style, and 'Data' indicates where the input for summarization would be placed. The flow is linear, demonstrating how these elements sequentially build a comprehensive and effective prompt, moving from defining the model's role to specifying the output style and finally the input data.](images/5c830f81a10aae883ca8b200b50873b6cc9e310a0c4e34819516969841487190.jpg)
Figure 6-11. An example of a complex prompt with many components.

![## Image Analysis: 31f73f200831cb3c38871f316a3af52762984dcc40bf6cdb0768027e1bae5d24.jpg

**Conceptual Understanding:**
This image conceptually represents the iterative and modular nature of prompt engineering. Its main purpose is to illustrate how a prompt can be progressively enhanced and made more sophisticated by adding distinct, modular components over several development cycles. The key idea communicated is that prompt design is not static but an evolving process where elements like instructions, data, persona, context, and tone are incrementally incorporated to achieve desired outcomes from a language model.

**Content Interpretation:**
The image displays a conceptual flow illustrating the iterative development of prompts, likely in the context of large language models. It shows how prompts evolve by adding modular components over multiple iterations. In 'Iteration 1', a prompt is composed of 'Instruction' and 'Data'. In 'Iteration 2', the component 'Persona' is added, along with 'Instruction' and 'Data'. In 'Iteration n', further components 'Context' and 'Tone' are added, alongside 'Instruction' and 'Data'. This signifies a progressive enrichment of the prompt, where each iteration adds more specific or guiding elements to achieve a desired outcome. The 'n' in 'Iteration n' indicates that this process can continue indefinitely, with more components potentially being added or refined.

**Key Insights:**
The main takeaways from this image are: 1. Prompt engineering is an iterative process, not a one-time setup. 2. Prompts can be built using modular components, allowing for flexible and incremental refinement. 3. As prompts iterate, they can incorporate increasing levels of specificity and guidance, such as defining a 'Persona', setting 'Context', and specifying 'Tone', in addition to core 'Instruction' and 'Data'. 4. The process emphasizes a systematic approach to evolving prompts by adding distinct elements to improve their effectiveness. The verbatim text 'Instruction', 'Data', 'Persona', 'Context', and 'Tone' explicitly names these modular components, serving as direct evidence for how prompts are constructed and refined.

**Document Context:**
This image directly supports the document's statement: 'Figure 6-12. Iterating over modular components is a vital part of prompt engineering.' It visually explains and breaks down what 'iterating over modular components' entails in prompt engineering, by showing specific components ('Instruction', 'Data', 'Persona', 'Context', 'Tone') being introduced and built upon in a sequential, iterative manner. This visual representation helps to clarify the practical application and importance of modularity and iteration in designing effective prompts.

**Summary:**
This image illustrates the iterative process of prompt engineering, specifically how modular components are added and refined over successive iterations. It begins with a basic prompt structure in 'Iteration 1' consisting of 'Instruction' and 'Data'. In 'Iteration 2', a new component, 'Persona', is added alongside the existing 'Instruction' and 'Data', indicating a refinement or specialization of the prompt. The process continues to 'Iteration n', which signifies further iterations where additional modular components like 'Context' and 'Tone' are introduced, expanding the prompt's complexity and specificity while retaining the core 'Instruction' and 'Data'. The arrows connecting the iterations (a solid arrow between Iteration 1 and 2, and a dashed arrow with ellipses between Iteration 2 and Iteration n) depict the progression and ongoing nature of this refinement process. The overall diagram visually demonstrates how a prompt can be incrementally built upon by adding distinct, modular elements.](images/31f73f200831cb3c38871f316a3af52762984dcc40bf6cdb0768027e1bae5d24.jpg)
Figure 6-12. Iterating over modular components is a vital part of prompt engineering.

Try it out yourself! Use the complex prompt to add and/or remove parts to observe its impact on the generated output. You will quickly notice when pieces of the puzzle are worth keeping. You can use your own data by adding it to the data variable:

# # Prompt components

persona $=$ "You are an expert in Large Language models. You excel at breaking   
down complex papers into digestible summaries.\n"   
instruction $=$ "Summarize the key findings of the paper provided.\n"   
context $=$ "Your summary should extract the most crucial points that can help   
researchers quickly understand the most vital information of the paper.\n"   
data_format $=$ "Create a bullet-point summary that outlines the method. Follow   
this up with a concise paragraph that encapsulates the main results.\n"   
audience $=$ "The summary is designed for busy researchers that quickly need to   
grasp the newest trends in Large Language Models.\n"   
tone $=$ "The tone should be professional and clear.\n"   
text $=$ "MY TEXT TO SUMMARIZE"   
data $=$ f"Text to summarize: {text}"

# The full prompt - remove and add pieces to view its impact on the generated output query $=$ persona $^ +$ instruction $^ +$ context $^ +$ data_format $^ +$ audience $^ +$ tone $^ +$ data

There is all manner of components that we could add and creative components like using emotional stimuli (e.g., “This is very impor‐ tant for my career.”2). Part of the fun in prompt engineering is that you can be as creative as possible to figure out which combination of prompt components contribute to your use case. There are few constraints to developing a format that works for you.

In a way, it is an attempt to reverse engineer what the model has learned and how it responds to certain prompts. However, note that some prompts work better for certain models compared to others as their training data might be different or they are trained for different purposes.

# In-Context Learning: Providing Examples

In the previous sections, we tried to accurately describe what the LLM should do. Although accurate and specific descriptions help the LLM to understand the use case, we can go one step further. Instead of describing the task, why do we not just show the task?

We can provide the LLM with examples of exactly the thing that we want to achieve. This is often referred to as in-context learning, where we provide the model with correct examples.3

As illustrated in Figure 6-13, this comes in a number of forms depending on how many examples you show the LLM. Zero-shot prompting does not leverage examples, one-shot prompts use a single example, and few-shot prompts use two or more examples.

![## Image Analysis: 1df0008f70bb07b20d199b41e7e1823bc613c3291bdc0444b6264d52f9402762.jpg

**Conceptual Understanding:**
This image represents and illustrates different strategies for 'in-context learning' in the context of natural language processing, specifically focusing on how examples are provided within a prompt to a language model. The main purpose is to visually differentiate between 'zero-shot', 'one-shot', and 'few-shot' prompting techniques by showing their respective structures using a sentiment classification task as an example. The key ideas being communicated are that the number of examples given directly influences the prompt's design and implicitly guides the language model's understanding and execution of a task.

**Content Interpretation:**
The image systematically illustrates three different approaches to crafting prompts for large language models, specifically for a sentiment classification task. It shows how the inclusion of examples within the prompt, or the lack thereof, defines the 'shot' type. The 'Zero-shot prompt' represents a scenario where the model is expected to perform the task based solely on the instruction, without any provided examples. The 'One-shot prompt' introduces the concept of providing a single, complete example to guide the model on the desired output format and behavior. The 'Few-shot prompt' extends this by offering multiple examples, thereby providing a richer context and clearer pattern for the model to follow when generating its response for a new, unseen input. Each prompt aims to classify text into one of three categories: neutral, negative, or positive, using example phrases related to food. The significance lies in demonstrating a core concept of in-context learning: the more relevant examples provided within the prompt, the better the language model can understand the task and generate appropriate responses, as it learns from the patterns established by these examples. The varying colors for sentiment labels (blue for Neutral, green for Positive, red for Negative) in the examples visually distinguish the classifications.

**Key Insights:**
The main takeaway from this image is that the effectiveness and clarity of a prompt for a large language model can be significantly enhanced by providing in-context examples. The image teaches that there are distinct categories of prompting—'Zero-shot', 'One-shot', and 'Few-shot'—defined by the number of examples included within the prompt itself. Specifically, 'Zero-shot prompt' relies purely on instruction without examples, evidenced by "Prompting without examples" and only the task and a new input. 'One-shot prompt' introduces one example, as shown by "Prompting with a single example" and the inclusion of one "Text: I think the food was alright. Sentiment: Neutral" pair before the new input. 'Few-shot prompt' suggests that providing 'more than one example' (evidenced by the three distinct examples: "Text: I think the food was alright. Sentiment: Neutral.", "Text: I think the food was great! Sentiment: Positive.", and "Text: I think the food was horrible... Sentiment: Negative.") can provide stronger guidance. These examples demonstrate that including diverse instances of input-output pairs helps the model infer the desired task, format, and style for subsequent inputs. The image highlights that the prompt structure evolves with the introduction of examples, moving from a simple instruction-input pair to a more complex structure incorporating multiple labeled examples.

**Document Context:**
This image directly supports the document's section on "In-Context Learning: Providing Examples" by visually defining and exemplifying the concepts of zero-shot, one-shot, and few-shot prompting. It serves as a concrete illustration of how prompts are structured when examples are either absent, singular, or multiple. The figure's position immediately before the text "Figure 6-13. An example of a complex prompt with many components." confirms its role in providing visual context and detailed examples for understanding complex prompt construction and the role of in-context examples in guiding language models.

**Summary:**
This image illustrates three distinct prompting strategies used in natural language processing: Zero-shot, One-shot, and Few-shot prompting. Each strategy is explained with its definition and an example of a sentiment classification task. The 'Zero-shot prompt' requires the model to classify text without any prior examples, presenting only the instruction: "Classify the text into neutral, negative, or positive." and the input "Text: I think the food was okay." with an empty sentiment slot "Sentiment: ....". The 'One-shot prompt' provides a single example to guide the model. It starts with the same instruction "Classify the text into neutral, negative, or positive." followed by one complete example: "Text: I think the food was alright." and "Sentiment: Neutral". After this example, the actual task for the model is presented: "Text: I think the food was okay." and "Sentiment:". The 'Few-shot prompt' offers multiple examples to establish a pattern for the model. It also begins with "Classify the text into neutral, negative, or positive." and then provides three distinct examples: "Text: I think the food was alright." with "Sentiment: Neutral.", "Text: I think the food was great!" with "Sentiment: Positive.", and "Text: I think the food was horrible..." with "Sentiment: Negative.". Finally, the task for the model is posed: "Text: I think the food was okay." and "Sentiment:". The image clearly demonstrates how the number of in-context examples provided directly influences the structure and content of the prompt, moving from no examples to one, and then to several, to guide a language model in performing a task like sentiment classification.](images/1df0008f70bb07b20d199b41e7e1823bc613c3291bdc0444b6264d52f9402762.jpg)
Figure 6-13. An example of a complex prompt with many components.

Adopting the original phrase, we believe that “an example is worth a thousand words.” These examples provide a direct example of what and how the LLM should achieve.

We can illustrate this method with a simple example taken from the original paper describing this method.4 The goal of the prompt is to generate a sentence with a made-up word. To improve the quality of the resulting sentence, we can show the generative model an example of what a proper sentence with a made-up word would be.

To do so, we will need to differentiate between our question (user) and the answers that were provided by the model (assistant). We additionally showcase how this interaction is processed using the template:

# Use a single example of using the made-up word in a sentence   
one_shot_prompt $=$ [ { "role": "user", "content": "A 'Gigamuru' is a type of Japanese musical instrument. An   
example of a sentence that uses the word Gigamuru is:"

},{ "role": "assistant", "content": "I have a Gigamuru that my uncle gave me as a gift. I love to play it at home." },{ "role": "user" "content": "To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:" } ] print(tokenizer.apply_chat_template(one_shot_prompt, tokenize $=$ False))

<s><|user|>   
A 'Gigamuru' is a type of Japanese musical instrument. An example of a sen  
tence that uses the word Gigamuru is:<|end|>   
<|assistant|>   
I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.<|   
end|>   
<|user|>   
To 'screeg' something is to swing a sword at it. An example of a sentence that   
uses the word screeg is:<|end|>   
<|assistant|>

The prompt illustrates the need to differentiate between the user and the assistant. If we did not, it would seem as if we were talking to ourselves. Using these interactions, we can generate output as follows:

# Generate the output outputs $=$ pipe(one_shot_prompt) print(outputs[0]["generated_text"])

During the intense duel, the knight skillfully screeged his opponent's shield, forcing him to defend himself.

It correctly generated the answer!

As with all prompt components, one- or few-shot prompting is not the be all and end all of prompt engineering. We can use it as one piece of the puzzle to further enhance the descriptions that we gave it. The model can still “choose,” through random sampling, to ignore the instructions.

# Chain Prompting: Breaking up the Problem

In previous examples, we explored splitting up prompts into modular components to improve the performance of LLMs. Although this works well for many use cases, this might not be feasible for highly complex prompts or use cases.

Instead of breaking the problem within a prompt, we can do so between prompts. Essentially, we take the output of one prompt and use it as input for the next, thereby creating a continuous chain of interactions that solves our problem.

To illustrate, let us say we want to use an LLM to create a product name, slogan, and sales pitch for us based on a number of product features. Although we can ask the LLM to do this in one go, we can instead break up the problem into pieces.

As a result, and as illustrated in Figure 6-14, we get a sequential pipeline that first creates the product name, uses that with the product features as input to create the slogan, and finally, uses the features, product name, and slogan to create the sales pitch.

![## Image Analysis: 20103bd01ed933ddde7d9f96541f8eaaaed5bc94a073f662d54716bc87a1682b.jpg

**Conceptual Understanding:**
This image conceptually represents 'chain prompting,' a technique used with Large Language Models (LLMs) to perform a series of interconnected tasks by feeding the output of one task as an input to the next. The main purpose of this diagram is to illustrate how a multi-faceted creative generation problem, specifically creating marketing assets for a product, can be systematically broken down and executed in a step-by-step manner where each successive step builds upon the context and output of the preceding one. The key ideas being communicated are the modularity of prompting, the iterative enrichment of context for the LLM, and the sequential flow of information to achieve a comprehensive final output.

**Content Interpretation:**
The image displays a multi-step, sequential process for generating marketing content (product name, slogan, and sales pitch) using a Large Language Model (LLM). Each step of the process takes specific inputs, processes them with an LLM, and generates an output, which then serves as an enhanced input for subsequent steps. The core components shown are input prompts (rectangular boxes with green headers and grey bodies), specific input data elements (blue rounded rectangles, e.g., `<features>`, `<name>`, `<slogan>`), the processing unit (pink rectangular box labeled 'LLM' with a small chat icon), and the generated output (blue rounded rectangles, e.g., `<name>`, `<slogan>`, `<sales_pitch>`).

The process flow is as follows:

**1. Generate product name**

*   **Input:** A prompt box containing the text "Create a product name based on these features:" followed by the input `<features>`.
*   **Processing:** The input is fed into an "LLM" (Large Language Model).
*   **Output:** The LLM generates a product `<name>`.

**2. Generate product slogan**

*   **Input:** This step receives the original `<features>` and the newly generated `<name>` (indicated by a dashed arrow from the output of step 1) into a prompt box containing the text "Create a product slogan based on the following:".
*   **Processing:** The input is fed into an "LLM".
*   **Output:** The LLM generates a product `<slogan>`.

**3. Generate sales pitch**

*   **Input:** This final step receives the original `<features>`, the previously generated `<name>` (indicated by a dashed arrow from the output of step 1), and the newly generated `<slogan>` (indicated by a dashed arrow from the output of step 2) into a prompt box containing the text "Create a sales pitch based on the following:".
*   **Processing:** The input is fed into an "LLM".
*   **Output:** The LLM generates a `<sales_pitch>`.

The significance is that each subsequent content generation task leverages the context provided by the outputs of the previous tasks, leading to a more integrated and coherent set of marketing materials. The extracted text elements explicitly define the prompts, inputs, and outputs at each stage, demonstrating the sequential, cumulative nature of the chain prompting technique.

**Key Insights:**
The main takeaway from this image is the effectiveness of chain prompting as a strategy for leveraging Large Language Models (LLMs) to perform complex, multi-step creative or analytical tasks. By breaking down a larger problem into smaller, sequential prompts, the LLM can build context iteratively, leading to more coherent and sophisticated final outputs.

Key insights supported by the image and its textual elements include:

*   **Iterative Context Building:** Each step in the chain (`Generate product name`, `Generate product slogan`, `Generate sales pitch`) uses the output from the previous steps as an enhanced input. For example, the prompt for the slogan explicitly incorporates `<name>`, and the sales pitch prompt incorporates both `<name>` and `<slogan>`, alongside the initial `<features>`. This demonstrates how context is progressively built and utilized.
*   **Problem Decomposition:** The overall task of creating a marketing package is successfully decomposed into manageable sub-tasks. This modular approach makes the process easier to design, manage, and debug.
*   **Enhanced Coherence and Quality:** By providing an LLM with relevant, previously generated information (e.g., a product name when generating a slogan), the subsequent outputs are more likely to be consistent and high-quality, aligning with the overall product identity. The accumulating inputs in the prompt boxes for each stage (e.g., `<features>`, then `<features>` + `<name>`, then `<features>` + `<name>` + `<slogan>`) serve as direct evidence for this strategy.
*   **LLM as a Core Processor:** The repeated presence of the 'LLM' box at each stage highlights its central role as the engine for transforming inputs into desired outputs throughout the chained process.

**Document Context:**
The image directly illustrates the concept of 'Chain Prompting: Breaking up the Problem' as described in the document's section title. It provides a visual explanation of how a complex problem, such as creating a complete marketing package, can be deconstructed into a series of smaller, interdependent tasks performed by an LLM. The text accompanying the image, "Figure 6-14. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch," perfectly summarizes the workflow depicted and reinforces its relevance to the document's discussion on chain prompting. This visual serves as a concrete example of how the abstract concept of chain prompting is implemented in a practical application, enhancing reader comprehension of the technique.

**Summary:**
This diagram illustrates a 'chain prompting' workflow for generating marketing content using a Large Language Model (LLM). The process is divided into three sequential stages, each building upon the information generated in the preceding steps.

**Stage 1: Generate Product Name**
The process begins by feeding initial product `<features>` into a prompt: "Create a product name based on these features:". This prompt is then processed by an "LLM" (Large Language Model), which generates a product `<name>`.

**Stage 2: Generate Product Slogan**
Next, the generated `<name>` is combined with the original `<features>` and fed into a new prompt: "Create a product slogan based on the following:". This enriched prompt is again processed by an "LLM", resulting in a product `<slogan>`.

**Stage 3: Generate Sales Pitch**
Finally, the original `<features>`, the generated `<name>`, and the generated `<slogan>` are all combined into the last prompt: "Create a sales pitch based on the following:". This comprehensive prompt is processed by an "LLM", which produces the final product `<sales_pitch>`.

The solid arrows indicate the direct flow of the prompt and its processing, while the dashed arrows signify the flow of generated information, specifically how the output of one LLM step (e.g., `<name>`, `<slogan>`) becomes a crucial input for the subsequent steps, enabling a cumulative and context-aware generation process. This sequential method allows for the creation of integrated and coherent marketing assets from a single initial set of product features.](images/20103bd01ed933ddde7d9f96541f8eaaaed5bc94a073f662d54716bc87a1682b.jpg)
Figure 6-14. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch.

This technique of chaining prompts allows the LLM to spend more time on each individual question instead of tackling the whole problem. Let us illustrate this with a small example. We first create a name and slogan for a chatbot:

# Create name and slogan for a product   
product_prompt $= [$ {"role": "user", "content": "Create a name and slogan for a chatbot that   
leverages LLMs."}   
]   
outputs $=$ pipe(product_prompt)   
product_description $=$ outputs[0]["generated_text"]   
print(product_description)

Name: 'MindMeld Messenger'

Slogan: 'Unleashing Intelligent Conversations, One Response at a Time'

Then, we can use the generated output as input for the LLM to generate a sales pitch:

# Based on a name and slogan for a product, generate a sales pitch   
sales_prompt $= [$ {"role": "user", "content": f"Generate a very short sales pitch for the   
following product: '{product_description}'"}   
]   
outputs $=$ pipe(sales_prompt)   
sales_pitch $=$ outputs[0]["generated_text"]   
print(sales_pitch)

Introducing MindMeld Messenger - your ultimate communication partner! Unleash intelligent conversations with our innovative AI-powered messaging platform. With MindMeld Messenger, every response is thoughtful, personalized, and timely. Say goodbye to generic replies and hello to meaningful interactions. Elevate your communication game with MindMeld Messenger - where every message is a step toward smarter conversations. Try it now and experience the future of messaging!

Although we need two calls to the model, a major benefit is that we can give each call different parameters. For instance, the number of tokens created was relatively small for the name and slogan whereas the pitch can be much longer.

This can be used for a variety of use cases, including:

Response validation

Ask the LLM to double-check previously generated outputs.

Parallel prompts

Create multiple prompts in parallel and do a final pass to merge them. For example, ask multiple LLMs to generate multiple recipes in parallel and use the combined result to create a shopping list.

Writing stories

Leverage the LLM to write books or stories by breaking down the problem into components. For example, by first writing a summary, developing characters, and building the story beats before diving into creating the dialogue.

In the next chapter, we will automate this process and go beyond chaining LLMs. We will chain other pieces of technology together, like memory, tool use, and more! Before that, this idea of prompt chaining will be explored further in the following sec‐ tions, which describe more complex prompt chaining methods like self-consistency, chain-of-thought, and tree-of-thought.

# Reasoning with Generative Models

In the previous sections, we focused mostly on the modular component of prompts, building them up through iteration. These advanced prompt engineering techniques, like prompt chaining, proved to be the first step toward enabling complex reasoning with generative models.

Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning. We highlight “resemble” as these models, at the time of writing, are generally considered to demonstrate this behavior through memorization of training data and pattern matching.

The output that they showcase, however, can demonstrate complex behavior and although it might not be “true” reasoning, they are still referred to as reasoning capa‐ bilities. In other words, we work together with the LLM through prompt engineering so we can mimic reasoning processes in order to improve the output of the LLM.

To allow for this reasoning behavior, it is a good moment to step back and explore what reasoning entails in human behavior. To simplify, our methods of reasoning can be divided into system 1 and 2 thinking processes.

System 1 thinking represents an automatic, intuitive, and near-instantaneous pro‐ cess. It shares similarities with generative models that automatically generate tokens without any self-reflective behavior. In contrast, system 2 thinking is a conscious, slow, and logical process, akin to brainstorming and self-reflection.5

If we could give a generative model the ability to mimic a form of self-reflection, we would essentially be emulating the system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking. In this section, we will explore several techniques that attempt to mimic these kinds of thought processes of human reasoners with the aim of improving the output of the model.

# Chain-of-Thought: Think Before Answering

The first and major step toward complex reasoning in generative models was through a method called chain-of-thought. Chain-of-thought aims to have the gen‐ erative model “think” first rather than answering the question directly without any reasoning.6

As illustrated in Figure 6-15, it provides examples in a prompt that demonstrate the reasoning the model should do before generating its response. These reasoning processes are referred to as “thoughts.” This helps tremendously for tasks that involve a higher degree of complexity, like mathematical questions. Adding this reasoning step allows the model to distribute more compute over the reasoning process. Instead of calculating the entire solution based on a few tokens, each additional token in this reasoning process allows the LLM to stabilize its output.

![## Image Analysis: 55bc92a9b27a22acda65213a70cc4dfda365615ec205b6945f984b58820d8614.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates two distinct methods of "prompting" (giving instructions or examples to) a generative AI model: "One-shot prompt" and "Chain-of-thought prompt." The main purpose of the image is to demonstrate how explicitly showing a reasoning process within an example (chain-of-thought prompting) can significantly improve the accuracy and the internal reasoning capabilities of a generative model when answering subsequent, similar questions, compared to merely providing a single input-output example without the intermediate thought steps. The key idea communicated is that prompting models to "think step-by-step" or "show their work" leads to more reliable and correct answers, especially for multi-step problems.

**Content Interpretation:**
The image clearly shows two systems or approaches for interacting with a generative model, focusing on problem-solving.

*   **One-shot prompt:** This approach demonstrates a simple input-output relationship.
    *   **Concept:** The model is given "Prompting with a single example" (e.g., the Roger tennis ball problem and its answer "11").
    *   **Process:** It then attempts to answer a new, similar "Q: The cafeteria had 23 apples..." question.
    *   **Outcome:** The model produces "A: The answer is 27. ❌", indicating an incorrect calculation, suggesting it failed to grasp the underlying multi-step logic from just a single, direct example. The lack of an explicit "Reasoning process (thought)" in this column highlights that the model doesn't output its internal steps.

*   **Chain-of-thought prompt:** This approach introduces an explicit reasoning step into the prompt.
    *   **Concept:** The model is given "Prompting with a reasoning example". The "Example" provided for Roger's tennis balls includes not just the question and final answer, but also the step-by-step "Reasoning process (thought)": "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11 The answer is 11."
    *   **Process:** After this reasoning-rich "Example," the same "Instruction" question about the cafeteria apples is given: "Q: The cafeteria had 23 apples...".
    *   **Outcome:** The model then provides an answer that also includes its "Reasoning process (thought)": "A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9. ✓". This detailed reasoning leads to the correct answer, as indicated by the "✓" symbol.

The significance is evident in the direct comparison of the outcomes for the second apple-related question. The "one-shot prompt" yields an incorrect answer (27), while the "chain-of-thought prompt" yields the correct answer (9) accompanied by a coherent, step-by-step thought process. The extracted text elements "Prompting with a single example" vs. "Prompting with a reasoning example," the presence and absence of "Reasoning process (thought)" labels, and the final "❌" vs. "✓" clearly support that chain-of-thought prompting enhances the model's ability to perform multi-step reasoning and arrive at correct solutions.

**Key Insights:**
The main takeaways and lessons from this image are:

1.  **Improved Accuracy through Explicit Reasoning:** Generative models are more likely to produce correct answers for complex, multi-step problems when they are provided with examples that demonstrate the reasoning process, not just the final answer. The "Chain-of-thought prompt" example explicitly shows the calculation "23 - 20 = 3" and "3 + 6 = 9", leading to "The answer is 9. ✓", whereas the "One-shot prompt" for the same problem results in "The answer is 27. ❌".
2.  **Encouraging Step-by-Step Thinking:** By showing a "Reasoning process (thought)" in the initial example ("Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11"), the generative model learns to apply a similar step-by-step approach when answering subsequent "Instruction" questions. This is evident in the detailed breakdown provided in the chain-of-thought answer for the apple problem.
3.  **Transparency of Thought Process:** Chain-of-thought prompting not only improves accuracy but also makes the model's reasoning more transparent and understandable, as the intermediate steps are explicitly stated. The text "Reasoning process (thought)" accompanying the detailed calculations highlights this transparency.
4.  **Beyond Simple Pattern Matching:** "One-shot prompt" relies on the model inferring patterns from a single input-output pair. For problems requiring sequential operations, this can lead to errors. "Chain-of-thought prompt" moves beyond simple pattern matching by guiding the model to perform a sequence of logical operations. The failure of the one-shot prompt (answer 27) contrasted with the success of the chain-of-thought prompt (answer 9) provides strong evidence for this.

**Document Context:**
This image is directly relevant to the document's section "Chain-of-Thought: Think Before Answering" and the accompanying text: "Figure 6-15. Chain-of-thought prompting uses reasoning examples to persuade the generative model to use reasoning in its answer." It visually explains and provides concrete examples of *how* "chain-of-thought prompting" works and *why* it is effective compared to standard "one-shot prompting."

**Summary:**
This image illustrates two distinct methods for prompting a generative AI model, highlighting the advantages of including a reasoning process in the prompt.

On the left, under "One-shot prompt", which is described as "Prompting with a single example," the process begins with an example question and its correct numerical answer: "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now? A: The answer is 11." Following this, the model is given a new, similar question: "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?" The model then provides a direct answer: "A: The answer is 27. ❌". The "❌" symbol clearly indicates that this answer is incorrect, suggesting the model failed to correctly perform the multi-step arithmetic without explicit reasoning.

On the right, under "Chain-of-thought prompt", which is described as "Prompting with a reasoning example," the process provides the same initial example but with an important addition. For Roger's tennis balls, the answer now includes a detailed "Reasoning process (thought)": "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11 The answer is 11." This example, labeled "Example," shows the steps taken to arrive at the solution. Afterward, the exact same "Instruction" question about the cafeteria apples is given: "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?" This time, because the model was previously shown a reasoning example, its answer also includes a "Reasoning process (thought)": "A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9. ✓". The "✓" symbol confirms this answer is correct.

In essence, the image demonstrates that while a "one-shot prompt" (providing just a question and a final answer) might not enable a generative model to correctly solve subsequent problems requiring multi-step logic, a "chain-of-thought prompt" (providing a question, a step-by-step reasoning process, and the final answer) effectively "persuades" the model to employ its own reasoning capabilities, leading to more accurate and transparent solutions. The contrast between the incorrect answer in the one-shot scenario and the correct, reasoned answer in the chain-of-thought scenario clearly illustrates the superior performance of the latter approach for complex tasks.](images/55bc92a9b27a22acda65213a70cc4dfda365615ec205b6945f984b58820d8614.jpg)
Figure 6-15. Chain-of-thought prompting uses reasoning examples to persuade the gen‐ erative model to use reasoning in its answer.

We use the example the authors used in their paper to demonstrate this phenomenon:

# Answering with chain-of-thought   
cot_prompt $=$ [ {"role": "user", "content": "Roger has 5 tennis balls. He buys 2 more cans   
of tennis balls. Each can has 3 tennis balls. How many tennis balls does he   
have now?"}, {"role": "assistant", "content": "Roger started with 5 balls. 2 cans of 3   
tennis balls each is 6 tennis balls. $5 ~ + ~ 6 ~ = ~ 1 1$ . The answer is 11."}, {"role": "user", "content": "The cafeteria had 23 apples. If they used 20   
to make lunch and bought 6 more, how many apples do they have?"}   
]

# Generate the output outputs $=$ pipe(cot_prompt) print(outputs[0]["generated_text"])

The cafeteria started with 23 apples. They used 20 apples, so they had 23 - 20 $= 3$ apples left. Then they bought 6 more apples, so they now have $3 ~ + ~ 6 ~ = ~ 9$ apples. The answer is 9.

Note how the model doesn’t generate only the answer but provides an explanation before doing so. By doing so, it can leverage the knowledge it has generated thus far to compute the final answer.

Although chain-of-thought is a great method for enhancing the output of a genera‐ tive model, it does require one or more examples of reasoning in the prompt, which the user might not have access to. Instead of providing examples, we can simply ask the generative model to provide the reasoning (zero-shot chain-of-thought). There are many different forms that work but a common and effective method is to use the phrase “Let’s think step-by-step,” which is illustrated in Figure 6-16.7

![## Image Analysis: d820b8d101cde759e9b9facbc03e15f17f5fc243781898f4f4952bc18bd34dea.jpg

**Conceptual Understanding:**
This image conceptually represents the 'Zero-shot chain-of-thought' prompting technique used with large language models (LLMs). Its main purpose is to demonstrate how a simple, direct instruction – specifically the phrase 'Let's think step-by-step' – can elicit a detailed, logical reasoning process from an LLM to solve a multi-step word problem, without the need for explicit examples or prior training on similar problems.

The key ideas communicated are:
*   The ability of LLMs to perform complex, multi-step reasoning when appropriately prompted.
*   The concept of 'zero-shot' learning, where a model generalizes its understanding to new tasks without specific examples.
*   The power of 'chain-of-thought' prompting to generate intermediate thought processes, making the AI's reasoning transparent and the final answer more reliable for problems that traditionally challenge direct prompting approaches.

**Content Interpretation:**
The image depicts a three-stage process for "Zero-shot chain-of-thought" prompting:

1.  **Instruction Input:** A user provides a complex word problem, as evidenced by the text "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?" This is explicitly labeled as "Instruction."
2.  **Reasoning Priming:** A specific prompt, "Let's think step-by-step," is given immediately after the instruction. This is labeled "Prime reasoning," indicating its crucial role in influencing the model's approach to problem-solving.
3.  **Step-by-Step Reasoning Output:** The system then produces a detailed solution, breaking down the problem into logical, intermediate steps. The text "A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9." serves as direct evidence of this reasoning process, showing calculations before the final answer. This is labeled "Reasoning process (thought)" and includes a green checkmark.

The diagram illustrates the concept of "chain-of-thought prompting" in a "zero-shot" context, meaning no prior examples are provided to the model. The title "Zero-shot chain-of-thought prompting without example" confirms this. The sequential boxes and their associated text demonstrate how a simple priming phrase can transform a direct question into a detailed, human-like reasoning output, thereby improving the accuracy and transparency of complex problem-solving by showing the intermediate steps.

**Key Insights:**
The image conveys several key takeaways:

1.  **Effectiveness of Simple Priming:** A concise phrase like "Let's think step-by-step" can profoundly influence an AI model's output, steering it towards a more structured and logical reasoning process, as evidenced by the 

**Document Context:**
This image fits within a document discussing advanced prompting techniques for AI models, specifically Large Language Models (LLMs). It serves as a clear visual example for the "Chain-of-thought prompting" method, highlighting its "zero-shot" variant, which means it works without requiring specific examples. The document's text directly preceding and following the image (e.g., "Figure 6-16. Chain-of-thought prompting without using examples. Instead, it uses the phrase 'Let’s think step-by-step' to prime reasoning in its answer") explicitly contextualizes the diagram, explaining its purpose and the mechanism it illustrates. It visually demonstrates how a simple phrase can effectively 'prime' an LLM to generate a step-by-step reasoning process, which is a key concept in improving AI's problem-solving capabilities.

**Summary:**
This diagram illustrates the "Zero-shot chain-of-thought" prompting technique, which allows an AI model to perform complex reasoning without being given specific examples. The process unfolds in a linear, three-step flow:

1.  **Instruction Input:** The process begins with an "Instruction" (indicated by the green dotted box and label). This is the initial query or problem presented to the AI, which in this case is a word problem: "Q: The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?" This question requires multiple steps of arithmetic to solve correctly.

2.  **Priming for Reasoning:** Following the instruction, a critical "Prime reasoning" step is introduced (indicated by the purple dotted box and label). This is not an example, but a simple, direct instruction: "Let's think step-by-step." This phrase acts as a trigger for the AI to engage its internal reasoning capabilities more thoroughly.

3.  **Reasoning Process and Output:** A downward arrow signifies the transition from the priming step to the AI's generated response. This response is presented in an orange-shaded box with a solid black outline, labeled "Reasoning process (thought)." Instead of just providing a direct answer, the AI demonstrates its thinking process: "A: The cafeteria had 23 apples originally. They used 20 to make lunch. So they had 23 - 20 = 3. They bought 6 more apples, so they have 3 + 6 = 9. The answer is 9." This detailed output includes the intermediate calculations, showing how the AI arrived at the final answer of 9, and is marked with a green checkmark, indicating a successful or correct solution.

In essence, the diagram demonstrates how the simple inclusion of "Let's think step-by-step" in a prompt can instruct an AI model to break down a problem, perform sequential logical steps, and articulate its thought process, ultimately leading to a more accurate and transparent solution for multi-step reasoning tasks without the need for prior training examples.](images/d820b8d101cde759e9b9facbc03e15f17f5fc243781898f4f4952bc18bd34dea.jpg)
Figure 6-16. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s think step-by-step” to prime reasoning in its answer.

Using the example we used before, we can simply append that phrase to the prompt to enable chain-of-thought-like reasoning:

# Zero-shot chain-of-thought   
zeroshot_cot_prompt $=$ [ {"role": "user", "content": "The cafeteria had 23 apples. If they used 20   
to make lunch and bought 6 more, how many apples do they have? Let's think   
step-by-step."}   
]

# Generate the output outputs $=$ pipe(zeroshot_cot_prompt) print(outputs[0]["generated_text"])

Step 1: Start with the initial number of apples, which is 23. Step 2: Subtract the number of apples used to make lunch, which is 20. So, 23 - $2 \Theta = 3$ apples remaining. Step 3: Add the number of apples bought, which is 6. So, $3 + 6 = 9$ apples.

The cafeteria now has 9 apples.

Without needing to provide examples, we again got the same reasoning behavior. This is why it is so important to “show your work” when doing calculations. By addressing the reasoning process the LLM can use the previously generated informa‐ tion as a guide through generating the final answer.

![## Image Analysis: 27c942dbd868306f7671a81fa28c07688c39ad4f1338b23b179fd7f2642c7cad.jpg

**Conceptual Understanding:**
The image conceptually represents a stylized logo or icon. Its main purpose is to serve as a visual identifier or emblem, likely for a brand, organization, or project. It conveys a sense of nature or agility through the animal motif, which appears to be a lemur or squirrel given its distinctive long, spiraling tail and body posture. The clear, simple green silhouette within a square frame suggests a modern and clean design approach.

**Content Interpretation:**
The image exclusively presents a stylized graphic of an animal silhouette, likely serving as a logo, icon, or a visual identifier. It is a standalone graphic with no depicted processes, concepts, relationships, or systems beyond its basic visual form. The significance lies in its potential as a brand mark or an emblem. All requested textual elements, such as process steps, decision points, annotations, and metadata, are absent from the image, thus there is no textual evidence to support interpretations beyond its visual identity as a logo.

**Key Insights:**
The primary takeaway from this image is its visual identity as a logo or icon featuring an animal silhouette. It communicates a non-textual brand or organizational marker. No specific data, processes, or detailed information can be extracted as there is no text or complex imagery. The image emphasizes visual recognition through a simple, memorable graphic. There is no textual evidence in the image itself to support any specific insights, as it contains no text.

**Document Context:**
As a standalone graphic without any text, this image most likely functions as a logo or an identifying icon within the document. Its purpose would be to brand a section, represent an entity, or serve as a visual marker. Without further context from the surrounding document, its specific relevance is limited to being a visual placeholder or an emblem. It does not provide informational content in the way a diagram or chart would, but rather visual identity. The absence of text means it doesn't directly contribute to the narrative through explicit information, but rather through visual association.

**Summary:**
The image displays a green silhouette of an animal, resembling a lemur or a squirrel, contained within a thin square outline. The animal is depicted in profile, facing towards the left side of the frame, with its body in a low, crouching or walking posture. Its four legs are visible, suggesting movement. A prominent feature is its long, bushy tail which extends upwards from its back and then curls inward, forming a distinct spiral shape. The overall design is simplistic and stylized, characteristic of a logo or an icon. No text, annotations, or other descriptive elements are present within the image.](images/27c942dbd868306f7671a81fa28c07688c39ad4f1338b23b179fd7f2642c7cad.jpg)

Although the prompt “Let’s think step by step” can improve the output, you are not constrained by this exact formulation. Alterna‐ tives exist like “Take a deep breath and think step-by-step” and “Let’s work through this problem step-by-step.”8

# Self-Consistency: Sampling Outputs

Using the same prompt multiple times can lead to different results if we allow for a degree of creativity through parameters like temperature and top_p. As a result, the quality of the output might improve or degrade depending on the random selection of tokens. In other words, luck!

To counteract this degree of randomness and improve the performance of generative models, self-consistency was introduced. This method asks the generative model the same prompt multiple times and takes the majority result as the final answer.9 During this process, each answer can be affected by different temperature and top_p values to increase the diversity of sampling.

As illustrated in Figure 6-17, this method can further be improved by adding chainof-thought prompting to improve its reasoning while only using the answer for the voting procedure.

![## Image Analysis: ece89d8bb15c0e598ae76c65a2ab120244bfd8b4cd1a129299e436cad6458463.jpg

**Conceptual Understanding:**
This image conceptually represents the 'Self-consistency' approach in the context of Large Language Models (LLMs). Its main purpose is to illustrate how querying an LLM to generate multiple 'Zero-shot chain-of-thought' reasoning paths, and then aggregating the results through a 'Majority vote', can yield a more reliable and accurate answer than relying on a single output. The image communicates the idea that diversity in reasoning, followed by a consensus mechanism, improves the robustness of AI-generated solutions to complex problems, specifically a mathematical word problem about tennis balls.

**Content Interpretation:**
The image depicts the 'Self-consistency' method, a technique used with Large Language Models (LLMs) to improve the accuracy of answers, particularly for multi-step reasoning problems. It shows a system where a single problem is given to multiple instances or reasoning paths of an LLM, each generating a detailed thought process and a final answer. The key concept is that by allowing the LLM to explore various reasoning trajectories, and then consolidating these via a 'majority vote', the system can identify the most probable correct answer, effectively filtering out erroneous individual reasoning paths. This process leverages the diversity in LLM outputs to enhance reliability.

**Key Insights:**
The main takeaways from this image are: 1. Large Language Models can generate multiple, diverse reasoning paths and answers for the same problem, some of which may be incorrect, as shown by 'A: The answer is 6. ❌'. 2. The 'Zero-shot chain-of-thought' prompt, indicated by 'Let's think step-by-step.', is crucial in encouraging LLMs to break down complex problems into manageable steps. 3. The 'Self-consistency' method improves answer reliability by leveraging multiple LLM 'Reasoning process (thought)' outputs. 4. A 'Majority vote' mechanism is used to consolidate these diverse answers, identifying the most frequently occurring and thus most likely correct solution. 5. This technique helps in overcoming individual LLM errors, leading to a more robust final answer, illustrated by the final 'The answer is 11. ✅' despite one path producing '6'. The image teaches that by not relying on a single reasoning path, the overall accuracy of LLM outputs for complex problems can be significantly enhanced.

**Document Context:**
This image directly supports the document's narrative on 'Self-Consistency: Sampling Outputs' by providing a clear visual explanation of the concept. It demonstrates how sampling multiple reasoning paths from an LLM and using majority voting can lead to a more accurate answer, as stated in the surrounding text, 'Figure 6-17. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer.'. It contextualizes the abstract concept of self-consistency with a concrete example, showing the internal steps an LLM might take and how potential errors are mitigated.

**Summary:**
The image illustrates the 'Self-consistency' method for Large Language Models (LLMs), specifically how sampling from multiple reasoning paths and applying a majority vote can lead to a more accurate answer. The process begins with a complex mathematical word problem presented as "Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?". The LLM is prompted to engage in a "Zero-shot chain-of-thought" by the instruction "Let's think step-by-step.". This initiates a "Reasoning process (thought)" where three distinct LLM instances generate their own reasoning paths and proposed answers. The first LLM path provides the reasoning "If he buys 2 cans of 3 tennis balls, then he has 2 × 3 = 6 tennis balls." and concludes with "A: The answer is 6. ❌", indicating an incorrect answer. The second LLM path offers the reasoning "Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11" and gives the answer "A: The answer is 11. ✅". The third LLM path provides similar reasoning: "2 cans of each 3 tennis balls totals 6 tennis balls. Add 5 on top. 5 + 6 = 11" and also concludes with "A: The answer is 11. ✅". Following these three parallel reasoning paths, a "Majority vote" is applied to the generated answers. Since two out of three LLM paths converged on the answer '11', the final consolidated answer is determined to be "The answer is 11. ✅". This detailed process demonstrates how self-consistency leverages the diversity of LLM outputs to achieve a more robust and reliable solution for multi-step reasoning tasks.](images/ece89d8bb15c0e598ae76c65a2ab120244bfd8b4cd1a129299e436cad6458463.jpg)
Figure 6-17. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer.

However, this does require a single question to be asked multiple times. As a result, although the method can improve performance, it becomes $n$ times slower where $n$ is the number of output samples.

# Tree-of-Thought: Exploring Intermediate Steps

The ideas of chain-of-thought and self-consistency are meant to enable more com‐ plex reasoning. By sampling from multiple “thoughts” and making them more thoughtful, we aim to improve the output of generative models.

These techniques only scratch the surface of what is currently being done to mimic complex reasoning. An improvement to these approaches can be found in tree-ofthought, which allows for an in-depth exploration of several ideas.

The method works as follows. When faced with a problem that requires multiple reasoning steps, it often helps to break it down into pieces. At each step, and as illustrated in Figure 6-18, the generative model is prompted to explore different solutions to the problem at hand. It then votes for the best solution and continues to the next step.10

![## Image Analysis: 074f0be04329c403933b838666619b7969f63d5f4c1633bf764d58c19218274e.jpg

**Conceptual Understanding:**
The image conceptually represents a 'Tree-of-thought' process, an iterative and branching problem-solving or generative reasoning paradigm. Its main purpose is to illustrate how an initial query or question ('Q:') can lead to the exploration of 'multiple paths' of intermediate thoughts, which are then systematically rated and refined to converge upon a 'Final answer.' The core idea is that by evaluating and selecting promising intermediate steps, a more robust and accurate final solution can be achieved, mimicking a structured thought process.

**Content Interpretation:**
The image illustrates a hierarchical, branching process for problem-solving or generating a solution, termed 'Tree-of-thought.' This process involves exploring multiple intermediate steps, or 'thoughts,' which are then evaluated and refined. The color-coding (green and pink/reddish shades) signifies a rating or selection mechanism, where green likely represents 'kept' or 'promising' thoughts, and pink/reddish represents 'pruned' or 'lowest rated' thoughts. The branching structure from an initial question ('Q:') to multiple intermediate thoughts, and then the convergence to a single 'Final answer,' depicts an iterative approach to problem-solving where generative models explore diverse lines of reasoning, evaluate their potential, and focus on the most effective paths.

**Key Insights:**
The main takeaway from this image is the conceptual framework of the 'Tree-of-thought' approach, which systematically explores and evaluates multiple reasoning paths. It teaches that complex problems can be broken down into intermediate 'thoughts' or steps ('Reasoning process (thought)'), and these steps can be rated ('Thoughts are rated') to identify the most promising directions. The visual representation highlights an iterative refinement process where less effective paths are discarded (implied by pink boxes) while more effective ones (green boxes) are pursued until a 'Final answer' is achieved. This methodology ensures a comprehensive exploration of possibilities while maintaining focus on productive lines of reasoning.

**Document Context:**
This image directly supports the document's section titled 'Tree-of-Thought: Exploring Intermediate Steps' by visually explaining how generative models leverage a tree-based structure to produce and rate intermediate thoughts. The accompanying text, 'By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned,' is perfectly illustrated by the diagram's branching paths, the 'Thoughts are rated' label, and the varying colors of the boxes representing the selection (keeping green) and pruning (pink) of thoughts, ultimately leading to a final, refined answer.

**Summary:**
The image illustrates the 'Tree-of-thought' process, a method for exploring multiple reasoning paths to arrive at a final answer. It begins with an initial 'Q:' or question. From this question, an initial set of 'Thoughts are rated,' represented by two solid green rectangular boxes, suggesting these are promising initial thoughts. These thoughts then branch into a 'Reasoning process (thought)' stage, where further intermediate thoughts are generated. This stage is depicted by four rectangular boxes: the first and third are light green, while the second and fourth are pink, visually representing a rating or pruning process. The paths continue to branch, leading to a third layer of intermediate thoughts, again represented by four rectangular boxes: the first and third are pink, and the second and fourth are solid green. This pattern suggests that across these layers, thoughts are continuously evaluated and refined, with more promising paths (green) being pursued and less promising ones (pink) being discarded or pruned. Ultimately, these refined paths converge into a single solid green rectangular box at the bottom, which is explicitly labeled 'Final answer'. The overall flow demonstrates a systematic exploration, evaluation, and selection of reasoning steps.](images/074f0be04329c403933b838666619b7969f63d5f4c1633bf764d58c19218274e.jpg)
Figure 6-18. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned.

This method is tremendously helpful when needing to consider multiple paths, like when writing a story or coming up with creative ideas. A disadvantage of this method is that it requires many calls to the generative models, which slows the application significantly. Fortunately, there has been a successful attempt to convert the tree-ofthought framework into a simple prompting technique.11

Instead of calling the generative model multiple times, we ask the model to mimic that behavior by emulating a conversation between multiple experts. These experts will question each other until they reach a consensus. An example of a tree-ofthought prompt is:

# Zero-shot tree-of-thought zeroshot_tot_prompt $=$ [

{"role": "user", "content": "Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results."} ]

We can use this prompt to explore how an LLM might respond to complex questions:

# Generate the output outputs $=$ pipe(zeroshot_tot_prompt) print(outputs[0]["generated_text"])

Expert 1: Step 1 - Start with the initial number of apples: 23 apples.

Expert 2: Step 1 - Subtract the apples used for lunch: $2 3 \ - \ 2 0 \ = \ 3$ apples remaining.

Expert 3: Step 1 - Add the newly bought apples: $3 + 6 = 9$ apples.

Expert 1: Step 2 - Confirm the final count: The cafeteria has 9 apples.

Expert 2: Step 2 - Review the calculations: 23 - $2 \Theta = 3$ , then $3 + 6 = 9$ . The calculations are correct.

Expert 3: Step 2 - Agree with the result: The cafeteria indeed has 9 apples.

All experts agree on the final count: The cafeteria has 9 apples.

We again get the correct answer but instead through a “discussion between experts.” It is interesting to see such a conservation between “experts” that demonstrates the creativity that comes with prompt engineering.

# Output Verification

Systems and applications built with generative models might eventually end up in production. When that happens, it is important that we verify and control the output of the model to prevent breaking the application and to create a robust generative AI application.

Reasons for validating the output might include:

Structured output

By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON.

Valid output

Even if we allow the model to generate structured output, it still has the capability to freely generate its content. For instance, when a model is asked to output either one of two choices, it should not come up with a third.

# Ethics

Some open source generative models have no guardrails and will generate out‐ puts that do not consider safety or ethical considerations. For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc.

# Accuracy

Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accu‐ rate, coherent, or free from hallucination.

Controlling the output of a generative model, as we explored with parameters like top_p and temperature, is not an easy feat. These models require help to generate consistent output conforming to certain guidelines.

Generally, there are three ways of controlling the output of a generative model:

Examples

Provide a number of examples of the expected output.

Grammar

Control the token selection process.

Fine-tuning

Tune a model on data that contains the expected output.

In this section, we will go through the first two methods. The third, fine-tuning a model, is left for Chapter 12 where we will go in depth into fine-tuning methods.

# Providing Examples

A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like. As we explored before, few-shot learning is a helpful technique that guides the output of the generative model. This method can be generalized to guide the structure of the output as well.

For example, let us consider an example where we want the generative model to create a character profile for an RPG game. We start by using no examples:

# Zero-shot learning: Providing no examples   
zeroshot_prompt $=$ [ {"role": "user", "content": "Create a character profile for an RPG game in   
JSON format."}

# Generate the output outputs $=$ pipe(zeroshot_prompt) print(outputs[0]["generated_text"])

\`\`\`json   
{   
"characterProfile": { "name": "Eldrin Stormbringer", "class": "Warlock", "race": "Half-Elf", "age": 27, "gender": "Male", "alignment": "Chaotic Good", "background": "Rogue", }, "attributes": { "strength": 10, "dexterity": 17, "constitution": 12, "intelligence": 12, "wisdom": 10, "charisma

The preceding truncated output is not valid JSON since the model stopped gener‐ ating tokens after starting the “charisma” attribute. Moreover, we might not want certain attributes. Instead, we can provide the model with a number of examples that indicate the expected format:

# One-shot learning: Providing an example of the output structure   
one_shot_template $=$ """Create a short character profile for an RPG game. Make   
sure to only use this format:   
{ "description": "A SHORT DESCRIPTION", "name": "THE CHARACTER'S NAME", "armor": "ONE PIECE OF ARMOR", "weapon": "ONE OR MORE WEAPONS"   
}   
"""   
one_shot_prompt $=$ [ {"role": "user", "content": one_shot_template}   
]

# Generate the output outputs $=$ pipe(one_shot_prompt) print(outputs[0]["generated_text"])

The model perfectly followed the example we gave it, which allows for more consis‐ tent behavior. This also demonstrates the importance of leveraging few-shot learning to improve the structure of the output and not only its content.

An important note here is that it is still up to the model whether it will adhere to your suggested format or not. Some models are better than others at following instructions.

# Grammar: Constrained Sampling

Few-shot learning has a big disadvantage: we cannot explicitly prevent certain output from being generated. Although we guide the model and give it instructions, it might still not follow it entirely.

Instead, packages have been rapidly developed to constrain and validate the output of generative models, like Guidance, Guardrails, and LMQL. In part, they leverage generative models to validate their own output, as illustrated in Figure 6-19. The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails.

![## Image Analysis: 623acde3e0490376e47a1e1f2a48a213588cf517ef46c744abfd0248d52c8102.jpg

**Conceptual Understanding:**
This image conceptually represents an automated system for validating and correcting the output of a Large Language Model (LLM). It illustrates a feedback loop mechanism designed to ensure that the LLM's generated text adheres to a specific structural "grammar" or format, such as JSON.

The main purpose of this diagram is to demonstrate a method for "constrained sampling" – that is, controlling the output of an LLM so it fits a predefined structure. It shows how an LLM can be used not only to generate content but also to critically evaluate that content against a set of rules and iteratively refine it until those rules are met. This enhances the reliability and usability of LLM-generated output by ensuring it is consistently formatted and parsable.

The key idea being communicated is the implementation of a self-correcting or self-validating system for LLMs, where one LLM (or a prompt to an LLM) checks the output of another (or its own prior output) for compliance with a specified format, entering an iterative refinement cycle until the output is satisfactory. This technique is crucial for applications requiring structured data from LLMs.

**Content Interpretation:**
The image depicts a process for validating and refining the output of a Large Language Model (LLM) to ensure it adheres to a specific format, in this case, JSON. 

**Processes:**
*   **Initial Generation:** A "Prompt" (Q:) is provided as input to an "LLM" to generate an initial response, represented by "A:".
*   **Iterative Evaluation:** The core process is an "Iterative process" labeled "Evaluate output." This process takes the current output "A:", applies a check using the explicit instruction "Check whether the following text adheres to the JSON format:", which is then executed by a second "LLM" (the checker). 
*   **Feedback Loop:** An arrow from the checker "LLM" loops back to the "A:" (current output) box within the iterative process, indicating that if the format check fails, the output is revised or regenerated, and the check is re-applied until successful.
*   **Final Output:** Once the output successfully adheres to the JSON format, it exits the iterative process and becomes the "Output" (A:).

**Concepts:**
*   **Constrained Output:** The diagram demonstrates how to constrain an LLM's free-form output to a specific structure (JSON). The text "Check whether the following text adheres to the JSON format:" explicitly states this constraint.
*   **Self-Correction/Iterative Refinement:** The iterative nature and the feedback loop illustrate a self-correction mechanism where the LLM's output is continuously refined until it meets the specified criteria.
*   **Modular LLM Usage:** The presence of two distinct "LLM" blocks suggests that different LLM instances or prompts can be specialized for generation and validation tasks.

**Relationships:**
*   **Generator-Checker Relationship:** The initial LLM acts as the content generator, while the second LLM serves as the format validator. This establishes a clear generator-checker dynamic.
*   **Sequential and Looping Flow:** The process demonstrates a sequential flow from prompt to initial output, followed by a looping, iterative flow for validation, concluding with a final output.

**Key Insights:**
**Main Takeaways:**
1.  **LLMs can perform iterative self-correction:** The diagram clearly shows that an LLM's output can be iteratively evaluated and refined by another LLM until it meets specified constraints. This is evidenced by the overarching label "Iterative process" and the feedback loop arrow from the checker LLM back to the output being evaluated.
2.  **Enforcing output constraints is critical for reliable and structured data:** The explicit instruction "Check whether the following text adheres to the JSON format:" highlights the importance of ensuring LLM outputs conform to predefined structures, which is essential for downstream processing and integration into other systems.
3.  **Modular LLM applications facilitate robust workflows:** The use of separate "LLM" entities for initial generation and subsequent validation suggests a modular approach where different LLM calls or instances can be dedicated to specific tasks, enhancing the overall system's reliability and control.

**Conclusions/Insights:**
*   Relying solely on an LLM for free-form generation may be insufficient when specific, structured output formats are required.
*   An LLM-powered iterative feedback loop is an effective strategy for achieving high-quality, format-compliant outputs, overcoming potential initial deviations from specified rules.
*   This approach demonstrates how to leverage LLMs to bridge the gap between their natural language generation capabilities and the need for structured, machine-readable data.

**Document Context:**
This image directly illustrates the concept of "Grammar: Constrained Sampling" as indicated by the document context. It visually explains how an LLM's output can be 'constrained' to adhere to specific 'rules' or 'grammar' (like JSON format) through an iterative checking and refinement process. The text following the image, "Figure 6-19. Use an LLM to check whether the output correctly follows our rules," perfectly aligns with the diagram's depiction of an LLM being used to validate adherence to a format (rules). The diagram provides a concrete example of how to implement such a constrained sampling mechanism in practice, making abstract concepts tangible for the reader.

**Summary:**
This diagram, titled "Evaluate output" and described as an "Iterative process," illustrates a workflow designed to ensure a Large Language Model (LLM) produces output that strictly adheres to a predefined format, specifically JSON. 

The process begins with a "Prompt" (labeled "Q:") which is fed into an "LLM." This initial LLM generates a response, depicted as "A:".

This initial response then enters an "Iterative process" for evaluation. Within this iterative loop, the current output "A:" is passed to a specialized check. This check involves another instruction: "Check whether the following text adheres to the JSON format:". The text to be checked is represented by "A:". This format check is performed by a second "LLM" (acting as a checker or validator).

Crucially, the diagram shows an arrow from this checker "LLM" looping back to the "A:" box within the iterative process. This signifies a feedback mechanism: if the checker LLM determines that the output does *not* adhere to the JSON format, the process repeats. The output is presumably revised or regenerated by the initial LLM (or corrected through further LLM interaction) and then re-checked. This iterative cycle continues until the checker LLM confirms that the output "A:" correctly follows the "JSON format".

Once the output successfully passes this iterative format validation, it exits the "Evaluate output" process and becomes the final "Output" (also labeled "A:"). This entire flow demonstrates a powerful technique for using LLMs not just for content generation, but also for enforcing structural constraints on that content, making the output more reliable and suitable for downstream processing.](images/623acde3e0490376e47a1e1f2a48a213588cf517ef46c744abfd0248d52c8102.jpg)
Figure 6-19. Use an LLM to check whether the output correctly follows our rules.

Similarly, as illustrated in Figure 6-20, this validation process can also be used to control the formatting of the output by generating parts of its format ourselves as we already know how it should be structured.

![## Image Analysis: 923584a2aa89b99f0ddf335f8c0de8f2ba5fd3aedcc73758a3b47c9db203ede7.jpg

**Conceptual Understanding:**
This image conceptually represents a process of 'constrained generation' or 'template-based completion' using a Large Language Model (LLM). Its main purpose is to illustrate how an LLM can be directed to generate only specific, missing data points (highlighted placeholders) within a predefined data structure (a JSON-like object), rather than generating the entire structure or free-form text. The image demonstrates a controlled interaction with an LLM for structured data population.

**Content Interpretation:**
The image illustrates a specific method of constrained sampling or generation using a Large Language Model (LLM). It demonstrates how an LLM can be used to generate only missing or unknown pieces of information within a pre-defined data structure, rather than generating the entire structure from scratch. The 'Expected format' acts as a template, specifying the structure and identifying the fields ('id', 'height', 'name', 'age') for which the LLM is responsible for generating values. The LLM's role is to provide the concrete data ('0', '1.81 cm', 'Vincent', '34') that fits into these specified slots. The final output is a complete data structure where the LLM-generated content is seamlessly integrated, demonstrating a focused and constrained generation approach. This highlights a use case where the LLM acts as a 'smart' data filler for specific, identified gaps in structured data.

**Key Insights:**
The main takeaway is that LLMs can be effectively used for constrained generation, specifically to fill in predefined placeholders within a structured format. This is evidenced by the 'Expected format' containing placeholders like '{id}', '{height}', '{name}', '{age}', and the instruction 'Only generate highlighted text'. The LLM's output consists only of the values for these placeholders ('0', '1.81 cm', 'Vincent', '34'), which are then inserted into the final structure. This approach allows for precise control over LLM output, ensuring that only the truly 'unknown' pieces of information are generated, which is a key aspect of 'Constrained Sampling' for maintaining data integrity and consistency.

**Document Context:**
This image directly supports the document's section on 'Grammar: Constrained Sampling' and specifically the accompanying text, 'Figure 6-20. Use an LLM to generate only the pieces of information we do not know beforehand.' It visually demonstrates the concept of using an LLM in a highly controlled manner to fill in specific, unknown data points within a predefined structure, rather than freely generating text. This is crucial for applications requiring structured output and data integrity, where only certain parts of the data are dynamic and need LLM generation.

**Summary:**
This diagram illustrates a process for using a Large Language Model (LLM) to fill in specific, unknown pieces of information within a predefined data structure. The process begins with an 'Expected format' which is a JSON-like structure containing placeholders for 'id', 'height', 'name', and 'age'. Only the text within these placeholders (highlighted in the original image) is intended for generation by the LLM. The LLM then processes this request, and its output consists of the generated values: '0', '1.81 cm', 'Vincent', and '34'. Finally, these generated values are integrated into the original 'Expected format', replacing the placeholders, to produce a complete JSON output with the specific, newly generated data points.](images/923584a2aa89b99f0ddf335f8c0de8f2ba5fd3aedcc73758a3b47c9db203ede7.jpg)
Figure 6-20. Use an LLM to generate only the pieces of information we do not know beforehand.

This process can be taken one step further and instead of validating the output we can already perform validation during the token sampling process. When sampling tokens, we can define a number of grammars or rules that the LLM should adhere to when choosing its next token. For instance, if we ask the model to either return “positive,” “negative,” or “neutral” when performing sentiment classification, it might still return something else. As illustrated in Figure 6-21, by constraining the sampling process, we can have the LLM only output what we are interested in. Note that this is still affected by parameters such as top_p and temperature.

![## Image Analysis: 2b4b32c2720ee40f380ce3a7d26c3e475e3aa5aedf671e408704877d5ba7caf3.jpg

**Conceptual Understanding:**
The image conceptually represents the process of 'constrained sampling' within a language model, specifically in the context of sentiment classification. Its main purpose is to visually demonstrate how the selection of an output token can be limited to a predefined set of 'legal' options, overriding the raw, unconstrained probabilities of all possible tokens. It communicates the key idea that by applying a constraint, the model is forced to choose from a specific subset of options, such as "positive, neutral, or negative" for sentiment, thereby guiding its output to fit a desired structure or category, regardless of other potentially higher-probability but 'illegal' tokens.

**Content Interpretation:**
The image demonstrates the application of 'constrained sampling' within a natural language processing task, specifically sentiment classification. It depicts a scenario where a language model is prompted to classify a sentence's sentiment, but its output is restricted to a predefined set of tokens. The process shows an input sentence "What a great movie!" which needs to be classified. The output (labeled "A:") is generated by a sampling mechanism that, despite the underlying probabilities of various sentiment tokens (e.g., "amazing," "horrible," "awful"), is constrained to select only from a specific subset: "positive," "neutral," or "negative." This is visually represented by a bar chart where these three 'legal' options are shown with distinct, darker red bars, indicating they are the active choices, while other options like "amazing" or "horrible" (even if they might have a higher unconstrained probability like "amazing" appears to have) are visually de-emphasized with lighter red bars, indicating they are excluded from the sampling process. The ellipses (...) also suggest that there are other possible tokens not explicitly listed but which are also being ignored by the constraint. The text "Only sample from 'legal' options" explicitly states this constraint.

**Key Insights:**
The main takeaway from this image is that constrained sampling allows for precise control over the output of a language model by restricting the set of permissible tokens from which it can sample. This mechanism ensures that the model's response adheres to a predefined "grammar" or set of allowed options, even if the model's unconstrained probabilities might favor other tokens. For instance, the prompt asks for classification into "positive, neutral, or negative." The knowledge extracted is that even though "amazing" might have the highest raw probability, the system *will not* choose it because it is not among the "legal" options. Instead, it will choose from "positive," "neutral," or "negative" based on their (potentially re-normalized) probabilities within the constrained set. This is critical for tasks requiring structured or specific outputs, preventing the model from generating responses outside the desired scope.

**Document Context:**
This image directly illustrates the concept of 'constrained sampling' as discussed in the "Grammar: Constrained Sampling" section of the document. It provides a concrete, visual example of how a language model's token selection can be limited to a specific, predefined set of 'legal' options, even when other tokens might have higher raw probabilities. This enhances comprehension of the topic by showing the practical implications of applying grammar constraints during the sampling phase, ensuring that the model adheres to desired output formats or categories. The image's content directly explains the text after it, which states, "Constrain the token selection to only three possible tokens: 'positive,' 'neutral,' and 'negative.'"

**Summary:**
The image illustrates the concept of constrained sampling in the context of classifying a sentence's sentiment. It starts with a question box labeled "Q:" asking to classify the sentence "What a great movie!" into "positive, neutral, or negative." Below this question is an answer box labeled "A:" with a dotted outline, indicating where the sampled output would appear. An arrow connects this answer box to a bar chart on the right, which visualizes the probabilities of various tokens, but specifically highlights the effect of constrained sampling. The bar chart is titled "Constrained sampling" with a subtitle "Only sample from "legal" options." The bars are labeled vertically with potential token classifications and their corresponding probabilities. The tokens listed, from highest to lowest probability depicted by bar length and color intensity (darker red indicating higher probability), are: "amazing", "positive", "..." (representing other options), "neutral", "..." (representing other options), "horrible", "negative", and "awful". The "positive", "neutral", and "negative" bars are distinctly highlighted in a darker red compared to others, indicating they are the "legal" options, while "amazing", "horrible", and "awful" are lighter, indicating they are not considered for sampling. The x-axis is labeled "probabilities". The diagram clearly shows that even if a token like "amazing" might have a higher probability, the constrained sampling mechanism limits the output to only the pre-defined "positive," "neutral," or "negative" options.](images/2b4b32c2720ee40f380ce3a7d26c3e475e3aa5aedf671e408704877d5ba7caf3.jpg)
Figure 6-21. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and “negative.”

Let us illustrate this phenomenon with llama-cpp-python, a library similar to trans formers that we can use to load in our language model. It is generally used to efficiently load and use compressed models (through quantization; see Chapter 12) but we can also use it to apply a JSON grammar.

We load the same model we used throughout this chapter but use a different format instead, namely GGUF. llama-cpp-python expects this format, which is generally used for compressed (quantized) models.

Since we are loading a new model, it is advised to restart the notebook. That will clear any previous models and empty the VRAM. You can also run the following to empty the VRAM:

import gc   
import torch   
del model, tokenizer, pipe   
# Flush memory   
gc.collect()   
torch.cuda.empty_cache()

Now that we have cleared the memory, we can load Phi-3. We set n_gpu_layers to -1 to indicate that we want all layers of the model to be run from the GPU. The n_ctx refers to the context size of the model. The repo_id and filename refer to the Hugging Face repository where the model resides:

from llama_cpp.llama import Llama

# Load Phi-3   
llm $=$ Llama.from_pretrained( repo_id="microsoft/Phi-3-mini-4k-instruct-gguf", filename="\*fp16.gguf", n_gpu_layers $\ n = -$ , n_ctx $\varXi$ 2048, verbose=False   
)

To generate the output using the internal JSON grammar, we only need to specify the response_format as a JSON object. Under the hood, it will apply a JSON grammar to make sure the output adheres to that format.

To illustrate, let’s ask the model to create an RPG character in JSON format to be used in a Dungeons & Dragons session:

# Generate output   
output $=$ llm.create_chat_completion( messages=[ {"role": "user", "content": "Create a warrior for an RPG in JSON for   
mat."}, ], response_format={"type": "json_object"}, temperature $\mathbf { \Psi } = \mathbf { \Psi }$ ,   
)['choices'][0]['message']["content"]

To check whether the output actually is JSON, we can attempt to process it as such:

# import json

# Format as json   
json_output $=$ json.dumps(json.loads(output), indent ${ = } 4$ )   
print(json_output) war-torn land. Witnessing the brutality and suffering caused by conflict, he dedicated his life to becoming a formidable warrior who could protect those unable to defend themselves."   
}

![## Image Analysis: 983db9a812b9c4dd9211445d5cbcfe95394338d894a5575907baec0d56c54816.jpg

**Conceptual Understanding:**
This image conceptually represents a structured data profile for a fictional character, "Eldrin Stormbringer", in what appears to be a role-playing game (RPG) or similar fantasy setting. The main purpose is to compactly convey all essential statistical, skill-based, equipment-related, and basic background information about this specific character. It communicates the character's identity, capabilities, and gear in a standardized, machine-readable format.

**Content Interpretation:**
The image presents a detailed character sheet in a JSON-like structure, outlining various systems related to character definition in a game or simulation:

*   **Character Identity and Core Stats**: The character is named "Eldrin Stormbringer" and belongs to the "Warrior" class, currently at "level": 10.
    *   **Evidence**: `"name": "Eldrin Stormbringer", "class": "Warrior", "level": 10,`
*   **Attributes System**: Six primary attributes are listed, providing a numerical representation of the character's inherent capabilities. "Strength" (18) and "Constitution" (16) are notably high, aligning with a "Warrior" class, while "Intelligence" (9) is comparatively lower.
    *   **Evidence**: `"attributes": { "strength": 18, "dexterity": 12, "constitution": 16, "intelligence": 9, "wisdom": 14, "charisma": 10 }`
*   **Skills System**: Detailed sub-systems for "melee_combat", "defense", and "endurance" show specific proficiencies and their values.
    *   "Melee Combat" highlights high "weapon_mastery" (20) and a substantial "armor_class" (18) with "hit_points" (35).
    *   "Defense" indicates a strong "shield_skill" (17) and a very high "block_chance" (90), suggesting a robust defensive playstyle.
    *   "Endurance" includes "health_regeneration" (2) and "stamina" (30), vital for sustained combat or actions.
    *   **Evidence**: The entire `"skills"` block and its nested components.
*   **Equipment System**: The character is equipped with "Ironclad Armor" and a "Steel Greatsword". The armor provides a "defense_bonus" (15), and the greatsword deals "damage" (8) with a "critical_chance" (20).
    *   **Evidence**: The entire `"equipment"` array with its two item objects.
*   **Background Information**: A narrative snippet provides context on the character's origins, mentioning a "small village on the outskirts of a torn land" and "witnessing the brutality and suffering caused by conflict". This suggests a backstory that might justify their warrior path or motivations.
    *   **Evidence**: `"background": "Eldrin grew up in a small village on the outskirts of a torn land. Witnessing the brutality and suffering caused by conflict, he` (truncated text).

**Key Insights:**
This image provides comprehensive data defining a combat-oriented character within a structured system.

*   **Main Takeaway 1: Eldrin Stormbringer is a well-developed, Level 10 Warrior.** The character's name, class, and level are explicitly stated, establishing their core identity within the game world.
    *   **Evidence**: `"name": "Eldrin Stormbringer", "class": "Warrior", "level": 10,`
*   **Main Takeaway 2: Eldrin is built for robust physical combat and defense.** High "strength", "constitution", "weapon_mastery", "armor_class", "shield_skill", and a very high "block_chance" indicate a front-line combatant capable of both dealing and withstanding damage.
    *   **Evidence**: `"strength": 18, "constitution": 16, "weapon_mastery": 20, "armor_class": 18, "shield_skill": 17, "block_chance": 90`
*   **Main Takeaway 3: The character possesses basic regenerative and stamina capabilities.** "Health regeneration" and "stamina" are defined, suggesting resource management is a factor in gameplay.
    *   **Evidence**: `"health_regeneration": 2, "stamina": 30`
*   **Main Takeaway 4: The equipment enhances both defense and offense.** "Ironclad Armor" provides a significant "defense_bonus", while the "Steel Greatsword" offers "damage" and a "critical_chance", rounding out the character's combat readiness.
    *   **Evidence**: `"name": "Ironclad Armor", "type": "Armor", "defense_bonus": 15`, and `"name": "Steel Greatsword", "type": "Weapon", "damage": 8, "critical_chance": 20`
*   **Main Takeaway 5: Eldrin's backstory is rooted in conflict and hardship.** The background snippet hints at a formative experience in a "torn land" marked by "brutality and suffering", providing a potential motivation for becoming a warrior.
    *   **Evidence**: `"background": "Eldrin grew up in a small village on the outskirts of a torn land. Witnessing the brutality and suffering caused by conflict, he`

**Document Context:**
Given the document context "Section: Format as json", this image serves as a concrete example of how character data, likely for a game or simulation, is structured using the JSON format. It demonstrates the hierarchical and key-value pair nature of JSON to organize complex information about an entity. It would likely appear in a technical document, developer guide, or data specification illustrating data schema or sample data for character profiles.

**Summary:**
This image displays a meticulously structured data profile, presented in a JSON-like format, for a fictional character named "Eldrin Stormbringer." The profile begins by identifying Eldrin as a "Warrior" of "level": 10.

The character's core capabilities are detailed under "attributes", which lists six key statistics: "strength": 18, "dexterity": 12, "constitution": 16, "intelligence": 9, "wisdom": 14, and "charisma": 10. These values numerically represent Eldrin's innate abilities.

Further defining Eldrin's combat prowess are the "skills," categorized into three sub-sections. "melee_combat" includes "weapon_mastery": 20, an "armor_class": 18, and "hit_points": 35. The "defense" section details a "shield_skill": 17 and a "block_chance": 90, indicating a strong defensive orientation. "endurance" covers sustainment with "health_regeneration": 2 and "stamina": 30.

Eldrin's "equipment" is listed as an array containing two items. The first is "Ironclad Armor," specified as "type": "Armor" with a "defense_bonus": 15. The second item is a "Steel Greatsword," classified as "type": "Weapon," providing "damage": 8 and a "critical_chance": 20.

Finally, a "background" section initiates a narrative description: "Eldrin grew up in a small village on the outskirts of a torn land. Witnessing the brutality and suffering caused by conflict, he..." This provides a glimpse into the character's origins and potential motivations, though the text is cut off.

In essence, the image provides a comprehensive and detailed snapshot of Eldrin Stormbringer's vital statistics, abilities, gear, and initial backstory, all organized in a clear, nested data structure.](images/983db9a812b9c4dd9211445d5cbcfe95394338d894a5575907baec0d56c54816.jpg)

The output is properly formatted as JSON. This allows us to more confidently use generative models in applications where we expect the output to adhere to certain formats.

# Summary

In this chapter, we explored the basics of using generative models through prompt engineering and output verification. We focused on the creativity and potential com‐ plexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropriate for different use cases.

We further explored advanced prompt engineering techniques such as in-context learning and chain-of-thought. These methods involve guiding generative models to reason through complex problems by providing examples or phrases that encourage step-by-step thinking thereby mimicking human reasoning processes.

Overall, this chapter demonstrated that prompt engineering is a crucial aspect of working with LLMs, as it allows us to effectively communicate our needs and prefer‐ ences to the model. By mastering prompt engineering techniques, we can unlock some of the potential of LLMs and generate high-quality responses that meet our requirements.

The next chapter will build upon these concepts by exploring more advanced tech‐ niques for leveraging generative models. We will go beyond prompt engineering and explore how LLMs can use external memory and tools.

# Advanced Text Generation Techniques and Tools

In the previous chapter, we saw how prompt engineering can do wonders for the accuracy of your text-generation large language model (LLM). With just a few small tweaks, these LLMs are guided toward more purposeful and accurate answers. This showed how much there is to gain using techniques that do not fine-tune the LLM but instead use the LLM more efficiently, such as the relatively straightforward prompt engineering.

In this chapter, we will continue this train of thought. What can we do to further enhance the experience and output that we get from the LLM without needing to fine-tune the model itself?

Fortunately, a great deal of methods and techniques allow us to further improve what we started with in the previous chapter. These more advanced techniques lie at the foundation of numerous LLM-focused systems and are, arguably, one of the first things users implement when designing such systems.

In this chapter, we will explore several such methods and concepts for improving the quality of the generated text:

Model I/O Loading and working with LLMs

Memory Helping LLMs to remember

Combining complex behavior with external tools

Chains

Connecting methods and modules

These methods are all integrated with the LangChain framework that will help us easily use these advanced techniques throughout this chapter. LangChain is one of the earlier frameworks that simplify working with LLMs through useful abstractions. Newer frameworks of note are DSPy and Haystack. Some of these abstractions are illustrated in Figure 7-1. Note that retrieval will be discussed in the next chapter.

![## Image Analysis: 6e51b28a08b83e316bcc0a0e1567fa40fe430e22570ec3425164cccc1d8715fd.jpg

**Conceptual Understanding:**
The image conceptually represents the architecture and core components of LangChain, a framework designed for developing applications powered by large language models (LLMs). It emphasizes modularity and the ability to link these modules in a sequence. The main purpose is to illustrate how LangChain breaks down the complex task of building LLM applications into manageable, reusable, and interconnected components. It conveys that by "chaining together" these modules, developers can create sophisticated LLM systems. Key ideas communicated are modularity, reusability, composability, and the concept of "chaining" different functionalities to achieve advanced LLM behaviors such as memory, data retrieval, and agentic capabilities.

**Content Interpretation:**
The image shows four key categories of modules within the LangChain framework: Model I/O, Memory, Retrieval, and Agents. These categories represent fundamental functionalities required for building advanced LLM applications. 1. Model I/O: This category, containing "Prompts," "LLMs," and "Output parser," signifies the foundational layer for interacting with LLMs. It addresses how input is fed to the model (Prompts), the core computational unit (LLMs), and how the model's responses are processed (Output parser). 2. Memory: Chained to Model I/O, the "Memory" module, with "Conversation memory" and "Summarization," highlights the importance of statefulness and context in LLM applications. "Conversation memory" allows the LLM to remember past interactions, while "Summarization" manages long histories. 3. Retrieval: Following Memory, the "Retrieval" module, listing "Embeddings," "Documents," and "Vector database," addresses the need for LLMs to access and utilize external information. "Embeddings" are for semantic search, "Documents" for data handling, and "Vector database" for efficient storage and querying. 4. Agents: The final module, "Agents," featuring "Autonomous," "ReAct," and "Conversation," represents the capability to enable LLMs to perform more complex, goal-oriented tasks. "Autonomous" agents make independent decisions, "ReAct" is a reasoning and acting paradigm, and "Conversation" agents are for interactive dialogues. The visual "chains" linking these modules (Model I/O -> Memory -> Retrieval -> Agents) are significant, conveying the central idea of LangChain: complex LLM systems are built by composing these distinct, yet interconnected, functionalities. This structure, supported by the explicit text "Modules chained together," demonstrates a pipeline where outputs from one stage can feed into the input of another, enabling synergistic effects for advanced AI capabilities.

**Key Insights:**
The main takeaways and insights from this image are: 1. LangChain is a modular framework for LLM applications, as indicated by the text "LangChain" and "Modules chained together," and the distinct categories like "Model I/O," "Memory," "Retrieval," and "Agents." 2. Building advanced LLM applications involves more than just direct LLM interaction. The inclusion of "Memory," "Retrieval," and "Agents" modules, alongside "Model I/O," highlights the necessity for functionalities such as managing conversation history ("Conversation memory"), accessing external data ("Embeddings," "Documents," "Vector database"), and enabling goal-oriented behaviors ("Autonomous," "ReAct"). 3. The power of LangChain stems from its ability to combine these modules. The visual "chain links" between the modules and the phrase "Modules chained together" explicitly demonstrate that these components are designed for integration, allowing for complex workflows where outputs from one stage, e.g., an LLM's response, can be stored in memory, memory can inform a retrieval step, and retrieved information can then be used by an agent to act. 4. LangChain provides specific tools and paradigms to address common LLM challenges, with concrete examples like "Output parser" for structured output, "Summarization" for context management, "Vector database" for efficient knowledge retrieval, and "ReAct" as a specialized agent design pattern.

**Document Context:**
This image, titled "LangChain" and described as "Modules chained together," directly supports the document's section on "Advanced Text Generation Techniques and Tools." It provides a high-level architectural overview of LangChain, which the accompanying text identifies as "a complete framework for using LLMs" with "modular components that can be chained together to allow for complex LLM systems." The diagram visually illustrates this concept, breaking down the framework into its fundamental building blocks and showing how they interoperate. It serves as a foundational visual aid for understanding the capabilities and structure of LangChain discussed in the surrounding text.

**Summary:**
The image presents a diagram illustrating the LangChain framework, a system designed to build complex applications using Large Language Models (LLMs). The core concept, highlighted by the text "LangChain" and "Modules chained together," is that various functionalities are provided as modular components that can be linked in sequence to create sophisticated LLM-powered solutions. The framework is structured into four main categories, presented as distinct modules connected by chain links: 1. Model I/O: This initial module focuses on the fundamental interactions with LLMs. It includes: • Prompts: The instructions or queries given to an LLM. • LLMs: The Large Language Models themselves, which process the prompts. • Output parser: A component for interpreting and structuring the responses received from the LLMs. This module forms the bridge between a user or system and the underlying language model. 2. Memory: Chained to the Model I/O module, the Memory module is responsible for retaining information over time, which is crucial for multi-turn interactions or applications requiring context. It encompasses: • Conversation memory: Stores the history of a dialogue, allowing the LLM to recall previous exchanges. • Summarization: Provides mechanisms to condense information, often used to manage lengthy conversation histories or documents efficiently. 3. Retrieval: Following the Memory module, the Retrieval module enables the LLM system to access and incorporate external information, going beyond its initial training data. Its components include: • Embeddings: Numerical representations of text that capture semantic meaning, used for efficient similarity search. • Documents: Refers to the handling and processing of various forms of textual data. • Vector database: A specialized database optimized for storing and querying vector embeddings, facilitating rapid retrieval of relevant information. 4. Agents: The final module in this chain, Agents, empowers the LLM system to perform more complex, goal-oriented tasks by enabling it to reason and act. This module includes different types of agents: • Autonomous: Agents capable of making independent decisions and executing actions without constant human intervention. • ReAct: A specific paradigm (Reasoning and Acting) where agents iteratively reason about the current state and decide on an action. • Conversation: Agents specifically designed to engage in dynamic and interactive dialogues, often incorporating memory and retrieval capabilities. In essence, the diagram illustrates a progression of capabilities, starting from basic LLM interaction, then adding memory for context, retrieval for external knowledge, and finally, agentic capabilities for complex task execution. The "chained together" visual metaphor strongly conveys the framework's design philosophy of composability, where these modules work synergistically to create advanced LLM applications.](images/6e51b28a08b83e316bcc0a0e1567fa40fe430e22570ec3425164cccc1d8715fd.jpg)
Figure 7-1. LangChain is a complete framework for using LLMs. It has modular compo‐ nents that can be chained together to allow for complex LLM systems.

Each of these techniques has significant strengths by themselves but their true value does not exist in isolation. It is when you combine all of these techniques that you get an LLM-based system with incredible performance. The culmination of these techniques is truly where LLMs shine.

# Model I/O: Loading Quantized Models with LangChain

Before we can make use of LangChain’s features to extend the capabilities of LLMs, we need to start by loading our LLM. As in previous chapters, we will be using Phi-3 but with a twist; we will use a GGUF model variant instead. A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM.

Bits, a series of 0s and 1s, represent values by encoding them in binary form. More bits result in a wider range of values but requires more memory to store those values, as shown in Figure 7-2.

![## Image Analysis: 82a58fffa294dcba2d1f03eb9265ebb6a931f541d05668988c5183c9d08de5b5.jpg

**Conceptual Understanding:**
This image conceptually represents the storage and interpretation of floating-point numbers using different bit lengths, specifically 32-bit and 16-bit formats. The main purpose is to illustrate how reducing the number of bits used to represent a floating-point number, particularly in its fractional part (mantissa), directly leads to a decrease in numerical precision. It highlights the trade-off between memory usage (fewer bits) and accuracy (lower precision).

**Content Interpretation:**
The image demonstrates the concept of floating-point precision as applied to the mathematical constant Pi (approximately 3.14159265...).

**Float 32-bit Representation (High Precision):**
*   It shows a sequence of 32 binary digits (`0` for sign, `10000000` for exponent, `1001001000001111111011011` for mantissa). This is consistent with a standard IEEE 754 single-precision float.
*   The formula `(-1)⁰ × 2¹ × 1.5707964` is presented, which is a common way to express a floating-point number (sign bit S, exponent E, mantissa M) as `(-1)^S * 2^(E-bias) * M`. Here, `S=0` (from the green `0`), `E-bias = 1` (from the `2¹`), and `M = 1.5707964`.
*   The calculation results in `3.1415927`, which is a highly accurate approximation of Pi.
*   The label `High precision` directly describes the accuracy achieved with the 32-bit representation. The presence of 23 bits in the mantissa (blue boxes) allows for this higher precision.
*   An arrow points to the last bit of the 32-bit sequence, labeled `1 bit`, likely emphasizing the granularity of the representation.

**Float 16-bit Representation (Low Precision):**
*   It shows a sequence of 19 binary digits (`0` for sign, `10000000` for exponent, `1001001000` for mantissa). While labeled "Float 16-bit," the visual representation explicitly shows 19 bits (1 green + 8 red + 10 blue). The critical aspect is the *reduced* number of bits in the mantissa compared to the 32-bit example, rather than the exact total bit count matching a standard 16-bit float. Specifically, the mantissa here has 10 bits compared to 23 bits in the 32-bit version.
*   The corresponding formula `(-1)⁰ × 2¹ × 1.571` is given. Notice that the mantissa value `1.571` is truncated compared to `1.5707964` in the 32-bit version.
*   The calculation results in `= 3.141`, which is a less accurate approximation of Pi.
*   The label `Low precision` directly indicates the reduced accuracy. The reduction in the number of bits available for the mantissa (blue boxes) directly leads to this loss of precision, as fewer decimal places can be accurately stored.

The extracted text `3.1415927 High precision` versus `3.141 Low precision` clearly demonstrates the quantitative difference in accuracy when the bit length for the floating-point number is reduced. The change in the mantissa value (`1.5707964` to `1.571`) is the direct textual evidence of the truncation and loss of detail that causes the lower precision.

**Key Insights:**
The main takeaways and insights from this image are:

*   **Impact of Bit Length on Precision:** The most significant lesson is that the number of bits allocated to represent a floating-point number directly impacts its precision. More bits allow for a more accurate representation of real numbers, while fewer bits lead to a less accurate, truncated representation.
    *   *Textual Evidence:* Comparing `3.1415927 High precision` (Float 32-bit) with `3.141 Low precision` (Float 16-bit). The explicit labels "High precision" and "Low precision" unequivocally state this conclusion.
*   **Approximation of Real Numbers:** Floating-point numbers are approximations of real numbers. The image illustrates how different bit lengths result in different levels of approximation for a constant like Pi.
    *   *Textual Evidence:* The calculations for both `Float 32-bit` (`= 3.1415927`) and `Float 16-bit` (`= 3.141`) are approximations of Pi, with the 32-bit version being closer to the true value.
*   **Quantization Effect:** Reducing the number of bits (quantization) inherently sacrifices detail. In this case, the mantissa of the 16-bit float (`1.571`) is a truncated version of the 32-bit float's mantissa (`1.5707964`), leading to the less precise final result.
    *   *Textual Evidence:* The difference between `1.5707964` in the 32-bit calculation and `1.571` in the 16-bit calculation explicitly shows the truncation.

**Document Context:**
This image fits within the document's broader narrative, specifically in a section about "Model I/O: Loading Quantized Models with LangChain," and is referred to as "Figure 7-2. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits." This context suggests the image serves to explain the fundamental concept of numerical precision in the context of model quantization. When machine learning models are "quantized," their parameters (which are often floating-point numbers) are converted to lower-precision formats (like 16-bit or even 8-bit integers) to reduce memory footprint and improve computational efficiency. This image provides a clear, simple illustration of the *effect* of such quantization on numerical accuracy, using Pi as a relatable example. It lays the groundwork for understanding why quantized models might be faster but potentially less accurate.

**Summary:**
The image visually compares how the mathematical constant Pi is represented using two different floating-point bit lengths: 32-bit and 16-bit. It clearly demonstrates the trade-off between the number of bits used for storage and the resulting precision of the number.

At the top, the "Float 32-bit" section illustrates a high-precision representation. It shows a complete 32-bit binary sequence, which is typically divided into a sign bit, an exponent, and a mantissa. The first bit, a `0` (in green), indicates the sign. The next eight bits, `10000000` (in red), represent the exponent. The remaining twenty-three bits, `1001001000001111111011011` (in blue), form the mantissa. Below this binary representation, a mathematical formula `(-1)⁰ × 2¹ × 1.5707964` is presented. This formula calculates the value of the floating-point number, yielding `3.1415927`. This result is explicitly labeled as `High precision`, accurately reflecting its closeness to the true value of Pi. An arrow points to the very last bit of the entire 32-bit sequence, labeled `1 bit`, to emphasize the smallest unit of information.

Below this, the "Float 16-bit" section shows a lower-precision representation. It presents a shorter binary sequence, visually consisting of 19 bits: a sign bit `0` (green), an 8-bit exponent `10000000` (red), and a 10-bit mantissa `1001001000` (blue). Although labeled "Float 16-bit", the visual representation of the mantissa is significantly shorter than the 32-bit version. The corresponding mathematical formula for this representation is `(-1)⁰ × 2¹ × 1.571`. Notice that the mantissa value `1.571` here is a truncated version of the `1.5707964` used in the 32-bit calculation, meaning it has fewer significant digits. Consequently, the calculated value is `= 3.141`, which is explicitly labeled as `Low precision`. This clearly shows that by reducing the number of bits, particularly those forming the mantissa (the fractional part), the numerical accuracy of the represented number decreases significantly.

In summary, the image effectively demonstrates that while fewer bits might save memory or computational resources, they inevitably lead to a loss of detail and precision when representing complex numbers, as evidenced by the less accurate approximation of Pi in the 16-bit example.](images/82a58fffa294dcba2d1f03eb9265ebb6a931f541d05668988c5183c9d08de5b5.jpg)
Figure 7-2. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits.

Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. This comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original.

To illustrate quantization, consider this analogy. If asked what the time is, you might say “14:16,” which is correct but not a fully precise answer. You could have said it is $^ { \infty } 1 4 ! 1 6$ and 12 seconds” instead, which would have been more accurate. However, mentioning seconds is seldom helpful and we often simply put that in discrete numbers, namely full minutes. Quantization is a similar process that reduces the precision of a value (e.g., removing seconds) without removing vital information (e.g., retaining hours and minutes).

In Chapter 12, we will further discuss how quantization works under the hood. You can also see a full visual guide to quantization in $\mathrm { { ^ { * } A } }$ Visual Guide to Quantization” by Maarten Grootendorst. For now, it is important to know that we will use an 8-bit variant of Phi-3 compared to the original 16-bit variant, cutting the memory requirements almost in half.

![## Image Analysis: 1ac4802e6f22a7602ed19fd4729af718e9e830e584e52d723bde55ea6b8fa850.jpg

**Conceptual Understanding:**
This image conceptually represents a stylized logo or an emblem. Its main purpose is to serve as a visual identifier, likely for a brand, a project, or a specific entity. The key idea being communicated is visual branding or a unique mark, without conveying any explicit informational content through text or a diagrammatic structure.

**Content Interpretation:**
The image is a graphic representation, specifically a logo or an emblem, featuring a stylized green animal within a square outline. The animal, which resembles a squirrel or a lemur with a distinctly curled tail, is depicted in a dynamic posture. There are no processes, concepts, relationships, or systems being explicitly shown or implied beyond the visual identity conveyed by the logo itself. No data, trends, or information are presented. All interpretations are based solely on the visual elements as no text was extracted from the image.

**Key Insights:**
The main takeaway from this image is that it is a visual identifier or a brand emblem. As there is no text or data, no specific conclusions or insights regarding the technical subject of 'Model I/O: Loading Quantized Models with LangChain' can be drawn from the image itself. The image's primary function is likely to provide a visual marker rather than convey specific information or lessons related to the document's content.

**Document Context:**
Given the document context section title 'Model I/O: Loading Quantized Models with LangChain,' the image of a green animal logo appears to be conceptually unrelated to the technical subject matter. It likely serves as a decorative element, a branding mark for an organization or project, or an arbitrary visual break within the document, rather than directly illustrating or supporting the technical content described in the section title. Its relevance is primarily as a visual identifier or aesthetic element within the document.

**Summary:**
The image displays a stylized, solid green silhouette of an animal, which appears to be either a squirrel or a lemur. The animal is shown in profile, facing right, with its body hunched and all four paws visible. Its prominent tail is long and curls upwards in a spiral shape over its back. The animal is contained within the outline of a simple green square. There is no text present anywhere in the image, including titles, notes, arrow labels, timeline information, headers, footers, or any form of process flow, boxes, shapes, or decision diamonds. Therefore, no textual transcription or process mapping based on text can be provided.](images/1ac4802e6f22a7602ed19fd4729af718e9e830e584e52d723bde55ea6b8fa850.jpg)

As a rule of thumb, look for at least 4-bit quantized models. These models have a good balance between compression and accuracy. Although it is possible to use 3-bit or even 2-bit quantized mod‐ els, the performance degradation becomes noticeable and it would instead be preferable to choose a smaller model with a higher precision.

First, we will need to download the model. Note that the link contains multiple files with different bit-variants. FP16, the model we choose, represents the 16-bit variant:

!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/ Phi-3-mini-4k-instruct-fp16.gguf

We use llama-cpp-python together with LangChain to load the GGUF file:

from langchain import LlamaCpp

# Make sure the model path is correct for your system!   
llm $=$ LlamaCpp( model_path $=$ "Phi-3-mini-4k-instruct-fp16.gguf", n_gpu_layer $\mathsf { S } \mathsf { = } \mathsf { - } \mathsf { 1 }$ , max_tokens $= 5 \Theta \Theta$ , n_ct ${ \it \Omega } = 2 \Theta 4 8$ , seed $= 4 2$ , verbose=False   
)

In LangChain, we use the invoke function to generate output:

llm.invoke("Hi! My name is Maarten. What is $1 + 1 ?$ )

#

Unfortunately, we get no output! As we have seen in previous chapters, Phi-3 requires a specific prompt template. Compared to our examples with transformers, we will need to explicitly use a template ourselves. Instead of copy-pasting this template each time we use Phi-3 in LangChain, we can use one of LangChain’s core functionalities, namely “chains.”

All examples in this chapter can be run with any LLM. This means that you can choose whether to use Phi-3, ChatGPT, Llama 3 or anything else when going through the examples. We will use Phi-3 as a default throughout, but the state-of-the-art changes quickly, so consider using a newer model instead. You can use the Open LLM Leaderboard (a ranking of open source LLMs) to choose whichever works best for your use case.

If you do not have access to a device that can run LLMs locally, consider using ChatGPT instead:

from langchain.chat_models import ChatOpenAI # Create a chat-based LLM chat_model $=$ ChatOpenAI(openai_api_key $\mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \tilde { \tilde { \mathbf { \tilde } } } } } } } } } } }$ "MY_KEY")

# Chains: Extending the Capabilities of LLMs

LangChain is named after one of its main methods, chains. Although we can run LLMs in isolation, their power is shown when used with additional components or even when used in conjunction with each other. Chains not only allow for extending the capabilities of LLMs but also for multiple chains to be connected together.

The most basic form of a chain in LangChain is a single chain. Although a chain can take many forms, each with a different complexity, it generally connects an LLM with some additional tool, prompt, or feature. This idea of connecting a component to an LLM is illustrated in Figure 7-3.

![## Image Analysis: f34a8e929c9d5de227f7fba41a912680373792724191ef0eb9829f32b9ff1b70.jpg

**Conceptual Understanding:**
This image conceptually represents the most basic form of a "chain" in the context of Large Language Models (LLMs). It illustrates how an LLM's capabilities can be extended by pre-processing or integrating with other "modular components". The main purpose is to demonstrate a fundamental architectural pattern where distinct functionalities are linked sequentially to form a more complex system, allowing for custom processing of an input before it reaches the LLM and subsequent generation of an output. The key idea communicated is that LLM operations are not always isolated but can be part of a larger, interconnected workflow.

**Content Interpretation:**
The image depicts a simplified system or workflow involving an LLM.
*   **`Single chain`**: This overarching label indicates that the diagram illustrates a fundamental, atomic unit of connection between components, implying a direct, linear flow.
*   **`Input`**: This label, associated with an incoming arrow, signifies the initial data or query entering the system.
*   **`Modular component`**: This blue rounded rectangle represents any specialized, interchangeable unit designed to perform a specific task *before* the LLM. The general term "modular component" (extracted verbatim) suggests flexibility and the ability to plug in different functionalities. The document context states, "like a prompt template or external memory," which are examples of such modular components.
*   **Chain Link Symbol**: This visual element explicitly shows the connection and sequential flow of information from the "Modular component" to the "LLM", indicating that the output of the modular component becomes the input for the LLM. This literally depicts the "chain" concept.
*   **`LLM`**: This pink rectangle, labeled "LLM" (extracted verbatim), represents the Large Language Model itself. The small icon in its corner might suggest its role in processing language or generating content (e.g., a speech bubble for conversational output, or a document for text generation).
*   **`Output`**: This label, associated with an outgoing arrow, indicates the final result or response produced by the chain after the LLM has processed the information.

In essence, the diagram shows that an `Input` first goes through a `Modular component` (e.g., to retrieve information, format a prompt, or apply a specific logic), and the result of this processing is then passed to the `LLM`. The `LLM` then performs its core function (e.g., generating text, answering questions) to produce the final `Output`. This demonstrates a foundational relationship for extending LLM capabilities by adding pre-processing or contextualization steps.

**Key Insights:**
The main takeaways and insights from this image, supported by the extracted text, are:

*   **LLMs can be part of a larger workflow:** The presence of both a "Modular component" and an "LLM" within a "Single chain" indicates that LLMs are often integrated with other tools or processes, rather than operating in isolation. This is key to "extending the capabilities of LLMs," as mentioned in the document context.
*   **Chains are fundamental building blocks:** The label "Single chain" implies that this is a basic, foundational structure for connecting components. More complex systems would likely involve multiple such chains or more elaborate configurations.
*   **Modular components enhance LLM functionality:** The "Modular component" acts as an intermediary, processing the "Input" before it reaches the "LLM". This shows that external components can be used to prepare data, add context (like "external memory"), or structure queries (like "prompt templates") to improve the LLM's performance or guide its behavior, leading to a more refined "Output".
*   **Clear input-process-output flow:** The diagram clearly illustrates a linear flow from "Input" to "Modular component" to "LLM" to "Output", establishing a clear sequence of operations in this basic chain.

**Document Context:**
This image, titled "Single chain," is presented in a section discussing "Chains: Extending the Capabilities of LLMs." It serves as a foundational diagram, illustrating the simplest possible configuration of a "chain." The text after the image further clarifies that "A single chain connects some modular component, like a prompt template or external memory, to the LLM." The diagram visually supports the concept of combining an LLM with other functional units to create more powerful and versatile applications, setting the stage for understanding more complex chain architectures.

**Summary:**
This diagram illustrates the fundamental concept of a "Single chain" in the context of extending Large Language Model (LLM) capabilities. It visually represents a linear workflow where an input is processed sequentially by two connected components to produce an output.

The process begins with an "Input," which is the initial data or query fed into the system. This input is first directed to a "Modular component." This component, highlighted in blue, is designed to perform a specific task, such as formatting the input, retrieving relevant information from an external source (like external memory), or constructing a specific "prompt template" as mentioned in the surrounding document text.

After being processed by the "Modular component," the resulting information is then passed to the "LLM" (Large Language Model), shown in pink, via a visual chain link, symbolizing a direct connection and sequential flow. The LLM then uses this prepared information to perform its core function, such as generating text, answering questions, or completing tasks.

Finally, the outcome of the LLM's processing is delivered as the "Output," representing the ultimate result of this single chain operation. In essence, this diagram explains that by combining an LLM with a modular component in a sequential "chain," the LLM's functionality can be enhanced or specialized to better handle diverse inputs and generate more relevant outputs.](images/f34a8e929c9d5de227f7fba41a912680373792724191ef0eb9829f32b9ff1b70.jpg)
Figure 7-3. A single chain connects some modular component, like a prompt template or external memory, to the LLM.

In practice, chains can become complex quite quickly. We can extend the prompt template however we want and we can even combine several separate chains together to create intricate systems. In order to thoroughly understand what is happening in a chain, let’s explore how we can add Phi-3’s prompt template to the LLM.

# A Single Link in the Chain: Prompt Template

We start with creating our first chain, namely the prompt template that Phi-3 expects. In the previous chapter, we explored how transformers.pipeline applies the chat template automatically. This is not always the case with other packages and they might need the prompt template to be explicitly defined. With LangChain, we will use chains to create and use a default prompt template. It also serves as a nice hands-on experience with using prompt templates.

The idea, as illustrated in Figure 7-4, is that we chain the prompt template together with the LLM to get the output we are looking for. Instead of having to copy-paste the prompt template each time we use the LLM, we would only need to define the user and system prompts.

![## Image Analysis: 1df6a201fdd57e850ffd256ca3873da3dc1b87ef84db15e58080c206cd04a636.jpg

**Conceptual Understanding:**
This image conceptually represents a streamlined workflow for interacting with a Large Language Model (LLM) using a **"Prompt template"**. The main purpose is to illustrate how a user's initial input (**"User prompt"**) is processed and structured by a **"Prompt template"** before being fed into the **"LLM"** to generate a final **"Output"**. The key idea communicated is that prompt templates act as an essential intermediary, simplifying and standardizing the input for LLMs, thereby making the interaction more efficient and predictable.

**Content Interpretation:**
The image illustrates a fundamental process in natural language processing (NLP) concerning the practical application of Large Language Models (LLMs). It depicts how a user's initial query or instruction, labeled **"User prompt"**, is first processed through a **"Prompt template"**. The "Prompt template" is shown as a container with multiple distinct, empty, colored rounded rectangular components inside (white, yellow, green, orange, purple, light blue), suggesting that it structures various parts of the input into a consistent format. This structured output from the template is then **chained** (indicated by the chain link symbol) as input to an **"LLM"** (Large Language Model), which is represented by a pink box with a speech bubble icon. The LLM processes this formatted input to generate a final **"Output"**. The significance lies in demonstrating that prompt templates act as an intermediary, simplifying and optimizing the input for the LLM to ensure consistent and effective responses.

**Key Insights:**
The main takeaway is that **Prompt templates** are crucial for simplifying and streamlining interactions with **Large Language Models (LLMs)**. 

**Key insights supported by textual evidence:**
1.  **Simplification of User Input**: The diagram shows a direct path from **"User prompt"** to **"Prompt template"** and then to the **"LLM"**. This arrangement, coupled with the concept of the template, demonstrates that users primarily focus on providing their raw input, while the template handles the structural complexities for the LLM. This aligns with the document's statement that "we only need to define the input prompts. The template will be constructed for you."
2.  **Structured Interaction for LLMs**: The internal, distinct, empty colored components within the **"Prompt template"** container (white, yellow, green, orange, purple, light blue) visually suggest that the template creates a structured input. This structure is essential for an LLM to consistently and effectively interpret and respond to queries, providing a clear interface for its processing capabilities.
3.  **Efficient Chaining Mechanism**: The **chain link symbol** explicitly connecting the **"Prompt template"** to the **"LLM"** visually reinforces the idea of "chaining" mentioned in the document. This indicates an automated and integrated workflow where the templated prompt is seamlessly passed to the LLM, leading to the **"Output"** without manual intervention for formatting.

**Document Context:**
This image directly supports the document's section titled "A Single Link in the Chain: Prompt Template" and specifically illustrates the concept introduced in the subsequent text: "Figure 7-4. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you." It visually clarifies how a user's input is handled by a template before reaching an LLM, making the abstract concept of 'chaining' and automatic template construction concrete for the reader. It helps to understand the practical architecture of interacting with LLMs in an efficient manner.

**Summary:**
This diagram illustrates a fundamental process for interacting with a Large Language Model (LLM) through the use of a "Prompt template." The workflow begins with a **"User prompt"** (shown in green), which represents the initial input or request from a user. This "User prompt" is fed directly into a component labeled **"Prompt template."**

The "Prompt template" is depicted as a larger container, within which several smaller, distinct, empty rounded rectangular shapes are present, in various colors such as white, yellow, green, orange, purple, and light blue. These internal shapes visually suggest that a prompt template is composed of multiple sections or slots, designed to structure and format the user's input in a specific way.

Following the "Prompt template," a **chain link symbol** signifies a direct and integrated connection to the next component, which is labeled **"LLM"** (Large Language Model). The "LLM" is represented by a pink rectangular box, also featuring a small speech bubble icon in its top-right corner, emphasizing its role in language processing and generation.

Finally, a gray horizontal arrow indicates that the output from the "LLM" leads to the **"Output"** stage, which represents the ultimate response or result generated by the system.

In essence, the diagram shows a simplified pipeline: a user provides a prompt, this prompt is automatically structured and formatted by a pre-defined template, the templated prompt is then processed by a Large Language Model, and the LLM produces a final output. This visual model highlights how prompt templates simplify the interaction with LLMs by handling the complex formatting, allowing users to focus only on defining their input.](images/1df6a201fdd57e850ffd256ca3873da3dc1b87ef84db15e58080c206cd04a636.jpg)
Figure 7-4. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you.

The template for Phi-3 is comprised of four main components:

• $< \mathsf { S } >$ to indicate when the prompt starts   
• <|user|> to indicate the start of the user’s prompt   
• <|assistant $| >$ to indicate the start of the model’s output   
• <|end|> to indicate the end of either the prompt or the model’s output

These are further illustrated in Figure 7-5 with an example.

![## Image Analysis: ee50c4b553c2cf0524b41c8bdb5e01b91d859ec12621abea72688099e93843d3.jpg

**Conceptual Understanding:**
This image represents the specific "prompt template" format expected by the "Phi-3" language model for structuring conversational turns. It illustrates how user input and model output are encapsulated using special tokens.

The main purpose of the image is to clearly define the required structure for interacting with the Phi-3 model, ensuring that prompts and responses are correctly delimited and understood by the model. It communicates key ideas related to tokenization, prompt engineering, and the interaction protocol for conversational AI models, specifically for Phi-3.

**Content Interpretation:**
The image illustrates the structured communication process between a user and the Phi-3 language model. It shows how raw text (like "What is 1 + 1") is embedded within a specific token-based framework. This template is crucial for the Phi-3 model to parse and understand the user's intent and to generate an appropriately formatted response. Without this specific template, the model might misinterpret the start and end of conversational turns or the roles (user/assistant).

Supporting evidence from extracted text:
- The title "Phi-3 template" directly states the image's purpose.
- "Beginning of sentence (BOS) token" for `<s` indicates a mandatory starting point for any input sequence.
- `<|user|>` and "Start of prompt" explicitly define the beginning of a user's turn.
- "Prompt" for "What is 1 + 1" confirms where the actual query resides.
- `<|end|>` and "End of prompt" precisely mark the conclusion of the user's turn.
- `<|assistant|>` and "Start of output" define the model's turn beginning.
- "Output" for "The answer to 1+1 is 2!" shows the model's generated content.
- `<|end|>` and "End of output" mark the model's response termination.
- The "User" and "Model" labels clearly delineate which parts of the template belong to each entity, demonstrating the turn-taking nature of the interaction.

**Key Insights:**
The main takeaways from this image are:
1.  **Strict Format Requirement:** Interacting with Phi-3 requires a precise, token-based template.
2.  **Role Identification:** Special tokens like `<|user|>` and `<|assistant|>` are used to clearly identify who is speaking.
3.  **Turn Delimitation:** The `<|end|>` token is essential for marking the completion of both user prompts and model outputs.
4.  **BOS Token:** The `<s` token initializes the entire sequence, likely for contextual or technical reasons within the model's architecture.

These insights lead to the conclusion that the Phi-3 model relies on structured input to correctly interpret and generate responses, highlighting the importance of proper prompt engineering beyond just the content of the query.

All transcribed labels and box contents directly support these insights, demonstrating the explicit definition of each component of the template. For example, the label "Beginning of sentence (BOS) token" for `<s` explains its function, and "Start of prompt" for `<|user|>` confirms its role in identifying the user's turn. Similarly, the consistent use of `<|end|>` tokens for both user and model turns, along with their respective "End of prompt" and "End of output" labels, reinforces the concept of clear turn delimitation.

**Document Context:**
This image is critical in a document section titled "A Single Link in the Chain: Prompt Template" because it visually explains the fundamental syntax required to "link" with the Phi-3 model. It directly supports the understanding of how to format input for this specific model, which is a foundational step in utilizing it effectively. Without this detailed template, users would not know how to correctly structure their prompts for Phi-3, potentially leading to misinterpretations by the model or incorrect outputs.

**Summary:**
The image, titled "Phi-3 template", visually outlines the exact structured format that the Phi-3 language model expects for processing prompts and generating outputs. This template defines a clear sequence of special tokens and content sections, ensuring the model accurately understands the interaction.

The entire structure is contained within a grey rectangular box, representing a complete interaction sequence.

The sequence begins with a beige rounded rectangle containing the text `<s`. This token is explained by an arrow pointing to it, labeled "Beginning of sentence (BOS) token", indicating the absolute start of any communication sequence.

Following this, a yellow rounded rectangle contains `<|user|>`. An arrow from this box is labeled "Start of prompt", signifying the initiation of the user's input.

Immediately after, a green rounded rectangle contains the example user query: "What is 1 + 1". This content is labeled "Prompt" by an associated arrow, indicating where the actual user query text should be placed.

The user's prompt section concludes with a light orange rounded rectangle containing `<|end|>`. An arrow from this box is labeled "End of prompt", clearly marking the termination of the user's input.

To the right of the "Start of prompt", "Prompt", and "End of prompt" labels, a dashed vertical line brackets them together with the bold label "User", indicating that these components originate from or pertain to the user.

Next, the template outlines the model's response structure. A purple rounded rectangle contains the token `<|assistant|>`. An arrow from this box is labeled "Start of output", signifying the beginning of the model's generated response.

Following this, a pink rounded rectangle contains the example model response: "The answer to 1+1 is 2!". An arrow from this box is labeled "Output", indicating where the actual model's response text resides.

The model's output section concludes with another light orange rounded rectangle containing `<|end|>`. An arrow from this box is labeled "End of output", precisely marking the termination of the model's response.

To the right of the "Start of output", "Output", and "End of output" labels, a dashed vertical line brackets them together with the bold label "Model", indicating that these components originate from or pertain to the model.

In summary, the Phi-3 template mandates a sequence starting with a BOS token, followed by a user turn delimited by `<|user|>` and `<|end|>` tokens, and then a model turn delimited by `<|assistant|>` and `<|end|>` tokens. This explicit structure is crucial for the model to correctly process conversational inputs and generate coherent outputs.](images/ee50c4b553c2cf0524b41c8bdb5e01b91d859ec12621abea72688099e93843d3.jpg)
Figure 7-5. The prompt template Phi-3 expects.

To generate our simple chain, we first need to create a prompt template that adheres to Phi-3’s expected template. Using this template, the model takes in a system_prompt, which generally describes what we expect from the LLM. Then, we can use the input_prompt to ask the LLM specific questions:

# from langchain import PromptTemplate

# Create a prompt template with the "input_prompt" variable   
template $=$ """<s><|user|>   
{input_prompt} $\lnot$ |end|>   
<|assistant|>"""   
prompt $=$ PromptTemplate( template=template, input_variables=["input_prompt"]   
)

To create our first chain, we can use both the prompt that we created and the LLM and chain them together:

basic_chain $=$ prompt | llm

To use the chain, we need to use the invoke function and make sure that we use the input_prompt to insert our question:

# Use the chain   
basic_chain.invoke( { "input_prompt": "Hi! My name is Maarten. What is $1 + 1 2 ^ { 1 1 }$ , }   
)

The answer to $1 ~ + ~ 1$ is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether.

The output gives us the response without any unnecessary tokens. Now that we have created this chain, we do not have to create the prompt template from scratch each time we use the LLM. Note that we did not disable sampling as before, so your output might differ. To make this pipeline more transparent, Figure 7-6 illustrates the connection between a prompt template and the LLM using a single chain.

![## Image Analysis: 9a73fbc4295e72d63d3aad574b042e00385508bf0b9f4009f7f2cccd1af3f330.jpg

**Conceptual Understanding:**
This image conceptually illustrates a basic pipeline for interacting with a Large Language Model (LLM). It represents the flow of information from a user's query through an intermediary system and ultimately to the LLM for a response. The main purpose is to demonstrate a 'single chain' mechanism where a user's input is transformed into a prompt, processed, and then handled by an LLM to generate an output. The key idea communicated is the sequential and structured nature of feeding a query into an AI system and obtaining a relevant answer.

**Content Interpretation:**
The image displays a workflow for processing a user's query using an LLM. It shows that an initial user input, represented by the phrase "What is 1 + 1?", is fed into a system. This input is then encapsulated as a `{user_prompt}` within a larger processing unit (the grey rounded rectangle containing several sub-components, including unlabeled rounded rectangles in yellow, green, and purple, and an orange rounded rectangle). This processing unit is connected via a visual 'chain' link to an 'LLM' (Large Language Model), which is depicted as a pink rectangle with a small chat bubble icon. The LLM processes the prompt and then outputs the response, "The answer to 1 + 1 is 2!". The 'chain' signifies a direct, sequential connection or a programmatic link between the initial prompt handling and the LLM's operation.

**Key Insights:**
The main takeaway from this image is the clear illustration of a direct user-LLM interaction within a 'chain' framework. It highlights that a user's natural language question is converted into a formalized `{user_prompt}` before being processed. The presence of the 'LLM' component emphasizes the central role of large language models in generating responses. The 'chain' visual implies a structured and sequential flow, where different components (represented by the grey box and its internal shapes, and the LLM) are linked to achieve a complete task. This shows how queries are systematically routed to an AI model for intelligent response generation.

**Document Context:**
This image serves as a visual example of a 'single chain' utilizing a specific template (Phi-3's template, as indicated by the surrounding document text). It fits within the document's narrative by demonstrating a foundational concept of how Large Language Models are integrated into applications, particularly focusing on how user input is templated or structured and then passed to an LLM for processing as part of a defined workflow or 'chain.' It explains the basic interaction pattern of sending a query to an LLM and receiving a response.

**Summary:**
This image illustrates a simple, single-step process flow demonstrating how a user's question interacts with a Large Language Model (LLM) within a 'chain' architecture. The process begins with a user's natural language query, which is then formalized as a user prompt. This prompt is passed through an intermediary system or component, represented by a complex container, and then linked via a 'chain' mechanism to an LLM. The LLM processes this prompt and subsequently generates a direct answer back to the user. This visual representation highlights the sequential nature of interaction from user input to LLM processing and finally to output generation, emphasizing the 'chain' as a connection between system components.](images/9a73fbc4295e72d63d3aad574b042e00385508bf0b9f4009f7f2cccd1af3f330.jpg)
Figure 7-6. An example of a single chain using Phi-3’s template.

The example assumes that the LLM needs a specific template. This is not always the case. With OpenAI’s GPT-3.5, its API handles the underlying template.

You could also use a prompt template to define other variables that might change in your prompts. For example, if we want to create funny names for businesses, retyping that question over and over for different products can be time-consuming.

Instead, we can create a prompt that is reusable:

# Create a Chain that creates our business' name   
template $=$ "Create a funny name for a business that   
sells {product}."   
name_prompt $=$ PromptTemplate( template $\iota =$ template, input_variables=["product"]   
)

Adding a prompt template to the chain is just the very first step you need to enhance the capabilities of your LLM. Throughout this chapter, we will see many ways in which we can add additional modular components to existing chains, starting with memory.

# A Chain with Multiple Prompts

In our previous example, we created a single chain consisting of a prompt template and an LLM. Since our example was quite straightforward, the LLM had no issues dealing with the prompt. However, some applications are more involved and require lengthy or complex prompts to generate a response that captures those intricate details.

Instead, we could break this complex prompt into smaller subtasks that can be run sequentially. This would require multiple calls to the LLM but with smaller prompts and intermediate outputs as shown in Figure 7-7.

![## Image Analysis: 70cbaae3e16e98144727557adab34bf51b7ebb59f03ec3c5c2bc4f901f4cbdce.jpg

**Conceptual Understanding:**
The image conceptually represents a 'sequential prompt chain' architecture in the context of Large Language Models (LLMs). Its main purpose is to illustrate how an initial 'Input' is processed through a series of interconnected prompts, 'Prompt 1' and 'Prompt 2', to generate a final 'Output'. The diagram emphasizes that the output of one prompt in the chain acts as the input for the subsequent prompt, and that each prompt leverages an underlying 'LLM' for its processing, demonstrating a modular and iterative approach to interacting with LLMs.

**Content Interpretation:**
The image illustrates a sequential prompt chaining mechanism, a common pattern in large language model (LLM) applications. It shows how an 'Input' is processed through a series of distinct 'Prompt 1' and 'Prompt 2' stages. The chain-link symbol explicitly denotes that the output or intermediate result of 'Prompt 1' becomes the input for 'Prompt 2'. Each prompt, 'Prompt 1' and 'Prompt 2', interacts bidirectionally with an 'LLM', suggesting that these prompts are queries or instructions sent to the LLM, and the LLM's responses are received back by the prompts for further processing or to inform the subsequent prompt. The system culminates in an 'Output' after 'Prompt 2' has completed its task. This architecture implies a modular approach to problem-solving with LLMs, breaking down complex tasks into smaller, manageable prompts that build upon each other.

**Key Insights:**
The main takeaway from this image is the visualization of a 'sequential chain' of prompts. Key insights include: 1. **Sequential Execution:** The chain-link symbol between 'Prompt 1' and 'Prompt 2' explicitly indicates that prompts are executed in a specific order, where the output of one serves as the input for the next. 2. **LLM Interaction:** Both individual prompts ('Prompt 1' and 'Prompt 2') communicate bidirectionally with a 'LLM', highlighting that LLMs are integral to the processing of each prompt in the chain. 3. **Modular Design:** The use of 'Multiple prompts' within a defined system suggests a modular approach to complex tasks, breaking them down into smaller, chained prompt operations. 4. **Input-Output Flow:** The overall system transforms an 'Input' into an 'Output' through this sequential, LLM-driven prompt chain. The specific textual elements 'Input', 'Prompt 1', 'Prompt 2', 'Output', 'Multiple prompts', and 'LLM' provide direct evidence for these insights, illustrating the components and their interactions in this sequential prompting architecture.

**Document Context:**
This image directly supports the document's narrative regarding 'A Chain with Multiple Prompts' and clarifies the statement 'With sequential chains, the output of a prompt is used as the input for the next prompt.' It visually represents the concept of chaining prompts by showing 'Prompt 1' feeding into 'Prompt 2' via a chain-link, and how these prompts interact with an 'LLM'. The diagram provides a clear structural understanding of how multiple prompts can be orchestrated sequentially to achieve a desired outcome, enhancing the reader's comprehension of sequential prompt design in the context of LLM applications.

**Summary:**
The image depicts a system of 'Multiple prompts' that process an 'Input' to produce an 'Output'. The system consists of two sequential prompts, 'Prompt 1' and 'Prompt 2', housed within a larger container labeled 'Multiple prompts'. An arrow labeled 'Input' points to 'Prompt 1', indicating the start of the process. 'Prompt 1' is linked to 'Prompt 2' by a chain-link symbol, signifying a sequential relationship where the output of 'Prompt 1' feeds into 'Prompt 2'. An arrow points from 'Prompt 2' to 'Output', marking the end of the processing chain. Both 'Prompt 1' and 'Prompt 2' have bidirectional communication arrows connecting them to an external component labeled 'LLM' (Large Language Model), which is represented by a pink rectangle with a small chat bubble icon in its top right corner. This indicates that both prompts interact with the LLM during their execution. The overall flow is a sequential chain where an initial input is processed by a series of prompts, each potentially leveraging an LLM, to generate a final output.](images/70cbaae3e16e98144727557adab34bf51b7ebb59f03ec3c5c2bc4f901f4cbdce.jpg)
Figure 7-7. With sequential chains, the output of a prompt is used as the input for the next prompt.

This process of using multiple prompts is an extension of our previous example. Instead of using a single chain, we link chains where each link deals with a specific subtask.

For instance, consider the process of generating a story. We could ask the LLM to generate a story along with complex details like the title, a summary, a description of the characters, etc. Instead of trying to put all of that information into a single prompt, we could dissect this prompt into manageable smaller tasks instead.

Let’s illustrate with an example. Assume that we want to generate a story that has three components:

• A title • A description of the main character • A summary of the story

Instead of generating everything in one go, we create a chain that only requires a single input by the user and then sequentially generates the three components. This process is illustrated in Figure 7-8.

![## Image Analysis: 8c924e35f3999105c584c0e0b9651e3a1f74fe6c1d31d8214b638c564fe2c79a.jpg

**Conceptual Understanding:**
This image conceptually represents a multi-stage, sequential prompt engineering workflow. Its main purpose is to illustrate how an initial input can be iteratively processed through a series of specialized prompts (Title, Character, Story), where each prompt's output informs the subsequent one, all facilitated by a Large Language Model (LLM), to produce a comprehensive final output.

**Content Interpretation:**
The image illustrates a sequential, chained prompting mechanism designed to leverage a Large Language Model (LLM) for complex content generation, specifically for creating a story. The process breaks down the task into modular steps: first generating a title, then defining characters based on that title, and finally crafting a story using information from both the title and characters. Each stage is a distinct prompt that iteratively refines or expands upon the previous stage's output, all while continuously interacting with the LLM to process and generate content.

**Key Insights:**
The main takeaway from this image is the demonstration of a powerful prompt chaining technique where the output of preceding prompts serves as input for subsequent ones. This iterative and sequential approach allows for the generation of complex, structured content (like a story) by breaking it down into manageable sub-tasks. It highlights that LLMs can be utilized interactively, with each prompt step engaging the LLM to build towards a final, coherent output. The visual chaining underscores the dependency and flow of information across prompts, emphasizing a modular and progressive generation strategy.

**Document Context:**
This image, Figure 7-8, immediately follows the section titled "A Chain with Multiple Prompts" and directly supports the explanatory text: "The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used." It visually clarifies how a multi-prompt chain operates, where the output of one prompt becomes a critical input for the next, ultimately leading to a more structured and comprehensive final output generated by an LLM. It exemplifies a common strategy in advanced prompt engineering for achieving complex creative or analytical tasks.

**Summary:**
The image depicts a process flow illustrating the concept of "Multiple prompts" interacting with an underlying "LLM" (Large Language Model) to generate a final output. The process starts with an "Input" which is fed into a "Title Prompt". The output of the "Title Prompt" is then chained as input to the "Character Prompt". Subsequently, the output of both the "Title Prompt" and the "Character Prompt" are used as input for the "Story Prompt". Each of these individual prompts ("Title Prompt", "Character Prompt", "Story Prompt") maintains a bidirectional communication with the "LLM", indicated by double-headed arrows. Finally, the "Story Prompt" produces the ultimate "Output". The overall structure is enclosed within a bounding box labeled "Multiple prompts", visually emphasizing that these prompts work together as a cohesive unit. The LLM is represented by a pink rectangular box with a small chat bubble icon in its top right corner.](images/8c924e35f3999105c584c0e0b9651e3a1f74fe6c1d31d8214b638c564fe2c79a.jpg)
Figure 7-8. The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used.

To generate that story, we use LangChain to describe the first component, namely the title. This first link is the only component that requires some input from the user. We define the template and use the "summary" variable as the input variable and "title" as the output.

We ask the LLM to “Create a title for a story about {summary}” where “{summary}” will be our input:

from langchain import LLMChain

# Create a chain for the title of our story   
template $=$ """<s><|user|>   
Create a title for a story about {summary}. Only return the title.<|end|>   
<|assistant|>"""   
title_prompt $=$ PromptTemplate(template $\iota =$ template, input_variables $=$ ["summary"])   
title $=$ LLMChain(llm=llm, prompt $: =$ title_prompt, output_key="title")

Let’s run an example to showcase these variables:

title.invoke({"summary": "a girl that lost her mother"})

{'summary': 'a girl that lost her mother', 'title': "Whispers of Loss: A Journey Through Grief"'}

This already gives us a great title for the story! Note that we can see both the input ("summary") as well as the output ("title").

Let’s generate the next component, namely the description of the character. We generate this component using both the summary as well as the previously generated title. Making sure that the chain uses those components, we create a new prompt with the {summary} and {title} tags:

# Create a chain for the character description using the summary and title template $=$ """<s><|user|>   
Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>   
<|assistant|>"""   
character_prompt $=$ PromptTemplate(   
template $\iota =$ template, input_variables $\mathbf { \sigma } =$ ["summary", "title"]   
)   
character $=$ LLMChain(llm $\mid =$ llm, prompt $=$ character_prompt, output_key="character")

Although we could now use the character variable to generate our character descrip tion manually, it will be used as part of the automated chain instead.

Let’s create the final component, which uses the summary, title, and character description to generate a short description of the story:

# Create a chain for the story using the summary, title, and character descrip tion   
template $=$ """<s><|user|>   
Create a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph. <|end|>   
<|assistant|>"""   
story_prompt $=$ PromptTemplate(   
template=template, input_variables $\mathbf { \equiv }$ ["summary", "title", "character"] )   
story $=$ LLMChain(llm $\mid =$ llm, prompt $\Bumpeq$ story_prompt, output_key $^ { \prime } =$ "story")

Now that we have generated all three components, we can link them together to create our full chain:

# Combine all three components to create the full chain llm_chain $=$ title | character | story

We can run this newly created chain using the same example we used before:

llm_chain.invoke("a girl that lost her mother")

Running this chain gives us all three components. This only required us to input a single short prompt, the summary. Another advantage of dividing the problem into smaller tasks is that we now have access to these individual components. We can easily extract the title; that might not have been the case if we were to use a single prompt.

# Memory: Helping LLMs to Remember Conversations

When we are using LLMs out of the box, they will not remember what was being said in a conversation. You can share your name in one prompt but it will have forgotten it by the next prompt.

Let’s illustrate this phenomenon with an example using the basic_chain we created before. First, we tell the LLM our name:

# Let's give the LLM our name basic_chain.invoke({"input_prompt": "Hi! My name is Maarten. What is $1 ~ + ~ 1 ? ~ \}$ )

Hello Maarten! The answer to $1 ~ + ~ 1$ is 2.

Next, we ask it to reproduce the name we have given it:

# Next, we ask the LLM to reproduce the name basic_chain.invoke({"input_prompt": "What is my name?"})

I'm sorry, but as a language model, I don't have the ability to know personal information about individuals. You can provide the name you'd like to know more about, and I can help you with information or general inquiries related to it.

Unfortunately, the LLM does not know the name we gave it. The reason for this forgetful behavior is that these models are stateless—they have no memory of any previous conversation!

As illustrated in Figure 7-9, conversing with an LLM that does not have any memory is not the greatest experience.

To make these models stateful, we can add specific types of memory to the chain that we created earlier. In this section, we will go through two common methods for helping LLMs to remember conservations:

• Conversation buffer • Conversation summary

![## Image Analysis: ff151c589887b9ba8eb5fcf99b10b2ff9d03658a5ad4028610f84c1609088d19.jpg

**Conceptual Understanding:**
The image conceptually represents the fundamental difference between Large Language Models (LLMs) that lack a memory mechanism and those that incorporate one. Its main purpose is to illustrate, through a direct comparison of two conversational flows, how the presence or absence of conversational memory impacts an LLM's ability to maintain context and respond appropriately to follow-up questions that rely on previously shared information. The key idea being communicated is that 'memory' (specifically, the 'Conversation history') is essential for an LLM to achieve coherent and context-aware interactions, especially when personal or previous data is involved.

**Content Interpretation:**
The image illustrates the fundamental difference in conversational capabilities of an LLM when it is configured to retain 'Conversation history' versus when it treats each 'Independent conversation' as a new, isolated interaction. The 'Without memory' section shows a standard stateless LLM, which cannot recall previously shared personal information. In contrast, the 'With memory' section demonstrates an LLM that stores and accesses past conversational data, enabling it to maintain context and provide relevant responses based on prior inputs. The processes shown are conversational exchanges between a user and an LLM, highlighting the system's ability (or inability) to persist and utilize information across turns. The significance lies in showing how 'memory' enhances the LLM's capability to understand and respond contextually, leading to a more natural and useful interaction.

**Key Insights:**
**Main Takeaways:**
1.  **Memory is Crucial for Context:** LLMs without memory treat each turn as a new, independent conversation, losing all prior context. This is evident in the 'Without memory' scenario where the LLM forgets the user's name.
2.  **Enhanced User Experience with Memory:** LLMs equipped with memory (conversation history) can recall past information, leading to more coherent and personalized interactions. This is shown in the 'With memory' scenario where the LLM correctly identifies the user's name.
3.  **Limitations of Stateless LLMs:** A stateless LLM (without memory) cannot access personal information previously provided, indicating a significant limitation in maintaining a continuous dialogue.

**Supporting Textual Evidence:**
*   'Without memory' section: The user states 'Hi! I'm Maarten. What is 1 + 1?', and the LLM responds 'Hello Maarten! The answer to 1 + 1 is 2.'. However, when the user then asks 'What was my name again?', the LLM replies, 'I don't have access to personal information such as your name.' This demonstrates a complete lack of recall for the 'Maarten' name mentioned just moments before.
*   'With memory' section: The exact same initial exchange occurs: 'Hi! I'm Maarten. What is 1 + 1?' followed by 'Hello Maarten! The answer to 1 + 1 is 2.'. But after the user asks 'What was my name again?', the LLM correctly responds, 'Your name is Maarten.' This explicitly shows the benefit of 'Conversation history' (as labeled by the arrow) in retaining and utilizing past information.

**Document Context:**
This image directly supports the document's narrative by visually explaining the concept of LLM memory, specifically in the context of reproducing user information. The preceding text mentions asking an LLM to reproduce the name `basic_chain.invoke({'input_prompt': 'What is my name?'})`, and this image provides a clear example of why memory is crucial for such a task. It demonstrates the expected outcome for both a memory-less and a memory-enabled LLM when prompted to recall personal information previously provided in the conversation, thereby illustrating the practical impact of the memory feature on LLM performance and user experience.

**Summary:**
The image presents two distinct conversational flows, contrasting how an LLM (Large Language Model) interacts with a user 'Without memory' versus 'With memory'. Each scenario demonstrates a user initiating a conversation and then asking a follow-up question that relies on information from the initial exchange. The 'Without memory' scenario shows the LLM failing to recall previous details, while the 'With memory' scenario correctly remembers and utilizes prior information. The structure highlights the critical role of conversation history in maintaining context.](images/ff151c589887b9ba8eb5fcf99b10b2ff9d03658a5ad4028610f84c1609088d19.jpg)
Figure 7-9. An example of a conversation between an LLM with memory and without memory.

# Conversation Buffer

One of the most intuitive forms of giving LLMs memory is simply reminding them exactly what has happened in the past. As illustrated in Figure 7-10, we can achieve this by copying the full conversation history and pasting that into our prompt.

![## Image Analysis: 071b13a5819683a5d33216d2c7e0ed0f357862ce771f91d843790fdb9cd9aa61.jpg

**Conceptual Understanding:**
*   **Concept:** This image conceptually represents the mechanism of a "conversation buffer" or "context window" for a Large Language Model (LLM). It illustrates how the memory of past interactions is maintained and provided to the LLM to enable context-aware responses.
*   **Main Purpose:** The main purpose is to show, step-by-step, how a conversational prompt is constructed by integrating prior dialogue history with a new user query into a structured format that an LLM can process to generate a coherent and contextually relevant response. It visualizes the process of "appending the entire conversation history to the input prompt" as described in the accompanying text.
*   **Key Ideas:** The key ideas communicated are:
    *   **Context Preservation:** LLMs require explicit provision of past conversation to maintain context.
    *   **Prompt Engineering:** Prompts are often templated and structured, including special tokens, role indicators, and clear labeling of different parts (history vs. current query).
    *   **Conversational AI Mechanism:** This is a fundamental technique for building stateful conversational agents with stateless LLMs.

**Content Interpretation:**
The image clearly demonstrates the process of **contextualizing a new user query for an LLM** by prepending the conversation history.

*   **Processes Shown:**
    *   **Recording Conversation History:** The "Conversation history" section shows a simple two-turn exchange between a user ("Hi! I'm Maarten. What is 1 + 1?") and an AI ("Hello Maarten! The answer to 1 + 1 is 2."). This indicates that previous interactions are logged.
    *   **Receiving New Input:** The "Prompt" section displays a new user query ("What is my name?"), representing the current turn in the conversation.
    *   **Prompt Assembly/Templating:** The "Prompt template" section illustrates the core process of combining these elements. The tokens "<s>" and "<|user|>" likely mark the beginning of a user's input block. The explicit "Conversation history:" label clearly demarcates the historical part. The exact transcription of the past conversation ("Human: Hi! I'm Maarten. What is 1 + 1? AI: Hello Maarten! The answer to 1 + 1 is 2.") being included verbatim in the template is direct evidence of appending the full history. The new user query ("What is my name?") is also included. The "<|end|>" token signifies the end of the current user input, and the "<|assistant|>" token signals that the LLM is expected to generate its response from this point.

*   **Relationships Shown:**
    *   There's a clear **input-output relationship** where the "Conversation history" and "Prompt" (current user input) act as inputs that are formatted and structured into the final "Prompt template" which is then sent to the LLM.
    *   The **sequential nature** of conversation is maintained by placing older turns before newer turns in the history.
    *   The **role identification** (Human, AI, user, assistant) within the template explicitly separates who said what, which is critical for an LLM to understand the dialogue structure.

*   **Significance of Information:**
    *   The example questions ("What is 1 + 1?" and "What is my name?") are simple but effectively demonstrate the need for context. The AI's response "Hello Maarten!" from the history is crucial for answering the subsequent "What is my name?" correctly, highlighting that the LLM needs to "remember" the user's name.
    *   The specific tokens like "<s>", "<|user|>", "<|end|>", and "<|assistant|>" are significant as they represent the structured formatting required by many LLMs to interpret conversational turns and roles correctly. Their presence underscores the importance of prompt engineering beyond just raw text.

**Key Insights:**
*   **Main Takeaways/Lessons:**
    1.  **Context is King for Conversational LLMs:** To have a coherent, multi-turn conversation, LLMs need to be explicitly provided with the history of the conversation. The verbatim inclusion of "Human: Hi! I'm Maarten. What is 1 + 1? AI: Hello Maarten! The answer to 1 + 1 is 2." within the "Prompt template" is direct evidence of this.
    2.  **Conversation History is Appended Verbatim:** The method shown involves taking the entire raw text of the previous turns and adding it to the current input. This is supported by the identical text appearing in "Conversation history" and then in the "Prompt template" under "Conversation history:".
    3.  **Prompt Templating is Structured:** LLM prompts are not just raw text; they often include special tokens or markers to guide the model on roles, turns, and the beginning/end of segments. The presence of "<s>", "<|user|>", "<|end|>", and "<|assistant|>" explicitly demonstrates this structured approach.
    4.  **Role Indicators are Important:** Labeling turns with "Human:" and "AI:" inside the history, and using tokens like "<|user|>" and "<|assistant|>", helps the LLM understand who is speaking and what role it is expected to play next. The explicit text "Human: Hi! I'm Maarten..." and "AI: Hello Maarten!..." in the template provides this evidence.

*   **Conclusions/Insights:**
    *   This technique (appending conversation history) allows LLMs, which are inherently stateless, to simulate statefulness in a conversation.
    *   The example directly shows *how* the LLM would be reminded of "Maarten" without requiring a separate memory retrieval system, simply by making the previous turn part of the current input.

*   **Textual Evidence for Insights:**
    *   **Context Preservation:** The arrow from "Conversation history" to the combined "Human: ... AI: ..." block within the "Prompt template" box explicitly shows the history being carried forward.
    *   **Appending Method:** The identical textual content from the "Conversation history" box appearing within the "Prompt template" box confirms it's appended.
    *   **Structured Prompt:** The specific tokens "<s>", "<|user|>", "<|end|>", and "<|assistant|>" are textual evidence of the structured nature of the prompt.
    *   **Role Identification:** The "Human:" and "AI:" prefixes within the combined history block clearly indicate role differentiation.

**Document Context:**
This image is a direct visual explanation for the "Conversation Buffer" section in the document, specifically illustrating the mechanism mentioned in the text "We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt." It concretely shows *how* that appending happens and what the resulting combined prompt looks like. It provides the foundational understanding for how an LLM can maintain a semblance of memory across conversational turns.

**Summary:**
This diagram illustrates how a Large Language Model (LLM) maintains conversational context by incorporating past dialogue into the current input prompt. It breaks down the process into three main parts: "Conversation history," the current "Prompt," and the resulting "Prompt template."

First, the **"Conversation history"** section on the left displays a previous interaction between a user and an AI. The user's turn is shown as: "Hi! I'm Maarten. What is 1 + 1?". The AI's response, represented by a robot icon, is: "Hello Maarten! The answer to 1 + 1 is 2." This entire exchange forms the historical context that needs to be preserved.

Below this, the **"Prompt"** section shows the current input from the user: "What is my name?". This is the new query for which the LLM needs to generate a response, ideally by utilizing the context provided by the "Conversation history."

The central mechanism is depicted in the **"Prompt template"** section on the right. This grey box shows how the conversation history and the new prompt are combined and formatted for the LLM. The process unfolds sequentially within this template:

1.  The template begins with a starting token: "<s>".
2.  It then includes a role indicator for the user: "<|user|>".
3.  A clear label, "Conversation history:", is then inserted to signify the upcoming historical dialogue.
4.  The entire previous conversation from the "Conversation history" section is then appended verbatim, structured with explicit role prefixes: "Human: Hi! I'm Maarten. What is 1 + 1? AI: Hello Maarten! The answer to 1 + 1 is 2." This block is depicted as a single pink-shaded box, illustrating the concatenation.
5.  Following the history, the current user prompt is added: "What is my name?". This is shown in a green-shaded box.
6.  An "end" token, "<|end|>", signifies the completion of the current user's input segment.
7.  Finally, an "assistant" role token, "<|assistant|>", is included. This informs the LLM that it is now expected to generate its response, taking on the role of the assistant, and leveraging all the preceding contextual information.

In essence, the image demonstrates that to enable an LLM to answer a question like "What is my name?" correctly (by recalling "Maarten"), the system prepends the full previous conversation, including both user and AI turns, along with specific formatting tokens, to the new user query. This combined "Prompt template" is then sent to the LLM, effectively "reminding" it of earlier interactions to ensure contextually appropriate responses.](images/071b13a5819683a5d33216d2c7e0ed0f357862ce771f91d843790fdb9cd9aa61.jpg)
Figure 7-10. We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt.

In LangChain, this form of memory is called a ConversationBufferMemory. Its implementation requires us to update our previous prompt to hold the history of the chat.

We’ll start by creating this prompt:

# Create an updated prompt template to include a chat history   
template $=$ """<s><|user|>Current conversation:{chat_history}   
{input_prompt}<|end|>   
<|assistant|>"""   
prompt $=$ PromptTemplate( template=template, input_variables=["input_prompt", "chat_history"]   
)

Notice that we added an additional input variable, namely chat_history. This is where the conversation history will be given before we ask the LLM our question.

Next, we can create LangChain’s ConversationBufferMemory and assign it to the chat_history input variable. ConversationBufferMemory will store all the conversa‐ tions we have had with the LLM thus far.

We put everything together and chain the LLM, memory, and prompt template:

from langchain.memory import ConversationBufferMemory   
# Define the type of memory we will use   
memory $=$ ConversationBufferMemory(memory_key="chat_history")   
# Chain the LLM, prompt, and memory together   
llm_chain $=$ LLMChain( prompt=prompt, llm=llm, memory=memory   
)

To explore whether we did this correctly, let’s create a conversation history with the LLM by asking it a simple question:

# Generate a conversation and ask a basic question llm_chain.invoke({"input_prompt": "Hi! My name is Maarten. What is $1 ~ + ~ 1 ? ~ \}$ )

{'input_prompt': 'Hi! My name is Maarten. What is $1 { \ : } + { \ : } 1 ? { } ^ { \prime }$ ,   
'chat_history': ',   
'text': " Hello Maarten! The answer to $1 ~ + ~ 1$ is 2. Hope you're having a great   
day!"}

You can find the generated text in the 'text' key, the input prompt in 'in put_prompt', and the chat history in 'chat_history'. Note that since this is the first time we used this specific chain, there is no chat history.

Next, let’s follow up by asking the LLM if it remembers the name we used:

# Does the LLM remember the name we gave it? llm_chain.invoke({"input_prompt": "What is my name?"})

{'input_prompt': 'What is my name?',   
'chat_history': "Human: Hi! My name is Maarten. What is $1 ~ + ~ 1 ? \backslash \{ \mathsf { n A I }$ : Hello   
Maarten! The answer to $1 ~ + ~ 1$ is 2. Hope you're having a great day!",   
'text': ' Your name is Maarten.'}

By extending the chain with memory, the LLM was able to use the chat history to find the name we gave it previously. This more complex chain is illustrated in Figure 7-11 to give an overview of this additional functionality.

![## Image Analysis: 60c11470427be92ed73144500bc7940ead8b05c89cfacea6fee714c46a0775f8.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural pattern for incorporating 'memory' into a Large Language Model (LLM) interaction. It illustrates how an LLM, which is typically stateless and processes each prompt independently, can maintain conversational context over multiple turns. The main purpose is to demonstrate that by explicitly including the 'conversation_history' alongside the current 'user_prompt' as inputs to the LLM, the model can generate context-aware responses that reflect prior dialogue. The core idea communicated is that 'memory' in an LLM chain is achieved through a pre-processing step where the historical exchange is appended to the current query, making the entire conversation available to the model for generating its response, rather than the LLM possessing an intrinsic memory function.

**Content Interpretation:**
This image illustrates the process of integrating 'memory' into a Large Language Model (LLM) chain. The main concepts shown are: explicit input of conversation history, distinct user prompts, the aggregation of these inputs, and the processing by an LLM to generate context-aware responses. It visualizes how an LLM can maintain conversational context by appending the entire conversation history to the input prompt, allowing it to 'remember' previous interactions and user-provided information. The significance is to demonstrate a common and effective method for achieving stateful conversation with stateless LLMs, where the 'memory' is managed external to the core LLM and provided as part of each new query. The extracted text elements from Section 1 clearly support these interpretations by showing: Human and AI dialogue as the source of 'conversation_history', a specific question as the 'user_prompt', explicit labels for these inputs as '{conversation_history}' and '{user_prompt}', the 'LLM' as the processing unit, and a contextually accurate response 'Your name is Maarten.' as the output, which directly reflects the initial name provided in the conversation history.

**Key Insights:**
The main takeaway from this image is that for an LLM to exhibit 'memory' or contextual awareness of past interactions, the entire conversation history must be explicitly provided as part of its input for each new prompt. This means LLMs do not inherently 'remember' previous turns; rather, their responses are conditioned on the concatenated history provided at inference time. The image teaches that managing conversational state in an LLM application requires bundling previous exchanges with the current query. The specific text elements 'Human: I'm Maarten. AI: Hi Maarten!' and 'What is my name?' as inputs, explicitly labeled '{conversation_history}' and '{user_prompt}', followed by the LLM processing and outputting 'Your name is Maarten.', provide direct evidence for this insight. It shows that by feeding the initial interaction ('I'm Maarten') into the '{conversation_history}', the LLM is able to retrieve and use that information when asked 'What is my name?'. This demonstrates the explicit mechanism of injecting memory into the LLM's context.

**Document Context:**
This image directly addresses the question posed in the document's section title: 'Does the LLM remember the name we gave it?'. It visually explains the mechanism by which an LLM, inherently stateless, can appear to 'remember' past interactions. The image elaborates on the concept mentioned in the subsequent text, 'We extend the LLM chain with memory by appending the entire conversation history to the input prompt.' It provides a clear, step-by-step visual representation of how the conversation history and the current user prompt are combined as inputs to the LLM, enabling the model to generate a response that is informed by prior dialogue. This integration of visual and textual information enhances the reader's understanding of how memory is implemented in LLM applications.

**Summary:**
This image illustrates how a Large Language Model (LLM) can maintain a sense of memory by explicitly including conversation history as part of its input. The process begins with two distinct inputs: the ongoing conversation history and the current user's prompt. The conversation history, represented by the blue text 'Human: I'm Maarten. AI: Hi Maarten!', contains prior turns of dialogue. The current user prompt, shown in green text as 'What is my name?', is the immediate question from the user. Both of these textual inputs are directed into an input aggregation stage, represented by a large gray container. Within this container, the '{conversation_history}' is processed as one distinct input component (light blue box), and the '{user_prompt}' is handled as another (light green box). There are also other placeholder input components shown as uncolored rounded rectangles (two at the top left, one yellow at the top right, and one light purple at the bottom), indicating a flexible input structure. After the inputs are aggregated within the gray container, they are passed to the 'LLM' (Large Language Model), depicted as a pink rectangle with a speech bubble icon, via a chain-link connection, symbolizing a sequential process or an LLM chain. The LLM then processes this combined input, using the conversation history to provide context for the current prompt. As a result, the LLM generates an output, shown as 'Your name is Maarten.', demonstrating that it has successfully 'remembered' the name mentioned earlier in the conversation history. This comprehensive setup ensures that the LLM's responses are contextually informed by previous interactions.](images/60c11470427be92ed73144500bc7940ead8b05c89cfacea6fee714c46a0775f8.jpg)
Figure 7-11. We extend the LLM chain with memory by appending the entire conversa‐ tion history to the input prompt.

# Windowed Conversation Buffer

In our previous example, we essentially created a chatbot. You could talk to it and it remembers the conversation you had thus far. However, as the size of the conversa‐ tion grows, so does the size of the input prompt until it exceeds the token limit.

One method of minimizing the context window is to use the last $k$ conversations instead of maintaining the full chat history. In LangChain, we can use Conversation BufferWindowMemory to decide how many conversations are passed to the input prompt:

from langchain.memory import ConversationBufferWindowMemory   
# Retain only the last 2 conversations in memory   
memory $=$ ConversationBufferWindowMemory( $k = 2$ , memory_key="chat_history")   
# Chain the LLM, prompt, and memory together   
llm_chain $=$ LLMChain( prompt=prompt, llm=llm, memory=memory   
)

Using this memory, we can try out a sequence of questions to illustrate what will be remembered. We start with two conversations:

# Ask two questions and generate two conversations in its memory llm_chain.predict(input_prompt $: =$ "Hi! My name is Maarten and I am 33 years old.

{'input_prompt': 'What is $3 + 3 ?$ ,   
'chat_history': "Human: Hi! My name is Maarten and I am 33 years old. What is $\textit { 1 + 1 2 }$ \nAI: Hello Maarten! It's nice to meet you. Regarding your question, $^ { 1 ~ + }$ 1 equals 2. If you have any other questions or need further assistance, feel free to ask! $\mathsf { \backslash n \backslash n }$ (Note: This response answers the provided mathematical query while maintaining politeness and openness for additional inquiries.)",   
'text': " Hello Maarten! It's nice to meet you as well. Regarding your new question, $3 + 3$ equals 6. If there's anything else you need help with or more questions you have, $\tau ^ { \prime } { \mathfrak { n } }$ here for you!"}

The interaction we had thus far is shown in "chat_history". Note that under the hood, LangChain saves it as an interaction between you (indicated with Human) and the LLM (indicated with AI).

Next, we can check whether the model indeed knows the name we gave it:

# Check whether it knows the name we gave it llm_chain.invoke({"input_prompt":"What is my name?"})

{'input_prompt': 'What is my name?', 'chat_history': "Human: Hi! My name is Maarten and I am 33 years old. What is $\textit { 1 + 1 2 }$ \nAI: Hello Maarten! It's nice to meet you. Regarding your question, $^ { 1 ~ + }$ 1 equals 2. If you have any other questions or need further assistance, feel free to ask!\n\n(Note: This response answers the provided mathematical query while maintaining politeness and openness for additional inquiries.)\nHuman: What is $3 + 3 2$ \nAI: Hello Maarten! It's nice to meet you as well. Regarding your new question, $3 + 3$ equals 6. If there's anything else you need help with or more questions you have, 'text': ' Your name is Maart $\tau ^ { \prime } \mathfrak { n }$ here for you!", as mentioned at the beginning of our conversation. Is there anything else you would like to know or discuss?'}

Based on the output in 'text' it correctly remembers the name we gave it. Note that the chat history is updated with the previous question.

Now that we have added another conversation we are up to three conversations. Con‐ sidering the memory only retains the last two conversations, our very first question is not remembered.

Since we provided an age in our first interaction, we check whether the LLM indeed does not know the age anymore:

# Check whether it knows the age we gave it llm_chain.invoke({"input_prompt":"What is my age?"})

{'input_prompt': 'What is my age?',   
'chat_history': "Human: What is $3 + 3 ?$ \nAI: Hello again! $3 + 3$ equals 6. If there's anything else I can help you with, just let me know!\nHuman: What is my name?\nAI: Your name is Maarten.",'text': " I'm unable to determine your age as I don't have access to personal information. Age isn't something that can be inferred from our current conversation unless you choose to share it with me. How else may I assist you today?"}

The LLM indeed has no access to our age since that was not retained in the chat history.

Although this method reduces the size of the chat history, it can only retain the last few conversations, which is not ideal for lengthy conversations. Let’s explore how we can summarize the chat history instead.

# Conversation Summary

As we have discussed previously, giving your LLM the ability to remember conver‐ sations is vital for a good interactive experience. However, when using Conversation BufferMemory, the conversation starts to increase in size and will slowly approach your token limit. Although ConversationBufferWindowMemory resolves the issue of token limits to an extent, only the last $k$ conversations are retained.

Although a solution would be to use an LLM with a larger context window, these tokens still need to be processed before generation tokens, which can increase compute time. Instead, let’s look toward a more sophisticated technique, Conversa tionSummaryMemory. As the name implies, this technique summarizes an entire con‐ versation history to distill it into the main points.

This summarization process is enabled by another LLM that is given the conversation history as input and asked to create a concise summary. A nice advantage of using an external LLM is that we are not confined to using the same LLM during conversation. The summarization process is illustrated in Figure 7-12.

![## Image Analysis: b0dad392dec2cca381261ffeb14e55af88624becab925feea80fd0fe4337f3b6.jpg

**Conceptual Understanding:**
This image conceptually illustrates an advanced method for preparing input prompts for Large Language Models (LLMs) in a conversational context. The main purpose is to demonstrate how to efficiently include conversation history in a prompt without exceeding token limits or introducing unnecessary verbosity, by leveraging a separate LLM for summarization. The key idea communicated is that summarization of prior interactions can improve the performance and efficiency of subsequent LLM queries, especially when combined with structured prompt templating.

**Content Interpretation:**
The image details a preprocessing step for creating a prompt for a Large Language Model (LLM), specifically focusing on how conversation history is managed. The core process involves taking a raw 'Conversation history' and a new 'Prompt', summarizing the history using an 'LLM', and then combining this summary with the new prompt into a structured 'Prompt template'. The significance lies in the transformation of a verbose conversation into a condensed, manageable input for the main LLM. The extracted text elements 'Hi! I'm Maarten. What is 1 + 1?' and 'The answer to 1 + 1 is... (drumroll) ...2.' represent the original, detailed conversation. The 'Summarize' arrow leading to the 'LLM' box signifies the action of condensation. The output 'Maarten asked what 1 + 1 is and I told him 2.' clearly shows the result of this summarization, retaining the key information concisely. The new 'Prompt' 'What is my name?' is directly incorporated. The 'Prompt template' elements like '<s', '<|user|>', 'Conversation history:', '<|end|>', and '<|assistant|>' define the structure and roles within the final prompt, illustrating how the summarized history and current query are framed for the LLM. This system shows an efficient way to maintain context in conversational AI without overwhelming the model with excessive input tokens.

**Key Insights:**
The main takeaway from this image is the strategic use of an intermediate LLM to summarize conversation history before integrating it into a final prompt for another LLM. This technique aims to make prompts more concise and efficient while preserving crucial context. The process highlights that conversation history does not need to be passed verbatim to maintain continuity. The specific text 'Summarize' on the arrow leading to the LLM box explicitly states this function. The original conversation text 'Hi! I'm Maarten. What is 1 + 1?' and 'The answer to 1 + 1 is... (drumroll) ...2.' transforms into 'Maarten asked what 1 + 1 is and I told him 2.' in the 'Prompt template', providing clear textual evidence of the summarization process and its outcome. This demonstrates a key insight into managing input length and relevance for LLMs.

**Document Context:**
This image directly supports the document's narrative by visually explaining the statement 'Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first.' It serves as a detailed diagram of the proposed method for handling conversation history in LLM interactions. The image's step-by-step visual representation of summarization and prompt construction clarifies how this approach is implemented, enhancing the reader's understanding of the system's architecture and the rationale behind using an intermediate LLM for summarization. It is a fundamental illustration of an optimization technique in prompt engineering for conversational agents.

**Summary:**
The image illustrates a process for constructing a Large Language Model (LLM) prompt by first summarizing the conversation history. The process begins with two distinct inputs: the 'Conversation history' and the current 'Prompt'. The 'Conversation history' consists of a dialogue between a user and an assistant. The user, identified as Maarten, initiates the conversation by saying, 'Hi! I'm Maarten. What is 1 + 1?' The assistant responds, 'The answer to 1 + 1 is... (drumroll) ...2.' This complete conversation history is then fed into an 'LLM' labeled with 'Summarize'. This LLM processes the conversation and outputs a concise summary. Simultaneously, a new user 'Prompt' is provided, which states, 'What is my name?'. Both the summarized conversation history and the new prompt are then integrated into a 'Prompt template'. The 'Prompt template' is structured with specific tags and content blocks. It starts with a '<s' tag, followed by '<|user|>', then a 'Conversation history:' header. Below this header is the summarized conversation, which reads: 'Maarten asked what 1 + 1 is and I told him 2.' Following the summarized history, the new user prompt 'What is my name?' is included. This is succeeded by an '<|end|>' tag and finally an '<|assistant|>' tag, indicating the expected start of the assistant's response. This method ensures that the full, potentially lengthy, conversation history is not directly passed but is first condensed, making the prompt more efficient and focused.](images/b0dad392dec2cca381261ffeb14e55af88624becab925feea80fd0fe4337f3b6.jpg)
Figure 7-12. Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first.

This means that whenever we ask the LLM a question, there are two calls:

• The user prompt • The summarization prompt

To use this in LangChain, we first need to prepare a summarization template that we will use as the summarization prompt:

# Create a summary prompt template   
summary_prompt_template $=$ """<s><|user|>Summarize the conversations and update with the new lines.   
Current summary:   
{summary}   
new lines of conversation:   
{new_lines}   
New summary:<|end|>   
<|assistant|>"""   
summary_prompt $=$ PromptTemplate(   
input_variables $=$ ["new_lines", "summary"],   
template $\iota =$ summary_prompt_template   
)

Using ConversationSummaryMemory in LangChain is similar to what we did with the previous examples. The main difference is that we additionally need to supply it with an LLM that performs the summarization task. Although we use the same LLM for both summarizing and user prompting, you could use a smaller LLM for the summarization task to speed up computation:

from langchain.memory import ConversationSummaryMemory   
# Define the type of memory we will use   
memory $=$ ConversationSummaryMemory( llm $\mid =$ llm, memory_key="chat_history", prompt=summary_prompt   
)   
# Chain the LLM, prompt, and memory together   
llm_chain $=$ LLMChain( prompt $: =$ prompt, llm $\mid =$ llm, memory=memory   
)

Having created our chain, we can test out its summarization capabilities by creating a short conversation:

# Generate a conversation and ask for the name   
llm_chain.invoke({"input_prompt": "Hi! My name is Maarten. What is $1 ~ + ~ 1 ? ~ \}$ )   
llm_chain.invoke({"input_prompt": "What is my name?"}) {'input_prompt': 'What is my name?',   
'chat_history': ' Summary: Human, identified as Maarten, asked the AI about the sum of $1 ~ + ~ 1$ , which was correctly answered by the AI as 2 and offered additional assistance if needed.',   
'text': ' Your name in this context was referred to as "Maarten". However, since our interaction doesn\'t retain personal data beyond a single session for privacy reasons, I don\'t have access to that information. How can I assist you further today?'}

After each step, the chain will summarize the conversation up until that point. Note how the first conversation was summarized in 'chat_history' by creating a description of the conversation.

We can continue the conversation and at each step, the conversation will be summar‐ ized and new information will be added as necessary:

# Check whether it has summarized everything thus far llm_chain.invoke({"input_prompt": "What was the first question I asked?"})

{'input_prompt': 'What was the first question I asked?',   
'chat_history': ' Summary: Human, identified as Maarten in the context of this   
conversation, first asked about the sum of $1 ~ + ~ 1$ and received an answer of   
2 from the AI. Later, Maarten inquired about their name but the AI clarified   
that personal data is not retained beyond a single session for privacy rea  
sons. The AI offered further assistance if needed.',   
'text': ' The first question you asked was "what\'s $1 ~ + ~ 1 ? " ~ \}$

After asking another question, the LLM updated the summary to include the previ‐ ous conversation and correctly inferred the original question.

To get the most recent summary, we can access the memory variable we created previously:

# Check what the summary is thus far memory.load_memory_variables({})

{'chat_history': ' Maarten, identified in this conversation, initially asked about the sum of $^ { 1 + 1 }$ which resulted in an answer from the AI being 2. Subsequently, he sought clarification on his name but the AI informed him that no personal data is retained beyond a single session due to privacy reasons. The AI then offered further assistance if required. Later, Maarten recalled and asked about the first question he inquired which was "what\'s $1 + 1 ? \ " \cdot \}$

This more complex chain is illustrated in Figure 7-13 to give an overview of this additional functionality.

![## Image Analysis: 8400c00d42d92d824baf823c11857add26da2b988c8efb60547b4bb58e737de3.jpg

**Conceptual Understanding:**
This image conceptually illustrates how a Large Language Model (LLM) system can maintain and utilize conversational memory to provide contextually aware responses. Its main purpose is to demonstrate a common architecture where past interactions are summarized by one LLM, and this summarized history is then combined with the current user prompt to inform the response of another LLM. This allows the system to recall information from earlier in the conversation, overcoming the inherent statelessness of individual LLM calls and providing a more coherent and intelligent user experience.

**Content Interpretation:**
The image illustrates a process of providing 'memory' to a Large Language Model (LLM) by summarizing previous conversation history. The process involves:

1.  **Initial Conversation:** An exchange between a human and an AI, for example, "Human: I'm Maarten. AI: Hi Maarten!".
2.  **Conversation Summarization:** This prior conversation is fed into an "LLM" whose function is to "Summarize" the text. This summarized output becomes the `{conversation_history}`.
3.  **New User Prompt:** A new query from the human, such as "What is my name?", is provided as the `{user_prompt}`.
4.  **Input Aggregation:** A main processing unit (represented by the large gray box) receives both the `{conversation_history}` and the `{user_prompt}` as inputs, along with other unspecified inputs (empty rounded rectangles).
5.  **Chained LLM Processing:** These aggregated inputs are then passed to another "LLM" which is conceptually 'chained' to the input processing unit. This LLM processes the combined context.
6.  **Contextual Response Generation:** The chained LLM produces a response that utilizes the past conversation history. In this case, it outputs "Your name is Maarten.", correctly recalling the name mentioned earlier.

The image demonstrates the concepts of conversational memory, LLM chaining, and the use of summarization as a strategy to manage context for more intelligent and continuous interactions with Large Language Models. The distinct labeling of `{conversation_history}` and `{user_prompt}` highlights the specific inputs being combined.

**Key Insights:**
The main takeaways and insights from this image are:

*   **LLMs can maintain conversational context through summarization:** The explicit "LLM" box labeled "Summarize" demonstrates that a separate LLM can be used to distill previous conversations into a concise `{conversation_history}`. This is crucial for managing the LLM's context window and enabling long-term memory in conversations.
*   **Context and current input are combined for intelligent responses:** The diagram shows that the `{conversation_history}` and the current `{user_prompt}` are both fed into the main LLM. This highlights that for an LLM to provide a contextually relevant answer, it needs access to both what has been said previously and the current query. The correct answer "Your name is Maarten." confirms this successful integration.
*   **Complex LLM applications can involve chaining multiple LLMs:** The presence of two distinct LLMs (one for summarization and another for the final response) connected in a flow suggests a modular, chained architecture. This implies that different LLMs can be tasked with specific functions (like summarization) within a larger system to achieve a more complex goal (like maintaining conversational memory). The 'chain' symbol further reinforces this concept.

**Document Context:**
This image directly relates to the document's broader narrative about enhancing LLM capabilities with memory. It specifically illustrates the mechanism mentioned in the surrounding text: "We extend the LLM chain with memory by summarizing the entire conversation history before giving it to the input prompt." The diagram visually explains how `memory.load_memory_variables({})` might be implemented conceptually, showing the flow from initial conversation to summarization, and then the integration of that summary (as `conversation_history`) with a new `user_prompt` into an LLM chain. It provides a concrete, visual example of the abstract concept of adding memory to an LLM, specifically using a summarization approach to manage conversational context.

**Summary:**
This diagram illustrates a method for extending a Large Language Model (LLM) with memory, enabling it to remember past conversational turns and provide contextually aware responses.

The process begins with an initial human-AI interaction, such as "Human: I'm Maarten. AI: Hi Maarten!". To retain this context for future queries, this past conversation history is fed into a dedicated "LLM" which performs a "Summarize" operation. The output of this summarization is a compressed representation of the conversation, labeled as "{conversation_history}".

When the human then asks a new question, such as "What is my name?", this current query is labeled as "{user_prompt}". Both the "{conversation_history}" (the summarized memory) and the "{user_prompt}" (the current question) are then bundled together as inputs into a larger processing unit. This aggregation of inputs, including the historical context, is then passed to another "LLM" that is "chained" to this input processing.

This final, chained LLM then uses both the current query and the summarized memory to generate an informed response. In this example, despite the question "What is my name?" not containing the name itself, the LLM successfully retrieves it from the "{conversation_history}" and responds with "Your name is Maarten." This demonstrates how summarizing past interactions allows the LLM to effectively manage and utilize conversational memory, making the interaction more natural and intelligent.](images/8400c00d42d92d824baf823c11857add26da2b988c8efb60547b4bb58e737de3.jpg)
Figure 7-13. We extend the LLM chain with memory by summarizing the entire conver‐ sation history before giving it to the input prompt.

This summarization helps keep the chat history relatively small without using too many tokens during inference. However, since the original question was not explicitly saved in the chat history, the model needed to infer it based on the context. This is a disadvantage if specific information needs to be stored in the chat history. Moreover, multiple calls to the same LLM are needed, one for the prompt and one for the summarization. This can slow down computing time.

Often, it is a trade-off between speed, memory, and accuracy. Where Conversation BufferMemory is instant but hogs tokens, ConversationSummaryMemory is slow but frees up tokens to use. Additional pros and cons of the memory types we have explored thus far are described in Table 7-1.

Table 7-1. The pros and cons of different memory types.   

<table><tr><td>Memory type Pros</td><td></td><td>Cons</td></tr><tr><td>Conversation Buffer</td><td>·Easiest implementation ·Ensures no information loss within context window</td><td>· Slower generation speed as more tokens are needed ·Only suitable for large-context LLMs ·Larger chat histories make information retrieval difficult</td></tr><tr><td>Windowed Conversation Buffer</td><td>· Large-context LLMs are not needed unless chat history is large ·No information loss over the last k interactions</td><td>· Only captures the last k interactions ·No compression of the last k interactions</td></tr><tr><td>Conversation Summary</td><td>· Captures the fll hstory ·Enables long conversations ·Reduces tokens needed to capture full history</td><td>·An additional callis necessary for each interaction · Quality is reliant on the LLM&#x27;s summarization capabilities</td></tr></table>

# Agents: Creating a System of LLMs

Thus far, we have created systems that follow a user-defined set of steps to take. One of the most promising concepts in LLMs is their ability to determine the actions they can take. This idea is often called agents, systems that leverage a language model to determine which actions they should take and in what order.

Agents can make use of everything we have seen thus far, such as model I/O, chains, and memory, and extend it further with two vital components:

• Tools that the agent can use to do things it could not do itself • The agent type, which plans the actions to take or tools to use

Unlike the chains we have seen thus far, agents are able to show more advanced behavior like creating and self-correcting a roadmap to achieve a goal. They can interact with the real world through the use of tools. As a result, these agents can perform a variety of tasks that go beyond what an LLM is capable of in isolation.

For example, LLMs are notoriously bad at mathematical problems and often fail at solving simple math-based tasks but they could do much more if we provide access to a calculator. As illustrated in Figure 7-14, the underlying idea of agents is that they utilize LLMs not only to understand our query but also to decide which tool to use and when.

![## Image Analysis: c302c1e4d5e6c7c5be3136a1d16e2279166beaab42bcdc09af3d1326ea915289.jpg

**Conceptual Understanding:**
This image conceptually represents the enhancement of Large Language Model (LLM) capabilities through tool integration. Its main purpose is to demonstrate that LLMs achieve significantly higher accuracy and reliability when they are given the ability to use external tools, such as a calculator, for tasks that require precise computation. The image communicates the key idea that while LLMs may struggle with exact numerical problems on their own, their performance can be dramatically improved by acting as intelligent agents that know when and how to leverage specialized external resources.

**Content Interpretation:**
The image illustrates two distinct processes an LLM undergoes when faced with a mathematical problem: one without external tools and another with external tools. In the 'Without tools' process, the LLM directly provides an answer, which is incorrect. In the 'With tools' process, the LLM first expresses its intent to use a calculator, then displays the calculation performed by the calculator, and finally provides the correct answer derived from the tool. This directly shows the enhancement in accuracy and capability when LLMs are empowered to utilize specialized external tools. The processes demonstrate the difference between an LLM's inherent computational limitations and its augmented abilities as an 'agent' capable of tool-use.

**Key Insights:**
The main takeaways from this image are: 1. Large Language Models (LLMs) have inherent limitations in performing precise arithmetic calculations accurately when relying solely on their internal knowledge. This is evidenced by the 'Incorrect answer' of '7.34' for '47 / 12 × 3.14' in the 'Without tools' scenario. 2. Empowering LLMs with the ability to autonomously identify and utilize appropriate external tools significantly enhances their accuracy and problem-solving capabilities. The LLM's statement 'Let me use a calculator to answer your question.' and the subsequent 'Correct answer' of '12.2983' using the calculator prove this point. 3. The integration of tool-use transforms LLMs into more robust and reliable 'agents' capable of tackling a wider range of problems, especially those requiring specific, non-generative computations.

**Document Context:**
This image directly supports the document's section on "Agents: Creating a System of LLMs" and the accompanying text: "Figure 7-14. Giving LLMs the ability to choose which tools they use for a particular problem results in more complex and accurate behavior." It provides a clear, visual, and concrete example of *how* and *why* integrating tool-use capabilities into LLMs leads to improved performance, especially for tasks requiring precision that LLMs might otherwise struggle with. It contextualizes the abstract concept of LLM agents by showing a practical application where a tool (calculator) is instrumental in achieving a correct outcome.

**Summary:**
The image presents a side-by-side comparison illustrating the performance of a Large Language Model (LLM) when answering a mathematical question, both "Without tools" and "With tools".

On the left, under the heading "Without tools", a user, represented by a human icon, asks "What is 47 / 12 × 3.14?". The LLM, represented by a small blue robot icon, directly provides its answer in a speech bubble: "The answer is 7.34". An upward-pointing arrow originating from below this answer bubble points to the text label "Incorrect answer", clearly indicating that the LLM's direct response is erroneous when not allowed to use external aids.

On the right, under the heading "With tools", the same user, again represented by the human icon, asks the identical question: "What is 47 / 12 × 3.14?". In this scenario, the LLM, represented by a small blue robot icon, first states its intention in a speech bubble: "Let me use a calculator to answer your question." Following this, a brown calculator icon is shown, beneath which the precise calculation is explicitly displayed: "47 / 12 × 3.14 = 12.2983". After successfully using the calculator, the LLM then provides its final answer in another speech bubble: "The answer is 12.2983". An upward-pointing arrow originating from below this final answer bubble points to the text label "Correct answer", signifying that the LLM's response is accurate as a result of employing an external computational tool.

This diagram serves to clearly illustrate that while an LLM may perform inaccurately on its own for tasks requiring precise arithmetic, its ability to recognize the need for and then effectively utilize specialized external tools like a calculator dramatically improves its accuracy and overall problem-solving capabilities for such specific types of questions.](images/c302c1e4d5e6c7c5be3136a1d16e2279166beaab42bcdc09af3d1326ea915289.jpg)
Figure 7-14. Giving LLMs the ability to choose which tools they use for a particular problem results in more complex and accurate behavior.

In this example, we would expect the LLM to use the calculator when it faces a math‐ ematical task. Now imagine we extend this with dozens of other tools, like a search engine or a weather API. Suddenly, the capabilities of LLMs increase significantly.

In other words, agents that make use of LLMs can be powerful general problem solv‐ ers. Although the tools they use are important, the driving force of many agent-based systems is the use of a framework called Reasoning and Acting (ReAct1).

# The Driving Power Behind Agents: Step-by-step Reasoning

ReAct is a powerful framework that combines two important concepts in behavior: reasoning and acting. LLMs are exceptionally powerful when it comes to reasoning as we explored in detail in Chapter 5.

Acting is a bit of a different story. LLMs are not able to act like you and I do. To give them the ability to act, we could tell an LLM that it can use certain tools, like a weather forecasting API. However, since LLMs can only generate text, they would need to be instructed to use specific queries to trigger the forecasting API.

ReAct merges these two concepts and allows reasoning to affect acting and actions to affect reasoning. In practice, the framework consists of iteratively following these three steps:

• Thought • Action • Observation

Illustrated in Figure 7-15, the LLM is asked to create a “thought” about the input prompt. This is similar to asking the LLM what it thinks it should do next and why. Then, based on the thought, an “action” is triggered. The action is generally an external tool, like a calculator or a search engine. Finally, after the results of the “action” are returned to the LLM it “observes” the output, which is often a summary of whatever result it retrieved.

To illustrate with an example, imagine you are on holiday in the United States and interested in buying a MacBook Pro. Not only do you want to know the price but you need it converted to EUR as you live in Europe and are more comfortable with those prices.

As illustrated in Figure 7-16, the agent will first search the web for current prices. It might find one or more prices depending on the search engine. After retrieving the price, it will use a calculator to convert USD to EUR assuming we know the exchange rate.

![## Image Analysis: 345c8fc54a1ca2c795359a9addfb3f7552e7fa95637a689d3c7680972c840bbb.jpg

**Conceptual Understanding:**
The image conceptually illustrates a fundamental reasoning framework called 'ReAct' used by agents, likely artificial intelligence agents, to systematically address and solve questions posed by a user. The main purpose of the image is to clearly define and explain the three core, interleaved steps—'Thought', 'Action', and 'Observation'—that constitute this framework, along with their respective roles and specific capabilities. It communicates the key idea that complex problem-solving by an agent can be broken down into a structured, iterative process where the agent first reasons internally, then performs an external action, and finally observes the outcome to guide its next steps, aiming for a transparent and effective problem-solving approach.

**Content Interpretation:**
The image presents the conceptual process flow of an agent designed to solve user questions, specifically employing the 'ReAct' framework. This framework decomposes the agent's problem-solving into three distinct, interleaved stages: 'Thought', 'Action', and 'Observation'. The diagram illustrates the operational definitions and roles of these stages within the agent's reasoning cycle. The 'Instruction' box sets the overarching goal for the agent. The 'ReAct steps' specify the sequential and iterative components of the agent's reasoning. The 'Step descriptions' elaborate on the nature of each ReAct step. 'Thought' represents the internal cognitive process of reasoning. 'Action' signifies external engagement, explicitly listing 'Search[entity]' and 'Calculator[formula]' as concrete tools or functions the agent can utilize. 'Observation' indicates the feedback mechanism, where the agent processes the outcome of its 'Action'. The relationships are hierarchical and sequential, showing how an overall instruction leads to a structured reasoning process composed of defined, interleaving steps.

**Key Insights:**
The main takeaway from this image is the clear articulation of the ReAct framework as a structured, iterative approach for agents to solve problems. It teaches that agent reasoning is not a monolithic process but a cycle of internal deliberation ('Thought') followed by external engagement ('Action') and subsequent learning from results ('Observation'). The explicit definition of action types (e.g., 'Search[entity]', 'Calculator[formula]') provides insight into the practical capabilities and tool-use of such agents. A key insight is the importance of the 'Observation' step for closing the feedback loop, enabling the agent to refine its understanding and strategy. The sequential and interleaved nature of these steps highlights a robust, transparent methodology for building intelligent agents. All transcribed text directly defines and explains these core concepts, providing the foundational evidence for these insights.

**Document Context:**
This image directly supports the document's section titled 'The Driving Power Behind Agents: Step-by-step Reasoning'. It serves as a visual example, as indicated by 'Figure 7-15. An example of a ReAct prompt template' (from the text after the image), illustrating the fundamental ReAct reasoning process that underpins agent behavior. By breaking down the complex concept of agent reasoning into discrete, understandable steps—Thought, Action, and Observation—it provides a concrete example of the 'step-by-step reasoning' agents employ to tackle user queries, thereby enhancing the reader's comprehension of the agent's operational methodology discussed in the surrounding text.

**Summary:**
The image illustrates the core components and a high-level process for an agent using the ReAct reasoning framework to solve a user's question. It outlines a structured, iterative approach combining internal reasoning with external interaction and feedback. The process begins with an overarching 'Instruction' to 'Solve the question from the user'. This instruction is carried out through 'ReAct steps', which are a sequence of interleaving stages: 'Thought', 'Action', and 'Observation'. Each of these steps is further defined by 'Step descriptions'. A 'Thought' is described as the ability to 'reason about the current situation'. An 'Action' is specified as being one of two types: '(1) Search[entity]' or '(2) Calculator[formula]', indicating the agent's capabilities for information retrieval and numerical computation. Finally, an 'Observation' is defined as 'the result of an action', completing the feedback loop and allowing the agent to evaluate its progress and adapt. This diagram provides a clear breakdown of the fundamental elements an agent employs for step-by-step problem-solving.](images/345c8fc54a1ca2c795359a9addfb3f7552e7fa95637a689d3c7680972c840bbb.jpg)
Figure 7-15. An example of a ReAct prompt template.

![## Image Analysis: 29e7ad7c3eaa9881dcc33c50943fdf54c952fb7082bd6bf2ba9a59ec68cb66cd.jpg

**Conceptual Understanding:**
This image conceptually represents a 'ReAct' (Reasoning and Acting) pipeline, which is a framework for enabling Large Language Models (LLMs) to perform tasks that require both reasoning and interacting with external environments or tools. The main purpose of the image is to illustrate, in a step-by-step manner, how an LLM can break down a complex user query into smaller, manageable sub-problems, use appropriate tools to find or compute necessary information, and then synthesize these results to generate a comprehensive answer. It visually explains the iterative process where the LLM's internal 'thoughts' guide its 'actions', and the 'observations' from those actions, in turn, inform subsequent thoughts, creating a continuous feedback loop towards solving the overall problem.

**Content Interpretation:**
The image shows a ReAct (Reasoning and Acting) pipeline demonstrating how a Large Language Model (LLM) iteratively solves a complex problem by generating thoughts, performing actions using external tools, and integrating observations. The pipeline illustrates a two-cycle process to answer a multi-part query.

**Processes Shown:**
1.  **Initial Prompt Processing:** The LLM receives a complex query that requires multiple steps to answer.
2.  **ReAct Cycle 1 (Information Retrieval):**
    *   **Thought Generation:** The LLM internally reasons about the next logical step.
    *   **Action Selection:** Based on its thought, the LLM selects an appropriate tool and formulates an action to use it.
    *   **Tool Execution:** An external tool (Google) is invoked to perform the specified action.
    *   **Observation Integration:** The results from the tool are provided back to the LLM as an observation.
3.  **ReAct Cycle 2 (Calculation):**
    *   **Subsequent Thought Generation:** The LLM uses the previous observation to inform its next reasoning step.
    *   **Subsequent Action Selection:** The LLM selects another tool (Calculator) and formulates an action to perform a calculation.
    *   **Tool Execution:** The Calculator tool performs the arithmetic operation.
    *   **Final Observation/Answer:** The result of the calculation is observed by the LLM, leading to the final answer to the prompt.

**Concepts and Relationships:**
*   **LLM as Agent:** The LLM acts as an intelligent agent, capable of reasoning ("Thought"), planning ("Action"), and learning from its environment ("Observation").
*   **Tool Integration:** LLMs can extend their capabilities by interacting with external tools (like search engines or calculators) to perform tasks beyond their internal knowledge or reasoning capabilities.
*   **Iterative Problem Solving:** Complex problems are broken down into smaller, manageable steps, and the solution is built up iteratively through multiple cycles of Thought-Action-Observation.
*   **Reasoning and Acting (ReAct):** The core framework where the LLM's reasoning informs its actions, and the results of those actions inform further reasoning.

**Significance of Information Presented:**
*   The specific query "What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?" highlights a need for both external data retrieval (USD price) and computation (EUR conversion).
*   The "Thought" bubbles (`"I should search the web"`, `"I need to use a calculator to do the calculation"`) explicitly show the LLM's internal reasoning process, demonstrating its ability to plan.
*   The "Action" bubbles (`"Google [price MacBook Pro]"`, `"Calculator [1,299 × .85]"`) demonstrate the LLM's ability to translate its thoughts into concrete, executable commands for tools.
*   The "Tool" execution (`Google "price MacBook Pro"`, `Calculator 1,299 × .85 = 1104.15`) confirms the successful interaction with external utilities.
*   The "Observation" bubbles (`"The price is $1,299,-"`, `"A MacBook Pro would cost €1104.15"`) illustrate how the LLM synthesizes the tool's output into a usable piece of information for its subsequent reasoning or final answer. The intermediate observation becomes a crucial input for the next cycle.

**Key Insights:**
The image provides several key takeaways about the capabilities and operational mechanisms of AI agents, particularly those based on Large Language Models (LLMs) using the ReAct framework:

1.  **Multi-step Problem Solving:** LLMs, when augmented with reasoning and acting capabilities, can effectively tackle complex queries that require multiple distinct steps and different types of information or operations. The initial prompt asking for both USD price and EUR conversion exemplifies this.
2.  **Integration of Reasoning and Action:** The ReAct pipeline demonstrates how an LLM's internal "Thought" processes directly inform its external "Action" choices. This tight coupling allows the agent to plan and execute tasks strategically, as seen by the LLM deciding to "search the web" first and then use a "calculator."
3.  **Leveraging External Tools:** LLMs can extend their inherent capabilities by intelligently selecting and using external tools. For instance, "Google [price MacBook Pro]" for information retrieval and "Calculator [1,299 × .85]" for arithmetic operations show how tools are seamlessly integrated to overcome limitations in the LLM's real-time knowledge or mathematical prowess.
4.  **Iterative Refinement and Learning:** The concept of "ReAct cycle 1" and "ReAct cycle 2" illustrates an iterative process where observations from one step (`"The price is $1,299,-"`) are used as input for the next reasoning and action phase (`"I need to use a calculator to do the calculation"`). This feedback loop enables the agent to progressively move towards a solution.
5.  **Transparency in Agent Operation:** The explicit labeling of "Thought," "Action," and "Observation" provides a degree of transparency into the agent's decision-making process, making it easier to understand *why* it chose certain steps. This helps in debugging and building trust in agent behavior.

These insights are directly supported by the verbatim text extractions: the sequential flow from "Initial prompt" to `ReAct cycle 1` (Thought, Action, Tool, Observation) and then to `ReAct cycle 2` (Thought, Action, Tool, Observation) demonstrates the iterative problem-solving. The specific content within each "Thought" (`"I should search the web"`, `"I need to use a calculator to do the calculation"`), "Action" (`"Google [price MacBook Pro]"`, `"Calculator [1,299 × .85]"`), and "Observation" (`"The price is $1,299,-"`, `"A MacBook Pro would cost €1104.15"`) boxes provides concrete evidence for how reasoning leads to action and how tool outputs are assimilated.

**Document Context:**
This image directly supports the document's section titled "The Driving Power Behind Agents: Step-by-step Reasoning" by providing a clear, visual example of a ReAct pipeline in action. It illustrates how an agent, specifically an LLM, can perform complex, multi-step reasoning tasks by breaking them down and leveraging external tools. The image acts as a concrete demonstration of the theoretical concepts discussed in the surrounding text, showing an iterative problem-solving approach where the agent's 'thinking' (Thought, Observation) directly guides its 'acting' (Action, Tool use). It answers the question of *how* agents perform step-by-step reasoning by showing the explicit flow of information and decision-making.

**Summary:**
The image illustrates a two-cycle ReAct pipeline where a Large Language Model (LLM) processes an initial prompt by breaking it down into steps, performing actions using tools, and making observations to arrive at a final answer. The process begins with an initial prompt asking for the price of a MacBook Pro in USD and its equivalent cost in EUR, given an exchange rate. 

In ReAct cycle 1, the LLM first formulates the "Thought" "I should search the web". Based on this thought, it decides on an "Action" which is "Google [price MacBook Pro]". This action is then performed by a "Tool" identified as "Google" with the input "price MacBook Pro". After the tool executes, the "Show results" arrow leads to an LLM "Observation" stating "The price is $1,299,-".

This observation then feeds into ReAct cycle 2. In this cycle, the LLM has the "Thought" "I need to use a calculator to do the calculation". It then determines the "Action" as "Calculator [1,299 × .85]". This action is performed by a "Tool" identified as "Calculator", which executes the operation "1,299 × .85 = 1104.15". Following the "Show results" arrow, the LLM makes a final "Observation" that "A MacBook Pro would cost €1104.15". This systematic, iterative process demonstrates how an LLM can use external tools and its own reasoning to address complex queries requiring multiple steps.](images/29e7ad7c3eaa9881dcc33c50943fdf54c952fb7082bd6bf2ba9a59ec68cb66cd.jpg)
Figure 7-16. An example of two cycles in a ReAct pipeline.

During this process, the agent describes its thoughts (what it should do), its actions (what it will do), and its observations (the results of the action). It is a cycle of thoughts, actions, and observations that results in the agent’s output.

# ReAct in LangChain

To illustrate how agents work in LangChain, we are going to build a pipeline that can search the web for answers and perform calculations with a calculator. These autonomous processes generally require an LLM that is powerful enough to properly follow complex instructions.

The LLM that we used thus far is relatively small and not sufficient to run these examples. Instead, we will be using OpenAI’s GPT-3.5 model as it follows these complex instructions more closely:

import os from langchain_openai import ChatOpenAI

# Load OpenAI's LLMs with LangChain   
os.environ["OPENAI_API_KEY"] $=$ "MY_KEY"   
openai_llm $=$ ChatOpenAI(model_name="gpt-3.5-turbo", temperature $\scriptstyle 1 = 0$ )

Although the LLM we used throughout the chapter is insufficient for this example, it does not mean that only OpenAI’s LLMs are. Larger useful LLMs exist but they require significantly more com‐ pute and VRAM. For instance, local LLMs often come in different sizes and within a family of models, increasing a model’s size leads to better performance. To keep the necessary compute at a mini‐ mum, we choose a smaller LLM throughout the examples in this chapter.

However, as the field of generative models evolves, so do these smaller LLMs. We would be anything but surprised if eventually smaller LLMs, like the one used in this chapter, would be capable enough to run this example.

After doing so, we will define the template for our agent. As we have shown before, it describes the ReAct steps it needs to follow:

# Create the ReAct template   
react_template $=$ """Answer the following questions as best you can. You have   
access to the following tools:

# {tools}

Use the following format:

Question: the input question you must answer Thought: you should always think about what to do

Action: the action to take, should be one of [{tool_names}]   
Action Input: the input to the action   
Observation: the result of the action   
... (this Thought/Action/Action Input/Observation can repeat N times)   
Thought: I now know the final answer   
Final Answer: the final answer to the original input question

Begin!

Question: {input} Thought:{agent_scratchpad}"

prompt $=$ PromptTemplate( template=react_template, input_variables=["tools", "tool_names", "input", "agent_scratchpad"]   
)

This template illustrates the process of starting with a question and generating inter‐ mediate thoughts, actions, and observations.

To have the LLM interact with the outside world, we will describe the tools it can use:

from langchain.agents import load_tools, Tool from langchain.tools import DuckDuckGoSearchResults

# You can create the tool to pass to an agent   
search $=$ DuckDuckGoSearchResults()   
search_tool $=$ Tool( name="duckduck", description="A web search engine. Use this to as a search engine for gen   
eral queries.", func search.run,   
)   
# Prepare tools   
tools $=$ load_tools(["llm-math"], llm $\left| = \right.$ openai_llm)   
tools.append(search_tool)

The tools include the DuckDuckGo search engine and a math tool that allows it to access a basic calculator.

Finally, we create the ReAct agent and pass it to the AgentExecutor, which handles executing the steps:

from langchain.agents import AgentExecutor, create_react_agent

# Construct the ReAct agent   
agent $=$ create_react_agent(openai_llm, tools, prompt)   
agent_executor $=$ AgentExecutor( agent $\ l =$ agent, tools $\mathbf { \sigma } =$ tools, verbose=True, handle_parsing_error $: =$ True   
)

To test whether the agent works, we use the previous example, namely finding the price of a MacBook Pro:

# What is the price of a MacBook Pro? agent_executor.invoke( { "input": "What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD." } ) While executing, the model generates multiple intermediate steps similar to the steps illustrated in Figure 7-17.

>Entering new AgentExecutor chain...   
I need to find the current price of a MacBook Pro in USD first before converting it to EUR. Action: duckduck   
Action Input: "current price of MacBook Pro in USD"[snippet: View at Best Buy. The best MacE Action: Calculator   
Action Input: \$2,249.00 \* 0.85Answer: 1911.649999999999I now know the final answer Final Answer: The current price of α MacBook Pro in USD is \$2,249.00. It would cost approxin

Figure 7-17. An example of the ReAct process in LangChain.

These intermediate steps illustrate how the model processes the ReAct template and what tools it accesses. This allows us to debug issues and explore whether the agent uses the tools correctly.

When finished, the model gives us an output like this:

{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?', 'output': 'The current price of a MacBook Pro in USD is $\$ 2$ ,249.00. It would cost approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.'}

Considering the limited tools the agent has, this is quite impressive! Using just a search engine and a calculator the agent could give us an answer.

Whether that answer is actually correct should be taken into account. By creating this relatively autonomous behavior, we are not involved in the intermediate steps. As such, there is no human in the loop to judge the quality of the output or reasoning process.

This double-edged sword requires a careful system design to improve its reliability. For instance, we could have the agent return the website’s URL where it found the MacBook Pro’s price or ask whether the output is correct at each step.

# Summary

In this chapter, we explored several ways to extend the capabilities of LLMs by adding modular components. We began by creating a simple but reusable chain that connected the LLM with a prompt template. We then expanded on this concept by adding memory to the chain, which allowed the LLM to remember conversations. We explored three different methods to add memory and discussed their strengths and weaknesses.

We then delved into the world of agents that leverage LLMs to determine their actions and make decisions. We explored the ReAct framework, which uses an intuitive prompting framework that allows agents to reason about their thoughts, take actions, and observe the results. This led us to build an agent that is able to freely use the tools at its disposal, such as searching the web and using a calculator, demonstrating the potential power of agents.

With this foundation in place, we are now poised to explore ways in which LLMs can be used to improve existing search systems and even become the core of new, more powerful search systems, as discussed in the next chapter.

# Semantic Search and Retrieval-Augmented Generation

Search was one of the first language model applications to see broad industry adop‐ tion. Months after the release of the seminal “BERT: Pre-training of deep bidirec‐ tional transformers for language understanding” (2018) paper, Google announced it was using it to power Google Search and that it represented “one of the biggest leaps forward in the history of Search.” Not to be outdone, Microsoft Bing also stated that “Starting from April of this year, we used large transformer models to deliver the largest quality improvements to our Bing customers in the past year.”

This is a clear testament to the power and usefulness of these models. Their addi‐ tion instantly and dramatically improves some of the most mature, well-maintained systems that billions of people around the planet rely on. The ability they add is called semantic search, which enables searching by meaning, and not simply keyword matching.

On a separate track, the fast adoption of text generation models led many users to ask the models questions and expect factual answers. And while the models were able to answer fluently and confidently, their answers were not always correct or up-to-date. This problem grew to be known as model “hallucinations,” and one of the leading ways to reduce it is to build systems that can retrieve relevant information and provide it to the LLM to aid it in generating more factual answers. This method, called RAG, is one of the most popular applications of LLMs.

# Overview of Semantic Search and RAG

There’s a lot of research on how to best use language models for search. Three broad categories of these models are dense retrieval, reranking, and RAG. Here is an overview of these three categories that the rest of the chapter will then explain in more detail:

# Dense retrieval

Dense retrieval systems rely on the concept of embeddings, the same concept we’ve encountered in the previous chapters, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings). Figure 8-1 shows how dense retrieval takes a search query, consults its archive of texts, and outputs a set of relevant results.

![## Image Analysis: 9959b6e4a675814181b201b2baa937740d6dfbfc693bc4fe741c75fc8fb8b9dc.jpg

**Conceptual Understanding:**
This image conceptually represents the workflow of a dense retrieval system. Its main purpose is to illustrate the input, processing, and output stages of semantic search using dense retrieval. The key ideas communicated are that a 'Search query' is transformed and compared (via 'Dense retrieval') against a corpus of 'Text' documents to identify and rank the most relevant documents, which are then presented as 'Results'. The interaction between dense retrieval and the text corpus, indicated by the bidirectional arrow, implies a process of embedding generation and similarity comparison.

**Content Interpretation:**
The image illustrates the core process of a dense retrieval system, a method used in semantic search. It shows how a 'Search query' is processed against a collection of 'Text' documents to yield ranked 'Results'. The central 'Dense retrieval' component processes the input query and interacts with the document corpus, likely by comparing vector embeddings, to identify and rank the most relevant documents. The listed results ('1- Document #40', '2- Document #68', '3- Document #2') signify the output of this ranking process, showing specific documents identified from a larger set.

**Key Insights:**
The main takeaway from this image is that dense retrieval functions as an intermediary system that takes a natural language 'Search query' and, by interacting with a 'Text' corpus (a collection of documents like 'Document #1', 'Document #2', 'Document #3', etc.), produces a ranked list of 'Results'. These results, such as '1- Document #40', '2- Document #68', and '3- Document #2', represent the most relevant documents according to the dense retrieval algorithm. This highlights the system's ability to not only identify relevant documents but also to order them by their perceived relevance to the original query.

**Document Context:**
This image serves as a direct visual explanation for the concept of dense retrieval, particularly within the context of semantic search. As stated in the surrounding document text, 'Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results,' this diagram precisely demonstrates that reliance. It visually maps the input (search query), the core process (dense retrieval interacting with a text corpus), and the output (ranked results), thereby enhancing the reader's understanding of how this specific type of semantic search operates.

**Summary:**
This diagram illustrates the operational flow of a Dense Retrieval system for semantic search. The process begins with a user's 'Search query', which is input into the 'Dense retrieval' system. The 'Dense retrieval' system then interacts bidirectionally with a corpus of 'Text', which is shown as a stack of documents, starting with 'Document #1', 'Document #2', 'Document #3', and then an ellipsis '...' to indicate more documents. The interaction signifies that the dense retrieval process compares the query against the embeddings of these documents. After processing, the 'Dense retrieval' system outputs 'Results', presented as a ranked list. The example results are '1- Document #40', '2- Document #68', and '3- Document #2', indicating the most relevant documents retrieved from the corpus in descending order of relevance. This visual effectively breaks down the abstract concept of dense retrieval into clear, sequential steps.](images/9959b6e4a675814181b201b2baa937740d6dfbfc693bc4fe741c75fc8fb8b9dc.jpg)
Figure 8-1. Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results.

# Reranking

Search systems are often pipelines of multiple steps. A reranking language model is one of these steps and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores. Figure 8-2 shows how rerankers are different from dense retrieval in that they take an additional input: a set of search results from a previous step in the search pipeline.

![## Image Analysis: f0f3dea6cdf050ea0a5be5968bd42fc273da324acc9bc78b7c7150c94a097739.jpg

**Conceptual Understanding:**
This image conceptually represents the function of a "Reranker" in a search system, specifically within the context of semantic search. The main purpose conveyed is to illustrate how a reranker takes an initial set of search results and a query, then reorders these results to improve their relevance, thereby enhancing the user's search experience. The key idea being communicated is that raw initial results can be significantly refined and optimized for relevance through an additional processing step.

**Content Interpretation:**
The image demonstrates a process flow for improving search result relevance.

*   The **"Query"** box signifies the user's explicit information need. This is the primary input driving the search.
*   The table titled **"Small number of relevant texts"** represents the initial output from a preliminary search or retrieval system. It shows three documents: "Document #40" as result #1, "Document #68" as result #2, and "Document #2" as result #3. The header "Small number of relevant texts" implies that these documents have already passed some initial relevance threshold, but their order might not be perfect.
*   The central **"Reranker"** box represents the core system or algorithm responsible for re-evaluating and reordering the documents. The arrow leading from both the "Query" and the initial results table to the "Reranker" indicates that the reranker uses both the query context and the initial document set to perform its function. The small graph-like icon in the top-right of the "Reranker" box subtly suggests an optimization or improvement process.
*   The **"Results (in improved order)"** section on the right is the output of the reranker. This is significant because it explicitly shows how the order of documents has changed:
    *   "1- Document #2 (was result #3)" indicates that "Document #2", initially ranked third, has now been elevated to the first position, implying it is the most relevant.
    *   "2- Document #40 (was result #1)" shows "Document #40", initially ranked first, has moved to the second position, suggesting it's less relevant than Document #2 but still highly relevant.
    *   "3- Document #68 (was result #2)" shows "Document #68", initially ranked second, has moved to the third position.

All extracted text elements clearly support the interpretation that the image depicts a system where an initial ranking is refined by a reranker to achieve a more accurate and relevant final ordering of documents in response to a query. The "improved order" text, along with the specific "was result #" notations, serves as direct evidence of this reordering and enhancement.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Rerankers enhance search result quality:** The image clearly shows that the "Reranker" takes an initial set of results and produces "Results (in improved order)." This directly demonstrates the value proposition of a reranker: to improve the ordering of search results based on relevance.
2.  **Initial relevance is not always optimal:** The fact that documents are reordered (e.g., "Document #2" moving from "#3" to "#1") highlights that initial search retrieval systems might not always present the most relevant documents at the very top. There's room for improvement.
3.  **Rerankers leverage both the query and initial results:** The arrows indicating input from both "Query" and "Small number of relevant texts" to the "Reranker" show that the reranking process considers both the user's explicit intent (the query) and the content of the initially retrieved documents to determine a better order.
4.  **Specific examples illustrate the improvement:** The detailed breakdown of the "Results (in improved order)" (e.g., "1- Document #2 (was result #3)") provides concrete evidence of how the reranker re-prioritizes documents, making the improvement tangible and understandable.

These insights are directly supported by the textual elements such as "Small number of relevant texts" (implying room for reordering), "Reranker" (the agent of change), "Results (in improved order)" (the outcome), and the specific "was result #" annotations which quantify the improvement.

**Document Context:**
This image fits directly into the document's narrative by visually explaining the concept of "Reranking" as a critical component of semantic search, as stated in the text "Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results." It serves as a clear, simplified diagram demonstrating the input, process, and output of a reranker, solidifying the abstract concept with a concrete example of how document relevance is improved. It addresses the "how" of reranking, showing the flow of information and the transformation of result order.

**Summary:**
The diagram illustrates the process of how a "Reranker" works to improve the order of search results for a given query. It begins with two inputs:

1.  **The Query:** Represented by an oval shape with a magnifying glass icon and the text "Query", this is the initial search request made by the user.
2.  **Initial Relevant Texts:** This is a table labeled "Small number of relevant texts" which provides a preliminary set of search results. In this example, the initial order of documents is:
    *   1: Document #40
    *   2: Document #68
    *   3: Document #2

Both the "Query" and these initial "Small number of relevant texts" are then fed into the central component: the **"Reranker"**. This "Reranker" is depicted as a light blue rectangular box, signifying its role as a processing unit. It analyzes both the query and the content of the initial documents to determine a more accurate order of relevance.

After processing by the "Reranker", the system produces **"Results (in improved order)"**. This output shows the reordered documents, highlighting how their relevance has been enhanced:
*   The document now ranked first is "1- Document #2", which was originally found as "result #3". This indicates a significant improvement in its perceived relevance.
*   The document now ranked second is "2- Document #40", which was originally "result #1". This means it was still relevant but is now deemed less so than Document #2.
*   The document now ranked third is "3- Document #68", which was originally "result #2".

In essence, the diagram clearly shows that a reranker acts as a refinement step in a search pipeline, taking an initial set of relevant but possibly imperfectly ordered results, combining them with the original query, and producing a new list where the most relevant documents are moved to the top, resulting in a vastly improved user experience.](images/f0f3dea6cdf050ea0a5be5968bd42fc273da324acc9bc78b7c7150c94a097739.jpg)
Figure 8-2. Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results.

# RAG

The growing LLM capability of text generation led to a new type of search systems that include a model that generates an answer in response to a query. Figure 8-3 shows an example of such a generative search system.

Generative search is a subset of a broader type of category of systems better called RAG systems. These are text generation systems that incorporate search capabilities to reduce hallucinations, increase factuality, and/or ground the gen‐ eration model on a specific dataset.

![## Image Analysis: fd42be46f4e8426028a36f157e36ac7af2652cfdd8cf5838f0448ddfb8a08510.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow and operational mechanism of a Retrieval-Augmented Generation (RAG) system. Its main purpose is to demonstrate how such a system takes a user's question, retrieves relevant information from a corpus of text documents, generates an answer based on this retrieved information, and, importantly, cites the sources used to formulate that answer. The image conveys the idea of an intelligent system that not only answers questions but also provides traceability and credibility to its responses by referencing its informational basis.

**Content Interpretation:**
This image illustrates the workflow of a Retrieval-Augmented Generation (RAG) system. It depicts how a user's question is processed by the RAG component, which then leverages a set of underlying text documents to formulate an answer. The system is designed to not only provide an answer but also to explicitly cite the sources (search results) that contribute to that answer, potentially offering multiple perspectives or answers derived from different sources. The core processes shown are: question intake, retrieval-augmented generation, information retrieval from documents, answer formulation, and source attribution.

**Key Insights:**
The main takeaways from this image are: 1. RAG systems process a user's 'Question' to generate an answer. 2. The 'Retrieval-augmented generation (RAG)' component interacts with external 'Text' documents (e.g., 'Text document #1', 'Text document #2', 'Text document #3') to inform its answer. 3. RAG systems produce a 'Result' that provides an answer to the 'Question'. 4. A critical feature of RAG systems is the ability to cite its 'Sources' (e.g., '1- Search result', '2- Search result', '3- Search result'), enhancing trustworthiness and verifiability. 5. The system can provide 'one possible answer' while also acknowledging 'others have also pointed out this other possible answer. [2][3]', indicating a comprehensive and perhaps multi-faceted response capability.

**Document Context:**
The image directly supports the document's section on 'RAG' and specifically illustrates the mechanism described in the accompanying text: 'A RAG system formulates an answer to a question and (preferably) cites its information sources.' It visually breaks down how a user's question is handled by a RAG system, emphasizing the interaction with external text sources and the subsequent generation of an answer along with its citations. This enhances the reader's understanding of the RAG concept by providing a clear, step-by-step visual representation of its functionality.

**Summary:**
This image illustrates the process of a Retrieval-Augmented Generation (RAG) system. A 'Question' is first input into the system. This 'Question' is then fed into the 'Retrieval-augmented generation (RAG)' component. The RAG system actively interacts with a body of 'Text', which is composed of distinct 'Text document #1', 'Text document #2', and 'Text document #3'. Based on this interaction and retrieval, the RAG system produces a 'Result'. The 'Result' is described as: 'This is one possible answer to your question [1], although others have also pointed out this other possible answer. [2][3]'. Crucially, the system also provides 'Sources' for the information presented in the result. These 'Sources' are listed as: '1- Search result', '2- Search result', and '3- Search result'. The overall diagram demonstrates how a RAG system answers a query by drawing information from multiple specified text documents and transparently citing the search results that informed its response, including potentially alternative answers.](images/fd42be46f4e8426028a36f157e36ac7af2652cfdd8cf5838f0448ddfb8a08510.jpg)
Figure 8-3. A RAG system formulates an answer to a question and (preferably) cites its information sources.

The rest of the chapter covers these three types of systems in more detail. While these are the major categories, they are not the only LLM applications in the domain of search.

# Semantic Search with Language Models

Let’s now dive into more detail on the major categories of systems that can upgrade the search capabilities of our language models. We’ll start with dense retrieval and then move on through reranking and RAG.

# Dense Retrieval

Recall that embeddings turn text into numeric representations. Those can be thought of as points in space, as we can see in Figure 8-4. Points that are close together mean that the text they represent is similar. So in this example, text 1 and text 2 are more similar to each other (because they are near each other) than text 3 (because it’s farther away).

![## Image Analysis: a1743747801cfd67862b6acc13ca870ed0c5bf3e3fe750dd6fd08be14b197f1d.jpg

**Conceptual Understanding:**
This image conceptually represents the core principle of text embeddings, which is fundamental to modern natural language processing, especially in areas like dense retrieval. The main purpose of the image is to visually demonstrate the intuitive idea that textual content can be transformed into numerical representations (vectors or 'points' in a space), where the semantic similarity between texts is directly proportional to the spatial proximity of their corresponding points.

The key idea being communicated is that a textual entity's 'meaning' or 'semantic content' can be quantitatively encoded such that related texts are grouped together in a geometric space. The visual shows a mapping from abstract text to concrete, measurable points on a grid, illustrating how 'closeness' in meaning translates to 'closeness' in physical (or abstract numerical) space. This provides a tangible understanding of how machines can 'understand' and compare the meanings of different texts by performing mathematical operations on their numerical representations.

**Content Interpretation:**
The image illustrates the fundamental concept of text embeddings in natural language processing (NLP). It represents how individual pieces of text, specifically "Text 1", "Text 2", and "Text 3", are transformed into points within a conceptual multi-dimensional space, visually depicted as a 2D grid. The primary relationship shown is the spatial distance between these text points, which directly correlates with their semantic similarity.

Specifically:
- **"Text 1" and "Text 2"** are positioned in close proximity to each other in the upper-left part of the grid. This signifies that these two texts possess similar meanings or discuss related topics.
- **"Text 3"** is located in the lower-right part of the grid, at a considerable distance from both "Text 1" and "Text 2". This spatial separation indicates that "Text 3" is semantically dissimilar or unrelated to "Text 1" and "Text 2".

The grid itself acts as a simplified coordinate system, demonstrating that each text corresponds to a unique vector (point) in this space. The significance of this arrangement is that it allows for the computational measurement of semantic relationships: the closer two text points are, the more similar their underlying meanings are considered to be. This forms the basis for tasks like dense retrieval, where the goal is to find texts that are semantically related to a query.

**Key Insights:**
The image provides several key takeaways and insights regarding text embeddings and semantic representation:

1.  **Text Representation as Points:** The fundamental insight is that textual data (e.g., "Text 1", "Text 2", "Text 3") can be effectively transformed into discrete points within a multi-dimensional space (visually represented by the grid). This transformation is crucial for computational processing of text.

2.  **Semantic Similarity through Proximity:** The most significant lesson is that the *meaning* of text is encoded in its *position* in this embedding space. Texts with similar meanings (e.g., "Text 1" and "Text 2" being close) will be located near each other, while texts with dissimilar meanings (e.g., "Text 3" being far from "Text 1" and "Text 2") will be distant.

3.  **Visual Intuition for Embeddings:** The simple visual arrangement clearly communicates the core intuition behind embeddings, making an abstract concept (high-dimensional vectors) concrete and understandable. It highlights that the "closeness" of ideas can be quantified and visualized.

4.  **Foundation for Dense Retrieval:** This visual is foundational for understanding dense retrieval systems. It demonstrates *why* measuring the distance between text embeddings can lead to effective semantic search and retrieval—because spatial closeness directly indicates semantic relevance. This allows systems to find relevant information even if exact keywords are not present.

**Evidence from text elements:**
- The labels "Text 1", "Text 2", and "Text 3" explicitly identify the entities being embedded.
- The *visual arrangement* of these labeled points on the grid provides the primary evidence:
    - "Text 1" and "Text 2" are placed close together, visually supporting the idea that they have similar meanings.
    - "Text 3" is placed far from both "Text 1" and "Text 2", visually supporting the idea that it has a different meaning.
- The grid itself represents the embedding space where these semantic relationships are graphically encoded.

**Document Context:**
This image directly supports the "Dense Retrieval" section by providing a foundational visual explanation of "the intuition of embeddings." It is positioned immediately before the descriptive text: "Figure 8-4. The intuition of embeddings: each text is a point and texts with similar meaning are close to each other." Therefore, it serves as a critical visual aid to introduce and clarify how text embeddings work conceptually.

The image helps readers understand:
- How textual data, which is typically unstructured, can be represented numerically in a structured space.
- The core principle that semantic similarity (meaning) can be visually and computationally translated into spatial proximity (distance) in an embedding space.
- The foundational concept required to grasp the mechanics of dense retrieval, where documents or passages are retrieved based on their semantic similarity to a query, rather than just keyword overlap. It visually sets the stage for understanding why dense retrieval is effective in capturing nuanced meaning.

**Summary:**
The image displays a two-dimensional grid, which serves as a simplified visual representation of an embedding space. Within this grid, three distinct textual entities are marked as grey circles with accompanying labels.

"Text 1" is located in the upper-left region of the grid. "Text 2" is positioned closely to the right and slightly above "Text 1". The close proximity of "Text 1" and "Text 2" visually represents their semantic similarity, implying that these texts share a related meaning or topic.

"Text 3" is situated in the lower-right region of the grid, significantly distant from both "Text 1" and "Text 2". This clear separation indicates that "Text 3" is semantically distinct or unrelated to "Text 1" and "Text 2".

The grid lines provide a structured backdrop, suggesting a coordinate system without explicit numerical labels. The core message conveyed is that in an embedding model, texts with similar meanings are represented by points that are spatially close in this conceptual space, whereas texts with different meanings are mapped to points that are further apart. This spatial encoding of meaning is a fundamental principle underlying dense retrieval and other semantic text processing techniques.](images/a1743747801cfd67862b6acc13ca870ed0c5bf3e3fe750dd6fd08be14b197f1d.jpg)
Figure 8-4. The intuition of embeddings: each text is a point and texts with similar meaning are close to each other.

This is the property that is used to build search systems. In this scenario, when a user enters a search query, we embed the query, thus projecting it into the same space as our text archive. Then we simply find the nearest documents to the query in that space, and those would be the search results (Figure 8-5).

![## Image Analysis: e9b9dfc8d3c488049df4aa80d13a7cc70212ccb452d4c699e1e1773239021246.jpg

**Conceptual Understanding:**
The image conceptually represents a semantic embedding space, often a high-dimensional vector space, reduced to two dimensions for visualization. In this space, textual entities like search queries and documents are mapped as points. The main purpose of the image is to visually demonstrate the fundamental principle of dense retrieval: semantically related items, such as a search query and its relevant results, are positioned close to each other in this embedding space. The image conveys the idea that 'closeness' in this abstract space directly correlates with 'relevance' in search and information retrieval contexts.

**Content Interpretation:**
The image illustrates the core concept of dense retrieval within a semantic embedding space. The grid represents a multi-dimensional vector space where textual items (queries and documents/texts) are transformed into numerical representations (embeddings) and plotted as points. The "Query" point represents a user's search query, while "Text 1", "Text 2", and "Text 3" represent different documents or pieces of text. The spatial proximity of these points is critical: the closer a text point is to the query point, the more semantically relevant it is considered. In this specific diagram, "Text 2" is shown to be closest to the "Query", indicating it is the most relevant result. "Text 1" is moderately relevant, and "Text 3" is the least relevant, based on their respective distances from the "Query". This visual metaphor demonstrates how dense retrieval algorithms identify relevant results by computing the distance or similarity between the query's embedding and the embeddings of various documents.

**Key Insights:**
The main takeaway from this image is that in dense retrieval, semantic similarity between a query and a document is directly represented by their spatial proximity in a continuous embedding space. The closer the numerical representations (embeddings) of a query and a text are, the more semantically related and relevant they are considered. This is evidenced by the placement of "Text 2" closest to the "Query" point, indicating it is the most relevant, while "Text 3" is furthest, signifying lower relevance. The image teaches that retrieval in these systems is based on geometric distance calculations (e.g., Euclidean distance or cosine similarity) between these embedded points.

**Document Context:**
This image directly supports the document's section on "Dense Retrieval" and specifically illustrates the principle stated immediately after the image: "Dense retrieval relies on the property that search queries will be close to their relevant results." It provides a clear visual example of how this proximity is represented in an embedding space, which is fundamental to understanding the mechanism of dense retrieval systems. The diagram helps readers grasp the conceptual underpinnings of why certain results are considered more relevant to a given query based on their vector representations.

**Summary:**
The image displays a 2D grid, representing a semantic embedding space. Within this space, there are four circular points labeled with text. One point, colored light purple, is labeled "Query". Three other points, colored light grey, are labeled "Text 1", "Text 2", and "Text 3". The grid lines provide a visual reference for the relative positions of these points. "Text 2" is positioned closest to the "Query" point, suggesting a high degree of relevance or semantic similarity. "Text 1" is further away from the "Query", indicating less relevance than "Text 2". "Text 3" is positioned the furthest from the "Query" point, implying the least relevance among the three text points shown. The arrangement visually demonstrates the core principle of dense retrieval, where items (like queries and texts) that are semantically similar are embedded closer to each other in a continuous vector space. This spatial proximity is the primary mechanism for retrieving relevant results, as detailed by the document's context.](images/e9b9dfc8d3c488049df4aa80d13a7cc70212ccb452d4c699e1e1773239021246.jpg)
Figure 8-5. Dense retrieval relies on the property that search queries will be close to their relevant results.

Judging by the distances in Figure 8-5, “text $2 ^ { \mathfrak { v } }$ is the best result for this query, followed by “text 1.” Two questions could arise here, however:

• Should text 3 even be returned as a result? That’s a decision for you, the system designer. It’s sometimes desirable to have a max threshold of similarity score to filter out irrelevant results (in case the corpus has no relevant results for the query). • Are a query and its best result semantically similar? Not always. This is why language models need to be trained on question-answer pairs to become better at retrieval. This process is explained in more detail in Chapter 10.

Figure 8-6 shows how we chunk a document before proceeding to embed each chunk. Those embedding vectors are then stored in the vector database and are ready for retrieval.

![## Image Analysis: ec1783752746a642dc8329d3dd0b0b245e54d8018dd8bb0d1b5d7552cf6ef0ce.jpg

**Conceptual Understanding:**
This image conceptually represents the workflow for creating a vector database from a knowledge source. Its main purpose is to illustrate how raw, external information is transformed into a structured, queryable format suitable for dense retrieval applications. The key idea communicated is that a large body of text ('External knowledge') needs to be broken down into smaller pieces ('Chunk documents') and then converted into numerical representations ('Embedding model') before it can be stored and efficiently searched within a 'Vector database'.

**Content Interpretation:**
The image depicts a data pipeline for preparing information for a vector database, a common component in dense retrieval systems. It illustrates the conversion of raw textual data into a structured format of numerical vectors. The process involves three main conceptual stages: acquiring raw knowledge, segmenting it into manageable chunks, and then transforming these chunks into vector embeddings for storage and efficient retrieval. Each distinct visual element, such as the document icons, the 'Embedding model' box, and the 'Vector database' grid, explicitly represents a stage in this transformation process. The color coding (blue, red, green) consistently applied across 'Chunk documents' and the 'Vector database' visually reinforces the idea that specific chunks are individually processed and then represented as corresponding vectors.

**Key Insights:**
The main takeaway from this image is the multi-step process required to convert unstructured 'External knowledge' into a queryable 'Vector database'. Key insights include: 1. Raw knowledge must first be 'Chunk documents' to be effectively processed. This is evidenced by the explicit '2 Chunk documents' label and the visual segmentation. 2. An 'Embedding model' is essential for transforming these text 'Chunk documents' into numerical vector representations. This is shown by the 'Embedding model' box acting as an intermediary step. 3. The 'Vector database' stores these numerical embeddings, not the original text, enabling semantic search and retrieval based on vector similarity. This is supported by the '3 Vector database' label and its visual representation as a grid of vectors derived from the model's output. The process highlights the importance of both data segmentation and feature engineering (embedding) for dense retrieval.

**Document Context:**
This image is highly relevant to the 'Dense Retrieval' section of the document. It visually explains the prerequisite steps for building a dense retrieval system, as explicitly stated in the accompanying text: 'Figure 8-6. Convert some external knowledge base to a vector database. We can then query this vector database for information about the knowledge base.' The diagram provides a clear visual breakdown of how an 'External knowledge' base is transformed through 'Chunk documents' and an 'Embedding model' into a 'Vector database', which is the core component for performing dense retrieval queries. It lays the groundwork for understanding how information is structured and made searchable in such systems.

**Summary:**
The image illustrates a three-step process for converting 'External knowledge' into a 'Vector database', a fundamental step for dense retrieval systems. The process begins with '1 External knowledge', which is depicted as a large document containing multiple lines of text. This external knowledge is then processed into '2 Chunk documents'. Visually, this is shown by the large document being segmented into three smaller, distinct documents, each with its own color (blue, red, and green), indicating that the original knowledge has been broken down into separate, manageable chunks. These 'Chunk documents' are then fed into an 'Embedding model', which is represented by a light blue box with the text 'Embedding model' and an icon indicating transformation. Finally, the output from the 'Embedding model' is stored in '3 Vector database', depicted as a grid-like structure containing colored squares (blue, red, and green), where each square represents a vector corresponding to a document chunk. The overall flow shows how raw information is segmented, transformed into numerical representations, and then stored in a specialized database optimized for similarity search.](images/ec1783752746a642dc8329d3dd0b0b245e54d8018dd8bb0d1b5d7552cf6ef0ce.jpg)
Figure 8-6. Convert some external knowledge base to a vector database. We can then query this vector database for information about the knowledge base.

# Dense retrieval example

Let’s take a look at a dense retrieval example by using Cohere to search the Wikipedia page for the film Interstellar. In this example, we will do the following:

1. Get the text we want to make searchable and apply some light processing to   
chunk it into sentences.   
2. Embed the sentences.   
3. Build the search index.   
4. Search and see the results.

Get your Cohere API key by signing up at https://oreil.ly/GxrQ1. Paste it in the following code. You will not have to pay anything to run through this example.

Let’s import the libraries we’ll need:

import cohere   
import numpy as np   
import pandas as pd   
from tqdm import tqdm   
# Paste your API key here. Remember to not share publicly   
api_key $=$ ''   
# Create and retrieve a Cohere API key from os.cohere.ai   
co $=$ cohere.Client(api_key)

Getting the text archive and chunking it. Let’s use the first section of the Wikipedia arti‐ cle on the film Interstellar. We’ll get the text, then break it into sentences:

text $=$

Interstellar is a 2014 epic science fiction film co-written, directed, and pro duced by Christopher Nolan.   
It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.   
Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind. Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.   
Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.   
Cinematographer Hoyte van Hoytema shot it on $3 5 \ \mathrm { m m }$ movie film in the Panavision anamorphic format and IMAX $7 6 m m$ .   
Principal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.   
Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects. Interstellar premiered on October 26, 2014, in Los Angeles.   
In the United States, it was first released on film stock, expanding to venues using digital projectors.   
The film had a worldwide gross over $\$ 677$ million (and $\$ 773$ million with subse quent re-releases), making it the tenth-highest grossing film of 2014.   
It received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.   
It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.   
Interstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades"""

# Split into a list of sentences texts $=$ text.split('.')

# Clean up to remove empty spaces and new lines texts $=$ [t.strip(' \n') for t in texts]

Embedding the text chunks. Let’s now embed the texts. We’ll send them to the Cohere API, and get back a vector for each text:

# Get the embeddings   
response $=$ co.embed( texts $\mathbf { \equiv }$ texts, input_type $\iota =$ "search_document",   
).embeddings

embeds $=$ np.array(response) print(embeds.shape)

This outputs (15, 4096), which indicates that we have 15 vectors, each one of size 4,096.

Building the search index. Before we can search, we need to build a search index. An index stores the embeddings and is optimized to quickly retrieve the nearest neighbors even if we have a very large number of points:

import faiss dim $=$ embeds.shape[1] index $=$ faiss.IndexFlatL2(dim) print(index.is_trained) index.add(np.float32(embeds))

Search the index. We can now search the dataset using any query we want. We simply embed the query and present its embedding to the index, which will retrieve the most similar sentence from the Wikipedia article.

Let’s define our search function:

def search(query, number_of_results $\mathbf { \Psi } = \mathbf { \Psi }$ ):

# 1. Get the query's embedding query_embed $=$ co.embed(texts $, =$ [query], input_type $\Bumpeq$ "search_query",).embeddings[0]

# 2. Retrieve the nearest neighbors distances , similar_item_ids $=$ index.search(np.float32([query_embed]), num ber_of_results)

# 3. Format the results   
texts_np $=$ np.array(texts) # Convert texts list to numpy for easier indexing   
results $=$ pd.DataFrame(data $=$ {'texts': texts_np[similar_item_ids[0]], 'distance': distances[0]})

# 4. Print and return the results print(f"Query:'{query}'\nNearest neighbors:") return results

We are now ready to write a query and search the texts!

query $=$ "how precise was the science"   
results $=$ search(query)   
results

This produces the following output:

Query: 'how precise was the science' Nearest neighbors:

<table><tr><td>texts</td><td></td><td>distance</td></tr><tr><td>0</td><td>It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics</td><td>10757.379883</td></tr><tr><td>1.</td><td>Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer,actedasascientific consultant,and wrotea tie-in book,The Science of Interstellar</td><td>11566.131836</td></tr><tr><td>2.</td><td>Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects</td><td>11922.833008</td></tr></table>

The first result has the least distance, and so is the most similar to the query. Looking at it, it answers the question perfectly. Notice that this wouldn’t have been possible if we were only doing keyword search because the top result did not include the same keywords in the query.

We can actually verify that by defining a keyword search function to compare the two. We’ll use the BM25 algorithm, which is one of the leading lexical search meth‐ ods. See this notebook for the source of these code snippets:

from rank_bm25 import BM25Okapi   
from sklearn.feature_extraction import _stop_words   
import string   
def bm25_tokenizer(text): tokenized_doc $=$ [] for token in text.lower().split(): token $=$ token.strip(string.punctuation) if len(token) $> ~ \Theta$ and token not in _stop_words.ENGLISH_STOP_WORDS: tokenized_doc.append(token) return tokenized_doc   
tokenized_corpus $= \ [ ]$   
for passage in tqdm(texts): tokenized_corpus.append(bm25_tokenizer(passage))   
bm25 $=$ BM25Okapi(tokenized_corpus)   
def keyword_search(query, top_ $\mathbf { k } = \mathbf { \Gamma }$ , num_candidates $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ ): print("Input question:", query) ##### BM25 search (lexical search) ##### bm25_scores $=$ bm25.get_scores(bm25_tokenizer(query)) top_n $=$ np.argpartition(bm25_scores, -num_candidates)[-num_candidates:] bm25_hits $=$ [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n] bm25_hits $=$ sorted(bm25_hits, key=lambda x: x['score'], reverse=True)

print(f"Top-3 lexical search (BM25) hits") for hit in bm25_hits[0:top_k]: print("\t{:.3f}\t{}".format(hit['score'], texts[hit['cor pus_id']].replace("\n", " ")))

Now when we search for the same query, we get a different set of results from the dense retrieval search:

keyword_search(query $=$ "how precise was the science")

Results:

Input question: how precise was the science   
Top-3 lexical search (BM25) hits 1.789 Interstellar is a 2014 epic science fiction film co-written, direc  
ted, and produced by Christopher Nolan 1.373 Caltech theoretical physicist and 2017 Nobel laureate in Phys  
ics[4] Kip Thorne was an executive producer, acted as a scientific consultant,   
and wrote a tie-in book, The Science of Interstellar 0.000 It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain,   
Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine

Note that the first result does not really answer the question despite it sharing the word “science” with the query. In the next section, we’ll see how adding a reranker can improve this search system. But before that, let’s complete our overview of dense retrieval by looking at its caveats and go over some methods of breaking down texts into chunks.

# Caveats of dense retrieval

It’s useful to be aware of some of the drawbacks of dense retrieval and how to address them. What happens, for example, if the texts don’t contain the answer? We still get results and their distances. For example:

<table><tr><td colspan="2">Query:&#x27;What is the mass of the moon?&#x27; Nearest neighbors:</td></tr><tr><td>texts</td><td>distance</td></tr><tr><td>0 The flm had a worldwide gross over $677 milon (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014</td><td>1.298275</td></tr><tr><td>It ha also received prase frommany astronomersfor tsscientficaccuacyandportryal of theoretical1.324389</td><td></td></tr><tr><td>astrophysics 2 Cinematographer Hoyte van Hoytema shot it on 35 mm movie flm in the Panavision anamorphic format and IMAx 70 mm</td><td>1.328375</td></tr></table>

In cases like this, one possible heuristic is to set a threshold level—a maximum distance for relevance, for example. A lot of search systems present the user with the best info they can get and leave it up to the user to decide if it’s relevant or not.

Tracking the information of whether the user clicked on a result (and were satisfied by it) can improve future versions of the search system.

Another caveat of dense retrieval is when a user wants to find an exact match for a specific phrase. That’s a case that’s perfect for keyword matching. That’s one reason why hybrid search, which includes both semantic search and keyword search, is advised instead of relying solely on dense retrieval.

Dense retrieval systems also find it challenging to work properly in domains other than the ones that they were trained on. So, for example, if you train a retrieval model on internet and Wikipedia data, and then deploy it on legal texts (without having enough legal data as part of the training set), the model will not work as well in that legal domain.

The final thing we’d like to point out is that this is a case where each sentence contained a piece of information, and we showed queries that specifically ask for that information. What about questions whose answers span multiple sentences? This highlights one of the important design parameters of dense retrieval systems: what is the best way to chunk long texts? And why do we need to chunk them in the first place?

# Chunking long texts

One limitation of Transformer language models is that they are limited in context sizes, meaning we cannot feed them very long texts that go above the number of words or tokens that the model supports. So how do we embed long texts?

There are several possible ways, and two possible approaches shown in Figure 8-7 include indexing one vector per document and indexing multiple vectors per document.

![## Image Analysis: 9c580e3c42d1714831c1f140dafacdf807df74a751ad25fc5e25e3d572881fe3.jpg

**Conceptual Understanding:**
This image illustrates two fundamental approaches to creating vector embeddings from textual documents. Conceptually, it demonstrates how a complete document can either be treated as a single entity to generate one overarching vector representation, or it can be segmented into smaller parts (chunks), with each part receiving its own distinct vector embedding. The main purpose is to highlight the benefits and differences between a holistic document-level embedding versus a more granular chunk-level embedding strategy, particularly emphasizing the utility of chunking for managing and processing longer texts. The image conveys the idea that representing parts of a document separately can offer more detailed and context-specific numerical representations.

**Content Interpretation:**
The image illustrates two distinct processes for converting a textual document into numerical vector embeddings. The left side, titled "One vector per document," shows the complete Wikipedia article for "Interstellar (film)" as a singular entity. This entire body of text, which details the film's production, plot, cast, and reception, is conceptually processed to yield a single "Document vector," represented by three small orange rectangles. This indicates that the entire semantic content of the document is compressed into one comprehensive numerical representation. The right side, titled "Chunk document into multiple chunks," depicts the same Wikipedia article but with a different processing strategy. Here, the document is implicitly segmented into smaller, manageable portions. Each of these segments, or chunks, then undergoes its own embedding process, resulting in separate "Chunk 1 vector," "Chunk 2 vector," and "Chunk 3 vector" representations. These are also represented by sets of three small orange rectangles. The significance is that by breaking a long document into chunks, a system can create more precise and contextually relevant embeddings for smaller portions of the text, which is particularly beneficial for managing information in lengthy documents. The detailed text content extracted from the Wikipedia article serves as the concrete example input for both embedding processes.

**Key Insights:**
1.  **Documents can be represented as numerical vectors (embeddings):** Both sides of the image explicitly show a document (or its chunks) leading to 

**Document Context:**
This image directly supports the document's section on "Chunking long texts" by providing a clear visual comparison of two methods for handling text in the context of vector embeddings. The image illustrates the core concept that while an entire document can be represented by a single vector, a more effective strategy for longer documents, as indicated by the surrounding text, is to split them into smaller, independently embedded chunks. This visual explanation helps readers understand the practical application and benefits of text chunking in natural language processing workflows, particularly for enhancing document comprehension and information retrieval.

**Summary:**
This image visually compares two approaches to generating vector embeddings from textual documents. On the left, the entire "Interstellar (film)" Wikipedia article is treated as a single unit, resulting in one "Document vector." This means the entire document's content is condensed into a single numerical representation. On the right, the same "Interstellar (film)" article is conceptually divided into multiple smaller segments or "chunks." Each of these chunks, such as "Chunk 1," "Chunk 2," and "Chunk 3," then gets its own individual vector embedding, labeled as "Chunk 1 vector," "Chunk 2 vector," and "Chunk 3 vector" respectively. This method provides a more granular representation of the document's content. The image highlights that for longer documents, splitting them into chunks for individual embeddings is generally a more effective strategy for capturing their nuances and facilitating tasks like information retrieval or semantic search, as each part of the document retains its distinct numerical representation.](images/9c580e3c42d1714831c1f140dafacdf807df74a751ad25fc5e25e3d572881fe3.jpg)
Figure 8-7. It’s possible to create one vector representing an entire document, but it’s bet‐ ter for longer documents to be split into smaller chunks that get their own embeddings.

One vector per document. In this approach, we use a single vector to represent the whole document. The possibilities here include:

• Embedding only a representative part of the document and ignoring the rest of the text. This may mean embedding only the title, or only the beginning of the document. This is useful to get quickly started with building a demo but it leaves a lot of information unindexed and therefore unsearchable. As an approach, it may work better for documents where the beginning captures the main points of a document (think: Wikipedia article). But it’s really not the best approach for a real system because a lot of information would be left out of the index and would be unsearchable. • Embedding the document in chunks, embedding those chunks, and then aggre‐ gating those chunks into a single vector. The usual method of aggregation here is to average those vectors. A downside of this approach is that it results in a highly compressed vector that loses a lot of the information in the document.

This approach can satisfy some information needs, but not others. A lot of the time, a search is for a specific piece of information contained in an article, which is better captured if the concept had its own vector.

Multiple vectors per document. In this approach, we chunk the document into smaller pieces, and embed those chunks. Our search index then becomes that of chunk embeddings, not entire document embeddings. Figure 8-8 shows a number of possi‐ ble text chunking approaches.

![## Image Analysis: 4c471e00148ace74824a9301374163ca19b0ce003ab649e76880da6031277cb7.jpg

**Conceptual Understanding:**
This image conceptually represents the process of 'text chunking' or 'text splitting,' which is the method of dividing a larger block of text into smaller, more manageable segments. Its main purpose is to visually illustrate and compare different strategies for performing this task, specifically focusing on whether the splitting is based on characters or tokens, and whether the resulting chunks include overlapping content. The image clearly communicates the idea that the chosen chunking method significantly impacts the structure and contextual integrity of the segmented text, with overlapping chunks explicitly shown as a technique to maintain continuity between parts.

**Content Interpretation:**
The image displays a conceptual diagram illustrating different text chunking strategies. It shows three distinct methods: character-based splitting, token-based splitting without overlap, and token-based splitting with a single overlapping token. Each method processes the same input sentence, "Llama 2 was trained on 40% more data than Llama 1," and demonstrates the resultant chunks. The purpose is to visually represent how granularity (characters vs. tokens) and the inclusion of overlap (or lack thereof) fundamentally change how a long text is segmented. The explicit labeling of "No overlap" and "Overlapping tokens" directly highlights the key difference between these methods, particularly emphasizing the technique of preserving context through shared tokens.

**Key Insights:**
The main takeaway from this image is the critical difference between various text chunking methods and their implications, especially regarding the preservation of context. The image teaches that chunking can be performed at the character level or the token level. More importantly, it demonstrates that incorporating 'overlapping tokens' is a deliberate strategy to ensure continuity of information between adjacent chunks, preventing a loss of context that might occur with strictly non-overlapping splits. The evidence for this is seen in the explicit "No overlap" label for the character split, contrasting with the "Overlapping tokens" labels and the visually highlighted (purple) shared tokens ('on', 'than') in the 'Every 5 tokens with 1 overlapping token' example. This highlights that while non-overlapping chunks are simpler, overlapping chunks are designed for applications where sequential context is crucial, directly supporting the idea that "Overlapping chunks can be important to prevent the absence of context."

**Document Context:**
This image is directly relevant to the section "Chunking long texts" in the document. It serves as a visual explanation of how texts are broken down into smaller, manageable units, which is a foundational concept in natural language processing and text analysis. The image specifically supports the subsequent text, "Figure 8-8. Several chunking methods and their effects on the input text. Overlapping chunks can be important to prevent the absence of context," by providing concrete examples of these methods and explicitly demonstrating the concept of overlapping chunks and their role in maintaining context. It transitions from a general input to specific, method-driven outputs, detailing the 'effects' mentioned in the text.

**Summary:**
The image illustrates three different methods for "chunking" an input text, showing how varying parameters and overlap strategies affect the resulting text segments. The initial input text is: "Llama 2 was trained on 40% more data than Llama 1." This text is then processed by three distinct chunking approaches.

First, a "Character split" method is applied, which chunks "Every 15 characters." This results in four discrete, non-overlapping chunks: "Llama 2 was tra", "ined on 40% mor", "e data than Lla", and "ma 1". An arrow connecting these chunks explicitly states "No overlap," indicating that there are no shared characters between adjacent chunks.

Second, a "Token split" method is shown, which chunks "Every 5 tokens." This approach divides the input into three non-overlapping token-based chunks: "Llama 2 was trained on", "40% more data than Llama", and "1". While not explicitly labeled with "No overlap" like the character split, the segmentation implies a non-overlapping division of tokens.

Third, another "Token split" method is demonstrated, this time specifying "Every 5 tokens with 1 overlapping token." This method also produces three chunks, but with a deliberate overlap to preserve context. The chunks are: "Llama 2 was trained on", "on 40% more data than", and "than Llama 1". Arrows between these chunks are labeled "Overlapping tokens." Specifically, the token "on" (highlighted in purple) is shared between the first and second chunks, and the token "than" (highlighted in purple) is shared between the second and third chunks. This visual representation clearly shows how a single token provides a bridge between consecutive chunks, maintaining a continuous flow of context.](images/4c471e00148ace74824a9301374163ca19b0ce003ab649e76880da6031277cb7.jpg)
Figure 8-8. Several chunking methods and their effects on the input text. Overlapping chunks can be important to prevent the absence of context.

The chunking approach is better because it has full coverage of the text and because the vectors tend to capture individual concepts inside the text. This leads to a more expressive search index. Figure 8-9 shows a number of possible approaches.

![## Image Analysis: 0d8326cbfac2f083421b17fe134d5d35053e14c9b2a6cf2b18bb04382e07f7f9.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates different strategies for **text chunking**, specifically in the context of preparing long texts for embedding (likely for applications like natural language processing, semantic search, or retrieval-augmented generation). The main purpose is to demonstrate how the same source document can be divided into distinct, manageable units (chunks) based on varying granularities and methodologies. It visually contrasts three approaches: "sentence-level chunking," "paragraph-level chunking," and "overlapping window of sentences" chunking, showing how each approach affects the size, composition, and contextual integrity of the resulting text segments that would then be converted into "vectors" (numerical representations for machine learning models).

**Content Interpretation:**
The image demonstrates three common text chunking strategies using a snippet from a Wikipedia article about "Interstellar (film)" as a consistent example.

1.  **"Each sentence is a chunk"**: This method breaks down the text into individual sentences, creating many small, fine-grained chunks. The individual sentences within the document snippet are highlighted in alternating colors (orange, green, red), and are then represented as "Chunk 1 vector" through "Chunk 15 vector," each shown as three small colored blocks.

2.  **"Each paragraph is a chunk"**: This method treats each complete paragraph as a single chunk, resulting in fewer, larger chunks that preserve more contextual information. All paragraphs in the example are highlighted uniformly in orange, and are represented as "Chunk 1 vector," "Chunk 2 vector," and "Chunk 3 vector," each shown as a single larger orange block.

3.  **"Overlapping window of sentences"**: This method uses a sliding window approach where each chunk consists of a predefined number of sentences, and successive chunks overlap by some sentences. This ensures context is maintained across chunk boundaries. The text highlights clearly show overlapping sections in different colors (orange, green, red). This approach results in numerous chunks, represented as "Chunk 1 vector" through "Chunk 15 vector," each shown as three small colored blocks, similar to the sentence-level chunks but with added contextual redundancy.

The consistent presence of the "Interstellar (film)" Wikipedia content and movie poster details across all three methods serves as a concrete, relatable example. The "Chunk X vector" labels illustrate the conceptual outcome where each text chunk is transformed into a numerical vector for computational processing.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Text chunking is a foundational step in text processing for embeddings:** The image's focus on illustrating different chunking methods for creating "vectors" emphasizes its crucial role before converting text into numerical representations for machine learning models. The repeated "Chunk X vector" labels explicitly show this purpose.

2.  **The choice of chunking strategy impacts granularity and context:**
    *   **Sentence-level chunking** provides high granularity (many small chunks, e.g., "Chunk 1 vector" through "Chunk 15 vector" each corresponding to a single sentence), potentially offering fragmented context.
    *   **Paragraph-level chunking** provides coarser granularity (fewer, larger chunks, e.g., "Chunk 1 vector," "Chunk 2 vector," "Chunk 3 vector" each representing an entire paragraph), preserving more extensive context.
    *   **Overlapping window chunking** balances granularity and context by creating chunks of multiple sentences with shared content between successive chunks, ensuring contextual continuity across boundaries. This also results in numerous chunks (up to "Chunk 15 vector" shown), indicating controlled size with built-in redundancy.

3.  **Different chunking strategies are suitable for different applications:** The comparison implicitly suggests that the optimal chunking method is application-dependent. Fine-grained tasks might use sentence chunking, broader contextual tasks might use paragraph chunking, and robust retrieval systems benefit from overlapping windows to maintain context.

4.  **Chunks are ultimately transformed into vectors:** The consistent label "Chunk X vector" across all strategies reinforces that these text segments are precursors to numerical representations used in modern AI/ML applications, enabling machines to process and understand textual content.

**Document Context:**
This image directly supports the document section titled "Chunking long texts" and the accompanying caption "Figure 8-9. A number of possible options for chunking a document for embedding." It serves as a visual explanation of *how* long texts can be segmented into smaller, manageable pieces, which is a critical preprocessing step for various natural language processing tasks. Specifically, it highlights different chunking strategies for preparing text for vector embeddings, essential for applications like semantic search or retrieval-augmented generation (RAG) with large language models. The figure visually demonstrates the practical implications of different chunking choices on the resulting data structure, making an abstract concept tangible within the broader discussion of text processing.

**Summary:**
This image illustrates three distinct methods for segmenting (or "chunking") a long text document, using a snippet from a Wikipedia article about the film "Interstellar" as an example. The goal of chunking is to break down a document into smaller, more manageable units that can then be converted into numerical "vectors" for computational processing in systems that rely on natural language understanding.

1.  **Each sentence is a chunk:** In the first method, the document is divided so that every single sentence forms its own individual chunk. The example shows the Wikipedia text where each sentence is highlighted in a different color (orange, green, red) to denote it as a separate chunk. Below this, there's a representation of "Chunk 1 vector," "Chunk 2 vector," and "Chunk 3 vector," continuing to "Chunk 15 vector," each depicted as three small colored blocks. This indicates that each sentence becomes a distinct unit for vectorization, resulting in many small, highly granular chunks.

2.  **Each paragraph is a chunk:** The second method treats each entire paragraph as a single chunk. In this section, the Wikipedia article's paragraphs are highlighted as whole units (all in orange). Correspondingly, "Chunk 1 vector," "Chunk 2 vector," and "Chunk 3 vector" are shown, each represented by a single, larger orange block. This approach results in fewer, larger chunks, where each chunk contains a broader context than a single sentence.

3.  **Overlapping window of sentences:** The third method employs a "sliding window" approach where chunks are created from a fixed number of sentences, but these chunks overlap with each other. For instance, a chunk might consist of sentences 1, 2, and 3, and the very next chunk would be sentences 2, 3, and 4. This ensures that context is not lost at the boundaries between chunks. The highlighted text clearly shows these overlaps, with different colors (orange, green, red) indicating the successive, partially shared windows of sentences. Similar to sentence-level chunking, this method also results in numerous chunks, with "Chunk 1 vector" through "Chunk 15 vector" depicted as three small colored blocks, signifying that while they contain multiple sentences, their overall 'processing size' might be similar to sentence chunks but with added contextual continuity.

Across all three methods, the image consistently displays a sidebar from the Wikipedia page detailing the movie "Interstellar," including "Theatrical release poster," "Directed by Christopher Nolan," "Written by Jonathan Nolan Christopher Nolan," "Produced by Emma Thomas Christopher Nolan Lynda Obst," and "Starring Matthew McConaughey Anne Hathaway Jessica Chastain." This consistent context grounds the abstract concept of chunking in a concrete example. The overall message is that the choice of how to chunk a document—whether by sentences, paragraphs, or overlapping windows—is a deliberate decision that affects the granularity and contextual integrity of the data when preparing it for further processing, such as creating vector embeddings.](images/0d8326cbfac2f083421b17fe134d5d35053e14c9b2a6cf2b18bb04382e07f7f9.jpg)
Figure 8-9. A number of possible options for chunking a document for embedding.

The best way of chunking a long text will depend on the types of texts and queries your system anticipates. Approaches include:

• Each sentence is a chunk. The issue here is this could be too granular and the vectors don’t capture enough of the context.   
• Each paragraph is a chunk. This is great if the text is made up of short para‐ graphs. Otherwise, it may be that every 3–8 sentences is a chunk.   
• Some chunks derive a lot of their meaning from the text around them. So we can incorporate some context via: — Adding the title of the document to the chunk. — Adding some of the text before and after them to the chunk. This way, the chunks can overlap so they include some surrounding text that also appears in adjacent chunks. This is what we can see in Figure 8-10.

Expect more chunking strategies to arise as the field develops—some of which may even use LLMs to dynamically split a text into meaningful chunks.

![## Image Analysis: f4dce5e596ef68c9b5b2d62ace7758036a2d724589a4ccd4d29bd5be38599c0d.jpg

**Conceptual Understanding:**
The image conceptually illustrates a preprocessing technique for handling long texts in the context of Large Language Models (LLMs). Its main purpose is to demonstrate a method of text 'chunking' where a large document is broken into smaller, *overlapping* segments. This overlap is key to preserving context across the individual chunks. Subsequently, these chunks are fed into an 'Embedding LLM' which converts them into numerical 'vectors,' each representing the semantic meaning of its respective text chunk. The key ideas communicated are text segmentation, context preservation through overlap, and the generation of text embeddings for machine processing.

**Content Interpretation:**
The image illustrates the processes of text chunking and text embedding by a Large Language Model (LLM). It demonstrates how a long text document, represented by a Wikipedia article about "Interstellar (film)," is divided into smaller, manageable, and crucially, *overlapping* segments (Chunk 1, Chunk 2, Chunk 3). This overlapping strategy is significant for retaining contextual information across chunk boundaries. Each of these text chunks is then processed by an "Embedding LLM," which transforms the textual content into a numerical vector representation (Chunk 1 vector, Chunk 2 vector, Chunk 3 vector). The significance lies in showing a practical method for preparing extensive textual data for LLMs, ensuring that even when the text is broken down, its overall context is preserved, which is vital for tasks requiring deep semantic understanding.

**Key Insights:**
The main takeaways from this image are: 1. Long texts often need to be broken down (chunked) for processing by LLMs due to context window limitations. 2. Employing *overlapping* chunks is a critical strategy to prevent loss of context at the boundaries of text segments, ensuring that an LLM has sufficient surrounding information. 3. LLMs can convert these textual chunks into numerical 'embeddings' or 'vectors' that capture the semantic meaning of the text, making it suitable for further computational analysis. This is evidenced by the explicit labels 'Chunk 1', 'Chunk 2', 'Chunk 3' with their visual overlap within the Wikipedia article text, the 'Embedding LLM' processing step, and the resulting 'Chunk 1 vector', 'Chunk 2 vector', 'Chunk 3 vector' outputs.

**Document Context:**
This image directly fits within the document's broader narrative, as indicated by the section title "Chunking long texts" and the accompanying text "Figure 8-10. Chunking the text into overlapping segments is one strategy to retain more of the context around different segments." It serves as a visual, detailed explanation of this specific strategy, demonstrating the 'how' behind retaining context when dealing with long texts for LLM processing.

**Summary:**
This diagram illustrates a common and effective strategy for processing long text documents with Large Language Models (LLMs), focusing specifically on "chunking" the text into overlapping segments to retain context.

The process begins with a long text document, here exemplified by a Wikipedia article titled "**Interstellar (film)**." This article contains various details about the film, including its plot, production, cast (Christopher Nolan, Jonathan Nolan, Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Michael Caine), and reception. The Wikipedia page also includes standard UI elements like "**89 languages**," "**Read**," "**Edit**," "**View History**," "**Tools**," and the subtitle "**From Wikipedia, the free encyclopedia**," along with an "**Interstellar Visual release poster**" and a detailed information box listing the "**Directed by** Christopher Nolan," "**Written by** Jonathan Nolan, Christopher Nolan," "**Produced by** Emma Thomas, Christopher Nolan, Lynda Obst," and "**Starring** Matthew McConaughey, Anne Hathaway, Jessica Chastain."

The first step in preparing this long text is **chunking**. The image shows the article being divided into three distinct sections, labeled "**Chunk 1**," "**Chunk 2**," and "**Chunk 3**." Crucially, these chunks are *overlapping*:
*   **Chunk 1** (highlighted orange) covers the initial descriptive text of the film.
*   **Chunk 2** (highlighted green) starts before Chunk 1 ends, ensuring that some of the context from the end of Chunk 1 is carried over. This chunk describes the screenplay's origins, scientific consultation, and filming details.
*   **Chunk 3** (highlighted red) similarly overlaps with the end of Chunk 2, providing continuous context. This final chunk covers the film's premiere, critical reception, box office success, scientific commendation, and awards.

Each of these text chunks (Chunk 1, Chunk 2, and Chunk 3) is then fed as input into a central processing unit labeled "**Embedding LLM**" (Large Language Model). This LLM's role is to understand the semantic meaning of the text within each chunk.

Finally, the **Embedding LLM** transforms each text chunk into a corresponding numerical representation known as a "vector." The outputs are explicitly labeled "**Chunk 1 vector**," "**Chunk 2 vector**," and "**Chunk 3 vector**." These vectors are dense numerical arrays that encapsulate the meaning of their respective text chunks, making them suitable for various computational tasks such as semantic search, clustering, or as input to other machine learning models.

In summary, this diagram effectively demonstrates how breaking a long document into overlapping segments helps in maintaining a rich contextual understanding for an LLM, enabling it to generate comprehensive and semantically meaningful vector embeddings for each part of the original text.](images/f4dce5e596ef68c9b5b2d62ace7758036a2d724589a4ccd4d29bd5be38599c0d.jpg)
Figure 8-10. Chunking the text into overlapping segments is one strategy to retain more of the context around different segments.

# Nearest neighbor search versus vector databases

Once the query is embedded, we need to find the nearest vectors to it from our text archive as we can see in Figure 8-11. The most straightforward way to find the nearest neighbors is to calculate the distances between the query and the archive. That can easily be done with NumPy and is a reasonable approach if you have thousands or tens of thousands of vectors in your archive.

![## Image Analysis: 4cdc4f539bdad8a45193cfd33e00cb78b4edcc6b403ffc6da1ba77d48a73e125.jpg

**Conceptual Understanding:**
This image conceptually represents the process of nearest neighbor search using a vector database, often applied in information retrieval or recommendation systems. Its main purpose is to illustrate how text data (documents and queries) are transformed into numerical vector embeddings, stored in a specialized database, and then compared to find the most similar items. The key idea communicated is that similarity between textual items can be quantified and efficiently searched by comparing their vector representations.

**Content Interpretation:**
The image depicts a system for finding similar documents based on a query. The processes shown include: Embedding, where text ("Documents" and "Query") is converted into numerical vector representations by an "Embedding model"; Storage, where these "embedded vectors" of "Documents" are held in a "Vector database"; Comparison, denoted by the "Compare" label and double-headed arrow within the "Vector database," indicating the measurement of similarity between the query embedding and stored document embeddings; and Retrieval, represented by "Retrieve most similar," which is the outcome of identifying the most relevant document(s). The relationships include a many-to-one flow from multiple "Documents" to one "Embedding model" and a one-to-one flow from a "Query" to another "Embedding model," both leading to a comparison within the "Vector database." The significance lies in illustrating a fundamental architecture for semantic search, where text meaning is captured in vectors for nuanced and context-aware retrieval. The "Embedding model"'s quality is crucial, and the "Vector database" provides the necessary infrastructure for efficient vector comparison. The "Retrieve most similar" step is the ultimate goal, delivering relevant results.

**Key Insights:**
Main Takeaways/Lessons:
- Vector Embeddings are Central: Both "Documents" and "Query" must first be transformed into numerical "embedded vectors" by an "Embedding model" before any similarity search can occur. This is evidenced by the explicit "Embedding model" boxes on both input paths.
- Specialized Databases for Similarity: "Vector database" implies that efficient similarity comparison of these embeddings requires specialized storage and indexing mechanisms. The "Compare" operation within the database highlights this core functionality.
- Similarity Search Process: The entire diagram outlines the end-to-end process: encode documents, encode query, compare query embedding with document embeddings, and "Retrieve most similar" documents. This provides a clear procedural understanding.

Conclusions/Insights:
- Semantic search relies on converting natural language into a numerical space where proximity indicates semantic similarity.
- Two distinct "Embedding model" instances (or a shared model used twice) are involved: one for indexing documents and one for processing real-time queries.
- The "Vector database" acts as the central hub for storing and performing the core similarity computations.

**Document Context:**
This image directly supports the document's section "Nearest neighbor search versus vector databases" by visually explaining the mechanics of how a vector database facilitates nearest neighbor search. It demonstrates the conceptual flow from raw text to meaningful retrieval.

**Summary:**
The diagram illustrates the process of performing a nearest neighbor search using a vector database. It begins with "Documents," representing a collection of text data, which are typically color-coded for distinction (blue, pink, green in this example). These documents are fed into an "Embedding model," which is responsible for converting the textual content into numerical "embedded vectors." These vectors are then stored within a "Vector database," shown with corresponding color-coded rows of smaller squares representing the embeddings of the blue, pink, and green documents. Concurrently, a user's "Query," also a piece of text, goes through its own "Embedding model" to be transformed into an embedded vector. This query embedding is then sent to the "Vector database." Inside the "Vector database," a crucial operation labeled "Compare" takes place. Here, the query's embedding is compared against all the stored document embeddings to determine their similarity. Finally, based on this comparison, the system performs the action to "Retrieve most similar" documents. The output is represented by a row of red colored embedded vectors, which visually suggests that the most similar result corresponds to the red document's embedding from the initial collection, indicating the identified relevant document. This entire flow demonstrates how text content is converted into a numerical format, stored efficiently, and then searched based on semantic similarity to find relevant information.](images/4cdc4f539bdad8a45193cfd33e00cb78b4edcc6b403ffc6da1ba77d48a73e125.jpg)
Figure 8-11. As we saw in Chapter 3, we can compare embeddings to quickly find the most similar documents to a query.

As you scale beyond to the millions of vectors, an optimized approach for retrieval is to rely on approximate nearest neighbor search libraries like Annoy or FAISS. These allow you to retrieve results from massive indexes in milliseconds and some of them can improve their performance by utilizing GPUs and scaling to clusters of machines to serve very large indices.

Another class of vector retrieval systems are vector databases like Weaviate or Pine‐ cone. A vector database allows you to add or delete vectors without having to rebuild the index. They also provide ways to filter your search or customize it in ways beyond merely vector distances.

# Fine-tuning embedding models for dense retrieval

Just as we discussed in Chapter 4 on text classification, we can improve the perfor‐ mance of an LLM on a task using fine-tuning. As in that case, retrieval needs to optimize text embeddings and not simply token embeddings. The process for this fine-tuning is to get training data composed of queries and relevant results.

Let’s look at one example from our dataset, the sentence “Interstellar premiered on October 26, 2014, in Los Angeles.” Two possible queries where this is a relevant result are:

• Relevant query 1: “Interstellar release date” • Relevant query 2: “When did Interstellar premier”

The fine-tuning process aims to make the embeddings of these queries close to the embedding of the resulting sentence. It also needs to see negative examples of queries that are not relevant to the sentence, for example:

• Irrelevant query: “Interstellar cast”

With these examples, we now have three pairs—two positive pairs and one negative pair. Let’s assume, as we can see in Figure 8-12, that before fine-tuning, all three queries have the same distance from the result document. That’s not far-fetched because they all talk about Interstellar.

![## Image Analysis: bca6220c405bf1117b82511b9ff7914f9eba86479e46f12d4c1ed41ad12527ba.jpg

**Conceptual Understanding:**
This image conceptually represents the idea of query relevance with respect to a specific document or factual statement in an information retrieval context. The main purpose is to illustrate, with concrete examples, what constitutes a 'relevant query' and an 'irrelevant query' for a given piece of information, particularly before any fine-tuning of an embedding model. It highlights that different queries can seek information related to a document, but only those directly addressing the document's content are considered relevant for retrieval purposes. The image communicates the key idea that effective information retrieval relies on accurately discerning the semantic alignment between a query and a document.

**Content Interpretation:**
The image illustrates the concept of query relevance in the context of dense retrieval. It shows a central piece of information, presented as a document fact, and then categorizes different potential user queries based on their relevance to this fact. The grey box, containing the text 'Interstellar premiered on October 26, 2014, in Los Angeles.', represents a document or a specific factual embedding. The two green boxes, labeled 'Relevant query: Interstellar release date' and 'Relevant query: When did Interstellar premier', represent queries whose semantic meaning aligns closely with the factual content in the document. The red box, labeled 'Irrelevant query: Interstellar cast', represents a query whose semantic meaning does not align with the specific factual content about the premiere date and location of the movie 'Interstellar'. The color coding (green for relevant, red for irrelevant) clearly distinguishes the types of queries. The grid background provides a visual space, possibly hinting at a vector space where embeddings might be located, though specific distances are not numerically shown.

**Key Insights:**
The main takeaway from this image is the clear distinction between relevant and irrelevant queries in relation to a specific piece of factual information. It demonstrates that queries can be semantically similar to a document in different ways. For example, 'Interstellar release date' and 'When did Interstellar premier' are both relevant to the fact 'Interstellar premiered on October 26, 2014, in Los Angeles.' because they both seek information about the premiere date. Conversely, 'Interstellar cast' is explicitly identified as irrelevant to the specific premiere information. This distinction is crucial for understanding how embedding models need to be trained to differentiate query relevance. The image implies that a well-tuned model should assign higher similarity scores to relevant queries (green boxes) and lower scores to irrelevant ones (red box) when compared to the document (grey box). The explicit labeling of 'Relevant query' and 'Irrelevant query' with specific examples provides the textual evidence for these insights.

**Document Context:**
This image directly supports the document section on 'Fine-tuning embedding models for dense retrieval' and specifically clarifies the accompanying text, 'Figure 8-12. Before fine-tuning, the embeddings of both relevant and irrelevant queries may be close to a particular document.' The diagram serves as a pre-fine-tuning conceptualization. It sets the stage by showing that, initially, a retrieval system might not effectively distinguish between relevant and irrelevant queries for a given document. The image visually presents a 'document' (the grey box) and examples of queries that should ideally be considered 'relevant' (green boxes) and 'irrelevant' (red box) to that document. The implication, given the surrounding text, is that before fine-tuning, the embedding space might position all these queries 'close' to the document, making it hard to retrieve only the truly relevant ones. The image provides concrete examples of what 'relevant' and 'irrelevant' mean in this context.

**Summary:**
The image is a conceptual diagram presented on a grid background, illustrating the relationship between a specific factual statement (a document's content) and various user queries, categorizing these queries as either 'relevant' or 'irrelevant' to the document's content. On the left side, a grey rectangular box contains the document's factual statement: 'Interstellar premiered on October 26, 2014, in Los Angeles.' On the right side, there are three other rectangular boxes, aligned vertically. The top two are green and represent 'relevant queries.' The first green box states: 'Relevant query: Interstellar release date'. The second green box states: 'Relevant query: When did Interstellar premier'. Below these, a red rectangular box represents an 'irrelevant query,' stating: 'Irrelevant query: Interstellar cast'. The visual arrangement implies that the queries are being evaluated against the information contained in the grey document box. The purpose is to demonstrate how certain queries closely align with the document's information (relevant) while others do not (irrelevant), providing a visual representation of query relevance in dense retrieval.](images/bca6220c405bf1117b82511b9ff7914f9eba86479e46f12d4c1ed41ad12527ba.jpg)
Figure 8-12. Before fine-tuning, the embeddings of both relevant and irrelevant queries may be close to a particular document.

The fine-tuning step works to make the relevant queries closer to the document and at the same time make irrelevant queries farther from the document. We can see this effect in Figure 8-13.

![## Image Analysis: 50d79a1e83bc31dac31a89134d5eb0f49971e5132289ac0d74efcc8302dc0030.jpg

**Conceptual Understanding:**
The image conceptually illustrates the principle of query relevance in information retrieval. Its main purpose is to show, through specific examples related to the movie 'Interstellar,' how certain queries are considered pertinent to a given factual statement while others are not. The key idea communicated is the definition of 'relevance' by pairing a document (the factual statement) with both queries that can be answered by it (relevant queries) and queries that cannot (irrelevant queries). This foundational concept is essential for tasks like fine-tuning search models.

**Content Interpretation:**
This image illustrates the concept of query relevance within a data retrieval context. It presents a single factual statement and categorizes example queries into "relevant" and "irrelevant" based on whether the query's intent can be answered by the factual statement. The grey box, "Interstellar premiered on October 26, 2014, in Los Angeles," represents a document or a piece of information. The green boxes, "Relevant query: Interstellar release date" and "Relevant query: When did Interstellar premier," are examples of queries whose information need is directly addressed by the factual statement. The red box, "Irrelevant query: Interstellar cast," represents a query whose information need is not addressed by the factual statement.

**Key Insights:**
The main takeaway from this image is the clear visual distinction between what constitutes a relevant versus an irrelevant query for a given piece of information. The image teaches that relevance is determined by the direct alignment between the query's information need and the content available in the document. The factual statement, "Interstellar premiered on October 26, 2014, in Los Angeles," serves as the basis. Queries like "Interstellar release date" and "When did Interstellar premier" are explicitly labeled as "Relevant query" because they directly seek information (the premiere date) provided in the factual statement. Conversely, the query "Interstellar cast" is labeled as "Irrelevant query" because the factual statement does not contain information about the cast, demonstrating a lack of direct alignment. This distinction is crucial for understanding how machine learning models are trained to identify and retrieve relevant information.

**Document Context:**
This image directly supports the document's section on "Fine-tuning embedding models for dense retrieval." The text immediately following the image, "After the fine-tuning process, the text embedding model becomes better at this search task by incorporating how we define relevance on our dataset using the examples we provided of relevant and irrelevant documents," clarifies that this image provides a concrete visual example of how relevance is defined for a dataset. It demonstrates the positive examples (relevant queries) and negative examples (irrelevant queries) that an embedding model would learn from during fine-tuning to improve its search capabilities for dense retrieval.

**Summary:**
The image displays a conceptual grid illustrating the relevance of search queries to a specific piece of factual information. A central grey box contains the factual statement: "Interstellar premiered on October 26, 2014, in Los Angeles." To the right of this central fact, two green boxes represent "Relevant queries." The top green box states: "Relevant query: Interstellar release date," and the bottom green box states: "Relevant query: When did Interstellar premier." Below the relevant queries, a red box indicates an "Irrelevant query," with the text: "Irrelevant query: Interstellar cast." The overall arrangement shows how different queries are classified based on their connection to the factual content, serving as an example for defining relevance in a dataset.](images/50d79a1e83bc31dac31a89134d5eb0f49971e5132289ac0d74efcc8302dc0030.jpg)
Figure 8-13. After the fine-tuning process, the text embedding model becomes better at this search task by incorporating how we define relevance on our dataset using the examples we provided of relevant and irrelevant documents.

# Reranking

A lot of organizations have already built search systems. For those organizations, an easier way to incorporate language models is as a final step inside their search pipeline. This step is tasked with changing the order of the search results based on relevance to the search query. This one step can vastly improve search results and it’s in fact what Microsoft Bing added to achieve the improvements to search results using BERT-like models. Figure 8-14 shows the structure of a rerank search system serving as the second stage in a two-stage search system.

![## Image Analysis: 8d5d6228d61534d07a267c859c6215862294a9eb29f287f2ea841fa737bceb64.jpg

**Conceptual Understanding:**
This image conceptually represents a two-stage search pipeline designed to retrieve and then refine the relevance of documents from a large text archive based on a user query. Its main purpose is to illustrate the process by which initial search results are subsequently reordered by a reranking component to present a more relevant set of 'Final results' to the user. The diagram highlights the sequential nature of the search and reranking operations and the inputs required at each stage to achieve an ordered output based on relevance criteria.

**Content Interpretation:**
The image details a two-stage information retrieval system, specifically a search pipeline that incorporates a reranking mechanism. The first stage focuses on an initial broad search, while the second stage refines the relevance of the results. This represents a common architecture in advanced search systems, where an initial retrieval step is followed by a more sophisticated ranking or re-ranking step to improve the quality of the final output. The dual input (query and initial results) to the 'Rerank' stage signifies that the reranker considers both the original user intent and the content of the initially retrieved documents to achieve a more accurate relevance ordering. The color gradient and numerical labels on the final results visually reinforce the concept of ordered relevance.

**Key Insights:**
The main takeaway is the clear demonstration of a two-stage search pipeline. The 'First stage' is for initial retrieval ('Search (dense, keyword, or hybrid)') from a 'Text archive (e.g., millions of documents)' based on a 'Query', yielding 'Initial results'. The 'Second stage' is a crucial 'Rerank' process that takes both the 'Initial results' and the original 'Query' to produce 'Final results'. The diagram emphasizes that the 'Rerank' step's goal is to order results by relevance, with '1' being 'Highest relevance' and '5' being 'Lowest relevance'. This highlights the importance of reranking for refining search quality beyond initial retrieval. The explicit mention of 'dense, keyword, or hybrid' search methods in the first stage indicates the flexibility and common approaches used in initial retrieval.

**Document Context:**
This image serves as a foundational diagram within a document discussing 'Reranking', specifically in the context of LLM rerankers as part of a search pipeline. It visually explains how LLM rerankers fit into the overall process of a search engine, acting after an initial search to reorder shortlisted results. The preceding text, 'Figure 8-14. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance,' directly introduces and contextualizes this diagram, making it a crucial visual aid for understanding the operational flow and the specific role of rerankers in enhancing search relevance.

**Summary:**
The image illustrates a two-stage search pipeline incorporating an LLM reranker to reorder search results by relevance. The process begins with a user's 'Query' and a 'Text archive' (e.g., millions of documents). In the 'First stage', a 'Search' mechanism (which can be 'dense, keyword, or hybrid') processes the query and the text archive to generate 'Initial results'. These 'Initial results', along with the original 'Query', are then fed into the 'Second stage'. The 'Second stage' involves a 'Rerank' process. This 'Rerank' step reorders the 'Initial results' to produce 'Final results'. The 'Final results' are explicitly numbered from '1' to '5', indicating a ranked order. A vertical bracket alongside these 'Final results' clearly labels '1' as 'Highest relevance' and '5' as 'Lowest relevance', demonstrating the objective of the reranking stage to refine the order of relevance.](images/8d5d6228d61534d07a267c859c6215862294a9eb29f287f2ea841fa737bceb64.jpg)
Figure 8-14. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance.

# Reranking example

A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking. Cohere’s Rerank endpoint is a simple way to start using a first reranker. We simply pass it the query and texts and get the results back. We don’t need to train or tune it:

query $=$ "how precise was the science"   
results $=$ co.rerank(query=query, documents ${ } = { }$ texts, top_ $\harpoonright$ , return_docu   
ments $\varprojlim \varprojlim 2$ True)   
results.results

We can print these results:

for idx, result in enumerate(results.results): print(idx, result.relevance_score , result.document.text)

# Output:

0 0.1698185 It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics   
1 0.07004896 The film had a worldwide gross over $\$ 677$ million (and $\$ 773$ million with subsequent re-releases), making it the tenth-highest grossing film of 2014   
2 0.0043994132 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar

This shows the reranker is much more confident about the first result, assigning it a relevance score of 0.16, while the other results are scored much lower in relevance.

In this basic example, we passed our reranker all 15 of our documents. More often, however, our index would have thousands or millions of entries, and we need to shortlist, say one hundred or one thousand results and then present those to the reranker. This shortlisting step is called the first stage of the search pipeline.

The first-stage retriever can be keyword search, dense retrieval, or better yet—hybrid search that uses both of them. We can revisit our previous example to see how adding a reranker after a keyword search system improves its performance.

Let’s tweak our keyword search function so it retrieves a list of the top 10 results using keyword search, then use rerank to choose the top 3 results from those 10:

def keyword_and_reranking_search(query, top_ $k = 3$ , num_candidates $\begin{array} { r l } { \mathit { \Pi } } & { { } = } \\ { \mathit { \Pi } } & { { } = } \end{array} .$ ): print("Input question:", query) ##### BM25 search (lexical search) ##### bm25_scores $=$ bm25.get_scores(bm25_tokenizer(query)) top_n $=$ np.argpartition(bm25_scores, -num_candidates)[-num_candidates:] bm25_hits $=$ [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n] bm25_hits $=$ sorted(bm25_hits, key=lambda x: x['score'], reverse=True) print(f"Top-3 lexical search (BM25) hits") for hit in bm25_hits[0:top_k]: print("\t{:.3f}\t{}".format(hit['score'], texts[hit['cor   
pus_id']].replace("\n", " "))) #Add re-ranking docs $=$ [texts[hit['corpus_id']] for hit in bm25_hits] print(f"\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)") results $=$ co.rerank(query=query, documents $\mathbf { \Psi } _ { 1 } =$ docs, top_n=top_k, return_docu   
ments=True) # print(results.results) for hit in results.results: # print(hit) print("\t{:.3f}\t{}".format(hit.relevance_score, hit.docu   
ment.text.replace("\n", " ")))

Now we can send our query and check the results of keyword search and then the result of keyword search shortlisting its top 10 results, then pass them on to the reranker:

keyword_and_reranking_search(query $=$ "how precise was the science")

# Results:

Input question: how precise was the science   
Top-3 lexical search (BM25) hits   
1.789 Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan   
1.373 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar   
0.000 Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects   
Top-3 hits by rank-API (10 BM25 hits re-ranked)   
0.004 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar   
0.004 Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind   
0.003 Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007

We see that keyword search assigns scores to only two results that share some of the keywords. In the second set of results, the reranker elevates the second result appropriately as the most relevant result for the query. This is a toy example that gives us a glimpse of the effect, but in practice, such a pipeline significantly improves search quality. On a multilingual benchmark like MIRACL, a reranker can boost performance from 36.5 to 62.8, measured as nDCG $@ 1 0$ (more on evaluation later in this chapter).

# Open source retrieval and reranking with sentence transformers

If you want to locally set up retrieval and reranking on your own machine, then you can use the Sentence Transformers library. Refer to the documentation at https:// oreil.ly/jJOhV for setup. Check the “Retrieve & Re-Rank” section for instructions and code examples for how to conduct these steps in the library.

# How reranking models work

One popular way of building LLM search rerankers is to present the query and each result to an LLM working as a cross-encoder. This means that a query and possible result are presented to the model at the same time allowing the model to view both these texts before it assigns a relevance score, as we can see in Figure 8-15. All of the documents are processed simultaneously as a batch yet each document is evaluated against the query independently. The scores then determine the new order of the results. This method is described in more detail in a paper titled “Multi-stage document ranking with BERT” and is sometimes referred to as monoBERT.

![## Image Analysis: 51c2f0a43488095d340c3edf1d1c001538f0d5079104511c1304195a5528f6bc.jpg

**Conceptual Understanding:**
This image conceptually illustrates the function of a reranker in a search system. Its main purpose is to demonstrate how an initial set of search results is processed and reordered based on a more detailed relevance assessment against the original query. The image communicates the idea that a specialized model ('Reranker') can take preliminary search outputs and refine their order by explicitly calculating a relevance score for each result, thereby presenting a more optimal 'New order' to the user.

**Content Interpretation:**
The image shows a conceptual model of how a reranking system refines search results. It demonstrates the transformation of an initial set of search results into a more semantically relevant order based on the interaction with the original query. The process highlights the 'Reranker' as the central component responsible for evaluating the relationship between the 'Query text' and each 'Document' in the 'Initial search result' to produce a specific 'Relevance score'. These scores, specifically 20%, 15%, and 80%, directly dictate the 'New order' ( #2, #3, #1), illustrating a direct causal relationship where higher relevance scores lead to a higher position in the new order. The system's purpose is to enhance the user experience by presenting the most pertinent documents first, even if they were not initially ranked highest.

**Key Insights:**
The main takeaway is that a reranker takes an initial set of search results and, by considering the original query, re-evaluates the relevance of each document to produce a refined, reordered list. Key insights include: (1) Reranking is a post-initial-search process designed to improve result quality, evidenced by the flow from 'Initial search result' to 'Reranker' to 'New order'. (2) The reranker assigns a specific 'Relevance score' to each document, such as 20%, 15%, and 80%, which are numerical quantifications of how well each document matches the query. (3) These scores directly determine the final 'New order' of documents, with the highest relevance score (80% for Document #2) corresponding to the highest rank (#1). This implies that a reranker provides a more nuanced understanding of document-query relevance than the initial search process.

**Document Context:**
This image directly supports the section titled 'How reranking models work' by visually explaining the core mechanism of a reranker. The text after the image, 'Figure 8-15. A reranker assigns a relevance score to each document by looking at the document and the query at the same time,' perfectly aligns with the diagram's depiction. The image serves as a detailed visual aid to understand the input (query and initial results), the processing step (reranker), and the output (relevance scores and reordered results) that are fundamental to explaining reranking models.

**Summary:**
This image illustrates the process of how a reranking model works in the context of search. It begins with a 'Query text' which is associated with an 'Initial search result' list containing multiple documents. These initial results, along with the query, are then fed into a 'Reranker' component. The 'Reranker' processes this information to assign a 'Relevance score' to each document. Based on these scores, a 'New order' for the documents is determined, effectively reordering the initial search results from most relevant to least relevant. For example, Document #2, which received an 80% relevance score, is now ranked #1. Document #40, with a 20% score, is ranked #2, and Document #68, with a 15% score, is ranked #3. The description systematically walks through the input, the reranking process, and the resulting output with specific numerical and textual labels.](images/51c2f0a43488095d340c3edf1d1c001538f0d5079104511c1304195a5528f6bc.jpg)
Figure 8-15. A reranker assigns a relevance score to each document by looking at the document and the query at the same time.

This formulation of search as relevance scoring basically boils down to being a classification problem. Given those inputs, the model outputs a score from 0–1 where 0 is irrelevant and 1 is highly relevant. This should be familiar from our classification discussions in Chapter 4.

To learn more about the development of using LLMs for search, "Pretrained trans‐ formers for text tanking: BERT and beyond" is a highly recommended look at the developments of these models until about 2021.

# Retrieval Evaluation Metrics

Semantic search is evaluated using metrics from the Information Retrieval (IR) field.   
Let’s discuss one of these popular metrics: mean average precision (MAP).

Evaluating search systems needs three major components: a text archive, a set of queries, and relevance judgments indicating which documents are relevant for each query. We see these components in Figure 8-16.

![## Image Analysis: 457e261628f6ac1a47662bde131437b87dc9dea25b0aeaefd34396f2d2fbecf4.jpg

**Conceptual Understanding:**
Conceptually, this image illustrates the fundamental data structure required for evaluating the performance of information retrieval (search) systems. It represents the 'ground truth' components needed for benchmarking. The main purpose is to demonstrate how a "test suite" is constructed, emphasizing the necessity of a defined document "Archive", a set of search "Queries", and precise "Relevance judgments" that objectively determine which documents are relevant to which queries. The key ideas communicated are the systematic preparation of data for objective evaluation and the critical role of human-assigned relevance judgments in establishing evaluation benchmarks.

**Content Interpretation:**
The image displays a structured setup for evaluating information retrieval systems. It consists of an "Archive" (a collection of "Text document #1" through "Text document #6"), a "Test suite" which encompasses "Queries" (e.g., "Query 1", "Query n") and "Relevance judgments". The "Relevance judgments" are presented as a matrix indicating the binary relevance (green checkmark for relevant, red X for not relevant) of each document in the archive to each specific query. For "Query 1", Text document #3 and Text document #5 are marked as relevant, while Text document #1, #2, #4, and #6 are not relevant. For "Query n", Text document #2, #3, and #6 are marked as relevant, while Text document #1, #4, and #5 are not relevant. This entire structure represents the 'ground truth' data against which the performance of a search system would be measured.

**Key Insights:**
The main takeaways from this image are:
1.  **Evaluation Requires a Benchmark:** To effectively evaluate a search system, a predefined and consistent "Test suite" is essential. The image explicitly labels this overarching component.
2.  **Test Suite Components:** A complete test suite fundamentally comprises specific "Queries" (e.g., "Query 1", "Query n") and corresponding "Relevance judgments". The image clearly separates these two elements within the "Test suite" box.
3.  **Document-Specific Relevance Judgments:** Relevance is not universal but specific to each document-query pair. The grid of green checkmarks and red 'X's demonstrates that each "Text document #" has an individual relevance assessment for each "Query", highlighting the granularity required for accurate evaluation. For example, "Text document #3" is relevant to both Query 1 and Query n, while "Text document #5" is relevant to Query 1 but not Query n.
4.  **Binary Ground Truth:** The use of only green checkmarks (relevant) and red 'X's (not relevant) signifies a binary judgment model for relevance, which is a common approach in information retrieval evaluation to establish clear ground truth.

These insights are directly evidenced by the explicit labels "Test suite", "Queries", "Relevance judgments", and the detailed matrix of green checkmarks and red 'X's mapping relevance for each document-query combination.

**Document Context:**
This image directly supports the "Retrieval Evaluation Metrics" section of the document, as indicated by the surrounding text: "Figure 8-16. To evaluate search systems, we need a test suite including queries and relevance judgments indicating which documents in our archive are relevant for each query." The diagram visually clarifies what a "test suite" entails, showing the interrelationship between the document "archive", the specific "queries", and the crucial "relevance judgments" that determine ground truth. It sets the foundational understanding for discussing and applying actual retrieval evaluation metrics by illustrating the data structure these metrics operate on.

**Summary:**
This image, titled "Test suite", provides a conceptual diagram illustrating the components necessary for evaluating information retrieval (search) systems. It is composed of two main visual elements: an "Archive" of documents on the left and a "Test suite" on the right, which contains "Queries" and corresponding "Relevance judgments".

On the left, a table clearly labeled "Archive" lists six distinct documents: "Text document #1", "Text document #2", "Text document #3", "Text document #4", "Text document #5", and "Text document #6". This represents the complete collection of documents available for retrieval.

On the right, a larger box, labeled "Test suite" at its top, is divided into two sections. The upper section displays a series of search queries. Specifically, it shows a search input field containing "Query 1" (preceded by a magnifying glass icon), followed by an ellipsis "... ", and then another search input field containing "Query n" (also preceded by a magnifying glass icon). This indicates that the test suite includes multiple queries, ranging from the first to the 'n-th' query.

The lower and larger section of the "Test suite" box is explicitly labeled "Relevance judgments" at its bottom. This section presents a matrix where each row corresponds to one of the "Text document #" from the "Archive", and each column corresponds to a "Query" (e.g., "Query 1" or "Query n"). Within this matrix, the relevance of each document to each query is explicitly marked using visual symbols: a green checkmark indicates that the document is relevant to the query, and a red 'X' indicates that it is not relevant.

For "Query 1", the relevance judgments are as follows:
*   "Text document #1": Not relevant (red X)
*   "Text document #2": Not relevant (red X)
*   "Text document #3": Relevant (green checkmark)
*   "Text document #4": Not relevant (red X)
*   "Text document #5": Relevant (green checkmark)
*   "Text document #6": Not relevant (red X)

For "Query n", the relevance judgments are as follows:
*   "Text document #1": Not relevant (red X)
*   "Text document #2": Relevant (green checkmark)
*   "Text document #3": Relevant (green checkmark)
*   "Text document #4": Not relevant (red X)
*   "Text document #5": Not relevant (red X)
*   "Text document #6": Relevant (green checkmark)

In essence, the image demonstrates that to evaluate a search system, one needs an archive of documents, a set of specific queries, and for each query, a predetermined, objective assessment (relevance judgment) for every document in the archive, indicating whether that document is truly relevant or not. This collection of queries and their associated relevance judgments forms the "ground truth" necessary to objectively measure the performance of any retrieval system.](images/457e261628f6ac1a47662bde131437b87dc9dea25b0aeaefd34396f2d2fbecf4.jpg)
Figure 8-16. To evaluate search systems, we need a test suite including queries and relevance judgments indicating which documents in our archive are relevant for each query.

Using this test suite, we can proceed to explore evaluating search systems. Let’s start with a simple example. Let’s assume we pass query 1 to two different search systems. And get two sets of results. Say we limit the number of results to three, as we can see in Figure 8-17.

![## Image Analysis: 3f3e21983c639c92125aebe6772542ec767c630eca3abbd6082bc71536eaa8ae.jpg

**Conceptual Understanding:**
The image conceptually illustrates a method for comparing the performance or output of two different information retrieval or search systems. Its main purpose is to visually demonstrate how a single query is used as an input to two separate search engines, which then generate their respective top results for evaluation. The core idea communicated is a controlled comparison experiment in search system analysis.

**Content Interpretation:**
The image depicts a comparative process for evaluating two distinct search systems. It shows the workflow where a single input query is processed simultaneously by two different search systems, each producing its own set of ranked results. This setup is fundamental for assessing the retrieval effectiveness of search engines against each other, typically by examining the relevance and order of the top documents returned.

**Key Insights:**
The main takeaway from this image is the standardized approach to comparing search systems: by submitting the exact same query to each system and then analyzing their respective outputs. The image emphasizes a parallel evaluation, highlighting that 'Query 1' is processed by both 'Search system 1' and 'Search system 2' independently to produce their 'Results' (1, 2, 3). This setup allows for a direct, controlled comparison of how each system responds to identical input.

**Document Context:**
This image directly supports the document's section on 'Retrieval Evaluation Metrics' by providing a visual representation of how two search systems can be compared. The accompanying text, 'Figure 8-17. To compare two search systems, we pass the same query from our test suite to both systems and look at their top results,' perfectly describes the process illustrated in the diagram. It clarifies the initial step of evaluating search system performance, which involves using a standardized query and analyzing the outputs.

**Summary:**
The image illustrates a method for comparing two search systems using a single query. A query, labeled 'Query 1', is fed in parallel to 'Search system 1' and 'Search system 2'. Both systems then generate a ranked list of results. For 'Search system 1', three results are explicitly shown, labeled '1', '2', and '3'. Similarly, for 'Search system 2', three corresponding results are shown, also labeled '1', '2', and '3'. The overall label 'Results' appears to the left, indicating the output from both systems. This diagram visually represents the process of sending an identical query to two different search engines and comparing their top search results to evaluate their performance.](images/3f3e21983c639c92125aebe6772542ec767c630eca3abbd6082bc71536eaa8ae.jpg)
Figure 8-17. To compare two search systems, we pass the same query from our test suite to both systems and look at their top results.

To tell which is a better system, we turn to the relevance judgments that we have for the query. Figure 8-18 shows which of the returned results are relevant.

![## Image Analysis: 104824366b9bf133333d09ff446016df94f4efca3b31fa8ff5b947ef33bd1437.jpg

**Conceptual Understanding:**
This image conceptually illustrates a basic framework for evaluating and comparing the performance of two distinct information retrieval systems. Its main purpose is to visually demonstrate how a single query is processed by multiple search systems, and how the relevance of the top retrieved results can be judged and used to determine which system performs 'better'. The image communicates the key idea that system performance is directly tied to its ability to return relevant documents for a given query, and that such comparisons are a fundamental part of retrieval evaluation.

**Content Interpretation:**
The image depicts a scenario for evaluating the effectiveness of two search systems, 'Search system 1' and 'Search system 2', using a common input 'Query 1'. It illustrates the 'Results' obtained from each system, specifically focusing on the top three ranked documents, and provides a binary relevance judgment (relevant or not relevant) for each document. The green checkmark signifies a relevant result, while the red 'x' denotes a non-relevant result. The significance lies in comparing the number of relevant results retrieved by each system for the identical query. 'Search system 1' yields two relevant results out of three (Result 1 and Result 3), whereas 'Search system 2' yields only one relevant result out of three (Result 3). This direct comparison highlights the differing performance of the systems, with 'Search system 1' appearing more effective for 'Query 1' based on the top three results.

**Key Insights:**
The main takeaway from this image is that the effectiveness of search systems can vary significantly for the same query, and this effectiveness can be quantitatively assessed through relevance judgments of retrieved documents. Specifically, for 'Query 1', 'Search system 1' demonstrates superior performance over 'Search system 2' by retrieving a higher number of relevant documents (2 relevant vs. 1 relevant) within the top three results. This illustrates the fundamental principle of comparing information retrieval systems: a system is considered 'better' if it retrieves more relevant documents in response to a query, especially at higher ranks. The textual elements 'Query 1', 'Search system 1', 'Search system 2', 'Results', '1', '2', '3', combined with the visual checkmarks and 'x's, provide clear evidence for this conclusion.

**Document Context:**
This image serves as a direct visual example to illustrate the concept of 'Retrieval Evaluation Metrics' as introduced in the document section. It provides a concrete, simplified scenario demonstrating how relevance judgments are used to compare the performance of different search systems. The image directly supports the subsequent text, 'Figure 8-18. Looking at the relevance judgments from our test suite, we can see that system 1 did a better job than system 2,' by visually substantiating the claim that 'system 1 did a better job than system 2' by showing the distribution of relevant and non-relevant results for each system in response to 'Query 1'. It enhances the reader's understanding of how performance differences are observed and assessed in information retrieval.

**Summary:**
The image illustrates a comparative evaluation of two search systems, 'Search system 1' and 'Search system 2', based on their response to a single input, 'Query 1'. The process begins with 'Query 1', which is simultaneously submitted to both search systems. Each system then generates a list of 'Results'. For 'Search system 1', the image displays three results, labeled '1', '2', and '3'. Result '1' is marked with a green checkmark, indicating relevance. Result '2' is marked with a red 'x', indicating non-relevance. Result '3' is marked with a green checkmark, indicating relevance. In parallel, for 'Search system 2', three results are also displayed, labeled '1', '2', and '3'. Result '1' is marked with a red 'x', indicating non-relevance. Result '2' is marked with a red 'x', indicating non-relevance. Result '3' is marked with a green checkmark, indicating relevance. This visual comparison clearly shows the individual relevance judgments for the top three results retrieved by each system, allowing for a direct assessment of their performance for the specified query.](images/104824366b9bf133333d09ff446016df94f4efca3b31fa8ff5b947ef33bd1437.jpg)
Figure 8-18. Looking at the relevance judgments from our test suite, we can see that system 1 did a better job than system 2.

This shows us a clear case where system 1 is better than system 2. Intuitively, we may just count how many relevant results each system retrieved. System 1 got two out of three correct, and system 2 got only one out of three correct. But what about a case like Figure 8-19 where both systems only get one relevant result out of three, but they’re in different positions?

![## Image Analysis: dd96673b06494286470d26405ec1d619c949e920c13c36b8a9f29f618fcddc80.jpg

**Conceptual Understanding:**
The image conceptually represents a scenario for evaluating the effectiveness of search retrieval systems. Its main purpose is to demonstrate how two different search systems respond to the same query and how their respective results are ranked in terms of relevance. The core idea being communicated is that while the total number of relevant items found might be similar, the *position* of those relevant items in the ranked list significantly impacts the perceived quality and utility of a search system. This highlights a fundamental challenge in information retrieval evaluation where ranking is as important as recall.

**Content Interpretation:**
The image displays a comparison of retrieval performance for two hypothetical search systems, 'Search system 1' and 'Search system 2', given a specific 'Query 1'. It illustrates the concept of ranked retrieval and the relevance of results at different positions. Both systems retrieve one relevant result within the top three, but their ranking differs significantly. 'Search system 1' places the relevant result at position '1', while 'Search system 2' places its relevant result at position '3'. The red 'X' marks at positions '2' and '3' for 'Search system 1', and at positions '1' and '2' for 'Search system 2', signify irrelevant results. This visual representation emphasizes that the position of a relevant document in the search results is crucial for user experience and system effectiveness, which is a key aspect of retrieval evaluation metrics.

**Key Insights:**
The main takeaway from this image is that the ranking of relevant results is critical in evaluating the performance of a search system. Even if two systems retrieve the same number of relevant documents within a given set of top results, the one that ranks the relevant documents higher is generally considered more effective. This is evident as 'Search system 1' places the relevant result at position '1', while 'Search system 2' places it at position '3', despite both having one relevant result among their top three. This illustrates the need for evaluation metrics that are sensitive to the position of relevant items, rewarding systems that prioritize and present relevant information at the top.

**Document Context:**
This image directly supports the surrounding document text from the 'Retrieval Evaluation Metrics' section, specifically the statement: 'Figure 8-19. We need a scoring system that rewards system 1 for assigning a high position to a relevant result—even though both systems retrieved only one relevant result in their top three results.' The diagram serves as a clear visual example demonstrating why evaluating search systems solely based on the number of relevant results within a certain cutoff (e.g., top three) is insufficient. It highlights the importance of positional relevance, showing how 'Search system 1' is superior due to placing its single relevant result at the highest rank (position 1), unlike 'Search system 2' which places its single relevant result at a lower rank (position 3).

**Summary:**
This image illustrates a comparison between two search systems, 'Search system 1' and 'Search system 2', for a single input 'Query 1'. The diagram visually represents the ranked results (positions 1, 2, and 3) from each system and indicates their relevance using green checkmarks for relevant results and red 'X' marks for irrelevant results. For 'Search system 1', the result at position '1' is relevant, while results at positions '2' and '3' are irrelevant. In contrast, for 'Search system 2', the results at positions '1' and '2' are irrelevant, and only the result at position '3' is relevant. This setup highlights the difference in how each system prioritizes and ranks relevant information, even if the total count of relevant results within the top three is the same for both systems (one relevant result each). The overall structure is a top-down flow, starting from the query, branching to the two search systems, and then displaying their respective ranked results side-by-side for easy comparison.](images/dd96673b06494286470d26405ec1d619c949e920c13c36b8a9f29f618fcddc80.jpg)
Figure 8-19. We need a scoring system that rewards system 1 for assigning a high position to a relevant result—even though both systems retrieved only one relevant result in their top three results.

In this case, we can intuit that system 1 did a better job than system 2 because the result in the first position (the most important position) is correct. But how can we assign a number or score to how much better that result is? Mean average precision is a measure that is able to quantify this distinction.

One common way to assign numeric scores in this scenario is average precision, which evaluates system 1’s result for the query to be 1 and system 2’s to be 0.3. So let’s see how average precision is calculated to evaluate one set of results, and then how it’s aggregated to evaluate a system across all the queries in the test suite.

# Scoring a single query with average precision

To score a search system on this query, we can focus on scoring the relevant docu‐ ments. Let’s start by looking at a query that only has one relevant document in the test suite.

The first one is easy: the search system placed the relevant result (the only available one for this query) at the top. This gets the system the perfect score of 1. Figure 8-20 shows this calculation: looking at the first position, we have a relevant result leading to a precision at position 1 of 1.0 (calculated as the number of relevant results at position 1, divided by the position we’re currently looking at).

![## Image Analysis: 53b4ac896fb7e4050f94ae4bad7d08b65a765f49d0a7bf5f0f908ebae940dda5.jpg

**Conceptual Understanding:**
The image conceptually represents the evaluation process for information retrieval systems, specifically demonstrating how to calculate 'average precision' for a single query. Its main purpose is to break down this complex metric into understandable, step-by-step components, from identifying relevant documents in the test suite to assessing individual search results' relevance and then computing precision values at specific ranks, culminating in the final average precision score. The image communicates the key idea that average precision considers both the relevance of retrieved documents and their rank in the search results.

**Content Interpretation:**
The image demonstrates the calculation of Average Precision for a single query in an information retrieval context. It visually explains the steps involved: identifying relevant documents, determining actual relevance of search results, calculating precision at each relevant position (k), and finally computing the average precision. The values and checks indicate a scenario where only the first result is relevant out of three, and this single relevant result is the only relevant document in the test suite for the query. The significance is to show how these metrics are derived from a set of search results and ground truth relevance.

**Key Insights:**
1.  **Fundamental Concept of Average Precision:** The image illustrates the core components necessary to calculate average precision: a query, a search system's results, the actual relevance of those results, and precision at each relevant position. This demonstrates that average precision is derived from evaluating the ranked list of search results against known relevant documents. 
2.  **Step-by-Step Calculation:** It clearly shows that precision is first calculated at each position (k) where a relevant document is found. In this example, only the first result is relevant, leading to Precision @ 1. The formula 'Precision @ k = 1 / 1 = 1' reinforces that for this specific scenario, precision at the relevant position is 1. 
3.  **Average Precision Formula:** The diagram demonstrates the final calculation of average precision by dividing the sum of precision at relevant positions by the total number of relevant documents. In this particular case, 'Average precision = 1 / 1 = 1', indicating a perfect score because the single relevant document was retrieved at the top position. 
4.  **Relevance Evaluation:** The use of green checkmarks and red 'X' marks explicitly illustrates the process of assessing whether each retrieved document is 'Actually relevant to query?', which is a critical input for precision calculations. The specific text elements 'Total number of relevant documents for query in the test suite: 1' and 'Actually relevant to query? ' (with corresponding checkmarks/X's) are crucial evidence for these insights.

**Document Context:**
This image serves as a direct visual explanation for the 'Scoring a single query with average precision' section of the document. It provides a concrete, step-by-step example of how to calculate average precision, which is a crucial concept in evaluating search system performance. The text after the image, 'Figure 8-20. To calculate mean average precision, we start by calculating precision at each position, starting with position 1,' perfectly aligns with the visual representation, indicating that the diagram illustrates the initial steps and the overall calculation process for mean average precision.

**Summary:**
The image illustrates the process of scoring a single query using average precision, detailing how individual precision at each position (k) is calculated and then used to compute the overall average precision. It starts by defining the query and the total number of relevant documents. Then, it shows the results from a search system, evaluates their relevance, calculates precision at each relevant position, and finally determines the average precision based on these calculations. The key takeaway is that for this specific scenario, with only one relevant document at the first position, the precision at k and average precision both evaluate to 1. The diagram clearly breaks down the steps of information retrieval evaluation metrics for a simplified example.](images/53b4ac896fb7e4050f94ae4bad7d08b65a765f49d0a7bf5f0f908ebae940dda5.jpg)
Figure 8-20. To calculate mean average precision, we start by calculating precision at each position, starting with position 1.

Since we’re only scoring relevant documents we can ignore the scores of nonrelevant documents and stop our calculation here. What if the system actually placed the only relevant result at the third position, however? How would that affect the score? Figure 8-21 shows how that results in a penalty.

![## Image Analysis: 212c75a55b3a89b9e99403a1aa54424a90809002084c0fe7306eca40497291be.jpg

**Conceptual Understanding:**
The image conceptually represents the process of evaluating a search engine's performance for a single query using the Average Precision metric. Its main purpose is to clearly demonstrate, step-by-step, how Average Precision is calculated, particularly highlighting the scenario where a single relevant document is ranked lower than irrelevant ones. The image communicates the idea that the ranking of results is crucial, and a system that places relevant documents lower down in the result list will receive a lower Average Precision score, indicating poorer performance in terms of relevance ordering.

**Content Interpretation:**
This image systematically illustrates the calculation of Average Precision (AP) for a search query. It shows how individual precision scores at various ranks (Precision @ k) are determined based on the relevance of retrieved documents and then aggregated to compute the overall Average Precision. The core concept demonstrated is the impact of result ranking on search effectiveness metrics, specifically how placing non-relevant documents ahead of a relevant document penalizes the score.

**Key Insights:**
The main takeaway from this image is that Average Precision is a rank-sensitive metric for evaluating search system performance. It clearly demonstrates that the position of relevant documents in the search results significantly impacts the overall score. Specifically, if a search system places non-relevant documents at higher ranks (like results 1 and 2 in this example) before finding the single relevant document (at result 3), the precision at those earlier positions is zero, thus lowering the final Average Precision. The textual evidence includes: 'Total number of relevant documents for query in the test suite : 1' sets the context. The 'Actually relevant to query?' column showing 'X' for results 1 and 2, and a checkmark for result 3, explicitly indicates the relevance judgments. The 'Precision @ k' calculations '0 / 1 = 0', '0 / 2 = 0', and '1 / 3 = 0.3' directly show how the late appearance of the relevant document leads to zero precision at earlier ranks and a lower overall average. The final 'Average precision = 0.3 / 1 = 0.3' confirms this.

**Document Context:**
This image directly supports the document's section titled 'Scoring a single query with average precision' and the accompanying text: 'Figure 8-21. If the system places nonrelevant documents ahead of a relevant document, its precision score is penalized.' It serves as a visual explanation of how this penalty manifests in the calculation of average precision. By detailing each step of the calculation, from identifying relevant documents to computing precision at each position and finally the average, the image provides concrete evidence for the statement that non-relevant documents at higher ranks negatively affect the precision score.

**Summary:**
The image illustrates the calculation of Average Precision for a single search query. It begins with a 'Query' and indicates that the 'Total number of relevant documents for query in the test suite' is 1. A 'Search system' then produces 'Results' which are ranked 1, 2, and 3. The column 'Actually relevant to query?' shows that Result 1 is not relevant (red 'X'), Result 2 is not relevant (red 'X'), and Result 3 is relevant (green checkmark). The image then calculates 'Precision at position (k)'. For 'Precision @ 1', the value is 0 because the first result is not relevant (indicated by a red 'X'). For 'Precision @ 2', the value is also 0 because neither the first nor the second result is relevant (indicated by two red 'X's). For 'Precision @ 3', the value is calculated based on the fact that one relevant document has been found up to this point (indicated by two red 'X's and one green checkmark). The 'Precision @ k' column provides the numerical calculations: '0 / 1 = 0' for position 1, '0 / 2 = 0' for position 2, and '1 / 3 = 0.3' for position 3. Finally, the 'Average precision' is calculated as the sum of precision values at relevant ranks divided by the total number of relevant documents. In this case, only the precision at position 3 (0.3) contributes, and the total number of relevant documents is 1, so 'Average precision = 0.3 / 1 = 0.3'. The purple boxes highlight the calculated precision values (0.3) and the final average precision (0.3). This comprehensive explanation details how a single relevant document appearing later in the search results impacts the overall average precision score.](images/212c75a55b3a89b9e99403a1aa54424a90809002084c0fe7306eca40497291be.jpg)
Figure 8-21. If the system places nonrelevant documents ahead of a relevant document, its precision score is penalized.

Let’s now look at a query with more than one relevant document. Figure 8-22 shows that calculation and how averaging now comes into the picture.

![## Image Analysis: 408e13e96c40de045645fa9f63d8ff0e59184a59cef98756545a5bb6fb43a748.jpg

**Conceptual Understanding:**
This image conceptually represents the process of evaluating the effectiveness of a search system's ranking for a given query using the 'Average precision' metric. Its main purpose is to visually and arithmetically demonstrate how Average Precision is calculated, illustrating the considerations of document relevance and their ranked positions in the search results. The core idea communicated is that Average Precision provides a single-number measure of search quality, reflecting the ability of a search system to rank relevant documents higher in its output, taking into account the precision at each point a relevant document is encountered.

**Content Interpretation:**
The image displays a sequential process for calculating 'Average precision' as a metric for evaluating search system performance. It specifically illustrates how to calculate this metric for a single query given a set of ranked search results and knowledge of the total relevant documents. The process involves identifying relevant documents among the retrieved results, calculating precision at each rank where a relevant document appears, and then averaging these precision values. The calculations demonstrate the application of the Average Precision formula, where only the precision scores at the ranks of relevant documents contribute to the sum, which is then divided by the total number of relevant documents that exist for the query. This visual breaks down a potentially complex metric into understandable, discrete steps.

**Key Insights:**
The main takeaway from this image is a clear, step-by-step understanding of how the Average Precision (AP) metric is calculated. It teaches that:

1.  **Relevance is Key:** The AP calculation first requires determining if each retrieved document is 'Actually relevant to query?'.
2.  **Total Relevant Documents:** The 'Total number of relevant documents for query in the test suite' (in this case, '2') is a critical denominator for the final average.
3.  **Precision at K for Relevant Docs Only:** 'Precision @ k' is calculated for each position (k) where a *relevant* document is found. For example, for the first relevant document at position '1', Precision @ 1 is '1 / 1 = 1'. For the second relevant document at position '3', Precision @ 3 is '2 / 3 = 0.67'. Non-relevant documents (like the one at position '2') do not contribute a precision value to the sum for AP.
4.  **Averaging Relevant Precisions:** The 'Average precision' is computed by summing these individual 'Precision @ k' values (from relevant documents only) and dividing by the 'Total number of relevant documents' in the test suite (e.g., ('1' + '0.67') / '2').

These insights are directly evidenced by the explicit labels 'Actually relevant to query?', 'Total number of relevant documents...', and the precise mathematical computations '1 / 1 = 1', '2 / 3 = 0.67', and 'Average precision = (1 + 0.67) / 2 = 0.8', along with the checkmarks indicating inclusion and 'X' marks indicating exclusion.

**Document Context:**
This image directly supports the document's section titled 'Scoring a single query with average precision' and the accompanying text: 'Figure 8-22. Average precision of a document with multiple relevant documents considers the precision at k results of all the relevant documents.' It serves as a concrete, step-by-step example of how Average Precision is computed. By breaking down the calculation from query to final score, it provides the reader with a clear understanding of the definition and application of this information retrieval metric, which is crucial for evaluating the effectiveness of search algorithms and systems within the broader narrative of the document.

**Summary:**
The image illustrates the calculation of 'Average precision' for a single query within a search system, considering three retrieved results and a total of two relevant documents. The process begins with a user's 'Query' which is processed by a 'Search system'. The system returns 'Results', which are then evaluated for their 'Actual relevance to query'.

For each result, its relevance is determined: Result 1 is relevant (green checkmark), Result 2 is not relevant (red 'X'), and Result 3 is relevant (green checkmark). The 'Total number of relevant documents for query in the test suite' is indicated as '2'.

Next, the 'Precision at position (k)' is considered. This involves calculating 'Precision @ k' for each position where a relevant document is found. The calculation is explicitly shown:

*   For Result 1 (position k=1), which is relevant: Precision @ 1 = '1 / 1 = 1'. The green checkmark next to 'Precision @ 1' indicates its inclusion in the average.
*   For Result 2 (position k=2), which is not relevant: No precision value is calculated for this position as it's not relevant. The red 'X' next to 'Precision @ 2' indicates it's excluded from the sum of precision values.
*   For Result 3 (position k=3), which is relevant: Precision @ 3 = '2 / 3 = 0.67'. The green checkmark next to 'Precision @ 3' indicates its inclusion in the average.

Finally, the 'Average precision' is calculated. This is done by summing the precision values only at the positions where relevant documents were found, and dividing by the total number of relevant documents in the test suite. The formula shown is:

Average precision = ('1' + '0.67') / '2' = '0.8'.

This comprehensive explanation demonstrates the step-by-step evaluation of search results to arrive at the Average Precision score, highlighting the contributions only from relevant documents at their respective ranks.](images/408e13e96c40de045645fa9f63d8ff0e59184a59cef98756545a5bb6fb43a748.jpg)
Figure 8-22. Average precision of a document with multiple relevant documents consid‐ ers the precision at $k$ results of all the relevant documents.

# Scoring across multiple queries with mean average precision

Now that we’re familiar with precision at k and average precision, we can extend this knowledge to a metric that can score a search system against all the queries in our test suite. That metric is called mean average precision. Figure 8-23 shows how to calculate this metric by taking the mean of the average precisions of each query.

![## Image Analysis: e370d1973404fc314a46e1c1ba2162984eb3bc4619a2bf17ee3d924e31822450.jpg

**Conceptual Understanding:**
This image conceptually represents the calculation of Mean Average Precision (MAP), a key performance metric in information retrieval. Its main purpose is to illustrate the step-by-step aggregation of precision scores from individual search queries to a single, overall system performance score. The diagram communicates the idea that a comprehensive evaluation of a search system involves assessing its performance across multiple user queries, taking into account both the relevance and the ranking of retrieved documents, and then combining these individual query performances into an aggregate metric (MAP) that can be used for system comparison.

**Content Interpretation:**
The image illustrates the process of calculating Mean Average Precision (MAP), a common metric in information retrieval to evaluate the performance of a search system. It shows how precision scores are first calculated for individual queries and then averaged. For 'Query 1', there are '2' relevant documents. Its 'Precision @ k' values are '1' and '0.67', leading to an 'Average precision' of '0.8'. For 'Query 2', there is '1' relevant document. Its 'Precision @ k' values include '1', resulting in an 'Average precision' of '1'. For 'Query 3', there is '1' relevant document. Its 'Precision @ k' value is '0.3', leading to an 'Average precision' of '0.3'. Finally, the 'Mean average precision' is calculated by averaging the 'Average precision' scores of the individual queries ('0.8', '1', '0.3'), which results in '0.7'. The purple-shaded boxes for 'Precision @ k' indicate specific precision values at certain recall points, while empty boxes suggest no relevant documents or no score at that specific rank. The green boxes indicate the count of 'Relevant documents for query'. The gray boxes show the 'Average precision' for each query. The final yellow box highlights the overall 'Mean average precision'.

**Key Insights:**
The main takeaway from this image is a clear, numerical demonstration of how Mean Average Precision (MAP) is computed. It teaches that MAP is not a direct average of all precision values, but rather an average of 'Average Precision' scores, where each 'Average Precision' is first calculated for a single query. The process involves identifying relevant documents, calculating precision at various retrieved ranks (Precision@k), determining an average precision for each query, and then averaging these per-query average precisions. The specific values, such as 'Relevant documents for query: 2' for Query 1, 'Average precision: 0.8' for Query 1, and the final 'Mean average precision: 0.7', provide concrete evidence of these steps. This illustrates that MAP accounts for both precision and the order of relevant documents, making it a robust metric for evaluating search relevance across multiple queries.

**Document Context:**
This image directly supports the document section titled 'Scoring across multiple queries with mean average precision'. It visually explains the concept described in the surrounding text, which states that 'The mean average precision takes into consideration the average precision score of a system for every query in the test suite. By averaging them, it produces a single metric that we can use to compare a search system against another.' The diagram serves as a concrete example of this abstract definition, showing the calculation with numerical values for three distinct queries, thereby enhancing the reader's understanding of how MAP is derived and used as a comparative metric for search systems.

**Summary:**
This image is a detailed diagram illustrating the calculation of Mean Average Precision (MAP) for a search system across three distinct queries. It systematically breaks down the process, starting from the number of relevant documents for each query, through individual precision scores at various retrieval points (Precision@k), to the average precision for each query, and finally, the overall mean average precision. The diagram presents a clear, step-by-step aggregation of scores to arrive at a single metric. The calculation proceeds horizontally for each query, then aggregates these query-specific average precisions to a final mean value. Each numerical value and label contributes to understanding how a system's retrieval effectiveness is quantified.](images/e370d1973404fc314a46e1c1ba2162984eb3bc4619a2bf17ee3d924e31822450.jpg)
Figure 8-23. The mean average precision takes into consideration the average precision score of a system for every query in the test suite. By averaging them, it produces a single metric that we can use to compare a search system against another.

You may be wondering why the same operation is called “mean” and “average.” It’s likely an aesthetic choice because MAP sounds better than average average precision.

Now we have a single metric that we can use to compare different systems. If you want to learn more about evaluation metrics, see the “Evaluation in Information Retrieval” chapter of Introduction to Information Retrieval (Cambridge University Press) by Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze.

In addition to mean average precision, another metric commonly used for search systems is normalized discounted cumulative gain (nDCG), which is more nuanced in that the relevance of documents is not binary (relevant versus not relevant) and one document can be labeled as more relevant than another in the test suite and scoring mechanism.

# Retrieval-Augmented Generation (RAG)

The mass adoption of LLMs quickly led to people asking them questions and expecting factual answers. While the models can answer some questions correctly, they also confidently answer lots of questions incorrectly. The leading method the industry turned to remedy this behavior is RAG, described in the paper “RetrievalAugmented Generation for Knowledge-Intensive NLP Tasks” (2020)1 and illustrated in Figure 8-24.

![## Image Analysis: c5b242c007ebb673a1a3f72d53490c988aa888a2e2792ce8d156f721ed29285a.jpg

**Conceptual Understanding:**
The image conceptually represents the fundamental workflow of a Retrieval-Augmented Generation (RAG) system. Its main purpose is to illustrate the two primary stages involved in generating an answer to a user's question by first retrieving relevant information from external data sources and then using that information to produce a 'grounded' response. The key idea communicated is the enhancement of traditional language models (implied by 'generation') with a retrieval mechanism to improve factual accuracy and reduce hallucinations.

**Content Interpretation:**
This image illustrates the core architecture and operational flow of a Retrieval-Augmented Generation (RAG) system. It conceptually depicts how a user's question is processed through two main stages: 'Retrieval' and 'Grounded generation,' ultimately leading to an 'Answer.' The diagram highlights the crucial interaction of the 'Retrieval' phase with 'Data source(s),' emphasizing that the generation of the answer is 'grounded' in external, retrieved information.

**Key Insights:**
The main takeaway from this image is the two-step sequential nature of a basic Retrieval-Augmented Generation (RAG) system. Firstly, it clarifies that 'Retrieval' is the initial processing step within the RAG system, which actively engages with external 'Data source(s)' to gather information relevant to the input 'Question.' Secondly, it shows that this retrieved information is then used by the 'Grounded generation' step to formulate the final 'Answer.' The diagram underscores the principle that RAG systems do not generate answers solely from their internal knowledge but augment this with external, retrieved context, ensuring the generated 'Answer' is 'grounded' in factual information.

**Document Context:**
This image serves as a foundational visual explanation within the 'Retrieval-Augmented Generation (RAG)' section of the document. It directly supports the subsequent text, 'Figure 8-24. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step.' The diagram visually breaks down this concept into its primary functional components and their sequence, thereby enhancing the reader's comprehension of the basic RAG pipeline before delving into more complex details. It establishes the core workflow that later discussions will likely elaborate upon.

**Summary:**
This diagram illustrates the fundamental architecture and sequential steps of a Retrieval-Augmented Generation (RAG) system. The process begins with a user's 'Question' input, which is then directed into the RAG system. The first core component within the system is '1) Retrieval,' where relevant information is gathered from 'Data source(s)' in a bidirectional exchange. Following the retrieval step, the gathered information, along with the original question, is fed into the '2) Grounded generation' component. This step is responsible for synthesizing an 'Answer' that is based on the retrieved data. The entire flow is depicted as a linear progression: Question -> Retrieval -> Grounded Generation -> Answer, with Retrieval actively interacting with external data sources.](images/c5b242c007ebb673a1a3f72d53490c988aa888a2e2792ce8d156f721ed29285a.jpg)
Figure 8-24. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step.

RAG systems incorporate search capabilities in addition to generation capabilities. They can be seen as an improvement to generation systems because they reduce their hallucinations and improve their factuality. They also enable use cases of “chat with my data” that consumers and companies can use to ground an LLM on internal company data, or a specific data source of interest (e.g., chatting with a book).

This also extends to search systems. More search engines are incorporating an LLM to summarize results or answer questions submitted to the search engine. Examples include Perplexity, Microsoft Bing AI, and Google Gemini.

# From Search to RAG

Let’s now turn our search system into a RAG system. We do that by adding an LLM to the end of the search pipeline. We present the question and the top retrieved documents to the LLM, and ask it to answer the question given the context provided by the search results. We can see an example in Figure 8-25.

This generation step is called grounded generation because the retrieved relevant information we provide the LLM establishes a certain context that grounds the LLM in the domain we’re interested in. Figure 8-26 shows how grounded generation fits after search if we continue our embeddings search example from earlier.

![## Image Analysis: e7cd7c5922267fb1968e49817daace0c407cf4c1030eba92000293ff0cbe03b7.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and workflow of a Retrieval-augmented generation (RAG) system. Its main purpose is to illustrate how such a system takes a user's question, retrieves relevant information from a specified data source, and then uses a large language model (LLM) to generate a grounded and citable answer. The image communicates the idea that combining information retrieval with advanced language generation leads to more accurate, contextually relevant, and verifiable responses, addressing the common issue of 'hallucination' in pure generative models by anchoring the output to real data.

**Content Interpretation:**
The image illustrates the operational flow of a Retrieval-augmented generation (RAG) system. It demonstrates a two-stage process for answering a user's question: first, retrieving and reranking relevant information from an external data source, and second, using a Large Language Model (LLM) to generate a 'grounded' answer based on that retrieved information. The significance is that the system provides explicit citations to its 'Sources' (e.g., search results), making the generated answer verifiable and transparent. The extracted text elements clearly delineate each step: 'Question' as input, '1) Retrieval (e.g., retrieval and reranking)' interacting with 'Data source' (Archive with specific 'Text document #1', 'Text document #2', 'Text document #3') to yield 'Relevant information', which then feeds into '2) Grounded generation (LLM)', finally producing a 'Result' with embedded citations like '[1]', '[2][3]' and a 'Sources' list ('1- Search result', '2- Search result', '3- Search result').

**Key Insights:**
The main takeaway from this image is the complete workflow of a Retrieval-augmented generation (RAG) system designed to answer user questions with grounded and citable responses. Key insights include: 1.  **Two-Stage Process:** The RAG system operates in two distinct phases: '1) Retrieval' and '2) Grounded generation'. 2.  **External Data Reliance:** The 'Retrieval' phase actively searches an 'Archive' (composed of 'Text document #1', 'Text document #2', 'Text document #3') from a 'Data source' to gather 'Relevant information'. 3.  **LLM Integration:** A 'Large Language Model (LLM)' is specifically utilized in the 'Grounded generation' phase to formulate the answer based on the retrieved data, ensuring the response is contextually relevant and not purely hallucinatory. 4.  **Attributable Results:** The system prioritizes transparency by providing a 'Result' that explicitly cites its 'Sources' ('1- Search result', '2- Search result', '3- Search result'), indicating where the information originated. The text 'This is one possible answer to your question [1], although others have also pointed out this other possible answer. [2][3]' underscores the system's ability to offer nuanced, multi-sourced, and verifiable answers. These specific text elements provide strong evidence for how RAG systems enhance answer quality and trustworthiness by linking generated content to its origin.

**Document Context:**
This image directly illustrates the concept of 'Generative search' as described in the accompanying document context, which states: 'Figure 8-25. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system).' The diagram precisely visualizes this by showing a 'Question' leading to a 'Retrieval-augmented generation (RAG) system' that produces a 'Result' with explicit 'Sources'. It fits within the document's narrative by providing a detailed, step-by-step explanation of how a generative search system, specifically one employing a RAG architecture, functions to deliver answers that are both informative and attributable.

**Summary:**
The diagram illustrates the workflow of a Retrieval-augmented generation (RAG) system, which processes a user's 'Question' to produce a 'Result' grounded in a 'Data source' and supported by 'Sources'. The process begins with a user submitting a 'Question'. This question is then fed into the 'Retrieval-augmented generation (RAG) system'. Inside this system, the first step is '1) Retrieval (e.g., retrieval and reranking)'. During this stage, the system interacts with a 'Data source', specifically an 'Archive' which contains 'Text document #1', 'Text document #2', and 'Text document #3'. The retrieval process extracts 'Relevant information' from these documents. This 'Relevant information' is then passed to the second stage of the RAG system, which is '2) Grounded generation (LLM)'. Here, a Large Language Model (LLM) uses the relevant information to formulate an answer. The output of the RAG system is a 'Result'. The description of the result states: 'This is one possible answer to your question [1], although others have also pointed out this other possible answer. [2][3]'. Below the result, a 'Sources' section provides the references for the generated answer: '1- Search result', '2- Search result', and '3- Search result'. The diagram effectively shows how a question is answered by first finding relevant information and then using that information to generate a coherent and attributable response.](images/e7cd7c5922267fb1968e49817daace0c407cf4c1030eba92000293ff0cbe03b7.jpg)
Figure 8-25. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system).

![## Image Analysis: 852e69a91bdde87aeaeefbd1f73f4a16f4a3b34ea0483091576f5d499437fd1d.jpg

**Conceptual Understanding:**
This image conceptually represents the workflow of a Retrieval-Augmented Generation (RAG) system. The main purpose is to illustrate how external knowledge or "relevant context" is retrieved and incorporated into an initial user prompt before it is passed to a Large Language Model (LLM). The key idea being communicated is the enhancement of LLM capabilities by providing it with specific, up-to-date, and relevant information from a knowledge base, thereby improving the accuracy and contextuality of its responses.

**Content Interpretation:**
The image depicts a system where a user's "Prompt" is first processed by an "Embedding model" to convert it into a numerical "embedding" vector. This embedding is then used to perform a similarity search within a "Search database" to find "Relevant context". This retrieved "Relevant context" is then concatenated or integrated with the original "Prompt" to form an "Updated prompt", which is finally passed to an "LLM".

*   **Prompt:** Represents the initial input query from the user. (Evidenced by the label "Prompt" and generic text lines within the shape).
*   **Embedding model:** This component is responsible for transforming the textual "Prompt" into a numerical vector representation (embedding). The small outward-pointing arrows icon suggests a transformation or expansion of the input into a vector space. (Evidenced by the label "Embedding model" and the transformation from text to gray boxes representing embeddings).
*   **Embeddings (gray boxes):** These symbolize the numerical representations of the "Prompt", which allow for semantic comparison.
*   **Search database:** This component stores a vast collection of information, likely in an embedded format, enabling efficient similarity searches. The colored rows of boxes within it suggest organized data segments. (Evidenced by the label "Search database" and its internal structure of colored boxes).
*   **Relevant context:** This is the specific information retrieved from the "Search database" that is most semantically similar or relevant to the initial "Prompt". The blue highlight indicates the selected information. (Evidenced by the label "Relevant context" and the highlighted text lines).
*   **Updated prompt:** This represents the augmented input for the LLM, comprising the original "Prompt" combined with the newly retrieved "Relevant context". The combined text with the blue highlight on top illustrates this augmentation. (Evidenced by the label "Updated prompt" and the visual combination of highlighted context with additional text).
*   **LLM (Large Language Model):** This is the final component that receives the enriched "Updated prompt" to generate a more informed and accurate response. The speech bubble icon indicates its function in generating natural language. (Evidenced by the label "LLM" and the speech bubble icon).

**Key Insights:**
The image teaches several key lessons about improving LLM performance and the mechanics of RAG:

*   **Enhancement of LLM Input:** The primary takeaway is that directly feeding a raw "Prompt" to an "LLM" can be suboptimal. By first retrieving "Relevant context" and creating an "Updated prompt", the LLM receives more comprehensive and targeted information. This is directly evidenced by the flow from "Prompt" through "Embedding model" to "Search database" to retrieve "Relevant context", which is then combined into an "Updated prompt" for the "LLM".
*   **Leveraging Embeddings for Semantic Search:** The process highlights the crucial role of "Embedding model" in converting text into numerical representations (embeddings) to enable semantic searches in a "Search database". This allows for finding conceptually similar information, not just keyword matches. This is evidenced by the "Prompt" going to the "Embedding model", which outputs the gray boxes (embeddings) that then query the "Search database".
*   **Improved Relevance and Accuracy:** By integrating "Relevant context" into the "Prompt", the LLM is provided with specific, factual information, which is critical for reducing "hallucinations" and improving the accuracy and relevance of its generated responses. The step where "Relevant context" is integrated into the "Updated prompt" explicitly supports this.
*   **Modularity of RAG Systems:** The diagram clearly shows distinct, modular components: a prompt, an embedding model, a search database, and an LLM. Each component performs a specific function, allowing for independent development and optimization. The distinct boxes and labels for each stage (e.g., "Embedding model", "Search database", "LLM") provide textual evidence for this modularity.

**Document Context:**
This image fits directly into a document section titled "From Search to RAG" and specifically expands on the idea of finding "the most relevant information to an input prompt by comparing the similarities between embeddings" and adding it to the prompt before giving it to the LLM (as per the provided text after the image: "Figure 8-26. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM."). It visually explains the architectural components and data flow involved in enriching user queries with external, retrieved knowledge. This is crucial for understanding how modern AI systems can access and utilize external, up-to-date information, moving beyond solely relying on their pre-trained knowledge.

**Summary:**
This diagram illustrates the "Retrieval-Augmented Generation" (RAG) process, a method used to enhance the capabilities of Large Language Models (LLMs) by providing them with relevant external information.

The process begins with a user's **Prompt**, which is an initial text query. This "Prompt" is then sent to an **Embedding model**. An "Embedding model" transforms the textual "Prompt" into a numerical representation called an "embedding" (represented by the series of gray boxes). These embeddings are essentially numerical vectors that capture the semantic meaning of the text.

Next, these embeddings are used to query a **Search database**. This database typically contains a vast amount of documents or information, also stored as embeddings. The system compares the embedding of the user's "Prompt" with the embeddings in the "Search database" to find the most semantically similar and thus **Relevant context**. This retrieved "Relevant context" is shown as highlighted text, signifying the specific information deemed pertinent to the user's query.

Following this, the "Relevant context" is combined with the original "Prompt" to create an **Updated prompt**. This "Updated prompt" is a richer, more informative input that includes both the user's initial question and the newly retrieved external information.

Finally, this **Updated prompt** is fed into an **LLM** (Large Language Model). By providing the LLM with this pre-contextualized "Updated prompt", the model can generate a more accurate, relevant, and well-informed response, drawing upon both its internal knowledge and the external "Relevant context" provided. This process ensures that the LLM has access to up-to-date and specific information, improving the quality and reliability of its output.](images/852e69a91bdde87aeaeefbd1f73f4a16f4a3b34ea0483091576f5d499437fd1d.jpg)
Figure 8-26. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM.

# Example: Grounded Generation with an LLM API

Let’s look at how to add a grounded generation step after the search results to create our first RAG system. For this example, we’ll use Cohere’s managed LLM, which builds on the search systems we’ve seen earlier in the chapter. We’ll use embedding search to retrieve the top documents, then we’ll pass those to the co.chat endpoint along with the questions to provide a grounded answer:

query $=$ "income generated"

# 1- Retrieval   
# We $^ { \prime } \mathbb { \mathbb { Z } } \mathbb { \mathbb { Z } }$ use embedding search. But ideally we'd do hybrid   
results $=$ search(query)   
# 2- Grounded Generation   
docs_dict $=$ [{'text': text} for text in results['texts']]   
response $=$ co.chat( message $=$ query, documents $=$ docs_dict   
)

print(response.text)

Result:

The film generated a worldwide gross of over $\$ 677$ million, or $\$ 773$ million with subsequent re-releases.

We are highlighting some of the text because the model indicated the source for these spans of text to be the first document we passed in:

citations $\mathbf { \Psi } _ { 1 } =$ [ChatCitation(star $\scriptstyle \mathtt { t } = 2 1$ , end $= 3 6$ , text='worldwide gross', document_ids $=$ ['doc_0']), ChatCitation(star $\tan \Theta$ , end $\scriptstyle 1 = 5 7$ , text='over $\$ 677$ million', document_ $\scriptstyle \mathbf { \cdot d } \mathbf { s } = [ \mathbf { \sigma } ^ { \prime } \mathbf { d } { \mathsf { o c } } _ { - } { \mathsf { o } } ^ { \prime } ] )$ , ChatCitation(start $\scriptstyle = 6 2$ , end $\mathtt { \Pi } = 1 \Theta 3$ , text='\$773 million with subsequent re-releases.', document_ids $\mathbf { \equiv } =$ ['doc_0'])]   
documents=[{'id': 'doc_0', 'text': 'The film had a worldwide gross over $\$ 677$ million (and $\$ 773$ million with subsequent re-releases), making it the tenthhighest grossing film of $2 \Theta 1 4 ^ { \prime } \rbrace ]$

# Example: RAG with Local Models

Let us now replicate this basic functionality with local models. We will lose the ability to do span citations and the smaller local model isn’t going to work as well as the larger managed model, but it’s useful to demonstrate the flow. We’ll start by downloading a quantized model.

# Loading the generation model

We start by downloading our model:

!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/ Phi-3-mini-4k-instruct-fp16.gguf

Using llama.cpp, llama-cpp-python, and LangChain, we load the text generation model:

from langchain import LlamaCpp

# Make sure the model path is correct for your system!   
llm $=$ LlamaCpp( model_path $=$ "Phi-3-mini-4k-instruct-fp16.gguf", n_gpu_layers $\ c = - 1$ , max_tokens $\begin{array} { r l } { : = } & { { } } \end{array}$ , n_ctx $= 2 \Theta 4 8$ , seed $= 4 2$ , verbose=False   
)

# Loading the embedding model

Let’s now load an embedding language model. In this example, we will choose the BAAI/bge-small-en-v1.5 model. At the time of writing, it is high on the MTEB leaderboard for embedding models and relatively small:

from langchain.embeddings.huggingface import HuggingFaceEmbeddings   
# Embedding model for converting text to numerical representations   
embedding_model $=$ HuggingFaceEmbeddings( model_name='thenlper/gte-small'   
)

We can now use the embedding model to set up our vector database:

from langchain.vectorstores import FAISS # Create a local vector database db $=$ FAISS.from_texts(texts, embedding_model)

# The RAG prompt

A prompt template plays a vital part in the RAG pipeline. It is the central place where we communicate the relevant documents to the LLM. To do so, we will create an additional input variable named context that can provide the LLM with the retrieved documents:

# from langchain import PromptTemplate

# Create a prompt template template $=$ """<|user|> Relevant information: {context}

Provide a concise answer the following question using the relevant information   
provided above:   
{question} $\mathit { \Pi } _ { < }$ |end|>   
<|assistant|>"""   
prompt $=$ PromptTemplate( template=template, input_variables=["context", "question"]   
)

from langchain.chains import RetrievalQA

# RAG pipeline   
rag $=$ RetrievalQA.from_chain_type( llm=llm, chain_type='stuff', retriever $=$ db.as_retriever(), chain_type_kwargs={ "prompt": prompt }, verbose=True   
)

Now we’re ready to call the model and ask it a question:

rag.invoke('Income generated')

Result:

The Income generated by the film in 2014 was over $\$ 677$ million worldwide. This made it the tenth-highest grossing film of that year. It should be noted, however, this figure includes both initial ticket sales as well as any subsequent re-releases. With these additional releases, total earnings surged to approximately $\$ 773$ million. The release format transitioned from traditional film stock projection in theaters to digital projectors once it was expanded to various venues in the United States. This shift might have contributed to wider audience reach and potentially higher grossing figures over time. However, specific data on how this affected total earnings isn't provided in the information above.

As always, we can adjust the prompt to control the model’s generation (e.g., answer length and tone).

# Advanced RAG Techniques

There are several additional techniques to improve the performance of RAG systems.   
Some of them are laid out here.

# Query rewriting

If the RAG system is a chatbot, the preceding simple RAG implementation would likely struggle with the search step if a question is too verbose, or to refer to context in previous messages in the conversation. This is why it’s a good idea to use an LLM to rewrite the query into one that aids the retrieval step in getting the right information. An example of this is a message such as:

User Question: “We have an essay due tomorrow. We have to write about some animal. I love penguins. I could write about them. But I could also write about dolphins. Are they animals? Maybe. Let’s do dolphins. Where do they live for example?”

This should actually be rewritten into a query like:

Query: “Where do dolphins live”

This rewriting behavior can be done through a prompt (or through an API call).   
Cohere’s API, for example, has a dedicated query-rewriting mode for co.chat.

# Multi-query RAG

The next improvement we can introduce is to extend the query rewriting to be able to search multiple queries if more than one is needed to answer a specific question. Take for example:

User Question: “Compare the financial results of Nvidia in 2020 vs. 2023”

We may find one document that contains the results for both years, but more likely, we’re better off making two search queries:

Query 1: “Nvidia 2020 financial results” Query 2: “Nvidia 2023 financial results”

We then present the top results of both queries to the model for grounded generation. An additional small improvement here is to also give the query rewriter the option to determine if no search is required and if it can directly generate a confident answer without searching.

# Multi-hop RAG

A more advanced question may require a series of sequential queries. Take for example a question like:

User Question: “Who are the largest car manufacturers in 2023? Do they each make EVs or not?”

To answer this, the system must first search for:

Step 1, Query 1: “largest car manufacturers $2 0 2 3 ^ { \mathfrak { n } }$

Then after it gets this information (the result being Toyota, Volkswagen, and Hyun‐ dai), it should ask follow-up questions:

Step 2, Query 1: “Toyota Motor Corporation electric vehicles” Step 2, Query 2: “Volkswagen AG electric vehicles” Step 2, Query 3: “Hyundai Motor Company electric vehicles”

# Query routing

An additional enhancement is to give the model the ability to search multiple data sources. We can, for example, specify for the model that if it gets a question about HR, it should search the company’s HR information system (e.g., Notion) but if the question is about customer data, that it should search the customer relationship management (CRM) (e.g., Salesforce).

# Agentic RAG

You may be able to now see that the list of previous enhancements slowly delegates more and more responsibility to the LLM to solve more and more complex problems. This relies on the LLM’s capability to gauge the required information needs as well as its ability to utilize multiple data sources. This new nature of the LLM starts to become closer and closer to an agent that acts on the world. The data sources can also now be abstracted into tools. We saw, for example, that we can search Notion, but by the same token, we should be able to post to Notion as well.

Not all LLMs will have the RAG capabilities mentioned here. At the time of writing, likely only the largest managed models may be able to attempt this behavior. Thank‐ fully, Cohere’s Command $\mathrm { R } +$ excels at these tasks and is available as an open-weights model as well.

# RAG Evaluation

There are still ongoing developments in how to evaluate RAG models. A good paper to read on this topic is “Evaluating verifiability in generative search engines” (2023), which runs human evaluations on different generative search systems.2

It evaluates results along four axes:

Fluency Whether the generated text is fluent and cohesive.

Perceived utility Whether the generated answer is helpful and informative.

Citation recall

The proportion of generated statements about the external world that are fully supported by their citations.

Citation precision

The proportion of generated citations that support their associated statements.

While human evaluation is always preferred, there are approaches that attempt to automate these evaluations by having a capable LLM act as a judge (called LLM-as-ajudge) and score the different generations along the different axes. Ragas is a software library that does exactly this. It also scores some additional useful metrics like:

Faithfulness

Whether the answer is consistent with the provided context

Answer relevance How relevant the answer is to the question

The Ragas documentation site provides more details about the formulas to actually calculate these metrics.

# Summary

In this chapter, we looked at different ways of using language models to improve existing search systems and even be the core of new, more powerful search systems. These include:

• Dense retrieval, which relies on the similarity of text embeddings. These are systems that embed a search query and retrieve the documents with the nearest embeddings to the query’s embedding.   
• Rerankers, systems (like monoBERT) that look at a query and candidate results and score the relevance of each document to that query. These relevance scores are then used to order the shortlisted results according to their relevance to the query, often producing an improved results ranking.   
• RAG, where search systems have a generative LLM at the end of the pipeline to formulate an answer based on retrieved documents while citing sources.

We also looked at one of the possible methods of evaluating search systems. Mean average precision allows us to score search systems to be able to compare across a test suite of queries and their known relevance to the test queries. Evaluating RAG systems requires multiple axes, however, like faithfulness, fluency, and others that can be evaluated by humans or by LLM-as-a-judge.

In the next chapter, we will explore how language models can be made multimodal and reason not just about text but images as well.

# Multimodal Large Language Models

When you think about large language models (LLMs), multimodality might not be the first thing that comes to mind. After all, they are language models! But we can quickly see that models can be much more useful if they’re able to handle types of data other than text. It’s very useful, for example, if a language model is able to glance at a picture and answer questions about it. A model that is able to handle text and images (each of which is called a modality) is said to be multimodal, as we can see in Figure 9-1.

![## Image Analysis: 5bbc40d784b2d5990a878ef3eb1976cf8547e79ac19c6d0bf4fc248438663d08.jpg

**Conceptual Understanding:**
The image conceptually illustrates the functionality of a 'Multimodal model'. Its main purpose is to demonstrate how such a model can accept and process data from various input modalities—specifically text, code, images, and audio—and subsequently generate outputs in different modalities, such as text and code. The core idea conveyed is the ability of a single artificial intelligence model to seamlessly integrate and understand information across diverse data types, breaking the traditional unimodal barrier in AI systems.

**Content Interpretation:**
The image depicts a conceptual model of how a 'Multimodal model' processes various types of data. The processes shown include the ingestion of different 'Input modality' types, their processing by the 'Multimodal model', and the generation of diverse 'Output modality' types. The concepts being illustrated are multimodality in AI, the ability of models to handle and integrate information from disparate data forms (text, code, image, audio), and the flexibility in generating different output formats. The relationship shown is a many-to-one input to the model, and a one-to-many output from the model. The 'Multimodal model' is the central system performing the integration and transformation. The significance of the data presented is that it provides concrete examples of the types of inputs (a simple textual statement 'This is a cat', a structured code snippet `{"type": "cat", "Description": "cute"}`, a visual representation of an image, and an abstract representation of audio) and outputs (a descriptive textual comment 'It's pixelated!' and a status code `{"status": "accepted"}`). This highlights the model's capacity not only to understand different inputs but also to respond with various output modalities, which may differ from the input modalities.

**Key Insights:**
The main takeaway from this image is that multimodal models are designed to process and understand information from multiple distinct data types simultaneously. This is evidenced by the 'Input modality' section, which explicitly lists 'Text', 'Code', 'Image', and 'Audio' as inputs feeding into the 'Multimodal model'. A key insight is that the output modalities are not restricted to the input modalities. For example, while an 'Image' and 'Audio' are provided as inputs, the 'Output modality' shown consists of 'Text' ('It's pixelated!') and 'Code' (`{"status": "accepted"}`). This demonstrates the model's ability to integrate and interpret diverse inputs to produce meaningful, albeit different, output forms. Another insight is the flexibility in output generation, where a single model can produce multiple types of outputs (textual description and a status code) based on its understanding of the combined inputs. The specific textual examples, such as the JSON structure for code inputs and outputs, provide concrete illustrations of how different data types are handled.

**Document Context:**
This image directly supports the document's discussion on 'Multimodal Large Language Models' by providing a clear visual representation of the concept. It clarifies the explanation that multimodal models can 'deal with different types (or modalities) of data, such as images, audio, video, or sensors.' The diagram also visually explains the point that 'It’s possible for a model to accept a modality as input yet not be able to generate in that modality,' by showing multiple input modalities (text, code, image, audio) leading to potentially different output modalities (text, code), not necessarily reproducing the input modalities. For example, an image input leads to text and code outputs, not another image. The diagram serves as a foundational visual aid to understand the operational definition and capabilities of multimodal models within the broader narrative of the document.

**Summary:**
The diagram illustrates the concept of a 'Multimodal model' by showing various input modalities being processed and resulting in different output modalities. On the left side, under the heading 'Input modality', four distinct types of inputs are presented: 'Text' with the phrase 'This is a cat' in a rounded rectangular box; 'Code' represented by a dark rectangular box containing JSON-like text `{"type": "cat", "Description": "cute"}`; an 'Image' displaying a small, pixellated illustration of a brown and white cat; and 'Audio' depicted by a waveform icon composed of vertical bars. All these input elements are connected by lines to a central pink rectangular box labeled 'Multimodal model'. From this 'Multimodal model', two distinct outputs are shown under the heading 'Output modality' on the right. One output is 'Text', presented as a rounded rectangular box containing the phrase 'It's pixelated!'. The second output is 'Code', shown in a dark rectangular box containing JSON-like text `{"status": "accepted"}`. The overall flow demonstrates how a single multimodal model can ingest diverse data types and produce different forms of output.](images/5bbc40d784b2d5990a878ef3eb1976cf8547e79ac19c6d0bf4fc248438663d08.jpg)
Figure 9-1. Models that are able to deal with different types (or modalities) of data, such as images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as input yet not be able to generate in that modality.

We have seen all manner of emerging behaviors rising from LLMs, from generaliza‐ tion capabilities and reasoning to arithmetic and linguistics. As models grow larger and smarter, so do their skill sets.1

The ability to receive and reason with multimodal input might further increase and help emerge capabilities that were previously locked. In practice, language does not solely live in a vacuum. As an example, your body language, facial expressions, intonation, etc. are all methods of communication that enhance the spoken word.

The same thing applies to LLMs; if we can enable them to reason about multimodal information, their capabilities might increase and we become able to deploy them to solve new kinds of problems.

In this chapter, we will explore a number of different LLMs that have multimodal capabilities and what that means for practical use cases. We will start by exploring how images are converted to numerical representations using an adaptation of the original Transformer technique. Then, we will show how LLMs can be extended to include vision tasks using this Transformer.

# Transformers for Vision

Throughout the chapters of this book, we have seen the success of using Transformerbased models for a variety of language modeling tasks, from classification and clustering to search and generative modeling. So it might not be surprising that researchers have been looking at a way to generalize some of the Transformer’s success to the field of computer vision.

The method they came up with is called the Vision Transformer (ViT), which has been shown to do tremendously well on image recognition tasks compared to the previously default convolutional neural networks (CNNs).2 Like the original Trans‐ former, ViT is used to transform unstructured data, an image, into representations that can be used for a variety of tasks, like classification, as illustrated in Figure 9-2.

ViT relies on an important component of the Transformer architecture, namely the encoder. As we saw in Chapter 1, the encoder is responsible for converting textual input into numerical representations before being passed to the decoder. However, before the encoder can perform its duties, the textual input needs to be tokenized first, as is illustrated in Figure 9-3.

![## Image Analysis: 89b2e4baab70bd68d9e4aad174f04afc319989576b2239f816492a21b5751c98.jpg

**Conceptual Understanding:**
This image conceptually represents the functional pipeline of Transformer models, distinguishing between a general Transformer for text-based tasks and a Vision Transformer (ViT) for image-based tasks. The main purpose is to illustrate how these advanced neural network architectures take unstructured 'Input Features' (Text or Image), process them through their respective 'Transformer' or 'Vision Transformer (ViT)' mechanisms, and produce a structured 'Output Prediction' in the form of classification categories with associated confidence levels. It communicates the idea that Transformers can be adapted for different types of data, providing concrete examples of text spam detection and image animal classification.

**Content Interpretation:**
The image displays two parallel processes illustrating the application of Transformer models for classification tasks on different types of unstructured data: text and images. The first process shows a standard 'Transformer' model performing text classification, specifically identifying spam from a given textual input. The second process demonstrates a 'Vision Transformer (ViT)' model performing image classification, identifying an animal in an image. Both processes take 'Input Features' and generate an 'Output Prediction' with associated probability percentages for different categories.

**Key Insights:**
1. Transformer models are adaptable for various data types, specifically text and images, for classification tasks. 2. Specialized Transformer architectures, such as the 'Vision Transformer (ViT)', are developed for specific data modalities (e.g., images). 3. These models provide probabilistic predictions for classification outputs, as evidenced by the percentage likelihoods for 'Spam'/'Not spam' and 'Cat'/'Dog'. 4. The example text, 'You have been selected to receive 1.4 million dollars!', serves as a common example of a spam phrase, demonstrating the model's ability to identify such content with a 78% confidence level. 5. Vision Transformers can accurately classify images, as shown by the 98% confidence in identifying a 'Cat'.

**Document Context:**
This image serves as a direct visual explanation for the document's 'Transformers for Vision' section. It supports the narrative by demonstrating how both the 'original Transformer' (for text) and the 'Vision Transformer' (for images) process unstructured data and output classification predictions. The visual examples make the theoretical concepts of applying Transformers to different modalities concrete and easily understandable, directly relating to the preceding and succeeding text that discusses these models.

**Summary:**
The image illustrates two distinct processes: one for text classification using a Transformer, and another for image classification using a Vision Transformer (ViT). In the first process, a text input, specifically the phrase "You have been selected to receive 1.4 million dollars!", is fed into a "Transformer" model. This model then outputs a prediction, indicating a 78% likelihood of the text being "Spam" and a 32% likelihood of it being "Not spam". The second process involves an image input, depicted as a cartoon cat. This image is processed by a "Vision Transformer (ViT)" model. The output prediction from the ViT indicates a 98% likelihood of the image being a "Cat" and a 2% likelihood of it being a "Dog". Both processes highlight how different types of unstructured data (Text and Image) are transformed into structured predictions by specialized Transformer models, showing the input features, the model, and the resulting output prediction with confidence levels for classification.](images/89b2e4baab70bd68d9e4aad174f04afc319989576b2239f816492a21b5751c98.jpg)
Figure 9-2. Both the original Transformer as well as the Vision Transformer take unstructured data, convert it to numerical representations, and finally use that for tasks like classification.

![## Image Analysis: 0ac32d0c67121f85cf6d7e342948eff598ff02bcdc9d43df90ad3bb34288a271.jpg

**Conceptual Understanding:**
This image conceptually represents the initial data preparation pipeline for textual input in a Natural Language Processing (NLP) system, specifically for a transformer model's encoder. The main purpose is to visually explain how a raw sentence is transformed into a sequence of discrete tokens, including special control tokens, before being processed by the transformer's encoder. The key ideas being communicated are tokenization, the role of special tokens (like `[CLS]` and `[SEP]`) in transformer architectures, and the input mechanism for an encoder.

**Content Interpretation:**
The image illustrates the initial stages of processing natural language text for a transformer model. It demonstrates the transformation of a raw textual input into a sequence of discrete tokens, which are then fed into an encoder. This process is fundamental for various Natural Language Processing (NLP) tasks where machine learning models need a structured, numerical representation of text to perform operations such as classification, translation, or text generation. The visual elements show a clear, sequential flow: raw text -> tokenized units -> encoder input.

**Key Insights:**
The main takeaways from this image are:

1.  **Necessity of Text Pre-processing:** Raw text, like "What a horrible movie!", cannot be directly processed by transformer models and requires conversion into a structured format.
2.  **Tokenization as a Key Step:** Tokenization is the process of breaking down text into smaller, meaningful units (tokens). This is evidenced by the input sentence being segmented into `[CLS]`, `what`, `a`, `horrible`, `movie`, `!`, `[SEP]` tokens.
3.  **Use of Special Tokens:** Transformer architectures often employ special tokens for various purposes. `[CLS]` (Classification) is commonly used to derive an aggregate representation of the entire input sequence for classification tasks. `[SEP]` (Separator) marks the end of a segment or separates multiple segments, as explicitly shown in the token sequence.
4.  **Encoder as the Processing Unit:** The tokenized sequence, including the special tokens, is fed into an "ENCODER", which is a central component of a transformer network responsible for generating contextualized embeddings of the input. This is indicated by the arrows leading from each token into the large "ENCODER" box.
5.  **Sequential Data Flow:** The diagram clearly depicts a sequential flow from raw text to tokens, and then from tokens to the encoder, illustrating a typical data preparation pipeline for NLP models.

**Document Context:**
The image, titled Figure 9-3 and appearing in a section about "Transformers for Vision," serves as a foundational explanation of text processing within transformer models. Despite the section's focus on vision, understanding how text is handled is crucial, especially in multimodal transformer architectures where text often complements or describes visual information. The accompanying text, "Figure 9-3. Text is passed to one or multiple encoders by first tokenizing it using a tokenizer," directly supports the visual workflow, clarifying that tokenization is the prerequisite step before encoding. This diagram helps the reader understand the textual input pipeline, which is a common component even in vision-focused transformer systems when text-based features or labels are involved.

**Summary:**
This diagram illustrates the fundamental process of preparing a raw textual input for a transformer model's encoder. The process begins with a human-readable sentence, which is then broken down into individual tokens, including special control tokens. These tokens are subsequently fed into an encoder for further processing. 

The workflow proceeds in three main stages:

1.  **Textual Input:** The initial stage involves the raw text. In this example, the input is the sentence "What a horrible movie!". This represents the unprocessed natural language data.

2.  **Tokens:** The textual input is then subjected to tokenization, where it is segmented into discrete units called tokens. This stage is labeled "Tokens" and shows the sentence broken down as follows:
    *   `[CLS]` (A special classification token, typically prepended to the input sequence. Its corresponding output embedding from the transformer's encoder is often used as the aggregate representation of the entire sequence for classification tasks).
    *   `what`
    *   `a`
    *   `horrible`
    *   `movie`
    *   `!` (The punctuation mark is treated as a distinct token).
    *   `[SEP]` (A special separator token, typically appended to the input sequence to indicate the end of the sentence or to separate different segments in multi-sentence inputs).

3.  **ENCODER:** All the individual tokens, including the special `[CLS]` and `[SEP]` tokens, are then passed into the "ENCODER". This represents a core component of a transformer architecture. The multiple arrows leading from each token into the ENCODER box signify that all tokens are input for encoding. A small symbolic icon, resembling a set of arrows, in the top right of the ENCODER box, suggests computation or transformation. The downward arrows originating from the bottom of the ENCODER indicate the output of this encoding process, which would typically be contextualized embeddings of the input tokens.](images/0ac32d0c67121f85cf6d7e342948eff598ff02bcdc9d43df90ad3bb34288a271.jpg)
Figure 9-3. Text is passed to one or multiple encoders by first tokenizing it using a tokenizer.

Since an image does not consist of words this tokenization process cannot be used for visual data. Instead, the authors of ViT came up with a method for tokenizing images into “words,” which allowed them to use the original encoder structure.

Imagine that you have an image of a cat. This image is represented by a number of pixels, let’s say $5 1 2 \times 5 1 2$ pixels. Each individual pixel does not convey much information but when you combine patches of pixels, you slowly start to see more information.

ViT uses a principle much like that. Instead of splitting up text into tokens, it converts the original image into patches of images. In other words, it cuts the image into a number of pieces horizontally and vertically as illustrated in Figure 9-4.

![## Image Analysis: 18216c84992965f3241c72220ba6f4d6deeef8964974b1a4df1d9e0e826508d3.jpg

**Conceptual Understanding:**
This image conceptually illustrates the "tokenization" process for image input. Its main purpose is to demonstrate how a complete image is broken down into a series of smaller, discrete sub-images or 'patches,' and then organized into a linear sequence. This process enables the conversion of a two-dimensional image into a one-dimensional sequence, which is a necessary step for applying sequence-based models, such as Transformers, to visual data.

**Content Interpretation:**
The image demonstrates the process of image tokenization, a method for preparing image data for models that process sequences, such as Transformers. It shows an "Original image" (a cat) being divided into a grid of "Patched image" segments, and then these patches are arranged into a linear "Flattened input" sequence. This sequence includes an empty patch, which often serves as a placeholder for a class token or as padding in Vision Transformer architectures. The overall process illustrates how a 2D image is transformed into a 1D sequence of visual tokens for machine learning processing.

**Key Insights:**
The main takeaway from this image is that images can be prepared for sequence-based models by a two-step tokenization process: first, dividing the "Original image" into smaller sub-images or "Patches," and second, arranging these patches sequentially to form a "Flattened input." This allows the 2D spatial information of an image to be represented as a 1D sequence of visual tokens. The presence of an empty patch in both the "Patched image" and "Flattened input" suggests the inclusion of special tokens (e.g., for classification) or padding, which are common practices in Vision Transformer architectures. The textual evidence "Original image", "Patched image", and "Flattened input" clearly delineate these stages and support the understanding of the transformation.

**Document Context:**
This image directly illustrates Figure 9-4, which describes "The 'tokenization' process for image input. It converts an image into patches of subimages." This explanation fits perfectly within a section discussing 'Transformers for Vision' by showing a foundational preprocessing step required to adapt image data for Transformer architectures, which typically operate on sequences of tokens rather than raw image pixels.

**Summary:**
The image illustrates the process of "tokenization" for an image input, a critical preprocessing step for models like Vision Transformers. This process begins with an "Original image," which is depicted as a pixel art cat with orange and white fur and blue eyes. This original image is then transformed into a "Patched image." In this stage, the original image is divided into a 3x3 grid of nine equal-sized sub-images or "patches." Visually, the cat is broken into these nine segments. Notably, the top-left patch in the "Patched image" grid is an empty white square, which often represents a special class token or padding in such architectures. Following the patching, the "Patched image" is converted into a "Flattened input." Here, the 3x3 grid of patches is linearized into a single, ordered sequence of nine patches. The patches are arranged from left to right, maintaining their relative order from the grid, with the empty square patch appearing first in the sequence, followed by the eight cat image patches. This entire sequence demonstrates how a continuous 2D image is effectively converted into a 1D sequence of discrete "tokens," making it suitable for processing by models designed for sequential data.](images/18216c84992965f3241c72220ba6f4d6deeef8964974b1a4df1d9e0e826508d3.jpg)
Figure 9-4. The “tokenization” process for image input. It converts an image into patches of subimages.

Just like we are converting text into tokens of text, we are converting an image into patches of images. The flattened input of image patches can be thought of as the tokens in a piece of text. However, unlike tokens, we cannot just assign each patch with an ID since these patches will rarely be found in other images, unlike the vocabulary of a text.

Instead, the patches are linearly embedded to create numerical representations, namely embeddings. These can then be used as the input of a Transformer model. That way, the patches of images are treated the same way as tokens. The full process is illustrated in Figure 9-5.

For illustrative purposes, the images in the examples were patched into $3 \times 3$ patches but the original implementation used $1 6 \times 1 6$ patches. After all, the paper is called “An Image is Worth 16x16 Words.”

What is so interesting about this approach is that the moment the embeddings are passed to the encoder, they are treated as if they were textual tokens. From that point forward, there is no difference in how a text or image trains.

Due to these similarities, the ViT is often used to make all kinds of language models multimodal. One of the most straightforward ways to use it is during the training of embedding models.

![## Image Analysis: 9db92eb5b04c517046fa926456e2bee5bf4d2bf51b716717cff95c95adea146f.jpg

**Conceptual Understanding:**
The image conceptually represents the pre-processing pipeline for a Vision Transformer (ViT). Its main purpose is to illustrate how a continuous visual input (an image) is discretized into smaller units (patches), linearly transformed into an embedding space, and augmented with a special classification token, before being fed into a Transformer's self-attention mechanism via an encoder. This process allows traditional Transformer architectures, originally designed for sequence data like text, to effectively process image data by treating image patches as equivalent to words or tokens.

**Content Interpretation:**
This image illustrates the initial stages of processing an image for a Vision Transformer (ViT) model. It shows how a standard image is broken down, transformed, and prepared as sequential input, similar to textual tokens, before being fed into a Transformer's encoder. The process involves image patching, flattening of patches, linear projection to create embeddings, addition of a class token, and finally, input to an encoder.

**Key Insights:**
1. **Image is tokenized into patches:** The 'Original image' is first divided into a grid of smaller pieces, referred to as 'Patched image', signifying that ViTs process images not as a whole, but as a sequence of distinct parts. This is evident from the transformation of the full cat image into nine individual segments. 2. **Patches are flattened and projected:** Each patch is then 'Flattened input' and passed through a 'Linear projection' layer, which converts these raw pixel values into vector representations (embeddings). This step highlights the transformation of visual information into a numerical format suitable for machine learning models. 3. **Class token is added for overall representation:** A special '[CLASS]' token is prepended to the 'Patch embeddings'. This token is often used in Transformer models to aggregate information from all other tokens (patches) for downstream tasks like classification, as stated in the context of ViTs treating inputs like textual tokens. 4. **Output fed to Encoder:** The combined 'Patch embeddings' (including the class token) are then fed into an 'ENCODER', which is the core component of the Transformer architecture responsible for learning relationships between these patches. This indicates the start of the deep learning processing where the model extracts features and context from the image patches.

**Document Context:**
This image directly supports the document's section on 'Transformers for Vision' by visually explaining 'The main algorithm behind ViT'. It provides a clear, step-by-step breakdown of how an image is converted into a format (patch embeddings) that a Transformer encoder can process, much like textual tokens, which is central to understanding ViT's operation as stated in the accompanying text.

**Summary:**
The image illustrates the core algorithm of a Vision Transformer (ViT), detailing how an original image is processed to become an input for a Transformer encoder. The process begins with an 'Original image' (depicting a cat), which is then divided into a grid of smaller, non-overlapping squares, resulting in a 'Patched image'. This 'Patched image' is then flattened; each individual patch is converted into a linear sequence of pixel values, represented by a series of rectangular segments labeled 'Flattened input'. A crucial step follows: these flattened patches are passed through a 'Linear projection' layer, which transforms each patch into a higher-dimensional vector. Alongside these projected patch vectors, a special '[CLASS]' token is prepended. These transformed vectors and the class token together form the 'Patch embeddings'. Finally, these 'Patch embeddings' are fed into an 'ENCODER', which processes them and outputs a sequence of processed embeddings, indicated by the downward arrows. This entire sequence demonstrates the transformation of visual data into a format suitable for a Transformer architecture, treating image patches similarly to how words (tokens) are handled in natural language processing.](images/9db92eb5b04c517046fa926456e2bee5bf4d2bf51b716717cff95c95adea146f.jpg)
Figure 9-5. The main algorithm behind ViT. After patching the images and linearly projecting them, the patch embeddings are passed to the encoder and treated as if they were textual tokens.

# Multimodal Embedding Models

In previous chapters, we used embedding models to capture the semantic content of textual representations, such as papers and documents. We saw that we could use these embeddings or numerical representations to find similar documents, apply classification tasks, and even perform topic modeling.

As we have seen many times before, embeddings often are an important driver behind LLM applications. They are an efficient method for capturing large-scale information and searching for the needle in the haystack of information.

That said, we have looked at text-only embedding models thus far, which focus on generating embeddings for textual representations. Although embedding models exist for solely embedding imagery, we will look at embedding models that can capture both textual as well as visual representations. We illustrate this in Figure 9-6.

![## Image Analysis: e98dc77b9c0b561bd8fb0e945b6cd257ca8cc5f85f67d5066247279e299f1251.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and operational flow of a multimodal embedding model. Its main purpose is to demonstrate how such a model takes inputs from different data modalities, specifically text and images, and transforms them into a unified, shared vector space. The key idea communicated is the creation of dense numerical representations (embeddings) for both text and images that are semantically comparable, allowing for cross-modal understanding and tasks.

**Content Interpretation:**
The image illustrates the core functionality of a multimodal embedding model. It demonstrates how such a model takes inputs from different modalities—specifically text and an image—and converts them into numerical vector representations, known as embeddings. The key concept shown is that these distinct modalities are transformed into a common, shared embedding space, implying that the 'Sentence embedding' and 'Image embedding' can be compared or related to each other because they reside in the same conceptual space. The model acts as a bridge, understanding and representing information from both text and vision.

**Key Insights:**
The main takeaway from this image is the concept of a shared embedding space for different data modalities. It clearly shows that a 'Multimodal embedding model' is capable of processing distinct inputs such as a 'Sentence' (e.g., 'This is a cat') and an 'Image' (a cat), and generating corresponding 'Sentence embedding' and 'Image embedding' outputs. The visual representation of these embeddings as series of squares highlights that they are numerical vectors. This implies that once embedded, information from different modalities can be directly compared or operated upon within the same mathematical space, enabling multimodal tasks. The model's ability to unify these representations is the crucial insight provided by the diagram.

**Document Context:**
This image directly supports the document's section on 'Multimodal Embedding Models' and the accompanying text, 'Figure 9-6. Multimodal embedding models can create embeddings for multiple modalities in the same vector space.' It visually explains the process by which a single model can handle diverse input types (text and images) and map them into a shared embedding space, which is fundamental to understanding how these models facilitate cross-modal understanding and tasks like image captioning or visual question answering. It concretely illustrates the abstract concept introduced in the text.

**Summary:**
The diagram illustrates the process of how a multimodal embedding model generates embeddings for different data types (modalities) within a unified vector space. It begins with two distinct inputs: a textual sentence, represented by a box containing the verbatim text “This is a cat” and labeled “Sentence”, and a visual input, depicted as an image of a cat and labeled “Image”. Both these inputs are fed into a central processing unit, a light blue rectangular box labeled “Multimodal embedding model”. This model processes both the textual and visual information. As outputs, the model produces two distinct embeddings: a “Sentence embedding”, represented by a blue rounded rectangle with this label, followed by a series of three empty squares symbolizing a vector, and an “Image embedding”, represented by a purple rounded rectangle with this label, also followed by a series of three empty squares symbolizing another vector. The ellipses (...) preceding both the sentence and the image inputs suggest that these are examples within a continuous stream of data.](images/e98dc77b9c0b561bd8fb0e945b6cd257ca8cc5f85f67d5066247279e299f1251.jpg)
Figure 9-6. Multimodal embedding models can create embeddings for multiple modali‐ ties in the same vector space.

An advantage is that this allows for comparing multimodal representations since the resulting embeddings lie in the same vector space (Figure 9-7). For instance, using such a multimodal embedding model, we can find images based on input text. What images would we find if we search for images similar to “pictures of a puppy”? Vice versa would also be possible. Which documents are best related to this question?

![## Image Analysis: 167baa9faf32c88f27f2fe360a2c0df430a9a100d1dcf1319af1eca690ca4d11.jpg

**Conceptual Understanding:**
This image conceptually illustrates the principle of multimodal embeddings within a vector space. Its main purpose is to demonstrate that different data types (modalities), such as images and text, when processed by a multimodal embedding model, are represented in a way that their semantic similarity is reflected by their spatial proximity in a multi-dimensional (here, 2D) space. The key idea being communicated is that content with similar meaning, irrespective of its original form, will be located close to each other in this shared embedding space. This allows for a unified understanding and comparison of diverse information sources based on their underlying meaning rather than their format.

**Content Interpretation:**
The image demonstrates the functioning of multimodal embedding models by illustrating how different modalities (images and text) are projected into a shared vector space. The primary concept shown is that items with similar semantic meanings are grouped closely together.

Specifically, the image shows:
*   **Embedding of Images and Text:** Images of a car, a puppy, and a cat, along with various text descriptions and sentiments, are implicitly converted into embeddings and plotted on a 2D grid.
*   **Clustering by Semantic Similarity:** Distinct clusters are formed where an image and its related textual descriptions or concepts are co-located. For example, the **car image** is spatially close to the text **"Car"** and **"Road"**, indicating a strong semantic link. Similarly, the **puppy image** is close to **"Snowing"** and **"A puppy"**.
*   **Representation of Nuanced Relationships:** The **cat image** is positioned near both **"My cat is cute"** and **"I don't like cats"**. This is significant because it shows that even opposing sentiments related to the same subject are understood to be semantically connected to that subject and to each other, forming a coherent cluster despite their contrasting nature. This suggests the model captures a broader conceptual understanding of 'cats' that includes diverse opinions.

All extracted text elements serve as direct evidence for these interpretations. The close physical proximity of text labels like "Car" to the car image, "A puppy" to the puppy image, and both "My cat is cute" and "I don't like cats" to the cat image visually confirms the model's ability to align and group semantically similar content across different modalities.

**Key Insights:**
The image provides several key takeaways and insights into multimodal embedding models:

1.  **Modality Independence for Semantic Understanding:** The primary lesson is that these models can effectively bridge different data modalities (images and text) to achieve a unified semantic understanding. The visual evidence shows the **car image** clustered with the text **"Car"** and **"Road"**, and the **puppy image** with **"Snowing"** and **"A puppy"**, directly demonstrating that the model perceives them as semantically equivalent or highly related, regardless of their input format.

2.  **Spatial Proximity as a Measure of Semantic Similarity:** The arrangement clearly indicates that closeness in the vector space corresponds to conceptual similarity. This implies that if a model is queried with a text like "Car," it could effectively retrieve the corresponding image, and vice-versa, due to their proximity.

3.  **Capacity to Encode Complex Relationships and Sentiments:** The most nuanced insight comes from the **cat image** being associated with both **"My cat is cute"** and **"I don't like cats"**. This suggests that multimodal embeddings can capture not just direct descriptions but also more abstract concepts like sentiment and differing opinions related to a subject. The fact that these opposing sentiments are still grouped near the cat image and near each other demonstrates a sophisticated understanding of the semantic field around 'cats' that encompasses a range of perspectives.

In conclusion, the image illustrates that multimodal embedding models are powerful tools for cross-modal information retrieval and understanding, capable of organizing diverse data into a coherent, semantically meaningful space, even for subjective or contrasting concepts.

**Document Context:**
This image is highly relevant to the document's section on "Multimodal Embedding Models." It serves as a visual explanation of the core principle stated in the text immediately following the figure: "Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space." The image concretely illustrates this abstract concept, making it accessible and understandable for the reader. It reinforces the idea that these models bridge the gap between different data types (like visual and textual information) by representing them in a common, semantically ordered space. By showing images and corresponding text phrases clustered together, it directly supports the document's argument about the effectiveness and underlying mechanism of multimodal embeddings.

**Summary:**
This image visually represents a two-dimensional vector space, depicted as a grid, to illustrate the core concept of multimodal embedding models. The fundamental principle shown is that items with similar semantic meanings, regardless of their original modality (such as an image or a text phrase), are positioned closely to each other in this conceptual space.

The diagram features three distinct clusters, each containing an image and associated text labels, connected by dashed lines to small and large green circles, indicating their relationships:

1.  **Vehicle Cluster (Top-Left):** This cluster centers around an image of a **yellow sports car** on a road at sunset. Connected to this image are two text labels: **"Car"** and **"Road"**. The text "Car" is very close to the image, signifying a direct and strong semantic link. The text "Road" is also nearby, indicating a related contextual element.

2.  **Puppy and Environment Cluster (Top-Right):** This cluster features an image of a **puppy wearing a snow hat** in a snowy setting. Associated with this image are the text labels **"Snowing"** and **"A puppy"**. The phrase "A puppy" is positioned directly below the image, serving as a direct description. The word "Snowing" is to the left of the image, indicating a related environmental condition.

3.  **Cat and Sentiment Cluster (Bottom-Center/Right):** Centered around an image of a **cartoon-style ginger cat**, this cluster connects to two contrasting sentiment phrases. The text **"My cat is cute"** is located to the bottom-left of the cat image, expressing a positive sentiment. Further to the right, but still within the same conceptual vicinity, is the phrase **"I don't like cats"**. This grouping demonstrates that even opposing sentiments about the same subject are considered semantically related and are clustered together, albeit with some spatial differentiation reflecting their specific meaning.

Overall, the image clearly and comprehensively illustrates how multimodal embeddings create a unified semantic space where the proximity of elements directly corresponds to their conceptual similarity, allowing for coherent cross-modal understanding.](images/167baa9faf32c88f27f2fe360a2c0df430a9a100d1dcf1319af1eca690ca4d11.jpg)
Figure 9-7. Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space.

There are a number of multimodal embedding models, but the most well-known and currently most-used model is Contrastive Language-Image Pre-training (CLIP).

# CLIP: Connecting Text and Images

CLIP is an embedding model that can compute embeddings of both images and texts. The resulting embeddings lie in the same vector space, which means that the embed‐ dings of images can be compared with the embeddings of text.3 This comparison capability makes CLIP, and similar models, usable for tasks such as:

Zero-shot classification

We can compare the embedding of an image with that of the description of its possible classes to find which class is most similar.

Clustering

Cluster both images and a collection of keywords to find which keywords belong to which sets of images.

# Search

Across billions of texts or images, we can quickly find what relates to an input text or image.

# Generation

Use multimodal embeddings to drive the generation of images (e.g., stable diffusion4).

# How Can CLIP Generate Multimodal Embeddings?

The procedure of CLIP is actually quite straightforward. Imagine that you have a dataset with millions of images alongside captions as we illustrate in Figure 9-8.

![## Image Analysis: 35712f27ced4e39109355f24117339c80fad096b6f286d6b9b8cd1530aab0bba.jpg

**Conceptual Understanding:**
This image conceptually represents multimodal data, specifically the pairing of visual information (images) with textual descriptions (captions). The main purpose of the image is to illustrate the fundamental data structure used for training multimodal embedding models, as implied by the surrounding document context. It communicates the key idea that such models learn by associating an image with its natural language explanation, thereby bridging the gap between vision and language. Each example explicitly shows an image labeled "Image" and its corresponding text labeled "Caption", highlighting this direct association.

**Content Interpretation:**
The image presents three distinct pairs of visual and textual data, which are fundamental components for training multimodal embedding models. Each pair consists of an "Image" (a visual representation) and a corresponding "Caption" (a natural language description of that image). The first pair shows a pixelated image of a cat with the caption "A pixelated image of a cute cat". The second pair features an image of a puppy in the snow with the caption "A puppy playing in the snow". The third pair displays an image of a supercar at sunset with the caption "A supercar on the road with the sunset in the background". These pairings demonstrate the concept of multimodal data where different modalities (vision and language) are semantically linked, allowing a model to learn the correspondence between visual features and their linguistic descriptions. The images are diverse in style (pixelated art vs. photography) and content (animals, vehicles, environments) yet each is accurately described by its caption, illustrating the variety of data used in such training.

**Key Insights:**
The main takeaway from this image is that multimodal embedding models, particularly those that integrate visual and linguistic understanding, are trained using paired data consisting of images and their corresponding textual descriptions. The image illustrates that the textual description accurately captures the content of the image, even for diverse image types like a "pixelated image of a cute cat" or "A supercar on the road with the sunset in the background". This pairing allows the model to learn the semantic relationship between visual elements and human language, enabling it to generate embeddings that represent both modalities in a shared space. The fidelity of the captions to the image content is key to this training process, as demonstrated by the precise descriptions provided for each visual example.

**Document Context:**
This image directly supports the document's section title "How Can CLIP Generate Multimodal Embeddings?" and the accompanying text, "The type of data that is needed to train a multimodal embedding model." It visually answers the question by illustrating the core data structure required: matched pairs of images and their descriptive captions. This pairing is crucial for models like CLIP (Contrastive Language-Image Pre-training) to learn to associate visual concepts with linguistic representations. By showing concrete examples of image-caption pairs, the image clarifies the input data format and the underlying principle of learning across different modalities, making the abstract concept of multimodal embeddings tangible for the reader.

**Summary:**
The image illustrates three distinct examples of multimodal data, specifically pairing an image with its descriptive text caption. On the left, a dotted purple line labeled "Image" vertically aligns with a row of three distinct images: a pixelated cat, a puppy in the snow, and a supercar on the road. Below this, a dotted blue line labeled "Caption" vertically aligns with the corresponding textual descriptions for each image. The first image depicts a pixelated orange and white cat with bright blue eyes, and its associated caption reads: "A pixelated image of a cute cat". The second image shows a black, tan, and white puppy wearing a white patch of snow on its head, sitting in a snowy environment with trees in the background, and its caption states: "A puppy playing in the snow". The third image displays a golden supercar driving on a road with a dramatic sunset visible in the background over distant hills, and its caption is: "A supercar on the road with the sunset in the background". The overall arrangement clearly demonstrates how visual content (images) is directly paired with accurate, descriptive textual information (captions).](images/35712f27ced4e39109355f24117339c80fad096b6f286d6b9b8cd1530aab0bba.jpg)
Figure 9-8. The type of data that is needed to train a multimodal embedding model.

This dataset can be used to create two representations for each pair, the image and its caption. To do so, CLIP uses a text encoder to embed text and an image encoder to embed images. As is shown in Figure 9-9, the result is an embedding for both the image and its corresponding caption.

![## Image Analysis: ca9d8c416252bbdae4b786e901b5f3f70300bebd4f9ce72af8d3c4743e01af26.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental input-to-embedding transformation within a multimodal machine learning model, specifically CLIP. Its main purpose is to illustrate how text and image data are converted into a shared, high-dimensional vector space (embeddings) using specialized encoders. The core idea communicated is that distinct input types (text and image) are processed in parallel by their respective encoders to generate numerical representations (embeddings) that are intended to be semantically linked or aligned, enabling a model to understand the relationship between text and visual content.

**Content Interpretation:**
The image illustrates the core mechanism for generating multimodal embeddings using separate, yet interacting, encoders for text and images. It demonstrates that a given text input (e.g., 'This is a cat') is transformed into a 'Sentence embedding' via a 'Text encoder', and a corresponding image input (e.g., an image of a cat) is transformed into an 'Image embedding' via an 'Image encoder (ViT)'. The explicit connection labeled '1 Embed input' signifies that these encoding processes are not independent but are jointly considered or aligned to produce embeddings that are semantically comparable across modalities. The use of 'ViT' for the image encoder specifies a particular architecture (Vision Transformer) commonly used in state-of-the-art image processing.

**Key Insights:**
The main takeaways from this image are: 1. CLIP uses distinct neural network architectures, a 'Text encoder' and an 'Image encoder (ViT)', to process different modalities (text and images). 2. Both encoders transform their respective inputs into 'embeddings', which are numerical representations. 3. The process of embedding inputs across modalities is interconnected ('Embed input'), implying an objective to learn a joint representation space where semantically similar text and images have similar embeddings. 4. The 'Image encoder' specifically utilizes a 'Vision Transformer (ViT)' architecture, highlighting a modern approach to image encoding. Evidence for these points comes directly from the labels: 'Text encoder', 'Image encoder (ViT)', 'Sentence embedding', 'Image embedding', and the connecting label '1 Embed input'.

**Document Context:**
This image directly supports the document's section titled 'How Can CLIP Generate Multimodal Embeddings?'. It visually explains the 'first step of training CLIP' as mentioned in the accompanying text, which states: 'In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively.' The diagram concretely shows how an example sentence and image are passed through their respective encoders to produce 'Sentence embedding' and 'Image embedding', making the abstract concept of multimodal embedding tangible. It sets the foundation for understanding how CLIP learns to associate text and images by creating aligned representations.

**Summary:**
This diagram illustrates the initial step of how CLIP (Contrastive Language-Image Pre-training) generates multimodal embeddings. The process involves two parallel encoding pathways: one for text and one for images. A 'Sentence' input, specifically the text 'This is a cat', is fed into a 'Text encoder'. This encoder processes the text and outputs a 'Sentence embedding', which is represented by a series of three empty square boxes, signifying a numerical vector. Simultaneously, an 'Image' input, depicted as a cat illustration, is fed into an 'Image encoder (ViT)'. The '(ViT)' specifies that the Image encoder is a Vision Transformer. This image encoder processes the image and generates an 'Image embedding', also represented by a series of three empty square boxes, denoting another numerical vector. A crucial connection exists between the 'Text encoder' and 'Image encoder (ViT)', indicated by a double-headed vertical arrow labeled with a circled '1' and the text 'Embed input'. This signifies that the embedding process for both modalities is interconnected, likely for the purpose of learning a shared or aligned embedding space, as suggested by the concept of multimodal embeddings. The small arrow-like icon in the top right corner of both 'Text encoder' and 'Image encoder (ViT)' boxes may represent an output or expansion capability of these encoders. The ellipses '...' on the input side for both the 'Sentence' and 'Image' suggest that these inputs might originate from a larger context or prior processing steps.](images/ca9d8c416252bbdae4b786e901b5f3f70300bebd4f9ce72af8d3c4743e01af26.jpg)
Figure 9-9. In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively.

The pair of embeddings that are generated are compared through cosine similarity. As we saw in Chapter 4, cosine similarity is the cosine of the angle between vectors, which is calculated through the dot product of the embeddings and divided by the product of their lengths.

When we start training, the similarity between the image embedding and text embed‐ ding will be low as they are not yet optimized to be within the same vector space. During training, we optimize for the similarity between the embeddings and want to maximize them for similar image/caption pairs and minimize them for dissimilar image/caption pairs (Figure 9-10).

After calculating their similarity, the model is updated and the process starts again with new batches of data and updated representations (Figure 9-11). This method is called contrastive learning, and we will go in depth into its inner workings in Chapter 10 where we will create our own embedding model.

![## Image Analysis: 8637a103827942b551b5c714d2e4d852c460b718a2851cddf47d98f1d44e9387.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of generating and comparing multimodal embeddings, specifically for text and image data. Its main purpose is to explain the fundamental architecture and workflow of a system (such as CLIP) that learns to understand the relationship between different types of data, by transforming them into a shared embedding space. The key ideas being communicated are: the independent encoding of text and images, the creation of their respective numerical embeddings, and the subsequent comparison of these embeddings to determine their semantic similarity or dissimilarity. Ultimately, it shows how a model makes a 'Prediction' about this relationship and how it is evaluated against a 'Label' representing the true relationship.

**Content Interpretation:**
This image illustrates a core process in multimodal machine learning, specifically how a model like CLIP generates and compares embeddings from different modalities (text and image). It shows the transformation of raw input data (a sentence and an image) into a unified numerical representation called embeddings using specialized encoders. The subsequent step involves comparing these embeddings to determine the semantic similarity or dissimilarity between the original text and image. The output of this comparison results in a 'Prediction' (model's output) which is then contrasted with a 'Label' (the ground truth or expected outcome). The process highlights the sequential steps of '1 Embed input' where raw data is converted into embeddings, and '2 Compare embeddings' where these numerical representations are analyzed for similarity. The 'Prediction' and 'Label' sections, with their '1 Similar' and '0 Dissimilar' states, demonstrate a binary classification task based on this similarity.

**Key Insights:**
The main takeaways from this image are: 1. Multimodal models like CLIP utilize separate, specialized encoders for different data types (e.g., 'Text encoder' for sentences, 'Image encoder (ViT)' for images) to process input. 2. These encoders transform diverse inputs into a common, comparable numerical format known as 'Sentence embedding' and 'Image embedding'. 3. The core mechanism for understanding the relationship between different modalities is the 'Compare embeddings' step, where the numerical representations are evaluated for similarity. 4. The process involves generating a model 'Prediction' (e.g., '1 Similar' or '0 Dissimilar') which is then validated against a ground truth 'Label' to assess accuracy or guide training. The exact text '1 Embed input', '2 Compare embeddings', 'Prediction', 'Label', '1 Similar', and '0 Dissimilar' directly supports these insights, illustrating the specific operations and outcomes involved in determining multimodal similarity.

**Document Context:**
This image is highly relevant to the document's section titled 'How Can CLIP Generate Multimodal Embeddings?' and specifically to the accompanying text that states, 'In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity.' The diagram visually breaks down this exact process. It explains how a sentence and an image are independently processed by respective encoders to create embeddings, and then how these embeddings are compared to determine their similarity. This direct visual explanation enhances the reader's understanding of the underlying mechanism described in the text, showing the inputs, the encoding steps, the embedding comparison, and the final classification into 'similar' or 'dissimilar' categories, which is crucial for training and evaluating multimodal models.

**Summary:**
The image illustrates the process of generating multimodal embeddings and comparing them, likely as part of a training or evaluation step for a model like CLIP. The workflow begins with two types of input: a 'Sentence' and an 'Image'. The 'Sentence' input is shown as 'This is a cat' and is fed into a 'Text encoder'. Simultaneously, an 'Image' (depicted as a cat) is fed into an 'Image encoder (ViT)'. Both encoders perform the first step, labeled '1 Embed input', transforming their respective inputs into numerical representations. The 'Text encoder' produces a 'Sentence embedding', represented by three gray boxes. The 'Image encoder (ViT)' produces an 'Image embedding', also represented by three gray boxes. The second step, labeled '2 Compare embeddings', involves comparing these generated 'Sentence embedding' and 'Image embedding' outputs. Following this comparison, the system generates a 'Prediction' which has two possible states: '1 Similar' (shown in a light gray box) or '0 Dissimilar' (shown in a light green box). This 'Prediction' is then compared against a 'Label' (ground truth), which also has two states: '1 Similar' (shown in a light green box) or '0 Dissimilar' (shown in a light gray box). The visual representation implies that when the prediction is '1 Similar' (light gray) and the label is '1 Similar' (light green), they match, and similarly for '0 Dissimilar'. The use of different background colors for '1 Similar' and '0 Dissimilar' in 'Prediction' versus 'Label' likely indicates correct and incorrect predictions or simply distinguishes the two categories.](images/8637a103827942b551b5c714d2e4d852c460b718a2851cddf47d98f1d44e9387.jpg)
Figure 9-10. In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity.

![## Image Analysis: 8b1e62fee2b4380c8ccfb6142c6d7a83b92f04c0eeacc597751954d3144d4c28.jpg

**Conceptual Understanding:**
This image conceptually represents the supervised training pipeline for the CLIP (Contrastive Language-Image Pre-training) model. Its main purpose is to illustrate how text and image inputs are converted into a shared, multimodal embedding space, and how the model learns to identify semantic similarities between these different modalities. The key idea communicated is the iterative process of encoding, comparing predictions with ground truth labels, and updating the model's encoders to reduce the distance between embeddings of semantically related text-image pairs and increase the distance for unrelated pairs, thereby enabling multimodal understanding.

**Content Interpretation:**
The image depicts the fundamental training mechanism of the CLIP model, specifically detailing how it learns to generate and refine multimodal embeddings. It illustrates a supervised learning process where a text input ('Sentence') and an image input ('Image') are processed by dedicated encoders ('Text encoder', 'Image encoder (ViT)') to produce respective 'Sentence embedding' and 'Image embedding'. These embeddings are then 'Compare[d]' to generate a 'Prediction' of similarity, which is contrasted against a ground truth 'Label'. The core idea is to train the encoders so that the 'Prediction' matches the 'Label', thus enabling the model to learn the semantic relationship between text and images in a shared embedding space. The process is iterative, with the 'Update model' step indicating backpropagation to adjust the encoder weights based on the prediction error.

**Key Insights:**
The main takeaway is that CLIP's training relies on a feedback loop where text and image inputs are independently embedded, their similarity is predicted, and then compared against a true label. Any discrepancy triggers an update to the 'Text encoder' and 'Image encoder (ViT)', refining their ability to produce semantically meaningful embeddings. The image teaches that the goal is to make 'Sentence embedding' and 'Image embedding' for related content closer in the embedding space. The specific text '1 Embed input', '2 Compare embeddings', and '3 Update model' clearly delineate these core training phases. The 'Prediction' and 'Label' tables, with '1 Similar' and '0 Dissimilar' options, show the binary classification task and how the model's output is evaluated against ground truth for learning.

**Document Context:**
This image directly supports the document's section 'How Can CLIP Generate Multimodal Embeddings?' by providing a visual, step-by-step explanation of the training process. The preceding text establishes the context of generating multimodal embeddings, and this figure precisely illustrates the 'third step of training CLIP' as described in the accompanying text, showing how 'the text and image encoders are updated to match what the intended similarity should be' to make embeddings 'closer in vector space if the inputs are similar.' It visually clarifies the abstract concepts of encoders, embeddings, similarity comparison, and model updates in a training loop.

**Summary:**
The image illustrates the three main steps involved in training CLIP (Contrastive Language-Image Pre-training) to generate multimodal embeddings. The process begins with taking a sentence and an image as input. In the first step, labeled '1 Embed input', both the sentence 'This is a cat' and the corresponding image of a cat are fed into their respective encoders. The 'Text encoder' processes the sentence to produce a 'Sentence embedding', represented by three grey squares. Simultaneously, the 'Image encoder (ViT)' processes the image to generate an 'Image embedding', also represented by three grey squares. These embeddings are vector representations of the inputs in a shared latent space. The second step, '2 Compare embeddings', involves comparing these two generated embeddings. This comparison leads to a 'Prediction' of their similarity, which is presented in a table format. The 'Prediction' table has two options: '1 Similar' and '0 Dissimilar'. In the depicted scenario, the model predicts '0 Dissimilar' (highlighted in green). This prediction is then evaluated against the ground truth 'Label' table, which also has options '1 Similar' (highlighted in green) and '0 Dissimilar'. In this specific example, the label indicates '1 Similar', suggesting a mismatch between the model's prediction and the actual relationship between the input sentence and image. The third step, '3 Update model', involves a feedback loop where the comparison between the 'Prediction' and the 'Label' is used to update both the 'Text encoder' and the 'Image encoder (ViT)'. This update aims to adjust the encoders' parameters so that for similar input pairs, their respective embeddings become closer in the vector space, and the model's 'Prediction' aligns more accurately with the true 'Label'. The overall flow highlights how CLIP learns to map text and images into a consistent multimodal embedding space through a process of encoding, comparing, and iteratively updating its encoders based on similarity judgments.](images/8b1e62fee2b4380c8ccfb6142c6d7a83b92f04c0eeacc597751954d3144d4c28.jpg)
Figure 9-11. In the third step of training CLIP, the text and image encoders are updated to match what the intended similarity should be. This updates the embeddings such that they are closer in vector space if the inputs are similar.

Eventually, we expect the embedding of an image of a cat would be similar to the embedding of the phrase “a picture of a cat.” As we will see in Chapter 10, to make sure the representations are as accurate as possible, negative examples of images and captions that are not related should also be included in the training process. Modeling similarity is not only knowing what makes things similar to one another, but also what makes them different and dissimilar.

# OpenCLIP

For our next example, we are going to be using models from the open source variant of CLIP, namely OpenCLIP. Using OpenCLIP, or any CLIP model, boils down to two things: processing the textual and image inputs before passing them to the main model.

Before doing so, let’s take a look at a small example where we will be using one of the images we have seen before, namely, an AI-generated image (through stable diffusion) of a puppy playing in the snow, as illustrated in Figure 9-12:

from urllib.request import urlopen from PIL import Image

# Load an AI-generated image of a puppy playing in the snow   
puppy_path $=$ "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large  
Language-Models/main/chapter09/images/puppy.png"   
image $=$ Image.open(urlopen(puppy_path)).convert("RGB")

caption $=$ "a puppy playing in the snow"

![## Image Analysis: 63199d0510623f89a7d11a556ddb5a15ef1e1f88c3a84a0732304eb9feb35e5a.jpg

**Conceptual Understanding:**
This image conceptually represents an 'AI-generated image of a puppy playing in the snow.' Its main purpose is to serve as a visual illustration, likely demonstrating the output or capabilities of an Artificial Intelligence system in creating specific visual content, as suggested by the accompanying document context. The image conveys a sense of cuteness and winter imagery through the depiction of the puppy and its snowy environment.

**Content Interpretation:**
The image exclusively presents a visual depiction of a small puppy in a snowy landscape. There are no processes, concepts, relationships, or systems conveyed through diagrams, charts, or textual elements within the image itself. The content is purely illustrative, focusing on the aesthetic appeal of a puppy playing in the snow. The significance lies in its visual charm and its role as an AI-generated image, as per the document context. There are no data, trends, or information presented beyond the visual subject.

**Key Insights:**
The main takeaway from this image, when considered with the document context, is the capability of Artificial Intelligence to generate realistic and aesthetically pleasing visual content, such as a cute puppy in a specific setting. The image itself does not contain any embedded textual evidence to support further complex insights, but its mere existence as an AI-generated image is the primary insight. The image visually conveys innocence, warmth (despite the snow), and the joy associated with puppies.

**Document Context:**
This image serves as a visual example of an AI-generated image, specifically of 'a puppy playing in the snow,' as indicated by the document context 'Section: Load an AI-generated image of a puppy playing in the snow' and 'Text after image: Figure 9-12. An AI-generated image of a puppy playing in the snow.' Its purpose within the document is to demonstrate the output or capability of an AI in generating specific visual content, illustrating the topic discussed in the accompanying text.

**Summary:**
The image displays a head-on view of a small, fluffy puppy sitting in a snow-covered environment. The puppy has predominantly black fur with distinct tan markings on its cheeks, eyebrows, and front legs, and white markings on its chest and between its eyes. A small, round mound of white snow rests perfectly on top of its head, resembling a cap. The puppy's eyes are large, dark, and round, giving it an endearing expression as it looks directly at the viewer. Its ears are floppy and dark, framing its face. The background is a blurred winter scene with a vast expanse of white snow covering the ground, and dark, bare trees visible in the distance, suggesting a forest or park setting. To the upper right, a snow-covered wooden bench is faintly visible. The overall impression is one of cuteness and winter charm.](images/63199d0510623f89a7d11a556ddb5a15ef1e1f88c3a84a0732304eb9feb35e5a.jpg)
Figure 9-12. An AI-generated image of a puppy playing in the snow.

Since we have a caption for this image, we can use OpenCLIP to generate embeddings for both.

To do so, we load in three models:

• A tokenizer for tokenizing the textual input • A preprocessor to preprocess and resize the image • The main model that converts the previous outputs to embeddings from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel model_id $=$ "openai/clip-vit-base-patch32"

# Load a tokenizer to preprocess the text clip_tokenizer $=$ CLIPTokenizerFast.from_pretrained(model_id)

# Load a processor to preprocess the images clip_processor $=$ CLIPProcessor.from_pretrained(model_id)

# Main model for generating text and image embeddings model $=$ CLIPModel.from_pretrained(model_id)

After having loaded in the models, preprocessing our input is straightforward. Let’s start with the tokenizer and see what happens if we preprocess our input:

# Tokenize our input   
inputs $=$ clip_tokenizer(caption, return_tensors $=$ "pt")   
inputs

This outputs a dictionary that contains the IDs of the input:

{'input_ids': tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}

To see what those IDs represent, we can convert them to tokens using the aptly named convert_ids_to_tokens function:

# Convert our input back to tokens clip_tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

This gives us the following output:

['<|startoftext|>',   
'a</w>',   
'puppy</w>',   
'playing</w>',   
'in</w>',   
'the</w>',   
'snow</w>',   
'<|endoftext|>']

As we often have seen before, the text is split up into tokens. Additionally, we now also see that the start and end of the text is indicated to separate it from a potential image embedding. You might also notice that the [CLS] token is missing. In CLIP, the [CLS] token is actually used to represent the image embedding.

Now that we have preprocessed our caption, we can create the embedding:

# Create a text embedding   
text_embedding $=$ model.get_text_features(\*\*inputs)   
text_embedding.shape

This results in an embedding that has 512 values for this single string:

torch.Size([1, 512])

Before we can create our image embedding, like the text embedding, we will need to preprocess it as the model expects the input image to have certain characteristics, like its size and shape.

To do so, we can use the processor that we created before:

# Preprocess image   
processed_image $=$ clip_processor( text=None, images $\mathbf { \Psi } =$ image, return_tensors $: =$ "pt"   
)["pixel_values"]

processed_image.shape

The original image was $5 1 2 \times 5 1 2$ pixels. Notice that the preprocessing of this image reduced its size to $2 2 4 \times 2 2 4$ pixels as that is its expected size:

torch.Size([1, 3, 224, 224])

Let’s visualize the results of this preprocessing as shown in Figure 9-13:

import torch   
import numpy as np   
import matplotlib.pyplot as plt   
# Prepare image for visualization   
img $=$ processed_image.squeeze(0)   
img $=$ img.permute(\*torch.arange(img.ndim - 1, -1, -1))   
img $=$ np.einsum("ijk->jik", img)   
# Visualize preprocessed image   
plt.imshow(img)   
plt.axis("off")

![## Image Analysis: a88cee8fb10bbef79dd611379eaf97a93d9950c1646f83a664f317870f56d6db.jpg

**Conceptual Understanding:**
This image represents a photographic example of a 'preprocessed input image' specifically for the CLIP (Contrastive Language-Image Pre-training) model. Its main purpose is to visually demonstrate the kind of image data that is fed into the CLIP system after undergoing initial preparation steps. The image conveys the idea that the model can process natural, real-world visual scenes, exemplified by a cute puppy in the snow, as its input. The absence of text within the image itself implies that the preprocessing focuses on visual attributes rather than textual content embedded in the picture's scene.

**Content Interpretation:**
This image visually presents an example of an input image that has undergone preprocessing by CLIP (Contrastive Language-Image Pre-training). The content shown is a photograph of a puppy in a snowy setting. The significance lies in illustrating what a 'preprocessed input image' looks like in the context of a machine learning model like CLIP. The visual details, such as the puppy's features, the snow, and the background elements (trees, bench), represent the raw visual information that would be encoded and analyzed by the CLIP model. The act of it being 'preprocessed' suggests that it has already gone through initial transformations (e.g., resizing, normalization) to be compatible with the model's input requirements.

**Key Insights:**
The main takeaway from this image, in conjunction with its caption, is understanding the nature of 'preprocessed input images' for advanced machine learning models like CLIP. It illustrates that such images are everyday photographs, and 'preprocessing' makes them suitable for model consumption. The visual evidence of a clear, albeit slightly desaturated, photograph reinforces that the input to CLIP is a recognizable image, not abstract data. It implicitly suggests that CLIP is capable of understanding and processing diverse, real-world visual content. No specific textual evidence is extracted from the image itself as it contains no text. The knowledge is derived from the visual content and the provided external document context and caption stating its role as a 'preprocessed input image by CLIP' for 'Figure 9-13.'

**Document Context:**
This image serves as a direct visual example for the document's section titled "Visualize preprocessed image," explicitly referenced by the text "Figure 9-13. The preprocessed input image by CLIP." It demonstrates a concrete instance of an image that has been prepared for processing by the CLIP model. Its inclusion is crucial for readers to understand the type of visual data that is fed into the model, thereby clarifying the practical application of preprocessing steps discussed in the document. The image contextually supports the technical explanation of CLIP by providing a relatable, real-world example of its input.

**Summary:**
The image displays a head-on, eye-level view of a small, fluffy puppy sitting in a snow-covered environment. The puppy has predominantly black fur on its head, back, and ears, with distinct tan or light brown markings on its cheeks, above its eyes, and on its front legs. A prominent white patch of fur is visible on its chest, extending upwards towards its chin, forming a star-like or cross-like shape. Another smaller white patch is visible on its forehead, between its eyes. The puppy's ears are dark, floppy, and frame its face. Its eyes are dark and appear to be looking directly at the viewer. A small amount of white snow rests on top of the puppy's head, resembling a small hat. The background is mostly bright white snow, indicating a wide-open snowy field or yard. In the upper background, dark, silhouetted trees with what appears to be snow on their branches are visible, suggesting a forest line. To the upper right, a snow-covered bench or similar outdoor furniture is vaguely discernible against the dark trees. The overall lighting is somewhat dim, giving the snow a slightly blueish tint in some areas, while the puppy's fur has noticeable contrast.](images/a88cee8fb10bbef79dd611379eaf97a93d9950c1646f83a664f317870f56d6db.jpg)
Figure 9-13. The preprocessed input image by CLIP.

To convert this preprocessed image into embeddings, we can call the model as we did before and explore what shape it returns:

# Create the image embedding   
image_embedding $=$ model.get_image_features(processed_image)   
image_embedding.shape

This gives us the following shape:

torch.Size([1, 512])

Notice that the shape of the resulting image embedding is the same as that of the text embedding. This is important as it allows us to compare their embeddings and see if they are similar.

We can use these embeddings to calculate how similar they are. To do so, we normal‐ ize the embeddings first before calculating the dot product to give us a similarity score:

# Normalize the embeddings text_embedding $/ =$ text_embedding.norm(dim=-1, keepdim=True) image_embedding $/ =$ image_embedding.norm(dim=-1, keepdim=True)

# Calculate their similarity   
text_embedding $=$ text_embedding.detach().cpu().numpy()   
image_embedding $=$ image_embedding.detach().cpu().numpy()   
score $=$ np.dot(text_embedding, image_embedding.T)   
score

This gives us the following score:

array([[0.33149648]], dtype=float32)

We get a similarity score of 0.33, which is difficult to interpret considering we don’t know what the model considers a low versus a high similarity score. Instead, let’s extend the example with more images and captions as illustrated in Figure 9-14.

<table><tr><td rowspan="4">A puppy playing in the snow A pixelated image of a cute cat</td><td></td><td></td><td></td></tr><tr><td>0.33</td><td>0.19</td><td>0.11</td></tr><tr><td>0.15</td><td>0.35</td><td>0.09</td></tr><tr><td>A supercar on the road with 0.08 the sunset in the background</td><td>0.13</td><td>0.31</td></tr></table>

It seems that a score of 0.33 is indeed high considering the similarities with other images are quite a bit lower.

# Using sentence-transformers to Load CLIP

sentence-transformers implements a few CLIP-based models that make it much easier to create embeddings. It only takes a few lines of code:

from sentence_transformers import SentenceTransformer, util # Load SBERT-compatible CLIP model model $=$ SentenceTransformer("clip-ViT-B-32")

# Encode the images image_embeddings $=$ model.encode(images)

# Encode the captions text_embeddings $=$ model.encode(captions)

#Compute cosine similarities sim_matrix $=$ util.cos_sim( image_embeddings, text_embeddings )

# Making Text Generation Models Multimodal

Traditionally, text generation models have been, as you might expect, models that interpret textual representations. Models like Llama 2 and ChatGPT excel at reason‐ ing about textual information and responding with natural language.

They are, however, limited to the modality they were trained in, namely text. As we have seen before with multimodal embedding models, the addition of vision can enhance the capabilities of a model.

In the case of text generation models, we would like it to reason about certain input images. For example, we could give it an image of a pizza and ask it what ingredients it contains. You could show it a picture of the Eiffel Tower and ask when it was built or where it is located. This conversational ability is further illustrated in Figure 9-15.

![## Image Analysis: cfdb037a3af13fdd2453beacfd8ecdae9f59f7288a69592ba8e2e9895da72e15.jpg

**Conceptual Understanding:**
This image conceptually represents an interactive dialogue demonstrating the capabilities of a multimodal text generation model, specifically its ability to interpret visual input and engage in subsequent reasoning. The main purpose is to illustrate how such models can process an image, describe its content, and then answer more complex, inferential questions about that content, effectively showcasing "reasoning about input images." The key ideas being communicated are multimodal understanding (combining visual and text processing), visual question answering, and higher-level cognitive reasoning within AI models.

**Content Interpretation:**
The image portrays a human-AI interaction that highlights several key processes and concepts: Multimodal Input Processing: The system takes an image (a "sports car driving on the road at sunset") as input, demonstrating its ability to process visual data. The user's first query, "Write down what you see in this picture," directly initiates this visual processing. Image Description/Captioning: The AI model's first response, "A sports car driving on the road at sunset," shows its capability to accurately describe the content of an image, translating visual information into natural language. Visual Question Answering (VQA) with Reasoning: The interaction extends beyond simple description. The user's second question, "What would it take to drive such a car?", requires the model to not just identify objects but to reason about their implications, costs, or prerequisites. Inferential Reasoning: The AI model's second response, "A lot of money and time," is not a direct visual observation but an inference based on general knowledge associated with "sports cars." This demonstrates a form of practical reasoning or common-sense inference. Conversational Capability: The sequential nature of the questions and answers illustrates the model's ability to engage in a multi-turn dialogue, where subsequent responses build upon previous context. All extracted text elements ("Write down what you see in this picture.", "A sports car driving on the road at sunset", "What would it take to drive such a car?", "A lot of money and time") directly support these interpretations by showing a clear progression from visual identification to inferential reasoning within the dialogue. The systematic process mapping is as follows: Start Point: The process begins with the presentation of an image (a sports car on the road at sunset) to the system. Step 1: User's Initial Query (Image Description) - The user initiates the interaction by providing a textual prompt related to the image: "Write down what you see in this picture." This step serves as a request for the model to describe the visual content of the provided image. Step 2: AI Model's Descriptive Response - The AI model processes the image and the user's request. - The model generates a descriptive textual response: "A sports car driving on the road at sunset." This completes the first turn of the dialogue, showcasing the model's image comprehension and captioning capability. Step 3: User's Follow-up Query (Reasoning) - The user then asks a follow-up question that requires higher-level reasoning beyond simple description: "What would it take to drive such a car?" This question implies an understanding of the object (a sports car) and prompts the model to infer characteristics or requirements associated with it. Step 4: AI Model's Reasoning-based Response - The AI model processes the follow-up question, likely leveraging its understanding of the sports car from the previous turn and general world knowledge. - The model provides a reasoning-based answer: "A lot of money and time." This response concludes the interaction, demonstrating the model's ability to reason and provide relevant information beyond mere visual identification. End Point: The interaction concludes after the AI model provides its reasoning-based response.

**Key Insights:**
The main takeaways from this image are: Multimodal AI models can accurately describe visual content. The AI's response "A sports car driving on the road at sunset" to the question "Write down what you see in this picture" provides direct evidence that the model can interpret an image and generate a coherent textual description. These models possess capabilities for higher-level reasoning and inference about visual input. The user's follow-up question, "What would it take to drive such a car?", probes beyond mere description. The AI's answer, "A lot of money and time," is not visually derivable but requires general knowledge and reasoning about the implications of owning/driving a sports car, thus demonstrating inferential capabilities. Multimodal models can engage in conversational question answering that integrates visual understanding. The sequential, responsive nature of the dialogue, where the second question builds on the implicitly understood subject of the first, shows that these models can maintain context and participate in a more natural, multi-turn interaction. These specific text elements collectively provide strong evidence that the illustrated model (BLIP-2, as per the document context) is capable of both descriptive and reasoning tasks when presented with multimodal input.

**Document Context:**
This image perfectly fits within the document's narrative for the "Making Text Generation Models Multimodal" section. The accompanying text, "Figure 9-15. An example of a multimodal text generation model (BLIP-2) that can reason about input images," explicitly states its purpose. The figure visually demonstrates how a model like BLIP-2 bridges the gap between visual and textual modalities. It shows that these advanced models don't just generate text from text but can process an image, understand its content, and then engage in a text-based dialogue that requires reasoning about that visual information. This underscores the evolution of text generation models to incorporate richer, multimodal understanding, moving beyond simple image captioning to more complex cognitive tasks.

**Summary:**
This image illustrates an interaction with a multimodal text generation model, likely BLIP-2, demonstrating its ability to understand and reason about images. The interaction unfolds as a dialogue between a user and the AI model (represented by a robot icon). First, the user presents an image depicting an orange sports car driving on a road at sunset. The user then asks the model to describe the image by typing, "Write down what you see in this picture." In response, the AI model processes the image and provides an accurate textual description: "A sports car driving on the road at sunset." This shows the model's foundational capability in image captioning or visual description. Following this, the user poses a more complex, inferential question related to the image: "What would it take to drive such a car?" This question moves beyond simple observation and requires the AI to draw upon external knowledge and perform reasoning. The AI model successfully responds to this reasoning-based query with the answer: "A lot of money and time." This second response highlights the model's advanced capability to not only understand what is in an image but also to reason about the implications, characteristics, or associated concepts of the visual content. In essence, the image demonstrates a progression from basic visual identification to more sophisticated reasoning, showcasing how multimodal text generation models can engage in a meaningful, context-aware dialogue about visual inputs.](images/cfdb037a3af13fdd2453beacfd8ecdae9f59f7288a69592ba8e2e9895da72e15.jpg)
Figure 9-15. An example of a multimodal text generation model (BLIP-2) that can reason about input images.

To bridge the gap between these two domains, attempts have been made to introduce a form of multimodality to existing models. One such method is called BLIP-2: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understand‐ ing and Generation 2. BLIP-2 is an easy-to-use and modular technique that allows for introducing vision capabilities to existing language models.

# BLIP-2: Bridging the Modality Gap

Creating a multimodal language model from scratch requires significant computing power and data. We would have to use billions of images, text, and image-text pairs to create such a model. As you can imagine, this is not easily feasible!

Instead of building the architecture from scratch, BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), that con‐ nects a pretrained image encoder and a pretrained LLM.5

By leveraging pretrained models, BLIP-2 only needs to train the bridge without needing to train the image encoder and LLM from scratch. It makes great use of the technology and models that are already out there! This bridge is illustrated in Figure 9-16.

![## Image Analysis: 8fb6049e10eddaf68db6b740453e1dfac353a10364c92850c11c217342f6f958.jpg

**Conceptual Understanding:**
The image conceptually represents a modular artificial intelligence architecture designed to enable communication between different data modalities, specifically vision and language. Its main purpose is to illustrate how a specialized, trainable component can effectively bridge two large, pre-existing, and fixed (pretrained) models. The key idea being communicated is the efficient integration of powerful, off-the-shelf vision and language models by introducing a lightweight, adaptable 'querying transformer' that learns to translate between their respective representations, thus solving the challenge of modality alignment without retraining the foundational models themselves. The visual metaphor of icebergs for the pretrained models suggests their vast and mostly 'frozen' nature, while the flame for the trainable Q-Former implies active learning and transformation.

**Content Interpretation:**
The image displays a sequential architecture for a multimodal AI system. It illustrates three main components: a 'Vision Transformer,' a 'Q-Former Querying transformer,' and a 'Large language model.' The Vision Transformer and the Large language model are both explicitly labeled as 'Pretrained,' indicating that their parameters are fixed and not updated during the training of this specific pipeline. They are also visually represented with iceberg-like bases, which can imply their massive, foundational nature, with only a part visible above the surface, and an asterisk symbol, potentially denoting their established, 'frozen' state. The Vision Transformer includes an L-shaped arrow symbol, likely signifying an output or connection point for visual features. The Q-Former, positioned in the middle, is the 'Trainable' component, identified by a flame icon, suggesting active learning or processing, and also an L-shaped arrow symbol. This Q-Former acts as the bridge or interface, taking input from the Vision Transformer and providing output to the Large language model. The Large language model also includes a speech bubble icon, clearly indicating its function in processing or generating language. The significance of the setup lies in the Q-Former being the only trainable part, meaning it is specifically designed to learn how to effectively transform representations from the vision domain into a format understandable by the language model, thereby 'bridging the modality gap' without requiring extensive retraining of the large, pre-existing vision and language models.

**Key Insights:**
The main takeaway from this image is the architectural innovation of using a dedicated 'Querying transformer' (Q-Former) as a trainable interface to connect two powerful, but otherwise static, 'Pretrained' models: a 'Vision Transformer' and a 'Large language model.' The explicit labeling of the Q-Former as 'Trainable' and the other two as 'Pretrained' highlights the key insight that this approach allows for efficient cross-modal understanding by focusing the learning on the intermediary component. This avoids the immense computational cost and complexity of jointly training or fine-tuning the entire large vision and language models. The use of distinct icons (asterisk for pretrained, flame for trainable, L-shaped arrow for connection/transformation, speech bubble for language) further reinforces the specific roles and states of each component in the pipeline, underscoring the Q-Former's unique and active role in facilitating the communication between the visual and linguistic domains.

**Document Context:**
This image directly illustrates the core architecture of BLIP-2 as described in the accompanying document context, 'BLIP-2: Bridging the Modality Gap.' It visually explains how the Querying Transformer (Q-Former) serves as the critical 'bridge between vision (ViT) and text (LLM)' and explicitly labels it as 'the only trainable component of the pipeline,' aligning perfectly with the text provided after the image (Figure 9-16). The diagram clarifies the relationship and roles of the three main components – the pretrained Vision Transformer, the trainable Q-Former, and the pretrained Large language model – providing a concrete visual representation of the abstract concept of bridging modalities in a computationally efficient manner.

**Summary:**
The image illustrates a conceptual pipeline for connecting vision and language models, specifically depicting the role of a Querying Transformer (Q-Former) as an intermediary. The process flows linearly from left to right. It starts with a "Pretrained Vision Transformer," an AI model for processing visual information, indicated by an asterisk symbol and a stylized L-shaped arrow, with its base represented by an iceberg. This component feeds into the central element, the "Q-Former Querying transformer." This Q-Former is explicitly labeled as "Trainable," highlighted by a flame icon and a similar L-shaped arrow, suggesting it is the active learning component. Finally, the output from the Q-Former goes to a "Pretrained Large language model," an AI model for understanding and generating text, also marked with an asterisk, a speech bubble icon, and an iceberg base, indicating its foundational and fixed nature. The overall diagram shows how a trainable component bridges two large, pretrained models in different modalities.](images/8fb6049e10eddaf68db6b740453e1dfac353a10364c92850c11c217342f6f958.jpg)
Figure 9-16. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the only trainable component of the pipeline.

To connect the two pretrained models, the Q-Former mimics their architectures. It has two modules that share their attention layers:

• An Image Transformer to interact with the frozen Vision Transformer for feature extraction   
• A Text Transformer that can interact with the LLM

The Q-Former is trained in two stages, one for each modality, as illustrated in Figure 9-17.

In step 1, image-document pairs are used to train the Q-Former to represent both images and text. These pairs are generally captions of images, as we have seen before when training CLIP.

The images are fed to the frozen ViT to extract vision embeddings. These embed‐ dings are used as the input of Q-Former’s ViT. The captions are used as the input of Q-Former’s Text Transformer.

![## Image Analysis: e4e835ef97746cea118d624b68c6e6eb74067fc322d7c4cf79f51ef059aea370.jpg

**Conceptual Understanding:**
The image conceptually illustrates a two-stage, modular architecture for a multimodal AI system, likely BLIP-2, designed to bridge the gap between vision and language. Its main purpose is to demonstrate how a 'Trainable' component, the 'Q-Former,' acts as an intermediary to connect a 'Pretrained Vision Transformer' with a 'Pretrained Large language model' for vision-to-language tasks.

The key ideas communicated are:
1.  **Modularity:** The system is built from distinct, specialized components.
2.  **Transfer Learning:** It leverages the extensive knowledge embedded in large, pre-trained models for both vision and language.
3.  **Adaptive Bridging:** A specific trainable component is introduced to learn the intricate mapping and interaction between these two modalities, rather than retraining the large foundational models entirely.
4.  **Two-Phase Process:** The learning is structured into a "Representation learning" phase, where vision and language features are aligned, followed by a "Generative learning" phase, where language is produced based on these aligned features.

**Content Interpretation:**
The image illustrates a two-stage multimodal learning architecture for a vision-and-language model, specifically detailing the interaction between a visual encoder, a trainable query-transformer (Q-Former), and a large language model.

**Processes Shown:**
*   **Vision-and-language Representation learning:** The first stage focuses on learning to represent both vision and language in a compatible format. This involves taking features from a pre-trained vision model and processing them through a trainable Q-Former, which contains both vision and text transformation capabilities.
*   **Vision-to-language Generative learning:** The second stage involves using the learned representations from the Q-Former to enable a large language model to generate text based on visual input.

**Concepts and Systems Shown:**
*   **Pretrained Vision Transformer:** An existing, pre-trained model responsible for encoding visual information. Its 'Pretrained' status (indicated by the header and asterisk `*`) and the iceberg graphic suggest it's a fixed, foundational component.
*   **Trainable Q-Former:** The central, trainable component that bridges the vision and language modalities. The 'Trainable' header and the flame icon 🔥 highlight its active role in adaptation. It comprises internal 'Vision Transformer' and 'Text Transformer' sub-components, implying its function in processing and integrating both types of information.
*   **Pretrained Large language model:** An existing, pre-trained model capable of generating language. Its 'Pretrained' status (header and asterisk `*`) and the speech bubble icon 💬 indicate its generative linguistic function. The iceberg graphic again reinforces its fixed, foundational nature.

**Significance of Information Presented:**
*   The distinction between 'Pretrained' and 'Trainable' components is significant, indicating that only the Q-Former is fine-tuned for the multimodal task, thereby leveraging the robust capabilities of existing large-scale models without retraining them entirely.
*   The flow from the Vision Transformer to the Q-Former and then to the Large Language Model demonstrates a pipeline where visual features are intelligently queried and transformed into a format suitable for an LLM to generate meaningful text, supporting the idea of creating 'soft visual prompts' as mentioned in the document context. The small square-with-arrow icons on the corners of most boxes (Vision Transformer, Q-Former, Q-Former's internal Vision and Text Transformers) could signify their role as feature extractors or transformers of data.

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Two-Stage Multimodal Learning Paradigm:** The image clearly outlines a sequential, two-stage process for multimodal AI: first focusing on "Vision-and-language Representation learning" and then moving to "Vision-to-language Generative learning." This structured approach is fundamental to its design.
2.  **Efficient Leveraging of Pretrained Models:** The architecture efficiently utilizes powerful, pre-existing "Pretrained" models for both visual encoding ("Vision Transformer") and language generation ("Large language model"). This minimizes the need for extensive training from scratch, as indicated by the 'Pretrained' labels, asterisk `*` symbols, and the iceberg graphics suggesting fixed components.
3.  **Q-Former as the Adaptive Modality Bridge:** The "Trainable Q-Former" is identified as the critical, adaptable component responsible for bridging the gap between the vision and language modalities. Its 'Trainable' status and the flame icon 🔥 underscore its role as the active learning element that connects the two distinct pretrained domains. The presence of both 'Vision Transformer' and 'Text Transformer' within the Q-Former demonstrates its function in cross-modal understanding and transformation.
4.  **Modular and Extensible Design:** The system's modularity, with distinct components handling specific functions (vision encoding, modality adaptation, language generation), allows for flexibility. It suggests that different pretrained vision models or large language models could potentially be integrated with the Q-Former.

**Conclusions/Insights:**
*   The primary insight is the strategic use of a relatively small, trainable component (Q-Former) to adapt and connect two powerful, frozen, large-scale pretrained models. This approach significantly reduces the computational cost of training a multimodal model compared to end-to-end training of all components.
*   The Q-Former's role is to effectively extract and format relevant visual information from the Vision Transformer in a way that the Large Language Model can interpret as 'soft visual prompts' for conditional language generation. This mechanism is crucial for enabling the LLM to 'understand' and respond to visual cues.

**Document Context:**
This image, Figure 9-17, directly illustrates the architectural details and operational flow of BLIP-2, a model aimed at bridging the modality gap between vision and language. It provides a visual representation of the two key steps mentioned in the surrounding document text: "In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM."

The diagram clarifies how these abstract steps are implemented by showing specific components like the 'Pretrained Vision Transformer,' 'Trainable Q-Former,' and 'Pretrained Large language model,' and how they interact. It explains *how* the representations are learned and *how* they are converted to prompts for the LLM, thereby deepening the reader's understanding of the BLIP-2 model's architecture and its unique two-stage training strategy.

**Summary:**
This diagram illustrates a two-stage framework for integrating vision and language capabilities, which is a core concept behind models like BLIP-2. The entire process is divided into two major phases: "Vision-and-language Representation learning" and "Vision-to-language Generative learning."

**Phase 1: Vision-and-language Representation learning**
This initial phase focuses on developing shared or compatible representations for visual and textual data.
*   It begins with a "Pretrained Vision Transformer." This is a foundational, pre-existing model (indicated by the asterisk `*` and the "Pretrained" label) that takes visual input and transforms it into numerical representations. The blue iceberg graphic beneath it symbolically suggests that this component is largely fixed or "frozen," relying on its already extensive pre-training.
*   The output from this Vision Transformer then feeds into the central component of this phase: the "Trainable Q-Former." This Q-Former (marked with a flame icon 🔥 and the "Trainable" label) is the part of the system that is actively learned or fine-tuned. Its role is to bridge the gap between vision and language. Internally, the Q-Former contains its own "Vision Transformer" and "Text Transformer" sub-components. These work together to process the visual information and relate it to language, essentially learning to extract relevant visual cues that can be understood by a language model.

**Phase 2: Vision-to-language Generative learning**
Once the Q-Former has learned to create these aligned representations, the process moves to generating language based on visual input.
*   The representations generated by the "Trainable Q-Former" are then passed to a "Pretrained Large language model." Similar to the initial Vision Transformer, this is a powerful, pre-existing language model (indicated by the asterisk `*` and "Pretrained" label) that has been trained on vast amounts of text. The speech bubble icon 💬 highlights its language generation capabilities. The iceberg graphic again signifies its frozen, pre-trained status. The Q-Former effectively provides "soft visual prompts" to this Large language model, enabling it to generate coherent and contextually relevant text based on the visual input it receives via the Q-Former.

In essence, the diagram shows how BLIP-2 intelligently connects powerful, off-the-shelf pretrained models (a Vision Transformer and a Large Language Model) using a relatively smaller, trainable "Q-Former." The Q-Former acts as a flexible adapter, learning how to query and summarize visual information in a way that allows the Large Language Model to effectively "see" and "understand" images for language generation tasks.](images/e4e835ef97746cea118d624b68c6e6eb74067fc322d7c4cf79f51ef059aea370.jpg)
Figure 9-17. In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM.

With these inputs, the Q-Former is then trained on three tasks:

Image-text contrastive learning This task attempts to align pairs of image and text embeddings such that they maximize their mutual information.

Image-text matching

A classification task to predict whether an image and text pair is positive (matched) or negative (unmatched).

Image-grounded text generation

Trains the model to generate text based on information extracted from the input image.

These three objectives are jointly optimized to improve the visual representations that are extracted from the frozen ViT. In a way, we are trying to inject textual information into the embeddings of the frozen ViT so that we can use them in the LLM. This first step of BLIP-2 is illustrated in Figure 9-18.

![## Image Analysis: ad775f33b3c7cad5f73f27b0a08b16f004f78b9b2ff497508003db716fdbc4ca.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and training methodology of a multi-modal artificial intelligence model, likely BLIP-2, specifically its "Q-Former" component. The main purpose of the diagram is to illustrate how raw image input is processed through a sequence of transformers, including a frozen pretrained vision model and trainable vision and text transformers within the "Q-Former," to learn unified visual-text representations. The key ideas communicated are the modularity of the system, the crucial role of "Learnable embeddings" for cross-modal alignment, and the application of three distinct multi-task training objectives ("Image-text matching", "Image-text contrastive learning", "Image-grounded text generation") to achieve comprehensive vision-language understanding.

**Content Interpretation:**
The image depicts a machine learning system architecture designed for vision-language understanding, specifically detailing the role of a "Q-Former" module. The process begins with an "Input image" being processed by a "Pretrained * Vision Transformer" to extract initial visual features. The output of this pretrained model feeds into the "Q-Former" module. Inside the Q-Former, a specialized "Vision Transformer" (marked with a flame icon) further processes the visual input, generating "Learnable embeddings". Simultaneously, a "Text Transformer" (also with a flame icon) processes textual input, such as the example "“A pixelated image of a cute cat”". The outputs from these Q-Former transformers are then utilized for three distinct multi-modal training tasks: "Image-text matching", "Image-text contrastive learning", and "Image-grounded text generation". These tasks represent the core objectives for learning robust visual-text representations by aligning information from both modalities. The significance lies in showing a structured approach to integrate powerful pretrained vision models with a trainable, cross-modal component (the Q-Former) to achieve sophisticated vision-language capabilities.

**Key Insights:**
The main takeaways from this image are: 1. Modular Architecture for Vision-Language Integration: The system uses a `Pretrained * Vision Transformer` to extract initial features, which are then refined and aligned with text by a specialized `Q-Former` module. This highlights a strategy of leveraging powerful existing models while introducing dedicated components for cross-modal tasks. 2. Multitask Learning for Robust Representations: The `Q-Former` is explicitly trained with three distinct objectives: `Image-text matching`, `Image-text contrastive learning`, and `Image-grounded text generation`. This multi-task approach is crucial for learning diverse and robust visual-text representations that can handle various types of vision-language understanding and generation tasks. 3. Learnable Intermediate Embeddings: The `Vision Transformer` within the `Q-Former` generates `Learnable embeddings`. These adaptable representations are key to effectively bridging the visual and textual modalities, allowing the model to dynamically learn how to best represent and relate image content to language. These insights are directly supported by the verbatim labels and the flow of information depicted in the diagram.

**Document Context:**
The image, Figure 9-18, directly illustrates the architectural components and workflow of BLIP-2, particularly focusing on its Q-Former module, as described in the document's context. The text provided, "In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations," directly corresponds to the diagram. The "Pretrained * Vision Transformer" is the frozen ViT, its output is used, and the three tasks ("Image-text matching", "Image-text contrastive learning", "Image-grounded text generation") are explicitly shown as training objectives, some of which are contrastive-like. The diagram visually expands on how visual inputs are processed, integrated with text, and then used for these specific tasks to bridge the modality gap and learn comprehensive visual-text representations, thus serving as a key explanatory figure for the BLIP-2 architecture.

**Summary:**
This diagram illustrates the architecture of a multi-modal model, likely BLIP-2, focusing on a component called the "Q-Former" designed to integrate image and text information. The process begins with an "Input image", which is a visual representation, in this case, a pixelated cat. This image is first fed into a "Pretrained * Vision Transformer". The asterisk "*" and the label "Pretrained" signify that this initial vision transformer is a pre-existing, likely frozen, model that extracts foundational visual features from the image. The irregular blue polygonal shape beneath it further implies its fixed or pre-computed nature. The output from this "Pretrained * Vision Transformer" then enters the main processing module, denoted by the dotted line boundary and labeled "Q-Former". Inside the Q-Former, there are two key components, both marked with a flame icon, suggesting they are actively trainable or core parts of the Q-Former's learning process: 1. A "Vision Transformer" that takes the output from the pretrained vision model. This transformer processes the visual input further and generates "Learnable embeddings", represented by a series of green blocks. These embeddings are adaptable and crucial for bridging the modalities. 2. A "Text Transformer" which receives textual input, exemplified by the phrase "“A pixelated image of a cute cat”". This transformer processes the natural language description. The outputs of these transformers within the Q-Former are then leveraged for three distinct multi-modal training tasks, positioned at the top of the Q-Former section: "Image-text matching", a task focused on determining the correspondence or similarity between an image and a piece of text; "Image-text contrastive learning", an objective that aims to learn joint representations where relevant image-text pairs are drawn closer together in an embedding space, while irrelevant pairs are pushed apart; and "Image-grounded text generation", a task where the model learns to produce descriptive text that accurately reflects the content of a given image, leveraging the textual processing capabilities. Essentially, the Q-Former acts as an intermediary, taking robust visual features from a pretrained model and aligning them with text through its own trainable vision and text transformers, thereby enabling the model to learn comprehensive visual-text representations across these three critical tasks. This detailed setup allows the system to understand and generate content that spans both visual and linguistic domains.](images/ad775f33b3c7cad5f73f27b0a08b16f004f78b9b2ff497508003db716fdbc4ca.jpg)
Figure 9-18. In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations.

In step 2, the learnable embeddings derived from step 1 now contain visual informa‐ tion in the same dimensional space as the corresponding textual information. The learnable embeddings are then passed to the LLM. In a way, these embeddings serve as soft visual prompts that condition the LLM on the visual representations that were extracted by the Q-Former.

There is also a fully connected linear layer in between them to make sure that the learnable embeddings have the same shape as the LLM expects. This second step of converting vision to language is represented in Figure 9-19.

![## Image Analysis: dbc6c57749d508af91ebc056a48819d8b9e3dad6e5fbb8e85c40e9ca5e88a97d.jpg

**Conceptual Understanding:**
This image conceptually represents a component of a multimodal AI system, specifically detailing how visual information is processed and prepared to be consumed by a large language model. The main purpose is to illustrate the bridging of the modality gap between visual features and linguistic processing. Key ideas include:
*   The use of a "Trainable" Q-Former for feature extraction from an initial input.
*   The concept of "Learnable embeddings" as an intermediate representation.
*   A projection mechanism to transform these "Learnable embeddings" into "Projected embeddings."
*   The integration of these "Projected embeddings" into a "Pretrained" large language model.
*   The entire flow aims to allow a language model to understand and generate responses based on visual inputs, likely by converting visual features into a format that the LLM can interpret as a "soft visual prompt."

**Content Interpretation:**
The image depicts a specific architectural pattern within a larger system (like BLIP-2), focusing on the interface between a vision component and a language model.

*   **Q-Former (Trainable):** This rectangular box labeled "Q-Former" and "Trainable" (along with the flame and speech bubble icons) signifies a module responsible for extracting relevant features or representations from an input. Being "Trainable" implies it learns to perform this task effectively during the model's training phase, adapting to the specific requirements of the downstream task. The flame could suggest a powerful or "burning" attention mechanism, and the speech bubble a query-based or communicative aspect, aligning with the Q-Former's role in querying visual features.
*   **Learnable embeddings:** These five green boxes labeled "Learnable embeddings" are the direct output of the Q-Former. They represent a high-dimensional, learned representation of the input's visual information, optimized by the trainable Q-Former.
*   **Projection Layer (implied):** The crisscrossing lines connecting "Learnable embeddings" to "Projected embeddings" visually represent a projection layer. This layer transforms the "Learnable embeddings" from their original dimension (5 units) to a new dimension (3 units, as shown by the red boxes), which is suitable for the subsequent language model. The label "Projected embeddings" confirms this transformation, emphasizing that these are the result of a projection.
*   **Large Language Model (Pretrained):** This pink box labeled "Large language model" and "Pretrained" (with the asterisk and expanding box icons, and the iceberg graphic) is the final recipient of the processed visual information. "Pretrained" indicates that this LLM has already undergone extensive training on vast text corpora, possessing a strong understanding of language. The projected embeddings serve as an input to this already capable model, effectively "prompting" it with visual context. The iceberg graphic might visually convey the depth and complexity of a large language model, with much of its knowledge and parameters "below the surface." The asterisk and expanding box icons could symbolize its vast capabilities or its role as a foundational model.

The extracted text explicitly details the components ("Q-Former", "Large language model") and the intermediate data representations ("Learnable embeddings", "Projected embeddings"), along with their training status ("Trainable", "Pretrained"). This provides direct evidence for the interpretation of a two-stage process for integrating visual information into an LLM.

**Key Insights:**
The image illustrates several key insights into multimodal AI system design, particularly within the BLIP-2 context:

*   **Modular Design:** The system is composed of distinct modules: a "Trainable Q-Former" for feature extraction and a "Pretrained Large language model" for language understanding. This modularity allows for specialized training and reuse of powerful pre-existing models.
*   **Controlled Information Flow:** Visual information is not directly fed to the LLM. Instead, it undergoes a transformation process through "Learnable embeddings" and "Projected embeddings." This suggests a deliberate mechanism to control and optimize the representation of visual information before it reaches the language model.
*   **Soft Visual Prompting:** As hinted by the document context ("The projected embeddings serve as a soft visual prompt"), the "Projected embeddings" are crucial for acting as a non-textual input that guides the LLM's understanding and generation in a visually grounded manner. The projection layer's role is to ensure these embeddings are in a format compatible with the LLM's input expectations.
*   **Leveraging Pretrained Models:** The use of a "Pretrained Large language model" is a significant takeaway, indicating an efficient strategy to leverage vast existing linguistic knowledge without needing to retrain the entire LLM for visual tasks. The trainable Q-Former then acts as an adapter.

The labels "Trainable" and "Pretrained" directly support the insight about modularity and leveraging existing knowledge. The progression from "Learnable embeddings" to "Projected embeddings" via a distinct projection step underscores the controlled information flow and the creation of a "soft visual prompt" suitable for the LLM.

**Document Context:**
This image fits within the document's narrative on "BLIP-2: Bridging the Modality Gap" by visually explaining a core mechanism of how BLIP-2 integrates visual information into a large language model. The accompanying text, "Figure 9-19. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt," directly refers to and clarifies the specific step depicted in this diagram. It illustrates the crucial "step 2" of the BLIP-2 architecture, demonstrating how the outputs of the Q-Former are prepared and fed to the LLM to enable multimodal understanding.

**Summary:**
This diagram illustrates a key process, likely within the BLIP-2 architecture, for integrating visual information into a large language model. The workflow proceeds in a clear, left-to-right sequence:

1.  **Initial Input Processing:** The process starts with an unspecified input, indicated by an ellipsis ("..."), which is directed towards a component called the "Q-Former." This "Q-Former" module is specifically labeled as "Trainable," meaning it learns to process inputs effectively during the training phase. It also features a flame icon, perhaps symbolizing its processing power or attention mechanism, and a small speech bubble icon, hinting at its role in querying or interacting with information.
2.  **Generating Learnable Embeddings:** The "Trainable Q-Former" processes the input and produces what are labeled as "Learnable embeddings." These are depicted as a series of five green rectangular boxes, representing a numerical representation of the input's features that have been learned and optimized by the Q-Former.
3.  **Projecting Embeddings:** Following the "Learnable embeddings," a crucial transformation occurs. These five green "Learnable embeddings" are connected to three red rectangular boxes labeled "Projected embeddings" by a network of crisscrossing lines. This visual representation signifies a projection layer. This layer takes the "Learnable embeddings" and maps them into a new, typically more compact or specifically formatted, set of "Projected embeddings" suitable for the next stage.
4.  **Input to Large Language Model:** Finally, these "Projected embeddings" are fed as input to a "Large language model." This language model is distinctly labeled as "Pretrained," indicating it has already been extensively trained on massive text datasets and possesses a deep understanding of language. The large language model box also includes an asterisk icon and an expanding box icon, possibly denoting its foundational nature and vast capabilities. Below the language model, a blue, crystalline, iceberg-like graphic suggests the immense depth and complexity of such a model, much of which lies "beneath the surface."

In essence, this diagram shows how a trainable Q-Former extracts and refines visual features into "Learnable embeddings," which are then transformed through a projection layer into "Projected embeddings." These projected embeddings then act as a specialized, "soft visual prompt" for an already powerful, pretrained large language model, enabling it to understand and reason about visual content. This carefully designed pipeline ensures that visual information is effectively adapted and communicated to the language model.](images/dbc6c57749d508af91ebc056a48819d8b9e3dad6e5fbb8e85c40e9ca5e88a97d.jpg)
Figure 9-19. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt.

When we put these steps together, they make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM. As a result, the LLM will be given information about the image in a similar manner to the context you would provide an LLM when prompting. The full in-depth process is illustrated in Figure 9-20.

![## Image Analysis: d86d1435890ab71ec39ae3d19ea2d4bc38cbe1835ebef9dfcd17d132d7dc31af.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural design and operational flow of the BLIP-2 model, a system designed to effectively integrate and process information from both visual and linguistic modalities. The main purpose is to illustrate how BLIP-2 'bridges the modality gap' between vision and language, enabling capabilities like vision-to-language generation. It communicates the key idea of using a shared representation learning stage (involving vision and text transformers) to prepare information for a subsequent generative learning stage with a large language model.

**Content Interpretation:**
The image details the architectural components and data flow of the BLIP-2 model. It illustrates a two-stage process: 'Vision-and-language Representation learning' and 'Vision-to-language Generative learning'.

1.  **Vision-and-language Representation learning**: This initial phase involves processing visual input using a 'Pretrained' 'Vision Transformer' to extract representations. These representations are then refined by a 'Trainable' 'Q-Former', which integrates both 'Vision Transformer' and 'Text Transformer' components to learn richer, shared vision-language representations.

2.  **Vision-to-language Generative learning**: The refined embeddings from the Q-Former are transformed through 'Learnable embeddings' and 'Projected embeddings' layers. These projected embeddings then serve as input to a 'Pretrained' 'Large language model' to generate textual output based on the visual input, thereby performing generative vision-to-language tasks.

The 'Pretrained' labels indicate that these components are initialized with prior knowledge, while 'Trainable' signifies that the Q-Former is fine-tuned during the BLIP-2 process. The 'iceberg' shapes beneath the 'Vision Transformer' and 'Large language model' suggest that these are substantial, underlying models that might be partially frozen or provide foundational capabilities.

**Key Insights:**
The main takeaways from this image are:

1.  **Two-Stage Architecture**: BLIP-2 operates in two distinct stages: 'Vision-and-language Representation learning' and 'Vision-to-language Generative learning'. This modular design allows for specialized processing at different phases.

2.  **Importance of Q-Former**: The 'Q-Former' is a central, 'Trainable' component that acts as the bridge between the 'Pretrained Vision Transformer' and the 'Large language model'. Its internal 'Vision Transformer' and 'Text Transformer' suggest it's designed to align and fuse information from both modalities effectively.

3.  **Leveraging Pretrained Models**: BLIP-2 efficiently utilizes existing powerful 'Pretrained Vision Transformer' and 'Pretrained Large language model' components, which likely reduces training costs and improves performance by building on established capabilities.

4.  **Embedding Projections for LLM Integration**: The 'Learnable embeddings' and 'Projected embeddings' layers demonstrate a mechanism to adapt the output from the vision-language alignment (Q-Former) into a format suitable for input into the 'Large language model' for text generation.

5.  **Generative Capability**: The ultimate goal, as indicated by 'Vision-to-language Generative learning' and the 'Large language model', is to generate descriptive or relevant text from visual inputs.

**Document Context:**
This image, described as 'Figure 9-20. The full BLIP-2 procedure.', directly supports the document's section title 'BLIP-2: Bridging the Modality Gap'. It visually explains the architecture and the step-by-step process of how BLIP-2 achieves this bridging, moving from visual input to language generation. The diagram serves as a foundational visual aid for understanding the technical implementation and the flow of information within the BLIP-2 framework, enhancing the reader's comprehension of the model's design and functionality.

**Summary:**
The image presents a comprehensive diagram illustrating the BLIP-2 procedure, which bridges vision and language. The process is divided into two main stages: 'Vision-and-language Representation learning' and 'Vision-to-language Generative learning'.

In the first stage, 'Vision-and-language Representation learning', an input image (depicted as a grid of cat images) is processed by a 'Pretrained' 'Vision Transformer'. This transformer, visually represented as a teal box with an asterisk and an 'iceberg' shape below it, extracts visual features. These features are then fed into the 'Trainable' 'Q-Former'. The Q-Former, indicated by a flame icon, contains two sub-components: a 'Vision Transformer' and a 'Text Transformer', both of which are also equipped with a small arrow icon.

The output from the Q-Former leads into the second stage, 'Vision-to-language Generative learning'. Here, the information first interacts with 'Learnable embeddings' (represented by a layer of green rectangular nodes) which are connected to 'Projected embeddings' (represented by a layer of red rectangular nodes). Finally, these projected embeddings are passed to a 'Pretrained' 'Large language model', shown as a pink box with an asterisk, a speech bubble icon, and an 'iceberg' shape below it. This large language model is responsible for generating language outputs based on the visual input.

The entire procedure demonstrates a flow from raw image input, through vision and language transformers, to learnable and projected embeddings, ultimately culminating in a large language model for generative learning, effectively bridging the two modalities.](images/d86d1435890ab71ec39ae3d19ea2d4bc38cbe1835ebef9dfcd17d132d7dc31af.jpg)
Figure 9-20. The full BLIP-2 procedure.

Since BLIP-2, many other visual LLMs have been released that have similar processes, like LLaVA, a framework for making textual LLMs multimodal6 or Idefics 2, an efficient visual LLM based on the Mistral 7B LLM.7 Both visual LLMs, although having different architectures, connect pretrained CLIP-like visual encoders with textual LLMs. The goal of these architectures is to project visual features from the input images to language embeddings such that they can be used as the input for an LLM. Similar to the Q-Former, they attempt to bridge the gap between images and text.

# Preprocessing Multimodal Inputs

Now that we know how BLIP-2 is created, there are a number of interesting use cases for such a model, not limited to captioning images, answering visual questions, and even performing prompting.

Before we go through some use cases, let’s first load the model and explore how you can use it:

from transformers import AutoProcessor, Blip2ForConditionalGeneration import torch

# Load processor and main model   
blip_processor $=$ AutoProcessor.from_pretrained("Salesforce/blip2-opt-2.7b")   
model $=$ Blip2ForConditionalGeneration.from_pretrained( "Salesforce/blip2-opt-2.7b", torch_dtype=torch.float16   
)

# Send the model to GPU to speed up inference device $=$ "cuda" if torch.cuda.is_available() else "cpu" model.to(device)

Using model.vision_model and model.language_model, we can see which ViT and generative model are used, respectively, in the BLIP-2 model we loaded.

We loaded two components that make up our full pipeline: a processor and a model. The processor can be compared to the tokenizer of language models. It converts unstructured input, such as images and text, to representations that the model gener‐ ally expects.

# Preprocessing images

Let’s start by exploring what the processor does to images. We start by loading the picture of a very wide image for illustration purposes:

# Load image of a supercar   
car_path $=$ "https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large  
Language-Models/main/chapter09/images/car.png"   
image $=$ Image.open(urlopen(car_path)).convert("RGB")

image

![## Image Analysis: c4cbc941d4752291d94b584453db0f887bb3193afd6faa041cd30a4d94d76bff.jpg

**Conceptual Understanding:**
This image conceptually represents a high-performance luxury sports car, commonly known as a supercar. The main purpose of the image is to evoke a sense of speed, elegance, and adventure, set against a dramatic natural backdrop during either sunrise or sunset. It communicates key ideas related to sophisticated automotive design, the thrill of driving, and the aesthetic beauty of both the vehicle and its environment.

**Content Interpretation:**
The image primarily depicts the aesthetic and dynamic concept of a high-performance supercar. The vehicle is shown in motion, suggested by the blurred background, implying speed and exhilaration. The warm, ambient lighting (sunrise or sunset) adds to a picturesque and aspirational atmosphere. The car's design emphasizes modern automotive engineering and luxury. There are no processes or systems shown, only a static visual representation of a car in a scenic setting. No data, trends, or specific information are presented, and therefore no text elements support these interpretations as no text was found in the image.

**Key Insights:**
The main takeaway from this image is the visual appeal and dynamic presence of a high-performance vehicle. It showcases an aspirational scene associated with luxury and speed, suggesting freedom and an exciting driving experience. The image reinforces the idea that supercars are designed for both extreme performance and striking aesthetic impact, often presented in visually captivating environments. No specific textual evidence can be extracted as no text is present in the image; all insights are derived from visual analysis.

**Document Context:**
Given the document context "Section: Load image of a supercar," this image directly fulfills the purpose by visually representing a supercar. It likely serves as a direct illustration or example within a larger discussion about high-performance vehicles, automotive design, luxury transport, or even digital rendering capabilities, as it appears to be a digitally rendered image. The image is a literal interpretation of the section title, providing the visual content requested.

**Summary:**
The image displays a sleek, modern, orange supercar driving on a paved road. The car is positioned centrally, facing slightly towards the viewer and to the left, with its bright, angular headlights illuminated. The body is a vibrant metallic orange, with sharp lines and aerodynamic features visible. The windshield reflects the ambient light, and a driver's silhouette can be faintly seen inside. The road ahead curves gently to the right, with faint white lines marking the lanes. In the background, the sky is filled with dramatic, dark clouds overhead, transitioning to a warm, soft orange and pink hue near the horizon where the sun is setting or rising, casting a glow. Distant hills or mountains are visible under the colorful sky. The overall impression is one of speed, elegance, and a journey against a beautiful, dynamic natural backdrop. No text is present in the image.](images/c4cbc941d4752291d94b584453db0f887bb3193afd6faa041cd30a4d94d76bff.jpg)

The image has $5 2 0 \times 4 9 2$ pixels, which is generally an unusual format. So let’s see what our processor does to it:

# Preprocess the image   
inputs $=$ blip_processor(image, return_tensors $=$ "pt").to(device, torch.float16)   
inputs["pixel_values"].shape

This gives us the following shape:

torch.Size([1, 3, 224, 224])

The result is a $2 2 4 \times 2 2 4$ -sized image. Quite a bit smaller than we initially had! This also means that all the original different shapes of the image will be processed into squares. So be careful inputting very wide or tall images as they might get distorted.

# Preprocessing text

Let’s continue this exploration of the processor with text instead. First, we can access the tokenizer used to tokenize the input text:

blip_processor.tokenizer

This gives us the following output:

GPT2TokenizerFast(name_or_path $=$ 'Salesforce/blip2-opt-2.7b', vocab_siz $= 5 \Theta 2 6 5$ ,   
model_max_length $| =$ 1000000000000000019884624838656, is_fast $=$ True, pad  
ding_side='right', truncation_side='right', special_tokens={'bos_token': '</   
$S > "$ , 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'},   
clean_up_tokenization_spaces $=$ True), added_tokens_decoder={   
1: AddedToken("<pad>", rstrip $\backsimeq$ False, lstrip $| =$ False, single_word=False, normal  
ized=True, special $\ l =$ True),   
2: AddedToken("</s>", rstrip $\ulcorner$ False, lstrip $| =$ False, single_word $=$ False, normal  
ized=True, special=True),   
}

The BLIP-2 model here uses a GPT2Tokenizer. As we explored in Chapter 2, how tokenizers deal with input text can differ greatly.

To explore how GPT2Tokenizer works, we can try it out with a small sentence. We start by converting the sentence to token IDs before converting them back to tokens:

# Preprocess the text   
text $=$ "Her vocalization was remarkably melodic"   
token_ids $=$ blip_processor(image, text=text, return_tensors $=$ "pt") token_ids $=$ token_ids.to(device, torch.float16)["input_ids"][0]   
# Convert input ids back to tokens   
tokens $=$ blip_processor.tokenizer.convert_ids_to_tokens(token_ids)   
tokens

This gives us the following tokens:

['</s>', 'Her', 'Ġvocal', 'ization', 'Ġwas', 'Ġremarkably', 'Ġmel', 'odic']

When we inspect the tokens, you might notice a strange symbol at the beginning of some tokens, namely, the $\dot { \mathrm { ~ G ~ } }$ symbol. This is actually supposed to be a space. However, an internal function takes characters in certain code points and moves them up by 256 to make them printable. As a result, the space (code point 32) becomes $\dot { \mathrm { ~ G ~ } }$ (code point 288).

We will convert them to underscores for illustrative purposes:

# Replace the space token with an underscore tokens $=$ [token.replace("Ġ", "_") for token in tokens] tokens

This gives us a nicer output:

['</s>', 'Her', '_vocal', 'ization', '_was', '_remarkably', '_mel', 'odic']

The output shows that the underscore indicates the beginning of a word. That way, words that are made up of multiple tokens can be recognized.

# Use Case 1: Image Captioning

The most straightforward usage of a model like BLIP-2 is to create captions of images that you have in your data. You might be a store that wants to create descriptions of its clothing or perhaps you are a photographer that does not have the time to manually label the $1 { , } 0 0 0 +$ pictures of a wedding.

The process of captioning an image closely follows the processing. An image is converted to pixel values that the model can read. These pixel values are passed to BLIP-2 to be converted into soft visual prompts that the LLM can use to decide on a proper caption.

Let’s take the image of a supercar and use the processor to derive pixels in the expected shape:

# Load an AI-generated image of a supercar image $=$ Image.open(urlopen(car_path)).convert("RGB")

# Convert an image into inputs and preprocess it   
inputs $=$ blip_processor(image, return_tensors $=$ "pt").to(device, torch.float16)   
image

![## Image Analysis: 6a4bdaa0e247a1c0299818e247cc45f459963e9ce27862fb75f8f3498b1e8fbd.jpg

**Conceptual Understanding:**
This image conceptually represents an 'input image' for a technical document focused on image processing. Its main purpose is to serve as a high-quality, aesthetically pleasing example of an image that would undergo 'conversion and preprocessing'. The image communicates ideas of modern automotive design, dynamic movement, and the beauty of a natural landscape at sunset, all within the context of demonstrating practical applications for image analysis and manipulation.

**Content Interpretation:**
The image depicts a high-performance orange sports car driving on a road at sunset. The central focus is the car, highlighting its design and dynamic presence. The setting sun and dramatic clouds create an atmospheric backdrop, emphasizing the beauty and freedom associated with driving such a vehicle in a scenic environment. No specific processes, concepts, relationships, or systems with textual elements are shown, as the image is a photograph rather than a diagram or flowchart. The significance lies in its aesthetic appeal and the visual communication of automotive luxury and adventure.

**Key Insights:**
The primary takeaway from this image is its function as a visual input example for image processing tasks. It showcases a common subject (a vehicle in a landscape) that might be used to demonstrate algorithms for object detection, scene understanding, or image enhancement. The image itself teaches lessons about aesthetic composition and the visual representation of speed and luxury. Since no text is present in the image, all insights are derived from its visual content. The image visually supports the idea that diverse and high-quality photographic content can serve as input for advanced image processing techniques.

**Document Context:**
The image is presented within a document section titled 'Convert an image into inputs and preprocess it'. Given this context, the image serves as a visual example of an 'input image' that would be subjected to conversion and preprocessing steps. It illustrates the type of visual data that might be fed into an image processing system, demonstrating a real-world scenario for the technical concepts discussed in the document.

**Summary:**
The image displays a sleek, modern, orange sports car from a front-center perspective, appearing to be in motion on a paved road. Its headlights are illuminated, casting a glow. The car occupies the lower half of the frame, with the road curving slightly to the left in the foreground and receding into the distance. In the background, a sunset casts warm orange and yellow light, with the sun visible as a bright circle near the horizon. Overcast clouds dominate the upper portion of the sky, with mountainous or hilly terrain visible in silhouette along the horizon. Green foliage can be seen on the right side of the road in the mid-ground. The overall impression is one of speed, luxury, and a picturesque journey during golden hour. There is no discernible text, labels, or annotations within the image itself.](images/6a4bdaa0e247a1c0299818e247cc45f459963e9ce27862fb75f8f3498b1e8fbd.jpg)

The next step is converting the image into token IDs using the BLIP-2 model. After doing so, we can convert the IDs into text (the generated caption):

# Generate image ids to be passed to the decoder (LLM)   
generated_ids $=$ model.generate(\*\*inputs, max_new_tokens $\begin{array} { r l } { : = { } } & { { } } \end{array}$ )   
# Generate text from the image ids   
generated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   
generated_text $=$ generated_text[0].strip()   
generated_text

generated_text contains the caption:

an orange supercar driving on the road at sunset

This seems like a perfect description for this image!

Image captioning is a great way to get to learn this model before stepping into more complex use cases. Try it out with a few images yourself and see where it performs well and where it performs poorly. Domain-specific images, like pictures of specific cartoon characters or imaginary creations, may fail as the model was trained on largely public data.

Let’s end this use case with a fun example, namely an image from the Rorschach test, which is illustrated in Figure 9-21. It is part of an old psychological experiment that tests the individual’s perception of inkblots.8 What someone sees in such an inkblot supposedly tells you something about a person’s personality characteristics. It is quite a subjective test but that just makes it more fun!

![## Image Analysis: f96ac1ff7268d609091578e49a5c2797d9127b695a102f3a4263f31b89cf25a3.jpg

**Conceptual Understanding:**
This image conceptually represents an ambiguous visual stimulus used in psychological assessment, specifically a Rorschach inkblot. Its main purpose is to demonstrate how individuals project their internal thoughts, feelings, and personality onto an undefined visual, thereby revealing aspects of their unconscious mind. The image communicates the idea that perception is not merely a passive reception of sensory input but an active process influenced by one's psychological state.

**Content Interpretation:**
The image is a Rorschach inkblot, specifically one of the ten standard cards used in the Rorschach test. It is an example of an ambiguous stimulus used in a projective psychological assessment. The blot's symmetrical, irregular shape, composed of shades of gray and black with white spaces, is designed to evoke varied perceptions and interpretations from individuals. The significance lies entirely in its ambiguity, which allows an individual's unconscious thoughts, feelings, and personality characteristics to be projected onto the inkblot during interpretation.

**Key Insights:**
The main takeaway from this image is the concept of a projective test, where ambiguous stimuli are used to elicit responses that are thought to reveal an individual's underlying personality traits, emotional states, or thought processes. The Rorschach inkblot specifically highlights that there is no 'right' or 'wrong' answer, and the interpretation is highly subjective and indicative of the observer's internal world. The image serves as an example of how a seemingly simple visual can be a complex diagnostic tool in psychology due to its inherent lack of objective meaning.

**Document Context:**
This image directly illustrates 'Figure 9-21. An image from the Rorschach test. What do you see in it?' as mentioned in the document context. It serves as a visual example of a Rorschach inkblot, which is a key tool in psychological assessment and often discussed in texts related to perception, personality psychology, or projective tests. Its placement suggests a discussion about how individuals perceive and interpret ambiguous stimuli, and how such interpretations can reveal insights into their cognitive processes and inner psychological states.

**Summary:**
The image displays a classic Rorschach inkblot, characterized by its symmetrical, abstract, and ambiguous form. The blot is primarily a dark, mottled gray or black color, spread across a light, cream-colored background. Its shape is roughly butterfly-like or bat-like, with distinct bilateral symmetry along a central vertical axis. The inkblot has jagged, irregular edges and varying shades of gray, creating areas of both dense color and lighter, more translucent patches. Several white spaces are prominent within the darker ink, particularly in the central region, which can also be perceived as distinct shapes or figures. The overall appearance is designed to be open to subjective interpretation, which is the fundamental principle of the Rorschach projective test.](images/f96ac1ff7268d609091578e49a5c2797d9127b695a102f3a4263f31b89cf25a3.jpg)
Figure 9-21. An image from the Rorschach test. What do you see in it?

Let’s take the image illustrated in Figure 9-21 and use that as our input:

# Load Rorschach image   
url $=$ "https://upload.wikimedia.org/wikipedia/commons/7/70/Ror   
schach_blot_01.jpg"   
image $=$ Image.open(urlopen(url)).convert("RGB")   
# Generate caption   
inputs $=$ blip_processor(image, return_tensors $=$ "pt").to(device, torch.float16)   
generated_ids $=$ model.generate(\*\*inputs, max_new_tokens $\begin{array} { r l } { : = { } } & { { } } \end{array}$ )   
generated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   
)

generated_text $=$ generated_text[0].strip() generated_text

As before, when we inspect the generated_text variable, we can take a look at the caption:

a black and white ink drawing of a bat

I can definitely see how the model would caption this image using such a description. Since this is a Rorschach test, what do you think it says about the model?

# Use Case 2: Multimodal Chat-Based Prompting

Although captioning is an important task, we can extend its use case even further. In the previous example, we showed going from one modality, vision (image), to another, text (caption).

Instead of following this linear structure, we can try to present both modalities simul‐ taneously by performing what is called visual question answering. In this particular use case, we give the model an image along with a question about that specific image for it to answer. The model needs to process both the image as well as the question at once.

To demonstrate, let’s start with the picture of a car and ask BLIP-2 to describe the image. To do so, we first need to preprocess the image as we did a few times before:

# Load an AI-generated image of a supercar image $=$ Image.open(urlopen(car_path)).convert("RGB")

To perform our visual question answering we need to give BLIP-2 more than just the image, namely the prompt. Without it, the model would generate a caption as it did before. We will ask the model to describe the image we just processed:

# Visual question answering prompt $=$ "Question: Write down what you see in this picture. Answer:"

# Process both the image and the prompt   
inputs $=$ blip_processor(image, text=prompt, return_tensors $\mathbf { \equiv }$ "pt").to(device,   
torch.float16)   
# Generate text   
generated_ids $=$ model.generate(\*\*inputs, max_new_tokens $= 3 \Theta$ )   
generated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens $\mathbf { \sigma } =$ True   
)   
generated_text $=$ generated_text[0].strip()   
generated_text

This gives us the following output:

It correctly describes the image. However, this is a rather simple example since our question is essentially asking the model to create a caption. Instead, we can ask follow-up questions in a chat-based manner.

To do so, we can give the model our previous conversation, including its answer to our question. We then ask it a follow-up question:

# Chat-like prompting   
prompt $=$ "Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:"   
# Generate output   
inputs $=$ blip_processor(image, text=prompt, return_tensors $\mathbf { \sigma } =$ "pt").to(device,   
torch.float16)   
generated_ids $=$ model.generate(\*\*inputs, max_new_tokens $\begin{array} { r l } { : = { } } & { { } } \end{array}$ )   
generated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   
)   
generated_text $=$ generated_text[0].strip()   
generated_text

This gives us the following answer:

\$1,000,000

$\$ 1,000,000$ is highly specific! This shows more chat-like behavior from BLIP-2, which allows for some interesting conversations.

Finally, we can make this process a bit smoother by creating an interactive chatbot using ipywidgets, an extension for Jupyter notebooks that allows us to make interac‐ tive buttons, input text, etc:

from IPython.display import HTML, display   
import ipywidgets as widgets   
def text_eventhandler(\*args): question $=$ args[0]["new"] if question: args[0]["owner"].value $=$ # Create prompt if not memory: prompt $=$ " Question: " $^ +$ question $^ +$ " 1 Answer:" else: template $=$ "Question: {} Answer: {}." prompt $=$ " ".join( [ template.format(memory[i][0], memory[i][1])

for i in range(len(memory)) ] ) + " Question: " $^ +$ question $^ +$ " Answer:"

# Generate text   
inputs $=$ blip_processor(image, text $\equiv$ prompt, return_tensors $=$ "pt")   
inputs $=$ inputs.to(device, torch.float16)   
generated_ids $=$ model.generate(\*\*inputs, max_new_tokens $= 1 \Theta \Theta$ )   
generated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   
)   
generated_text $=$ generated_text[0].strip().split("Question")[0]

# Update memory memory.append((question, generated_text))

# Assign to output   
output.append_display_data(HTML("<b>USER:</b> " $^ +$ question))   
output.append_display_data(HTML("<b>BLIP-2:</b> " $^ +$ generated_text))   
output.append_display_data(HTML("<br>"))   
# Prepare widgets   
in_text $=$ widgets.Text()   
in_text.continuous_update $=$ False   
in_text.observe(text_eventhandler, "value")   
output $=$ widgets.Output()   
memory $=$ []   
# Display chat box   
display( widgets.VBox( children $| =$ [output, in_text], layout=widgets.Layout(display $^ { \prime } =$ "inline-flex", flex_flow="column  
reverse"), )   
) UsER: Write down what you see in this picture. BLIP-2: A sports car driving on the road at sunset USER:What would it cost me to drive that car? BLIP-2: \$1,000,000   
USER: Why are sports cars expensive?   
BLIP-2: Because they're fast.

It seems that we can continue the conversation and ask a bunch of questions. Using this chat-based approach, we essentially created a chatbot that can reason about images!

# Summary

In this chapter, we explored various methods for making LLMs multimodal by bridging the gap between textual and visual representations. We started by discus‐ sing Transformers for vision, which are models that convert images into numerical representations. This was achieved through the use of image encoders and patch embeddings, which allow the model to process images at various scales.

We then explored the creation of embedding models that can convert both images and text to numerical representations using CLIP. We saw how CLIP uses contrastive learning to align image and text embeddings in a shared space, allowing for tasks like zero-shot classification, clustering, and search. The chapter also introduced Open‐ CLIP, an open source variant of CLIP that is easy to use for multimodal embedding tasks.

Finally, we explored how text generation models could be made multimodal and dived into the BLIP-2 model. The core idea of these multimodal text generation models involves projecting visual features from input images to text embeddings that can be used by LLMs. We saw how this model could be used for image captioning and multimodal chat-based prompting, where both modalities are combined to generate responses. Overall, this chapter highlighted the power of multimodality in LLMs and demonstrated its applications in various areas such as image captioning, search, and chat-based prompting.

In Part III of the book, we will cover training and fine-tuning techniques. In Chap‐ ter 10, we will explore how to create and fine-tune a text embedding model, which is a core technology that drives many language modeling applications. This next chapter serves as an introduction into both training and fine-tuning language models.

# Training and Fine-Tuning Language Models

# Creating Text Embedding Models

Text embedding models lie at the foundation of many powerful natural language processing applications. They lay the groundwork for empowering already impres‐ sive technologies such as text generation models. We have already used embedding models throughout this book in a number of applications, such as supervised classifi‐ cation, unsupervised classification, semantic search, and even giving memory to text generation models like ChatGPT.

It is nearly impossible to overstate the importance of embedding models in the field as they are the driving power behind so many applications. As such, in this chapter, we will discuss a variety of ways that we can create and fine-tune an embedding model to increase its representative and semantic power.

Let’s start by discovering what embedding models are and how they generally work.

# Embedding Models

Embeddings and embedding models have already been discussed in quite a number of chapters (Chapters 4, 5, and 8) thereby demonstrating their usefulness. Before going into training such a model, let’s recap what we have learned with embedding models.

Unstructured textual data by itself is often quite hard to process. They are not values we can directly process, visualize, and create actionable results from. We first have to convert this textual data to something that we can easily process: numeric representations. This process is often referred to as embedding the input to output usable vectors, namely embeddings, as shown in Figure 10-1.

![## Image Analysis: 19cdb3b3d3ecdd0195aecb0dcf193a03ed19c48fb15742f8a76fc43ac9487b06.jpg

**Conceptual Understanding:**
This image conceptually represents the process of 'embedding' textual data. Its main purpose is to visually explain how raw 'Textual input' (like documents, sentences, or phrases) is transformed into a 'Numerical representation' (also called an 'Embedding') using an 'Embedding model.' It conveys the idea that human-readable text needs to be converted into a mathematical format for computational processing, allowing machines to understand and work with language data.

**Content Interpretation:**
The image illustrates a core concept in natural language processing and machine learning: the process of generating embeddings. It explicitly shows the transformation of 'Textual input' into a 'Numerical representation' through an 'Embedding model.' The input is represented as a document, while the output is shown as a sequence of numerical slots, implying a vector or array of numbers. The 'Embedding model' acts as the computational component responsible for this transformation. The small arrow icon on the embedding model suggests a mapping or projection from one space (text) to another (numerical vector space). The significance lies in showing how unstructured textual data is converted into a structured, quantifiable format suitable for algorithmic processing, enabling tasks like similarity search, classification, and clustering.

**Key Insights:**
The main takeaway from this image is the foundational principle of how textual data is prepared for machine learning models. It teaches that 'Textual input' is not directly usable by most algorithms and must be converted into a 'Numerical representation' or 'Embedding.' The 'Embedding model' is the key component performing this transformation. This conversion allows for the semantic meaning and contextual relationships within the text to be captured in a dense, continuous vector space, which is critical for various downstream NLP tasks. The image highlights a unidirectional flow, indicating that the embedding process is a one-way transformation from text to numerical vectors for subsequent analysis.

**Document Context:**
This image serves as a fundamental illustration for the 'Embedding Models' section, immediately following the explanatory text: 'Figure 10-1. We use an embedding model to convert textual input, such as documents, sentences, and phrases, to numerical representations, called embeddings.' The image visually reinforces and clarifies this key concept, providing a diagrammatic representation of the textual description. It visually explains the core function of an embedding model by showing its input (text), the transformation process (the model itself), and its output (numerical embeddings). It is crucial for understanding the foundational mechanism discussed in the surrounding document text.

**Summary:**
The image illustrates the process of converting textual input into a numerical representation, known as an embedding. It begins with 'Textual input,' visually represented as a document with multiple lines of text. This input is then fed into an 'Embedding model,' which is depicted as a light blue rectangular box. From the 'Embedding model,' an output is generated, which is labeled as 'Embedding' and further described as 'Numerical representation.' This numerical output is visually represented by a series of five gray, empty rectangular cells, signifying discrete numerical values. The flow is strictly sequential, moving from left to right, showing a clear transformation from human-readable text to a machine-understandable numerical format. The small icon with radiating arrows on the 'Embedding model' box suggests the transformation or mapping of data into a different space, typical of embedding processes.](images/19cdb3b3d3ecdd0195aecb0dcf193a03ed19c48fb15742f8a76fc43ac9487b06.jpg)
Figure 10-1. We use an embedding model to convert textual input, such as documents, sentences, and phrases, to numerical representations, called embeddings.

This process of embedding the input is typically performed by an LLM, which we refer to as an embedding model. The main purpose of such a model is to be as accurate as possible in representing the textual data as an embedding.

However, what does it mean to be accurate in representation? Typically, we want to capture the semantic nature—the meaning—of documents. If we can capture the core of what the document communicates, we hope to have captured what the document is about. In practice, this means that we expect vectors of documents that are similar to one another to be similar, whereas the embeddings of documents that each discuss something entirely different should be dissimilar. We’ve seen this idea of semantic similarity several times already in this book, and it is visualized in Figure 10-2. This figure is a simplified example. While two-dimensional visualization helps illustrate the proximity and similarity of embeddings, these embeddings typically reside in high-dimensional spaces.

![## Image Analysis: 594a37f946c963cbc4d050d932575da43fa013e4d5be43d2d1e75e2a9943288b.jpg

**Conceptual Understanding:**
The image conceptually represents a 'semantic space' or an 'embedding space' in two dimensions. Its main purpose is to illustrate the principle of 'semantic similarity.' The core message conveyed is that textual data (phrases, in this case) that share similar meanings or contexts are located closer to each other in this multi-dimensional space, even if their exact wording differs. Conversely, phrases with unrelated meanings are positioned further apart. The image demonstrates how embedding models work by mapping words or phrases into numerical vectors such that their geometric proximity reflects their semantic relatedness. This visualization helps explain how machines can 'understand' and group text based on meaning rather than just keywords.

**Content Interpretation:**
The image visually represents the concept of semantic similarity in a two-dimensional embedding space. It illustrates how different textual phrases are mapped to points in this space such that phrases with similar meanings or contexts are located in closer proximity to each other. The image demonstrates three primary clusters of semantic relationships: 1. **Skincare Product Reviews**: The phrases 'This acne cream cleared my skin' and 'This horrible lotion aggravated my breakouts' are positioned closely on the left side of the grid, indicating their shared topic of skincare products, despite their opposing sentiments. 2. **Vacuum Cleaner Performance Reviews**: The phrases 'This vacuum cleans efficiently' and 'The weak suction left dirt behind' are located close to each other in the top-right quadrant, reflecting their common subject of vacuum cleaner performance. 3. **Shipping Experience Reviews**: The phrases 'My order was late' and 'Extremely fast shipping' are grouped together in the bottom-right quadrant, signifying their shared theme of shipping experiences. The significance of this arrangement is that it visually confirms the idea that embedding models can capture nuanced semantic relationships. Even though some phrases within a cluster express opposite sentiments (e.g., 'cleared my skin' vs. 'aggravated my breakouts'), their proximity in the semantic space shows they are related by topic, differentiating them from phrases about entirely different subjects like shipping or vacuum cleaners. The placement of 'This acne cream cleared my skin' relatively higher than 'This horrible lotion aggravated my breakouts' suggests a potential positive-negative axis within the skincare topic. Similarly, 'This vacuum cleans efficiently' is positioned to the left and slightly below 'The weak suction left dirt behind,' and 'Extremely fast shipping' is below and to the right of 'My order was late,' further hinting at how sentiment or specific aspects of a topic might influence precise positioning within a cluster.

**Key Insights:**
The main takeaway from this image is a clear visualization of semantic similarity as represented by embedding models. It teaches that: 1. **Meaning is Quantifiable**: Textual data, regardless of exact wording, can be mapped into a numerical space where meaning is preserved and represented by proximity. 2. **Topical Clustering**: Phrases pertaining to the same general topic (e.g., skincare, vacuum cleaners, shipping) will naturally cluster together in the semantic space, even if they express different sentiments. For example, 'This acne cream cleared my skin' and 'This horrible lotion aggravated my breakouts' are grouped because both relate to skincare products. 3. **Sentiment/Specificity Differentiation**: Within a topical cluster, phrases with opposing sentiments or specific details are still distinct but closely related. For instance, 'This vacuum cleans efficiently' and 'The weak suction left dirt behind' are close, but their relative positions subtly differentiate their positive and negative evaluations of a vacuum's performance. 4. **Foundation for AI Applications**: This spatial representation of meaning is foundational for many AI/ML applications, allowing algorithms to understand and process natural language more effectively. The specific text elements like 'This acne cream cleared my skin' being near 'This horrible lotion aggravated my breakouts' provide direct evidence for topical clustering, while 'This vacuum cleans efficiently' being separated from 'The weak suction left dirt behind' (but still close) illustrates the differentiation within a topic.

**Document Context:**
This image directly supports the document's section on 'Embedding Models' and the subsequent text explaining 'The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in n-dimensional space (two dimensions are illustrated here).' It serves as a visual demonstration of this core concept. By showing distinct clusters of semantically related phrases (e.g., skincare, vacuum performance, shipping) in a 2D space, the image provides concrete examples of how abstract textual meanings can be represented numerically. This helps readers grasp how embedding models translate human language into a quantifiable form, making it possible to measure the similarity between different pieces of text. It illustrates the fundamental principle that enables applications like recommendation systems, search relevance, and sentiment analysis.

**Summary:**
The image displays a two-dimensional grid, representing a semantic space, with several grey, rounded rectangular text boxes positioned at various coordinates. Each text box contains a short phrase. The grid itself has evenly spaced horizontal and vertical lines, creating squares. There are six text boxes in total. The arrangement of the text boxes illustrates the concept of semantic similarity, where phrases with related meanings are located closer to each other in this space. For example, phrases about skincare are clustered together on the left side, while phrases about vacuum cleaners and shipping experiences are clustered on the right side. This visual representation helps to understand how embedding models map textual data into a numerical space based on their underlying meaning, allowing for the identification of semantic relationships.](images/594a37f946c963cbc4d050d932575da43fa013e4d5be43d2d1e75e2a9943288b.jpg)
Figure 10-2. The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in $n$ -dimensional space (two dimensions are illustra‐ ted here).

An embedding model, however, can be trained for a number of purposes. For example, when we are building a sentiment classifier, we are more interested in the sentiment of texts than their semantic similarity. As illustrated in Figure 10-3, we can fine-tune the model such that documents are closer in n-dimensional space based on their sentiment rather than their semantic nature.

Either way, an embedding model aims to learn what makes certain documents similar to one another and we can guide this process. By presenting the model with enough examples of semantically similar documents, we can steer toward semantics whereas using examples of sentiment would steer it in that direction.

![## Image Analysis: 007d466cd083af023b5ea2e45744797488b178ba90b12b334fb546ee1be70887.jpg

**Conceptual Understanding:**
This image conceptually represents an embedding space for text data, specifically customer reviews. Its main purpose is to illustrate the capability of an embedding model to identify and group text based on sentiment similarity. The key idea communicated is that an embedding model can learn to position texts with similar emotional tones (e.g., positive or negative reviews) closer to each other in a multi-dimensional space, effectively separating them from texts with different tones.

**Content Interpretation:**
The image shows a conceptual two-dimensional embedding space where customer reviews are plotted based on their underlying sentiment. The primary concept illustrated is how an embedding model can map text data into a vector space such that semantically or sentimentally similar items are grouped together. Positive reviews ('This vacuum cleans efficiently', 'This acne cream cleared my skin', 'Extremely fast shipping') are depicted in green and clustered on one side of the grid, indicating their similarity in positive sentiment. Negative reviews ('The suction left dirt behind', 'It aggravated my breakouts', 'My order was late') are depicted in red and clustered on the opposite side of the grid, signifying their shared negative sentiment and dissimilarity to the positive reviews. This visual separation demonstrates the model's ability to differentiate and cluster content based on sentiment.

**Key Insights:**
The main takeaway from this image is that embedding models can effectively capture and represent sentiment. By converting textual reviews into points in a multi-dimensional space (visualized here in 2D), the model groups similar sentiments together. The image illustrates that positive sentiments, as exemplified by the green boxes containing the text 'This vacuum cleans efficiently', 'This acne cream cleared my skin', and 'Extremely fast shipping', form a distinct cluster. Conversely, negative sentiments, represented by the red boxes with text 'The suction left dirt behind', 'It aggravated my breakouts', and 'My order was late', form another separate cluster. This clear segregation provides strong evidence that embedding models can differentiate between sentiments, leading to insights that can be used for tasks like sentiment analysis, product recommendations, or customer feedback categorization.

**Document Context:**
This image directly supports the document's narrative in the 'Embedding Models' section by visually demonstrating how an embedding model can be trained to focus on sentiment similarity. The accompanying text, 'In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green),' is explicitly illustrated by the arrangement of the text boxes on the grid. It serves as a concrete example of how embedding spaces can reflect abstract concepts like sentiment, enhancing the reader's comprehension of embedding model capabilities beyond just semantic similarity.

**Summary:**
The image displays a grid or a two-dimensional plot, populated with six rectangular text boxes, each containing a customer review. These reviews are color-coded to indicate sentiment: green for positive and red for negative. The grid implicitly represents an embedding space where text data points are located based on their characteristics. On the left side of the grid, three green boxes are clustered together, representing positive reviews. These include: 'This vacuum cleans efficiently', 'This acne cream cleared my skin', and 'Extremely fast shipping'. On the right side of the grid, three red boxes are clustered, representing negative reviews. These are: 'The suction left dirt behind', 'It aggravated my breakouts', and 'My order was late'. The spatial separation clearly illustrates that reviews with similar sentiments are positioned closer to each other, while reviews with differing sentiments (positive vs. negative) are positioned far apart on the grid.](images/007d466cd083af023b5ea2e45744797488b178ba90b12b334fb546ee1be70887.jpg)
Figure 10-3. In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green).

There are many ways in which we can train, fine-tune, and guide embedding mod‐ els, but one of the strongest and most widely used techniques is called contrastive learning.

# What Is Contrastive Learning?

One major technique for both training and fine-tuning text embedding models is called contrastive learning. Contrastive learning is a technique that aims to train an embedding model such that similar documents are closer in vector space while dissimilar documents are further apart. If this sounds familiar, it’s because it’s very similar to the word2vec method from Chapter 2. We have seen this notion previously in Figures 10-2 and 10-3.

The underlying idea of contrastive learning is that the best way to learn and model similarity/dissimilarity between documents is by feeding a model examples of similar and dissimilar pairs. In order to accurately capture the semantic nature of a docu‐ ment, it often needs to be contrasted with another document for a model to learn what makes it different or similar. This contrasting procedure is quite powerful and relates to the context in which documents are written. This high-level procedure is demonstrated in Figure 10-4.

![## Image Analysis: 583d4cd94b8f6b0a5a40bfc205103be02798a184c877898cf69a292fa17816f6.jpg

**Conceptual Understanding:**
The image conceptually represents a simplified data flow diagram illustrating the function of an 'Embedding model' in the context of document comparison. Its main purpose is to show how two distinct inputs, 'Document A' and 'Document B', are processed by this model to produce an output indicating their 'Similar/dissimilar' relationship. The key idea communicated is the core operation of a system designed to evaluate the semantic or structural relationship between pairs of documents, which is a fundamental aspect of contrastive learning and information retrieval.

**Content Interpretation:**
This image illustrates the core mechanism of an embedding model in the context of document comparison. It shows how the model takes two documents as input, processes them, and then outputs a judgment on their similarity or dissimilarity. The key processes shown are: 1. Input reception of 'Document A' and 'Document B'. 2. Processing by the 'Embedding model'. 3. Output generation indicating 'Similar/dissimilar' status. The significance lies in demonstrating a fundamental operation where document pairs are evaluated to determine their relationship, which is a cornerstone of contrastive learning. All extracted text elements, including 'Input', 'Document A', 'Document B', 'Embedding model', 'Output', and 'Similar/dissimilar', directly support this interpretation by detailing the components and their roles in this comparative process.

**Key Insights:**
The main takeaway from this image is that an 'Embedding model' is designed to perform a comparison between two distinct inputs, specifically 'Document A' and 'Document B', to ascertain their relationship. The ultimate goal of this comparison, as indicated by the 'Output: Similar/dissimilar', is to classify whether the documents share common attributes or diverge. This highlights the foundational principle of contrastive learning, where the model learns to differentiate or group items based on their inherent characteristics. The textual evidence—'Document A', 'Document B', 'Embedding model', and 'Similar/dissimilar'—collectively demonstrates that the model acts as a comparator, processing two inputs to yield a judgment on their similarity.

**Document Context:**
This image directly supports the document's section 'What Is Contrastive Learning?' by providing a visual representation of the concept. The text immediately following the image states, 'Figure 10-4. Contrastive learning aims to teach an embedding model whether documents are similar or dissimilar. It does so by presenting groups of documents to a model that are similar or dissimilar to a certain degree.' The image perfectly illustrates this statement, showing 'Document A' and 'Document B' as the 'groups of documents' presented to an 'Embedding model' to determine if they are 'Similar/dissimilar'. It serves as a clear, high-level diagram explaining the operational input-process-output flow of an embedding model within a contrastive learning framework, making the abstract concept more concrete for the reader.

**Summary:**
The image illustrates a fundamental process in contrastive learning, showing how an embedding model evaluates the relationship between two input documents. The process begins with 'Input', which consists of two distinct documents labeled 'Document A' and 'Document B'. These two documents are fed into a central processing unit, depicted as a light blue rectangle labeled 'Embedding model'. The 'Embedding model' processes both documents to determine their relationship. Following this processing, the system produces an 'Output', which is explicitly stated as 'Similar/dissimilar'. This output signifies the model's assessment of whether 'Document A' and 'Document B' are alike or different. The overall flow is linear, starting from the distinct inputs, through the model, to a binary classification output.](images/583d4cd94b8f6b0a5a40bfc205103be02798a184c877898cf69a292fa17816f6.jpg)
Figure 10-4. Contrastive learning aims to teach an embedding model whether docu‐ ments are similar or dissimilar. It does so by presenting groups of documents to a model that are similar or dissimilar to a certain degree.

Another way to look at contrastive learning is through the nature of explanations. A nice example of this is an anecdotal story of a reporter asking a robber “Why did you rob a bank?” to which he answers, “Because that is where the money is.”1 Although a factually correct answer, the intent of the question was not why he robs banks specifically but why he robs at all. This is called contrastive explanation and refers to understanding a particular case, “Why P?” in contrast to alternatives, “Why P and not $\mathbf { Q } ? ^ { \mathfrak { n } _ { 2 } }$ In the example, the question could be interpreted in a number of ways and may be best modeled by providing an alternative: “Why did you rob a bank (P) instead of obeying the law (Q)?”

The importance of alternatives to the understanding of a question also applies to how an embedding learns through contrastive learning. By showing a model similar and dissimilar pairs of documents, it starts to learn what makes something similar/dissim‐ ilar and more importantly, why.

For example, you could teach a model to understand what a dog is by letting it find features such as “tail,” “nose,” “four legs,” etc. This learning process can be quite difficult since features are often not well-defined and can be interpreted in a number of ways. A being with a “tail,” “nose,” and “four legs” can also be a cat. To help the model steer toward what we are interested in, we essentially ask it, “Why is this a dog and not a cat?” By providing the contrast between two concepts, it starts to learn the features that define the concept but also the features that are not related. We get more information when we frame a question as a contrast. We further illustrate this concept of contrastive explanation in Figure 10-5.

![## Image Analysis: 549eeeec4a81d134c751b43d2d97be27034a67e800558c337bb0bf906ccba24e.jpg

**Conceptual Understanding:**
The image conceptually represents the process of identifying or classifying an object (specifically an animal) based on its distinguishing characteristics. It highlights the difference between general classification and distinguishing between similar but different categories.

The main purpose of the image is to illustrate how adding *contrastive* features (i.e., features that differentiate between similar categories) enhances the ability to make more precise classifications. It shows that identifying something as "a horse" might use one set of features, but identifying it as "a horse *and not a zebra*" requires an additional, contrastive feature.

Key ideas and concepts being communicated include feature extraction, classification, differentiation, and the fundamental principle of contrastive learning, where understanding differences between similar items improves overall identification accuracy.

**Content Interpretation:**
The image shows two levels of classification thought processes.

The first level is a basic classification: identifying an animal as a "horse" based on common equine characteristics. The evidence for this is the question "Why is this a horse?" and the listed attributes: "four legs", "tail", "fur", "long manes", "gallops", "ear length". These are general characteristics of a horse.

The second level is a more refined, contrastive classification: distinguishing a "horse" from a "zebra". The evidence for this is the question "Why is this a horse and not a zebra?" and the listed attributes: "four legs", "tail", "fur", "long manes", "gallops", "ear length", and crucially, "no stripes".

The significance lies in demonstrating that simply identifying features common to a category might not be enough to differentiate it from similar categories. The addition of a contrastive feature ("no stripes") is what allows for the clear distinction between a horse and a zebra. This implies that for robust classification, models need to learn not just what an object *is*, but also what it *is not* in comparison to similar concepts. The verbatim text for the first question and its six features, followed by the second question and its seven features (including "no stripes"), directly supports this interpretation.

**Key Insights:**
**Main Takeaways:**
1.  **Feature Importance:** Different classification tasks require different sets of features. Simple identification might rely on general traits, while differentiation requires specific, contrastive traits.
2.  **Contrastive Learning Principle:** To distinguish between similar concepts, it's essential to identify features that are present in one and absent (or different) in the other. The "no stripes" feature is the key insight for differentiating a horse from a zebra, beyond their shared characteristics.
3.  **Refined Classification:** Classification becomes more robust and accurate when contrastive information is considered.

**Conclusions/Insights:** The image illustrates the core idea that defining something often benefits from also defining what it *isn't*, especially when faced with highly similar alternatives. This is fundamental to contrastive learning, where the "degrees of similarity" (as mentioned in the document context) are learned by identifying distinguishing characteristics.

**Textual Evidence:** The two questions and their associated lists directly demonstrate this:
*   The first list for "Why is this a horse?" provides common horse characteristics: "four legs", "tail", "fur", "long manes", "gallops", "ear length".
*   The second list for "Why is this a horse and not a zebra?" includes all the common characteristics *plus* "no stripes," which is the critical differentiating feature. This "no stripes" is the textual evidence for the contrastive aspect, directly supporting the principles of contrastive learning.

**Document Context:**
The image directly supports the document section "What Is Contrastive Learning?" and the subsequent text: "When we feed an embedding model different contrasts (degrees of similar‐ ity), it starts to learn what makes things different from one another and thereby the distinctive characteristics of concepts." The image provides a concrete, easy-to-understand example of how an entity (a horse) is first identified by its general features, and then precisely distinguished from a similar entity (a zebra) by a *contrastive* feature. This visualizes the "learning what makes things different from one another" aspect of contrastive learning.

**Summary:**
This image illustrates the concept of identifying and distinguishing objects based on their characteristics, specifically demonstrating the principle behind contrastive learning. It presents two scenarios, each posed as a question followed by a list of relevant attributes. 

The first scenario asks, "Why is this a horse?". It is followed by a series of six characteristics, each enclosed in a rounded rectangular box: "four legs", "tail", "fur", "long manes", "gallops", and "ear length". These attributes define what generally makes an animal a horse.

The second scenario poses a more specific question: "Why is this a horse and not a zebra?". This question is followed by a list of seven characteristics, again in individual rounded rectangular boxes: "four legs", "tail", "fur", "long manes", "gallops", "ear length", and critically, "no stripes". This second list includes all the characteristics from the first list but adds "no stripes", which is the key feature that distinguishes a horse from a zebra.

The image effectively shows that while a set of features can help identify a concept (like "horse"), a more precise and contrastive understanding requires identifying features that differentiate it from closely related concepts (like "zebra"). The addition of "no stripes" in the second set of characteristics highlights how an "embedding model" might learn to discern "degrees of similarity" by focusing on the unique, distinguishing attributes that make concepts different from one another. This visual example concretely supports the idea that understanding *what something isn't* (a zebra, in this case) is as crucial as understanding *what it is* (a horse) for robust classification and comprehension of distinctive characteristics.](images/549eeeec4a81d134c751b43d2d97be27034a67e800558c337bb0bf906ccba24e.jpg)
Figure 10-5. When we feed an embedding model different contrasts (degrees of similar‐ ity), it starts to learn what makes things different from one another and thereby the distinctive characteristics of concepts.

One of the earliest and most popular examples of contrastive learn‐ ing in NLP is actually word2vec, as we discussed in Chapters 1 and 2. The model learns word representations by training on individual words in a sentence. A word close to a target word in a sentence will be constructed as a positive pair whereas randomly sampled words constitute dissimilar pairs. In other words, positive examples of neighboring words are contrasted with randomly selected words that are not neighbors. Although not widely known, it is one of the first major breakthroughs in NLP that leverages contrastive learning with neural networks.

There are many ways we can apply contrastive learning to create text embed‐ ding models but the most well-known technique and framework is sentencetransformers.

# SBERT

Although there are many forms of contrastive learning, one framework that has popularized the technique within the natural language processing community is sentence-transformers.3 Its approach fixes a major problem with the original BERT implementation for creating sentence embeddings, namely its computational overhead. Before sentence-transformers, sentence embeddings often used an archi‐ tectural structure called cross-encoders with BERT.

A cross-encoder allows two sentences to be passed to the Transformer network simultaneously to predict the extent to which the two sentences are similar. It does so by adding a classification head to the original architecture that can output a similarity score. However, the number of computations rises quickly when you want to find the highest pair in a collection of 10,000 sentences. That would require $\mathrm { n } { \cdot } ( \mathrm { n } { - } 1 ) / 2$ $= 4 9 , 9 9 5 , 0 0 0$ inference computations and therefore generates significant overhead. Moreover, a cross-encoder generally does not generate embeddings, as shown in Figure 10-6. Instead, it outputs a similarity score between the input sentences.

A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors, like GloVe.4

![## Image Analysis: f8878b1561c06088bf1757067bb03b3effc446630bdfcc757c13a0090f96da60.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow of a cross-encoder model, specifically utilizing BERT, for the task of determining the semantic 'Similarity' between two input sentences. The main purpose conveyed is to illustrate how two distinct pieces of text ('Sentence A' and 'Sentence B') are combined and jointly processed by a neural network to produce a unified score indicating their relatedness. It highlights the crucial step of concatenating the sentences with a '<SEP>' token before feeding them into the 'BERT' model, which is followed by a 'Classifier' to yield the final '0...1' similarity output.

**Content Interpretation:**
The image displays a cross-encoder architecture, specifically demonstrating how two sentences are processed to determine their similarity using a BERT model. The input consists of two distinct sentences, 'Sentence A' and 'Sentence B', which are tokenized and then combined into a single sequence with a special separator. This combined sequence is then fed into a powerful language model (BERT), followed by a classifier, to produce a similarity score. This architecture is fundamental for tasks requiring fine-grained interaction between two input texts, such as natural language inference or paraphrase detection. The sequence of steps illustrates a common methodology in natural language processing for comparing textual inputs.

**Key Insights:**
The main takeaway from this image is the complete, simultaneous processing of two sentences by a cross-encoder using BERT to determine their similarity. Key insights include: 1. Sentences are tokenized into individual words (e.g., 'My', 'dog', 'is', 'cute' for Sentence A; 'I', 'have', 'a', 'dog' for Sentence B). 2. A special '<SEP>' token is used to clearly delineate the boundary between the two sentences when they are concatenated. 3. The entire concatenated sequence of tokens is processed together by a 'BERT' model, allowing for deep, bidirectional interactions between all tokens from both sentences. 4. Following BERT, a 'Classifier' component interprets BERT's output to make a final prediction. 5. The output is a single 'Similarity' score, ranging from '0...1', indicating the degree of relatedness between the input sentences. This architecture ensures that the model considers the full context of both sentences simultaneously before making a similarity judgment.

**Document Context:**
This image directly illustrates the 'architecture of a cross-encoder' as referenced in the document context, specifically under the 'SBERT' section. It visually explains the process described in the accompanying text: 'Both sentences are concatenated, separated with a <SEP> token, and fed to the model simultaneously.' The diagram visually confirms this by showing 'Sentence A' and 'Sentence B' being tokenized, then joined with '<SEP>', and subsequently processed together by 'BERT' and a 'Classifier' to yield a 'Similarity' score between '0...1'. This figure is crucial for understanding how cross-encoders, particularly those based on BERT, function in comparing two sentences.

**Summary:**
The image illustrates the architecture of a cross-encoder model designed for determining sentence similarity. It begins with two input sentences, 'Sentence A' and 'Sentence B'. 'Sentence A' is tokenized into individual words: 'My', 'dog', 'is', 'cute'. 'Sentence B' is tokenized into 'I', 'have', 'a', 'dog'. These tokenized sentences are concatenated sequentially, separated by a special '<SEP>' token. This combined sequence of tokens ('My', 'dog', 'is', 'cute', '<SEP>', 'I', 'have', 'a', 'dog') is then fed into a 'BERT' model. The output from the 'BERT' model is subsequently passed to a 'Classifier'. Finally, the 'Classifier' produces a 'Similarity' score, which is a numerical value ranging from '0...1', indicating the degree of similarity between 'Sentence A' and 'Sentence B'. The overall flow demonstrates how a cross-encoder processes two sentences together to predict a single similarity score.](images/f8878b1561c06088bf1757067bb03b3effc446630bdfcc757c13a0090f96da60.jpg)
Figure 10-6. The architecture of a cross-encoder. Both sentences are concatenated, sepa‐ rated with $\alpha < S E P >$ token, and fed to the model simultaneously.

Instead, the authors of sentence-transformers approached the problem differently and searched for a method that is fast and creates embeddings that can be compared semantically. The result is an elegant alternative to the original cross-encoder archi‐ tecture. Unlike a cross-encoder, in sentence-transformers the classification head is dropped, and instead mean pooling is used on the final output layer to generate an embedding. This pooling layer averages the word embeddings and gives back a fixed dimensional output vector. This ensures a fixed-size embedding.

The training for sentence-transformers uses a Siamese architecture. In this archi‐ tecture, as visualized in Figure 10-7, we have two identical BERT models that share the same weights and neural architecture. These models are fed the sentences from which embeddings are generated through the pooling of token embeddings. Then, models are optimized through the similarity of the sentence embeddings. Since the weights are identical for both BERT models, we can use a single model and feed it the sentences one after the other.

![## Image Analysis: b45b1ff0888784dfa58438ac50b74a6504486d823b39361c8f49bfad4bb6eeab.jpg

**Conceptual Understanding:**
The image conceptually represents the architecture of a Siamese neural network, specifically a bi-encoder, applied to the task of generating sentence embeddings. The main purpose of this model is to transform two distinct input sentences into dense vector representations (embeddings) in a way that allows for meaningful comparison, typically to determine their semantic similarity. The key idea being communicated is how a pre-trained language model like BERT can be adapted and fine-tuned in a dual-encoder setup with shared weights to produce high-quality, comparable sentence-level embeddings, which are then combined and processed further for specific downstream tasks like similarity prediction or classification.

**Content Interpretation:**
The image depicts a Siamese network architecture used in sentence-transformers. This system takes two sentences as input and processes them through two identical BERT models with tied weights to produce dense vector representations (sentence embeddings). The core processes shown are: 1. Input sentences: "My dog is cute" and "I have a dog". 2. Encoding by BERT: Both sentences are processed by BERT, producing token-level representations. 3. Tied weights: The BERT models share parameters, which is crucial for learning comparable embeddings. 4. Token embeddings: Intermediate representations of individual tokens from BERT. 5. Pooling: A mechanism to convert variable-length token embeddings into a fixed-size sentence embedding. 6. Sentence embeddings: The final vector representations for each sentence, labeled 'u' and 'v'. 7. Combination for output: The embeddings 'u', 'v', and their absolute difference '|u - v|' are concatenated. 8. Softmax: A final layer for classification or similarity, indicating the output of the model for a specific task (e.g., predicting similarity). The significance is that by processing sentences in parallel with shared weights and combining their representations, the model learns to quantify the semantic relationship or similarity between sentence pairs.

**Key Insights:**
1. **Parallel Processing of Sentences:** The architecture processes two sentences, "Sentence A" ("My dog is cute") and "Sentence B" ("I have a dog"), concurrently. 2. **Siamese Network Principle:** It employs two identical BERT models, crucially linked by "Tied weights," ensuring that both models learn the same transformation function. This is a defining characteristic of a Siamese network, enabling the comparison of two inputs. 3. **Hierarchical Embedding Generation:** Sentences are first broken down into "Token embeddings" by BERT, which are then aggregated into a single "Sentence embeddings" ('u' for Sentence A, 'v' for Sentence B) via a "Pooling" layer. 4. **Semantic Comparison Focus:** The final output combines the two sentence embeddings ('u', 'v') along with their element-wise absolute difference ('|u - v|'), indicating that the model is designed to analyze relationships and differences between the two input sentences, typically for similarity tasks. 5. **Softmax for Downstream Tasks:** The final "Softmax" layer implies that the combined representation is used for classification or scoring, often for tasks like determining the similarity score between the two input sentences. These insights are directly derived from the labels "Tied weights", "Token embeddings", "Pooling", "Sentence embeddings" 'u' and 'v', and the final output (u, v, |u - v|) followed by "Softmax".

**Document Context:**
This image serves as a foundational diagram for understanding the architecture of sentence-transformers models, specifically highlighting the use of a Siamese network (bi-encoder). In the context of the "SBERT" section mentioned, it visually explains how SBERT generates semantically meaningful sentence embeddings by leveraging BERT in a Siamese configuration. This architecture is crucial for tasks requiring sentence similarity, semantic search, or clustering, as it allows for efficient computation of similarity scores between sentences by comparing their pre-computed embeddings. The figure directly supports the surrounding text by providing a visual explanation of the model's structure, input processing, and output generation, which is a core component of how SBERT works.

**Summary:**
The image illustrates the architecture of a Siamese network, also known as a bi-encoder, specifically designed for sentence-transformers models. This architecture processes two input sentences, "Sentence A" and "Sentence B", in parallel to generate their respective sentence embeddings. The process begins with each sentence, "My dog is cute" for Sentence A and "I have a dog" for Sentence B, being fed into separate BERT models. Critically, these two BERT models share "Tied weights", ensuring that they learn similar representations for both sentences. After processing by BERT, which produces "Token embeddings" for each word/token in the sentence (represented by the grid of squares), a "Pooling" layer is applied to aggregate these token embeddings into a single fixed-size representation for each sentence. This pooling step results in "Sentence embeddings", denoted as 'u' for Sentence A (represented by red squares) and 'v' for Sentence B (represented by green squares). Finally, these two sentence embeddings, u and v, are combined in a specific manner for downstream tasks. The combination involves forming a triplet: the embedding 'u', the embedding 'v', and the absolute difference between them, '|u - v|'. This combined representation, (u, v, |u - v|), is then passed through a "Softmax" layer, typically for tasks like classification or similarity scoring.](images/b45b1ff0888784dfa58438ac50b74a6504486d823b39361c8f49bfad4bb6eeab.jpg)
Figure 10-7. The architecture of the original sentence-transformers model, which leverages a Siamese network, also called a bi-encoder.

The optimization process of these pairs of sentences is done through loss functions, which can have a major impact on the model’s performance. During training, the embeddings for each sentence are concatenated together with the difference between the embeddings. Then, this resulting embedding is optimized through a softmax classifier.

The resulting architecture is also referred to as a bi-encoder or SBERT for sentenceBERT. Although a bi-encoder is quite fast and creates accurate sentence representa‐ tions, cross-encoders generally achieve better performance than a bi-encoder but do not generate embeddings.

The bi-encoder, like a cross-encoder, leverages contrastive learning; by optimizing the (dis)similarity between pairs of sentences, the model will eventually learn the things that make the sentences what they are.

To perform contrastive learning, we need two things. First, we need data that consti‐ tutes similar/dissimilar pairs. Second, we will need to define how the model defines and optimizes similarity.

# Creating an Embedding Model

There are many methods through which an embedding model can be created but generally, we look toward contrastive learning. This is an important aspect of many embedding models as the process allows it to efficiently learn semantic representations.

However, this is not a free process. We will need to understand how to generate contrastive examples, how to train the model, and how to properly evaluate it.

# Generating Contrastive Examples

When pretraining your embedding model, you will often see data being used from natural language inference (NLI) datasets. NLI refers to the task of investigating whether, for a given premise, it entails the hypothesis (entailment), contradicts it (contradiction), or neither (neutral).

For example, when the premise is $^ { \mathfrak { e } } \mathrm { H e }$ is in the cinema watching Coco” and the hypothesis $^ { \mathfrak { e } } \mathrm { H e }$ is watching Frozen at home,” then these statements are contradictions. In contrast, when the premise is $^ { \mathfrak { e } } \mathrm { H e }$ is in the cinema watching Coco” and the hypothesis “In the movie theater he is watching the Disney movie Coco,” then these statements are considered entailment. This principle is illustrated in Figure 10-8.

![## Image Analysis: 9634d1a72674ca0e4c0c3b0341f4fca1e824448bc6895efd62ff02314e064366.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental process of Natural Language Inference (NLI) and its application in generating contrastive examples for machine learning tasks. 

The main purpose of the image is to illustrate, with a concrete example, how a 'Premise' statement can be evaluated against different 'Hypothesis' statements to determine their logical relationship—specifically, whether the hypothesis is entailed by the premise (a 'Positive example') or contradicts the premise (a 'Negative example'). This demonstrates a structured method for creating labeled data, essential for training models that need to distinguish between semantically similar and semantically contradictory sentence pairs. The overall message is about leveraging NLI for data augmentation in contrastive learning scenarios.

**Content Interpretation:**
The image illustrates the core concepts of Natural Language Inference (NLI), specifically showcasing how a given 'Premise' relates to various 'Hypothesis' statements, resulting in classifications of 'Entailment' or 'Contradiction'.

**Processes Shown:**
1.  **Premise Formulation:** An initial statement serving as the factual or given information.
2.  **Hypothesis Generation:** Development of two distinct statements that are evaluated against the premise.
3.  **Relationship Classification:** Determining the logical relationship between the premise and each hypothesis.
4.  **Example Generation:** Labeling these relationships as either a 'Positive example' (for entailment) or a 'Negative example' (for contradiction).

**Concepts Shown:**
*   **Premise:** The foundational statement from which inferences are drawn.
*   **Hypothesis:** A statement proposed for evaluation against the premise.
*   **Entailment:** A relationship where the hypothesis is logically inferable from the premise.
*   **Contradiction:** A relationship where the hypothesis is logically inconsistent with the premise.
*   **Positive example:** A pair of premise and hypothesis that demonstrates entailment.
*   **Negative example:** A pair of premise and hypothesis that demonstrates contradiction.

**Relationships Shown:**
*   A one-to-many relationship where a single 'Premise' (He is in the cinema watching Coco) can lead to multiple 'Hypothesis' evaluations.
*   Direct logical relationships between the 'Premise' and each 'Hypothesis', resulting in a specific NLI label (Entailment or Contradiction). These labels are explicitly linked to generating 'Positive example' and 'Negative example' pairs, respectively.

**Supporting Evidence from Extracted Text:**
*   The text 'He is in the cinema watching Coco' clearly defines the 'Premise'.
*   The text 'In the movie theater, he is watching the Disney movie Coco' defines the first 'Hypothesis'. Its direct association with 'Entailment' and 'Positive example' indicates a consistent logical relationship.
*   The text 'He is watching Frozen at home' defines the second 'Hypothesis'. Its direct association with 'Contradiction' and 'Negative example' signifies a logically conflicting relationship.
*   The labels 'Premise', 'Hypothesis', 'Entailment', 'Contradiction', 'Positive example', and 'Negative example' explicitly name and categorize these components, leaving no ambiguity about the concepts and relationships being depicted.

**Key Insights:**
The main takeaways from this image are:
1.  **Natural Language Inference (NLI) Basics:** The image effectively demonstrates the core principles of NLI, which involves determining the logical relationship between a 'Premise' and a 'Hypothesis'.
2.  **Categorization of Relationships:** NLI relationships are categorized into 'Entailment' (where the hypothesis is true if the premise is true) and 'Contradiction' (where the hypothesis is false if the premise is true).
3.  **Generation of Contrastive Examples:** The diagram explicitly shows how NLI allows for the creation of 'Positive example' (entailment) and 'Negative example' (contradiction) pairs, which are crucial for contrastive learning methodologies.
4.  **Practical Application for Machine Learning:** The clear distinction between entailment and contradiction, exemplified here, provides a method for generating labeled data to train models that can understand and classify semantic relationships between sentences.

**Evidence for these insights from the extracted text:**
*   The text 'Premise' and 'Hypothesis' establishes the two core components of NLI.
*   The example 'Premise: He is in the cinema watching Coco' provides a concrete starting point.
*   The 'Hypothesis: In the movie theater, he is watching the Disney movie Coco' leading to 'Entailment' and 'Positive example' clearly illustrates what constitutes a positive contrastive example.
*   The 'Hypothesis: He is watching Frozen at home' leading to 'Contradiction' and 'Negative example' clearly illustrates what constitutes a negative contrastive example.
*   The explicit labels 'Positive example' and 'Negative example' directly link the NLI classifications to their utility in generating contrastive data, which is essential for training models that require such examples.

**Document Context:**
This image is directly relevant to the 'Generating Contrastive Examples' section of the document, as stated in the preceding text. It visually explains how the inherent structure of Natural Language Inference (NLI) datasets can be leveraged to create specific types of training data: 'negative examples' (contradictions) and 'positive examples' (entailments) for contrastive learning. By presenting a concrete example of a premise leading to both an entailed and a contradictory hypothesis, the image provides a fundamental illustration of the mechanism for generating these contrastive examples. This directly supports the document's argument about utilizing NLI structures for training data creation, making a complex concept readily understandable through a clear, illustrative flow.

**Summary:**
This diagram illustrates the concept of Natural Language Inference (NLI) by demonstrating how a single 'Premise' statement can lead to different 'Hypothesis' classifications: 'Entailment' (a positive example) or 'Contradiction' (a negative example). 

The process begins with the initial statement, labeled 'Premise', which is: 'He is in the cinema watching Coco'. This premise then branches into two distinct 'Hypothesis' statements. 

The first hypothesis, located at the top, is: 'In the movie theater, he is watching the Disney movie Coco'. This hypothesis is derived from the premise and is categorized as 'Entailment', further specified as a 'Positive example'. This indicates that the first hypothesis logically follows or is implied by the premise. 

The second hypothesis, located at the bottom, is: 'He is watching Frozen at home'. This hypothesis is also derived from the premise but is categorized as 'Contradiction', further specified as a 'Negative example'. This indicates that the second hypothesis is logically inconsistent with or directly contradicts the premise. 

Overall, the diagram clearly shows how contrastive examples (positive for entailment, negative for contradiction) can be generated from an NLI framework, which is crucial for training models.](images/9634d1a72674ca0e4c0c3b0341f4fca1e824448bc6895efd62ff02314e064366.jpg)
Figure 10-8. We can leverage the structure of NLI datasets to generate negative examples (contradiction) and positive examples (entailments) for contrastive learning.

If you look closely at entailment and contradiction, then they describe the extent to which two inputs are similar to one another. As such, we can use NLI datasets to generate negative examples (contradictions) and positive examples (entailments) for contrastive learning.

The data that we are going to be using throughout creating and fine-tuning embedding models is derived from the General Language Understanding Evaluation benchmark (GLUE). This GLUE benchmark consists of nine language understanding tasks to evaluate and analyze model performance.

One of these tasks is the Multi-Genre Natural Language Inference (MNLI) corpus, which is a collection of 392,702 sentence pairs annotated with entailment (contradic‐ tion, neutral, entailment). We will be using a subset of the data, 50,000 annotated sentence pairs, to create a minimal example that does not need to be trained for hours on end. Do note, though, that the smaller the dataset, the more unstable training or fine-tuning an embedding model is. If possible, larger datasets are preferred assuming it is still quality data:

from datasets import load_dataset

# Load MNLI dataset from GLUE   
# $\Theta =$ entailment, $ { 1 } =$ neutral, $2 \ =$ contradiction   
train_dataset $=$ load_dataset( "glue", "mnli", split="train"   
).select(range(50_000))   
train_dataset $=$ train_dataset.remove_columns("idx")

Next, we take a look at an example:

# dataset[2]

{'premise': 'One of our number will carry out your instructions minutely.', 'hypothesis': 'A member of my team will execute your orders with immense precision.   
'label': 0}

This shows an example of an entailment between the premise and the hypothesis as they are positively related and have near identical meanings.

# Train Model

Now that we have our dataset with training examples, we will need to create our embedding model. We typically choose an existing sentence-transformers model and fine-tune that model, but in this example, we are going to train an embedding from scratch.

This means that we will have to define two things. First, a pretrained Transformer model that serves as embedding individual words. We will use the BERT base model (uncased) as it is a great introduction model. However, many others exist that also have been evaluated using sentence-transformers. Most notably, microsoft/ mpnet-base often gives good results when used as a word embedding model.

from sentence_transformers import SentenceTransformer

By default, all layers of an LLM in sentence-transformers are trainable. Although it is possible to freeze certain layers, it is generally not advised since the performance is often better when unfreezing all layers.

Next, we will need to define a loss function over which we will optimize the model. As mentioned at the beginning of this section, one of the first instances of sentencetransformers uses softmax loss. For illustrative purposes, we are going to be using that for now, but we will go into more performant losses later on:

from sentence_transformers import losses

# Define the loss function. In softmax loss, we will also need to explicitly   
set the number of labels.   
train_loss $=$ losses.SoftmaxLoss( model $=$ embedding_model, sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimen   
sion(), num_label $; = 3$   
)

Before we train our model, we define an evaluator to evaluate the model’s perfor‐ mance during training, which also determines the best model to save.

We can perform evaluation of the performance of our model using the Semantic Textual Similarity Benchmark (STSB). It is a collection of human-labeled sentence pairs, with similarity scores between 1 and 5.

We use this dataset to explore how well our model scores on this semantic similarity task. Moreover, we process the STSB data to make sure all values are between 0 and 1:

from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# Create an embedding similarity evaluator for STSB   
val_sts $=$ load_dataset("glue", "stsb", split="validation")   
evaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts["sentence1"], sentences2=val_sts["sentence2"], scores $\mathbf { \Psi } _ { 1 } =$ [score/5 for score in val_sts["label"]], main_similarity="cosine",   
)

Now that we have our evaluator, we create SentenceTransformerTrainingArgu ments, similar to training with Hugging Face Transformers (as we will explore in the next chapter):

from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments

# Define the training arguments

args $=$ SentenceTransformerTrainingArguments( output_dir $\ ' =$ "base_embedding_model", num_train_epochs $\scriptstyle \mathbf { \alpha } = { \mathbf { \beta } }$ , per_device_train_batch_size $= 3 2$ , per_device_eval_batch_size $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , warmup_steps $\mathord { \left. \vert { \left. ^ { } = 1 \Theta \Theta \right.}  \right. }$ , fp1 $\hat { \bf \Phi } = \overrightharpoon { \bf { \Phi } }$ True, eval_step ${ \displaystyle \ i = 1 0 0 }$ , logging_step $\begin{array} { r l } { : = } & { { } } \end{array}$ ,   
)

Of note are the following arguments:

num_train_epochs The number of training rounds. We keep this at 1 for faster training but it is generally advised to increase this value.

per_device_train_batch_size The number of samples to process simultaneously on each device (e.g., GPU or CPU) during evaluation. Higher values generally means faster training.

per_device_eval_batch_size

The number of samples to process simultaneously on each device (e.g., GPU or CPU) during evaluation. Higher values generally means faster evaluation.

warmup_steps

The number of steps during which the learning rate will be linearly increased from zero to the initial learning rate defined for the training process. Note that we did not specify a custom learning rate for this training process.

fp16

By enabling this parameter we allow for mixed precision training, where compu‐ tations are performed using 16-bit floating-point numbers (FP16) instead of the default 32-bit (FP32). This reduces memory usage and potentially increases the training speed.

Now that we have defined our data, embedding model, loss, and evaluator, we can start training our model. We can do that using SentenceTransformerTrainer:

from sentence_transformers.trainer import SentenceTransformerTrainer

# Train embedding model   
trainer $=$ SentenceTransformerTrainer( model embedding_model, args $=$ args, train_dataset $\Bumpeq$ train_dataset, loss $=$ train_loss, evaluator $=$ evaluator   
)   
trainer.train()

After training our model, we can use the evaluator to get the performance on this single task:

# Evaluate our trained model evaluator(embedding_model)

{'pearson_cosine': 0.5982288436666162, 'spearman_cosine': 0.6026682018489217, 'pearson_manhattan': 0.6100690915500567, 'spearman_manhattan': 0.617732600131989, 'pearson_euclidean': 0.6079280934202278, 'spearman_euclidean': 0.6158926913905742, 'pearson_dot': 0.38364924527804595, 'spearman_dot': 0.37008497926991796, 'pearson_max': 0.6100690915500567, 'spearman_max': 0.617732600131989}

We get several different distance measures. The one we are interested in most is 'pearson_cosine', which is the cosine similarity between centered vectors. It is a value between 0 and 1 where a higher value indicates higher degrees of similarity. We get a value of 0.59, which we consider a baseline throughout this chapter.

![## Image Analysis: 02fe52845e5d0f3c695678a26727075f428c0943fc52f29d64a31b8e42f572ac.jpg

**Conceptual Understanding:**
This image represents a logo or a brand mark. Conceptually, it serves as a visual identity for an entity, possibly a project, organization, or tool. Its main purpose is to provide a recognizable and consistent visual identifier, establishing brand presence rather than conveying specific technical or informational content. It appears to be a stylized green animal, possibly a lemur or squirrel, within a square outline.

**Content Interpretation:**
The image is a logo, specifically a stylized depiction of a green animal resembling a lemur or squirrel, enclosed within a square. The significance is primarily as a visual identifier or brand mark. It does not present any data, trends, or specific technical information. All extracted 'text elements' (or rather, the absence thereof) confirm that its purpose is solely symbolic or branding-related, providing a visual identity rather than conveying detailed content.

**Key Insights:**
The main takeaway from this image is that it is a logo, intended for branding or visual identification. It provides no specific technical or instructional insights directly related to 'evaluating a trained model evaluator'. The insight is purely about the visual identity of the entity presenting the information. There are no textual elements to provide further evidence beyond its visual nature as a logo.

**Document Context:**
The image, a logo featuring a green, stylized animal, serves as a visual identifier or branding element within the document. Given the section title, 'Evaluate our trained model evaluator(embedding_model)', the logo likely represents the entity (e.g., a project, organization, or tool) that developed or is associated with the 'embedding_model' or its evaluation process. It functions as a consistent visual mark, helping to establish brand recognition or differentiate the source of the information. It does not directly contribute to the technical explanation of model evaluation but rather to the overall branding and presentation of the document.

**Summary:**
The image displays a stylized green logo featuring an animal, possibly a lemur or squirrel, with a long, upward-curling tail. The animal is depicted in profile, standing on all fours, and is enclosed within a simple square border. There is no text or alphanumeric content present within the image itself. The overall presentation is that of a brand or organizational emblem. Given its placement in a document context discussing model evaluation, this logo likely serves as a visual identifier for the project, organization, or tool being discussed, offering a consistent brand element rather than conveying specific technical information directly.](images/02fe52845e5d0f3c695678a26727075f428c0943fc52f29d64a31b8e42f572ac.jpg)

Larger batch sizes tend to work better with multiple negative rank‐ ings (MNR) loss as a larger batch makes the task more difficult. The reason for this is that the model needs to find the best match‐ ing sentence from a larger set of potential pairs of sentences. You can adapt the code to try out different batch sizes and get a feeling of its effects.

# In-Depth Evaluation

A good embedding model is more than just a good score on the STSB benchmark! As we observed earlier, the GLUE benchmark has a number of tasks for which we can evaluate our embedding model. However, there exist many more benchmarks that allow for the evaluation of embedding models. To unify this evaluation procedure, the Massive Text Embedding Benchmark (MTEB) was developed.5 The MTEB spans 8 embedding tasks that cover 58 datasets and 112 languages.

To publicly compare state-of-the-art embedding models, a leaderboard was created with the scores of each embedding model across all tasks:

from mteb import MTEB

# Choose evaluation task evaluation $=$ MTEB(tasks $=$ ["Banking77Classification"])

{'Banking77Classification': {'mteb_version': '1.1.2',   
'dataset_revision': '0fd18e25b25c072e09e0d92ab615fda904d66300',   
'mteb_dataset_name': 'Banking77Classification',   
'test': {'accuracy': 0.4926298701298701,   
'f1': 0.49083335791288685,   
'accuracy_stderr': 0.010217785746224237,   
'f1_stderr': 0.010265814957074591,   
'main_score': 0.4926298701298701,   
'evaluation_time': 31.83}}}

This gives us several evaluation metrics for this specific task that we can use to explore its performance.

The great thing about this evaluation benchmark is not only the diversity of the tasks and languages but that even the evaluation time is saved. Although many embedding models exist, we typically want those that are both accurate and have low latency. The tasks for which embedding models are used, like semantic search, often benefit from and require fast inference.

Since testing your model on the entire MTEB can take a couple of hours depending on your GPU, we will use the STSB benchmark throughout this chapter instead for illustration purposes.

![## Image Analysis: e0587241a1f1dc64d7a5a0e89ec5ccf107ec0936a73bcec4ef285784f0f96018.jpg

**Conceptual Understanding:**
The image represents a logo or brand identifier. Its main purpose is to visually identify an organization, project, or product, potentially related to the 'MTEB' context mentioned in the document. The stylized animal, which appears to be a monkey or lemur, enclosed within a square outline, conceptually conveys themes of agility, nature, or a distinct identity for the entity it represents.

**Content Interpretation:**
The image depicts a highly stylized, dark green silhouette of an animal, which appears to be a monkey or lemur, shown in a crouching or walking posture with its head facing right. The animal has a distinctive long, curled tail that extends upwards and then curls back over its body. The entire animal figure is contained within a simple, thin green square outline. There are no processes, concepts, relationships, or systems explicitly shown beyond this singular graphic emblem. There is no data, trends, or specific information presented. Since there is no text in the image, there is no textual evidence to support further interpretations, only visual recognition of a logo.

**Key Insights:**
The main takeaway from this image is that it functions as a visual emblem or logo. Without accompanying text or further context directly within the image, specific lessons, conclusions, or insights beyond its role as a brand identifier cannot be extracted. The image primarily provides visual branding. No textual evidence is available from the image itself to support further insights.

**Document Context:**
Based on the provided document context, "Section: Choose evaluation task evaluation = MTEB(tasks = ["Banking77Classification"])," this logo likely represents the "MTEB" project, organization, or framework. It acts as a visual identifier for the entity involved in evaluation tasks such as "Banking77Classification." While the logo itself does not directly explain the evaluation task or its specifics, it serves to visually brand and anchor the related content, indicating its association with the broader MTEB initiative within the document.

**Summary:**
The image displays a logo featuring a dark green, stylized silhouette of an animal, resembling a monkey or lemur. The animal is depicted in a four-legged posture, looking towards the right, with a long, distinctive tail that curls upwards and then loops over its back. This entire animal figure is enclosed within a simple, thin square outline, also in dark green. There is no text, annotations, process flows, decision points, arrows, metadata, timeline information, headers, or footers present within this image. It is a purely graphic logo.](images/e0587241a1f1dc64d7a5a0e89ec5ccf107ec0936a73bcec4ef285784f0f96018.jpg)

Whenever you are done training and evaluating your model, it is important to restart the notebook. This will clear your VRAM up for the next training examples throughout this chapter. By restart‐ ing the notebook, we can be sure that all VRAM is cleared.

# Loss Functions

We trained our model using softmax loss to illustrate how one of the first sentencetransformers models was trained. However, not only is there a large variety of loss functions to choose from, but softmax loss is generally not advised as there are more performant losses.

Instead of going through every single loss function out there, there are two loss functions that are typically used and seem to perform generally well, namely:

• Cosine similarity • Multiple negatives ranking (MNR) loss

There are many more loss functions to choose from than just those discussed here. For example, a loss like MarginMSE works great for training or fine-tuning a cross-encoder. There are a num‐ ber of interesting loss functions implemented in the sentencetransformers framework.

# Cosine similarity

The cosine similarity loss is an intuitive and easy-to-use loss that works across many different use cases and datasets. It is typically used in semantic textual similarity tasks. In these tasks, a similarity score is assigned to the pairs of texts over which we optimize the model.

Instead of having strictly positive or negative pairs of sentences, we assume pairs of sentences that are similar or dissimilar to a certain degree. Typically, this value lies between 0 and 1 to indicate dissimilarity and similarity, respectively (Figure 10-9).

![## Image Analysis: 3c95c96682f96d245e44e6b766a720a399c0f7f26bc4af91a88dd2d20edbdf5a.jpg

**Conceptual Understanding:**
The image conceptually represents how semantic similarity between sentences can be measured using vector space models and cosine similarity. Each sentence ('Sentence₁', 'Sentence₂', 'Sentence₃') is embedded as a vector originating from the origin. The main purpose of the image is to visually explain that the angle between these sentence vectors directly correlates with their semantic similarity: a smaller angle implies higher similarity, while a larger angle implies lower similarity. It also provides the mathematical formula for cosine similarity to solidify this concept. The key ideas communicated are the representation of sentences as vectors, the use of angular distance as a metric for semantic similarity, and the mathematical definition of cosine similarity.

**Content Interpretation:**
The image illustrates the concept of cosine similarity, specifically in the context of comparing the semantic similarity of sentences. It depicts sentences as vectors originating from a common point in a multi-dimensional space. 'Sentence₁' is a reference vector. 'Sentence₂' is shown to be 'Similar' to 'Sentence₁' by having a smaller angle (θ) between their respective vectors. Conversely, 'Sentence₃' is shown to be 'Dissimilar' to 'Sentence₁' by having a larger angle (θ) between their vectors. The significance of the angles (θ) is that a smaller angle indicates higher similarity, while a larger angle indicates lower similarity. The provided formula, 'Cosine similarity (S₁, S₂) = (||S₁|| ||S₂|| × cos (θ)) / (||S₁|| ||S₂||)', quantifies this relationship by calculating the cosine of the angle between the vectors S₁ and S₂, which ranges from -1 (completely dissimilar) to 1 (identical). The cancellation of '||S₁|| ||S₂||' in the numerator and denominator implies the formula effectively simplifies to cos(θ), highlighting that cosine similarity is solely dependent on the angle between the vectors, not their magnitudes.

**Key Insights:**
The main takeaway from this image is that cosine similarity measures the similarity between two vectors (e.g., sentence embeddings) based on the cosine of the angle between them. A smaller angle (θ) between two sentence vectors, as indicated by the visual relationship between 'Sentence₁' and '(Similar) Sentence₂', signifies greater semantic similarity. Conversely, a larger angle (θ) between two sentence vectors, as shown by 'Sentence₁' and '(Dissimilar) Sentence₃', signifies less semantic similarity. The exact formula 'Cosine similarity (S₁, S₂) = (||S₁|| ||S₂|| × cos (θ)) / (||S₁|| ||S₂||)' mathematically confirms that the similarity is directly proportional to the cosine of the angle, meaning the dot product normalized by the magnitudes. This demonstrates that cosine similarity focuses purely on the orientation of the vectors in space.

**Document Context:**
This image directly supports the document's section on 'Cosine similarity' and explains the fundamental concept behind 'cosine similarity loss'. The accompanying text, 'Figure 10-9. Cosine similarity loss aims to minimize the cosine distance between semantically similar sentences and to maximize the distance between semantically dissimilar sentences,' is perfectly illustrated by this diagram. The diagram visually demonstrates how smaller angles (θ) between vectors correspond to higher similarity (as seen with 'Sentence₂'), and larger angles correspond to lower similarity (as seen with 'Sentence₃'). This visual representation is crucial for understanding why minimizing cosine distance for similar sentences (making the angle smaller, cos(θ) closer to 1) and maximizing it for dissimilar sentences (making the angle larger, cos(θ) closer to -1 or 0) is the objective of cosine similarity loss.

**Summary:**
The image illustrates the concept of cosine similarity, specifically how it applies to sentences represented as vectors in a multi-dimensional space. It shows a Cartesian-like coordinate system with three vectors originating from the origin. The blue vector is labeled "Sentence₁". The green vector is labeled "(Similar) Sentence₂", indicating that it is semantically similar to Sentence₁. The red vector is labeled "(Dissimilar) Sentence₃", indicating it is semantically dissimilar to Sentence₁. The angular separation between vectors is crucial for cosine similarity. There is an orange dotted arc labeled "θ" showing the angle between Sentence₁ and (Similar) Sentence₂. Another black dotted arc, also labeled "θ", shows the angle between Sentence₁ and (Dissimilar) Sentence₃. Visually, the angle between Sentence₁ and Sentence₂ is smaller than the angle between Sentence₁ and Sentence₃. Below the diagram, the formula for Cosine similarity is provided as: "Cosine similarity (S₁, S₂) = (||S₁|| ||S₂|| × cos (θ)) / (||S₁|| ||S₂||)". This formula calculates the cosine of the angle between two vectors, which quantifies their similarity regardless of their magnitude.](images/3c95c96682f96d245e44e6b766a720a399c0f7f26bc4af91a88dd2d20edbdf5a.jpg)
Figure 10-9. Cosine similarity loss aims to minimize the cosine distance between seman‐ tically similar sentences and to maximize the distance between semantically dissimilar sentences.

Cosine similarity loss is straightforward—it calculates the cosine similarity between the two embeddings of the two texts and compares that to the labeled similarity score. The model will learn to recognize the degree of similarity between sentences.

Cosine similarity loss intuitively works best using data where you have pairs of sen‐ tences and labels that indicate their similarity between 0 and 1. To use this loss with our NLI dataset, we need to convert the entailment (0), neutral (1), and contradiction (2) labels to values between 0 and 1. The entailment represents a high similarity between the sentences, so we give it a similarity score of 1. In contrast, since both neutral and contradiction represent dissimilarity, we give these labels a similarity score of 0:

from datasets import Dataset, load_dataset   
# Load MNLI dataset from GLUE   
# $\Theta =$ entailment, $ { 1 } =$ neutral, $2 \ =$ contradiction   
train_dataset $=$ load_dataset( "glue", "mnli", split $=$ "train"   
).select(range(50_000))   
train_dataset $=$ train_dataset.remove_columns("idx")   
# (neutral/contradiction) $\scriptstyle = { \boldsymbol { \mathcal { O } } }$ and (entailment) $= \mathcal { I }$   
mapping $=$ {2: 0, 1: 0, 0:1}   
train_dataset $=$ Dataset.from_dict({ "sentence1": train_dataset["premise"], "sentence2": train_dataset["hypothesis"], "label": [float(mapping[label]) for label in train_dataset["label"]]   
})

As before, we create our evaluator:

from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# Create an embedding similarity evaluator for stsb   
val_sts $=$ load_dataset("glue", "stsb", split="validation")   
evaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts["sentence1"], sentences2=val_sts["sentence2"], scores $=$ [score/5 for score in val_sts["label"]], main_similarity="cosine"   
)

Then, we follow the same steps as before but select a different loss instead:

from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments

# Define model embedding_model $=$ SentenceTransformer("bert-base-uncased")

# Loss function train_loss $=$ losses.CosineSimilarityLoss(model $. =$ embedding_model)

# Define the training arguments   
args $=$ SentenceTransformerTrainingArguments( output_dir="cosineloss_embedding_model", num_train_epochs $^ { , = 1 }$ , per_device_train_batch_siz $\begin{array} { r l } { \mathbf { \epsilon } } & { { } : \mathbf { \sigma } } \\ { \mathbf { \epsilon } } & { { } : \mathbf { \sigma } } \end{array}$ , per_device_eval_batch_size $= 3 2$ ,

warmup_steps $\mathord { \left. \vert { \left. ^ { } = 1 \Theta \Theta \right.}  \right. }$ , fp1 $\dot { \bf \nu } =$ True, eval_steps ${ \tt \Gamma } = 1 0 0$ , logging_step $\quad : = \quad$ , )

# Train model   
trainer $=$ SentenceTransformerTrainer( model $=$ embedding_model, args $=$ args, train_dataset=train_dataset, loss $=$ train_loss, evaluator=evaluator   
)   
trainer.train()

Evaluating the model after training gives us the following score:

# Evaluate our trained model evaluator(embedding_model)

{'pearson_cosine': 0.7222322163831805, 'spearman_cosine': 0.7250508271229599, 'pearson_manhattan': 0.7338163436711481, 'spearman_manhattan': 0.7323479193408869, 'pearson_euclidean': 0.7332716434966307, 'spearman_euclidean': 0.7316999722750905, 'pearson_dot': 0.660366792336156, 'spearman_dot': 0.6624167554844425, 'pearson_max': 0.7338163436711481, 'spearman_max': 0.7323479193408869}

A Pearson cosine score of 0.72 is a big improvement compared to the softmax loss example, which scored 0.59. This demonstrates the impact the loss function can have on performance.

Make sure to restart your notebook so we can explore a more common and perform‐ ant loss, namely multiple negatives ranking loss.

# Multiple negatives ranking loss

Multiple negatives ranking (MNR) loss,6 often referred to as InfoNCE7 or NTXen‐ tLoss,8 is a loss that uses either positive pairs of sentences or triplets that contain a pair of positive sentences and an additional unrelated sentence. This unrelated sentence is called a negative and represents the dissimilarity between the positive sentences.

For example, you might have pairs of question/answer, image/image caption, paper title/paper abstract, etc. The great thing about these pairs is that we can be confident they are hard positive pairs. In MNR loss (Figure 10-10), negative pairs are construc‐ ted by mixing a positive pair with another positive pair. In the example of a paper title and abstract, you would generate a negative pair by combining the title of a paper with a completely different abstract. These negatives are called in-batch negatives and can also be used to generate the triplets.

![## Image Analysis: 081ec99e11d77a7aeb81036ea404436f86df48ce9744c87f67c11845ace76cbc.jpg

**Conceptual Understanding:**
This image conceptually represents the objective of a 'Multiple negatives ranking loss' function within a multi-dimensional embedding space (simplified to 2D for visualization). The main purpose of the diagram is to visually explain how such a loss function works: by adjusting the positions of text embeddings so that related pairs (e.g., questions and their correct answers) are brought closer together, while unrelated pairs (e.g., questions and incorrect answers) are pushed farther apart. The key ideas communicated are distance, semantic similarity (proximity), dissimilarity (distance), the concept of an embedding space, and the mechanism by which a ranking loss function uses 'negatives' (unrelated answers) to learn better, more discriminative representations for text data.

**Content Interpretation:**
The image illustrates the core mechanism of a ranking loss in machine learning, specifically for tasks like information retrieval or question-answering, where the goal is to embed text items (questions, answers) into a vector space. 

*   **'Question' (blue circle):** This is the anchor point or the query embedding from which distances are measured. Its position is generally fixed for a given comparison.
*   **'Related answer' (light green circle with arrow to green circle near question):** This pair represents a positive example. The green dotted arrow pointing from the 'Related answer' (light green) to the unlabelled green circle closer to the 'Question' signifies that the loss function aims to pull related answers *closer* to their corresponding questions in the embedding space, thereby minimizing their distance. This makes them more semantically similar.
*   **'Unrelated answer' (light pink/grey circles with arrows to red circles):** These represent negative examples. The red dotted arrows pointing from the 'Unrelated answer' (light pink/grey) to the unlabelled red circles further away illustrate that the loss function aims to push unrelated answers *further away* from the 'Question' embedding, maximizing their distance. This ensures that semantically dissimilar items are indeed distant. 

The text labels 'Question', 'Related answer', and 'Unrelated answer' explicitly identify the entities being compared. The green dotted arrow visually depicts the 'minimize distance' objective, while the red dotted arrows visually depict the 'maximize distance' objective, fully supporting the interpretation of a ranking loss mechanism.

**Key Insights:**
The main takeaways from this image are: 
1.  **Distance-based Similarity:** Semantic similarity (being related) is represented by small distances in an embedding space, while dissimilarity (being unrelated) is represented by large distances. 
2.  **Loss Function Objective:** Ranking loss functions, particularly those using multiple negatives, optimize these distances by actively *reducing* the distance between positive pairs ('Question' and 'Related answer') and *increasing* the distance between negative pairs ('Question' and 'Unrelated answer') relative to a query. 
3.  **Visualization of Embeddings:** The diagram provides a visual representation of the desired state of text embeddings after training with such a loss function, where relevant items are clustered and irrelevant ones are dispersed. 

The textual evidence, including the labels 'Question', 'Related answer', and 'Unrelated answer', combined with the visual cues of the green dotted arrow indicating convergence and the red dotted arrows indicating divergence, directly supports these insights by illustrating the push-and-pull dynamics of the loss function.

**Document Context:**
This image directly illustrates the 'Multiple negatives ranking loss' discussed in the document's section of the same name. The text after the image, 'Figure 10-10. Multiple negatives ranking loss aims to minimize the distance between related pairs of text, such as questions and answers, and maximize the distance between unrelated pairs, such as questions and unrelated answers,' perfectly describes the visual representation. The diagram serves as a clear visual aid to understand the abstract concept of manipulating distances in an embedding space for text-based tasks like question answering and information retrieval, enhancing the reader's comprehension of the technical concept.

**Summary:**
This diagram illustrates the objective of a "Multiple negatives ranking loss" function, which is designed to arrange text representations (embeddings) in a conceptual space based on their relevance. The diagram uses circles to represent these text embeddings and arrows to indicate the desired adjustments in their positions relative to a "Question." 

At the top-left, a blue circle labeled "Question" serves as the reference point for the analysis. 

Below and to the right of the "Question," a light green circle labeled "Related answer" is depicted. A green dotted arrow originates from this "Related answer" and points towards another unlabelled green circle that is positioned closer to the "Question." This green arrow signifies the loss function's goal: to minimize the distance between the "Question" and a "Related answer," effectively pulling related items closer together in the embedding space. 

In contrast, there are two instances of "Unrelated answer" shown to the right of the "Question." 
1. The first "Unrelated answer" is a light pink/grey circle located in the top-right. A red dotted arrow extends from this "Unrelated answer" to an unlabelled red circle positioned further away from the "Question." 
2. The second "Unrelated answer" is another light pink/grey circle located below the first one, in the center-right. A red dotted arrow also extends from this "Unrelated answer" to another unlabelled red circle, again positioned further away from the "Question." 

These red dotted arrows illustrate the second objective of the ranking loss: to maximize the distance between the "Question" and "Unrelated answers," pushing dissimilar items farther apart. The use of two "Unrelated answer" examples highlights the "multiple negatives" aspect, where the model learns to push away several irrelevant options simultaneously. 

In summary, the diagram visually explains how related text items are encouraged to cluster together, while unrelated items are dispersed, forming distinct regions in the embedding space, which is fundamental for effective text retrieval and understanding.](images/081ec99e11d77a7aeb81036ea404436f86df48ce9744c87f67c11845ace76cbc.jpg)
Figure 10-10. Multiple negatives ranking loss aims to minimize the distance between related pairs of text, such as questions and answers, and maximize the distance between unrelated pairs, such as questions and unrelated answers.

After having generated these positive and negative pairs, we calculate their embed‐ dings and apply cosine similarity. These similarity scores are then used to answer the question, are these pairs negative or positive? In other words, it is treated as a classification task and we can use cross-entropy loss to optimize the model.

To make these triplets we start with an anchor sentence (i.e., labeled as the “prem‐ ise”), which is used to compare other sentences. Then, using the MNLI dataset, we only select sentence pairs that are positive (i.e., labeled as “entailment”). To add negative sentences, we randomly sample sentences as the “hypothesis.”

import random   
from tqdm import tqdm   
from datasets import Dataset, load_dataset   
# # Load MNLI dataset from GLUE   
mnli $=$ load_dataset("glue", "mnli", split $=$ "train").select(range(50_000))   
mnli $=$ mnli.remove_columns("idx")

mnli $=$ mnli.filter(lambda x: True if $\times [ \mathrm {  ~ \bar { ~ } { ~ \bf ~ \alpha ~ } ~ } ] = \mathrm {  ~ \bar { ~ } { ~ \bf ~ \alpha ~ } ~ }$ else False)

# Prepare data and add a soft negative   
train_dataset $=$ {"anchor": [], "positive": [], "negative": []}   
soft_negatives $=$ mnli["hypothesis"]   
random.shuffle(soft_negatives)   
for row, soft_negative in tqdm(zip(mnli, soft_negatives)): train_dataset["anchor"].append(row["premise"]) train_dataset["positive"].append(row["hypothesis"]) train_dataset["negative"].append(soft_negative)   
train_dataset $=$ Dataset.from_dict(train_dataset)

Since we only selected sentences labeled with “entailment,” the number of rows reduced quite a a bit from 50,000 to 16,875 rows.

Let’s define the evaluator:

from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator   
# Create an embedding similarity evaluator for stsb   
val_sts $=$ load_dataset("glue", "stsb", split="validation")   
evaluator $=$ EmbeddingSimilarityEvaluator( sentences $1 =$ val_sts["sentence1"], sentences2=val_sts["sentence2"], scores $\mathbf { \Psi } _ { 1 } =$ [score/5 for score in val_sts["label"]], main_similarity="cosine"   
)

We then train as before but with MNR loss instead:

from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments

# Define model embedding_model $=$ SentenceTransformer('bert-base-uncased')

# Loss function train_loss $=$ losses.MultipleNegativesRankingLoss(model=embedding_model)

# Define the training arguments   
args $=$ SentenceTransformerTrainingArguments( output_dir $=$ "mnrloss_embedding_model", num_train_epochs $\mathbf { \Psi } = \mathbf { \Psi }$ , per_device_train_batch_size $= 3 2$ , per_device_eval_batch_size $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , warmup_steps $\begin{array} { r l } { \mathrm { ~  ~ \tau ~ } } & { { } = } \\ { \mathrm { ~  ~ \tau ~ } } & { { } = } \end{array}$ , fp16=True, eval_steps ${ \tt \Gamma } = 1 0 0$ , logging_step $\begin{array} { r } { \ \vdots = \ } \\ { \mathfrak { d } \mathfrak { d } } \end{array}$ ,   
)

# Train model

trainer $=$ SentenceTransformerTrainer( model embedding_model, args $=$ args, train_dataset $\Bumpeq$ train_dataset, loss $=$ train_loss, evaluator=evaluator   
)   
trainer.train()

Let’s see how this dataset and loss function compare to our previous examples:

# Evaluate our trained model evaluator(embedding_model)

{'pearson_cosine': 0.8093892326162132, 'spearman_cosine': 0.8121064796503025, 'pearson_manhattan': 0.8215001523827565, 'spearman_manhattan': 0.8172161486524246, 'pearson_euclidean': 0.8210391407846718, 'spearman_euclidean': 0.8166537141010816, 'pearson_dot': 0.7473360302629125, 'spearman_dot': 0.7345184137194012, 'pearson_max': 0.8215001523827565, 'spearman_max': 0.8172161486524246}

Compared to our previously trained model with softmax loss (0.72), our model with MNR loss (0.80) seems to be much more accurate!

![## Image Analysis: 0d645c4bf91765b5c1e2d41d45620f6400a6c52be4bd356c2323ac400f5594f5.jpg

**Conceptual Understanding:**
This image conceptually represents a brand or organizational identity. Its main purpose is to serve as a visual emblem, marking a specific entity, project, or tool within the broader document. The key idea being communicated is identification and branding, rather than conveying data, processes, or technical concepts directly.

**Content Interpretation:**
The image is a logo featuring a stylized animal figure, possibly a lemur, within a square frame. The simplicity of the design, using a silhouette and a single color (green), suggests it is intended for branding, identification, or as a visual marker. The animal's pose implies movement or a natural state. Given the document context regarding 'evaluating our trained model evaluator (embedding_model)', this logo might represent the organization, a project, or a specific tool related to the document's content. However, without additional textual context within the image, its direct functional meaning within a technical diagram or process is absent. The significance lies in its role as a visual identifier.

**Key Insights:**
The primary takeaway from this image is its function as a visual identifier or logo. It signifies the presence of a brand, organization, or project, likely related to the academic, technical, or research context of the document. The stylized animal within a square communicates a distinct visual identity without conveying any specific technical or procedural information. No specific knowledge regarding model evaluation, processes, or data can be extracted directly from this purely graphical image.

**Document Context:**
Given the section title 'Evaluate our trained model evaluator(embedding_model)', the image, being a logo, likely serves as a brand identifier for the entity, project, or tool being discussed in the document. It does not provide direct technical information or a process flow related to the evaluation of a model evaluator. Instead, it acts as a visual element that may signify authorship, intellectual property, or a specific platform associated with the 'embedding_model' or the 'model evaluator'. Its presence is likely for branding or organizational context rather than illustrative content specific to the evaluation process itself.

**Summary:**
The image displays a green silhouette of an animal, likely a lemur or a similar primate, characterized by its long, upward-curling tail. The animal is depicted in a quadrupedal stance, appearing to be walking or poised. This silhouette is contained within a simple, open green square outline. The overall design is a stylized logo or emblem. There is no discernible text, process flow, or annotation within the image itself.](images/0d645c4bf91765b5c1e2d41d45620f6400a6c52be4bd356c2323ac400f5594f5.jpg)

Larger batch sizes tend to be better with MNR loss as a larger batch makes the task more difficult. The reason for this is that the model needs to find the best matching sentence from a larger set of potential pairs of sentences. You can adapt the code to try out different batch sizes and get a feeling of the effects.

There is a downside to how we used this loss function. Since negatives are sampled from other question/answer pairs, these in-batch or “easy” negatives that we used could potentially be completely unrelated to the question. As a result, the embedding model’s task of then finding the right answer to a question becomes quite easy. Instead, we would like to have negatives that are very related to the question but not the right answer. These negatives are called hard negatives. Since this would make the task more difficult for the embedding model as it has to learn more nuanced representations, the embedding model’s performance generally improves quite a bit.

A good example of a hard negative is the following. Let’s assume we have the follow‐ ing question: “How many people live in Amsterdam?” A related answer to this ques‐ tion would be: “Almost a million people live in Amsterdam.” To generate a good hard negative, we ideally want the answer to contain something about Amsterdam and the number of people living in this city. For example: “More than a million people live in Utrecht, which is more than Amsterdam.” This answer relates to the question but is not the actual answer, so this would be a good hard negative. Figure 10-11 illustrates the differences between easy and hard negatives.

![## Image Analysis: 7614ab02926e990ef35f433bf92279bb37a45f966f6feec7ad002afd7191aae0.jpg

**Conceptual Understanding:**
The image conceptually illustrates a classification scheme for negative examples in the context of question answering, likely for machine learning model evaluation. Its main purpose is to define and provide concrete examples for 'Easy negative', 'Semi-hard negative', and 'Hard negative' statements relative to a given question and its correct answer. The key idea being communicated is the varying degrees of irrelevance or incorrectness that negative examples can possess, which is critical for robust model training and testing. It helps in understanding how challenging a particular negative example might be for a model to correctly identify as false or irrelevant.

**Content Interpretation:**
This image illustrates the concepts of 'Easy negative', 'Semi-hard negative', and 'Hard negative' in the context of evaluating a model's ability to answer questions correctly. It depicts a flow starting from a specific 'Question' and a corresponding 'Answer', then branches to show three distinct categories of incorrect or irrelevant responses, each labeled as a type of negative example. The relationships shown are hierarchical, demonstrating how different types of incorrect answers vary in their similarity to the correct answer and the original question. Specifically, 'Easy negative' represents a completely irrelevant statement, 'Semi-hard negative' is topically related but not relevant to the specific question, and 'Hard negative' is highly relevant but factually incorrect or misleading.

**Key Insights:**
The main takeaway from this image is the clear distinction between easy, semi-hard, and hard negative examples for model evaluation. 'Easy negative' examples are completely irrelevant, as shown by 'He was waiting in line for the bus' for a question about Amsterdam's population. 'Semi-hard negative' examples have some topical connection but are not direct answers to the question, exemplified by 'The capital of the Netherlands is Amsterdam' for a population query. 'Hard negative' examples are the most challenging, being closely related to the question but factually incorrect, as demonstrated by 'A million people live in Utrecht, which is more than in Amsterdam'. These distinctions are vital for rigorously testing and training models, especially in question-answering systems, to ensure they can differentiate subtle differences and avoid plausible but incorrect answers.

**Document Context:**
This image directly supports the document's section on "Evaluate our trained model evaluator(embedding_model)" by visually defining different categories of negative examples. The surrounding text, "Figure 10-11. An easy negative is typically unrelated to both the question and answer. A semi-hard negative has some similarities to the topic of the question and answer but is somewhat unrelated. A hard negative is very similar to the question but is generally the wrong answer," explicitly aligns with and explains the visual distinctions presented in the diagram. It is crucial for understanding how a model evaluator might categorize and use different types of incorrect responses to test the robustness and accuracy of an embedding model.

**Summary:**
The diagram illustrates three categories of negative examples – Easy, Semi-hard, and Hard – in the context of a question-answering scenario. It begins with a 'Question' about the population of Amsterdam, followed by a correct 'Answer'. From this correct answer, three different types of negative examples are branched. The 'Easy negative' example, 'He was waiting in line for the bus', is entirely unrelated to the question and answer. The 'Semi-hard negative' example, 'The capital of the Netherlands is Amsterdam', shares some topical similarity (Amsterdam, Netherlands) but is not directly related to the question's focus on population numbers. The 'Hard negative' example, 'A million people live in Utrecht, which is more than in Amsterdam', is highly similar, discussing population and a city in the Netherlands, but presents an incorrect fact related to the question's subject, making it a challenging negative.](images/7614ab02926e990ef35f433bf92279bb37a45f966f6feec7ad002afd7191aae0.jpg)
Figure 10-11. An easy negative is typically unrelated to both the question and answer. A semi-hard negative has some similarities to the topic of the question and answer but is somewhat unrelated. A hard negative is very similar to the question but is generally the wrong answer.

Gathering negatives can roughly be divided into the following three processes:

Easy negatives Through randomly sampling documents as we did before.

Semi-hard negatives

Using a pretrained embedding model, we can apply cosine similarity on all sentence embeddings to find those that are highly related. Generally, this does not lead to hard negatives since this method merely finds similar sentences, not question/answer pairs.

# Hard negatives

These often need to be either manually labeled (for instance, by generating semihard negatives) or you can use a generative model to either judge or generate sentence pairs.

Make sure to restart your notebook so we can explore the different methods of fine-tuning embedding models.

# Fine-Tuning an Embedding Model

In the previous section, we went through the basics of training an embedding model from scratch and saw how we could leverage loss functions to further optimize its performance. This approach, although quite powerful, requires creating an embed‐ ding model from scratch. This process can be quite costly and time-consuming.

Instead, the sentence-transformers framework allows nearly all embedding models to be used as a base for fine-tuning. We can choose an embedding model that was already trained on a large amount of data and fine-tune it for our specific data or purpose.

There are several ways to fine-tune your model, depending on the data availability and domain. We will go through two such methods and demonstrate the strength of leveraging pretrained embedding models.

# Supervised

The most straightforward way to fine-tune an embedding model is to repeat the process of training our model as we did before but replace the 'bert-base-uncased' with a pretrained sentence-transformers model. There are many to choose from but generally, all-MiniLM-L6-v2 performs well across many use cases and due to its small size is quite fast.

We use the same data as we used to train our model in the MNR loss example but instead use a pretrained embedding model to fine-tune. As always, let’s start by loading the data and creating the evaluator:

from datasets import load_dataset   
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator   
# Load MNLI dataset from GLUE   
# $\Theta =$ entailment, $ { 1 } =$ neutral, $2 \ =$ contradiction   
train_dataset $=$ load_dataset( "glue", "mnli", split="train"   
).select(range(50_000))   
train_dataset $=$ train_dataset.remove_columns("idx")   
# Create an embedding similarity evaluator for stsb   
val_sts $=$ load_dataset("glue", "stsb", split $\equiv$ "validation")   
evaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts["sentence1"], sentences $2 =$ val_sts["sentence2"], scores $=$ [score/5 for score in val_sts["label"]], main_similarity="cosine"   
)

The training steps are similar to our previous examples but instead of using 'bertbase-uncased', we can use a pretrained embedding model instead:

from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments

# Define model embedding_model $=$ SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

# Loss function train_loss $=$ losses.MultipleNegativesRankingLoss(model=embedding_model)

# Define the training arguments   
args $=$ SentenceTransformerTrainingArguments( output_dir $\ ' =$ "finetuned_embedding_model", num_train_epochs $^ { - 1 }$ , per_device_train_batch_size $= 3 2$ , per_device_eval_batch_size $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , warmup_steps $\mathord { \left. \vert { \left. ^ { } = 1 \Theta \Theta \right.}  \right. }$ , fp1 $\hat { \bf \Phi } =$ True, eval_steps ${ \tt \Gamma } = 1 0 0$ , logging_step $\mathord { \left. = \right.} 1 0 0 $ ,   
)   
# Train model   
trainer $=$ SentenceTransformerTrainer( model $=$ embedding_model, args $=$ args, train_dataset $\Bumpeq$ train_dataset, loss $=$ train_loss, evaluator=evaluator   
)   
trainer.train()

Evaluating this model gives us the following score:

# Evaluate our trained model evaluator(embedding_model)

{'pearson_cosine': 0.8509553350510896, 'spearman_cosine': 0.8484676559567688, 'pearson_manhattan': 0.8503896832470704, 'spearman_manhattan': 0.8475760325664419, 'pearson_euclidean': 0.8513115442079158, 'spearman_euclidean': 0.8484676559567688, 'pearson_dot': 0.8489553386816947, 'spearman_dot': 0.8484676559567688, 'pearson_max': 0.8513115442079158, 'spearman_max': 0.8484676559567688}

Although a score of 0.85 is the highest we have seen thus far, the pretrained model that we used for fine-tuning was already trained on the full MNLI dataset, whereas we only used 50,000 examples. It might seem redundant but this example demonstrates how to fine-tune a pretrained embedding model on your own data.

Instead of using a pretrained BERT model like 'bert-baseuncased' or a possible out-of-domain model like 'all-mpnetbase-v2', you can also perform masked language modeling on the pretrained BERT model to first adapt it to your domain. Then, you can use this fine-tuned BERT model as the base for training your embedding model. This is a form of domain adaptation. In the next chapter, we will apply masked language modeling on a pretrained model.

Note that the main difficulty of training or fine-tuning your model is finding the right data. With these models, we not only want to have very large datasets, but the data in itself needs to be of high quality. Developing positive pairs is generally straightforward but adding hard negative pairs significantly increases the difficulty of creating quality data.

As always, restart your notebook to free up VRAM for the following examples.

# Augmented SBERT

A disadvantage of training or fine-tuning these embedding models is that they often require substantial training data. Many of these models are trained with more than a billion sentence pairs. Extracting such a high number of sentence pairs for your use case is generally not possible as in many cases, there are only a couple of thousand labeled data points available.

Fortunately, there is a way to augment your data such that an embedding model can be fine-tuned when there is only a little labeled data available. This procedure is referred to as Augmented SBERT.9

In this procedure, we aim to augment the small amount of labeled data such that they can be used for regular training. It makes use of the slow and more accurate cross-encoder architecture (BERT) to augment and label a larger set of input pairs. These newly labeled pairs are then used for fine-tuning a bi-encoder (SBERT).

As shown in Figure 10-12, Augmented SBERT involves the following steps:

1. Fine-tune a cross-encoder (BERT) using a small, annotated dataset (gold   
dataset).   
2. Create new sentence pairs.   
3. Label new sentence pairs with the fine-tuned cross-encoder (silver dataset).   
4. Train a bi-encoder (SBERT) on the extended dataset (gold $^ +$ silver dataset).

Here, a gold dataset is a small but fully annotated dataset that holds the ground truth. A silver dataset is also fully annotated but is not necessarily the ground truth as it was generated through predictions of the cross-encoder.

![## Image Analysis: da89cd23f7ccc533f40c37faf10b95e02b9ad85645e23c2f2c643db4f308fbf6.jpg

**Conceptual Understanding:**
The image conceptually represents a semi-supervised data augmentation strategy for training a sentence embedding model, likely SBERT (Bi-encoder). The main purpose is to show how a high-quality but small 'Gold dataset' can be amplified by a 'Cross-encoder' to generate a larger 'Silver dataset' from 'Unlabeled dataset', which together then train a more robust final 'Bi-encoder'. The core message is the intelligent use of limited ground-truth data to create more extensive, albeit machine-generated, training resources for an advanced language model.

**Content Interpretation:**
The image depicts a semi-supervised learning pipeline for augmenting SBERT, specifically focusing on how a 'Bi-encoder' is trained. The core idea is to use a small, reliable 'Gold dataset' to bootstrap the labeling of a much larger 'Unlabeled dataset', thereby creating a 'Silver dataset'. This 'Silver dataset', along with the 'Gold dataset', is then used to train the target 'Bi-encoder'. The process involves sequential model training and data generation steps, demonstrating a common strategy for data augmentation in natural language processing (NLP) tasks when labeled data is scarce.

**Key Insights:**
The main takeaway is that Augmented SBERT uses a multi-stage training process to effectively leverage limited labeled data and abundant unlabeled data. Key insights include: 1. A 'Gold dataset' (ground-truth) is crucial for initial high-quality training. 2. A 'Cross-encoder' acts as an intermediary, trained on the 'Gold dataset' to accurately label an 'Unlabeled dataset'. 3. The generated 'Silver dataset' (machine-annotated) expands the training data by providing more examples, albeit with potentially lower confidence labels. 4. The final 'Bi-encoder' benefits from a diverse training set, combining the precision of the 'Gold dataset' and the volume of the 'Silver dataset', improving its generalization capabilities. This method illustrates a practical approach to overcoming data scarcity in deep learning.

**Document Context:**
This image directly explains the methodology behind 'Augmented SBERT' as referenced in the document's 'Augmented SBERT' section. It visually describes the steps involved in augmenting the SBERT model's training data by creating a 'silver dataset' from an unlabeled dataset using a cross-encoder, before using both gold and silver datasets to train the bi-encoder (SBERT). The text after the image, 'Figure 10-12. Augmented SBERT works through training a cross-encoder on a small gold dataset, then using that to label an unlabeled dataset to generate a larger silver dataset. Finally, both the gold and silver datasets are used to train the bi-encoder.', perfectly aligns with and confirms the process flow depicted.

**Summary:**
The image illustrates the Augmented SBERT training process, which leverages a small, high-quality 'Gold dataset' and a larger 'Unlabeled dataset' to train a 'Bi-encoder'. The process begins by training a 'Cross-encoder' using the 'Gold dataset'. This trained 'Cross-encoder' then performs inference on the 'Unlabeled dataset' to generate a 'Silver dataset', which is characterized as 'Machine-annotated'. Finally, the 'Bi-encoder' is trained using both the original 'Gold dataset' and the newly created 'Silver dataset'. This approach allows the model to benefit from the accuracy of ground-truth data while also leveraging a larger, machine-annotated dataset for broader coverage, enhancing the overall training of the Bi-encoder.](images/da89cd23f7ccc533f40c37faf10b95e02b9ad85645e23c2f2c643db4f308fbf6.jpg)
Figure 10-12. Augmented SBERT works through training a cross-encoder on a small gold dataset, then using that to label an unlabeled dataset to generate a larger silver dataset. Finally, both the gold and silver datasets are used to train the bi-encoder.

Before we get into the preceding steps, let’s first prepare the data. Instead of our original 50,000 documents, we take a subset of 10,000 documents to simulate a setting where we have limited annotated data. As we did in our example with cosine similarity loss, give entailment a score of 1 whereas neutral and contradiction get a score of 0:

import pandas as pd   
from tqdm import tqdm   
from datasets import load_dataset, Dataset   
from sentence_transformers import InputExample   
from sentence_transformers.datasets import NoDuplicatesDataLoader   
# Prepare a small set of 10000 documents for the cross-encoder   
dataset $=$ load_dataset("glue", "mnli", split $\mathbf { \varepsilon } =$ "train").select(range(10_000))   
mapping $=$ {2: 0, 1: 0, 0:1}   
# Data loader   
gold_examples $=$ [ InputExample(texts $\mathbf { \equiv }$ [row["premise"], row["hypothesis"]], label=map   
ping[row["label"]]) for row in tqdm(dataset)   
]

gold_dataloader $=$ NoDuplicatesDataLoader(gold_examples, batch_size $= 3 2$ )

# Pandas DataFrame for easier data handling   
gold $=$ pd.DataFrame( { "sentence1": dataset["premise"], "sentence2": dataset["hypothesis"], "label": [mapping[label] for label in dataset["label"]] }   
)

This is the gold dataset since it is labeled and represents our ground truth.

Using this gold dataset, we train our cross-encoder (step 1):

from sentence_transformers.cross_encoder import CrossEncoder   
# Train a cross-encoder on the gold dataset   
cross_encoder $=$ CrossEncoder("bert-base-uncased", num_labels $\scriptstyle : = \ .$ )   
cross_encoder.fit( train_dataloader $: =$ gold_dataloader, epochs $^ { - 1 }$ , show_progress_bar $: =$ True, warmup_steps $\mathord { \left. \vert { \left. ^ { } = 1 \Theta \Theta \right.}  \right. }$ , use_amp=False   
)

After training our cross-encoder, we use the remaining 400,000 sentence pairs (from our original dataset of 50,000 sentence pairs) as our silver dataset (step 2):

# Prepare the silver dataset by predicting labels with the cross-encoder   
silver $=$ load_dataset( "glue", "mnli", split $=$ "train"   
).select(range(10_000, 50_000))   
pairs $=$ list(zip(silver["premise"], silver["hypothesis"]))

If you do not have any additional unlabeled sentence pairs, you can randomly sample them from your original gold dataset. To illustrate, you can create a new sentence pair by taking the premise from one row and the hypothesis from another. This allows you to easily generate 10 times as many sentence pairs that can be labeled with the cross-encoder.

This strategy, however, likely generates significantly more dissimi‐ lar than similar pairs. Instead, we can use a pretrained embedding model to embed all candidate sentence pairs and retrieve the top-k sentences for each input sentence using semantic search. This rough reranking process allows us to focus on sentence pairs that are likely to be more similar. Although the sentences are still chosen based on an approximation since the pretrained embedding model was not trained on our data, it is much better than random sampling.

Note that we assume that these sentence pairs are unlabeled in this example. We will use our fine-tuned cross-encoder to label these sentence pairs (step 3):

import numpy as np

# Label the sentence pairs using our fine-tuned cross-encoder   
output $=$ cross_encoder.predict( pairs, apply_softmax $\equiv$ True,   
show_progress_bar=True   
)   
silver $=$ pd.DataFrame( { "sentence1": silver["premise"], "sentence2": silver["hypothesis"], "label": np.argmax(output, axis $\mathbf { \Psi } = \mathbf { \Psi }$ ) }   
)

Now that we have a silver and gold dataset, we simply combine them and train our embedding model as we did before:

# Combine gold $^ +$ silver data $=$ pd.concat([gold, silver], ignore_index $\equiv$ True, axis $\scriptstyle = 0$ ) data $=$ data.drop_duplicates(subset=["sentence1", "sentence2"], keep $^ { 1 = }$ "first") train_dataset $=$ Dataset.from_pandas(data, preserve_index=False)

As always, we need to define our evaluator:

from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# Create an embedding similarity evaluator for stsb   
val_sts $=$ load_dataset("glue", "stsb", split="validation")   
evaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts["sentence1"], sentences2=val_sts["sentence2"], scores $=$ [score/5 for score in val_sts["label"]], main_similarity="cosine"   
)

We train the model the same as before except now we use the augmented dataset:

from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments

# Define model embedding_model $=$ SentenceTransformer("bert-base-uncased")

# Loss function train_loss $=$ losses.CosineSimilarityLoss(model $. =$ embedding_model)

# Define the training arguments args $=$ SentenceTransformerTrainingArguments(

output_dir $=$ "augmented_embedding_model", num_train_epochs $^ { = 1 }$ , per_device_train_batch_size $: = 3 2$ , per_device_eval_batch_size $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , warmup_steps $\mathord { \left. \vert { \left. ^ { } = 1 \Theta \Theta \right.}  \right. }$ , fp1 $\mathrm { { } } ; = \mathrm { { } }$ True, eval_steps ${ \tt \Gamma } = 1 0 0$ , logging_step ${ \displaystyle \langle = 1 0 0 }$ , )

# Train model   
trainer $=$ SentenceTransformerTrainer( model embedding_model, args $=$ args, train_dataset=train_dataset, loss $=$ train_loss, evaluator=evaluator   
)   
trainer.train()

Finally, we evaluate the model:

evaluator(embedding_model)

{'pearson_cosine': 0.7101597020018693, 'spearman_cosine': 0.7210536464320728, 'pearson_manhattan': 0.7296749443525249, 'spearman_manhattan': 0.7284184255293913, 'pearson_euclidean': 0.7293097297208753, 'spearman_euclidean': 0.7282830906742256, 'pearson_dot': 0.6746605824703588, 'spearman_dot': 0.6754486790570754, 'pearson_max': 0.7296749443525249, 'spearman_max': 0.7284184255293913}

The original cosine similarity loss example had a score of 0.72 with the full dataset. Using only $2 0 \%$ of that data, we managed to get a score of 0.71!

This method allows us to increase the size of datasets that you already have available without the need to manually label hundreds of thousands of sentence pairs. You can test the quality of your silver data by also training your embedding model only on the gold dataset. The difference in performance indicates how much your silver dataset potentially adds to the quality of the model.

You can restart your notebook a final time for the last example, namely unsupervised learning.

# Unsupervised Learning

To create an embedding model, we typically need labeled data. However, not all real-world datasets come with a nice set of labels that we can use. We instead look for techniques to train the model without any predetermined labels—unsuper‐ vised learning. Many approaches exist, like Simple Contrastive Learning of Sentence Embeddings (SimCSE),10 Contrastive Tension (CT),11 Transformer-based Sequential Denoising Auto-Encoder (TSDAE),12 and Generative Pseudo-Labeling (GPL).13

In this section, we will focus on TSDAE, as it has shown great performance on unsupervised tasks as well as domain adaptation.

# Transformer-Based Sequential Denoising Auto-Encoder

TSDAE is a very elegant approach to creating an embedding model with unsuper‐ vised learning. The method assumes that we have no labeled data at all and does not require us to artificially create labels.

The underlying idea of TSDAE is that we add noise to the input sentence by remov‐ ing a certain percentage of words from it. This “damaged” sentence is put through an encoder, with a pooling layer on top of it, to map it to a sentence embedding. From this sentence embedding, a decoder tries to reconstruct the original sentence from the “damaged” sentence but without the artificial noise. The main concept here is that the more accurate the sentence embedding is, the more accurate the reconstructed sentence will be.

This method is very similar to masked language modeling, where we try to recon‐ struct and learn certain masked words. Here, instead of reconstructing masked words, we try to reconstruct the entire sentence.

After training, we can use the encoder to generate embeddings from text since the decoder is only used for judging whether the embeddings can accurately reconstruct the original sentence (Figure 10-13).

![## Image Analysis: bbda92faf7ca0ae117173787cb75f00e3ea9727aa7539108cc0f7cbf8d6d6c2a.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and operational flow of a Denoising Auto-Encoder, specifically in the context of Transformer models (like BERT). The main purpose is to illustrate how a model can learn meaningful sentence embeddings by reconstructing an original sentence from a corrupted version. The key idea communicated is the self-supervised learning paradigm where 'noise' (deleted words) is introduced to an input, and the model is trained to recover the original, clean input, thereby improving its understanding and representation of sentences.

**Content Interpretation:**
The image depicts a Transformer-Based Sequential Denoising Auto-Encoder (TSDAE) system. It shows a process where an input sentence, initially with "noise" (missing words), is transformed into a clean, reconstructed sentence. The core components are an Encoder and a Decoder, both specified as 'bert-base-cased' models, and a Pooling layer. The process involves corrupting an input by deleting words, generating a sentence embedding from the corrupted input, and then reconstructing the original sentence from this embedding. This demonstrates a method for learning robust sentence representations by training a model to denoise text.

**Key Insights:**
The main takeaway is the operational flow of a Denoising Auto-Encoder using Transformer models. Key insights include: 1. Input sentences are intentionally corrupted by "Deleted" words to create "Text with noise." 2. An "Encoder" (e.g., 'bert-base-cased') processes this noisy input. 3. A "Pooling" step condenses the encoded information into a "Sentence embedding." 4. A "Decoder" (e.g., 'bert-base-cased') takes the embedding and attempts to "Reconstruct" the original text, yielding "Text without noise." This process highlights the principle of learning robust representations by forcing the model to recover missing information, which is a common technique in self-supervised learning for natural language processing.

**Document Context:**
This image visually explains the mechanism of a Transformer-Based Sequential Denoising Auto-Encoder (TSDAE), directly supporting the document's description that TSDAE "randomly removes words from an input sentence that is passed through an encoder to generate a sentence embedding. From this sentence embedding, the original sentence is reconstructed." It serves as a detailed visual aid, clarifying the abstract concept through a concrete example and illustrating the flow of data through the model components. It is essential for understanding how sentence embeddings are generated in a denoising context.

**Summary:**
The diagram illustrates the process of a Transformer-Based Sequential Denoising Auto-Encoder (TSDAE). It begins with an input labeled "Text with noise," exemplified by the sentence "... capital of ............ is Amsterdam." In this initial step, parts of the sentence are intentionally removed, indicated by the red shaded area and labeled "Deleted." The corrupted sentence is then fed into an "Encoder," which is specified as e.g., 'bert-base-cased'. Following the encoder, a "Pooling" operation is performed. The output of the pooling layer is a "Sentence embedding," represented by a series of interconnected empty squares. This sentence embedding then proceeds to a "Decoder," also specified as e.g., 'bert-base-cased'. The decoder's function is to reconstruct the original sentence. The final output is labeled "Text without noise," an example of which is "The capital of the Netherlands is Amsterdam." The green highlighted text "The capital of" and "the Netherlands" are associated with the label "Reconstruct," signifying the restoration of the deleted words to form the complete, original sentence.](images/bbda92faf7ca0ae117173787cb75f00e3ea9727aa7539108cc0f7cbf8d6d6c2a.jpg)
Figure 10-13. TSDAE randomly removes words from an input sentence that is passed through an encoder to generate a sentence embedding. From this sentence embedding, the original sentence is reconstructed.

Since we only need a bunch of sentences without any labels, training this model is straightforward. We start by downloading an external tokenizer, which is used for the denoising procedure:

# Download additional tokenizer import nltk nltk.download("punkt")

Then, we create flat sentences from our data and remove any labels that we have to mimic an unsupervised setting:

from tqdm import tqdm   
from datasets import Dataset, load_dataset   
from sentence_transformers.datasets import DenoisingAutoEncoderDataset   
# Create a flat list of sentences   
mnli $=$ load_dataset("glue", "mnli", split $=$ "train").select(range(25_000))   
flat_sentences $=$ mnli["premise"] $^ +$ mnli["hypothesis"]

# Add noise to our input data damaged_data $=$ DenoisingAutoEncoderDataset(list(set(flat_sentences)))

# Create dataset train_dataset $=$ {"damaged_sentence": [], "original_sentence": []}

for data in tqdm(damaged_data): train_dataset["damaged_sentence"].append(data.texts[0]) train_dataset["original_sentence"].append(data.texts[1])   
train_dataset $=$ Dataset.from_dict(train_dataset)

This creates a dataset of 50,000 sentences. When we inspect the data, notice that the first sentence is the damaged sentence and the second sentence the original:

train_dataset[0]

{'damaged_sentence': 'Grim jaws are. 'original_sentence': 'Grim faces and hardened jaws are not people-friendly.'}

The first sentence shows the “noisy” data whereas the second shows the original input sentence. After creating our data, we define our evaluator as before:

from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# Create an embedding similarity evaluator for stsb   
val_sts $=$ load_dataset("glue", "stsb", split="validation")   
evaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts["sentence1"], sentences2=val_sts["sentence2"], scores $=$ [score/5 for score in val_sts["label"]], main_similarity="cosine"   
)

Next, we run the training as before but with the [CLS] token as the pooling strategy instead of the mean pooling of the token embeddings. In the TSDAE paper, this was shown to be more effective since mean pooling loses the position information, which is not the case when using the [CLS] token:

from sentence_transformers import models, SentenceTransformer

# Create your embedding model   
word_embedding_model $=$ models.Transformer("bert-base-uncased")   
pooling_model $=$ models.Pooling(word_embedding_model.get_word_embedding_dimen   
sion(), "cls")   
embedding_model $=$ SentenceTransformer(modules $=$ [word_embedding_model, pool   
ing_model])

Using our sentence pairs, we will need a loss function that attempts to reconstruct the original sentence using the noise sentence, namely DenoisingAutoEncoderLoss. By doing so, it will learn how to accurately represent the data. It is similar to masking but without knowing where the actual masks are.

Moreover, we tie the parameters of both models. Instead of having separate weights for the encoder’s embedding layer and the decoder’s output layer, they share the same weights. This means that any updates to the weights in one layer will be reflected in the other layer as well:

# from sentence_transformers import losses

# Use the denoising auto-encoder loss train_loss $=$ losses.DenoisingAutoEncoderLoss( embedding_model, tie_encoder_decoder=True ) train_loss.decoder $=$ train_loss.decoder.to("cuda")

Finally, training our model works the same as we have seen several times before but we lower the batch size as memory increases with this loss function:

from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments

# Define the training arguments   
args $=$ SentenceTransformerTrainingArguments( output_dir="tsdae_embedding_model", num_train_epochs $^ { = 1 }$ , per_device_train_batch_size $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , per_device_eval_batch_size $= 1 6$ , warmup_steps $\mathord { \left. \vert { \left. ^ { } = 1 \Theta \Theta \right.}  \right. }$ , fp1 ${ \sf 6 = }$ True, eval_step ${ \displaystyle \ i = 1 0 0 }$ , logging_step $\quad : = \quad$ ,   
)   
# Train model   
trainer $=$ SentenceTransformerTrainer( model $=$ embedding_model, args $=$ args, train_dataset=train_dataset, loss $=$ train_loss, evaluator $=$ evaluator   
)   
trainer.train()

After training, we evaluate our model to explore how well such an unsupervised technique performs:

# Evaluate our trained model evaluator(embedding_model)

{'pearson_cosine': 0.6991809700971775, 'spearman_cosine': 0.713693213167873, 'pearson_manhattan': 0.7152343356643568, 'spearman_manhattan': 0.7201441944880915, 'pearson_euclidean': 0.7151142243297436, 'spearman_euclidean': 0.7202291660769805, 'pearson_dot': 0.5198066451871277, 'spearman_dot': 0.5104025515225046, 'pearson_max': 0.7152343356643568, 'spearman_max': 0.7202291660769805}

After fitting our model, we got a score of 0.70, which is quite impressive considering we did all this training with unlabeled data.

# Using TSDAE for Domain Adaptation

When you have very little or no labeled data available, you typically use unsupervised learning to create your text embedding model. However, unsupervised techniques are generally outperformed by supervised techniques and have difficulty learning domain-specific concepts.

This is where domain adaptation comes in. Its goal is to update existing embedding models to a specific textual domain that contains different subjects from the source domain. Figure 10-14 demonstrates how domains can differ in content. The target domain, or out-domain, generally contains words and subjects that were not found in the source domain or in-domain.

![## Image Analysis: 3be549cc4c2e7abbc6daf6a6937f3519915fcd2bbd945437b447da7ec7475268.jpg

**Conceptual Understanding:**
This image conceptually illustrates the challenge and objective of 'domain adaptation' within the context of document embeddings. Its main purpose is to visualize the distinction between an 'in-domain' – a set of topics or documents from which an embedding model is primarily trained or derived – and various 'out-domain' categories, which represent different, often unrelated, topics to which the model's understanding needs to be extended or generalized. The key ideas communicated are the semantic separation of diverse topics and the necessity of adapting models to effectively interpret information across these distinct domains.

**Content Interpretation:**
The image depicts a conceptual mapping of various document topics within a semantic space, organized into 'In-domain' and 'Out-domain' categories. The 'In-domain' is represented by a cluster of programming languages (SQL, Python, Scala, Java, Rust), suggesting this as the primary knowledge base. The 'Out-domain' is illustrated by several distinct clusters: animation studios (Disney, Studio Ghibli, Pixar, DreamWorks), music bands (Queen, AC/DC, The Who), and countries (UK, Belgium, Italy). The grid background implies a multi-dimensional embedding space where proximity suggests semantic similarity. The arrows originating from the 'Out-domain' label pointing to 'Pixar', 'UK', 'Queen', and 'The Who' specifically highlight examples of diverse target domains to which an embedding model might need to generalize its understanding. Similarly, the 'In-domain' arrow pointing to 'Python' emphasizes a core topic within the source domain. The different colored boxes (red for programming languages, blue for animation studios, purple for music bands, green for countries) implicitly categorize these topics visually, reinforcing their distinctness.

**Key Insights:**
The main takeaways from this image are: 1. Documents can be grouped into distinct conceptual domains, as exemplified by programming languages, animation studios, music bands, and countries. 2. The concept of 'In-domain' refers to a primary or source set of topics from which an embedding model is learned or generalized. 3. 'Out-domain' refers to other, distinct sets of topics where the model's learned knowledge needs to be applied or adapted. 4. Domain adaptation is necessary when dealing with diverse document content, as an embedding model trained on one domain (e.g., programming languages like Python) may not inherently understand concepts from a vastly different domain (e.g., music bands like Queen or countries like the UK). These insights are directly supported by the explicit labels 'Documents about:', 'In-domain', and 'Out-domain', coupled with the clear categorization of topics within the respective clusters (e.g., 'SQL', 'Python', 'Scala', 'Java', 'Rust' for in-domain, and 'Disney', 'Studio Ghibli', 'Pixar', 'DreamWorks', 'Queen', 'AC/DC', 'The Who', 'UK', 'Belgium', 'Italy' for out-domain). The visual separation and the arrows connecting the general 'Out-domain' label to specific examples reinforce the idea of generalizing knowledge across different topic spaces.

**Document Context:**
This image serves as a direct visual aid for the document's section 'Using TSDAE for Domain Adaptation'. It provides a concrete and intuitive illustration of the core concept described in the surrounding text: 'In domain adaptation, the aim is to create and generalize an embedding model from one domain to another.' By clearly separating 'In-domain' topics (programming languages) from a variety of 'Out-domain' topics (animation studios, music, countries), the image helps the reader understand the challenge and objective of domain adaptation in the context of document content and embedding models. It visually defines what 'one domain' and 'another domain' look like, enhancing comprehension of the technical concept.

**Summary:**
The image visually represents the concept of domain adaptation for documents, categorizing various topics into 'In-domain' and 'Out-domain' clusters on a grid. The 'In-domain' is primarily illustrated by programming languages, while the 'Out-domain' encompasses diverse topics such as animation studios, music bands, and countries. The grid layout implies a semantic space where related topics are grouped. Arrows originating from the 'Out-domain' label, and the specific arrow for 'In-domain' pointing to 'Python', visually indicate the source and target domains for adaptation. The overall purpose is to show how an embedding model trained on one set of topics (in-domain) needs to generalize its understanding to different, distinct sets of topics (out-domain).](images/3be549cc4c2e7abbc6daf6a6937f3519915fcd2bbd945437b447da7ec7475268.jpg)
Figure 10-14. In domain adaptation, the aim is to create and generalize an embedding model from one domain to another.

One method for domain adaptation is called adaptive pretraining. You start by pre‐ training your domain-specific corpus using an unsupervised technique, such as the previously discussed TSDAE or masked language modeling. Then, as illustrated in Figure 10-15, you fine-tune that model using a training dataset that can be either outside or in your target domain. Although data from the target domain is preferred, out-domain data also works since we started with unsupervised training on the target domain.

![## Image Analysis: 24d52c947bfd367159748ea17225ab9c6b18662d1e58cc55ffc3d5853be8578a.jpg

**Conceptual Understanding:**
This image conceptually illustrates a methodology for "Adaptive pre-training" within the domain adaptation process. Its main purpose is to demonstrate how unsupervised learning, specifically with models like MLM and TSDAE applied to a target domain, can serve as a preparatory step before a supervised learning phase utilizing models like SBERT, potentially on a non-target domain, to improve overall model performance or transferability. It depicts a flow from an initial, domain-specific unsupervised learning stage to a subsequent, more general or task-specific supervised stage, all under the umbrella of adaptive pre-training.

**Content Interpretation:**
The image illustrates a two-stage process for model pre-training, termed "Adaptive pre-training." The first stage is "Unsupervised," where models such as "MLM" (Masked Language Modeling) and "TSDAE" (Transformer-based Sequential Denoising Autoencoder) are applied to a "Target domain." This suggests a domain-specific unsupervised learning phase to acquire generalized representations. The second stage, following an explicit directional arrow, is "Supervised." Here, a model named "SBERT" (Sentence-BERT) is involved, which is typically used for supervised tasks like fine-tuning on labeled data, and it's associated with a "Non-target domain." This implies that the unsupervised pre-training on the target domain provides a foundation that is then adapted or leveraged for supervised learning on a different, non-target domain, or perhaps the 'non-target domain' refers to a more general dataset after domain-specific pre-training. The overall process emphasizes an adaptive approach where initial unsupervised learning informs subsequent supervised tasks.

**Key Insights:**
The main takeaway is that adaptive pre-training for domain adaptation can involve a sequence of unsupervised and supervised learning steps. Specifically: 1.  **Unsupervised Learning is a Foundation:** "MLM, TSDAE" are used in an "Unsupervised" manner on a "Target domain" to build initial representations. This highlights the importance of unsupervised methods for learning domain-specific features. 2.  **Transition to Supervised Learning:** There's a clear progression from unsupervised to "Supervised" learning, indicating that the knowledge gained during unsupervised pre-training is then utilized for supervised tasks. 3.  **Model Specificity:** Specific models are mentioned: "MLM, TSDAE" for unsupervised pre-training and "SBERT" for the supervised phase, suggesting common architectures used in this adaptive pre-training paradigm. 4.  **Domain Relevance:** The distinction between "Target domain" for unsupervised and "Non-target domain" for supervised (or general domain) emphasizes the concept of adapting knowledge from a specific domain to potentially a broader or different task space.

**Document Context:**
The image directly supports the document's section titled "Using TSDAE for Domain Adaptation" and the accompanying text: "Figure 10-15. Domain adaptation can be performed with adaptive pretraining and adaptive fine-tuning." It visually explains the "adaptive pretraining" component of domain adaptation by showing a conceptual flow from unsupervised learning on a target domain to supervised learning on a non-target domain, featuring specific model architectures like TSDAE and SBERT. This diagram helps the reader understand the initial pre-training step in the broader domain adaptation strategy, highlighting the transition from unsupervised to supervised methodologies.

**Summary:**
This image depicts a conceptual flow for "Adaptive pre-training" in the context of machine learning model development. The process begins with an "Unsupervised" phase, which involves models like "MLM, TSDAE" operating on a "Target domain." This phase is followed by a transition, indicated by an arrow, to a "Supervised" phase. In the supervised phase, a model identified as "SBERT" is utilized, typically associated with a "Non-target domain." The entire sequence, from the unsupervised training on the target domain to the supervised application on a non-target domain, is encompassed and labeled as "Adaptive pre-training." This illustrates how an unsupervised pre-training step can be adapted for subsequent supervised tasks, potentially bridging different domains or model types to enhance performance.](images/24d52c947bfd367159748ea17225ab9c6b18662d1e58cc55ffc3d5853be8578a.jpg)
Figure 10-15. Domain adaptation can be performed with adaptive pretraining and adaptive fine-tuning.

Using everything you have learned in this chapter, you should be able to reproduce this pipeline! First, you can start with TSDAE to train an embedding model on your target domain and then fine-tune it using either general supervised training or Augmented SBERT.

# Summary

In this chapter, we looked at creating and fine-tuning embedding models through various tasks. We discussed the concept of embeddings and their role in representing textual data in a numerical format. We then explored the foundational technique of many embedding models, namely contrastive learning, which learns primarily from (dis)similar pairs of documents.

Using a popular embedding framework, sentence-transformers, we then created embedding models using a pretrained BERT model while exploring different loss functions, such as cosine similarity loss and MNR loss. We discussed how the collec‐ tion of (dis)similar pairs or triples of documents is vital to the performance of the resulting model.

In the sections that followed, we explored techniques for fine-tuning embedding models. Both supervised and unsupervised techniques were discussed such as Aug‐ mented SBERT and TSDAE for domain adaptation. Compared to creating an embed‐ ding model, fine-tuning generally needs less data and is a great way to adapt existing embedding models to your domain.

In the next chapter, methods for fine-tuning representations for classification will be discussed. Both BERT models and embedding models will make an appearance as well as a wide range of fine-tuning techniques.

# Fine-Tuning Representation Models for Classification

In Chapter 4, we used pretrained models to classify our text. We kept the pretrained models as they were without any modifications to them. This might make you wonder, what happens if we were to fine-tune them?

If we have sufficient data, fine-tuning tends to lead to some of the best-performing models possible. In this chapter, we will go through several methods and applications for fine-tuning BERT models. “Supervised Classification” on page 323 demonstrates the general process of fine-tuning a classification model. Then, in “Few-Shot Classifi‐ cation” on page 333, we look at SetFit, which is a method for efficiently fine-tuning a high-performing model using a small number of training examples. In “Continued Pretraining with Masked Language Modeling” on page 340, we will explore how to continue training a pretrained model. Lastly, classification on a token level is explored in “Named-Entity Recognition” on page 345.

We will focus on nongenerative tasks, as generative models will be covered in Chapter 12.

# Supervised Classification

In Chapter 4, we explored supervised classification tasks by leveraging pretrained representation models that were either trained to predict sentiment (task-specific model) or to generate embeddings (embedding model), as shown in Figure 11-1.

![## Image Analysis: ff3ee819dc97446e0f37a71de6d656c83cc4efb3cf9cc2f508e88e454b445cf3.jpg

**Conceptual Understanding:**
This image conceptually illustrates different architectural approaches for supervised classification within machine learning, specifically focusing on the role of pre-trained models and the distinction between 'frozen' (nontrainable) and 'trainable' components. 

The main purpose of the image is to demonstrate two pathways for obtaining a classification 'Output' ('1 Positive'). One path showcases the direct use of a 'Task-specific model' that is 'Nontrainable' and '"Frozen"' for classification. The second path depicts using an 'Embedding model' that is also 'Nontrainable' and '"Frozen"' to 'Create embeddings', which are then processed by a subsequent 'Trainable' component to achieve the final classification. 

The key idea communicated is the practical application of transfer learning by either directly utilizing a fixed, pre-trained classifier or by extracting fixed, high-quality features (embeddings) from a pre-trained model and then training a smaller, task-specific layer on top of these features. It visually explains how different parts of a model can have their weights either fixed or adjustable during the training process for specific tasks.

**Content Interpretation:**
The image illustrates two distinct processes for supervised classification using machine learning models. 

**Path 1: Direct Classification with a Nontrainable Task-specific Model**
An input is processed by a 'Task-specific model' whose function is to 'Perform classification'. This model is explicitly identified as 'Nontrainable' and '"Frozen"', indicating that its internal weights are fixed and not updated during training. The output of this model is a classification result, specified as 'Output 1 Positive'.

**Path 2: Classification using Frozen Embeddings and a Trainable Component**
An input is processed by an 'Embedding model' designed to 'Create embeddings'. This model is also designated as 'Nontrainable' and '"Frozen"', meaning it acts as a fixed feature extractor. The generated embeddings then pass through a series of intermediate steps (represented by four red rectangular blocks) before being fed into a separate component. This final component, visually represented by a grey rounded rectangle with a flame, an S-curve, and dots, is explicitly labeled 'Trainable'. Its parameters can be updated during training to learn a classification mapping from the embeddings. The output of this trainable component is also a classification result, 'Output 1 Positive'.

The significance of the 'Nontrainable' / '"Frozen"' labels is paramount. They indicate that the initial models (both the 'Task-specific model' and the 'Embedding model') are pre-trained and their learned representations are utilized without modification for the current task. This leverages prior knowledge and can be computationally efficient. The 'Trainable' label for the final classification layer in the second path highlights a common transfer learning strategy: utilizing robust, generic features from a frozen embedding model and then training a smaller, task-specific model on top of these features to adapt to a particular dataset or classification problem.

**Key Insights:**
The image conveys several key insights into supervised classification architectures and the application of transfer learning:

*   **Leveraging Pre-trained Models:** Both pathways demonstrate the use of pre-existing knowledge. The 'Task-specific model' for direct classification and the 'Embedding model' for feature extraction are both explicitly labeled 'Nontrainable' and '"Frozen"', underscoring the practice of reusing models without modifying their learned weights. This highlights the efficiency and benefits of using models pre-trained on large datasets.
*   **Two Strategies for Classification:** The diagram presents two primary methods for achieving classification with frozen models:
    1.  **Direct Frozen Classification:** Using a pre-trained 'Task-specific model' that is frozen to directly 'Perform classification'.
    2.  **Frozen Embeddings with a Trainable Head:** Using a frozen 'Embedding model' to 'Create embeddings' (robust feature vectors), then feeding these into a 'Trainable' component that performs the final classification. This demonstrates how a specialized, adaptable layer can be added to leverage generic, fixed features.
*   **The Utility of Embeddings:** The bottom path emphasizes embeddings as a powerful intermediate representation. By extracting 'embeddings' with a frozen model, complex input data is transformed into a fixed, high-quality feature space, making it suitable for subsequent, potentially simpler, 'Trainable' classification layers.
*   **Advantages of Freezing Layers:** The '"Frozen"' and 'Nontrainable' labels highlight practical benefits such as:
    *   **Computational Efficiency:** Reducing the number of parameters to train, leading to faster training times and lower resource consumption.
    *   **Transfer Learning:** Effectively transferring knowledge from a source task (on which the model was pre-trained) to a new target task by reusing powerful feature extractors.
    *   **Mitigating Overfitting:** By keeping a large portion of the model fixed, especially when dealing with limited target task data, it helps prevent overfitting to the small dataset.

**Document Context:**
This image directly supports the document's section on 'Supervised Classification' and the subsequent text, which states: 'In Chapter 4, we used pretrained models to perform classification without updating their weight. These models were kept “frozen.”' The diagram visually elaborates on this statement by showing concrete examples of how 'frozen' (nontrainable) models are utilized. It helps the reader understand the architectural implications and different strategies involved when working with pre-trained models where weights are intentionally kept static, and how this contrasts with or complements trainable components in a classification pipeline. It acts as a foundational visual aid to explain the practical application of 'frozen' models in the context of supervised learning.

**Summary:**
This image, titled 'Supervised Classification', provides a visual explanation of two primary strategies for performing classification using pre-trained models, particularly highlighting the concept of 'frozen' (nontrainable) versus 'trainable' components. 

**The first workflow (top path):**
An input, represented by a series of three dots (...), is fed into a 'Task-specific model' (a light blue box with a dashed border, an asterisk in the top-left, and a small 'arrow-out-of-box' symbol in the top-right). This model's explicit function is to 'Perform classification'. Crucially, this 'Task-specific model' is part of a larger conceptual block that is clearly labeled 'Nontrainable' below it, and further described as '"Frozen"' within an accompanying iceberg graphic. This signifies that the internal parameters (weights) of this model are fixed and will not be adjusted during any subsequent training process. After processing by this frozen, task-specific model, the system directly produces an 'Output' (a green rounded rectangle) labeled '1 Positive', indicating a binary classification result.

**The second workflow (bottom path):**
Similarly, an input, also represented by a series of three dots (...), is fed into an 'Embedding model' (another light blue box with a dashed border, an asterisk in the top-left, and a small 'arrow-out-of-box' symbol in the top-right). The purpose of this model is to 'Create embeddings'. Like the 'Task-specific model', this 'Embedding model' is also part of the 'Nontrainable' and '"Frozen"' component, meaning its weights remain fixed. This model acts as a fixed feature extractor, converting raw inputs into numerical vector representations (embeddings).

These generated embeddings then pass through a sequence of four interconnected red rectangular blocks. These blocks visually suggest intermediate processing layers or a data pipeline. The output of these red blocks is then fed into a distinct grey rounded rectangular component. This component is visually depicted with a flame icon (often symbolizing an activation function), a purple S-curve graph (suggesting a learned mapping or activation), and a series of eight dots below the curve. This grey component is explicitly labeled 'Trainable' directly beneath it. This is the key distinction for this path, meaning that the parameters of this specific model *can* be adjusted and learned during a training phase. This trainable component then produces its own 'Output' (a green rounded rectangle) also labeled '1 Positive'.

In summary, the diagram visually articulates two strategies for supervised classification: using a directly pre-trained, frozen classifier versus using a pre-trained, frozen embedding extractor in conjunction with a new, trainable classification head. It explains how leveraging fixed, powerful feature extractors (the frozen models) can be combined with a lightweight, trainable component to adapt to new classification tasks.](images/ff3ee819dc97446e0f37a71de6d656c83cc4efb3cf9cc2f508e88e454b445cf3.jpg)
Figure 11-1. In Chapter 4, we used pretrained models to perform classification without updating their weight. These models were kept “frozen.”

Both models were kept frozen (nontrainable) to showcase the potential of leveraging pretrained models for classification tasks. The embedding model uses a separate trainable classification head (classifier) to predict the sentiment of movie reviews.

In this section, we will take a similar approach but allow both the model and the classification head to be updated during training. As shown in Figure 11-2, instead of using an embedding model, we will fine-tune a pretrained BERT model to create a task-specific model similar to the one we used in Chapter 2. Compared to the embedding model approach, we will fine-tune both the representation model and the classification head as a single architecture.

![## Image Analysis: da47e0c7bdceb4968afc62f38ff2c4974e427753b17e2e49f17f0a32ab903e90.jpg

**Conceptual Understanding:**
This image conceptually illustrates two fundamental architectural paradigms for applying pre-trained models to downstream supervised classification tasks: one where the core pre-trained model is "frozen" and acts purely as a feature extractor, and another where the entire pre-trained model is "fine-tuned" alongside a classification layer. The main purpose of the image is to visually compare and contrast these two methodologies, highlighting which components are "trainable" (i.e., have their parameters updated during training) versus "nontrainable" (i.e., remain fixed). It aims to convey the difference between simply attaching a trainable head to a static feature extractor versus training the entire model stack end-to-end to adapt it to a new task. The key ideas communicated are transfer learning, the concept of freezing layers, and the more comprehensive fine-tuning of large pre-trained models like BERT.

**Content Interpretation:**
The image displays two architectural patterns for implementing supervised classification, specifically in the context of leveraging pre-trained language models. The first pattern, labeled "Frozen" architecture, depicts a scenario where an "Embedding model" is "Nontrainable" and effectively "Frozen," meaning its weights are not updated during fine-tuning. This frozen embedding model feeds into a "Classification head" which is "Trainable," implying that only the parameters of this final classification layer are adjusted. The classification head is visually represented by a sigmoid-like curve, often used in binary classification, with a flame icon indicating its trainability. The second pattern, "Trainable architecture," shows a "BERT" model, which is "Trainable" (indicated by a flame icon), followed by a "Classification head" which is also "Trainable" (indicated by a flame icon and a neural network diagram). In this setup, the entire architecture, from the BERT model to the classification head, is fine-tuned as a single, cohesive unit. The neural network diagram within the classification head suggests a multi-layer perceptron or a more complex classification layer capable of learning from the features provided by BERT. The presence of flame icons consistently highlights the components that undergo parameter updates during the training process.

**Key Insights:**
The main takeaways from this image are: 

1.  **Two Fine-tuning Strategies:** There are distinct approaches to fine-tuning pre-trained models like embedding models or BERT for supervised classification. 
    *   Evidence: The presence of two clearly separated sections titled ""Frozen" architecture" and "Trainable architecture."

2.  **"Frozen" Model Approach (Feature Extractor):** In one approach, the pre-trained feature extractor (e.g., "Embedding model") is kept static ("Nontrainable "Frozen""), and only a newly added "Classification head" is trained. This strategy leverages the pre-trained model as a fixed feature extractor. 
    *   Evidence: Text "Nontrainable" and the "Frozen" iceberg under "Embedding model," and "Trainable" under "Classification head" in the "Frozen" architecture section.

3.  **End-to-End "Trainable" Model Approach (Full Fine-tuning):** In another approach, the entire pre-trained model (e.g., "BERT") along with the "Classification head" is trained ("Fine-tuned as a single architecture"). This allows for more adaptive learning, potentially leading to better performance by adjusting the pre-trained model's internal representations to the specific downstream task. 
    *   Evidence: Text "Trainable" under "BERT" and "Trainable" under "Classification head" in the "Trainable architecture" section, and the subtitle "Fine-tuned as a single architecture."

4.  **Trainability Indication:** The flame icon consistently indicates which parts of the architecture are trainable and undergo weight updates during the learning process.
    *   Evidence: Flame icons appear next to "Trainable" components: the "Classification head" in the "Frozen" architecture, and both "BERT" and the "Classification head" in the "Trainable architecture."

These insights are crucial for understanding the practical considerations and trade-offs when applying pre-trained models to specific tasks, particularly in terms of computational cost and performance.

**Document Context:**
This image directly supports the "Supervised Classification" section of the document by visually comparing two methodologies for utilizing pre-trained models. The text after the image, "Figure 11-2. Compared to the “frozen” architecture, we instead train both the pretrained BERT model and the classification head. A backward pass will start at the classification head and go through BERT," serves as a direct explanation of the "Trainable architecture" shown on the right side of the figure, explicitly contrasting it with the "Frozen" architecture on the left. The image clarifies the architectural choices and their implications for model training, specifically demonstrating how the gradients flow back through the entire BERT model during a backward pass in the trainable setup, which is a key concept in fine-tuning large language models for downstream tasks.

**Summary:**
This image illustrates two distinct architectures for fine-tuning pre-trained models in supervised classification tasks: the "Frozen" architecture and the "Trainable" architecture. 

The "Frozen" architecture involves an "Embedding model" that is "Nontrainable" (symbolized by an iceberg with "Frozen" text). This model's outputs are then fed into a "Classification head," which is "Trainable" (indicated by a flame icon and a curve representing a classifier). In this setup, only the classification head is updated during training.

In contrast, the "Trainable architecture" uses a "BERT" model, which is fully "Trainable" (indicated by a flame icon). The output from BERT is then passed to a "Classification head," which is also fully "Trainable" (indicated by a flame icon and a neural network diagram). Here, both the BERT model and the classification head are fine-tuned together as a single architecture. 

Essentially, the image demonstrates two strategies: one where the feature extractor (embedding model) remains static, and another where the entire model stack, including the pre-trained language model (BERT) and the classification layer, is trained end-to-end. This aligns with the document context by showing how a "backward pass will start at the classification head and go through BERT" in the trainable architecture.](images/da47e0c7bdceb4968afc62f38ff2c4974e427753b17e2e49f17f0a32ab903e90.jpg)
Figure 11-2. Compared to the “frozen” architecture, we instead train both the pretrained BERT model and the classification head. A backward pass will start at the classification head and go through BERT.

To do so, instead of freezing the model, we allow it to be trainable and update its parameters during training. As illustrated in Figure 11-3, we will use a pretrained BERT model and add a neural network as a classification head, both of which will be fine-tuned for classification.

![## Image Analysis: 44737b1db3c6fc0c15654da8c2d6357b8a0de9d2c3e2439ba7d5879735b938f1.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a deep learning model for supervised text classification, specifically illustrating the use of a pre-trained language model (BERT) combined with a task-specific classification layer. The main purpose of the diagram is to show how a raw text input is processed through tokenization, encoded by a pre-trained representation model, and then classified by a trainable head to produce a final prediction, such as sentiment (positive or negative) with associated probabilities. It conveys the idea of transfer learning in natural language processing, where the knowledge from a large pre-trained model is adapted for a new, specific task through fine-tuning.

**Content Interpretation:**
The image displays a sequential process for text classification, likely sentiment analysis, using a hybrid neural network architecture. It demonstrates how a raw text "Input" of "What a horrible movie!" is first processed. This input is broken down into "Tokens" including special tokens like "[CLS]", "what", "a", "horrible", "movie", "!", and "[SEP]". These tokens are then fed into a large rectangular block labeled "Pretrained" "BERT", which represents a Bidirectional Encoder Representations from Transformers model. The BERT model processes these tokens, and its outputs are then passed to a lower rectangular block labeled "Classification head" which contains a "Feedforward neural network". This feedforward neural network then produces the final classification. The output shows two percentage values, "25%" and "75%", associated with "Positive" and "Negative" classifications respectively. A dashed line encompasses both the "BERT" model and the "Feedforward neural network", with a label "Trainable" and a flame icon, indicating that these parts of the model can be fine-tuned or trained for the specific task. The significance of the output "25% Positive" and "75% Negative" is that for the given input "What a horrible movie!", the model predicts a high probability of negative sentiment.

**Key Insights:**
The main takeaways from this image are: 1.  **Modular Architecture for NLP Tasks:** The image demonstrates a standard modular architecture for supervised classification in NLP, consisting of a pre-trained representation model and a task-specific classification head. The text labels "Pretrained" and "Classification head" explicitly show this separation. 2.  **Role of Pre-trained Models (BERT):** A pre-trained model like "BERT" is used to generate powerful contextualized representations of input text, as indicated by the arrows going from tokenized input to BERT and then to subsequent layers. 3.  **Fine-tuning for Specific Tasks:** The "Trainable" label and the dashed box around BERT and the "Feedforward neural network" highlight that these components can be fine-tuned or trained on a specific dataset for the target task. This contrasts with purely fixed feature extraction. 4.  **Tokenization Process:** Input text is not fed directly but broken down into "Tokens", including special tokens like "[CLS]" (classification token) and "[SEP]" (separator token), which are crucial for BERT's operation. 5.  **Output of Classification:** The model outputs probability distributions for different classes (e.g., "Positive" and "Negative"), providing a measure of confidence in its prediction (e.g., "25%" Positive, "75%" Negative). The specific text "What a horrible movie!" leading to "75% Negative" is strong evidence for sentiment classification.

**Document Context:**
The image directly supports the document section on "Supervised Classification" by visually detailing the architecture of a task-specific model. It serves as a concrete example of how a pre-trained representation model, specifically BERT, is leveraged for a classification task by adding a specialized classification head. The accompanying text, "Figure 11-3. The architecture of a task-specific model. It contains a pretrained representation model (e.g., BERT) with an additional classification head for the specific task," perfectly aligns with the visual representation, explaining that the diagram illustrates a common approach in natural language processing where powerful pre-trained models are fine-tuned for specific downstream tasks. The example input "What a horrible movie!" and its classified output (25% Positive, 75% Negative) clearly demonstrate the model's function in sentiment analysis.

**Summary:**
This image illustrates the architecture of a task-specific model designed for supervised classification, specifically sentiment analysis. The process begins with an "Input" text, which is then tokenized and processed by a "Pretrained" BERT model. The output from BERT is fed into a "Trainable" "Feedforward neural network" (the "Classification head"), which predicts the sentiment (Positive/Negative) with associated probabilities. The BERT model itself is also considered "Trainable" in this context, as indicated by the dashed box and flame icon, implying fine-tuning. The diagram clearly shows the flow of information from raw text to a classified output, highlighting the modular nature of using a pre-trained representation model combined with a task-specific classification layer.](images/44737b1db3c6fc0c15654da8c2d6357b8a0de9d2c3e2439ba7d5879735b938f1.jpg)
Figure 11-3. The architecture of a task-specific model. It contains a pretrained represen‐ tation model (e.g., BERT) with an additional classification head for the specific task.

In practice, this means that the pretrained BERT model and the classification head are updated jointly. Instead of independent processes, they learn from one another and allow for more accurate representations.

# Fine-Tuning a Pretrained BERT Model

We will be using the same dataset we used in Chapter 4 to fine-tune our model, namely the Rotten Tomatoes dataset, which contains 5,331 positive and 5,331 nega‐ tive movie reviews from Rotten Tomatoes:

from datasets import load_dataset   
# Prepare data and splits   
tomatoes $=$ load_dataset("rotten_tomatoes")   
train_data, test_data $=$ tomatoes["train"], tomatoes["test"]

The first step in our classification task is to select the underlying model we want to use. We use "bert-base-cased", which was pretrained on the English Wikipedia as well as a large dataset consisting of unpublished books.1

We define the number of labels that we want to predict beforehand. This is necessary to create the feedforward neural network that is applied on top of our pretrained model:

from transformers import AutoTokenizer, AutoModelForSequenceClassification   
# Load model and tokenizer   
model_id $=$ "bert-base-cased"   
model $=$ AutoModelForSequenceClassification.from_pretrained( model_id, num_labels $\ : = \ 1$   
)   
tokenizer $=$ AutoTokenizer.from_pretrained(model_id)

Next, we will tokenize our data:

from transformers import DataCollatorWithPadding   
# Pad to the longest sequence in the batch   
data_collator $=$ DataCollatorWithPadding(tokenizer $=$ tokenizer)   
def preprocess_function(examples): """Tokenize input data""" return tokenizer(examples["text"], truncation=True)   
# Tokenize train/test data   
tokenized_train $=$ train_data.map(preprocess_function, batched=True)   
tokenized_test $=$ test_data.map(preprocess_function, batched=True)

Before creating the Trainer, we will want to prepare a special DataCollator. A DataCollator is a class that helps us build batches of data but also allows us to apply data augmentation.

During this process of tokenization, and as shown in Chapter 9, we will add padding to the input text to create equally sized representations. We use DataCollatorWith Padding for that.

Of course, an example would not be complete without defining some metrics:

import numpy as np   
from datasets import load_metric   
def compute_metrics(eval_pred): """Calculate F1 score""" logits, labels $=$ eval_pred   
load_f1 $=$ load_metric("f1")   
$\textsf { f } 1 \ =$ load_f1.compute(predictions $=$ predictions, references $=$ labels)["f1"]   
return {"f1": f1}

With compute_metrics we can define any number of metrics that we are interested in and that can be printed out or logged during training. This is especially helpful during training as it allows for detecting overfitting behavior.

Next, we instantiate our Trainer:

from transformers import TrainingArguments, Trainer   
# Training arguments for parameter tuning   
training_args $=$ TrainingArguments( "model", learning_rate $\begin{array} { r l } { \mathbf { \Psi } : } & { { } \mathbf { \Psi } : \mathbf { \Psi } } \end{array}$ , per_device_train_batch_size $= 1 6$ , per_device_eval_batch_size $= 1 6$ , num_train_epochs $^ { = 1 }$ , weight_decay ${ \tt = } \Theta$ .01, save_strategy="epoch", report_to="none"   
)   
# Trainer which executes the training process   
trainer $=$ Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer $\mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \tilde { \tilde { \mathbf { \tilde } } } } } } } } } } }$ tokenizer, data_collator $=$ data_collator, compute_metrics $=$ compute_metrics,   
)

The TrainingArguments class defines hyperparameters we want to tune, such as the learning rate and how many epochs (rounds) we want to train. The Trainer is used to execute the training process.

Finally, we can train our model and evaluate it:

trainer.evaluate()

{'eval_loss': 0.3663691282272339, 'eval_f1': 0.8492366412213741, 'eval_runtime': 4.5792, 'eval_samples_per_second': 232.791, 'eval_steps_per_second': 14.631, 'epoch': 1.0}

We get an F1 score of 0.85, which is quite a bit higher than the task-specific model we used in Chapter 4, which resulted in an F1 score of 0.80. It shows that fine-tuning a model yourself can be more advantageous than using a pretrained model. It only costs us a couple of minutes to train.

# Freezing Layers

To further showcase the importance of training the entire network, the next example will demonstrate how you can use Hugging Face Transformers to freeze certain layers of your network.

We will freeze the main BERT model and allow only updates to pass through the classification head. This will be a great comparison as we will keep everything the same, except for freezing specific layers.

To start, let’s reinitialize our model so we can start from scratch:

# Load model and tokenizer   
model $=$ AutoModelForSequenceClassification.from_pretrained( model_id, num_labels $: = 2$   
)   
tokenizer $=$ AutoTokenizer.from_pretrained(model_id)

Our pretrained BERT model contains a lot of layers that we can potentially freeze. Inspecting these layers gives insight into the structure of the network and what we might want to freeze:

# Print layer names   
for name, param in model.named_parameters(): print(name)   
bert.embeddings.word_embeddings.weight   
bert.embeddings.position_embeddings.weight   
bert.embeddings.token_type_embeddings.weight   
bert.embeddings.LayerNorm.weight   
bert.embeddings.LayerNorm.bias   
bert.encoder.layer.0.attention.self.query.weight   
bert.encoder.layer.0.attention.self.query.bias   
bert.encoder.layer.11.output.LayerNorm.weight   
bert.encoder.layer.11.output.LayerNorm.bias   
bert.pooler.dense.weight   
bert.pooler.dense.bias   
classifier.weight   
classifier.bias

There are 12 (0–11) encoder blocks consisting of attention heads, dense networks, and layer normalization. We further illustrate this architecture in Figure 11-4 to demonstrate everything that could be potentially frozen. On top of that, we have our classification head.

![## Image Analysis: d97e9205a204b23d81aaebb3cdd276f75bb1b1f755e37b8df0b3b3a59236f663.jpg

**Conceptual Understanding:**
The image conceptually represents the architecture of a BERT model augmented with a classification head. Its main purpose is to illustrate how the core BERT encoder layers process input and then pass their learned representations to a subsequent feedforward neural network that performs a final classification, such as binary sentiment analysis, providing probabilistic outputs. It communicates the idea of using pre-trained deep encoders for feature extraction, followed by a task-specific prediction layer.

**Content Interpretation:**
The image displays a stacked architecture consisting of a series of "ENCODER" blocks, labeled from "0" to "11" (with an ellipsis indicating omitted layers), forming the core of a BERT model. Below these encoders are multiple small purple rectangular blocks representing input data (likely token embeddings). The output from the "ENCODER" stack is then fed into a "Feedforward neural network", which serves as an additional classification head. This head produces a binary classification output, exemplified by "Positive 25% 75% Negative", indicating predicted probabilities or scores for two distinct classes. The small diagonal arrow symbol within each "ENCODER" block is a visual cue for transformer encoder mechanisms.

**Key Insights:**
1.  **BERT's Modularity:** The distinct separation of "ENCODER" layers from the "Feedforward neural network" demonstrates that BERT's pre-trained encoder stack acts as a robust feature extractor, to which various task-specific heads can be appended. 
2.  **Deep Layered Architecture:** The labeling "0" through "11" for the "ENCODER" blocks signifies BERT's deep, multi-layered structure, crucial for learning complex language representations. 
3.  **Probabilistic Classification Output:** The output "Positive 25% 75% Negative" shows that the classification head provides probabilistic scores, offering a measure of confidence for each predicted class, which is vital for decision-making in real-world applications. 
4.  **Standard Fine-tuning Paradigm:** The overall arrangement exemplifies a common fine-tuning approach where a pre-trained transformer model is adapted for new tasks by adding simple classification layers on top.

**Document Context:**
This image directly relates to the document's broader narrative by illustrating "The basic architecture of BERT with the additional classification head," as stated in the figure caption. It visually explains how the foundational BERT encoder layers are combined with a specialized "Feedforward neural network" for specific downstream tasks, particularly classification. This helps the reader understand the modularity and extensibility of BERT for fine-tuning.

**Summary:**
This diagram illustrates the basic architecture of BERT (Bidirectional Encoder Representations from Transformers) when extended with an additional classification head for tasks like binary classification. 

At the core, the architecture consists of multiple "ENCODER" layers, depicted as stacked blocks. These layers are numerically indexed from "0" to "11", indicating a total of 12 such encoder layers, which process the input sequence. An ellipsis "..." is used to signify that intermediate layers between 1 and 11 are present but not explicitly drawn. Each "ENCODER" block also contains a small diagonal arrow symbol, which is a common visual shorthand for the internal mechanisms of a transformer encoder, such as attention. A large purple bounding box visually groups these "ENCODER" layers, emphasizing that they form the main body of the BERT model. 

Below these ENCODER layers, a series of small, individual purple rectangular blocks represent the input to the system, likely the token embeddings of a sequence. These inputs feed into the ENCODER layers. After processing through all the stacked ENCODER layers, the output (typically the pooled output or the representation of the [CLS] token from the final layer) is then fed downwards through vertical lines and arrows. 

This output then enters a distinct component labeled "Feedforward neural network". This "Feedforward neural network" acts as the "additional classification head" for BERT. It processes the representations generated by the BERT encoders to make a final prediction. 

The ultimate output of this system is a classification, shown at the bottom. In this example, it's a binary classification task with two categories: "Positive" and "Negative". The diagram indicates a predicted probability or score distribution: "25%" for "Positive" and "75%" for "Negative", suggesting that for this specific input, the model has a higher confidence (75%) that the input belongs to the "Negative" class. 

In essence, the image demonstrates how BERT's deep encoding capabilities are leveraged, and a simple feedforward network is added on top to transform BERT's learned representations into a specific classification prediction, complete with probability scores.](images/d97e9205a204b23d81aaebb3cdd276f75bb1b1f755e37b8df0b3b3a59236f663.jpg)
Figure 11-4. The basic architecture of BERT with the additional classification head.

We could choose to only freeze certain layers to speed up computing but still allow the main model to learn from the classification task. Generally, we want frozen layers to be followed by trainable layers.

We are going to freeze everything except for the classification head as we did in Chapter 2:

for name, param in model.named_parameters(): # Trainable classification head if name.startswith("classifier"): param.requires_grad $=$ True # Freeze everything else else: param.requires_grad $=$ False

As shown in Figure 11-5, we have frozen everything except for the feedforward neural network, which is our classification head.

![## Image Analysis: e94c0e3bde96225a4ef1a466fa2279d43d1001753835c76a294357a9dde5b589.jpg

**Conceptual Understanding:**
This image conceptually represents a neural network architecture configured for a transfer learning or fine-tuning task. Its main purpose is to illustrate the distinction between 'frozen' (nontrainable) and 'trainable' layers within a deep learning model, particularly in the context of leveraging pre-trained encoders for a specific downstream application. The key ideas communicated are the modularity of deep learning models, the strategy of keeping foundational layers fixed to preserve general knowledge, and the addition of a task-specific head for specialized learning, ultimately leading to a classification output.

**Content Interpretation:**
This image illustrates a neural network architecture that outlines a transfer learning or fine-tuning process. It depicts a multi-layered encoder system followed by a feedforward neural network, culminating in a binary classification output. The core process involves: 

1.  **Encoder Layers (0-11):** A stack of twelve 'ENCODER' blocks, indexed from 0 to 11. These blocks represent the foundational layers of a pre-trained model (like BERT, as indicated by the document context). The text 'Nontrainable' and 'Frozen' explicitly states that the parameters/weights of these layers are not updated during the subsequent training phase.

2.  **Intermediate Representations:** The output of the encoder layers feeds into a series of intermediate representations, visually depicted as eight small purple rectangular blocks. These do not contain explicit text but signify the processed features from the encoders.

3.  **Feedforward Neural Network:** This component receives input from the intermediate representations. It is labeled 'Feedforward neural network' and is explicitly designated as 'Trainable,' indicating that its parameters will be updated and learned during the fine-tuning process.

4.  **Binary Classification Output:** The final stage shows the output of the feedforward network as a binary classification with example probabilities: 'Positive 25%' and '75% Negative'. This represents the model's prediction for two distinct classes.

**Key Insights:**
The image provides several key insights regarding neural network fine-tuning and architecture:

*   **Transfer Learning Strategy:** The most prominent takeaway is the clear depiction of a transfer learning approach, where a significant portion of a pre-trained model ('ENCODER' layers 0-11) is kept 'Nontrainable' and 'Frozen' to leverage learned features, while a new, smaller component ('Feedforward neural network') is made 'Trainable' to adapt to a specific downstream task. This is evidenced by the explicit labels 'Nontrainable', 'Frozen' applied to the encoder layers and 'Trainable' for the feedforward network.
*   **Efficient Fine-tuning:** By freezing the extensive encoder layers, the image implies a more efficient fine-tuning process. This prevents catastrophic forgetting of pre-trained knowledge and reduces the computational cost and time required to train the model on a new, potentially smaller dataset. The 'Frozen' state directly supports this.
*   **Task-Specific Adaptation:** The 'Feedforward neural network' acts as a task-specific head, responsible for mapping the high-level features extracted by the encoders to the desired output. Its 'Trainable' status indicates that this component is specifically optimized for the new task, demonstrating how general knowledge from the pre-trained model is specialized.
*   **Binary Classification Output:** The 'Positive 25%' and '75% Negative' output clearly indicates that the model is performing a binary classification task and provides an example of how the model's predictions might be presented probabilistically.

**Document Context:**
The image directly supports and visually explains the document's narrative, particularly the section on 'Print layer names' and the text immediately following the figure: 'We fully freeze all encoder blocks and embedding layers such that the BERT model does not learn new representations during fine-tuning.'

The diagram precisely illustrates this freezing mechanism. The 'ENCODER' blocks (0-11) are visually bracketed and explicitly labeled 'Nontrainable' and 'Frozen', perfectly matching the description of 'freezing all encoder blocks and embedding layers.' The 'Feedforward neural network' is then labeled 'Trainable', indicating the part of the model that *does* learn new representations for the specific task. Thus, the image serves as a clear, detailed visual aid demonstrating the fine-tuning strategy applied to a BERT model, showing which layers are kept static and which are adapted to the new task, thereby enhancing comprehension of the technical procedure being described in the text.

**Summary:**
This diagram illustrates a neural network architecture designed for fine-tuning, likely for a classification task, as indicated by the 'Positive' and 'Negative' outputs. The model consists of two main components: a stack of 'ENCODER' layers and a 'Feedforward neural network'.

The initial part of the model comprises twelve 'ENCODER' layers, explicitly indexed from '0' to '11' on the left side of the diagram. These encoder blocks are enclosed within a purple bracket and are explicitly labeled as 'Nontrainable' and visually indicated as 'Frozen' by an accompanying iceberg icon. This signifies that the weights of these encoder layers are fixed and will not be updated during the training process, preserving their pre-trained knowledge.

The output from these encoder layers feeds into a series of eight small, purple rectangular blocks (which have no text but visually represent intermediate representations or embedding outputs). These then connect to a larger block labeled 'Feedforward neural network.' This feedforward network is separately enclosed by a dotted line and is explicitly labeled as 'Trainable,' also accompanied by a flame icon. This indicates that the weights of this specific part of the network *will* be adjusted and learned during the fine-tuning phase to adapt the model to the specific downstream task.

Finally, the output of the 'Feedforward neural network' leads to a classification result. This is shown as two adjacent blocks: '25%' labeled 'Positive' on its left, and '75%' labeled 'Negative' on its right. This represents a binary classification prediction, where the model outputs probabilities for two classes, in this example, predicting a 25% chance of being 'Positive' and a 75% chance of being 'Negative'. The overall diagram clearly depicts a transfer learning strategy where a pre-trained, frozen encoder provides robust features to a trainable, task-specific classification head.](images/e94c0e3bde96225a4ef1a466fa2279d43d1001753835c76a294357a9dde5b589.jpg)
Figure 11-5. We fully freeze all encoder blocks and embedding layers such that the BERT model does not learn new representations during fine-tuning.

Now that we have successfully frozen everything but the classification head, we can move on to train our model:

from transformers import TrainingArguments, Trainer   
# Trainer which executes the training process   
trainer $=$ Trainer( model=model, args $\mathbf { \equiv }$ training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator $\mathbf { \equiv } =$ data_collator, compute_metrics $=$ compute_metrics,   
)   
trainer.train()

You might notice that training has become much faster. That is because we are only training the classification head, which provides us with a significant speedup compared to fine-tuning the entire model:

trainer.evaluate()

{'eval_loss': 0.6821751594543457, 'eval_f1': 0.6331058020477816, 'eval_runtime': 4.0175, 'eval_samples_per_second': 265.337, 'eval_steps_per_second': 16.677, 'epoch': 1.0}

When we evaluate the model, we only get an F1 score of 0.63, which is quite a bit lower compared to our original 0.85 score. Instead of freezing nearly all layers, let’s freeze everything up until encoder block 10 as illustrated in Figure 11-6, and see how it affects performance. A major benefit is that this reduces computation but still allows updates to flow through part of the pretrained model:

![## Image Analysis: 40f99b2f062eac6a13fefd6288bcb4c51676eeaf354aa949b8e961216ae48e72.jpg

**Conceptual Understanding:**
This image conceptually represents a common strategy in deep learning, specifically transfer learning and fine-tuning applied to a transformer-based neural network architecture (implied to be BERT by the document context). The main purpose is to visually distinguish between the "frozen" (nontrainable) and "trainable" parts of the model during a fine-tuning phase. It communicates the idea that early layers of a pre-trained model are often kept fixed to preserve general learned representations, while later layers and a task-specific head are fine-tuned for a particular application.

**Content Interpretation:**
The image demonstrates the architectural structure and training strategy for a deep learning model. The repetitive "ENCODER" labels for layers "0" through "11" indicate a stacked encoder architecture, characteristic of models like BERT. The explicit labels "Nontrainable" and ""Frozen"" associated with "ENCODER" blocks "0" through "9" (and the iceberg icon) signify that these layers' parameters are held constant. This practice, known as feature extraction or freezing layers, leverages the general representations learned during the pre-training phase of the model. The number "10" (from 0 to 9) precisely tells us how many initial encoder blocks are frozen. The label "Trainable" (with the flame icon) attached to "ENCODER" blocks "10" and "11", the series of "eight small, rectangular purple blocks", and the "Feedforward neural network" indicates that these components' parameters will be updated during the training process. This is the fine-tuning part, where the model adapts to the specific downstream task. The "Feedforward neural network" acts as the task-specific classification head. Its role is to take the representations from the encoder layers and transform them into class predictions. This final output signifies a binary classification task, likely sentiment analysis (given "Positive" and "Negative"). The percentages "25%" and "75%" represent the predicted probabilities for each class, demonstrating a specific prediction from the model. All extracted text elements, especially "ENCODER", "Nontrainable", ""Frozen"", "Trainable", "Feedforward neural network", and "Positive 25% 75% Negative", precisely support the interpretation of a deep learning model being fine-tuned, with early layers frozen and later layers adapted for a classification task.

**Key Insights:**
Main Takeaway 1: Strategic Fine-tuning: The image demonstrates a common and effective strategy for fine-tuning large pre-trained language models like BERT. Instead of training the entire model from scratch or fine-tuning all layers, only a subset of the later layers and a newly added classification head are updated. This is evidenced by the clear distinction between "Nontrainable" (first 10 encoders) and "Trainable" (last 2 encoders and subsequent network). Main Takeaway 2: Leveraging Pre-trained Knowledge: Freezing the initial encoder blocks (0-9) implies that these layers have already learned highly general and transferable representations of language from a massive dataset during pre-training. By keeping them "Frozen", the model retains this fundamental knowledge, which is crucial for transfer learning. This is directly supported by the labels "Nontrainable" and ""Frozen"" for layers "0" through "9". Main Takeaway 3: Task-Specific Adaptation: The "Trainable" sections, particularly "ENCODER" blocks "10" and "11" and the "Feedforward neural network", are responsible for adapting the model's representations to the nuances of the specific downstream task. This allows the model to become proficient at a new task without losing the broader language understanding. The output "Positive 25% 75% Negative" clearly indicates a task-specific classification objective. Main Takeaway 4: Efficiency in Training: This selective training approach can significantly reduce computational resources and training time compared to fine-tuning the entire model, while often achieving comparable or better performance. While not explicitly stated, this is an implicit benefit of the "Nontrainable" / "Trainable" distinction in deep learning.

**Document Context:**
This image fits perfectly within a document section discussing the "Trainer which executes the training process" and specifically explains the fine-tuning strategy for a BERT model. The accompanying text, "Figure 11-6. We freeze the first 10 encoder blocks of our BERT model. Everything else is trainable and will be fine-tuned," directly correlates with the visual representation. The diagram serves as a clear visual aid to understand how the fine-tuning is implemented, illustrating which parts of the BERT architecture are modified during training and which remain fixed. It visually confirms the number of frozen encoder blocks (0-9, totaling 10 blocks) and highlights the trainable components.

**Summary:**
This diagram illustrates the architecture of a neural network, likely a BERT model, and highlights its configuration during a fine-tuning training process. The model is composed of multiple "ENCODER" blocks stacked sequentially. The top portion of the diagram, encompassing "ENCODER" blocks from layer "0" through layer "9" (a total of 10 blocks), is designated as "Nontrainable" or ""Frozen"". This means that the weights and parameters within these initial ten encoder layers will remain unchanged during the training phase. An iceberg icon visually reinforces this "frozen" state, implying that these foundational layers retain the generalized knowledge acquired during the model's initial pre-training. Below the frozen layers, "ENCODER" blocks "10" and "11" are depicted. These, along with all subsequent components, are marked as "Trainable", indicated by a flame icon. This signifies that their weights and parameters will be updated and adjusted through backpropagation during the fine-tuning process. Following these trainable encoder blocks, the data passes through a series of "eight small, rectangular purple blocks" representing further processing or feature extraction steps. Finally, the processed information is fed into a "Feedforward neural network", which is also "Trainable". This feedforward network acts as the task-specific classification head. The ultimate output of this network is a binary classification, exemplified by a prediction of "25%" for "Positive" and "75%" for "Negative". This setup demonstrates a common transfer learning approach where a pre-trained model's core language understanding (in the frozen layers) is preserved, while its later layers and a new output head are adapted to solve a specific problem, such as sentiment analysis.](images/40f99b2f062eac6a13fefd6288bcb4c51676eeaf354aa949b8e961216ae48e72.jpg)
Figure 11-6. We freeze the first 10 encoder blocks of our BERT model. Everything else is trainable and will be fine-tuned.

# Load model   
model_id $=$ "bert-base-cased"   
model $=$ AutoModelForSequenceClassification.from_pretrained( model_id, num_labels $\scriptstyle : = \ .$   
)   
tokenizer $=$ AutoTokenizer.from_pretrained(model_id)   
# Encoder block 11 starts at index 165 and   
# we freeze everything before that block   
for index, (name, param) in enumerate(model.named_parameters()): if index $<$ 165: param.requires_grad $=$ False   
# Trainer which executes the training process   
trainer $=$ Trainer( model=model, args $\mathbf { \Psi } = \mathbf { \Psi }$ training_args, train_dataset $: =$ tokenized_train, eval_dataset=tokenized_test,

tokenizer $\mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \mathbf { \tilde { \tilde { \tilde { \mathbf { \tilde } } } } } } } } } } }$ tokenizer, data_collator $=$ data_collator, compute_metrics $=$ compute_metrics, ) trainer.train()

After training, we evaluate the results:

trainer.evaluate()   
{'eval_loss': 0.40812647342681885, 'eval_f1': 0.8, 'eval_runtime': 3.7125, 'eval_samples_per_second': 287.137, 'eval_steps_per_second': 18.047, 'epoch': 1.0}

We got an F1 score of 0.8, which is much higher than our previous score of 0.63 when freezing all layers. It demonstrates that although we generally want to train as many layers as possible, you can get away with training less if you do not have the necessary computing power.

To further illustrate this effect, we tested the effect of iteratively freezing encoder blocks and fine-tuning them as we did thus far. As shown in Figure 11-7, training only the first five encoder blocks (red vertical line) is enough to almost reach the performance of training all encoder blocks.

![## Image Analysis: 2d2a2f55867a8fe0f84fa5397a091f7c65a03726ad22166d15bf3026005800e5.jpg

**Conceptual Understanding:**
This image conceptually represents the optimization process of a machine learning model, specifically focusing on the impact of fine-tuning different layers of an encoder. The main purpose of the graph is to demonstrate how the model's F1-score (a measure of accuracy) changes as progressively more encoder blocks are unfrozen and included in the training process. The core message conveyed is that while unfreezing more blocks initially improves performance, there is a point where performance gains plateau, suggesting that an optimal number of trainable blocks might exist that offers good performance without the need to train the entire encoder.

**Content Interpretation:**
The image is a line graph illustrating the relationship between the number of trainable (unfrozen) encoder blocks in a model and its resulting F1-score, a common metric for evaluating classification model performance. The graph shows that as more encoder blocks are made trainable, the F1-score generally increases. Specifically, the performance rapidly improves from "None" trainable blocks (F1-score ~0.70) up to "0-4" trainable blocks (F1-score ~0.842). Beyond "0-4" trainable blocks, the F1-score improvement significantly slows down and the curve flattens, as explicitly indicated by the "Performance stabilizing" annotation. The red dotted line at "0-4" trainable blocks marks the approximate point where this stabilization begins. The F1-score at "all" trainable blocks (~0.858) is only marginally higher than at "0-4", indicating diminishing returns for unfreezing additional blocks.

**Key Insights:**
1. **Initial Performance Improvement:** Making more encoder blocks trainable initially leads to a significant improvement in model performance, as evidenced by the sharp increase in the "F1-score" from "None" trainable blocks (~0.70) up to "0-4" trainable blocks (~0.842).
2. **Early Stabilization/Diminishing Returns:** After a certain number of encoder blocks are made trainable (specifically after "0-4" blocks, highlighted by the red dotted line), the performance gains in "F1-score" become marginal, and the model's performance "stabilizes." This suggests that continuously unfreezing all subsequent blocks yields minimal additional benefit.
3. **Optimizing Training Resources:** It is not necessarily optimal to unfreeze and train "all" encoder blocks. Nearly peak performance (F1-score ~0.842) is achieved with "0-4" trainable blocks, which is very close to the F1-score when "all" blocks are trained (~0.858). This implies that a balance can be struck between performance and computational cost/complexity by identifying the point of stabilization.

**Document Context:**
This image directly relates to the document context concerning the "Trainer which executes the training process" and specifically addresses "The effect of freezing certain encoder blocks on the performance of the model." The accompanying text states, "Training more blocks leads to improved performance but stabilizes early on," which is directly supported and visually demonstrated by this graph. The graph provides empirical evidence for this claim, showing the exact points of performance increase and subsequent stabilization, thus enhancing the reader's understanding of hyperparameter tuning strategies for deep learning models, particularly in transfer learning scenarios.

**Summary:**
This line graph, titled "Effect of frozen encoder blocks on training performance," illustrates how the F1-score (a performance metric shown on the Y-axis) changes as more encoder blocks are made trainable (unfrozen) during a training process (shown on the X-axis as "Trainable encoder blocks").

The Y-axis ranges from 0 to 0.86, with major tick marks at 0, 0.70, 0.72, 0.74, 0.76, 0.78, 0.80, 0.82, 0.84, and 0.86, representing increasing F1-score values. The X-axis represents the number or range of encoder blocks that are unfrozen and allowed to be trained, starting from "None" (all blocks frozen) and progressing through "0-1", "0-2", "0-3", up to "0-10", and finally "all" (meaning all encoder blocks are trainable).

Initially, when "None" of the encoder blocks are trainable, the F1-score is approximately 0.70. As the number of trainable encoder blocks increases, the F1-score generally rises. For instance, with "0-1" blocks trainable, the F1-score jumps to about 0.79. This positive trend continues, with the F1-score reaching its peak around 0.842 when "0-4" encoder blocks are trainable. This point, "0-4", is highlighted by a vertical red dotted line, indicating a significant benchmark or turning point.

After "0-4" trainable blocks, the F1-score continues to increase slightly but at a much slower rate. The curve flattens significantly from "0-4" onwards, and an annotation pointing to this section explicitly states "Performance stabilizing." Even when "all" encoder blocks are made trainable, the F1-score only slightly increases to approximately 0.858, suggesting that most of the performance gains are achieved by training just a subset of the initial encoder blocks, and further unfreezing yields diminishing returns.

In summary, the graph demonstrates that allowing more encoder blocks to be trained initially improves model performance considerably. However, there's a clear point (around "0-4" blocks) where the performance gains largely stabilize, indicating that unfreezing all available encoder blocks may not be necessary to achieve near-optimal results.](images/2d2a2f55867a8fe0f84fa5397a091f7c65a03726ad22166d15bf3026005800e5.jpg)
Figure 11-7. The effect of freezing certain encoder blocks on the performance of the model. Training more blocks leads to improved performance but stabilizes early on.

When you are training for multiple epochs, the difference (in training time and resources) between freezing and not freezing often becomes larger. It is therefore advised to play around with a balance that works for you.

# Few-Shot Classification

Few-shot classification is a technique within supervised classification where you have a classifier learn target labels based on only a few labeled examples. This technique is great when you have a classification task but do not have many labeled data points readily available. In other words, this method allows you to label a few high-quality data points per class on which to train the model. This idea of using a few labeled data points for training your model is shown in Figure 11-8.

![## Image Analysis: d1505ea7432063fbdf4c5a499d2e2da7e084c14d464b7d2f4ee54ac0490bb20e.jpg

**Conceptual Understanding:**
The image conceptually represents the initial phase of a few-shot classification task, specifically applied to sentiment analysis. Its main purpose is to visually demonstrate how a very limited set of labeled data (movie reviews with positive or negative sentiment) can be presented as the input from which a system must learn to classify a new, unlabeled piece of text. The key idea communicated is that even with scarce labeled examples, a prediction task for new data can be formulated, highlighting the challenge and essence of few-shot learning where models learn to generalize from minimal data points.

**Content Interpretation:**
The image displays the fundamental setup for a few-shot classification task, specifically in sentiment analysis. It shows four labeled text examples (movie reviews) used as a small training set, and one new, unlabeled text example that needs classification. The 'processes' being shown are the input of existing labeled data and the task of classifying new, unlabeled data based on those limited examples. The 'concepts' include sentiment classification (positive/negative), labeled data, and the challenge of inferring labels for new data with minimal prior information. The blue boxes represent textual input (movie reviews). The pink boxes represent negative sentiment, explicitly labeled '0 negative'. The green boxes represent positive sentiment, explicitly labeled '1 positive'. The label 'Few labeled data points' signifies that these four examples constitute the entire small training set. The final blue box represents a new data point for which the sentiment is unknown, denoted by the yellow box with a question mark '?'. All extracted text elements directly support this interpretation by providing the exact movie review texts, their numerical and descriptive sentiment labels, and the explicit statement about the scarcity of labeled data.

**Key Insights:**
The main takeaway from this image is the definition and visualization of few-shot classification. It teaches that in this paradigm, a very small number of pre-categorized examples are used to infer the category of a new, unclassified item. The insight conveyed is the process of learning from minimal examples to make a prediction on unseen data. The specific text elements provide strong evidence for these insights: 'What a horrible movie...' is classified as '0 negative', 'Very disappointed' as '0 negative', 'Some flaws but still a great experience' as '1 positive', and 'Best movie ever!' as '1 positive'. These are collectively labeled as 'Few labeled data points', explicitly stating the limited nature of the training data. The presence of 'Never want to see this movie again!' with a '?' clearly indicates the task of predicting a label for a new input using the context of the 'few labeled data points'.

**Document Context:**
This image directly supports the document's section on 'Few-Shot Classification' and the accompanying text: 'In few-shot classification, we only use a few labeled data points to learn from.' It serves as a clear visual example of what 'few labeled data points' look like in practice for a sentiment analysis task, demonstrating both the input (the labeled examples) and the output (the classification of an unlabeled example) within this learning paradigm. The image illustrates the practical scenario where a limited dataset is available for training, making the abstract concept of few-shot classification concrete and understandable for the reader.

**Summary:**
The image illustrates the core concept of few-shot classification, particularly in the context of sentiment analysis. It presents four initial examples, each consisting of a movie review text and its corresponding sentiment label. The first two examples, 'What a horrible movie...' and 'Very disappointed', are labeled as '0 negative'. The next two examples, 'Some flaws but still a great experience' and 'Best movie ever!', are labeled as '1 positive'. These four examples are collectively identified as 'Few labeled data points'. An arrow points from these labeled data points to a new, unlabeled example: the text 'Never want to see this movie again!'. This new text is paired with a question mark ('?') in a yellow box, indicating that its sentiment label is unknown and needs to be determined based on the previously provided 'few labeled data points'. The overall visual flow demonstrates the process where a model would learn from a very limited set of labeled data to then classify a novel, unseen data point.](images/d1505ea7432063fbdf4c5a499d2e2da7e084c14d464b7d2f4ee54ac0490bb20e.jpg)
Figure 11-8. In few-shot classification, we only use a few labeled data points to learn from.

# SetFit: Efficient Fine-Tuning with Few Training Examples

To perform few-shot text classification, we use an efficient framework called SetFit.2 It is built on top of the architecture of sentence-transformers to generate high-quality textual representations that are updated during training. Only a few labeled examples are needed for this framework to be competitive with fine-tuning a BERT-like model on a large, labeled dataset as we explored in the previous example.

The underlying algorithm of SetFit consists of three steps:

# 1. Sampling training data

Based on in-class and out-class selection of labeled data it generates positive (similar) and negative (dissimilar) pairs of sentences

# 2. Fine-tuning embeddings

Fine-tuning a pretrained embedding model based on the previously generated training data

3. Training a classifier Create a classification head on top of the embedding model and train it using the previously generated training data

Before fine-tuning an embedding model, we need to generate training data. The model assumes the training data to be samples of positive (similar) and negative (dissimilar) pairs of sentences. However, when we are dealing with a classification task, our input data is generally not labeled as such.

Say, for example, we have the training dataset in Figure 11-9 that classifies text into two categories: text about programming languages, and text about pets.

Figure 11-9. Data in two classes: text about programming languages and text about pets.   

<table><tr><td>Text</td><td>Class</td></tr><tr><td>I write my codein Python</td><td>Programming languages</td></tr><tr><td>I should practice SQL</td><td>Programming languages</td></tr><tr><td> My dog is a labrador</td><td>Pets</td></tr><tr><td>Ihave a Siamese cat</td><td>Pets</td></tr></table>

In step 1, SetFit handles this problem by generating the necessary data based on in-class and out-class selection as we illustrate in Figure 11-10. For example, when we have 16 sentences about sports, we can create $1 6 ^ { * } \left( 1 6 - 1 \right) / 2 = 1 2 0$ pairs that we label as positive pairs. We can use this process to generate negative pairs by collecting pairs from different classes.

<table><tr><td>Text1</td><td>Text 2</td><td>Pair type</td></tr><tr><td>I write my code in Python</td><td> I should practice SQL</td><td>Positive</td></tr><tr><td> My dog is a labrador</td><td>I have a Siamese cat</td><td>Positive</td></tr><tr><td>I write my codein Python</td><td> My dog is a labrador</td><td>Negative</td></tr><tr><td> Ihave a Siamese cat</td><td>I should practice SQL</td><td>Negative</td></tr></table>

Figure 11-10. Step 1: sampling training data. We assume sentences within a class are similar and create positive pairs while sentences in different classes become negative pairs.

In step 2, we can use the generated sentence pairs to fine-tune the embedding model. This leverages a method called contrastive learning to fine-tune a pretrained BERT model. As we reviewed in Chapter 10, contrastive learning allows accurate sentence embeddings to be learned from pairs of similar (positive) and dissimilar (negative) sentences.

Since we generated these pairs in the previous step, we can use them to fine-tune a SentenceTransformers model. Although we have discussed contrastive learning before, we again illustrate the method in Figure 11-11 as a refresher.

![## Image Analysis: 5b9c11e14f61a4a8ebeaaa573eeec97b7204fffe17b0f7f794bc7328dacb33bd.jpg

**Conceptual Understanding:**
This image conceptually illustrates the architecture and process for fine-tuning a SentenceTransformers model, specifically using a BERT-based approach with contrastive learning. The main purpose is to learn high-quality, semantically meaningful sentence embeddings by training the model to distinguish between "positive" and "negative" sentence pairs. It visually breaks down how raw sentences are transformed into contextualized token embeddings, then aggregated into fixed-size sentence embeddings, and finally used in a learning objective to optimize these representations.

**Process Flow Transcription:**
*   **Initial Input Concept:** Positive and negative sentence pairs (text in green and red above the central BERT model).
*   **Input Sentence 1 Box:** "I write code in Python"
*   **Input Sentence 2 Box:** "My dog is a Labrador"
*   **Annotation for Input Sentence 2:** "Example: negative sentence pair" (text in red, pointing to "My dog is a Labrador" box).
*   **Central Processing Unit:** "BERT" (inside a light blue rectangular box with an arrow icon in the top right corner).
*   **Output from BERT (Left Branch):**
    *   **Token Embeddings (Left):** Multiple blue rectangular blocks, with implied labels for individual tokens: "I", "write", "code", "in", "Python". (Labels explicitly provided are "write" and "Python" next to specific blocks, with vertical ellipses indicating omitted tokens.)
    *   **Label for Token Embeddings (Shared):** "Token embeddings" (below BERT, connecting to both blue and purple blocks).
*   **Output from BERT (Right Branch):**
    *   **Token Embeddings (Right):** Multiple purple rectangular blocks, with implied labels for individual tokens: "My", "dog", "is", "a", "Labrador". (Labels explicitly provided are "My dog" and "Labrador" next to specific blocks, with vertical ellipses indicating omitted tokens.)
    *   **Label for Token Embeddings (Shared):** "Token embeddings" (below BERT, connecting to both blue and purple blocks).
*   **Next Step (Left Branch):** "Pooling" (inside a rounded rectangular box).
*   **Output from Pooling (Left Branch):**
    *   **Sentence Embedding (Left):** A single blue rectangular block.
    *   **Label for Sentence Embedding (Left):** "u" (next to the blue block).
    *   **Label for Sentence Embeddings (Shared):** "Sentence embeddings" (below "Token embeddings", connecting to both "u" and "v" blocks).
*   **Output from Token Embeddings (Right Branch, directly to Sentence Embedding):**
    *   **Sentence Embedding (Right):** A single purple rectangular block.
    *   **Label for Sentence Embedding (Right):** "v" (next to the purple block).
    *   **Label for Sentence Embeddings (Shared):** "Sentence embeddings" (below "Token embeddings", connecting to both "u" and "v" blocks).
*   **Final Input Layer:** "(u, v, |u - v|)" (inside a rounded rectangular box, below the "u" and "v" sentence embeddings).
*   **Final Output Layer:** "Softmax" (below the "(u, v, |u - v|)" box).

**Annotations and Metadata Transcription:**
*   **General Labels:**
    *   "Positive and negative sentence pairs" (green and red text at the top, above BERT).
    *   "Example: negative sentence pair" (red text to the right of the "My dog is a Labrador" box).
    *   "Token embeddings" (text label below BERT).
    *   "Sentence embeddings" (text label below the token embeddings and pooling).
    *   "write" (text label next to a blue token embedding block).
    *   "Python" (text label next to a blue token embedding block).
    *   "My dog" (text label next to a purple token embedding block).
    *   "Labrador" (text label next to a purple token embedding block).
    *   "u" (text label next to the blue sentence embedding block).
    *   "v" (text label next to the purple sentence embedding block).
*   **Visual Indicators:** Vertical ellipses "..." are present above "I write code in Python", above "My dog is a Labrador", between token embedding blocks on both sides, and above "Softmax". These indicate continuation or omission of intermediate layers/tokens.
*   **Colors:** "Positive" in green, "negative" in red in the "Positive and negative sentence pairs" label. "negative" in red in "Example: negative sentence pair". The sentence "I write code in Python" is mostly black, with "Python" highlighted in blue. The sentence "My dog is a Labrador" is mostly black, with "Labrador" highlighted in purple. Token embeddings are blue on the left and purple on the right. Sentence embeddings are blue for "u" and purple for "v".

**Systematic Process Mapping:**
The diagram illustrates the process of fine-tuning a SentenceTransformers model using contrastive learning.

1.  **Start with Sentence Pairs:** The process begins with "Positive and negative sentence pairs" as input. An example is provided with two sentences: "I write code in Python" and "My dog is a Labrador", where "My dog is a Labrador" is explicitly labeled as an "Example: negative sentence pair".
2.  **BERT Processing:** Both input sentences ("I write code in Python" and "My dog is a Labrador") are fed into a "BERT" model.
3.  **Token Embeddings Generation:** The BERT model outputs "Token embeddings" for each word/subword token in the input sentences.
    *   For "I write code in Python", individual token embeddings are generated for "I", "write", "code", "in", "Python" (represented by blue blocks; specific labels shown for "write" and "Python").
    *   For "My dog is a Labrador", individual token embeddings are generated for "My", "dog", "is", "a", "Labrador" (represented by purple blocks; specific labels shown for "My dog" and "Labrador").
    *   Vertical ellipses "..." indicate that not all token embeddings are explicitly shown.
4.  **Pooling for Sentence Embeddings (Left Branch):** The token embeddings from the left branch (e.g., for "I write code in Python") are passed through a "Pooling" layer.
5.  **Sentence Embedding 'u' Generation:** The "Pooling" layer combines the token embeddings to produce a single "Sentence embedding", labeled "u" (a single blue block).
6.  **Sentence Embedding 'v' Generation (Right Branch):** Similarly, the token embeddings from the right branch (e.g., for "My dog is a Labrador") are combined (implicitly through pooling or a similar mechanism, though not explicitly labeled as "Pooling" on this side, it's understood for sentence embeddings) to produce a single "Sentence embedding", labeled "v" (a single purple block).
7.  **Final Input for Learning:** The generated sentence embeddings "u" and "v" are then combined along with their absolute difference, forming the input tuple "(u, v, |u - v|)".
8.  **Softmax Layer:** This combined input is fed into a "Softmax" layer. The Softmax layer will likely perform a classification task (e.g., distinguishing positive from negative pairs) or a similarity prediction, enabling the model to learn and fine-tune the embeddings based on the contrastive learning objective.
    *   Vertical ellipses "..." below Softmax suggest further processing or a final output from this layer.

**Content Interpretation:**
The image details the architecture and data flow for fine-tuning a SentenceTransformers model. It showcases processes such as sentence encoding via BERT, generation of token embeddings, aggregation of token embeddings into sentence embeddings through pooling, and the final learning step using a Softmax layer. Key concepts include positive and negative sentence pairs, which are central to the contrastive learning paradigm, as well as the use of BERT as a base encoder. The relationships depicted illustrate how a sequence of words is transformed into a fixed-size vector representation for the entire sentence, and how pairs of these sentence embeddings, along with their differences, are utilized for a supervised learning task.

**Key Insights:**
The main takeaway is a comprehensive understanding of the SentenceTransformers fine-tuning process. Key insights include:
1.  **Foundation in BERT:** The process leverages the powerful contextual embeddings from "BERT" as a starting point, demonstrating the importance of transfer learning from large pre-trained language models.
2.  **Necessity of Pooling:** A "Pooling" step is essential to aggregate variable-length token embeddings into a single, fixed-size "Sentence embedding," which is crucial for many downstream NLP tasks. This is explicitly shown on the left branch.
3.  **Contrastive Learning Mechanism:** The explicit mention of "Positive and negative sentence pairs" and an "Example: negative sentence pair" highlights that the model is trained to differentiate between semantically similar and dissimilar sentences. This is a powerful technique for learning robust semantic representations.
4.  **Joint Consideration of Embeddings and Difference:** The final input "(u, v, |u - v|)" to the "Softmax" layer reveals that the model learns not only from the individual sentence embeddings (u, v) but also critically from their absolute difference (|u - v|). This allows the model to directly learn a metric of similarity or dissimilarity between sentences.
5.  **End-to-End Fine-tuning:** The diagram illustrates a complete fine-tuning pipeline, from raw text input through BERT and pooling to a final classification/learning head ("Softmax"), demonstrating how the entire architecture is trained to produce useful sentence embeddings for tasks like semantic search or similarity.

**Document Context:**
This image perfectly complements the document's section "2. Fine-tuning embeddings" and specifically elaborates on "Figure 11-11. Step 2: Fine-tuning a SentenceTransformers model." It visually explains the theoretical underpinnings and practical workflow of how a SentenceTransformers model learns to create meaningful sentence representations. By showing the components and data flow, it clarifies the statement "Using contrastive learning, embeddings are learned from positive and negative sentence pairs" provided in the text accompanying the figure. It bridges the conceptual explanation with a concrete architectural diagram, making the process tangible for the reader.

**Summary:**
This diagram illustrates the step-by-step process of fine-tuning a SentenceTransformers model using a BERT-based architecture and contrastive learning. The primary goal is to generate high-quality sentence embeddings that effectively capture semantic meaning and relationships between sentences.

The process begins by taking "Positive and negative sentence pairs" as input. For instance, the diagram shows two example sentences: "I write code in Python" and "My dog is a Labrador," with the latter explicitly marked as an "Example: negative sentence pair," indicating a pair of sentences that should be considered semantically dissimilar during training.

Both of these input sentences are fed into a "BERT" model. BERT processes these sentences and generates "Token embeddings" for each individual word or sub-word token. On the left side, for the sentence "I write code in Python," we see blue rectangular blocks representing token embeddings for words like "write" and "Python," with vertical ellipses indicating other tokens. Similarly, on the right side, for "My dog is a Labrador," purple rectangular blocks represent token embeddings for phrases like "My dog" and "Labrador."

Following the token embedding generation, a "Pooling" operation is applied to the sequence of token embeddings from the left branch. This pooling step aggregates all the individual token embeddings of a sentence into a single, fixed-size vector known as a "Sentence embedding." For the left sentence, this results in the sentence embedding labeled "u" (a single blue block). A similar aggregation process occurs for the right sentence, producing its "Sentence embedding" labeled "v" (a single purple block).

Finally, these two sentence embeddings, "u" and "v," are combined along with their absolute difference, resulting in the tuple "(u, v, |u - v|)." This combined feature set is then fed into a "Softmax" layer. The Softmax layer typically performs a classification task, allowing the model to learn and distinguish between positive and negative sentence pairs. By optimizing the model to correctly classify these pairs, the underlying BERT and pooling layers are fine-tuned to produce sentence embeddings (u and v) that are semantically closer for positive pairs and further apart for negative pairs. The vertical ellipses below Softmax suggest further downstream processing or a final output from the learning objective.](images/5b9c11e14f61a4a8ebeaaa573eeec97b7204fffe17b0f7f794bc7328dacb33bd.jpg)
Figure 11-11. Step 2: Fine-tuning a SentenceTransformers model. Using contrastive learning, embeddings are learned from positive and negative sentence pairs.

The goal of fine-tuning this embedding model is that it can create embeddings that are tuned to the classification task. The relevance of the classes, and their rela‐ tive meaning, are distilled into the embeddings through fine-tuning the embedding model.

In step 3, we generate embeddings for all sentences and use those as the input of a classifier. We can use the fine-tuned SentenceTransformers model to convert our sentences into embeddings that we can use as features. The classifier learns from our fine-tuned embeddings to accurately predict unseen sentences. This last step is illustrated in Figure 11-12.

![## Image Analysis: 2ce2f6d9bcd1d6a4c45e18660fc42ea76b85fd460594ca098586253bcd4ca4df.jpg

**Conceptual Understanding:**
This image conceptually represents a workflow for **text classification** using **sentence embeddings**. The main purpose is to demonstrate how natural language sentences can be transformed into numerical representations (embeddings) using "Fine-tuned SentenceTransformers" and then classified into predefined categories (in this case, "Code" or "Pets") by a "Classifier". It communicates the key idea of leveraging pre-trained or fine-tuned language models to create meaningful numerical vectors for text, which can then be used for downstream machine learning tasks like classification.

**Content Interpretation:**
The image shows a system for classifying text based on its content.

*   **Input Sentences:** The top row, with text like "I write my code in Python", "I should practice SQL", and "My dog is a Labrador", represents the raw textual data that needs to be categorized. This indicates the diverse nature of inputs the system can handle.
*   **Fine-tuned SentenceTransformers:** This central component demonstrates the use of advanced natural language processing models. The term "Fine-tuned" (extracted verbatim from the box) is significant, implying that the model has been adapted or trained further on a specific dataset to perform better for the target task, likely improving the quality of the embeddings for the classification. SentenceTransformers (also verbatim) are models known for generating dense vector representations of sentences.
*   **Sentence Embeddings:** The small grid-like boxes directly below the "Fine-tuned SentenceTransformers", each accompanied by the original sentence, represent the numerical vector embeddings generated for each sentence. These embeddings are crucial as they convert human-readable text into a machine-understandable format, capturing semantic meaning.
*   **Classifier:** This component, with the label "Classifier" and the sub-text "For example using scikit-learn or Pytorch", signifies the machine learning model responsible for making the final categorization. The annotation "Any classification model can be used" emphasizes the flexibility and modularity of this step, allowing different algorithms to be employed based on performance or preference.
*   **Output Probabilities and Categories:** The bottom row, displaying percentages like "79%" and "21%" along with labels "Code" and "Pets", represents the classification output. For each input sentence, the classifier assigns a probability score to each category.
    *   For the sentence "I write my code in Python", the classification is predominantly "Code" (79%), which aligns with the content of the sentence.
    *   For the sentence "I should practice SQL", it is highly classified as "Code" (93%), again consistent with the programming-related content.
    *   For the sentence "My dog is a Labrador", the classification is predominantly "Pets" (85%), correctly reflecting the subject matter.
    This output clearly demonstrates the system's ability to accurately infer the category of a sentence based on its semantic content.

**Key Insights:**
The main takeaways and insights from this image are:

*   **Importance of Embeddings in NLP:** The image clearly shows that input text is first converted into "embeddings" by "Fine-tuned SentenceTransformers" before classification. This highlights the crucial role of numerical representations of text for machine learning tasks.
*   **Benefits of Fine-tuning:** The explicit mention of "Fine-tuned SentenceTransformers" suggests that adapting pre-trained models to specific domains or tasks can significantly enhance their performance in generating contextually relevant embeddings, leading to better downstream classification accuracy.
*   **Modular Architecture:** The design implies a modular architecture where the embedding generation (SentenceTransformers) and classification ("Classifier") steps are distinct. The note "Any classification model can be used" further emphasizes this modularity and flexibility in choosing the best classifier for the task.
*   **Practical Application of Text Classification:** The example sentences ("I write my code in Python", "My dog is a Labrador") and their corresponding "Code" / "Pets" classifications demonstrate a practical application of text classification, where sentences are categorized based on their underlying semantic topic.
*   **Probabilistic Output:** The output shows not just a single class but a probability distribution across classes (e.g., "79% Code", "21% Pets"). This indicates that the classifier provides a confidence score for its predictions, which is valuable for understanding the certainty of the classification.

**Document Context:**
This image, titled "Figure 11-12. Step 3: Training a classifier," and appearing in "Section: 2. Fine-tuning embeddings," is central to understanding how fine-tuned text embeddings are utilized in a machine learning pipeline, specifically for classification. It follows the idea of fine-tuning embeddings (as per the section title) and directly illustrates "Step 3: Training a classifier" by showing how those generated embeddings are then fed into a classifier for a specific task. The figure visually demonstrates the practical application of the concepts discussed in the preceding text, showing the process from raw text to categorized output using advanced NLP techniques.

**Summary:**
This diagram illustrates a three-step process for classifying text into specific categories, in this case, "Code" or "Pets," using modern natural language processing techniques.

**Step 1: Input Sentences.** The process begins with raw text sentences. Three examples are provided at the top: "I write my code in Python," "I should practice SQL," and "My dog is a Labrador." These represent the diverse inputs the system aims to classify.

**Step 2: Embedding Generation using Fine-tuned SentenceTransformers.** These input sentences are then fed into a component labeled "Fine-tuned SentenceTransformers." The term "Fine-tuned" indicates that these models have been specially adapted or further trained to generate highly relevant numerical representations, or "embeddings," for sentences pertinent to the classification task. These SentenceTransformers convert each sentence into a dense numerical vector, visually represented by small, grid-like boxes beneath the "Fine-tuned SentenceTransformers" component, with the original sentence text still associated (e.g., "I write my code in Python" is linked to its embedding). These embeddings capture the semantic meaning of the sentences in a format that machine learning models can understand.

**Step 3: Classification.** The generated sentence embeddings are then passed to a "Classifier." This is the machine learning model responsible for taking the numerical embeddings and assigning them to a predefined category. The diagram specifies that this classifier can be, "For example using scikit-learn or Pytorch," highlighting the common frameworks that can be used. A key annotation next to this box states, "Any classification model can be used," emphasizing the flexibility to integrate various classification algorithms.

**Step 4: Output Classification with Probabilities.** Finally, the classifier produces an output for each sentence, indicating its predicted category ("Code" or "Pets") along with a confidence percentage for each.
*   For the sentence "I write my code in Python," the system predicts "Code" with 79% confidence and "Pets" with 21%.
*   For "I should practice SQL," it assigns 93% to "Code" and 7% to "Pets."
*   For "My dog is a Labrador," the classification is 15% for "Code" and 85% for "Pets."
These results demonstrate how the system accurately categorizes sentences based on their content, providing a clear and interpretable outcome for each input. This entire workflow showcases how semantic information encoded in fine-tuned embeddings can effectively power text classification applications.](images/2ce2f6d9bcd1d6a4c45e18660fc42ea76b85fd460594ca098586253bcd4ca4df.jpg)
Figure 11-12. Step 3: Training a classifier. The classifier can be any scikit-learn model or a classification head.

When we put all the steps together, we get an efficient and elegant pipeline for performing classification when you only have a few labels per class. It cleverly makes use of the idea that we have labeled data, although not in the way that we would like it. The three steps together are illustrated in Figure 11-13 to give a single overview of the entire procedure.

First, sentence pairs are generated based on in-class and out-class selection. Second, the sentence pairs are used to fine-tune a pretrained SentenceTransformer model. Third, the sentences are embedded with the fine-tuned model on which a classifier is trained to predict the classes.

![## Image Analysis: 107272a49ae6bde9fdc4dcab843e51b2d7f5874efd5b9f5a5c13e73139a72af4.jpg

**Conceptual Understanding:**
This image conceptually represents the SetFit framework, a method designed to fine-tune sentence embeddings for improved performance in downstream tasks, particularly text classification. The main purpose conveyed is to illustrate the three core procedural steps that make up the SetFit process: data preparation, model optimization, and final task-specific training. The key ideas communicated are the generation of positive and negative sentence pairs for contrastive learning, the fine-tuning of a pretrained SentenceTransformer using a Siamese network (exemplified by BERT models and a SoftMax layer), and the subsequent use of these fine-tuned embeddings to train a classifier.

**Content Interpretation:**
This image details the three sequential steps involved in the SetFit framework. The processes shown include data generation (creating positive and negative sentence pairs), model fine-tuning (adjusting a pretrained SentenceTransformer using a Siamese BERT network with a SoftMax layer), and ultimately, sentence embedding and classification (using the fine-tuned model to create embeddings for a new classifier). The significance of the positive and negative sentence pairs in Step 1, such as 'I write my code in Python' with 'I should practice SQL' as positive and 'I write my code in Python' with 'My dog is a Labrador' as negative, is to provide the training signal for the SentenceTransformer to learn semantic similarity and dissimilarity. The use of 'BERT' models within the 'Fine-tune pretrained SentenceTransformer' step highlights a common architecture for generating robust sentence embeddings, with 'SoftMax' being crucial for calculating probabilities or similarity scores. The 'Classifier' in the final step is shown taking the 'Fine-tuned SentenceTransformers' output, indicating that these optimized embeddings are then used as features for downstream classification tasks, demonstrating the framework's application.

**Key Insights:**
The image teaches several key lessons about the SetFit framework. Firstly, SetFit relies on a contrastive learning approach, evident from the 'Generate positive and negative sentence pairs' step, where the model learns to distinguish between related and unrelated sentences. Secondly, it highlights the use of a 'pretrained SentenceTransformer', indicating a transfer learning paradigm where a general-purpose model is adapted for a specific task. The architecture in Step 2, employing two 'BERT' models feeding into 'SoftMax', reveals a Siamese network structure, commonly used for learning embeddings that capture semantic similarity. Lastly, the 'Embed sentences and train classifier' step demonstrates how the fine-tuned embeddings are directly utilized as powerful features for a 'Classifier', showing the practical application of the improved embeddings in a downstream task. The entire process emphasizes an efficient way to achieve high-quality sentence embeddings for classification with minimal labeled data.

**Document Context:**
This image directly supports the document's section '2. Fine-tuning embeddings' by visually explaining 'The three main steps of SetFit', as explicitly stated in the text after the image. It provides a concrete, step-by-step diagram of how the SetFit method achieves its goal of fine-tuning embeddings. The image clarifies the abstract concept of 'fine-tuning embeddings' by showing the practical implementation, from data preparation to model training and classification. It visually answers how the SetFit framework processes input, refines a model, and produces actionable outputs, which is crucial for understanding the broader topic of 'fine-tuning embeddings' within the document.

**Summary:**
The image illustrates the three main steps of the SetFit framework, a method for fine-tuning embeddings, as described in Figure 11-13. The process begins with data preparation, moves to fine-tuning a SentenceTransformer model, and concludes with embedding sentences and training a classifier.

**Step 1: Generate positive and negative sentence pairs**
This initial step focuses on creating a dataset of related and unrelated sentence pairs. Two examples are provided:
*   **Positive Pair (Green box):** "• I write my code in Python" and "• I should practice SQL". These sentences are considered semantically related.
*   **Negative Pair (Pink box):** "• I write my code in Python" and "• My dog is a Labrador". These sentences are considered semantically unrelated.

**Step 2: Fine-tune pretrained SentenceTransformer**
The sentence pairs generated in Step 1 are fed into a pretrained SentenceTransformer for fine-tuning. This step depicts a Siamese network architecture, where two identical BERT models process the input sentences:
*   One BERT model receives an input like "...in Python" (derived from "I write my code in Python").
*   Another BERT model receives an input like "...a Labrador" (derived from "My dog is a Labrador").
The outputs of these BERT models, which are sentence embeddings (represented by sequences of four grey rectangles), are then processed by a SoftMax layer. The SoftMax layer is used to adjust the SentenceTransformer's weights based on the positive and negative pairs, bringing related sentences closer in the embedding space and pushing unrelated ones further apart.

**Step 3: Embed sentences and train classifier**
In the final step, the now 'Fine-tuned SentenceTransformers' are used to generate embeddings for sentences, which are then used to train a classifier. An example input sentence, "I write my code in Python", is fed into the 'Fine-tuned SentenceTransformers'. The output of these transformers (again, represented by a sequence of four grey rectangles, which are the sentence embeddings) is then passed to a 'Classifier'. This 'Classifier' takes these embeddings as input and produces a classification output (represented by two green squares, indicating categories or labels).](images/107272a49ae6bde9fdc4dcab843e51b2d7f5874efd5b9f5a5c13e73139a72af4.jpg)
Figure 11-13. The three main steps of SetFit.

# Fine-Tuning for Few-Shot Classification

We previously trained on a dataset containing roughly 8,500 movie reviews. However, since this is a few-shot setting, we will only sample 16 examples per class. With two classes, we will only have 32 documents to train on compared to the 8,500 movie reviews we used before!

# from setfit import sample_dataset

# We simulate a few-shot setting by sampling 16 examples per class sampled_train_data $=$ sample_dataset(tomatoes["train"], num_samples ${ \tt = } 1 6$ )

After sampling the data, we choose a pretrained SentenceTransformer model to finetune. The official documentation contains an overview of pretrained SentenceTrans former models from which we are going to be using "sentence-transformers/ all-mpnet-base-v2". It is one of the best-performing models on the MTEB leader‐ board, which shows the performance of embedding models across a variety of tasks:

from setfit import SetFitModel

# Load a pretrained SentenceTransformer model model $=$ SetFitModel.from_pretrained("sentence-transformers/all-mpnet-base-v2")

After loading in the pretrained SentenceTransformer model, we can start defining our SetFitTrainer. By default, a logistic regression model is chosen as the classifier to train.

Similar to what we did with Hugging Face Transformers, we can use the trainer to define and play around with relevant parameters. For example, we set the num_epochs to 3 so that contrastive learning will be performed for three epochs:

from setfit import TrainingArguments as SetFitTrainingArguments from setfit import Trainer as SetFitTrainer

# Define training arguments   
args $=$ SetFitTrainingArguments( num_epochs $^ { = 3 }$ , # The number of epochs to use for contrastive learning num_iterations $= 2 0$ # The number of text pairs to generate   
)   
args.eval_strategy $=$ args.evaluation_strategy   
# Create trainer   
trainer $=$ SetFitTrainer( model=model, args $=$ args, train_dataset $\Bumpeq$ sampled_train_data, eval_dataset $=$ test_data, metric="f1"   
)

We only need to call train to start the training loop. When we do, we should get the following output:

# Training loop trainer.train()

\*\*\*\*\* Running training \*\*\*\*\* Num unique pairs $=$ 1280 Batch size $\ l = \ 1 6$ Num epochs $= 3$ Total optimization steps $=$ 240

Notice that the output mentions that 1,280 sentence pairs were generated for finetuning the SentenceTransformer model. As a default, 20 sentence pair combinations are generated for each sample in our data, which would be $2 0 \div 3 2 = 6 8 0$ samples. We will have to multiply this value by 2 for each positive and negative pair generated, $6 8 0 \div 2 = 1 . 2 8 0$ sentence pairs. Generating 1,280 sentence pairs is quite impressive considering we only had 32 labeled sentences to start with!

When we do not specifically define a classification head, by default a logistic regression is used. If we would like to specify a classifi‐ cation head ourselves, we can do so by specifying the following model in SetFitTrainer:

# Load a SetFit model from Hub   
model $=$ SetFitModel.from_pretrained( "sentence-transformers/all-mpnet-base-v2", use_differentiable_head=True, head_params={"out_features": num_classes},   
)   
# Create trainer   
trainer $=$ SetFitTrainer( model $=$ model,   
)

Here, num_classes refers to the number of classes that we want to predict.

Next, let’s evaluate the model to get a feeling of its performance:

# Evaluate the model on our test data trainer.evaluate()

With only 32 labeled documents, we get an F1 score of 0.85. Considering that the model was trained on a tiny subset of the original data, this is very impressive! Moreover, in Chapter 2, we got the same performance but instead trained a logistic regression model on the embeddings of the full data. Thus, this pipeline demonstrates the potential of taking the time to label just a few instances.

Not only can SetFit perform few-shot classification tasks, but it also has support for when you have no labels at all, also called zero-shot classification. SetFit generates synthetic examples from the label names to resemble the classification task and then trains a SetFit model on them. For example, if the target labels are “happy” and “sad,” then synthetic data could be “The example is happy” and “This example is sad.”

# Continued Pretraining with Masked Language Modeling

In the examples thus far, we leveraged a pretrained model and fine-tuned it to perform classification. This process describes a two-step process: first pretraining a model (which was already done for us) and then fine-tuning it for a particular task. We illustrate this process in Figure 11-14.

![## Image Analysis: dbbb3266f1204e3fa8798ae05ee7fc916f364b8408af1c8d3f5f1c7daca70052.jpg

**Conceptual Understanding:**
This image conceptually illustrates the two primary phases in the life cycle of a BERT (Bidirectional Encoder Representations from Transformers) model: **Pretraining** and **Fine-tuning**.

The main purpose of the image is to convey how a BERT model is initially trained on a broad, general language understanding task (like masked language modeling, learning from scratch) and subsequently adapted or specialized for a specific, narrower downstream task (like text classification). It highlights the fundamental transfer learning paradigm in natural language processing (NLP) where a general model is made task-specific.

The key ideas communicated are:
*   **Two-stage process:** Pretraining first, then fine-tuning.
*   **General vs. Specific tasks:** Pretraining covers general language understanding; fine-tuning focuses on specific applications.
*   **Masked Language Modeling (MLM):** An example of a pretraining objective, where the model learns context by predicting missing words.
*   **Text Classification:** An example of a fine-tuning objective, where the model learns to categorize text inputs.
*   **Probabilistic output:** Fine-tuning outputs a confidence score for different categories, demonstrating the model's prediction.

**Content Interpretation:**
The image clearly shows the process flow for training and applying a BERT model, specifically detailing its pretraining and fine-tuning stages.

*   **Processes Shown:**
    *   **Pretraining from scratch (Stage 1):** This process involves training a "BERT" model on a large corpus of text to learn general language representations without explicit labels. The "e.g., masked language modeling:" annotation clearly indicates the specific task used for pretraining. The input "[CLS] What a horrible movie [MASK]!" demonstrates how the model is presented with a sentence where a word is intentionally hidden. The subsequent outputs "What a horrible dream!", "What a horrible idea!", "What a horrible day!" (with 'dream', 'idea', 'day' highlighted in red) illustrate the model's attempt to predict the masked word based on context. The "..." signifies that the model considers multiple plausible words for the masked position, learning semantic and syntactic relationships.
    *   **Fine-tuning on target task (Stage 2):** This process adapts the already pretrained "BERT" model to a specific downstream task. The prominent arrow connecting the first "BERT" box to the second "BERT" box visually reinforces this transfer. The "e.g., classification:" annotation specifies the task. The input "What a horrible movie!" is a complete sentence, and the output "Positive 5% | 95% Negative" shows the model classifying the sentiment of the sentence. The percentages "5%" and "95%" indicate the model's confidence in its predictions for the 'Positive' and 'Negative' classes, respectively, clearly demonstrating a sentiment analysis task where the sentence is predominantly classified as 'Negative'.

*   **Concepts/Relationships:**
    *   **Transfer Learning:** The entire diagram exemplifies transfer learning in NLP, where knowledge gained from a general task (pretraining) is transferred and refined for a specific task (fine-tuning). The explicit sequential numbering (1 and 2) and the connecting arrow firmly establish this two-stage relationship.
    *   **Model Reuse:** The use of "BERT" in both stages implies that the same underlying model architecture, or at least a model initialized with the pretrained weights, is used and adapted.
    *   **Contextual Understanding:** The masked language modeling task ("What a horrible movie [MASK]!") directly shows BERT's ability to understand context to fill in missing words.
    *   **Sentiment Analysis:** The classification example ("What a horrible movie!" leading to "95% Negative") illustrates BERT's application in sentiment analysis, a common NLP task.

*   **Significance of Information:**
    *   The change in task from masked word prediction to sentiment classification signifies the versatility and adaptability of the BERT architecture.
    *   The high "95% Negative" confidence for "What a horrible movie!" demonstrates the effectiveness of fine-tuning for specific tasks, allowing the model to make highly confident, task-specific predictions.
    *   The "from scratch" label for pretraining emphasizes the initial training without prior task-specific knowledge, while "on target task" for fine-tuning highlights the specialization.

**Key Insights:**
The main takeaways and lessons from this image, supported by the extracted textual evidence, are:

*   **Two-Phase Model Development is Standard for BERT:** BERT models are typically developed in a two-phase approach: first "Pretraining from scratch" and then "Fine-tuning on target task." This is directly evidenced by the two distinct, numbered sections (1 and 2) with these labels and the arrow connecting the BERT models between them.
*   **Pretraining Focuses on General Language Understanding:** The pretraining stage (labeled "1 Pretraining from scratch") is designed to teach the BERT model general linguistic knowledge. The specific task "e.g., masked language modeling:" and its input "[CLS] What a horrible movie [MASK]!" clearly show the model learning to predict missing words based on context, thereby building a foundational understanding of language structure and semantics. The varied plausible predictions like "dream!", "idea!", and "day!" demonstrate the model's capacity to infer contextually appropriate words.
*   **Fine-tuning Adapts Pretrained Models to Specific Tasks:** The second stage ("2 Fine-tuning on target task") leverages the general language understanding gained during pretraining to perform specific downstream NLP tasks. The "e.g., classification:" example illustrates this, where the pretrained BERT model is adapted to analyze the sentiment of the sentence "What a horrible movie!".
*   **Fine-tuned Models Provide Task-Specific Outputs:** For specific tasks like classification, the fine-tuned BERT model provides clear, quantifiable outputs. In the classification example, the model outputs probabilistic scores ("Positive 5%" and "95% Negative") for the given input, indicating its prediction and confidence for a particular category. This demonstrates the practical application of the model after fine-tuning.

In essence, the image teaches that BERT's power comes from its ability to first learn a rich, general representation of language through self-supervised pretraining and then efficiently transfer this knowledge to excel at various specific tasks with relatively little task-specific data.

**Document Context:**
This image, Figure 11-14, directly illustrates the core process described in the accompanying text: "To fine-tune the model on a target task—for example, classification—we either start with pretraining a BERT model or use a pretrained one." The figure visually breaks down this statement into its two fundamental stages, providing concrete examples for both pretraining and fine-tuning, which are crucial for understanding how BERT models are utilized in practice. It clarifies the sequential nature of these steps and the types of tasks performed in each phase, making the abstract concept of pretraining and fine-tuning highly accessible.

**Summary:**
This diagram illustrates the two-step process involved in preparing and using a BERT (Bidirectional Encoder Representations from Transformers) model for various Natural Language Processing (NLP) tasks.

**Step 1: Pretraining from scratch**
The first stage, labeled "1 Pretraining from scratch," involves training a BERT model from its initial state. This phase is designed to equip the model with a broad understanding of language without being specifically tailored for any particular end-user application. A common method for this is "masked language modeling," as shown in the example. In this task, the model is presented with a sentence where one or more words are intentionally hidden or "masked." For instance, the input is "[CLS] What a horrible movie [MASK]!", where "[CLS]" is a special token indicating the start of the sequence and "[MASK]" signifies the missing word. The BERT model's goal during pretraining is to predict the original masked word based on the surrounding context. The diagram shows several possible predictions the model might generate, such as "What a horrible dream!", "What a horrible idea!", or "What a horrible day!", with the predicted words highlighted in red. The "..." indicates that there could be many other plausible predictions. This process helps BERT learn the complex relationships and contexts between words in a language.

**Step 2: Fine-tuning on target task**
Once the BERT model has undergone pretraining and acquired a general understanding of language, it proceeds to the second stage, labeled "2 Fine-tuning on target task." This stage involves taking the already pretrained BERT model (indicated by the arrow connecting the two "BERT" boxes) and further training it on a specific, narrower NLP task. An example of such a task is "classification," which could be used for sentiment analysis or categorizing text. In this example, the fine-tuned BERT model is given a complete sentence, "What a horrible movie!", as input. Instead of predicting a masked word, its task is now to classify the sentiment of the sentence. The output demonstrates the model's prediction: it assigns a "5%" probability to the sentence being "Positive" and a "95%" probability to it being "Negative." This shows that the fine-tuned model confidently classifies "What a horrible movie!" as having a negative sentiment.

In summary, the image clearly depicts how a BERT model first learns general language patterns through tasks like masked language modeling and then specializes in a particular task, such as text classification, by adjusting its learned knowledge during the fine-tuning phase. This two-step approach allows BERT to be highly effective across a wide range of NLP applications.](images/dbbb3266f1204e3fa8798ae05ee7fc916f364b8408af1c8d3f5f1c7daca70052.jpg)
Figure 11-14. To fine-tune the model on a target task—for example, classification—we either start with pretraining a BERT model or use a pretrained one.

This two-step approach is typically used throughout many applications. It has its limitations when faced with domain-specific data. The pretrained model is often trained on very general data, like Wikipedia pages, and might not be tuned to your domain-specific words.

Instead of adopting this two-step approach, we can squeeze another step between them, namely continue pretraining an already pretrained BERT model. In other words, we can simply continue training the BERT model using masked language modeling (MLM) but instead use data from our domain. It is like going from a general BERT model to a BioBERT model specialized for the medical domain, to a fine-tuned BioBERT model to classify medication.

This will update the subword representations to be more tuned toward words it would not have seen before. This process is illustrated in Figure 11-15 and dem‐ onstrates how this additional step updates a masked language modeling task. Con‐ tinuing pretraining on a pretrained BERT model has been shown to improve the performance of models in classification tasks and is a worthwhile addition to the fine-tuning pipeline.3

![## Image Analysis: 2d3a2abea1cb3de55e024ee237366ce841388c88589efdfe546f751e632d25f3.jpg

**Conceptual Understanding:**
This image conceptually illustrates a common and advanced transfer learning strategy used with large language models, specifically BERT. The main purpose is to demonstrate how adding an intermediate 'continued pretraining' phase on domain-specific data can significantly enhance the model's ability to understand and perform tasks within that particular domain, before it is 'fine-tuned' for a specific target task. It highlights the evolution of a language model's capabilities from general language understanding to specialized domain expertise, and then to task-specific application. The key ideas communicated are the three stages of model adaptation (initial pretraining, domain-specific continued pretraining, and target task fine-tuning), the role of masked language modeling in pretraining, and the impact of domain-specific data on the model's predictions and ultimate task performance.

**Content Interpretation:**
This image illustrates the transfer learning methodology for a BERT (Bidirectional Encoder Representations from Transformers) model, specifically demonstrating a three-step training pipeline: initial pretraining, continued pretraining on domain-specific data, and final fine-tuning for a target task. The 'BERT' model is the central component, adapted through these stages. The first stage, 'Pretraining from scratch', shows general masked language modeling where the model learns to fill in masked words with general concepts like 'dream', 'idea', 'day' for the input '[CLS] What a horrible movie [MASK]!'. The second stage, 'Continued pretraining from pretrained', demonstrates domain adaptation. By training on 'domain-specific data', the same masked input leads to more relevant predictions like 'movie', 'ending', 'premise', indicating that BERT has learned domain-specific vocabulary and context. The third stage, 'Fine-tuning on target task', applies the specialized BERT model to a 'classification' task, exemplified by sentiment analysis on 'What a horrible movie!', resulting in a '95% Negative' classification. The ellipses '...' at the bottom of the first two stages suggest that these are just examples and more predictions or processes would occur.

**Key Insights:**
The main takeaway from this image is the significant benefit of an intermediate 'continued pretraining' step using 'On domain-specific data' when adapting a large language model like BERT for specific tasks. This step allows the model to develop a more nuanced understanding of the target domain before task-specific fine-tuning. The textual evidence in '1 Pretraining from scratch' shows general masked word predictions (e.g., 'What a horrible dream!', 'What a horrible idea!', 'What a horrible day!') for the input '[CLS] What a horrible movie [MASK]!'. In contrast, '2 Continued pretraining from pretrained' on 'domain-specific data' yields more semantically relevant predictions (e.g., 'What a horrible movie!', 'What a horrible ending!', 'What a horrible premise!') for the same masked input. This demonstrates that continued pretraining effectively specializes the model's knowledge. Finally, '3 Fine-tuning on target task' for 'e.g., classification' leverages this specialized understanding to achieve a specific task, here showing a '95% Negative' sentiment for 'What a horrible movie!', illustrating the practical application of the pretraining steps. The insight is that domain-specific continued pretraining refines the model's internal representations, making it more effective for downstream tasks within that domain.

**Document Context:**
This image directly supports the document's section on 'Continued Pretraining with Masked Language Modeling'. It visually explains the concept described in the text after the image: 'Instead of a two-step approach, we can add another step that continues to pretrain the pretrained model before fine-tuning it on the target task.' The figure clearly distinguishes between the initial pretraining, the crucial intermediate 'continued pretraining' step, and the final fine-tuning. It highlights how the 'masks were filled with abstract concepts in 1' (e.g., dream, idea, day) and 'movie-specific concepts in 2' (e.g., movie, ending, premise), making the abstract explanation concrete. This visual helps to clarify the advantages of adding domain-specific continued pretraining to improve model performance for specific tasks by showing how the model's understanding of masked words evolves from general to domain-specific.

**Summary:**
This image illustrates a three-stage process for applying the BERT model, specifically highlighting the benefits of an intermediate 'continued pretraining' step before 'fine-tuning'. The process begins with '1 Pretraining from scratch' where a BERT model is generally trained using tasks like 'e.g., masked language modeling'. An input sentence such as '[CLS] What a horrible movie [MASK]!' is processed, and the BERT model predicts general, abstract concepts for the masked word, exemplified by outputs like 'What a horrible dream!', 'What a horrible idea!', and 'What a horrible day!', followed by an ellipsis indicating more possibilities. This pretrained BERT model then proceeds to '2 Continued pretraining from pretrained'. In this stage, the BERT model is further pretrained 'On domain-specific data'. The same input sentence, '[CLS] What a horrible movie [MASK]!', is used, but the model, now exposed to domain-specific data, predicts more relevant concepts for the masked word, such as 'What a horrible movie!', 'What a horrible ending!', and 'What a horrible premise!', again followed by an ellipsis. Finally, the model moves to '3 Fine-tuning on target task'. Here, the BERT model is fine-tuned for a specific task, 'e.g., classification'. The complete sentence 'What a horrible movie!' is input for classification, and the model outputs a sentiment, showing 'Positive 5%' and '95% Negative', indicating a strong negative sentiment. Each of the three main stages (Pretraining, Continued pretraining, Fine-tuning) is represented by a box containing the text 'BERT' and a small arrow icon, connected sequentially by arrows.](images/2d3a2abea1cb3de55e024ee237366ce841388c88589efdfe546f751e632d25f3.jpg)
Figure 11-15. Instead of a two-step approach, we can add another step that continues to pretrain the pretrained model before fine-tuning it on the target task. Notice how the masks were filled with abstract concepts in 1 while they were filled with movie-specific concepts in 2.

Instead of having to pretrain an entire model from scratch, we can simply continue pretraining before fine-tuning it for classification. This also helps the model to adapt to a certain domain or even the lingo of a specific organization. The genealogy of models a company might want to adopt is further illustrated in Figure 11-16.

![## Image Analysis: 2943378f91e01a9c2f4d913dbed5d1cfc77ec4963ef28f87879963062fcf567a.jpg

**Conceptual Understanding:**
This image conceptually represents a hierarchical and sequential approach to developing highly specialized artificial intelligence models, particularly in the realm of natural language processing (NLP). The main purpose is to illustrate the efficiency and effectiveness of leveraging pre-trained models through a multi-stage fine-tuning process. It conveys the key idea of transfer learning, where knowledge gained from a general task is transferred and adapted to more specific tasks and domains, culminating in models optimized for very precise applications.

**Content Interpretation:**
The image illustrates a three-step process for adapting pre-trained language models for specific applications:

**Processes Shown:**
*   **Initial Pretraining:** Depicted by "Pretrained model" and "Representation model" (e.g., BERT). This signifies the foundational step where a large language model is trained on a vast and diverse dataset, learning general language patterns and representations.
*   **Domain Adaptation (Continued Pretraining):** Illustrated by "Continued pretrained model" and "Fine-tuned on ACME corp data" leading to an "ACME Representation model." This process tailors the general model to a specific organizational or industry domain by continuing its training on data relevant to that domain ("ACME corp data"). This helps the model understand the specific jargon, concepts, and nuances pertinent to ACME.
*   **Task-Specific Fine-tuning:** Represented by "Final fine-tuned models" and "Fine-tuned on a specific task," yielding multiple specialized "ACME representation model" outputs. This final stage adapts the domain-specific model to perform very particular tasks, demonstrating the versatility of the approach.

**Concepts Shown:**
*   **Transfer Learning:** The entire diagram exemplifies transfer learning, where a model initially trained for a broad task (language understanding) is repurposed and refined for new, specific tasks.
*   **Representation Models:** The recurring "Representation model" label signifies models that learn to convert input data (like text) into numerical representations that capture semantic meaning, which are then useful for various downstream tasks.
*   **Specialization/Adaptation:** The progression from a generic model to an "ACME Representation model" and then to task-specific models highlights the specialization process through targeted data.

**Relationships Shown:**
*   **Hierarchical Dependency:** Each stage builds upon the previous one. A "Pretrained model" is a prerequisite for a "Continued pretrained model," which in turn is a prerequisite for "Final fine-tuned models." This shows a clear dependency and refinement pipeline.
*   **One-to-Many Output:** The final stage demonstrates that a single domain-adapted model ("ACME Representation model") can be fine-tuned into multiple task-specific models, each serving a different purpose (customer service topic classification, semantic search, NER).

**Significance of Information:**
*   The highlighting of "ACME corp data" emphasizes the importance of domain-specific data for adapting generic models to enterprise contexts.
*   The red text "a specific task" and the specific task examples ("customer service topic classification," "semantic search," "NER") underscore the ultimate goal: creating highly performant models for precise business applications.
*   The "e.g., BERT" annotation provides a concrete example of a well-known pre-trained model, grounding the abstract concept in a widely recognized technology.
*   The icon in the top-right corner of each box might signify that these are reusable or modular components, or perhaps that they can be further extended or integrated.

All extracted text elements, especially the stage headers ("Pretrained model," "Continued pretrained model," "Final fine-tuned models") and the descriptive text within the boxes (e.g., "Fine-tuned on ACME corp data," "Fine-tuned for customer service topic classification"), directly support these interpretations by explicitly naming the models, the data used for training, and the specific tasks they are adapted for.

**Key Insights:**
**Main Takeaways/Lessons:**
*   **Efficiency through Staged Fine-tuning:** It is more efficient to start with a generally "Pretrained model" (like "BERT") and progressively fine-tune it. This avoids training a model from scratch for every new domain or task.
*   **Importance of Domain Adaptation:** For enterprise or specific industry applications, "Continued pretraining" "on ACME corp data" is crucial. This step creates an "ACME Representation model" that understands the specific language and context of the domain, making subsequent task-specific fine-tuning more effective.
*   **Versatility of Domain-Adapted Models:** A single domain-adapted "ACME Representation model" can serve as a powerful foundation for a variety of "specific tasks," such as "customer service topic classification," "semantic search," and "NER." This showcases the reusability and broad applicability of the domain-adapted model.
*   **Tailoring for Business Needs:** The process allows organizations to leverage state-of-the-art pre-trained models and tailor them precisely to their unique business requirements and data.

**Conclusions/Insights:**
*   The methodology presented is a best practice for deploying large language models in specialized environments. It balances the computational cost of training massive models with the need for high performance on very specific, real-world tasks.
*   The "ACME corp data" is a critical bridge between a generic understanding of language and specialized, enterprise-level applications.
*   The ability to derive multiple "Final fine-tuned models" from a single "Continued pretrained model" highlights the scalability and flexibility of this approach for diverse NLP needs within an organization.

**Textual Evidence for Insights:**
*   "Pretrained model" and "e.g., BERT" demonstrate starting with a strong, general foundation.
*   "Continued pretrained model" and "Fine-tuned on ACME corp data" directly support the need for domain adaptation. The resulting "ACME Representation model" is the evidence of this step.
*   "Final fine-tuned models" and "Fine-tuned on a specific task" (with the multiple task examples: "customer service topic classification," "semantic search," "NER") clearly show the versatility and task-specific optimization. The fact that all these final models stem from the "ACME representation model" (from step 2) reinforces the idea of reusability for diverse tasks.

**Document Context:**
This image, appearing in a section titled "Continued Pretraining with Masked Language Modeling" and followed by the caption "Figure 11-16. The three-step approach illustrated for specific use cases.", serves as a visual explanation of a recommended methodology for adapting large language models for practical applications, particularly within an enterprise context. It directly supports the discussion by providing a clear, step-by-step diagram of how a generic pre-trained model can be progressively specialized to meet the unique data and task requirements of an organization (like "ACME"). The diagram illustrates the abstract concepts of continued pretraining and fine-tuning, demonstrating their practical implementation and the benefits of a multi-stage adaptation process.

**Summary:**
This diagram illustrates a three-step approach for transforming a general-purpose, pre-trained language model into highly specialized models tailored for specific organizational tasks. This method is crucial for leveraging powerful AI models efficiently in real-world business scenarios.

**Step 1: The Pretrained Model**
The process begins with a "Pretrained model," which is a foundational "Representation model" that has already learned extensive language patterns from a vast amount of text data. A common example of such a model is "BERT." At this stage, the model possesses a general understanding of language but is not yet specialized for any particular domain or task.

**Step 2: The Continued Pretrained Model (Domain Adaptation)**
Next, this initial "Pretrained model" undergoes "Continued pretraining." This critical intermediate step involves further training the model by "Fine-tuned on ACME corp data." This means the model is exposed to and learns from the specific text and language used within the "ACME" corporation. The result is an "ACME Representation model," which is now better equipped to understand the terminology, context, and nuances of the ACME domain. This domain adaptation ensures the model is relevant to the specific business environment before being applied to individual tasks.

**Step 3: The Final Fine-tuned Models (Task Specialization)**
Finally, the "ACME Representation model" from Step 2 is used as a base for creating "Final fine-tuned models." This stage involves fine-tuning the domain-adapted model "on a specific task." This means the model is further trained on a smaller dataset specifically prepared for a particular application. The diagram shows three distinct examples of how this ACME-specific model can be specialized:
1. An "ACME representation model" "Fine-tuned for customer service topic classification" will excel at categorizing customer inquiries into predefined topics.
2. Another "ACME representation model" can be "Fine-tuned for semantic search," enabling more accurate and contextually relevant search results within ACME's data.
3. A third "ACME representation model" can be "Fine-tuned for NER" (Named Entity Recognition), allowing it to identify and classify specific entities (like names, organizations, locations) within ACME's text data.

In essence, this three-step process shows how to take a broadly intelligent language model, make it smart about a specific business domain, and then give it specialized skills for multiple different jobs, all while maximizing efficiency and performance. Each box also contains a small icon in its top-right corner, possibly indicating that these are modular components or can be further extended.](images/2943378f91e01a9c2f4d913dbed5d1cfc77ec4963ef28f87879963062fcf567a.jpg)
Figure 11-16. The three-step approach illustrated for specific use cases.

In this example, we will demonstrate how to apply step 2 and continue pretraining an already pretrained BERT model. We use the same data that we started with, namely the Rotten Tomatoes reviews.

We start by loading the "bert-base-cased" model we have used thus far and prepare it for MLM:

from transformers import AutoTokenizer, AutoModelForMaskedLM # Load model for masked language modeling (MLM) model $=$ AutoModelForMaskedLM.from_pretrained("bert-base-cased") tokenizer $=$ AutoTokenizer.from_pretrained("bert-base-cased")

We need to tokenize the raw sentences. We will also remove the labels since this is not a supervised task:

def preprocess_function(examples): return tokenizer(examples["text"], truncation=True)

# Tokenize data tokenized_train $=$ train_data.map(preprocess_function, batched=True) tokenized_train $=$ tokenized_train.remove_columns("label") tokenized_test $=$ test_data.map(preprocess_function, batched=True) tokenized_test $=$ tokenized_test.remove_columns("label")

Previously, we used DataCollatorWithPadding, which dynamically pads the input it receives.

Instead, we will have a DataCollator that will perform the masking of tokens for us. There are two methods that are generally used for this: token and whole-word masking. With token masking, we randomly mask $1 5 \%$ of the tokens in a sentence. It might happen that part of a word will be masked. To enable masking of the entire word, we could apply whole-word masking, as illustrated in Figure 11-17.

![## Image Analysis: 28ba4d16c1ecbd5ac7b2dff5098b7793e41f10f8174d532cf4ef0a373cbbf2e9.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates different strategies for masking tokens within a text sequence, a fundamental technique used in training masked language models (MLMs) in natural language processing (NLP). The main purpose of the image is to visually differentiate between two distinct masking approaches: "Token masking" and "Whole-word masking". It clearly demonstrates how an initial input sentence is first processed through tokenization, and then how these two different masking methods apply to the resulting tokenized sequence, showcasing their operational differences and the impact on the masked output.

**Content Interpretation:**
The image demonstrates three main processes: Input, Tokenization, Token masking, and Whole-word masking.

*   **Input Text:** The process begins with a natural language sentence: "Her vocalization was remarkably melodic". This is the raw data that needs to be processed.
*   **Tokenization:** This is the first transformation step, where the input text is converted into a sequence of smaller units called tokens. The arrow label "Split input up into tokens" indicates this. The sentence is broken down into: "[CLS]", "Her", "vocal", "##ization", "was", "remarkably", "melodic", "[SEP]". The special tokens `[CLS]` and `[SEP]` mark the beginning and end/separation of the sequence. The word "vocalization" is split into subword tokens "vocal" and "##ization", demonstrating a subword tokenization strategy (e.g., WordPiece or BPE) used to handle complex words and manage vocabulary.
*   **Token Masking:** This process, labeled "Randomly mask individual tokens", illustrates a masking strategy where *any* individual token can be randomly selected and replaced with a `[MASK]` token. In the example, the subword token "##ization" is replaced by "[MASK]", resulting in the sequence: "[CLS]", "Her", "vocal", "[MASK]", "was", "remarkably", "melodic", "[SEP]". This shows that the masking operation is applied at the granularity of individual subword tokens.
*   **Whole-word Masking:** This process, labeled "Randomly mask whole-words", demonstrates a more advanced masking strategy. If any subword token belonging to a complete word (like "vocalization", which comprises "vocal" and "##ization") is selected for masking, then *all* constituent subword tokens of that entire word are replaced with `[MASK]` tokens. In this case, both "vocal" and "##ization" are replaced by "[MASK]", yielding the sequence: "[CLS]", "Her", "[MASK]", "[MASK]", "was", "remarkably", "melodic", "[SEP]". This strategy forces the model to predict an entire word, even if it was originally split into multiple subword tokens.

The significance of these presentations lies in explaining the nuances of data preparation for self-supervised learning in NLP. Different masking strategies can influence how a language model learns contextual representations. Whole-word masking, for instance, forces the model to learn richer contextual embeddings for entire words, which might be more beneficial for downstream tasks.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Tokenization is the foundational step for text processing in NLP:** Raw text must first be converted into a sequence of tokens. The image demonstrates this by showing "Her vocalization was remarkably melodic" being tokenized into "[CLS] Her vocal ##ization was remarkably melodic [SEP]", including special tokens and subword segmentation.
2.  **Two distinct and important masking strategies exist for training language models:**
    *   **Token masking:** This method involves randomly masking individual subword tokens. The arrow label "Randomly mask individual tokens" and the specific change from "##ization" to "[MASK]" in the "Token masking" row exemplify this. This approach treats each subword token as an independent unit for masking.
    *   **Whole-word masking:** This method ensures that if any subword token within a full word is selected for masking, the entire original word (i.e., all its constituent subword tokens) is masked. The arrow label "Randomly mask whole-words" and the transformation of "vocal ##ization" into "[MASK] [MASK]" in the "Whole-word masking" row clearly illustrate this. This strategy makes the prediction task more challenging by obscuring the full word, potentially leading to better word-level contextual representations.
3.  **The choice of masking strategy significantly impacts the training data and model learning:** The visual contrast between the 

**Document Context:**
This image is highly relevant to a document discussing natural language processing, specifically the preprocessing steps for training self-supervised models like BERT or similar masked language models (MLMs). The preceding text mentions "Tokenize data tokenized_train = train_data.map(preprocess_function, batched=True)", which directly relates to the initial tokenization step shown in the image. The image then elaborates on different ways to "randomly mask tokens", visually clarifying a critical technique used to create training examples for masked language modeling, further reinforced by the caption "Figure 11-17. Different methods for randomly masking tokens." It helps readers understand the specific mechanisms by which raw text is transformed into a format suitable for training language models that rely on predicting masked tokens, and highlights the distinction between masking individual subword units versus entire words.

**Summary:**
This diagram illustrates the process of preparing text data for natural language processing models, specifically focusing on two distinct methods of "masking" tokens: "Token masking" and "Whole-word masking". Masking is a technique used in self-supervised learning where parts of the input are hidden, and the model is trained to predict the missing pieces, thereby learning contextual representations of language.

The process begins with an **Input** sentence: "Her vocalization was remarkably melodic".

1.  **Tokenization:** The first step involves splitting this input sentence into smaller, manageable units called tokens. This is indicated by the arrow label "Split input up into tokens". The original sentence is transformed into a sequence of tokens: "[CLS]", "Her", "vocal", "##ization", "was", "remarkably", "melodic", "[SEP]". Here, `[CLS]` and `[SEP]` are special tokens used to mark the beginning and end of a sequence, respectively. The word "vocalization" is broken down into two subword tokens, "vocal" and "##ization", a common practice in modern tokenization to handle complex words and reduce vocabulary size.

2.  **Token masking:** The next stage demonstrates "Token masking". This method "Randomly mask individual tokens". In this example, one of the subword tokens from the original tokenized sequence, "##ization", is randomly selected and replaced with a `[MASK]` token. The sequence then becomes: "[CLS]", "Her", "vocal", "[MASK]", "was", "remarkably", "melodic", "[SEP]". This highlights that masking occurs at the level of individual tokens, even if they are parts of a larger word.

3.  **Whole-word masking:** The final stage illustrates "Whole-word masking", which "Randomly mask whole-words". This method takes a more holistic approach. If any subword token belonging to a complete word is chosen for masking, then *all* subword tokens that make up that original word are masked. In this specific example, the word "vocalization" (which was tokenized into "vocal" and "##ization") is targeted. Consequently, both "vocal" and "##ization" are replaced with `[MASK]` tokens. The resulting sequence is: "[CLS]", "Her", "[MASK]", "[MASK]", "was", "remarkably", "melodic", "[SEP]".

In summary, the image effectively contrasts these two masking strategies, showing how "Token masking" replaces individual subword tokens, while "Whole-word masking" replaces all subword tokens that form a complete word, thus providing a clearer explanation of their operational differences and impact on the processed text.](images/28ba4d16c1ecbd5ac7b2dff5098b7793e41f10f8174d532cf4ef0a373cbbf2e9.jpg)
Figure 11-17. Different methods for randomly masking tokens.

Generally, predicting whole words tends to be more complicated than tokens, which makes the model perform better as it needs to learn more accurate and precise repre‐ sentations during training. However, it tends to take a bit more time to converge. We will be going with token masking in this example using DataCollatorForLan guageModeling for faster convergence. However, we can use whole-word masking by replacing DataCollatorForLanguageModeling with DataCollatorForWholeWord Mask. Lastly, we set the probability that a token is masked in a given sentence to $1 5 \%$ (mlm_probability):

from transformers import DataCollatorForLanguageModeling

# Masking Tokens   
data_collator $=$ DataCollatorForLanguageModeling( tokenizer $=$ tokenizer, mlm=True, mlm_probability=0.15   
)

Next, we will create the Trainer for running the MLM task and specify certain parameters:

# Training arguments for parameter tuning   
training_args $=$ TrainingArguments( "model", learning_rate $\begin{array} { r l } { \mathbf { \Psi } : } & { { } \mathbf { \Psi } : \mathbf { \Psi } } \end{array}$ , per_device_train_batch_size $= 1 6$ , per_device_eval_batch_size $= 1 6$ , num_train_epochs $= 1 0$ , weight_decay $\mathord {  = } 0 \mathrm { ~ , ~ } 0 1$ , save_strategy $^ { \prime } =$ "epoch", report_to="none"   
)   
# Initialize Trainer   
trainer $=$ Trainer( model=model, args $=$ training_args, train_dataset=tokenized_train, eval_dataset $: =$ tokenized_test, tokenizer $=$ tokenizer, data_collato $\ ' =$ data_collator   
)

Several parameters are worth noting. We train for 20 epochs and keep the task short. You can experiment with the learning rate and weight decay to ascertain whether they assist in fine-tuning the model.

Before we start our training loop we will first save our pretrained tokenizer. The tokenizer is not updated during training so there is no need to save it after training. We will, however, save our model after we continue pretraining:

# Save pre-trained tokenizer tokenizer.save_pretrained("mlm")

# Train model trainer.train()

# Save updated model model.save_pretrained("mlm")

This gives us an updated model in the mlm folder. To evaluate its performance we would normally fine-tune the model on a variety of tasks. For our purposes, however, we can run some masking tasks to see if it has learned from its continued training.

We will do so by loading in our pretrained model before we continue pretraining. Using the sentence "What a horrible [MASK]!" the model will predict which word would be in place of "[MASK]":

from transformers import pipeline # Load and create predictions mask_filler $=$ pipeline("fill-mask", model $\cdot ^ { = }$ "bert-base-cased") preds $=$ mask_filler("What a horrible [MASK]!")

# Print results   
for pred in preds: print $f ^ { \prime \prime } > > >$ {pred["sequence"]}")

$> > >$ What a horrible idea! $> > >$ What a horrible dream! >>> What a horrible thing! >>> What a horrible day! >>> What a horrible thought!

The output demonstrates concepts like “idea,” “dream,” and “day,” which definitely make sense. Next, let’s see what our updated model predicts:

# Load and create predictions mask_filler $=$ pipeline("fill-mask", model $\cdot ^ { = }$ "mlm") preds $=$ mask_filler("What a horrible [MASK]!")

# Print results   
for pred in preds: print $f ^ { \prime \prime } > > >$ {pred["sequence"]}")

$> > >$ What a horrible movie! $> > >$ What a horrible film! $> > >$ What a horrible mess! $> > >$ What a horrible comedy! $> > >$ What a horrible story!

A horrible movie, film, mess, etc. clearly shows us that the model is more biased toward the data that we fed it compared to the pretrained model.

The next step would be to fine-tune this model on the classification task that we did at the beginning of this chapter. Simply load the model as follows and you are good to go:

from transformers import AutoModelForSequenceClassification

# Fine-tune for classification   
model $=$ AutoModelForSequenceClassification.from_pretrained("mlm", num_labels $^ { , = 2 }$ )   
tokenizer $=$ AutoTokenizer.from_pretrained("mlm")

# Named-Entity Recognition

In this section, we will delve into the process of fine-tuning a pretrained BERT model specifically for NER (named-entity recognition). Instead of classifying entire documents, this procedure allows for the classification of individual tokens and/or words, including people and locations. This is especially helpful for de-identification and anonymization tasks when there is sensitive data.

NER shares similarities with the classification example we explored at the beginning of this chapter. Nevertheless, a key distinction lies in the preprocessing and classifica‐ tion of data. Given that we are focusing on classifying individual words instead of entire documents, we must preprocess the data to consider this granular structure. Figure 11-18 provides a visual representation of this word-level approach.

![## Image Analysis: 2798158f7240f6b7dc0cfc6aaad81685c9ceb015cd24aba1c139ef76aa72a4d3.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of Named-Entity Recognition (NER) applied to a textual input using a BERT model that has been specifically fine-tuned for this task. The main purpose is to demonstrate how the BERT model takes a natural language sentence, processes it, and then identifies and categorizes specific words or phrases within that sentence into predefined categories, such as "Person" and "Location." The key idea being communicated is the capability of specialized large language models like BERT to perform detailed linguistic analysis, specifically entity extraction.

**Content Interpretation:**
The image shows a fundamental natural language processing (NLP) system, specifically focusing on Named-Entity Recognition.

*   **Input Sentence:** The phrase "I am Maarten and I live in the Netherlands." (extracted from the top pink box) represents raw, unstructured text input. This is the data that the NER system needs to analyze.
*   **BERT Model (Fine-tuned for NER):** The central teal box labeled "BERT" with the sub-label "Fine-tuned for NER" signifies the core processing component. "BERT" stands for Bidirectional Encoder Representations from Transformers, a powerful pre-trained language model. The "Fine-tuned for NER" aspect highlights that the general-purpose BERT model has been specialized (trained further on a dataset of sentences with annotated entities) to excel at identifying named entities. This detail is crucial as it explains *how* the model achieves its task.
*   **Tokenized Output:** The series of individual rounded boxes at the bottom, containing "I", "am", "Maarten", "and", "I", "live", "in", "the", "Netherlands", "." demonstrate the tokenization process, where the input sentence is broken down into its constituent words or sub-word units. This is a common first step in many NLP pipelines.
*   **Named Entity Labels:** The labels "Person" and "Location" (extracted below "Maarten" and "the Netherlands" respectively, connected by dashed lines) are the direct output of the NER task. They represent the model's classification of specific token sequences as named entities.
    *   "Maarten" being labeled "Person" correctly identifies a proper noun as a person's name.
    *   "the Netherlands" being labeled "Location" correctly identifies a geographical name.

All extracted text elements work together to visually explain the transformation of a simple sentence into a structured output where key entities are highlighted and categorized. The progression from the full sentence, through the BERT model, to the tokenized and labeled output explicitly demonstrates the flow of data and the function of the fine-tuned BERT model in identifying specific types of named entities.

**Key Insights:**
The image provides several key takeaways:

*   **BERT's Application in NER:** A BERT model can be effectively utilized for Named-Entity Recognition tasks. The central box explicitly states "BERT Fine-tuned for NER", which is direct evidence for this.
*   **Importance of Fine-Tuning:** For specialized NLP tasks like NER, general-purpose models like BERT benefit significantly from fine-tuning. The phrase "Fine-tuned for NER" is critical, indicating that the model's accuracy in identifying "Person" and "Location" from the sentence "I am Maarten and I live in the Netherlands." relies on this specific training.
*   **NER Identifies Specific Entity Types:** NER aims to categorize specific textual spans into predefined categories. The labels "Person" (for "Maarten") and "Location" (for "the Netherlands") clearly demonstrate the output of such categorization, showing that the model can distinguish between different types of named entities.
*   **Tokenization as a Precursor/Output Stage:** The breakdown of the sentence into individual tokens ("I", "am", "Maarten", etc.) illustrates that NER often operates on a token level, either as an input preparation step or as part of the output representation.
*   **The Process is Automated:** The diagram implicitly suggests an automated process where an input sentence is fed into a system (BERT), which then automatically identifies and labels entities.

**Document Context:**
This image fits directly within a document section titled "Named-Entity Recognition." It serves as a clear visual example, augmenting the textual explanation of how a BERT model, specifically fine-tuned for NER, performs its task. The text after the image, "Figure 11-18. Fine-tuning a BERT model for NER allows for the detection of named entities, such as people or locations," perfectly summarizes the visual content and reinforces its role in demonstrating a core concept of NER using a modern NLP technique.

**Summary:**
This diagram illustrates the process of Named-Entity Recognition (NER) using a Bidirectional Encoder Representations from Transformers (BERT) model that has been specifically adapted for this task. The flow begins with an input sentence, which is then processed by the BERT model, leading to the identification and labeling of specific named entities within that sentence.

The process unfolds in three main stages:

1.  **Input:** The starting point is a complete natural language sentence: "I am Maarten and I live in the Netherlands." This sentence is presented in a pink, rounded rectangular box at the top of the diagram, representing the raw text data that needs to be analyzed.
2.  **Processing:** This input sentence is then fed into a central processing unit, represented by a teal rectangular box. This unit is identified as "BERT," a powerful language model. Crucially, the label "Fine-tuned for NER" indicates that this BERT model has undergone additional training specifically to excel at the task of identifying named entities, rather than performing a general language understanding task. A downward arrow connects the input sentence to this BERT processing unit, signifying the flow of data.
3.  **Output and Entity Identification:** Following processing by the fine-tuned BERT model, the original sentence is broken down into individual word-level units, or tokens. These tokens are displayed in a sequence of smaller, rounded rectangular boxes at the bottom: "I", "am", "Maarten", "and", "I", "live", "in", "the", "Netherlands", ".". A downward arrow connects the BERT model to this sequence of tokens. As part of the NER task, certain tokens or groups of tokens are identified as specific types of named entities.
    *   The token "Maarten" is highlighted (with a dashed line underneath) and explicitly labeled as "Person," indicating that the BERT model correctly recognized it as a person's name.
    *   Similarly, the sequence of tokens "the Netherlands" is highlighted (with a dashed line underneath) and explicitly labeled as "Location," signifying that the BERT model accurately identified it as a geographical location.

In essence, the diagram visually demonstrates how a specialized BERT model can parse a sentence, understand its components, and accurately tag key information such as names of people and places, thereby transforming unstructured text into semi-structured data by categorizing named entities. This detailed process is central to understanding how modern NLP systems extract meaningful information from text.](images/2798158f7240f6b7dc0cfc6aaad81685c9ceb015cd24aba1c139ef76aa72a4d3.jpg)
Figure 11-18. Fine-tuning a BERT model for NER allows for the detection of named entities, such as people or locations.

Fine-tuning the pretrained BERT model follows a similar architecture akin to what we observed with document classification. However, there is a fundamental shift in the classification approach. Rather than relying on the aggregation or pooling of token embeddings, the model now makes predictions for individual tokens in a sequence. It is crucial to emphasize that our word-level classification task does not entail classifying entire words, but rather the tokens that collectively constitute those words. Figure 11-19 provides a visual representation of this token-level classification.

![## Image Analysis: c660820391d613dc87625f2b7dba577a50fb7cc9db7bf1687e301043f69f8420.jpg

**Conceptual Understanding:**
This image conceptually represents the workflow of Named-Entity Recognition (NER) using a pre-trained BERT model adapted for token classification. The main purpose is to demonstrate how a sentence is broken down into tokens, processed by BERT, and then individually classified by a subsequent neural network to identify named entities. It visually clarifies the fine-tuning process where classifications are made at the token level rather than for whole words or documents, as mentioned in the accompanying text. The image communicates the key idea of granular, token-based entity recognition within a natural language processing pipeline.

**Content Interpretation:**
The image illustrates a token-level classification process, specifically for Named-Entity Recognition (NER), using a BERT (Bidirectional Encoder Representations from Transformers) model followed by a Feedforward neural network. It shows the sequential steps involved in taking a raw text input, tokenizing it, processing it through BERT, and then classifying each individual token into an entity type or 'No entity'. This process highlights how BERT's subword tokenization breaks down words (like 'Maarten' into 'Maar' and '##ten') and how each token's representation is then used for fine-grained classification.

**Key Insights:**
1. BERT models process input text by tokenizing sentences into subword units, including special tokens like '[CLS]' (for classification tasks) and '[SEP]' (for separation). As seen with 'My name is Maarten' tokenizing into '[CLS]', 'My', 'name', 'is', 'Maar', '##ten', '[SEP]'.
2. The BERT model generates contextualized embeddings or representations for each input token. These are implicitly represented by the purple boxes outputted from the 'BERT' layer.
3. For tasks like Named-Entity Recognition, a downstream 'Feedforward neural network' is typically used to classify these token embeddings. 
4. Classification is performed at the token level, assigning an entity label (e.g., 'Person') or 'No entity' to each individual token, which is evidenced by the final output boxes 'No entity', 'No entity', 'No entity', 'Person', 'Person'.
5. This approach allows for granular entity recognition, even when named entities are split across multiple tokens due to subword tokenization (e.g., 'Maar' and '##ten' both classified as 'Person' to form 'Maarten').

**Document Context:**
This image directly supports the document's section on 'Named-Entity Recognition' and the text immediately following it, which states: 'During the fine-tuning process of a BERT model, individual tokens are classified instead of words or entire documents.' It visually explains this statement by detailing the mechanism of tokenization, BERT processing, and subsequent token-level classification to identify named entities within a sentence. The example clearly demonstrates token-level classification for the named entity 'Maarten'.

**Summary:**
The image illustrates the process of Named-Entity Recognition (NER) using a BERT model, specifically demonstrating how an input sentence is processed at the token level to classify each token. The process begins with an 'Input' sentence: 'My name is Maarten'. This input sentence is then tokenized into individual 'Tokens'. The tokenization process yields the following tokens: '[CLS]', 'My', 'name', 'is', 'Maar', '##ten', and '[SEP]'. Each of these tokens is fed into the 'BERT' model. The BERT model processes these tokens, producing a corresponding output representation (indicated by the purple square boxes) for each token. These representations are then passed to a 'Feedforward neural network'. Finally, the Feedforward neural network classifies each token. The classifications shown are: 'No entity' for '[CLS]', 'No entity' for 'My', 'No entity' for 'name', 'No entity' for 'is', 'Person' for 'Maar', and 'Person' for '##ten'. This detailed flow highlights how BERT handles subword tokenization (e.g., 'Maarten' becoming 'Maar' and '##ten') and how a downstream network then uses BERT's output to assign entity labels to individual tokens.](images/c660820391d613dc87625f2b7dba577a50fb7cc9db7bf1687e301043f69f8420.jpg)
Figure 11-19. During the fine-tuning process of a BERT model, individual tokens are classified instead of words or entire documents.

# Preparing Data for Named-Entity Recognition

In this example, we will use the English version of the CoNLL-2003 dataset, which contains several different types of named entities (person, organization, location, miscellaneous, and no entity) and has roughly 14,000 training samples.4

# The CoNLL-2003 dataset for NER dataset $=$ load_dataset("conll2003", trust_remote_code=True)

While researching datasets to use for this example, there were a few more that we wanted to share. wnut_17 is a task that focuses on emerging and rare entities, those that are more difficult to spot. Furthermore, the tner/mit_movie_trivia and tner/mit_res taurant datasets are quite fun to use. tner/mit_movie_trivia is for detecting entities like actor, plot, and soundtrack whereas tner/ mit_restaurant aims to detect entities such as amenity, dish, and cuisine.5

Let’s inspect the structure of the data with an example:

example $=$ dataset["train"][848] example

{'id': '848',   
'tokens': ['Dean', 'Palmer', 'hit', 'his', '30th', 'homer', 'for', 'the', 'Rangers', '.'],   
'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],   
'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],   
'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}

This dataset provides us with labels for each word given in a sentence. These labels can be found in the ner_tags key, which refers to the following possible entities:

label2id $= \ \left\{ \begin{array} { r l } \end{array} \right.$ "O": 0, "B-PER": 1, "I-PER": 2, "B-ORG": 3, "I-ORG": 4, "B-LOC": 5, "I-LOC": 6, "B-MISC": 7, "I-MISC": 8   
}   
id2label $=$ {index: label for label, index in label2id.items()}   
label2id

{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}

These entities correspond to specific categories: a person (PER), organization (ORG), location (LOC), miscellaneous entities (MISC), and no entity (O). Note that these entities are prefixed with either a B (beginning) or an I (inside). If two tokens that follow each other are part of the same phrase, then the start of that phrase is indicated with B, which is followed by an I to show that they belong to each other and are not independent entities.

This process is further illustrated in Figure 11-20. In the figure, since “Dean” is the start of the phrase and “Palmer” is the end, we know that “Dean Palmer” is a person and that “Dean” and “Palmer” are not individual people.

![## Image Analysis: a59533f2d4efa37dd287c99d68567c00bd636a21c58caa14028836eb1e0d8774.jpg

**Conceptual Understanding:**
The image conceptually represents the process of Named Entity Recognition (NER) for extracting named entities like persons and locations from a sentence. Its main purpose is to illustrate the BIO (Beginning, Inside, Outside) tagging scheme, which is fundamental in sequence tagging tasks for NER. It shows how a sequence of individual words, each potentially carrying an entity tag (B-PER, I-PER, B-LOC), can be combined to form a complete, recognized named entity or 'Full phrase', such as a 'Person' or a 'Location'.

**Content Interpretation:**
This image visually represents the core concepts of Named Entity Recognition (NER) specifically demonstrating the 'BIO' (Beginning, Inside, Outside) tagging scheme for identifying and extracting multi-word entities from text. It illustrates how a sequence of words in a sentence is analyzed, tagged at the word level, and then composed into full, meaningful entities. The process shows the identification of specific types of entities, namely 'Person' and 'Location', by combining contiguous words that are part of the same entity.

**Key Insights:**
The main takeaway is the clear illustration of the BIO tagging scheme used in Named Entity Recognition. The image demonstrates that: 1. Entities spanning multiple words are identified by tagging the first word with 'B-' followed by the entity type (e.g., 'B-PER' for 'Dean'). 2. Subsequent words that are part of the same entity are tagged with 'I-' followed by the entity type (e.g., 'I-PER' for 'Palmer'). 3. This tagging allows for the reconstruction of the 'Full phrase' entities ('Dean Palmer' as 'Person', 'Rangers' as 'Location'). The textual evidence 'Start of phrase' and 'Part of previous phrase' explicitly support the mechanism of building multi-word entities from individual word tags, highlighting how the 'B-' and 'I-' tags work in conjunction to define entity boundaries and types.

**Document Context:**
The image directly supports the document's discussion about the "CoNLL-2003 dataset for NER," which is a widely used benchmark for Named Entity Recognition tasks. It provides a concrete visual example of how named entities are typically annotated in such datasets, explaining the "B-" (Beginning) and "I-" (Inside) tagging scheme. This visual explanation enhances the reader's understanding of how models process and recognize entire phrases as entities, rather than just single words, which is crucial for comprehending the application and results of NER using datasets like CoNLL-2003 as indicated by the surrounding text: "By indicating the start and end of the phrase with the same entity, we can recognize entities of entire phrases."

**Summary:**
The image illustrates the process of Named Entity Recognition (NER) for identifying multi-word phrases and their corresponding entity types within a sentence. It systematically shows how individual words are tagged and then grouped to form complete entities. Starting with the sentence, each word is presented in a rounded rectangular box. Words identified as part of an entity are further tagged with either 'B-' (Beginning) or 'I-' (Inside) prefix, followed by the entity type (e.g., 'PER' for Person, 'LOC' for Location). For instance, 'Dean' is tagged 'B-PER' indicating the start of a Person entity, and 'Palmer' is tagged 'I-PER' indicating it's part of the previous Person entity. These individual tagged words are then shown to be combined into a 'Full phrase' below, which includes the entire phrase and its consolidated entity type (e.g., 'Dean Palmer Person'). Similarly, 'Rangers' is tagged 'B-LOC', marking the beginning of a Location entity, and is subsequently recognized as the 'Full phrase' 'Rangers Location'. The remaining words in the sentence ('hit', 'his', '30th', 'homer', 'for', 'the', '.') are presented without specific entity tags, implying they are not part of the highlighted entities. The arrows and labels clearly indicate the progression from individual word tags to recognized full phrases, explaining the 'Start of phrase' and 'Part of previous phrase' concepts in NER.](images/a59533f2d4efa37dd287c99d68567c00bd636a21c58caa14028836eb1e0d8774.jpg)
Figure 11-20. By indicating the start and end of the phrase with the same entity, we can recognize entities of entire phrases.

Our data is preprocessed and split up into words but not yet tokens. To do so, we will tokenize it further with the tokenizer of the pretrained model we used throughout this chapter, namely bert-base-cased:

from transformers import AutoModelForTokenClassification # Load tokenizer tokenizer $=$ AutoTokenizer.from_pretrained("bert-base-cased")

# Load model   
model $=$ AutoModelForTokenClassification.from_pretrained( "bert-base-cased", num_labels=len(id2label), id2label $\ l =$ id2label, label2id=label2id   
)

Let’s explore how the tokenizer would process our example:

# Split individual tokens into sub-tokens   
token_ids $=$ tokenizer(example["tokens"], is_split_into_words=True)["input_ids"]   
sub_tokens $=$ tokenizer.convert_ids_to_tokens(token_ids)   
sub_tokens ['[CLS]',   
'Dean',   
'Palmer',   
'hit',   
'his',   
'30th',   
'home',   
'##r',   
'for',   
'the',   
'Rangers', ，，   
.',   
'[SEP]']

The tokenizer added the [CLS] and [SEP] tokens as we learned in Chapters 2 and 3. Note that the word 'homer' was further split up into the tokens 'home' and '##r'. This creates a bit of a problem for us since we have labeled data at the word level but not at the token level. This can be resolved by aligning the labels with their subtoken counterparts during tokenization.

Let’s consider the word 'Maarten', which has the label B-PER to signal that this is a person. If we pass that word through the tokenizer, it splits the word up into the tokens 'Ma', '##arte', and '##n'. We cannot use the B-PER entity for all tokens as that would signal that the three tokens are all independent people. Whenever an entity is split into tokens, the first token should have B (for beginning) and the following should be I (for inner).

Therefore, 'Ma' will get the B-PER to signal the start of a phrase, and '##arte', and '##n' will get the I-PER to signal they belong to a phrase. This alignment process is illustrated in Figure 11-21.

![## Image Analysis: 871b38f046caebac8d7401cacbe58b3225356792395bbe291a001cb2d6df62ad.jpg

**Conceptual Understanding:**
This image illustrates the conceptual process of named entity recognition (NER) labeling when applied to text that has undergone sub-word tokenization. It specifically shows how a single named entity (like a proper noun) that might be represented as one token in the input can be broken down into multiple sub-tokens by a tokenizer (e.g., a WordPiece or Byte-Pair Encoding tokenizer), and the subsequent need to align the NER labels correctly to these resulting sub-tokens.

The main purpose is to demonstrate the alignment strategy for NER labels when individual words are split into sub-tokens. It highlights the transition from a simple "Beginning-of-entity" (B-PER) label for a whole word to a "Beginning-of-entity" (B-PER) for the first sub-token and "Inside-of-entity" (I-PER) for subsequent sub-tokens that comprise the original named entity.

**Key ideas or concepts being communicated:**
*   **Sub-word Tokenization:** The process of breaking down words into smaller units (sub-tokens), often used in transformer-based models to handle out-of-vocabulary words and improve efficiency.
*   **Named Entity Recognition (NER):** The task of identifying and classifying named entities (e.g., person names, organizations, locations) in text.
*   **IOB (Inside-Outside-Beginning) Tagging Scheme:** A common scheme for NER, where 'B-' denotes the beginning of an entity, 'I-' denotes a token inside an entity, and 'O' (outside) for non-entity tokens (not explicitly shown for "My name is" but implied).
*   **Label Alignment:** The crucial step of ensuring that the labels assigned to the original input tokens are correctly propagated and adapted to the new, often more granular, sub-tokens generated by the tokenizer.

**Content Interpretation:**
The image clearly shows a text processing pipeline involving three main stages: "Input", "Tokenized", and "Aligned". It illustrates the concept of tokenization, specifically sub-word tokenization, where a single word like "Maarten" is broken down into smaller units ("Ma", "##arte", "##n"). The "##" prefix in "##arte" and "##n" is a standard convention indicating that these are continuations of a previous token, not standalone words. It demonstrates the task of Named Entity Recognition (NER) by showing the entity "Maarten" being labeled as a "Person" (PER). Crucially, it elucidates the relationship between original word-level entities and sub-word tokens and the challenge of maintaining label consistency across these different granularities. The IOB tagging scheme is directly demonstrated through the use of "B-PER" (Beginning-Person) and "I-PER" (Inside-Person) labels.

The splitting of "Maarten" into "Ma", "##arte", "##n" highlights a common behavior of sub-word tokenizers, which are designed to handle complex words and out-of-vocabulary terms by breaking them into known sub-units. This is significant because it complicates the direct transfer of word-level labels. The transformation from "Maarten B-PER" to "Ma B-PER ##arte I-PER ##n I-PER" showcases the solution to this complication: aligning labels. The first sub-token ("Ma") gets the "B-PER" tag, indicating the start of the entity. Subsequent sub-tokens that belong to the same original entity ("##arte" and "##n") receive the "I-PER" tag, signifying they are inside the same entity. This is vital for correctly identifying the full extent of a named entity after tokenization. The annotations "Start of phrase" and "Part of previous phrase" explicitly clarify the tagging logic: "B-PER" marks the beginning, while "I-PER" marks the continuation.

**Extracted text elements supporting these interpretations:**
*   **Row labels:** "Input", "Tokenized", "Aligned" delineate the three stages.
*   **Input row content:** "My name is Maarten B-PER" establishes the initial word-level entity and label.
*   **Top arrow label:** "Start of phrase" confirms the "B-PER" meaning for the initial word.
*   **Tokenized row content:** "My name is Ma ##arte ##n" directly shows the outcome of sub-word tokenization, fragmenting "Maarten".
*   **Aligned row content:** "My name is Ma B-PER ##arte I-PER ##n I-PER" provides the solution, showing fine-grained labels assigned to sub-tokens.
*   **Bottom arrow labels:** "Start of phrase" (for "Ma B-PER") and "Part of previous phrase" (for "##arte I-PER" and "##n I-PER") precisely define the "B-PER" and "I-PER" tag meanings in the sub-token context, making the alignment logic explicit.

**Key Insights:**
**Main takeaways or lessons this image teaches:**
1.  **Sub-word tokenization breaks down words:** Modern NLP tokenizers often split single words, particularly proper nouns or complex terms, into smaller sub-word units. This is evident from "Maarten" in the "Input" stage being transformed into "Ma", "##arte", "##n" in the "Tokenized" stage.
2.  **NER label alignment is necessary for sub-word tokens:** When words are tokenized into sub-words, the original word-level NER labels cannot be directly applied. Instead, a new alignment process is required to assign appropriate labels to each sub-token. This is demonstrated by the transition from a single "B-PER" for "Maarten" to "B-PER" for "Ma" and "I-PER" for "##arte" and "##n".
3.  **IOB tagging handles multi-token entities:** The "B-" (Beginning) and "I-" (Inside) prefixes in the NER tags (e.g., "B-PER", "I-PER") are crucial for correctly identifying named entities that span multiple tokens, especially after sub-word tokenization. The explicit annotations "Start of phrase" and "Part of previous phrase" underscore this lesson.
4.  **The "##" prefix signifies token continuation:** The use of "##" before "arte" and "n" in the tokenized sequence provides a visual and textual cue that these sub-tokens are not independent words but parts of a larger word that was split.

**Conclusions or insights this image supports:**
*   **Complexity of NLP pipelines:** This diagram highlights that even seemingly simple tasks like NER require careful handling of intermediate steps like tokenization, which can introduce complexities.
*   **Importance of granular labeling:** For high-quality NER, it's essential to have a labeling scheme that can precisely mark the boundaries and constituents of entities, even at the sub-word level. The "Aligned" stage clearly shows this granular labeling.
*   **Design considerations for models:** NLP models that use sub-word tokenizers must be designed to correctly interpret and predict these "B-" and "I-" tags across multiple sub-tokens to reconstruct the full named entity.

**Evidence for these insights from extracted text:**
*   **Input: "My name is Maarten B-PER" & "Start of phrase" (top annotation):** Provides the baseline; a single word "Maarten" is identified as a person and marks the start of an entity.
*   **Tokenized: "My name is Ma ##arte ##n":** Directly shows the result of sub-word tokenization, breaking "Maarten" into three distinct sub-tokens, with "##" indicating continuations. This is evidence for lesson 1.
*   **Aligned: "My name is Ma B-PER ##arte I-PER ##n I-PER" with associated "Start of phrase" and "Part of previous phrase" annotations:** This is the strongest evidence for lessons 2 and 3. It explicitly shows how the "B-PER" tag is assigned to the first sub-token ("Ma") of the original entity, and "I-PER" tags are assigned to the subsequent sub-tokens ("##arte", "##n") to indicate that they are part of the *same* named entity. The "##" prefix in the tokenized and aligned states directly supports lesson 4.

**Document Context:**
Given the document context "Section: Split individual tokens into sub-tokens" and the figure caption "Figure 11-21. The alignment process of labeling tokenized input," this image is central to explaining a critical aspect of Natural Language Processing (NLP) pipelines, particularly when working with advanced tokenization methods like sub-word tokenization (e.g., WordPiece, Byte-Pair Encoding). It addresses the practical challenge of how to correctly transfer and align labels (specifically Named Entity Recognition, or NER, labels) from an original, potentially word-level, annotation scheme to a sub-word tokenized representation. This is crucial for maintaining the integrity and accuracy of annotations when data is processed for machine learning models that operate on sub-word units. The image serves as a clear visual example of the solution to this challenge.

**Summary:**
This diagram illustrates "The alignment process of labeling tokenized input," demonstrating how a text phrase is processed from its original form through tokenization, and then how named entity labels are correctly assigned to the resulting sub-word tokens.

Let's break down the process:

1.  **Input:**
    The process begins with an "Input" phrase: "My name is Maarten". In this initial stage, "Maarten" is recognized as a named entity, specifically a "Person" (PER). It's labeled with "B-PER", which stands for "Beginning of Person entity", indicating that "Maarten" is the start of a person's name. The annotation "Start of phrase" points to this labeled word, clarifying its role.

2.  **Tokenized:**
    Next, the input phrase undergoes "Tokenization." This is a process where the text is broken down into smaller units, or tokens. While "My," "name," and "is" remain as single tokens, the word "Maarten" is split into three smaller "sub-tokens": "Ma," "##arte," and "##n." The "##" prefix is a convention used by many sub-word tokenizers to indicate that "##arte" and "##n" are continuations of a preceding token, not standalone words. This sub-word tokenization is often used in modern NLP models to handle complex words and improve model efficiency by reducing the vocabulary size.

3.  **Aligned:**
    The final stage is "Alignment." Here, the named entity labels (like "B-PER") from the original input are adapted and assigned to the sub-tokens generated during tokenization.
    *   The sub-token "Ma" receives the label "B-PER." An arrow annotated "Start of phrase" points to it, signifying that "Ma" is the beginning of the "Person" entity in this tokenized sequence.
    *   The subsequent sub-token, "##arte," receives the label "I-PER," which stands for "Inside Person entity." This indicates that "##arte" is a part of the same "Person" entity that started with "Ma." An arrow annotated "Part of previous phrase" points to it to emphasize this continuation.
    *   Similarly, the last sub-token, "##n," also receives the label "I-PER," indicating it continues to be "Inside Person entity." Another arrow labeled "Part of previous phrase" clarifies its role as a continuation of the same entity.

In summary, this diagram effectively demonstrates how a single named entity, initially identified as a whole word with a "Beginning" tag, is carefully segmented into sub-word tokens and then re-labeled with a combination of "Beginning" and "Inside" tags. This precise "alignment" ensures that even after complex tokenization, the full extent of a named entity can still be accurately identified and reconstructed, which is fundamental for tasks like Named Entity Recognition in advanced NLP applications.](images/871b38f046caebac8d7401cacbe58b3225356792395bbe291a001cb2d6df62ad.jpg)
Figure 11-21. The alignment process of labeling tokenized input.

We create a function, align_labels, that will tokenize the input and align these tokens with their updated labels during tokenization:

def align_labels(examples): token_ids $=$ tokenizer( examples["tokens"], truncation=True, is_split_into_words=True ) labels $=$ examples["ner_tags"] updated_labels $= \ [ ]$ for index, label in enumerate(labels): # Map tokens to their respective word word_ids $=$ token_ids.word_ids(batch_index $\ l =$ index) previous_word_idx $=$ None label_ids $=$ [] for word_idx in word_ids: # The start of a new word if word_idx $\downarrow =$ previous_word_idx: previous_word_idx $=$ word_idx updated_label $= ~ - 1 0 0$ if word_idx is None else label[word_idx] label_ids.append(updated_label) # Special token is -100 elif word_idx is None: label_ids.append(-100) # If the label is B-XXX we change it to I-XXX else: updated_label $=$ label[word_idx] if updated_label $\% 2 \ = \ 1$ :

updated_labels.append(label_ids)

token_ids["labels"] $=$ updated_labels return token_ids

tokenized $=$ dataset.map(align_labels, batched=True)

Looking at our example, note that additional labels (-100) were added for the [CLS] and [SEP] tokens:

# Difference between original and updated labels print(f"Original: {example["ner_tags"]}") print(f"Updated: {tokenized["train"][848]["labels"]}")

Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0] Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]

Now that we have tokenized and aligned the labels, we can start thinking about defining our evaluation metrics. This is also different from what we have seen before. Instead of a single prediction per document, we now have multiple predictions per document, namely per token.

We will make use of the evaluate package by Hugging Face to create a compute_met rics function that allows us to evaluate performance on a token level:

import evaluate

# Load sequential evaluation   
seqeval $=$ evaluate.load("seqeval")   
def compute_metrics(eval_pred): # Create predictions logits, labels $=$ eval_pred predictions $=$ np.argmax(logits, axis $^ { = 2 }$ ) true_predictions $=$ [] true_labels $=$ [] # Document-level iteration for prediction, label in zip(predictions, labels): # Token-level iteration for token_prediction, token_label in zip(prediction, label): # We ignore special tokens if token_label $\downarrow =$ -100: true_predictions.append([id2label[token_prediction]]) true_labels.append([id2label[token_label]])

results $=$ seqeval.compute( predictions=true_predictions, references $=$ true_labels ) return {"f1": results["overall_f1"]}

# Fine-Tuning for Named-Entity Recognition

We are nearly there. Instead of DataCollatorWithPadding, we need a collator that works with classification on a token level, namely DataCollatorForTokenClassifica tion:

from transformers import DataCollatorForTokenClassification # Token-classification DataCollator data_collator $=$ DataCollatorForTokenClassification(tokenizer=tokenizer)

Now that we have loaded our model, the rest of the steps are similar to previous training procedures in this chapter. We define a trainer with specific arguments that we can tune and create a Trainer:

# Training arguments for parameter tuning   
training_args $=$ TrainingArguments( "model", learning_rate $^ { 1 \pm }$ 2e-5, per_device_train_batch_size $= 1 6$ , per_device_eval_batch_size $= 1 6$ , num_train_epochs $: = 1$ , weight_decay $\mathbf { \Sigma } = \mathbf { \Sigma }$ .01, save_strategy="epoch", report_to $1 =$ "none"   
)   
# Initialize Trainer   
trainer $=$ Trainer( model model, args $=$ training_args, train_dataset=tokenized["train"], eval_dataset $: =$ tokenized["test"], tokenizer=tokenizer, data_collato $\ ' =$ data_collator, compute_metrics $=$ compute_metrics,   
)   
trainer.train()

We then evaluate the model that we created:

# Evaluate the model on our test data trainer.evaluate()

Lastly, let’s save the model and use it in a pipeline for inference. This allows us to check certain data so we can manually inspect what happens during inference and if we are satisfied with the output:

from transformers import pipeline   
# Save our fine-tuned model   
trainer.save_model("ner_model")   
# Run inference on the fine-tuned model   
token_classifier $=$ pipeline( "token-classification", model="ner_model",   
)   
token_classifier("My name is Maarten.")   
[{'entity': 'B-PER', 'score': 0.99534035, 'index': 4, 'word': 'Ma', 'start': 11, 'end': 13},   
{'entity': 'I-PER', 'score': 0.9928328, 'index': 5, 'word': '##arte', 'start': 13, 'end': 17},   
{'entity': 'I-PER', 'score': 0.9954301, 'index': 6, 'word': '##n', 'start': 17, 'end': 18}]

In the sentence "My name is Maarten", the word "Maarten" and its subtokens were correctly identified as a person!

# Summary

In this chapter, we explored several tasks for fine-tuning pretrained representation models on specific classification tasks. We started by demonstrating how to fine-tune a pretrained BERT model and extended the examples by freezing certain layers of its architectures.

We experimented with a few-shot classification technique called SetFit, which involves fine-tuning a pretrained embedding model together with a classification head using limited labeled data. Using only a few labeled data points, this model generated similar performance to the models we explored in earlier chapters.

Next, we delved into the concept of continued pretraining, where we used a pre‐ trained BERT model as a starting point and continued training it using different data. The underlying process, masked language modeling, is not only used for creating a representation model but can also be used to continue pretraining models.

Finally, we looked at named-entity recognition, a task that involves identifying spe‐ cific entities such as people and places in unstructured text. Compared to previous examples, this classification was done on a word level rather than on a document level.

In the next chapter, we continue with the field of fine-tuning language models but focus on generative models instead. Using a two-step process, we will explore how to fine-tune a generative model to properly follow instructions and then fine-tune it for human preference.

# Fine-Tuning Generation Models

In this chapter, we will take a pretrained text generation model and go over the pro‐ cess of fine-tuning it. This fine-tuning step is key in producing high-quality models and an important tool in our toolbox to adapt a model to a specific desired behavior. Fine-tuning allows us to adapt a model to a specific dataset or domain.

Throughout this chapter, we will guide you among the two most common methods for fine-tuning text generation models, supervised fine-tuning and preference tuning. We will explore the transformative potential of fine-tuning pretrained text generation models to make them more effective tools for your application.

# The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and Preference Tuning

There are three common steps that lead to creating a high-quality LLM:

# 1. Language modeling

The first step in creating a high-quality LLM is to pretrain it on one or more massive text datasets (Figure 12-1). During training, it attempts to predict the next token to accurately learn linguistic and semantic representations found in the text. As we saw before in Chapters 3 and 11, this is called language modeling and is a self-supervised method.

This produces a base model, also commonly referred to as a pretrained or founda‐ tion model. Base models are a key artifact of the training process but are harder for the end user to deal with. This is why the next step is important.

![## Image Analysis: ea197ac3019334dc171b91ca06af26941c4bb908825ca413033ff1d91c4dbe98.jpg

**Conceptual Understanding:**
The image conceptually represents the operational principle of Large Language Models (LLMs) during text generation. Its main purpose is to define LLMs by explaining the fundamental mechanism through which they produce human-like text. The central message conveyed is that LLMs predict the 'probability' of a word based on the 'previous words used in a sentence', emphasizing a context-dependent, probabilistic approach to language generation. The image essentially illustrates the 'language modeling' task performed by an LLM at a high level.

**Content Interpretation:**
The image illustrates the fundamental mechanism of Large Language Models (LLMs). It shows that LLMs are systems designed to generate text that appears human-like. The core process highlighted is 'predicting the probability of a word'. This prediction is explicitly stated to be dependent on the 'previous words used in a sentence'. This signifies a conditional probability, where the likelihood of a word is determined by the words that have already appeared. The blue highlight on 'probability' and the arrow originating from 'the previous words used in a sentence' and pointing to 'probability' visually reinforce that the input context (previous words) directly informs the probabilistic prediction. There are no other processes, relationships, or complex systems shown; the image serves as a concise definition of an LLM's generative process.

**Key Insights:**
The main takeaway from this image is a clear and concise definition of how Large Language Models (LLMs) generate human-like text. Key insights include: 1. LLMs are models that generate text resembling human writing. 2. The core mechanism involves predicting the 'probability' of a word. 3. This prediction is always conditional on 'the previous words used in a sentence', highlighting the context-aware nature of LLMs. 4. The visual emphasis on 'probability' (blue text and arrow) underscores its central role. These insights are directly supported by the verbatim text: 'Large language models (LLMs) are models that can generate human-like text by predicting the probability of a word given the previous words used in a sentence.'

**Document Context:**
This image is highly relevant to the 'Language modeling' section of the document, as indicated by 'Section: 1. Language modeling'. It serves as a foundational definition for what Large Language Models (LLMs) are and how they function in the context of language modeling. The subsequent text in the document, 'Figure 12-1. During language modeling, the LLM aims to predict the next token based on an input. This is a process without labels,' directly references this image (Figure 12-1) and elaborates on the process of an LLM predicting the next token based on input. The image thus provides the essential conceptual framework for understanding the mechanics of language modeling discussed in the surrounding text, clarifying that LLMs achieve human-like text generation through a probabilistic, context-aware prediction of words.

**Summary:**
The image defines Large Language Models (LLMs) and explains their primary function. LLMs are described as models capable of generating text that closely resembles human writing. This generation is achieved through a core mechanism: predicting the likelihood, or 'probability', of a specific word appearing next. This prediction is not arbitrary; it is always 'given the previous words used in a sentence', meaning the LLM utilizes the preceding context to determine the most probable next word. The blue highlighting of the word 'probability' and the curved blue arrow pointing towards it from 'the previous words used in a sentence' visually emphasizes the critical role of probabilistic prediction based on prior context in an LLM's operation. The text 'the previous words used in a sentence' is grayed out, indicating its role as the input or contextual information for the prediction.](images/ea197ac3019334dc171b91ca06af26941c4bb908825ca413033ff1d91c4dbe98.jpg)
Figure 12-1. During language modeling, the LLM aims to predict the next token based on an input. This is a process without labels.

# 2. Fine-tuning 1 (supervised fine-tuning)

LLMs are more useful if they respond well to instructions and try to follow them. When humans ask the model to write an article, they expect the model to generate the article and not list other instructions for example (which is what a base model might do).

With supervised fine-tuning (SFT), we can adapt the base model to follow instructions. During this fine-tuning process, the parameters of the base model are updated to be more in line with our target task, like following instructions. Like a pretrained model, it is trained using next-token prediction but instead of only predicting the next token, it does so based on a user input (Figure 12-2).

![## Image Analysis: ab900aa9da61bed8dc156b9b4bc3f6ab0bd69651e23d5b0c4f9dfa8de3a85724.jpg

**Conceptual Understanding:**
This image conceptually represents a simple, direct question-and-answer dialogue between a human user and a Large Language Model (LLM). The main purpose is to illustrate the LLM's capability to understand a specific query about a technical subject ('reinforcement learning') and provide a clear, accurate, and concise definition in response. It communicates the key idea that LLMs can serve as informative resources, capable of explaining complex topics, and highlights a core definition within the field of machine learning.

**Content Interpretation:**
This image displays a conversational interaction, specifically a question-and-answer exchange, between a 'User' and an 'LLM'. The user initiates the interaction by asking for information, and the LLM provides a definition for a technical concept. The processes shown are: 1. User input, representing a query. 2. LLM processing and generating a response. 3. LLM output, providing a definition. The core concept demonstrated is the LLM's ability to understand and define technical terms. The significance of the information presented is the definition of 'Reinforcement learning (RL)' as a type of machine learning where an 'agent' learns to make decisions by taking actions in an environment to maximize a reward signal. The blue curved arrow pointing from 'Reinforcement learning (RL)' to 'agent' in the LLM's response highlights the 'agent' as a central component or concept within reinforcement learning. All extracted text elements directly form the core of this interaction and definition.

**Key Insights:**
The main takeaways from this image are: 1. Large Language Models (LLMs) can effectively understand and respond to user queries, particularly those seeking definitions of technical concepts. 2. Reinforcement learning (RL) is a specific type of machine learning. 3. A key characteristic of reinforcement learning involves an 'agent' making decisions by taking actions in an environment. 4. The goal of the 'agent' in reinforcement learning is to maximize a 'reward signal'. The specific text elements provide direct evidence for these insights: 'User: "Tell me something about reinforcement learning."' initiates the query. 'LLM: Reinforcement learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize a reward signal.' provides the comprehensive definition. The blue curved arrow visually emphasizes 'agent' as a critical component, reinforcing its importance.

**Document Context:**
This image serves as a clear example of a user-LLM interaction, specifically demonstrating an LLM providing a definitional response. Given the document context regarding 'Fine-tuning 1 (supervised fine-tuning)' and the explanation that 'the LLM aims to predict the next token based on an input that has additional labels. In a sense, the label is the user’s input,' this image likely illustrates a desired output or a pair of input/output (prompt/completion) that would be used during a supervised fine-tuning process. The user's query acts as the input, and the LLM's comprehensive and correct definition of reinforcement learning acts as the 'label' or target output that the LLM is trained to produce for such inputs. It shows a successfully fine-tuned LLM providing an accurate and relevant response.

**Summary:**
The image depicts a conversational exchange between a 'User' and an 'LLM' (Large Language Model), illustrating how an LLM might respond to a query about reinforcement learning. The interaction begins with the user's input, followed by the LLM's detailed, explanatory response. A key term in the LLM's response, 'agent,' is visually highlighted by a blue curved arrow, drawing attention to its significance within the definition of reinforcement learning. The overall structure is simple: a user asks a question, and the LLM provides a relevant, informative answer, specifically defining a technical concept.](images/ab900aa9da61bed8dc156b9b4bc3f6ab0bd69651e23d5b0c4f9dfa8de3a85724.jpg)
Figure 12-2. During supervised fine-tuning, the LLM aims to predict the next token based on an input that has additional labels. In a sense, the label is the user’s input.

SFT can also be used for other tasks, like classification, but is often used to go from a base generative model to an instruction (or chat) generative model.

# 3. Fine-tuning 2 (preference tuning)

The final step further improves the quality of the model and makes it more aligned with the expected behavior of AI safety or human preferences. This is called preference tuning. Preference tuning is a form of fine-tuning and, as the name implies, aligns the output of the model to our preferences, which are defined by the data that we give it. Like SFT, it can improve upon the original model but has the added benefit of distilling preference of output in its training process. These three steps are illustrated in Figure 12-3 and demonstrate the process of starting from an untrained architecture and ending with a preferencetuned LLM.

![## Image Analysis: 4375c8eda4edede4c6a51d6055b40daf501931d0681f4a9ebd7860985cdf1a0d.jpg

**Conceptual Understanding:**
This image conceptually represents the developmental pipeline for Large Language Models (LLMs), illustrating the evolution from a raw, untrained model to a refined, high-quality version. The main purpose is to demonstrate the sequential training and fine-tuning steps that transform a basic LLM into one capable of following instructions and aligning with human preferences. It conveys the key idea that creating a sophisticated LLM is a multi-stage process involving distinct training methodologies.

**Content Interpretation:**
The image depicts the sequential stages involved in the development and refinement of a Large Language Model (LLM). It illustrates how an LLM evolves from its initial, raw state through various training and fine-tuning processes to achieve a high-quality, 'Preference-tuned' status. The process highlights three main transformation steps: language modeling, supervised fine-tuning, and preference-based fine-tuning. The use of distinct box labels and arrow text clearly defines each stage and the method used to transition between them, emphasizing a structured, multi-stage approach to LLM development. The color change of the 'LLM' text and boxes also visually reinforces the progression and transformation.

**Key Insights:**
The main takeaway from this image is the systematic and multi-stage nature required to develop a high-quality LLM. It teaches that an LLM is not directly 'high-quality' but rather achieves this status through incremental refinements. The specific text elements 'Untrained LLM', 'Language modeling', 'Base LLM', 'Fine-tuning 1 (Supervised)', 'Instruction-tuned LLM', 'Fine-tuning 2 (Preference)', and 'Preference-tuned LLM' provide clear evidence for this. The insight is that the process moves from a foundational understanding ('Base') to instruction-following capabilities ('Instruction-tuned') and finally to alignment with human preferences ('Preference-tuned'), indicating a progression from basic language comprehension to advanced, human-aligned interaction. The image implicitly shows that 'Fine-tuning 2 (Preference)' is the final step for achieving a 'high-quality LLM', building upon prior training stages.

**Document Context:**
This image directly supports the document's section '3. Fine-tuning 2 (preference tuning)' by visually outlining the entire developmental journey of an LLM, specifically placing 'Fine-tuning 2 (Preference)' as the final and crucial step in creating a high-quality LLM. The preceding text and the figure title 'Figure 12-3. The three steps of creating a high-quality LLM.' establish this diagram as a foundational explanation of the overall process, where preference tuning is the culminating phase. It provides a clear visual context for understanding where 'Fine-tuning 2' fits within the broader LLM development pipeline, making the text's discussion of this step more comprehensible.

**Summary:**
The image illustrates the three-step process for developing a high-quality Large Language Model (LLM), moving from an initial untrained state to a preference-tuned state. The process begins with an 'Untrained LLM', which undergoes 'Language modeling' to become a 'Base LLM'. This 'Base LLM' is then subjected to 'Fine-tuning 1 (Supervised)', transforming it into an 'Instruction-tuned LLM'. Finally, the 'Instruction-tuned LLM' undergoes 'Fine-tuning 2 (Preference)', resulting in a 'Preference-tuned LLM'. Each stage represents an increasingly sophisticated and refined version of the LLM, with the final 'Preference-tuned LLM' being the desired high-quality outcome. The visual progression is clear, showing the sequential nature of these critical development stages for LLMs.](images/4375c8eda4edede4c6a51d6055b40daf501931d0681f4a9ebd7860985cdf1a0d.jpg)
Figure 12-3. The three steps of creating a high-quality LLM.

In this chapter, we use a base model that was already trained on massive datasets and explore how we can fine-tune it using both fine-tuning strategies. For each method, we start with the theoretical underpinnings before using them in practice.

# Supervised Fine-Tuning (SFT)

The purpose of pretraining a model on large datasets is that it is able to reproduce language and its meaning. During this process, the model learns to complete input phrases as shown in Figure 12-4.

![## Image Analysis: 2b4e61435943e3c8d0af0c27f97e696ce0727b7c5caab55a6e38d295e6f0f105.jpg

**Conceptual Understanding:**
This image conceptually illustrates the inference process of a Large Language Model (LLM) in its base or pretrained state. The main purpose is to demonstrate how a foundational LLM predicts the subsequent word given an initial text prompt. It conveys the idea that these models are designed to generate contextually relevant continuations for input sequences, essentially predicting 'what comes next' in language.

**Content Interpretation:**
The image depicts the fundamental process of a Large Language Model (LLM) performing next-word prediction. The central component is the 'LLM' itself, explicitly labeled as 'Base (Pretrained)', indicating its state before any potential fine-tuning. It functions as a black box that takes a partial sentence as input and generates the most probable continuation. The input 'The car is' represents the context provided to the LLM, and the output 'Red' is the LLM's predicted completion for that context. This illustrates the LLM's capability to understand and extend linguistic sequences.

**Key Insights:**
The main takeaway from this image is the core function of a base, pretrained Large Language Model: predicting the next word or sequence of words. It highlights that LLMs are trained on vast datasets to learn patterns in language, enabling them to complete sentences or generate continuations. The specific example 'The car is' -> 'Red' demonstrates a simple yet effective instance of this predictive power. The implication is that this foundational capability is the starting point upon which more complex tasks, potentially involving supervised fine-tuning, are built.

**Document Context:**
This image is directly relevant to the 'Supervised Fine-Tuning (SFT)' section of the document. The text accompanying the figure, 'Figure 12-4. A base or pretrained LLM was trained to predict the next word(s).', explicitly states its purpose. The diagram visually supports this explanation by showing a 'Base (Pretrained)' LLM taking an incomplete phrase ('The car is') and predicting a logical next word ('Red'), thus setting the stage for understanding how such a base model operates before SFT might further refine its predictive capabilities.

**Summary:**
The image illustrates a simplified process of how a Base (Pretrained) Large Language Model (LLM) predicts the next word in a sequence. An input text string, 'The car is', is fed into the LLM. The LLM, acting as a predictive model, processes this input and generates 'Red' as the predicted next word. This demonstrates the core functionality of a pretrained LLM in generating coherent text continuations based on its training.](images/2b4e61435943e3c8d0af0c27f97e696ce0727b7c5caab55a6e38d295e6f0f105.jpg)
Figure 12-4. A base or pretrained LLM was trained to predict the next word(s).

This example also illustrates that the model was not trained to follow instructions and instead will attempt to complete a question rather than answer it (Figure 12-5).

![## Image Analysis: f8bf31c641be62f9f24ba539908700f9912611460d7b6f2210e7f6592d275772.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental behavior of a large language model (LLM) in its 'base' or 'pretrained' state. The main purpose conveyed is to highlight that such models, without further fine-tuning, do not inherently understand and follow explicit instructions to answer questions directly. Instead, they operate by attempting to predict the next word or sequence of words based on their extensive training data, often leading to outputs that continue a pattern or generate new, related queries, rather than providing a conclusive answer to the initial prompt. It communicates the key idea that raw, pretrained LLMs lack instruction-following capabilities and require additional training, such as Supervised Fine-Tuning (SFT), to align their outputs with human instructions.

**Content Interpretation:**
The image depicts the interaction with a "Base (Pretrained) LLM". The process involves providing an input (a question) and observing the model's output. The output "2." is a correct numerical answer to "1 + 1", but it is immediately followed by *another question* ("What is 1 + 1 + 1?"), then *another number*, and *the same question again*. This pattern of output demonstrates that the base LLM is not *answering* the question in an instruction-following manner. Instead, it appears to be *predicting a plausible sequence of words/numbers* that might follow such an input, which includes generating new questions or repeating previous outputs, rather than providing a direct and final answer. The speech bubble icon on the LLM box symbolizes its generative nature or conversational capability, and in this context, it emphasizes that the output is "spoken" or generated by the LLM, but not necessarily an instruction-following response. All transcribed text elements, including the input "What is 1 + 1?", the label "Base (Pretrained) LLM", and the generated output sequence "2. What is 1 + 1 + 1? 3. What is 1 + 1 + 1? ...", collectively support this interpretation by showing a clear prompt-response interaction where the response deviates from a direct answer to the initial query.

**Key Insights:**
The main takeaways from this image are: 1. Base (Pretrained) LLMs are not inherently good at following instructions or directly answering questions; their primary function is next-word prediction. 2. When prompted with a question, a base LLM might generate a mix of plausible continuations, including numbers, new questions, or repetitions, rather than a concise answer. 3. The behavior shown suggests the explicit need for further fine-tuning (like SFT) to imbue LLMs with instruction-following capabilities. The transcribed output sequence, "2. What is 1 + 1 + 1? 3. What is 1 + 1 + 1? ...", serves as direct textual evidence for these insights, demonstrating the model's tendency to predict next words and generate new questions rather than adhering to the initial prompt. The label "Base (Pretrained) LLM" establishes the context of the model's state.

**Document Context:**
This image directly supports the "Supervised Fine-Tuning (SFT)" section by visually demonstrating *why* SFT is necessary. It shows the "before SFT" state of an LLM, where it lacks the ability to follow instructions, thus setting the stage for explaining how SFT addresses this limitation. The text immediately following the image, "Figure 12-5. A base LLM will not follow instructions but instead attempts to predict each next word. It may even create new questions.", directly describes and is perfectly illustrated by the behavior shown in this diagram.

**Summary:**
This diagram illustrates the behavior of a "Base (Pretrained) LLM" when presented with a simple instruction or question. The process begins with an input query: "What is 1 + 1?". This query is fed into a rectangular box labeled "Base" with the clarification "(Pretrained)" above it, and "LLM" (Large Language Model) inside it. A small speech bubble icon is present on the top right corner of the LLM box, signifying its generative nature. Instead of providing a direct answer, the LLM generates a sequence of outputs. The first part of the output is "2.", which is numerically correct for "1 + 1". However, this is immediately followed by a new question: "What is 1 + 1 + 1?". The pattern continues with "3." and then again "What is 1 + 1 + 1?", indicated by an ellipsis ("...") suggesting an ongoing, non-terminating, and instruction-ignoring sequence. This flow visually demonstrates that a base, pretrained LLM does not comprehend and follow explicit instructions to answer questions but rather continues to generate text based on learned patterns and next-word prediction, sometimes creating new questions instead of providing a definitive response. This behavior highlights the need for further training, such as Supervised Fine-Tuning (SFT), to enable instruction-following capabilities in LLMs.](images/f8bf31c641be62f9f24ba539908700f9912611460d7b6f2210e7f6592d275772.jpg)
Figure 12-5. A base LLM will not follow instructions but instead attempts to predict each next word. It may even create new questions.

We can use this base model and adapt it to certain use cases, such as following instructions, by fine-tuning it.

# Full Fine-Tuning

The most common fine-tuning process is full fine-tuning. Like pretraining an LLM, this process involves updating all parameters of a model to be in line with your target task. The main difference is that we now use a smaller but labeled dataset whereas the pretraining process was done on a large dataset without any labels (Figure 12-6).

![## Image Analysis: bd7ad0f59343345438f13e2ddcdc1bbac97193ac4c25a2f2842fc1d265638281.jpg

**Conceptual Understanding:**
This image conceptually represents the standard workflow for training and fine-tuning Large Language Models (LLMs). Its main purpose is to illustrate the transition of an LLM from an initial raw state to a highly specialized state, highlighting the two primary phases: pre-training (language modeling) and fine-tuning. It communicates the key idea that different types of data (unlabeled vs. labeled) and learning paradigms (self-supervised vs. supervised) are employed at each stage to develop and refine an LLM's capabilities.

**Content Interpretation:**
The image depicts the lifecycle stages of a Large Language Model (LLM) and the associated training methodologies. It shows how an LLM evolves from an initial 'Untrained' state, through a 'Base' pre-trained state, to a 'Fine-tuned' state. This evolution is driven by two main processes: 'Language modeling' (pre-training) and 'Full fine-tuning' (adaptation). The significance lies in differentiating the data types and learning paradigms used at each major step: unlabeled data for self-supervised pre-training and labeled data for supervised fine-tuning. The changing color of the 'LLM' boxes (light gray to light pink to darker pink) visually reinforces the progression and refinement of the model.

**Key Insights:**
The main takeaway from this image is the sequential, multi-stage process involved in developing sophisticated Large Language Models. It highlights that an LLM typically starts 'Untrained', then becomes a 'Base LLM' through 'Language modeling' using 'Unlabeled data (Self-supervised)', which is a form of pre-training. Subsequently, this 'Base LLM' is further specialized into a 'Fine-tuned LLM' through 'Full fine-tuning' using 'Labeled data (Supervised)'. This clearly demonstrates the transition from unsupervised learning on general data to supervised learning on specific, task-oriented data to achieve a fully specialized model. The distinction between 'Unlabeled data (Self-supervised)' and 'Labeled data (Supervised)' is crucial for understanding the methodologies employed in pre-training versus fine-tuning.

**Document Context:**
This image directly supports the document's section on 'Full Fine-Tuning' by visually explaining the process. It clarifies how 'full fine-tuning' fits into the broader LLM development pipeline, distinguishing it from the initial 'language modeling' (pre-training) phase. The image reinforces the textual statement 'Compared to language modeling (pretraining), full fine-tuning uses a smaller but labeled dataset' by explicitly showing 'Unlabeled data (Self-supervised)' for 'Language modeling' and 'Labeled data (Supervised)' for 'Full fine-tuning', thereby illustrating the key difference in data types and supervision paradigms.

**Summary:**
The image illustrates the progression of a Large Language Model (LLM) from an untrained state to a fine-tuned state through two distinct processes: Language Modeling and Full Fine-Tuning. Starting with an 'Untrained LLM' (represented by a light gray box with 'LLM' in pink text), the model undergoes 'Language modeling'. This process is powered by 'Unlabeled data (Self-supervised)', depicted as a stack of document icons. The output of language modeling is a 'Base LLM' (represented by a light pink box with 'LLM' in pink text). Subsequently, this Base LLM undergoes 'Full fine-tuning'. This stage utilizes 'Labeled data (Supervised)', shown as a stack of document icons with a purple and green top layer. The result of full fine-tuning is a 'Fine-tuned LLM' (represented by a darker pink box with 'LLM' in pink text). Each LLM box also contains a small page/document icon in its top-right corner.](images/bd7ad0f59343345438f13e2ddcdc1bbac97193ac4c25a2f2842fc1d265638281.jpg)
Figure 12-6. Compared to language modeling (pretraining), full fine-tuning uses a smaller but labeled dataset.

You can use any labeled data for full fine-tuning, making it also a great technique for learning domain-specific representations. To make our LLM follow instructions, we will need question-response data. This data, as shown in Figure 12-7, is queries by the user with corresponding answers.

![## Image Analysis: 3b8922650b7d09dd17ec684174b08f83a1a6a178d441d864d6f72607601ad977.jpg

**Conceptual Understanding:**
This image conceptually represents 'Instruction data' used in the context of training or fine-tuning large language models. Its main purpose is to illustrate, through concrete examples, how instructions from a user are paired with corresponding answers or outputs to form the dataset that enables a language model to perform diverse tasks. It communicates the key idea that instruction data is structured as explicit requests (instructions) and the expected correct or desired responses (outputs), potentially with additional input context, for various natural language processing tasks.

**Content Interpretation:**
The image illustrates the concept of 'Instruction data' used in the context of large language models (LLMs) by providing concrete examples for different tasks. Each example demonstrates a specific type of instruction-output pair that an LLM would process or be trained on. The first example showcases a 'Question answering' task where an instruction, "What are large language models?", is paired with a factual, explanatory output detailing what LLMs are. The second example presents a 'Sentiment analysis' task, where an instruction, "Rate this review", is provided along with an input review, "This was a horrible place to eat!", and the expected output, "This is a negative review." The blue curved arrows in both outputs highlight specific keywords ("word" and "negative"), which might imply their significance in the generation or classification process, or perhaps as points of interest in a finer-grained analysis of the output. The presence of 'Input' in the sentiment analysis example, in addition to 'Instruction', suggests that some tasks require an explicit piece of content to be processed alongside the instruction.

**Key Insights:**
The main takeaway from this image is that 'Instruction data' for large language models consists of explicit user instructions paired with desired outputs or answers, enabling the models to learn various tasks. The image demonstrates that this data can encompass a wide range of tasks, as indicated by "(Many tasks)" in the title and the distinct examples of "Question answering" and "Sentiment analysis." It also shows that the format of the instruction data can vary slightly depending on the task; for instance, sentiment analysis requires an additional "Input" text besides the general "Instruction." The highlighted words "word" and "negative" in the outputs suggest that fine-tuning might involve specific attention to or generation of key terms relevant to the instruction. This reinforces the idea that fine-tuning involves teaching the model to produce specific, task-appropriate responses to given instructions and inputs.

**Document Context:**
This image serves as a clear visual explanation for the 'Full Fine-Tuning' section mentioned in the document context. It concretely demonstrates what 'instruction data' looks like, which is crucial for understanding how large language models are fine-tuned to perform various tasks. The examples provided directly illustrate the concept of "instruction data with instructions by a user and corresponding answers" and how "The instructions can contain many different tasks," as stated in the text after the image (Figure 12-7). By showing specific instruction-output pairs for different tasks like question answering and sentiment analysis, the image clarifies the nature of the data used in fine-tuning processes, making the abstract concept of 'instruction data' tangible for the reader.

**Summary:**
The image titled "Instruction data (Many tasks)" presents two distinct examples of instruction data used for training or evaluating large language models across different tasks. The first example, shown in green, demonstrates a "Question answering" task. It begins with an "Instruction: "What are large language models?"" followed by an "Output: "Large language models (LLMs) are models that can generate human-like text by predicting the probability of a word given the previous words used in a sentence."" A blue curved arrow points from the word "word" at the end of the output sentence. The second example, shown in purple, illustrates a "Sentiment analysis" task. This task starts with an "Instruction: "Rate this review"", followed by an "Input: "This was a horrible place to eat!"", and concludes with an "Output: "This is a negative review."" A blue curved arrow points from the word "negative" at the end of this output sentence. Both examples collectively show how varied instructions and their corresponding expected outputs (or inputs) form the instruction data for multiple tasks.](images/3b8922650b7d09dd17ec684174b08f83a1a6a178d441d864d6f72607601ad977.jpg)
Figure 12-7. Instruction data with instructions by a user and corresponding answers. The instructions can contain many different tasks.

During full fine-tuning, the model takes the input (instructions) and applies nexttoken prediction on the output (response). In turn, instead of generating new ques‐ tions, it will follow instructions.

# Parameter-Efficient Fine-Tuning (PEFT)

Updating all parameters of a model has a large potential of increasing its performance but comes with several disadvantages. It is costly to train, has slow training times, and requires significant storage. To resolve these issues, attention has been given to parameter-efficient fine-tuning (PEFT) alternatives that focus on fine-tuning pre‐ trained models at higher computational efficiency.

# Adapters

Adapters are a core component of many PEFT-based techniques. The method pro‐ poses a set of additional modular components inside the Transformer that can be fine-tuned to improve the model’s performance on a specific task without having to fine-tune all the model weights. This saves a lot of time and compute.

Adapters are described in the paper “Parameter-efficient transfer learning for NLP”, which showed that fine-tuning $3 . 6 \%$ of the parameters of BERT for a task can yield comparable performance to fine-tuning all the model’s weights.1 On the GLUE benchmark, the authors show they reach within $0 . 4 \%$ of the performance of full fine-tuning. In a single Transformer block, the paper’s proposed architecture places adapters after the attention layer and the feedforward neural network as illustrated in Figure 12-8.

![## Image Analysis: 0f7de0dc9efbf06122ddf4ec2a4a6a140deea5eb952c394876d3c6c7c01f2b8e.jpg

**Conceptual Understanding:**
The image conceptually represents a single 'Transformer block', which is a fundamental building block in transformer-based neural network architectures. Its main purpose is to illustrate the precise architectural location of 'Adapter' components within this block. The diagram shows that adapters are inserted sequentially, specifically after the 'Self-attention' mechanism and after the 'Feedforward neural network', indicating a modular approach to modifying or extending the functionality of a transformer without altering its core structure significantly. The key idea communicated is the integration points for adapter layers within a typical transformer block's computational flow.

**Content Interpretation:**
The image illustrates the architectural composition of a 'Transformer block' within a neural network. It specifically highlights the placement and integration of 'Adapter' components. The sequence of processes shown includes a 'Self-attention' mechanism, followed by an 'Adapter (component 1)', then a 'Feedforward neural network', and finally another 'Adapter (component 2)'. This diagram demonstrates a modular approach to modifying or extending the capabilities of a standard transformer block by inserting adapter layers at key points in the computational flow. The varying colors of the internal components (pale orange for Self-attention, green with orange border for Adapter component 1, light blue for Feedforward neural network, and green with blue border for Adapter component 2) visually distinguish these distinct modules.

**Key Insights:**
The main takeaway from this image is the specific, strategic placement of 'Adapter' components within a 'Transformer block' architecture. Two adapter components are explicitly shown: 'Adapter (component 1)' is positioned immediately after the 'Self-attention' layer, and 'Adapter (component 2)' is placed directly after the 'Feedforward neural network'. This indicates that adapters are designed to augment or modify the outputs of core transformer sub-layers (self-attention and feedforward networks) in a modular fashion. The sequential flow 'Self-attention' -> 'Adapter (component 1)' -> 'Feedforward neural network' -> 'Adapter (component 2)' within the encompassing 'Transformer block' confirms this integrated design. This detailed placement suggests that adapters are not replacing core components but rather enhancing them, allowing for targeted modifications or parameter-efficient fine-tuning without altering the larger, frozen model weights, as implied by the surrounding document context.

**Document Context:**
This image directly supports the document's section on 'Adapters' by visually demonstrating their integration within a 'Transformer block'. The text immediately following the figure (Figure 12-8) explains that 'Adapters add a small number of weights in certain places in the network that can be fine-tuned efficiently while leaving the majority of model weights frozen.' This image provides the precise 'places' where these adapters are inserted, showing that they are placed after the self-attention layer and after the feedforward neural network within a transformer block. It enhances comprehension by providing a concrete visual representation of the abstract concept of 'adapters' and their role in network architecture for efficient fine-tuning.

**Summary:**
The image depicts the internal structure of a 'Transformer block', illustrating the sequential arrangement of its core components and the strategic insertion of 'Adapter' components. An incoming connection leads into the 'Transformer block'. Within this block, the process begins with 'Self-attention'. Following 'Self-attention' is the first 'Adapter (component 1)'. After this, the data flows into a 'Feedforward neural network'. Finally, a second 'Adapter (component 2)' is placed after the 'Feedforward neural network', from which an outgoing arrow indicates the end of the block's processing for that stage. The entire sequence is enclosed within a larger light blue rectangular block labeled 'Transformer block', with each internal component represented by a rounded rectangular box.](images/0f7de0dc9efbf06122ddf4ec2a4a6a140deea5eb952c394876d3c6c7c01f2b8e.jpg)
Figure 12-8. Adapters add a small number of weights in certain places in the network that can be fine-tuned efficiently while leaving the majority of model weights frozen.

It’s not enough to only alter one Transformer block, however, so these components are part of every block in the model, as Figure 12-9 shows.

![## Image Analysis: ed4def64e89b6130f89272f4e44b33a89f03cb8b7509a05ef1646272bd911702.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a Transformer model, specifically illustrating the integration points of 'Adapter' components. Its main purpose is to demonstrate the modular insertion of these adapter layers within the standard 'Self-attention' and 'Feedforward neural network' structures of individual 'Transformer blocks'. The image visually clarifies how adapters are distributed across a 'Stack of Transformer blocks', indicating that they are not a monolithic addition but rather distinct, potentially specialized, components positioned at strategic points throughout the model's depth.

**Content Interpretation:**
The image displays the internal structure of a Transformer model, focusing on how 'Adapter' components are integrated. It illustrates that a 'Transformer' is composed of a 'Stack of Transformer blocks'. Each 'Transformer block' (e.g., 'Transformer block 1' and 'Transformer block 2') contains a sequence of operations: 'Self-attention', followed by an 'Adapter' component, then a 'Feedforward neural network', and finally another 'Adapter' component. This shows a modular and repeatable insertion of adapter layers within the standard Transformer architecture. The numbering of adapters ('component 1', 'component 2', 'component 3', 'component 4') indicates that they are distinct instances or potentially different types of adapters.

**Key Insights:**
The main takeaway from this image is that 'Adapter' components are modularly integrated into the core architecture of Transformer blocks, specifically placed after the 'Self-attention' layer and after the 'Feedforward neural network' layer within each block. This is evidenced by the repeated sequence: 'Self-attention' -> 'Adapter (component X)' -> 'Feedforward neural network' -> 'Adapter (component Y)'. The image also conveys that these adapter components are not singular but appear as distinct instances (e.g., 'component 1', 'component 2', 'component 3', 'component 4'), suggesting the possibility of varied or stacked adapter implementations across the Transformer's depth.

**Document Context:**
This image directly supports the document's 'Adapters' section by visually demonstrating how 'Adapter components span the various Transformer blocks in the model', as stated in the text after the image. It provides a clear, architectural diagram that makes the abstract concept of adapter integration concrete, illustrating their specific placement within the self-attention and feedforward neural network layers of each Transformer block. The image serves as a foundational visual explanation for understanding the role and location of adapters in deep learning models like Transformers.

**Summary:**
The image illustrates the internal architecture of a Transformer model, specifically highlighting the integration of 'Adapter' components within its 'Stack of Transformer blocks'. The overall structure is enclosed within a larger pink box labeled 'Transformer' at the top left. Inside this, a 'Stack of Transformer blocks' is represented by a blue bordered box. This stack contains two main sequential units: 'Transformer block 1' and 'Transformer block 2'.

Starting with 'Transformer block 1', the data flow progresses through the following layers:
1. 'Self-attention'
2. 'Adapter (component 1)'
3. 'Feedforward neural network'
4. 'Adapter (component 2)'

Following 'Transformer block 1', the process continues into 'Transformer block 2', which also comprises a similar sequence of layers:
1. 'Self-attention'
2. 'Adapter (component 3)'
3. 'Feedforward neural network'
4. 'Adapter (component 4)'

A vertical line runs through the center of the 'Stack of Transformer blocks', indicating a sequential data flow from top to bottom. A downward-pointing arrow at the very bottom suggests the continuation of this processing flow beyond the depicted blocks. The diagram visually clarifies that adapter components are inserted at multiple distinct points within and between the core computational layers (self-attention and feedforward neural network) of each Transformer block, and these components are themselves numbered to indicate distinct instances or types (component 1, 2, 3, 4).](images/ed4def64e89b6130f89272f4e44b33a89f03cb8b7509a05ef1646272bd911702.jpg)
Figure 12-9. Adapter components span the various Transformer blocks in the model.

Seeing all the adapter’s components across the model like this enables us to see indi‐ vidual adapters as shown in Figure 12-10, which is a collection of these components spanning all the blocks of the model. Adapter 1 can be a specialist in, say, medical text classification, while Adapter 2 can specialize in named-entity recognition (NER). You can download specialized adapters from https://oreil.ly/XraXg.

![## Image Analysis: 52683c8739c8721a0173a0b98e333ac4cadf070c001522bc61459b3635a3aafe.jpg

**Conceptual Understanding:**
The image conceptually represents the architecture of a Transformer model enhanced with a modular component called 'adapters'. Its main purpose is to illustrate how these 'adapters' are integrated into the Transformer's layered structure and, crucially, how different sets of adapters can be interchanged. The image conveys the key idea of achieving task-specific specialization in large models by swapping in small, efficient 'adapter' modules, thus maintaining a common core model while allowing for diverse applications or fine-tuning without extensive retraining of the entire model. This highlights the principles of modularity and efficiency in neural network design.

**Content Interpretation:**
The image illustrates the architectural design of a Transformer model augmented with 'adapter' components. The core 'Stack of Transformer blocks' consists of alternating 'Self-attention' and 'Feedforward neural network' layers. Crucially, 'Adapter (component X)' modules are integrated between these core layers. For example, 'Adapter (component 1)' is placed after 'Self-attention' and before 'Feedforward neural network' in the first block, and 'Adapter (component 2)' is placed after 'Feedforward neural network'. This pattern repeats for subsequent blocks with 'Adapter (component 3)' and 'Adapter (component 4)'. The presence of 'Adapter 1' and 'Adapter 2' alongside the main Transformer architecture, each containing a corresponding set of four adapter components, signifies the modular and interchangeable nature of these adapters. The consistent naming 'Adapter (comp. X)' across the main Transformer stack and the individual adapter sets further emphasizes that these separate adapter modules can be swapped into the Transformer's architecture. The vertical flow indicated by the arrow suggests sequential processing through these layers.

**Key Insights:**
The main takeaway from this image is the concept of modularity and reconfigurability in Transformer models through the use of adapters. The image teaches that: 1.  Adapters are integrated within the core 'Stack of Transformer blocks', specifically alongside 'Self-attention' and 'Feedforward neural network' layers, as shown by 'Adapter (component 1)' through 'Adapter (component 4)'. 2.  Different sets of adapters, such as 'Adapter 1' and 'Adapter 2', can be prepared independently. Each set contains components that directly correspond to the integration points within the Transformer (e.g., 'Adapter (comp. 1)' in 'Adapter 1' corresponds to 'Adapter (component 1)' in the stack). 3.  These adapter sets are designed to be swappable, allowing the base Transformer architecture to be reused while its functionality is specialized for different tasks by simply changing the inserted adapters. This is evidenced by the distinct 'Adapter 1' and 'Adapter 2' modules, each capable of fitting into the 'Adapter (component)' slots of the main Transformer, as suggested by the associated document text.

**Document Context:**
This image directly supports the document's section on 'Adapters' and the accompanying text: 'Figure 12-10. Adapters that specialize in specific tasks can be swapped into the same architecture (if they share the same original model architecture and weights).' It visually explains how adapters are integrated into a Transformer model and how different sets of adapters can be interchanged to specialize the model for various tasks without altering the fundamental Transformer blocks. The visual breakdown into 'Adapter (component X)' within the main stack and 'Adapter 1'/'Adapter 2' as independent, swappable units provides a clear illustration of this concept, reinforcing the idea of modularity and task-specific customization.

**Summary:**
The image illustrates the modular architecture of a Transformer model, specifically highlighting the integration and interchangeability of 'adapters'. On the left, a 'Transformer' model is depicted as a 'Stack of Transformer blocks'. Each block conceptually contains a 'Self-attention' mechanism and a 'Feedforward neural network', with 'Adapters' strategically placed within this structure. Specifically, 'Adapter (component 1)' and 'Adapter (component 2)' are shown within the first conceptual Transformer block, while 'Adapter (component 3)' and 'Adapter (component 4)' are shown within a subsequent block. This vertical arrangement with a downward arrow signifies a sequential data flow through these layers. To the right of this main Transformer architecture, two distinct sets of adapters are presented as 'Adapter 1' and 'Adapter 2'. Each of these adapter sets contains four components: 'Adapter (comp. 1)', 'Adapter (comp. 2)', 'Adapter (comp. 3)', and 'Adapter (comp. 4)'. The visual representation clearly implies that these sets of adapters ('Adapter 1' or 'Adapter 2') can be swapped into the positions occupied by the 'Adapter (component X)' within the main 'Stack of Transformer blocks'. This design allows for different specialized tasks to be performed by the Transformer model by simply exchanging the adapter components while keeping the core Transformer blocks (Self-attention and Feedforward neural network) constant.](images/52683c8739c8721a0173a0b98e333ac4cadf070c001522bc61459b3635a3aafe.jpg)
Figure 12-10. Adapters that specialize in specific tasks can be swapped into the same architecture (if they share the same original model architecture and weights).

The paper “AdapterHub: A framework for adapting transformers” introduced the Adapter Hub as a central repository for sharing adapters.2 A lot of these earlier adapt‐ ers were more focused on BERT architectures. More recently, the concept has been applied to text generation Transformers in papers like “LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention”.3

# Low-Rank Adaptation (LoRA)

As an alternative to adapters, low-rank adaptation (LoRA) was introduced and is at the time of writing is a widely used and effective technique for PEFT. LoRA is a technique that (like adapters) only requires updating a small set of parameters. As

![## Image Analysis: 0e8d6d754d32a32f16c7cee311436c96fd2676807caaf2ed2ceb29a2e6b24c9b.jpg

**Conceptual Understanding:**
The image conceptually represents two contrasting approaches to adapting pre-trained Large Language Models (LLMs) to specific tasks or datasets. Its main purpose is to visually illustrate the difference between a traditional "Full fine-tuning" method, which modifies the entire model, and the "Low-Rank Adaptation (LoRA)" method, which achieves similar adaptation by only fine-tuning a small subset of the model and adding a new, small set of parameters. The image conveys the key idea that LoRA is a more parameter-efficient way to fine-tune LLMs.

**Content Interpretation:**
The image illustrates two distinct processes for adapting a Large Language Model (LLM): traditional full fine-tuning and Low-Rank Adaptation (LoRA). The top path depicts full fine-tuning, where the entire base model is updated. The bottom path demonstrates LoRA, where a subset of the base model is fine-tuned, and then a small set of new parameters is added to the original LLM to achieve the fine-tuned state. This highlights LoRA's efficiency by not requiring the modification of the entire base model's parameters.

*   **Processes Shown:**
    *   **Full Fine-tuning:** Represented by the top arrow, labeled "Update entire model (Full fine-tuning)". This process takes a "Base LLM" and transforms it into a "Fine-tuned LLM" by adjusting all its parameters.
    *   **Low-Rank Adaptation (LoRA):** Represented by the bottom flow. It involves taking a "Subset of base model" (an unlabeled light pink box) and performing a "Fine-tune (LoRA)" operation on it, resulting in an unlabeled dark pink box. This result is then used to "Add small set of new parameters" to the base "LLM", forming a fine-tuned LLM structure composed of the original LLM and the newly added small set of parameters.

*   **Concepts Shown:**
    *   **Base LLM:** The original, untrained or pre-trained Large Language Model.
    *   **Fine-tuned LLM:** The adapted or specialized Large Language Model after training.
    *   **Subset of base model:** A specific, smaller portion or component of the Base LLM that is targeted for adaptation in LoRA.
    *   **New parameters:** The additional, low-rank parameters introduced by LoRA that are responsible for the adaptation, rather than modifying the original LLM's vast parameter space.

*   **Relationships:**
    *   **Direct Modification (Full Fine-tuning):** The Base LLM is directly and entirely transformed into the Fine-tuned LLM.
    *   **Additive Modification (LoRA):** The Base LLM remains largely intact, and its functionality is adapted by adding new, external parameters derived from fine-tuning a subset of its components.

*   **Significance:** The primary significance is showcasing LoRA as a more efficient alternative to full fine-tuning. Full fine-tuning involves updating the "entire model", which is computationally expensive and parameter-heavy. In contrast, LoRA only requires fine-tuning a "subset of base model" and then adding a "small set of new parameters", significantly reducing the computational load and storage requirements for adapted models. The different shades of pink for the LLM boxes, from light to dark, visually suggest the transformation or updated state.

**Key Insights:**
The image provides clear insights into the two major strategies for adapting Large Language Models, with a focus on the advantages of LoRA:

*   **Traditional Fine-tuning (Full Fine-tuning):** This method involves updating the "entire model" of a Base LLM. This implies a comprehensive, and often computationally intensive, modification of all the model's parameters to adapt it to a new task or dataset. The evidence for this is the arrow labeled "Update entire model (Full fine-tuning)" directly connecting a "Base LLM" to a "Fine-tuned LLM".
*   **Low-Rank Adaptation (LoRA):** This method is a more parameter-efficient alternative. Instead of updating the entire model, LoRA focuses on a "Subset of base model", fine-tunes this subset (as indicated by "Fine-tune (LoRA)"), and then combines the original "LLM" with a "small set of new parameters" resulting from this process. This significantly reduces the number of parameters that need to be trained and stored for each fine-tuned version. The text "Add small set of new parameters" explicitly highlights this core efficiency gain.
*   **Efficiency and Resource Management:** The primary takeaway is that LoRA offers a method to fine-tune LLMs that is significantly more resource-efficient than full fine-tuning, as it only deals with a "small set of new parameters" rather than the immense parameter count of an "entire model". This allows for more manageable storage and faster deployment of customized LLMs.

**Document Context:**
This image directly supports the document's section on "Low-Rank Adaptation (LoRA)" by visually explaining how LoRA works in contrast to traditional full fine-tuning. The surrounding text, "illustrated in Figure 12-11, it creates a small subset of the base model to fine-tune instead of adding layers to the model.4 Figure 12-11. LoRA requires only fine-tuning a small set of parameters that can be kept separately from the base LLM," is perfectly aligned with the visual representation. The diagram serves to clarify the statement that LoRA fine-tunes a small subset and adds new parameters, providing a clear visual distinction from updating the entire model. It helps the reader understand the fundamental difference in approach and the efficiency benefits of LoRA as discussed in the text.

**Summary:**
The image presents a comparison between two methods of fine-tuning a Large Language Model (LLM): traditional "Full fine-tuning" and the more parameter-efficient "Low-Rank Adaptation (LoRA)". The diagram is divided into two conceptual areas: "Base" and "Fine-tuned".

In the traditional "Full fine-tuning" method, a "Base LLM" is directly updated in its entirety. This process is labeled by the arrow as "Update entire model (Full fine-tuning)", resulting in a "Fine-tuned LLM". Both the Base LLM and the resulting Fine-tuned LLM are depicted as pink rectangles with a speech bubble icon in the top right corner, with the Fine-tuned LLM being a darker shade of pink.

The LoRA method is shown below this, originating from a "Subset of base model", indicated by a dotted line connecting from the "Base LLM". Within a grey rounded rectangle, an unlabeled light pink box (representing a component of the subset) undergoes a process labeled "Fine-tune (LoRA)". This process leads to an unlabeled darker pink box, which represents the fine-tuned subset. An arrow originating from this darker pink box is labeled "Add small set of new parameters". This arrow points towards the final state of the LoRA-tuned model, which consists of a light pink "LLM" box (also with a speech bubble icon) augmented by a separate, small, dark pink square representing the added new parameters. This combined representation is categorized under the "Fine-tuned" section, alongside the full fine-tuned LLM.

In essence, the image visually contrasts the comprehensive modification of the entire model in full fine-tuning with LoRA's approach of fine-tuning only a subset and then adding a small, distinct set of new parameters to achieve a fine-tuned LLM.](images/0e8d6d754d32a32f16c7cee311436c96fd2676807caaf2ed2ceb29a2e6b24c9b.jpg)
illustrated in Figure 12-11, it creates a small subset of the base model to fine-tune instead of adding layers to the model.4   
Figure 12-11. LoRA requires only fine-tuning a small set of parameters that can be kept separately from the base LLM.

Like adapters, this subset allows for much quicker fine-tuning since we only need to update a small part of the base model. We create this subset of parameters by approximating large matrices that accompany the original LLM with smaller matri‐ ces. We can then use those smaller matrices as a replacement and fine-tune them instead of the original large matrices. Take for example the $1 0 \times 1 0$ matrix we see in Figure 12-12.

![## Image Analysis: 65120786edb2e73ff6f385c2f0b7fee2c2e8b46b060e3a2fc890c659392f7370.jpg

**Conceptual Understanding:**
The image conceptually represents a 'Weight matrix', a fundamental component in machine learning models like neural networks. Its main purpose is to visually and textually define a simple matrix, illustrating its dimensions (10x10), rank, and the direct correlation to its 'Total parameters: 100'. The image communicates the key idea that such matrices are composed of individual parameters, setting a tangible foundation for understanding the complexity and scale of parameters in larger models, particularly in the context of large language models (LLMs).

**Content Interpretation:**
The image depicts a foundational concept in machine learning and neural networks: a 'Weight matrix'. It visually represents a 10x10 matrix composed of individual cells, each corresponding to a parameter. The associated text, 'Full rank (10 × 10)', explicitly defines its dimensions and rank, while 'Total parameters: 100' quantifies the number of learnable values within this specific matrix. The grid structure with its distinct cells emphasizes the discrete nature of these parameters. This image is a simplified, concrete example of the 'massive weight matrices' that constitute a significant bottleneck in large language models, providing a visual basis for understanding the scale of parameters involved.

**Key Insights:**
The main takeaway from this image is a clear, simplified illustration of what a 'Weight matrix' entails in terms of its structure, rank, and total number of parameters. Specifically, it teaches that a matrix of 'Full rank (10 × 10)' will have 'Total parameters: 100', where each cell in the 10x10 grid represents one parameter. This directly conveys how matrix dimensions determine the parameter count. This understanding is crucial for grasping the 'massive weight matrices' bottleneck in LLMs and appreciating the motivation behind parameter-efficient fine-tuning techniques like LoRA, which aim to reduce the number of trainable parameters.

**Document Context:**
This image is highly relevant to the 'Low-Rank Adaptation (LoRA)' section and the accompanying text, 'A major bottleneck of LLMs is their massive weight matrices. Only one of these may have 150 million parameters and each Transformer block would have its version of these.' It serves as a fundamental visual example of a 'Weight matrix', showing how its dimensions (10 × 10) directly translate to the 'Total parameters: 100'. By illustrating a smaller, manageable matrix, it helps the reader grasp the concept before transitioning to the immense scale of matrices (e.g., 150 million parameters) found in large language models (LLMs). This visual explanation sets the stage for understanding why techniques like LoRA are necessary to address the computational challenges posed by such massive matrices.

**Summary:**
The image displays a 10x10 grid of light blue squares, bordered by darker blue lines, representing a matrix. To the left of this grid, text labels it as a 'Weight matrix', specifying 'Full rank (10 × 10)' and 'Total parameters: 100'. This visually demonstrates a simple weight matrix with its dimensions and the corresponding number of parameters, making the concept of matrix parameters clear and accessible.](images/65120786edb2e73ff6f385c2f0b7fee2c2e8b46b060e3a2fc890c659392f7370.jpg)
Figure 12-12. A major bottleneck of LLMs is their massive weight matrices. Only one of these may have 150 million parameters and each Transformer block would have its version of these.

We can come up with two smaller matrices, which when multiplied, reconstruct a 10 $\times 1 0$ matrix. This is a major efficiency win because instead of using 100 weights (10 times 10) we now only have 20 weights (10 plus 10), as we can see in Figure 12-13.

![## Image Analysis: e23527b1df751e2ae4477ad5a929b41f31234fe4749a542144b5a19405e144b2.jpg

**Conceptual Understanding:**
This image conceptually represents the mechanics of low-rank matrix decomposition as applied in techniques like Low-Rank Adaptation (LoRA). Its main purpose is to visually illustrate how the 'rank' chosen for the decomposition directly correlates with the 'total parameters' required for that approximation. It aims to convey that increasing the rank increases the model's capacity (by allowing more parameters), while still maintaining a parameter-efficient representation compared to a full-rank matrix. The key idea being communicated is the trade-off between the expressive power (higher rank, more parameters) and computational efficiency (fewer parameters overall due to decomposition) in low-rank approximations.

**Content Interpretation:**
This image illustrates the Low-Rank Adaptation (LoRA) technique's core principle by showing the decomposition of a hypothetical weight matrix into two smaller matrices for different ranks. The two main concepts demonstrated are: 1. **Low-Rank Decomposition:** A larger matrix is represented by two smaller matrices (e.g., A and B). 2. **Parameter Count vs. Rank:** As the 'rank' (R) of the decomposition increases, the total number of parameters required for the approximation also increases. 

Specifically, the image shows: 
- For 'Low-rank weight matrix (rank = 1)': It decomposes into a vertical matrix (conceptually 10 rows x 1 column) and a horizontal matrix (conceptually 1 row x 10 columns). Each of these components has 10 parameters (represented by 10 blue blocks each), leading to a 'Total parameters: 20'. The label 'Rank = 1' below the vertical matrix further clarifies its rank dimension. 
- For 'Low-rank weight matrix (rank = 2)': It decomposes into a vertical matrix (conceptually 10 rows x 2 columns) and a horizontal matrix (conceptually 2 rows x 10 columns). The vertical matrix would have 10x2 = 20 parameters (visually indicated by 10 blocks high with a 'Rank = 2' label indicating its width/rank). The horizontal matrix would have 2x10 = 20 parameters (visually shown as two rows of 10 blocks each, totaling 20 blocks). This results in a 'Total parameters: 40'. The label 'Rank = 2' below the vertical matrix clarifies its rank dimension. 

The significance is that increasing the rank allows for a more expressive low-rank approximation, but at the cost of more parameters. The image uses visual blocks to intuitively represent the number of elements (parameters) in these decomposed matrices, thereby explaining how the parameter count scales with the chosen rank.

**Key Insights:**
The main takeaways from this image are: 
1. **Low-rank adaptation reduces parameters:** By decomposing a large weight matrix (implicitly represented) into two smaller matrices (visualized by blue blocks), the total number of parameters required for the approximation is significantly less than a full matrix. 
2. **Rank directly impacts parameter count:** Increasing the rank of the decomposition (from 1 to 2) directly increases the total number of parameters (from 20 to 40). This is evident from the labels 'rank = 1' and 'Total parameters: 20' compared to 'rank = 2' and 'Total parameters: 40'. 
3. **Visual representation of matrix decomposition:** The image provides a simplified but effective visual model for understanding how matrices are broken down, with vertical stacks representing one part of the decomposition and horizontal stacks representing another. The specific labels 'Rank = 1' and 'Rank = 2' alongside the horizontal braces under the vertical matrices explicitly define the rank dimension being depicted. 

These insights are crucial for understanding the computational efficiency benefits of LoRA in machine learning, particularly for fine-tuning large models.

**Document Context:**
The image is situated in the 'Low-Rank Adaptation (LoRA)' section of the document and directly supports the understanding of how LoRA works by demonstrating the parameter efficiency of low-rank decomposition. The text immediately following the image, 'Figure 12-13. Decomposing a large weight matrix into two smaller matrices leads to a compressed, low-rank version of the matrix that can be fine-tuned more efficiently,' perfectly aligns with the visual explanation. The image visually clarifies the concept of 'decomposing a large weight matrix into two smaller matrices' and illustrates how varying the 'rank' affects the 'compressed' nature by showing the 'total parameters' for rank 1 and rank 2, directly relating to 'fine-tuned more efficiently' by managing the number of trainable parameters.

**Summary:**
The image visually demonstrates the concept of low-rank decomposition of a weight matrix, specifically comparing a rank-1 and a rank-2 decomposition in terms of their total parameters. It shows how increasing the rank from 1 to 2 increases the number of parameters needed for the low-rank approximation. On the left side, a low-rank weight matrix with rank = 1 is depicted. It is composed of a vertical column of 10 blue square blocks and a horizontal row of 10 blue square blocks. The total number of parameters for this rank-1 decomposition is explicitly stated as 20. A horizontal bracket below the vertical column is labeled 'Rank = 1', indicating the rank of the first decomposed matrix. On the right side, a low-rank weight matrix with rank = 2 is presented. This is shown as a vertical column of 10 blue square blocks, with a horizontal bracket underneath labeled 'Rank = 2', signifying that this represents a matrix with 2 columns (a 10x2 matrix). Next to it, two horizontal rows of 10 blue square blocks each are depicted, totaling 20 blocks, representing a 2x10 matrix. The total parameters for this rank-2 decomposition are explicitly stated as 40. The image clearly illustrates that a higher rank in decomposition (from 1 to 2) requires more parameters (from 20 to 40) while still being a compressed representation compared to a full-rank matrix. The dotted line vertically separates the rank-1 and rank-2 examples.](images/e23527b1df751e2ae4477ad5a929b41f31234fe4749a542144b5a19405e144b2.jpg)
Figure 12-13. Decomposing a large weight matrix into two smaller matrices leads to a compressed, low-rank version of the matrix that can be fine-tuned more efficiently.

During training, we only need to update these smaller matrices instead of the full weight changes. The updated change matrices (smaller matrices) are then combined with the full (frozen) weights as illustrated in Figure 12-14.

![## Image Analysis: b9f6eb221d95441d417d6282b6f516bbdeddf3625f82551b7b7810f52911c190.jpg

**Conceptual Understanding:**
This image conceptually represents and contrasts two methods of adapting or fine-tuning machine learning models: 'Full fine-tuning' and 'Low-rank adaptation (LoRA)'. The main purpose is to illustrate the distinct approaches these methods take in updating model weights during training. It highlights that full fine-tuning modifies all existing weights, while LoRA selectively updates only a small, low-rank subset of parameters, keeping the majority of the original weights fixed or 'frozen'. This conveys the core idea of LoRA as a more parameter-efficient fine-tuning strategy.

**Content Interpretation:**
The image illustrates the fundamental difference between full fine-tuning and low-rank adaptation (LoRA) in the context of machine learning model training. It shows that full fine-tuning updates all parameters of a model, which is computationally expensive, whereas LoRA achieves adaptation by updating only a small, low-rank representation of the weights, keeping the majority of the original model's weights frozen. This signifies LoRA's advantage in efficiency and reduced computational cost for fine-tuning large models.

**Key Insights:**
The main takeaway from this image is the efficiency benefit of Low-rank adaptation (LoRA) over full fine-tuning. Full fine-tuning requires updating all model weights, making it resource-intensive. In contrast, LoRA updates only a 'small representation of the weights,' keeping most of the original model 'Frozen During training.' This is achieved by introducing and updating smaller, low-rank matrices (indicated by 'Rank = 2') that are then combined with the frozen original weights. This approach significantly reduces the number of trainable parameters and computational cost, making it a more practical method for adapting large models. The image clearly demonstrates that LoRA operates by adding the updated low-rank components to the output derived from the frozen original weights.

**Document Context:**
The image is directly relevant to the 'Low-Rank Adaptation (LoRA)' section of the document, as stated in the context. It visually explains the core concept of LoRA by contrasting it with traditional full fine-tuning. The image supports the surrounding text, 'Compared to full fine-tuning, LoRA aims to update a small representation of the original weights during training,' by providing a clear diagram of how this update mechanism differs between the two methods. It serves as a foundational visual aid to understand why LoRA is considered an efficient alternative.

**Summary:**
This image visually compares two methods of training machine learning models: Full fine-tuning and Low-rank adaptation (LoRA). On the left, 'Full fine-tuning' involves taking an 'Input' and updating 'all weights' of a large model, represented by a large red grid. This entire grid is labeled 'Updated During training'. The output of this process is shown as a horizontal sequence of five colored blocks with varying shades of gray, representing the fully updated output. 

On the right, 'Low-rank adaptation' (LoRA) presents a more efficient approach. It starts with an 'Input' and aims to 'Update a small representation of the weights'. Here, the main set of weights, represented by a large pink grid, remains 'Frozen During training'. Instead of updating the entire original weight matrix, LoRA introduces smaller, low-rank matrices for adaptation. This is illustrated by an 'Updated During training' vertical blue bar (matrix A) and a horizontal blue bar (matrix B) which are multiplied (implied by their arrangement and subsequent addition). These smaller matrices are explicitly labeled 'Rank = 2'. The output of these updated small representation weights is then added ('+') to the output of the frozen original weights (represented by a horizontal sequence of five gray blocks), resulting in a combined updated output (a horizontal sequence of five blue blocks with varying shades). This entire process highlights that only a small, low-rank portion of the weights is updated, while the majority remain frozen.](images/b9f6eb221d95441d417d6282b6f516bbdeddf3625f82551b7b7810f52911c190.jpg)
Figure 12-14. Compared to full fine-tuning, LoRA aims to update a small representation of the original weights during training.

But you might suspect that performance would drop. And you would be right. But where does this trade-off make sense?

Papers like “Intrinsic dimensionality explains the effectiveness of language model fine-tuning” demonstrate that language models “have a very low intrinsic dimen‐ sion.”5 This means that we can find small ranks that approximate even the massive matrices of an LLM. A 175B model like GPT-3, for example, would have a weight matrix of $1 2 , 2 8 8 \times 1 2 , 2 8 8$ inside each of its 96 Transformer blocks. That’s 150 million parameters. If we can successfully adapt that matrix into rank 8, that would only require two $1 2 , 2 8 8 \times 2$ matrices resulting in 197K parameters per block. These are major savings in speed, storage, and compute as explained further in the previously referenced LoRA paper.

This smaller representation is quite flexible in that you can select which parts of the base model to fine-tune. For instance, we can only fine-tune the Query and Value weight matrices in each Transformer layer.

# Compressing the model for (more) efficient training

We can make LoRA even more efficient by reducing the memory requirements of the model’s original weights before projecting them into smaller matrices. The weights of an LLM are numeric values with a given precision, which can be expressed by the number of bits like float64 or float32. As illustrated in Figure 12-15, if we lower the amount of bits to represent a value, we get a less accurate result. However, if we lower the number of bits we also lower the memory requirements of that model.

![## Image Analysis: 04416080792254b2817cb655f4acea0f11333c848c96454bdb91c18e31c7a55b.jpg

**Conceptual Understanding:**
The image conceptually represents the encoding of a floating-point number (specifically, the constant pi) in binary format, comparing two different levels of precision: 32-bit and 16-bit. Its main purpose is to visually demonstrate how reducing the number of bits used to store a floating-point number affects its accuracy or precision. It conveys the key idea that data compression at the bit level for numerical values directly impacts the fidelity of those values, resulting in less precise approximations when fewer bits are allocated.

**Content Interpretation:**
The image directly illustrates how numbers, specifically the mathematical constant pi (π), are represented in computer systems using different floating-point bit precisions. It demonstrates the internal binary structure and the resulting decimal value obtained from that binary representation. The primary concept shown is the relationship between the number of bits allocated for a floating-point number and its achievable precision. The process involves showing a binary sequence, a mathematical formula that decodes this sequence into a decimal number, and the descriptive precision level. The 32-bit representation (Float 32-bit) uses a longer binary string, leading to a more accurate approximation of pi (3.1415927). In contrast, the 16-bit representation (Float 16-bit) uses a shorter binary string, resulting in a less accurate approximation (3.141). The reduction in bits directly correlates with a reduction in the number of significant digits that can be accurately represented.

**Key Insights:**
1. **Reduced Bit-Depth Leads to Lower Precision:** The most significant takeaway is that reducing the number of bits for a floating-point representation (e.g., from 32-bit to 16-bit) inherently decreases the precision with which a number can be represented. This is evidenced by 'Float 32-bit' yielding '3.1415927 High precision' and 'Float 16-bit' yielding '3.141 Low precision' for the same conceptual value, pi. 
2. **Binary Representation Structure:** The image implicitly shows the structure of a floating-point number, composed of a sign bit (the initial '0' in both examples, and derived from '(-1)⁰'), an exponent part (the red-colored bits, used in '2¹'), and a mantissa/significand part (the blue-colored bits, contributing to '1.5707964' and '1.571').
3. **Trade-off between Size and Accuracy:** The comparison vividly demonstrates the trade-off: fewer bits (16-bit) mean a smaller memory footprint and potentially faster computation, but at the sacrifice of numerical accuracy. This is crucial for understanding optimizations in machine learning models. The text 'High precision' for 32-bit and 'Low precision' for 16-bit directly supports this insight.

**Document Context:**
This image is highly relevant to the document's section on 'Compressing the model for (more) efficient training.' It serves as a fundamental illustration of why model compression, particularly by reducing the bit-depth of numerical representations, can lead to efficiency gains but at the cost of numerical precision. By showing the concrete example of pi, it visually explains the concept of 'lowered accuracy when we halve the number of bits,' as mentioned in the surrounding text, providing a clear foundation for understanding the implications of using float 16-bit representations in machine learning models for memory and computational efficiency.

**Summary:**
The image illustrates the representation of the mathematical constant pi (π) using two different floating-point precisions: Float 32-bit and Float 16-bit. Each representation includes its binary sequence, the formula used to derive the decimal value, and the resulting decimal approximation, along with a descriptor of its precision level. The 32-bit representation shows a longer binary sequence resulting in a higher precision approximation of pi, while the 16-bit representation uses a shorter binary sequence, leading to a lower precision approximation. The key takeaway is that reducing the number of bits used for floating-point representation directly results in decreased numerical precision. This visual comparison provides a clear understanding of the trade-off between data size and accuracy.](images/04416080792254b2817cb655f4acea0f11333c848c96454bdb91c18e31c7a55b.jpg)
Figure 12-15. Attempting to represent pi with float 32-bit and float 16-bit representa‐ tions. Notice the lowered accuracy when we halve the number of bits.

With quantization, we aim to lower the number of bits while still accurately repre‐ senting the original weight values. However, as shown in Figure 12-16, when directly mapping higher precision values to lower precision values, multiple higher precision values might end up being represented by the same lower precision values.

![## Image Analysis: ff963c798d5da3e701a25586858b5edd2e3011b8212e9762c735c98d64da79a0.jpg

**Conceptual Understanding:**
The image conceptually represents the process of quantizing neural network model weights and highlights a specific issue that arises from this compression technique. The main purpose is to demonstrate that when original weight values are very close to each other, the act of quantization (mapping them to a smaller set of discrete values) can cause these distinct values to be represented by the same quantized value. Consequently, upon reconstruction, these originally distinct weights become identical, leading to a loss of granularity and potentially impacting model performance. It visually explains the 'Problem: Similar weights are now the same when reconstructed'.

**Content Interpretation:**
This image illustrates the process of weight quantization and a critical issue that arises when similar original weights are quantized. It shows three conceptual layers: original, quantized, and reconstructed weights. The core concept is that quantization, aimed at compressing weights into smaller blocks, can inadvertently merge distinct but similar weight values into a single quantized representation. This leads to a loss of information and differentiation when these weights are later reconstructed, as multiple original values collapse into one.

**Key Insights:**
The main takeaway from this image is that compressing model weights through quantization can lead to a loss of discriminative power for weights that are numerically close to each other. Specifically:
1.  **Quantization is a many-to-one mapping for similar values:** As evidenced by the two 'Original weights' (purple circles) mapping to a single 'Quantized weight' (purple circle in a box).
2.  **Loss of distinctiveness upon reconstruction:** The 'Problem: Similar weights are now the same when reconstructed' clearly indicates that once merged during quantization, the original distinct values cannot be recovered, leading to a single 'Reconstructed weight' (red-outlined circle) where there were originally two.
3.  **Impact on model performance:** This loss of distinctiveness means the model loses the ability to differentiate between inputs that previously relied on these slightly different but similar weight values, potentially affecting its accuracy or precision.

**Document Context:**
This image directly supports the document's section titled 'Compressing the model for (more) efficient training'. It specifically visualizes the potential drawbacks of quantization, which is a technique used for model compression. By showing how similar weights lose their individual identity after quantization and reconstruction, it provides a foundational understanding of a challenge that needs to be addressed when aiming for efficient, yet accurate, compressed models. The image serves as a visual explanation for the textual description given after it: 'Figure 12-16. Quantizing weights that are close to one another results in the same reconstructed weights thereby removing any differentiating factor.'

**Summary:**
The image illustrates a common problem encountered during the quantization of model weights, specifically when compressing to smaller blocks. It shows three horizontal lines representing different stages of weights: 'Original weights', 'Quantized weights', and 'Reconstructed weights'.

Starting from the 'Original weights' line, which spans from 'Low' to 'High' values, several individual weight values are depicted as circles. The first four distinct blue circles are shown, followed by two distinct purple circles that are relatively close in value, both residing on the 'High' end of the scale. The process then moves to 'Quantized weights', where each original weight is mapped to a quantized representation. The image shows several light blue rectangular blocks, each containing a smaller inner circle representing the quantized value. The first four original blue weights map to distinct quantized blue values. Crucially, the two similar 'Original weights' (purple circles) on the 'High' end are both mapped to a single, identical 'Quantized weight' (a purple circle within a light blue rectangle). This specific mapping where two distinct original weights become one quantized value is highlighted with a vertical red-shaded area.

Finally, these 'Quantized weights' are then 'Reconstructed weights'. The first four quantized values are reconstructed back into distinct blue circles, similar to their original forms. However, the single purple 'Quantized weight' (which originated from two similar original weights) is reconstructed into a single red-outlined circle. An annotation box to the right highlights the 'Problem: Similar weights are now the same when reconstructed'. This clearly demonstrates that while the original weights were distinct, their proximity led them to be mapped to the same quantized value, which then results in them being reconstructed as a single, identical value, thereby losing their original distinctiveness.](images/ff963c798d5da3e701a25586858b5edd2e3011b8212e9762c735c98d64da79a0.jpg)
Figure 12-16. Quantizing weights that are close to one another results in the same reconstructed weights thereby removing any differentiating factor.

Instead, the authors of QLoRA, a quantized version of LoRA, found a way to go from a higher number of bits to a lower value and vice versa without differentiating too much from the original weights.6

They used blockwise quantization to map certain blocks of higher precision values to lower precision values. Instead of directly mapping higher precision to lower preci‐ sion values, additional blocks are created that allow for quantizing similar weights. As shown in Figure 12-17, this results in values that can be accurately represented with lower precision.

![## Image Analysis: 57175204a571e4d1d68226d9aa885f5b1da6a031bc2a329a1abd03a0567f2da7.jpg

**Conceptual Understanding:**
This image conceptually represents the process of **blockwise quantization** as a method for model compression. Its main purpose is to illustrate how grouping 'Original weights' into 'several smaller blocks' for 'Quantized weights' can lead to a more effective 'Reconstructed weights' state, especially for 'Similar weights'. The diagram conveys the idea that by quantizing weights in blocks, one can achieve better representation upon reconstruction for values that are numerically close, thereby improving the efficiency of training without significant loss of accuracy.

**Content Interpretation:**
The image demonstrates the process of blockwise quantization, a technique used to compress model weights. It illustrates three stages: 'Original weights', 'Quantized weights', and 'Reconstructed weights'.

1.  **Original weights:** Depicted as individual blue and purple circles along a continuous 'Low' to 'High' scale, representing the initial, high-precision weights.
2.  **Quantized weights:** These are intermediate representations where original weights are grouped and mapped into 'smaller blocks'. Each block is a light blue rectangle containing an inner circle, indicating a discrete quantized value. Arrows show the mapping from original to quantized weights.
3.  **Reconstructed weights:** These are the final, restored weights after quantization, also represented by circles along a 'Low' to 'High' scale. Arrows show the mapping from quantized blocks back to reconstructed weights.

The key concept is the compression of weights into 'several smaller blocks'. The diagram highlights a specific scenario (marked by a green background) where 'Similar weights' (initially purple circles in 'Original weights') are processed. The accompanying text, 'Similar weights are better represented when reconstructed', emphasizes that this blockwise quantization approach is particularly effective at maintaining accuracy for values that are close to each other, leading to better reconstruction fidelity (represented by green circles in 'Reconstructed weights') compared to the individually quantized and reconstructed blue weights.

**Key Insights:**
The main takeaway from this image is that **blockwise quantization is a method to compress model weights that particularly benefits the representation of similar values.**

Key insights derived with textual evidence:
*   **Compression Strategy:** The top annotation 'Compress to several smaller blocks of weights' indicates the core strategy. Instead of quantizing each weight individually, weights are grouped into 'blocks' to facilitate compression.
*   **Stages of Weight Transformation:** The labels 'Original weights', 'Quantized weights', and 'Reconstructed weights' clearly define the three sequential stages of this process, from initial high-precision values to a compressed representation, and finally back to a lower-precision restored form.
*   **Improved Fidelity for Similar Weights:** The annotation 'Similar weights are better represented when reconstructed' (highlighted in green, referring to the purple and green circles) reveals a key advantage of blockwise quantization. It suggests that by grouping similar weights, the quantization process can maintain better fidelity for these values upon reconstruction, leading to more accurate results despite the overall compression.

**Document Context:**
This image is highly relevant to the document's section 'Compressing the model for (more) efficient training'. It visually explains a method (blockwise quantization) for achieving model compression. By illustrating how weights are quantized and then reconstructed, it provides a foundational understanding of how models can be made more efficient by reducing the precision of their weights while attempting to preserve accuracy. The emphasis on 'Similar weights are better represented when reconstructed' directly supports the idea that this compression method aims for accuracy, which is crucial for efficient training without significant performance degradation.

**Summary:**
The image illustrates the concept of blockwise quantization for model compression, showing how original weights are transformed into quantized and then reconstructed weights. It begins with 'Original weights' distributed along a scale from 'Low' to 'High'. These individual weights, represented by blue and purple circles, are then compressed into 'several smaller blocks of weights'. In the 'Quantized weights' row, these original weights are mapped to blocks, where each block (light blue rectangles with an inner circle) represents a range of values. The diagram explicitly highlights a section where 'Similar weights' (represented by purple circles in the original weights) are grouped together. When these quantized blocks are then transformed into 'Reconstructed weights', the similar weights (now represented by green circles in the reconstructed row) are noted to be 'better represented when reconstructed' compared to the other blue weights, which appear to have lost more detail in their reconstruction. The entire process shows a reduction in the number of distinct points from the original to the reconstructed state, with an emphasis on preserving fidelity for similar values through blockwise processing.](images/57175204a571e4d1d68226d9aa885f5b1da6a031bc2a329a1abd03a0567f2da7.jpg)
Figure 12-17. Blockwise quantization can accurately represent weights in lower precision through quantization blocks.

A nice property of neural networks is that their values are generally normally dis‐ tributed between $^ { - 1 }$ and 1. This property allows us to bin the original weights to lower bits based on their relative density, as illustrated in Figure 12-18. The mapping between weights is more efficient as it takes into account the relative frequency of weights. This also reduces issues with outliers.

![## Image Analysis: 798b4419912711ad9df21f4a60aeb02b42be14856ae86c64992a6eabaaef306a.jpg

**Conceptual Understanding:**
This image conceptually illustrates the principle of 'distribution-aware quantization' for model weights. Its main purpose is to demonstrate how this technique efficiently compresses model weights by adapting quantization bin sizes to the statistical distribution of the original weights. The core idea communicated is that by making quantization 'aware' of the weight distribution, it's possible to maintain distinct representations for 'similar weights' in dense regions while using larger, more encompassing bins for 'fewer outliers' in sparse regions, thus achieving better compression and potentially more efficient training without significant loss of information. It visually explains how the number of distinct quantized values is optimized based on the data's inherent spread.

**Content Interpretation:**
The image conceptually illustrates a method of quantizing model weights that is sensitive to their statistical distribution, specifically 'distribution-aware blocking'. It shows how a continuous distribution of 'Original weights' is mapped to a discrete set of 'Quantized weights'. The core concept is that the size and placement of quantization bins are not uniform but adapt to the density of the original weight distribution.

- The upper part shows a frequency distribution of 'Original weights' as a bell-shaped curve and corresponding histogram bars. The blue and purple dots on this curve represent individual original weight values.
- The lower part represents the 'Quantized weights' as a series of connected blocks (bins). Each block contains a single dot (blue, purple, or red), which is the representative quantized value for all original weights falling into that block's range.
- The varying widths of the quantized blocks directly illustrate the 'distribution-aware' nature. In areas where 'Original weights' are dense (like the center of the distribution, indicated by the 'Similar weights end up in different bins' text and purple dots), the quantized blocks are narrower and more numerous, allowing for finer differentiation between similar original weight values. This ensures that 'Similar weights' are 'end up in different bins'.
- Conversely, in areas where 'Original weights' are sparse (like the tails of the distribution, indicated by the 'Block can be bigger since there are fewer outliers' text and the red dot), the quantized blocks are wider. This allows for efficient representation of 'fewer outliers' by assigning them a broader quantization range, thus reducing the total number of unique quantized values needed.

**Key Insights:**
The main takeaways from this image are:

1.  **Adaptive Quantization:** Quantization is not uniform; the bin sizes ('blocks') adapt to the underlying distribution of the 'Original weights'. This is evident from the varying widths of the 'Quantized weights' blocks, which directly reflects the density of the 'Original weights' distribution above them.
2.  **Precision for Dense Regions:** In regions where 'Original weights' are close to one another (dense parts of the distribution), 'Similar weights end up in different bins'. This is shown by the narrower quantized blocks in the central part of the distribution, where purple original weights, though similar, are mapped to distinct purple quantized bins.
3.  **Efficiency for Sparse Regions/Outliers:** In regions with fewer weights or 'outliers', the 'Block can be bigger since there are fewer outliers'. This allows for greater compression by using wider bins in less critical areas, as shown by the wider quantized block on the right, which encompasses a broader range for the red outlier weight. This approach balances precision where it's needed most (dense regions) with efficient compression in less dense regions.
4.  **Improved Model Compression:** By being 'distribution-aware', this method allows for more efficient model compression. It reduces the number of unique values needed to represent the weights while preserving more information in critical regions (where weights are similar) compared to uniform quantization.

**Document Context:**
This image directly supports the document's section on 'Compressing the model for (more) efficient training'. It provides a visual explanation for how 'distribution-aware blocks' work, specifically addressing the claim that 'we can prevent values close to one another from being represented with the same quantized value'. The diagram clarifies the mechanism behind this statement by showing how quantization bins adapt their size based on the density of the original weight distribution, thereby optimizing the compression process while minimizing the loss of critical information, especially for closely spaced weights. It visually demonstrates the balance between reducing the number of unique weight representations and maintaining sufficient precision for distinct values.

**Summary:**
The image illustrates the concept of distribution-aware quantization, a method used to compress models by efficiently representing weights. At the top, a histogram-like representation with a superimposed curve depicts the distribution of 'Original weights'. The curve shows varying densities of weights across its range. Below this, arrows point down to a series of connected blocks, which represent 'Quantized weights'. These blocks vary in width and contain a single dot, indicating the quantized value. The diagram highlights three key aspects of distribution-aware blocking:

1.  **Distribution-aware block:** This label points to the leftmost, wider block in the quantized weights section, indicating that blocks are designed considering the underlying distribution.
2.  **Similar weights end up in different bins:** This label is positioned above a central section of narrower, equally sized blocks. In this region, the original weights (purple dots on the curve) are close to each other but are mapped to distinct, closely spaced quantized bins (purple dots in the blocks below). This illustrates that even similar values are assigned different quantized representations if the original distribution is dense in that region.
3.  **Block can be bigger since there are fewer outliers:** This label points to the rightmost, wider block in the quantized weights section. It shows that in regions with fewer original weights (outliers), the quantized block can cover a larger range, efficiently representing the sparse data.](images/798b4419912711ad9df21f4a60aeb02b42be14856ae86c64992a6eabaaef306a.jpg)
Figure 12-18. Using distribution-aware blocks we can prevent values close to one another from being represented with the same quantized value.

Combined with the blockwise quantization, this normalization procedure allows for accurate representation of high precision values by low precision values with only a small decrease in the performance of the LLM. As a result, we can go from a 16-bit float representation to a measly 4-bit normalized float representation. A 4-bit representation significantly reduces the memory requirements of the LLM during training. Note that the quantization of LLMs in general is also helpful for inference as quantized LLMs are smaller in size and therefore require less VRAM.

There are more elegant methods to further optimize this like double quantization and paged optimizers, which you can read about more in the QLoRA paper discussed earlier. For a complete and highly visual guide to quantization, see this blog post.

# Instruction Tuning with QLoRA

Now that we have explored how QLoRA works, let us put that knowledge into practice! In this section, we will fine-tune a completely open source and smaller version of Llama, TinyLlama, to follow instructions using the QLoRA procedure. Consider this model a base or pretrained model, one that was trained with language modeling but cannot yet follow instructions.

# Templating Instruction Data

To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template. This chat template, as illustrated in Figure 12-19, differenti‐ ates between what the LLM has generated and what the user has generated.

![## Image Analysis: 4a9f71b279ca575e765813406af4aad6fd6199d6afdd729327497e6348776f13.jpg

**Conceptual Understanding:**
The image conceptually represents a standardized message format or a 'chat template' used for providing conversational input to an AI model and receiving its output. Its main purpose is to illustrate how a single turn of a dialogue, comprising a user's query and an AI's response, is structured with specific tokens to provide clarity and context to the model. The key idea being communicated is the importance of explicit role identification and sequence termination markers in templating instruction data for language models.

**Content Interpretation:**
The image details a chat template structure used for instructing or interacting with a language model. It shows the clear demarcation between a user's input and a model's response using specific tokens. The text 'What is 1 + 1' represents a user query. The text 'The answer to 1 + 1 is 2!' represents the model's generated answer. The '<|user|>' token explicitly defines the preceding content as the user's role, and '<|assistant|>' explicitly defines the subsequent content as the assistant's role. The '</s>' token serves as an end-of-sequence marker, indicating the completion of a turn or a segment of text. These elements collectively illustrate how a single turn of a conversational interaction is formatted, providing essential contextual cues for the AI model. The lines connecting the text blocks to external labels ('User context', 'End-of-sequence (eos) token', 'Model's answer') clarify the functional role of each component within the template.

**Key Insights:**
The main takeaway from this image is the precise structure of a chat template designed for interacting with AI models, particularly large language models. Key insights include: 1. Conversational turns are clearly delineated by role-specific tokens such as '<|user|>' and '<|assistant|>' to assign utterances to the correct speaker. 2. The 'End-of-sequence (eos) token' (</s>) is vital for marking the end of a complete user input or model response, which helps the model understand turn boundaries. 3. The template provides explicit context, categorizing segments as 'User context' and 'Model's answer', which are essential for the model to parse and generate appropriate responses. This structured format ensures that the model correctly interprets who is speaking and when a segment of communication is complete, enabling coherent and context-aware interactions.

**Document Context:**
Given the document context 'Section: Templating Instruction Data' and the caption 'Figure 12-19. The chat template that we use throughout this chapter,' this image serves as a foundational visual example for understanding the chat template structure. It directly illustrates the specific template described and used throughout the chapter, making the abstract concept of 'templating instruction data' concrete. It provides the exact format for a conversational turn, which is crucial for readers to comprehend how instruction data for models is constructed, demonstrating the practical application of the concepts discussed in the surrounding text.

**Summary:**
This image displays a structured chat template, illustrating how user input and model responses are formatted, along with special tokens for context and sequence termination. It begins with the '<|user|>' token, indicating the start of a user's turn, followed by the user's query: 'What is 1 + 1'. This entire section, '<|user|>' and 'What is 1 + 1', is collectively labeled as 'User context' via a red line. The user's input is then immediately followed by an '</s>' token, which is identified as an 'End-of-sequence (eos) token' by an orange line. Subsequently, the template includes the '<|assistant|>' token, marking the beginning of the model's response. This is followed by the model's answer: 'The answer to 1 + 1 is 2!'. The '<|assistant|>' token and the response, 'The answer to 1 + 1 is 2!', are together denoted as 'Model's answer' by a blue line. Finally, the model's response is also terminated by an '</s>' token, another 'End-of-sequence (eos) token'. The entire structure is enclosed within a single gray box, representing the complete chat template.](images/4a9f71b279ca575e765813406af4aad6fd6199d6afdd729327497e6348776f13.jpg)
Figure 12-19. The chat template that we use throughout this chapter.

We chose this chat template to use throughout the examples since the chat version of TinyLlama uses the same format. The data that we are using is a small subset of the UltraChat dataset.7 This dataset is a filtered version of the original UltraChat dataset that contains almost $2 0 0 \mathrm { k }$ conversations between a user and an LLM.

We create a function, format_prompt, to make sure that the conversations follow this template:

from transformers import AutoTokenizer   
from datasets import load_dataset   
# Load a tokenizer to use its chat template   
template_tokenizer $=$ AutoTokenizer.from_pretrained( "TinyLlama/TinyLlama-1.1BChat-v1.0"   
)   
def format_prompt(example): """Format the prompt to using the <|user|> template TinyLLama is using""" # Format answers chat $=$ example["messages"] prompt $=$ template_tokenizer.apply_chat_template(chat, tokenize=False) return {"text": prompt}   
# Load and format the data using the template TinyLLama is using   
dataset $=$ ( load_dataset("HuggingFaceH4/ultrachat_200k", split $\mathbf { \varepsilon } =$ "test_sft") .shuffle(seed $= 4 2$ ) .select(range(3_000))   
)   
dataset $=$ dataset.map(format_prompt)

We select a subset of 3,000 documents to reduce the training time, but you can increase this value to get more accurate results.

Using the "text" column, we can explore these formatted prompts:

# Example of formatted promptprint(dataset["text"][2576])

<|user|>   
Given the text: Knock, knock. Who's there? Hike.   
Can you continue the joke based on the given text material "Knock, knock. Who's there? Hike"?</s>   
<|assistant|>   
Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>   
<|user|>   
Can you tell me another knock-knock joke based on the same text material   
"Knock, knock. Who's there? Hike"?</s>   
<|assistant|>   
Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here   
and let's go for a walk! $! < / \mathsf { s } >$

# Model Quantization

Now that we have our data, we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. We use the bitsandbytes package to compress the pretrained model to a 4-bit representation.

In BitsAndBytesConfig, you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant):

import torch   
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig   
model_name $=$ "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"   
# 4-bit quantization configuration - Q in QLoRA   
bnb_config $=$ BitsAndBytesConfig( load_in_4bit=True, # Use 4-bit precision model loading bnb_4bit_quant_type $\ast =$ "nf4", # Quantization type bnb_4bit_compute_dtype $\Bumpeq$ "float16", # Compute dtype bnb_4bit_use_double_quant=True, # Apply nested quantization   
)   
# Load the model to train on the GPU   
model $=$ AutoModelForCausalLM.from_pretrained( model_name, device_map="auto", # Leave this out for regular SFT quantization_config=bnb_config,   
)   
model.config.use_cache $=$ False   
model.config.pretraining_tp $\ c = ~ 1$   
# Load LLaMA tokenizer   
tokenizer $=$ AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)   
tokenizer.pad_token $=$ "<PAD>"   
tokenizer.padding_side $=$ "left"

This quantization procedure allows us to decrease the size of the original model while retaining most of the original weights’ precision. Loading the model now only uses ${ \sim } 1$ GB VRAM compared to the ${ \sim } 4$ GB of VRAM it would need without quantization. Note that during fine-tuning, more VRAM will be necessary so it does not cap out on the ${ \sim } 1$ GB VRAM needed to load the model.

# LoRA Configuration

Next, we will need to define our LoRA configuration using the peft library, which represents hyperparameters of the fine-tuning process:

from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

# Prepare LoRA Configuration   
peft_config $=$ LoraConfig( lora_alpha $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , # LoRA Scaling lora_dropout $= 0 . 1$ , # Dropout for LoRA Layers $\Gamma { = } 6 4$ , # Rank bias $=$ "none", task_type $\ast =$ "CAUSAL_LM", target_modules $=$ # Layers to target ["k_proj", "gate_proj", "v_proj", "up_proj", "q_proj", "o_proj",   
"down_proj"]   
)

# Prepare model for training model $=$ prepare_model_for_kbit_training(model) model $=$ get_peft_model(model, peft_config)

There are several parameters worth mentioning:

r

This is the rank of the compressed matrices (recall this from Figure 12-13) Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64.

lora_alpha

Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r.

target_modules

Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa.

Playing around with the parameters is a worthwhile experiment to get an intuitive understanding of values that work and those that do not. You can find an amazing resource of additional tips on LoRA fine-tuning in the Ahead of AI newsletter by Sebastian Raschka.

This example demonstrates an efficient form of fine-tuning your model. If you want to perform full fine-tuning instead, you can remove the quantization_config parameter when loading the model and skip the creation of peft_config. By removing those, we would go from “Instruction tuning with QLoRA” to “full instruction tuning.”

# Training Configuration

Lastly, we need to configure our training parameters as we did in Chapter 11:

from transformers import TrainingArguments output_dir $=$ "./results"

# Training arguments   
training_arguments $=$ TrainingArguments( output_dir=output_dir, per_device_train_batch_size $^ { = 2 }$ , gradient_accumulation_steps $\mathbf { \Psi } = \mathbf { \Psi }$ , optim="paged_adamw_32bit", learning_rate=2e-4, lr_scheduler_type $^ { 1 \pm }$ "cosine", num_train_epochs $^ { - 1 }$ , logging_step $ \Longrightarrow 1 0$ , fp16=True, gradient_checkpointing=True   
)

There are several parameters worth mentioning:

num_train_epochs The total number of training rounds. Higher values tend to degrade performance so we generally like to keep this low.

learning_rate

Determines the step size at each iteration of weight updates. The authors of QLoRA found that higher learning rates work better for larger models $\mathrm { \Omega } > 3 3 \mathrm { B }$ parameters).

lr_scheduler_type

A cosine-based scheduler to adjust the learning rate dynamically. It will linearly increase the learning rate, starting from zero, until it reaches the set value. After that, the learning rate is decayed following the values of a cosine function.

optim

The paged optimizers used in the original QLoRA paper.

Optimizing these parameters is a difficult task and there are no set guidelines for doing so. It requires experimentation to figure out what works best for specific datasets, model sizes, and target tasks.

Although this section describes instruction tuning, we could also use QLoRA to fine-tune an instruction model. For instance, we could fine-tune a chat model to generate specific SQL code or to create JSON output that adheres to a specific format. As long as you have the data available (with appropriate query-response items), QLoRA is a great technique for nudging an existing chat model to be more appropriate for your use case.

# Training

Now that we have prepared all our models and parameters, we can start fine-tuning our model. We load in SFTTrainer and simply run trainer.train():

from trl import SFTTrainer   
# Set supervised fine-tuning parameters   
trainer $=$ SFTTrainer( model=model, train_dataset=dataset, dataset_text_field="text", tokenizer=tokenizer, args $=$ training_arguments, max_seq_length $= 5 1 2$ , # Leave this out for regular SFT peft_config=peft_config,   
)

# Train model trainer.train()

# Save QLoRA weights trainer.model.save_pretrained("TinyLlama-1.1B-qlora")

During training the loss will be printed every 10 steps according to the log ging_steps parameter. If you are using the free GPU provided by Google Colab, which is the Tesla T4 at the time of writing, then training might take up to an hour. A good time to take a break!

# Merge Weights

After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights. Although the tokenizer was not updated during training, we save it to the same folder as the model for easier access:

from peft import AutoPeftModelForCausalLM   
model $=$ AutoPeftModelForCausalLM.from_pretrained( "TinyLlama-1.1B-qlora", low_cpu_mem_usage=True, device_map="auto",   
)   
# Merge LoRA and base model   
merged_model $=$ model.merge_and_unload()

After merging the adapter with the base model, we can use it with the prompt template that we defined earlier:

from transformers import pipeline   
# Use our predefined prompt template   
prompt $=$ """<|user|>   
Tell me something about Large Language Models.</s>   
<|assistant|>   
# Run our instruction-tuned model   
pipe $=$ pipeline(task $: =$ "text-generation", model=merged_model, tokenizer=tokenizer)   
print(pipe(prompt)[0]["generated_text"])

Large Language Models (LLMs) are artificial intelligence (AI) models that learn language and understand what it means to say things in a particular language. They are trained on huge amounts of text…

The aggregate output shows that the model now closely follows our instructions, which is not possible with the base model.

# Evaluating Generative Models

Evaluating generative models poses a significant challenge. Generative models are used across many diverse use cases, making it a challenge to rely on a singular metric for judgment. Unlike more specialized models, a generative model’s ability to solve mathematical questions does not guarantee success in solving coding questions.

At the same time, evaluating these models is vital, particularly in production settings where consistency is important. Given their probabilistic nature, generative models do not necessarily generate consistent outputs; there is a need for robust evaluation.

In this section, we will explore a few common evaluation methods, but we want to emphasize the current lack of golden standards. No one metric is perfect for all use cases.

# Word-Level Metrics

One common metrics category for comparing generative models is word-level evalu‐ ation. These classic techniques compare a reference dataset with the generated tokens on a token(set) level. Common word-level metrics include perplexity,8 ROUGE,9 BLEU,10 and BERTScore.11

Of note is perplexity, which measures how well a language model predicts a text. Given input text, the model predicts how likely the next token is. With perplexity, we assume a model performs better if it gives the next token a high probability. In other words, the models should not be “perplexed” when presented with a well-written document.

As illustrated in Figure 12-20, when presented with the input “When a measure becomes a,” the model is asked how probable the word “target” is as the next word.

![## Image Analysis: 527ba3853577c3105cdfac863060514b22b7a02f35f2b71b176958a0d0c015fa.jpg

**Conceptual Understanding:**
This image represents the fundamental task of 'next-word prediction' in Large Language Models (LLMs). Conceptually, it illustrates how an LLM processes a sequence of words (the 'context') to predict the most probable subsequent word (the 'next word'). The main purpose is to simplify and visually explain this core mechanism by drawing a parallel between a philosophical statement about measures and targets, and the probabilistic prediction task performed by LLMs. It conveys the idea that an LLM's 'understanding' of the preceding text (the 'context') allows it to 'target' and predict the 'next word'.

**Content Interpretation:**
The image conceptually illustrates how Large Language Models (LLMs) perform next-word prediction. It draws a parallel between a well-known quote and the operational mechanism of LLMs. The quote, "When a measure becomes a target, it ceases to be a good measure," serves as a high-level statement, where specific parts are then linked to the LLM's task. The phrase "When a measure" is visually associated with "context," implying that the preceding text (the 'measure') provides the 'context' for prediction. The word "target" from the quote is linked to "next word," indicating that the 'next word' an LLM tries to predict is its 'target'. The entire system depicts a simplified, high-level view of how an LLM processes input to generate output, focusing on the probabilistic nature of predicting the subsequent token or word based on the given sequence of words (context).

**Key Insights:**
The main takeaway from this image is a simplified, yet effective, conceptualization of next-word prediction in LLMs. The image teaches that:1.  **Context is Key:** The orange highlighting and arrow connecting "When a measure" to "context" emphasize that the preceding text serves as the crucial 'context' for prediction. An LLM's 'measure' of the input is its understanding of the 'context'.2.  **Prediction as Targeting:** The green highlighting and arrow connecting "target" to "next word" illustrate that the act of predicting the subsequent word is akin to identifying a 'target'. The LLM's goal is to accurately 'target' the correct 'next word'.3.  **Core LLM Task:** The overall structure highlights the central role of "Given the context, what is the probability of the next word?" as the fundamental operation for many LLMs, reinforcing the idea presented in the accompanying document text.These insights are directly derived from the verbatim textual elements and the visual connections established by the arrows and highlighted words.

**Document Context:**
This image directly supports the document's "Word-Level Metrics" section and the text "Figure 12-20. Next-word prediction is a central feature of many LLMs." It visually explains the fundamental task of next-word prediction that LLMs undertake. By connecting the abstract concept of "measure" and "target" from the initial quote to the concrete terms "context" and "next word" in the question, the image clarifies what an LLM is essentially doing: it uses the preceding words (context) to determine the most probable subsequent word (next word). This serves as a foundational visual for understanding how LLMs generate text and how their performance might be evaluated using word-level metrics.

**Summary:**
The image illustrates the core concept of next-word prediction, central to Large Language Models (LLMs), by connecting a classic adage to the task an LLM performs. It presents the quote: "When a measure becomes a target, it ceases to be a good measure." Below this quote, a question is posed: "Given the context, what is the probability of the next word?" Visual connections are established using arrows. The phrase "When a measure" (highlighted in orange) from the quote points via a downward arrow to the word "context" (also highlighted in orange) in the question. Similarly, the word "target" (highlighted in green) from the quote connects via a horizontal and then downward arrow to the phrase "next word" (also highlighted in green) in the question. This visually maps how a language model uses the preceding 'context' to predict the 'next word', conceptualizing the prediction task as determining the 'target' word given the prior 'measure' of text.](images/527ba3853577c3105cdfac863060514b22b7a02f35f2b71b176958a0d0c015fa.jpg)
Figure 12-20. Next-word prediction is a central feature of many LLMs.

Although perplexity, and other word-level metrics, are useful metrics to understand the confidence of the model, they are not a perfect measure. They do not account for consistency, fluency, creativity, or even correctness of the generated text.

# Benchmarks

A common method for evaluating generative models on language generation and understanding tasks is on well-known and public benchmarks, such as MMLU,12 GLUE,13 TruthfulQA,14 GSM8k,15 and HellaSwag.16 These benchmarks give us information about basic language understanding but also complex analytical answer‐ ing, like math problems.

Aside from natural language tasks, some models specialize in other domains, like programming. These models tend to be evaluated on different benchmarks, such as HumanEval,17 which consists of challenging programming tasks for the model to solve. Table 12-1 gives an overview of common public benchmarks for generative models.

Table 12-1. Common public benchmarks for generative models   

<table><tr><td>Benchmark Description</td><td></td><td>Resources</td></tr><tr><td>MMLU</td><td>The Massive Multitask Language Understanding (MMLU) benchmark tests the model on 57 different tas, including clasification,questionanswering,andsentiment analysis.</td><td>https://oreil.ly/ nrG_g</td></tr><tr><td>GLUE</td><td>TheGeneral Language Understanding Evaluation (GLUE)benchmark consists of language understanding tasks covering a wide degree of difficulty.</td><td>https://orel/ LV_fb</td></tr><tr><td></td><td>TruthfulQATruthfulQA measures the truthfulness of a model&#x27;s generated text.</td><td>https://oreil.ly/ i2Br</td></tr><tr><td>GSM8k</td><td>The GSM8k dataset contains grade-schol math word problems.It is linguisticallydiverse and created by human problem writers.</td><td>https://oreil.ly/ 0OBXY</td></tr><tr><td>HellaSswag</td><td>HellSwag is a challenge dataset for evaluating common-sense inference.It consists of multiple-choice questions that the model needs to answer.It can select one offour answer choices for each question.</td><td>https://oreill/ aDvBP</td></tr><tr><td>HumanEval</td><td>The HumanEval benchmark is used for evaluating generated code based on 164 programming problems.</td><td>https://orel/l duix</td></tr></table>

Benchmarks are a great way to get a basic understanding on how well a model performs on a wide variety of tasks. A downside to public benchmarks is that models can be overfitted to these benchmarks to generate the best responses. Moreover, these are still broad benchmarks and might not cover very specific use cases. Lastly, another downside is that some benchmarks require strong GPUs with a long running time (over hours) to compute, which makes iteration difficult.

# Leaderboards

With so many different benchmarks, it is hard to choose which benchmark best suits your model. Whenever a model is released, you will often see it evaluated on several benchmarks to showcase how it performs across the board.

As such, leaderboards were developed containing multiple benchmarks. A common leaderboard is the Open LLM Leaderboard, which, at the time of writing, includes six benchmarks, including HellaSwag, MMLU, TruthfulQA, and GSM8k. Models that top the leaderboard, assuming they were not overfitted on the data, are generally regar‐ ded as the “best” model. However, since these leaderboards often contain publicly available benchmarks, there is a risk of overfitting on the leaderboard.

# Automated Evaluation

Part of evaluating a generative output is the quality of its text. For instance, even if two models were to give the same correct answer to a question, the way they derived that answer might be different. It is often not just about the final answer but also the construction of it. Similarly, although two summaries might be similar, one could be significantly shorter than another, which is often important for a good summary.

To evaluate the quality of the generated text above the correctness of the final answer, LLM-as-a-judge was introduced.18 In essence, a separate LLM is asked to judge the quality of the LLM to be evaluated. An interesting variant of this method is pairwise comparison. Two different LLMs will generate an answer to a question and a third LLM will be the judge to declare which is better.

As a result, this methodology allows for automated evaluation of open-ended ques‐ tions. A major advantage is that as LLMs improve, so do their capabilities to judge the quality of output. In other words, this evaluation methodology grows with the field.

# Human Evaluation

Although benchmarks are important, the gold standard of evaluation is gener‐ ally considered to be human evaluation. Even if an LLM scores well on broad benchmarks, it still might not score well on domain-specific tasks. Moreover, bench‐ marks do not fully capture human preference and all methods discussed before are merely proxies for that.

A great example of a human-based evaluation technique is the Chatbot Arena.19 When you go to this leaderboard you are shown two (anonymous) LLMs you can interact with. Any question or prompt you ask will be sent to both models and you will receive their output. Then, you can decide which output you prefer. This process allows for the community to vote on which models they prefer without knowing which ones are presented. Only after you vote do you see which model generated which text.

At the time of writing, this method has generated over $8 0 0 { , } 0 0 0 { + }$ human votes that were used to compute a leaderboard. These votes are used to calculate the relative skill level of LLMs based on their win rates. For instance, if a low-ranked LLM beats a high-ranked LLM, its ranking changes significantly. In chess, this is referred to as the Elo rating system.

This methodology therefore uses crowdsourced votes, which helps us understand the quality of the LLM. However, it is still the aggregated opinion of a wide variety of users, which might not relate to your use case.

As a result, there is no one perfect method of evaluating LLMs. All mentioned methodologies and benchmarks provide an important, although limited evaluation perspective. Our advice is to evaluate your LLM based on the intended use case. For coding, HumanEval would be more logical than GSM8k.

But most importantly, we believe that you are the best evaluator. Human evaluation remains the gold standard because it is up to you to decide whether the LLM works for your intended use case. As with the examples in this chapter, we highly advise that you also try these models and perhaps develop some questions yourself. For example, the authors of this book are Arabic (Jay Alammar) and Dutch (Maarten Grootendorst), and we often ask questions in our native language when approached with new models.

One final note on this topic is a quote we hold dear:

When a measure becomes a target, it ceases to be a good measure. —Goodhart’s Law20

In the context of LLMs, when using a specific benchmark, we tend to optimize for that benchmark regardless of the consequences. For instance, if we focus purely on optimizing for generating grammatically correct sentences, the model could learn to only output one sentence: “This is a sentence.” It is grammatically correct but tells you nothing about its language understanding capabilities. Thus, the model may excel at a specific benchmark but potentially at the expense of other useful capabilities.

# Preference-Tuning / Alignment / RLHF

Although our model can now follow instructions, we can further improve its behav‐ ior by a final training phase that aligns it to how we expect it to behave in different scenarios. For instance, when asked “What is an LLM?” we might prefer an elaborate answer that describes the internals of an LLM compared to the answer “It is a large language model” without further explanations. How exactly do we align our (human) preference for one answer over the other with the output of an LLM?

To start with, recall that an LLM takes a prompt and outputs a generation as illustra‐ ted in Figure 12-21.

![## Image Analysis: 62a223f8c316e75c43506fa91976c192a2298ccb33f05a70ccbe8503ee58ad9a.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental input-output flow of a Large Language Model (LLM). The main purpose is to illustrate the basic operation where an LLM receives an initial input prompt and subsequently produces a specific generation or output. The key idea being communicated is the direct, sequential transformation of a user's prompt into an AI-generated response by an LLM.

**Content Interpretation:**
This image illustrates the fundamental, high-level process of a Large Language Model (LLM) in generating an output from a given input. The key process shown is the transformation of a specific input, "Input prompt 1", through the "LLM", resulting in a distinct output, "Generation A". The presence of a small icon on the "LLM" box, resembling a document or speech bubble, subtly reinforces that the LLM is involved in text-based processing or conversation. The significance of this simple flow is to demonstrate the core function of an LLM: taking an input and producing a generated output.

**Key Insights:**
The main takeaway from this image is the core operational mechanism of a Large Language Model (LLM): it takes an input and produces a corresponding output. The specific textual elements "Input prompt 1", "LLM", and "Generation A" explicitly denote the three sequential stages of this process. This highlights the concept of an LLM as a generative system, transforming an initial prompt into a distinct generated response. The simplicity of the diagram emphasizes this fundamental input-process-output paradigm, which is crucial for understanding how LLMs function at their most basic level.

**Document Context:**
This image, described in the document context as Figure 12-21 and located within the "Preference-Tuning / Alignment / RLHF" section, serves as a foundational visual for understanding the basic operational step of an LLM before diving into more complex concepts like preference-tuning, alignment, or Reinforcement Learning from Human Feedback (RLHF). It establishes the fundamental input-output relationship of an LLM, which is a prerequisite for understanding how its generations might then be evaluated and fine-tuned for alignment. The text after the image, "Figure 12-21. An LLM takes an input prompt and outputs a generation," directly reinforces and summarizes the content of this diagram.

**Summary:**
The image presents a straightforward sequential process illustrating how a Large Language Model (LLM) functions. It begins with an "Input prompt 1" which serves as the initial query or data provided to the system. This input is then processed by the "LLM" itself, depicted as a central component, which processes the prompt. The outcome of the LLM's processing is labeled as "Generation A", representing the output or response produced by the model based on the initial input. The flow is strictly linear, indicating a direct cause-and-effect relationship from input to processing to output.](images/62a223f8c316e75c43506fa91976c192a2298ccb33f05a70ccbe8503ee58ad9a.jpg)
Figure 12-21. An LLM takes an input prompt and outputs a generation.

We can ask a person (preference evaluator) to evaluate the quality of that model generation. Say they assign it a certain score, like 4 (see Figure 12-22).

![## Image Analysis: f7c500f7504f0e4187d93f4b407c213ec020f0f4ab93675c9cb5c6a419504e7a.jpg

**Conceptual Understanding:**
Conceptually, this image represents a simplified workflow for evaluating the output of a Large Language Model (LLM). Its main purpose is to illustrate how a generated response from an LLM is assessed for quality against an input prompt, resulting in a quantifiable 'preference score'. This process is fundamental to techniques like Reinforcement Learning from Human Feedback (RLHF), where model outputs are judged to refine the model's behavior and align it with desired criteria. The image conveys the idea of a prompt-response cycle followed by a qualitative and quantitative assessment phase.

**Content Interpretation:**
This image depicts a feedback loop or evaluation mechanism for Large Language Models (LLMs). It illustrates the process of generating text from a prompt and then evaluating the quality of that generation using a 'Preference evaluator'. The core components are the LLM's generation capability and a system for scoring the output based on its perceived quality relative to the input prompt. The diagram highlights the input, the LLM's role, the output, and the subsequent human or automated assessment of that output's effectiveness or 'goodness'.

**Key Insights:**
1. The diagram illustrates a basic feedback mechanism for LLMs, crucial for their alignment and fine-tuning. 
2. The process involves an input prompt leading to an LLM generation.
3. A critical step is the evaluation of this generation in the context of the original prompt.
4. This evaluation is performed by a 'Preference evaluator', which assigns a numerical 'Preference score'.
5. The preference score exists on a spectrum, from 'Bad' (-6) to 'Good' (6), indicating a quantitative measure of quality.
6. The question 'How good is this generation in response to this prompt?' explicitly defines the criteria for evaluation.
7. The specific score '4' shown implies a relatively good generation, leaning towards the 'Good' end of the spectrum.

**Document Context:**
This image is placed in the "Preference-Tuning / Alignment / RLHF" section of the document, and the text after the image (Figure 12-22. Use a preference evaluator (human or otherwise) to evaluate the quality of the generation.) explicitly states its purpose. Therefore, the image serves to visually explain a fundamental step in Reinforcement Learning from Human Feedback (RLHF) or preference-tuning processes for LLMs, specifically the evaluation phase where a generated response is judged for its quality. It demonstrates how a prompt and a corresponding generation are assessed to derive a 'preference score', which is critical for aligning LLMs with desired human preferences or performance criteria.

**Summary:**
The image illustrates a process for evaluating the quality of a Large Language Model (LLM) generation using a preference evaluator. The left side of the diagram shows the generation process, starting with an "Input prompt 1" which is fed into an "LLM". The LLM then produces "Generation A". This generated output, along with the original prompt, is then presented with the question "How good is this generation in response to this prompt?". The right side of the diagram, separated by a dotted vertical line, shows the evaluation of this generation. A "Preference evaluator" (which the document context states can be human or otherwise) assesses the quality and assigns a "Preference score". In this specific example, the score is "4", as indicated by the purple '4' with an upward arrow pointing to it. This score is then mapped onto a vertical scale ranging from "6" (labeled "Good" in green) down to "-6" (labeled "Bad" in red), with intermediate values of "3", "0", and "-3". A purple triangle pointer indicates the current score on this scale, which is not explicitly aligned with '4' but points towards the 'Good' end near '6'. The overall flow demonstrates how a prompt is processed by an LLM to generate text, and subsequently, how that generated text is evaluated for quality against the original prompt.](images/f7c500f7504f0e4187d93f4b407c213ec020f0f4ab93675c9cb5c6a419504e7a.jpg)
Figure 12-22. Use a preference evaluator (human or otherwise) to evaluate the quality of the generation.

Figure 12-23 shows a preference tuning step updating the model based on that score:

• If the score is high, the model is updated to encourage it to generate more like this type of generation. • If the score is low, the model is updated to discourage such generations.

![## Image Analysis: 21939d28e7d57cc4e67db8ce4017f23c3c4934c94b3c4ed01cc2e83d65000dce.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified model of a preference-tuning or alignment feedback loop used in training Large Language Models (LLMs). The main purpose is to illustrate how an LLM's generative behavior can be iteratively improved or aligned with desired outcomes by using an external 'Preference evaluator' that provides a score-based feedback signal. The image conveys the idea of learning through reinforcement and discouragement based on the quality of generated content.

**Content Interpretation:**
The image depicts a feedback loop for preference tuning an LLM. The processes shown include: an initial "Input prompt 1" being fed into an "LLM", the LLM generating an output called "Generation A", a "Preference evaluator" assessing "Generation A", and a feedback signal from the evaluator updating the LLM. The core relationship is the iterative refinement of the LLM's behavior based on the evaluation score. The significance lies in the explicit rules: "If high score: do more of this" and "If low score: do less of this", which define how the LLM learns to align with desired outcomes. The numerical annotation "4" labels the feedback arrow, suggesting it's part of a larger sequence or reference.

**Key Insights:**
The main takeaway is that LLM alignment, specifically preference tuning, is an iterative process driven by an external evaluation. The image teaches that LLMs learn not just from raw data but also from explicit feedback on the quality of their generations. Key insights include: 1. An 'Input prompt 1' triggers an LLM to produce a 'Generation A'. 2. A 'Preference evaluator' assesses 'Generation A', yielding a score. 3. This score dictates the LLM's update: 'If high score: do more of this' (reinforcement), and 'If low score: do less of this' (discouragement). 4. This feedback loop (labeled '4') enables continuous refinement and alignment of the LLM's behavior. The extracted text provides direct evidence for each of these points, detailing the components and their functional relationships.

**Document Context:**
This image directly illustrates the concept of "Preference-Tuning" or "Alignment" within the context of Reinforcement Learning from Human Feedback (RLHF), as indicated by the document's section title. It visually explains the mechanism by which "Preference tuning methods update the LLM based on the evaluation score," which is explicitly mentioned in the text following the image (Figure 12-23). It serves as a foundational diagram for understanding how LLMs are made to align with specific criteria or human preferences through iterative feedback.

**Summary:**
This diagram illustrates a fundamental process in Large Language Model (LLM) alignment, specifically a method of "preference tuning" where an LLM is refined based on an evaluation score. The process begins with an "Input prompt 1", which serves as the initial query or instruction given to the system. This prompt is then fed into the "LLM" (Large Language Model). The LLM processes this input and generates an output, labeled as "Generation A". This generated output, "Generation A", is then passed to a component called the "Preference evaluator". The role of this evaluator is to assess the quality or desirability of "Generation A" and assign it a score. Crucially, this score is used in a feedback loop to "Update the LLM based on this score". The update mechanism is guided by two explicit rules: "If high score: do more of this" - This indicates that if the "Preference evaluator" assigns a high score to "Generation A", the LLM's internal parameters are adjusted to encourage or reinforce the type of generation that led to this high score. "If low score: do less of this" - Conversely, if a low score is assigned, the LLM is adjusted to discourage or reduce the likelihood of producing similar generations in the future. This entire feedback mechanism, labeled with the number "4" on the diagram, represents an iterative process. The LLM continuously learns and improves its ability to generate outputs that align with the preferences encoded or learned by the "Preference evaluator" by reinforcing desired behaviors and suppressing undesired ones. This iterative adjustment is central to making LLMs more helpful, harmless, and aligned with human intentions.](images/21939d28e7d57cc4e67db8ce4017f23c3c4934c94b3c4ed01cc2e83d65000dce.jpg)
Figure 12-23. Preference tuning methods update the LLM based on the evaluation score.

As always, we need many training examples. So can we automate the preference evaluation? Yes, we can by training a different model called a reward model.

# Automating Preference Evaluation Using Reward Models

To automate preference evaluation, we need a step before the preference-tuning step, namely to train a reward model, as shown in Figure 12-24.

![## Image Analysis: 2d83307b547b3f65cbf1506d8fba7ea3b9eb86cb12d3ac4e5f6374e309a4c1f1.jpg

**Conceptual Understanding:**
The image conceptually represents a two-phase fine-tuning pipeline for Large Language Models (LLMs) to integrate human preferences. The main purpose is to illustrate how an LLM, initially trained for instructions, is subsequently enhanced to align with preferences. This involves an explicit step of training a separate reward model as a precursor to the preference-based fine-tuning of the LLM itself. The key ideas communicated are the sequential nature of LLM fine-tuning, the distinct role of a reward model in capturing preferences, and the transformation of an LLM's capabilities from instruction-following to preference-aligned behavior.

**Content Interpretation:**
The image depicts a sequential pipeline for enhancing a Large Language Model (LLM) through preference-based fine-tuning. It shows the transformation of an LLM from an initial state, where it is already instruction-tuned, to a final state where it is preference-tuned. The core process involves two key steps: first, the training of a reward model, and second, the fine-tuning of the LLM itself using this reward model. The arrow labeled "Fine-tuning 2 (Preference)" explicitly connects these stages, indicating that the reward model's training precedes and informs the preference-based fine-tuning of the LLM. The distinct color coding (magenta for instruction-tuned, blue/magenta gradient for preference-tuned) visually reinforces the transformation of the LLM.

**Key Insights:**
The main takeaway from this image is that the process of achieving a preference-tuned Large Language Model (LLM) from an instruction-tuned one involves an intermediate, crucial step of training a reward model. This reward model is developed prior to and independently of the final preference-based fine-tuning of the LLM. The entire process is labeled as "Fine-tuning 2 (Preference)," implying that preference tuning is a distinct, subsequent stage to an initial instruction-tuning phase. The textual evidence "Instruction-tuned LLM" as the starting point, "Train reward model" as an intermediate step, and "Fine-tune LLM" under the "Fine-tuning 2 (Preference)" label, all leading to a "Preference-tuned LLM," robustly supports these insights. This highlights a common two-stage fine-tuning paradigm in LLM development, where a reward model is essential for aligning the LLM with human preferences.

**Document Context:**
This image directly supports the document's section "Automating Preference Evaluation Using Reward Models" and the accompanying text "Figure 12-24. We train a reward model before fine-tuning the LLM." It provides a clear visual representation of the workflow described, demonstrating the crucial step of training a reward model *before* the actual preference-based fine-tuning of the LLM. This illustrates the foundational methodology for integrating preference evaluation into the LLM development cycle, making the abstract concept concrete and understandable.

**Summary:**
The image illustrates a two-stage process for transitioning a Large Language Model (LLM) from an instruction-tuned state to a preference-tuned state. The process begins with an "Instruction-tuned LLM," represented by a magenta rectangular box with a speech bubble icon in the top right corner. The next step involves training a reward model, depicted by a green dashed-border rectangular box labeled "Train reward model." Following the training of the reward model, there is a second fine-tuning stage, indicated by a thick gray arrow labeled "Fine-tuning 2 (Preference)," which points to a red dashed-border rectangular box labeled "Fine-tune LLM." The final output of this process is a "Preference-tuned LLM," represented by a blue-to-magenta gradient rectangular box, also with a speech bubble icon in the top right corner. This diagram clearly shows the sequential steps, starting with an already instruction-tuned model, then training a reward model, and finally using that reward model to fine-tune the LLM for preferences.](images/2d83307b547b3f65cbf1506d8fba7ea3b9eb86cb12d3ac4e5f6374e309a4c1f1.jpg)
Figure 12-24. We train a reward model before fine-tuning the LLM.

Figure 12-25 shows that to create a reward model, we take a copy of the instructiontuned model and slightly change it so that instead of generating text, it now outputs a single score.

![## Image Analysis: 25a6e90c6763ab848bda82fa9b5026c67d0359fe5717098425620f2e45e71c56.jpg

**Conceptual Understanding:**
This image conceptually illustrates the architectural transformation of a Large Language Model (LLM) into a Reward Model. The main purpose is to show the specific modification required at the 'head' or output layer of an LLM to change its primary function from language generation/prediction to quality classification. The key idea communicated is that by replacing an LLM's original 'language modeling head' with a 'quality classification head', the LLM can be effectively re-purposed to act as a reward model for evaluating preferences or quality.

**Content Interpretation:**
The image depicts a system transformation process. It shows how an initial system, an "LLM" (Large Language Model), is modified to become a "Reward model". The key process involves altering the internal architecture of the LLM.

Specifically, the process starts with an LLM that contains an "LM head" (Language Modeling head). The transformation explicitly involves two actions: the "LM head" is "Remove"d from the LLM. Subsequently, a "Quality classification head" is "Add"ed to the model. The overarching process is labeled as "Tweak the LLM to become a reward model", indicating the purpose of these architectural changes. The final system, after these modifications, is the "Reward model", which now incorporates the "Quality classification head" instead of the original "LM head". This illustrates a method for adapting an LLM for tasks related to quality assessment or preference evaluation.

**Key Insights:**
1.  **Large Language Models are adaptable:** LLMs are not limited to their original language modeling tasks but can be re-purposed for other objectives by modifying their specific architectural components.
    *   **Textual Evidence:** The initial state is an "LLM" with an "LM head", which is then "Tweak[ed]... to become a reward model" by replacing its head.
2.  **Specific architectural modification for reward models:** The conversion of an LLM into a reward model entails a precise change: removing the original language modeling output layer and integrating a new output layer designed for quality classification.
    *   **Textual Evidence:** The diagram explicitly shows an arrow labeled "Remove" pointing from "LM head" and another arrow labeled "Add" pointing to "Quality classification head".
3.  **Function of a Reward Model:** A reward model, as derived from an LLM in this manner, is essentially an LLM configured to perform quality assessment or classification rather than language generation.
    *   **Textual Evidence:** The final state is explicitly labeled "Reward model", containing a "Quality classification head", which implies its function is to classify quality.

**Document Context:**
This image directly supports the document section "Automating Preference Evaluation Using Reward Models" by visually demonstrating the architectural modification required for this automation. It specifically illustrates the mechanism described in the accompanying text: "The LLM becomes a reward model by replacing its language modeling head with a quality classification head." The diagram provides a clear, step-by-step visual explanation of how an LLM's internal components are altered to re-purpose its function from language modeling to quality classification for reward generation, which is a foundational concept for the broader topic of automating preference evaluation.

**Summary:**
The diagram illustrates a fundamental architectural modification process for transforming a Large Language Model (LLM) into a Reward Model.

The process begins with an **LLM** (Large Language Model). A key component within this LLM is its **LM head** (Language Modeling head), which is responsible for the traditional language generation or prediction capabilities of the model.

To convert this LLM into a Reward Model, a specific **tweak** is performed. The instruction indicates: "Tweak the LLM to become a reward model". This transformation involves two distinct actions:

1.  **Removal of the original component:** The "LM head" is explicitly "Remove[d]" from the LLM architecture. This signifies that the LLM's original function for producing language tokens is no longer the primary focus.
2.  **Addition of a new component:** A new component, called a "Quality classification head", is "Add[ed]" to the model. This new head is designed to perform classification tasks, specifically for evaluating or classifying quality.

The result of this process is a **Reward model**. This newly configured model leverages the underlying capabilities of the LLM but re-purposes its output layer to perform quality classification, thus enabling it to function as a reward mechanism for automated preference evaluation. The diagram clearly shows the initial state (LLM with LM head), the transformation steps (remove LM head, add quality classification head), and the final state (Reward model with quality classification head).](images/25a6e90c6763ab848bda82fa9b5026c67d0359fe5717098425620f2e45e71c56.jpg)
Figure 12-25. The LLM becomes a reward model by replacing its language modeling head with a quality classification head.

# The Inputs and Outputs of a Reward Model

The way we expect this reward model to work is that we give it a prompt and a generation, and it outputs a single number indicating the preference/quality of that generation in response to that prompt. Figure 12-26 shows the reward model generating this single number.

![## Image Analysis: 4fc9e76d7f277b5ad177299b50f54b4a4bb537dd9626f511c00c982784c85cf3.jpg

**Conceptual Understanding:**
This image conceptually represents a quality assessment pipeline for generated text. Its main purpose is to illustrate how an AI-driven 'Reward model' processes a text 'Generation A', derived from an 'Input prompt 1', to assign a 'Completion quality score'. The diagram communicates the idea of automated content evaluation, where a numerical score (e.g., '4') indicates the perceived quality, ranging from 'Bad' (-6) to 'Good' (6), effectively quantifying the desirability or correctness of the generated output.

**Content Interpretation:**
This image displays a workflow diagram outlining the automatic assessment of generated text quality. It shows a sequence where an initial input prompt leads to a text generation, which is then evaluated by a specialized machine learning component. The core process involves feeding 'Generation A' into a 'Reward model' that utilizes a 'Quality classification head' to produce a numerical 'Completion quality score'. The significance of this score is further elucidated by an accompanying qualitative scale, where higher scores (like '6') correspond to 'Good' quality, and lower scores (like '-6') correspond to 'Bad' quality. The example score '4' suggests a good quality completion.

**Key Insights:**
The main takeaway from this image is the systematic approach to evaluating the quality of text generated by an AI model. Key insights include: 1. Text generation starts with an 'Input prompt 1' and produces 'Generation A'. 2. A 'Reward model', specifically its 'Quality classification head', is responsible for assessing this generation. 3. The output is a numerical 'Completion quality score', such as '4'. 4. This score is interpretable on a continuous scale, explicitly marked from '-6' (Bad) to '6' (Good), providing a clear qualitative understanding of the numerical output. This process highlights how AI models can be equipped with an intrinsic mechanism for quality control or how their outputs can be objectively measured.

**Document Context:**
This image, described by the document context as 'The Inputs and Outputs of a Reward Model' and clarifying how a 'reward model trained on human preference' is used 'to generate the completion quality score', directly illustrates a fundamental mechanism in natural language processing and artificial intelligence for evaluating the performance and output quality of generative models. It visually explains the operational flow from an initial prompt and its generated response through a reward model to obtain a quantifiable quality metric, which is crucial for training and fine-tuning such models. It fits into a broader discussion on how AI systems can self-evaluate or be evaluated for the quality of their creative or informational outputs.

**Summary:**
This diagram illustrates a process for evaluating the quality of a text generation using a reward model. The process begins with an 'Input prompt 1' which leads to 'Generation A'. This generated text, 'Generation A', is then fed into a 'Reward model'. Within the 'Reward model', there is a component labeled 'Quality classification head'. The output of this reward model is a 'Completion quality score', exemplified here as '4'. This score is assessed against a vertical quality scale ranging from -6 to 6. The top of the scale (6) is labeled 'Good' in green text, while the bottom of the scale (-6) is labeled 'Bad' in red text. Intermediate points on the scale are marked '3', '0', and '-3'. A purple triangular pointer indicates a score on this scale, which appears to be between 0 and 3, illustrating the range for the quality score.](images/4fc9e76d7f277b5ad177299b50f54b4a4bb537dd9626f511c00c982784c85cf3.jpg)
Figure 12-26. Use a reward model trained on human preference to generate the comple‐ tion quality score.

# Training a Reward Model

We cannot directly use the reward model. It needs to first be trained to properly score generations. So let’s get a preference dataset that the model can learn from.

# Reward model training dataset

One common shape for preference datasets is for a training example to have a prompt, with one accepted generation and one rejected generation. (Nuance: it’s not always a good versus bad generation; it can be that the two generations are both good, but that one is better than the other). Figure 12-27 shows an example preference training set with two training examples.

![## Image Analysis: e191e2ed73d351836e2f6a239a8a3dcced4c33c46c84bfe1c52f4dabf6caa1b0.jpg

**Conceptual Understanding:**
This image conceptually represents the data architecture for a preference tuning dataset, which is a key component in reward model training, particularly in Reinforcement Learning from Human Feedback (RLHF). Its main purpose is to demonstrate how individual data points are structured, showcasing an input prompt paired with both a 'human-preferred' (accepted) generation and a 'human-dispreferred' (rejected) generation. The core message conveyed is the comparative nature of this dataset, where a model learns by being presented with contrasting examples of desirable and undesirable outputs for the same input, thereby encoding human preferences directly into the training data.

**Content Interpretation:**
The image illustrates the fundamental structure of a preference tuning dataset. It shows how for a given input prompt, two different generations are presented: one explicitly labeled as 'Accepted' and another as 'Rejected'. This pairing provides a clear contrast for training models to understand and align with human preferences. The visual organization into examples (1 and 2) and the clear distinction between 'Accepted' and 'Rejected' generations, each originating from the same 'Input prompt', convey the comparative nature of the data points.

**Key Insights:**
The main takeaway from this image is that preference tuning datasets are designed around a comparative framework. For any given input, there will be at least two distinct outputs: one explicitly chosen as preferred ('Accepted') and another explicitly chosen as not preferred ('Rejected'). This structured comparison, evidenced by 'Generation 1.A' being 'Accepted' and 'Generation 1.B' being 'Rejected' for 'Input prompt 1', and similarly for 'Input prompt 2' with 'Generation 2.A' and 'Generation 2.B', is crucial for training AI models. It enables models to learn the nuances of human preferences and improve their ability to generate outputs that are considered desirable, rather than simply accurate or fluent. The repetition across 'Example # 1' and 'Example # 2' reinforces this consistent data organization principle.

**Document Context:**
This image directly illustrates the statement provided in the document context: 'Preference tuning datasets are often made up of prompts with accepted and rejected generations.' It serves as a visual example of how a 'Reward model training dataset' is structured, specifically highlighting the paired accepted and rejected generations for each input prompt. The diagram provides a concrete visual representation of the abstract concept discussed in the text, enhancing understanding of the data format used for training models via human preferences.

**Summary:**
The image displays a structured dataset format commonly used for preference tuning in machine learning, specifically for reward model training. It consists of two examples, each illustrating how an input prompt is paired with both an 'Accepted' and a 'Rejected' generation. 

For 'Example # 1':
An 'Input prompt 1' is shown. This prompt leads to two distinct generations: 'Generation 1.A' which is labeled as 'Accepted', and 'Generation 1.B' which is labeled as 'Rejected'. Both 'Generation 1.A' and 'Generation 1.B' are associated with the same 'Input prompt 1'.

Similarly, for 'Example # 2':
An 'Input prompt 2' is presented. Corresponding to this prompt are two generations: 'Generation 2.A' which is designated as 'Accepted', and 'Generation 2.B' which is designated as 'Rejected'. Both 'Generation 2.A' and 'Generation 2.B' originate from the same 'Input prompt 2'.

This structure clearly demonstrates the comparative nature of preference datasets, where models learn by distinguishing between preferred and non-preferred outputs for identical inputs.](images/e191e2ed73d351836e2f6a239a8a3dcced4c33c46c84bfe1c52f4dabf6caa1b0.jpg)
Figure 12-27. Preference tuning datasets are often made up of prompts with accepted and rejected generations.

One way to generate preference data is to present a prompt to the LLM and have it generate two different generations. As shown in Figure 12-28, we can ask human labelers which of the two they prefer.

![## Image Analysis: ccedf5de4b7f11e85ba395d5480b11d45e72749ef28a60b6a6bd31ae0a7dd74e.jpg

**Conceptual Understanding:**
The image conceptually represents a human preference data collection pipeline for Large Language Models. Its main purpose is to illustrate how two different outputs from an LLM, generated from the same input prompt, are presented to a human for a comparative judgment. The core idea being communicated is the process of gathering explicit human feedback (preferences) to evaluate and ultimately improve the quality and alignment of LLM generations with human values and criteria.

**Content Interpretation:**
The image depicts a core data collection process for training a reward model, specifically for a Large Language Model (LLM). It demonstrates how a single input prompt is used to elicit two different responses from an LLM. These two responses, labeled "Generation A" and "Generation B", are then presented to a human for a preference judgment. The presence of the question "Which do you prefer?" and the options "A" and "B" clearly indicates a human-in-the-loop evaluation step aimed at gathering comparative feedback. This process is fundamental for creating a dataset that teaches a reward model to align with human preferences, which is a key component in reinforcement learning from human feedback (RLHF) methodologies.

**Key Insights:**
The main takeaway from this image is the critical role of human feedback in evaluating and improving Large Language Models. Specifically, it illustrates a method for collecting preference data, which is essential for training reward models. The process highlights that an LLM can produce multiple diverse responses ("Generation A", "Generation B") for a single prompt ("Input prompt 1"). The act of asking "Which do you prefer?" and providing options "A" and "B" demonstrates a systematic approach to capture human judgments. This data can then be used to guide the LLM's behavior towards outputs that are more aligned with human expectations and quality standards, forming the basis for reward-based learning.

**Document Context:**
This image directly supports the document's section on "Reward model training dataset" and the accompanying text: "Figure 12-28. Output two generations and ask a human labeler which one they prefer." It visually explains the practical step of how data for a reward model is collected. By detailing the input prompt, the LLM's dual generation, and the subsequent human preference selection, the image clarifies the methodology for acquiring the comparative human feedback necessary to train models that can predict which output a human would prefer.

**Summary:**
This diagram illustrates the process of generating two distinct responses from a Large Language Model (LLM) based on a single input prompt, and subsequently soliciting human preference between these two generations. The workflow begins with an initial input, labeled "Input prompt 1", which is fed into the "LLM". The LLM then processes this prompt and produces two separate outputs, identified as "Generation A" and "Generation B". Following the generation of these two outputs, a human evaluation step is introduced. This step presents the question "Which do you prefer?" to a human labeler, offering two choices: "A" (corresponding to Generation A) and "B" (corresponding to Generation B). The human labeler is expected to select one of these options, thereby indicating their preference. The process is clear, moving from a single input to a model, generating two comparative outputs, and concluding with a human decision point to select a preferred output.](images/ccedf5de4b7f11e85ba395d5480b11d45e72749ef28a60b6a6bd31ae0a7dd74e.jpg)
Figure 12-28. Output two generations and ask a human labeler which one they prefer.

# Reward model training step

Now that we have the preference training dataset, we can proceed to train the reward model.

A simple step is that we use the reward model to:

1. Score the accepted generation   
2. Score the rejected generation

![## Image Analysis: e3f2d9160b96c77c802299adb5651b6eb575535029a6435252ada988025f0548.jpg

**Conceptual Understanding:**
This image conceptually represents the training objective for a 'Reward model'. Its main purpose is to demonstrate how a reward model is trained to differentiate between good ('Accepted') and bad ('Rejected') generations by assigning them distinct quality scores. The image communicates the idea that for a given input prompt, different generated outputs will be evaluated, and the model's goal during training is to ensure that the 'accepted' generation consistently receives a higher 'completion quality score' than the 'rejected' generation.

**Content Interpretation:**
This image illustrates a core aspect of training a reward model, likely within a machine learning context such as Reinforcement Learning from Human Feedback (RLHF). It shows the mechanism by which the model learns to differentiate between 'accepted' (higher quality) and 'rejected' (lower quality) text generations. The process demonstrates a comparative evaluation, where different generations stemming from the same input prompt are assessed by the reward model to assign a 'completion quality score'. The key intent is to train the model to output a higher score for the desired generation compared to the undesired one.

**Key Insights:**
The main takeaways from this image are: 1. **Reward Model Function:** Reward models are utilized to evaluate and assign quality scores to different generated outputs (e.g., text, code). 2. **Comparative Training:** The training of a reward model often involves comparing a preferred ('Accepted') generation against a dispreferred ('Rejected') generation stemming from the same initial prompt. 3. **Scoring Differential:** The core 'Training objective' is to ensure that the 'Accepted score should be larger than rejected score'. This establishes a clear quality hierarchy that the model learns to replicate. 4. **Quality Quantification:** Quality is quantified through a 'Completion quality score' (e.g., 3 for accepted, -1 for rejected), indicating the model's learned preference. These insights are directly supported by the text: 'Reward model', 'Quality classification head', 'Completion quality score', the numerical scores '3' and '-1', and the explicit 'Training objective: Accepted score should be larger than rejected score'.

**Document Context:**
This image directly supports the document's discussion on the 'Reward model training step'. It visually elucidates the training objective mentioned in the surrounding text, which states, 'Figure 12-29 shows the training objective: to ensure the accepted generation has a higher score than the rejected generation. Figure 12-29. The reward model aims to evaluate the quality scores of generations in response to a prompt.' The diagram provides a concrete example of how this objective is realized by demonstrating the scoring mechanism and the desired outcome (accepted score > rejected score), thereby making the abstract training goal tangible for the reader.

**Summary:**
This image illustrates the training objective for a reward model, which is designed to evaluate the quality of generations in response to a given prompt. The diagram is divided into two main conceptual paths: 'Accepted' and 'Rejected'. Both paths begin with the same 'Input prompt 1'. For the 'Accepted' path, this prompt leads to 'Generation 1.A'. For the 'Rejected' path, it leads to 'Generation 1.B'. Both 'Generation 1.A' and 'Generation 1.B' are then fed into a central 'Reward model', specifically processed by its 'Quality classification head'. The output of the reward model for each generation is a 'Completion quality score'. In this example, the 'Accepted' generation ('Generation 1.A') receives a score of '3', while the 'Rejected' generation ('Generation 1.B') receives a score of '-1'. The overarching 'Training objective', explicitly stated at the bottom of the diagram, is that the 'Accepted score should be larger than rejected score'. This visually demonstrates the goal of training the reward model to assign higher quality scores to preferred outputs and lower scores to undesirable ones.](images/e3f2d9160b96c77c802299adb5651b6eb575535029a6435252ada988025f0548.jpg)
Figure 12-29 shows the training objective: to ensure the accepted generation has a higher score than the rejected generation.   
Figure 12-29. The reward model aims to evaluate the quality scores of generations in response to a prompt.

When we combine everything together as shown in Figure 12-30, we get the three stages to preference tuning:

1. Collect preference data   
2. Train a reward model   
3. Use the reward model to fine-tune the LLM (operating as the preference evaluator)

![## Image Analysis: 7c51557a871cacc654682d702a1c6ca8a032b3f63a0a21bfafac948e9f7aa64f.jpg

**Conceptual Understanding:**
The image conceptually represents the workflow for 'preference tuning' a Large Language Model (LLM). Its main purpose is to illustrate the sequential stages involved in developing and utilizing a 'reward model' to enhance an LLM's ability to align with human preferences. The core idea communicated is that an initially 'instruction-tuned' LLM undergoes a process of data collection, reward model training, and subsequent fine-tuning guided by this reward model, resulting in a 'preference-tuned' LLM that better reflects desired characteristics based on human feedback.

**Content Interpretation:**
The image depicts a three-stage process for fine-tuning a Large Language Model (LLM) using human preferences. This process starts with an existing 'Instruction-tuned LLM'. The first stage, 'Collect preference data', involves gathering human feedback or preferences. This data is then used in the second stage, 'Train reward model', to develop a model capable of evaluating the quality of LLM outputs based on these preferences. The 'Train reward model' step 'Produced' a 'Reward model'. This 'Reward model' is subsequently 'Used to fine-tune the LLM' in the third stage. This third stage, labeled 'Fine-tuning 2 (Preference)', involves the actual 'Fine-tune LLM' process, where the reward model's feedback guides the LLM's adjustments. The ultimate goal is to transform the initial 'Instruction-tuned LLM' into a 'Preference-tuned LLM', making its outputs more aligned with desired human preferences. The dashed line for 'Used to fine-tune the LLM' suggests that the reward model's role is specifically to guide this fine-tuning.

**Key Insights:**
The main takeaway from this image is the systematic, multi-step approach required for preference tuning of an LLM. Key insights include: 1. Preference tuning begins with an 'Instruction-tuned LLM'. 2. It necessitates an explicit 'Collect preference data' stage to gather human feedback. 3. A crucial intermediate step is to 'Train reward model' from this preference data, which 'Produced' a distinct 'Reward model'. 4. This 'Reward model' is not the final LLM but an evaluation component 'Used to fine-tune the LLM'. 5. The actual 'Fine-tuning 2 (Preference)' of the LLM uses the insights from the reward model. 6. The ultimate output is a 'Preference-tuned LLM', indicating a significant transformation and improvement in alignment with human preferences.

**Document Context:**
This image directly supports the document's narrative on 'Reward model training step' and expands upon Figure 12-30, which outlines the three stages of preference tuning: collecting preference data, training a reward model, and fine-tuning the LLM. It visually breaks down these stages, showing the input (Instruction-tuned LLM), intermediate steps (Collect preference data, Train reward model, Reward model), the specific fine-tuning operation ('Fine-tuning 2 (Preference)'), and the final output (Preference-tuned LLM). The diagram clarifies the relationship between the reward model and the LLM fine-tuning process, specifically how the reward model is produced and then utilized.

**Summary:**
The image illustrates the three stages of preference tuning for a Large Language Model (LLM), transitioning it from an 'Instruction-tuned' state to a 'Preference-tuned' state. The process begins with an 'Instruction-tuned LLM'. The first stage involves collecting 'preference data', followed by training a 'reward model'. The 'Train reward model' step produces a standalone 'Reward model'. This reward model is then 'Used to fine-tune the LLM'. Concurrently, the 'Train reward model' step directly feeds into 'Fine-tuning 2 (Preference)', which is the process of 'Fine-tune LLM'. The ultimate outcome of this entire sequence is a 'Preference-tuned LLM'. Each step is clearly delineated, showing the flow of data and models.](images/7c51557a871cacc654682d702a1c6ca8a032b3f63a0a21bfafac948e9f7aa64f.jpg)
Figure 12-30. The three stages of preference tuning: collecting preference data, training a reward model, and finally fine-tuning the LLM.

Reward models are an excellent idea that can be further extended and developed. Llama 2, for example, trains two reward models: one that scores helpfulness and another that scores safety (Figure 12-31).

![## Image Analysis: 4531625577c074e84d859e0c8f5ba23fa72de8903a0dcee48dbfe4b28693e516.jpg

**Conceptual Understanding:**
This image conceptually represents a workflow for training and fine-tuning a Large Language Model (LLM) to incorporate human preferences, evolving it from an instruction-tuned state to a preference-tuned state. The main purpose is to illustrate how specialized 'reward models' for aspects like 'helpfulness' and 'safety' are developed from an initial LLM and then used to guide a subsequent 'preference fine-tuning' step. The image conveys the idea of a multi-stage, iterative process in LLM development, highlighting the critical role of feedback mechanisms (reward models) in aligning the LLM's behavior with desired human values or performance criteria (preferences).

**Content Interpretation:**
This image details a process workflow for refining a Large Language Model (LLM) through a sequence of training and fine-tuning steps, specifically incorporating the use of multiple reward models. The core processes shown are: 1. Initial LLM state: An "Instruction-tuned LLM" serves as the starting point. 2. Reward Model Training: This instruction-tuned LLM is utilized to "Train reward model." 3. Reward Model Production: This training results in two distinct reward models: "Helpfulness RM" and "Safety RM," labeled as "Reward model #1" and "Reward model #2" respectively. 4. Preference Fine-tuning: Both the original "Instruction-tuned LLM" and the produced reward models are fed into a process to "Fine-tune LLM." The connection from the instruction-tuned LLM is explicitly labeled "Fine-tuning 2 (Preference)," indicating a specific type of fine-tuning. The connection from the Helpfulness and Safety RMs is labeled "Used to fine-tune the LLM," highlighting their role as inputs for this fine-tuning step. 5. Final LLM State: The outcome of this fine-tuning process is a "Preference-tuned LLM." The image illustrates how different reward models contribute to different aspects (helpfulness and safety) of the fine-tuning process for a large language model.

**Key Insights:**
The main takeaways from this image are: 1. LLM Development Stages: The process of developing a sophisticated LLM involves distinct stages, moving from an "Instruction-tuned" state to a "Preference-tuned" state. 2. Role of Reward Models: Reward models are crucial components in the fine-tuning process, specifically for embedding preferences into the LLM. The text "Train reward model" and "Used to fine-tune the LLM" directly indicate this. 3. Multiple Reward Models for Specific Goals: The diagram explicitly shows two reward models, "Helpfulness RM" and "Safety RM," indicating that different aspects or desired behaviors (e.g., helpfulness, safety) can be independently modeled and then integrated into the LLM's fine-tuning. This supports the idea of a multi-faceted reward system. 4. Preference-based Fine-tuning: The label "Fine-tuning 2 (Preference)" emphasizes that a specific fine-tuning phase is dedicated to incorporating preferences, which are likely derived from human feedback or other preference data. This fine-tuning uses the outputs of the specialized reward models.

**Document Context:**
This image directly supports the document's section on "Reward model training step" by visually representing the process through which reward models are developed and subsequently used to refine an LLM. The accompanying text, "Figure 12-31. We can use multiple reward models to perform the scoring," further emphasizes the diagram's point about leveraging multiple reward models (Helpfulness RM, Safety RM) to achieve a "Preference-tuned LLM." The image serves as a crucial visual aid for understanding the multi-stage fine-tuning approach, clearly showing the transition from an instruction-tuned LLM to a preference-tuned LLM via the integration of specialized reward mechanisms.

**Summary:**
This image illustrates a two-stage process for developing a Large Language Model (LLM), starting from an instruction-tuned model and culminating in a preference-tuned model, with a specific focus on the role of multiple reward models. The process begins with an "Instruction-tuned" LLM. This instruction-tuned LLM is used to "Train reward model." The training of the reward model "Produced" two distinct reward models: "Reward model #1: Helpfulness RM" and "Reward model #2: Safety RM." These two reward models, "Helpfulness RM" and "Safety RM," are then "Used to fine-tune the LLM" in a subsequent step. Concurrently, there is a direct connection from the initial "Instruction-tuned LLM" to the "Fine-tune LLM" step, labeled "Fine-tuning 2 (Preference)." Both the outputs of the reward models and the direct instruction-tuned LLM contribute to the "Fine-tune LLM" process. The ultimate output of this entire workflow is a "Preference-tuned" LLM. The diagram clearly shows that the instruction-tuned LLM undergoes a 'Fine-tuning 2 (Preference)' stage, where it leverages the outputs of specialized reward models (Helpfulness and Safety) to become a preference-tuned LLM. The visual elements, such as the solid and dotted boxes and arrows, delineate the distinct stages and the flow of information or models through the process.](images/4531625577c074e84d859e0c8f5ba23fa72de8903a0dcee48dbfe4b28693e516.jpg)
Figure 12-31. We can use multiple reward models to perform the scoring.

A common method to fine-tune the LLM with the trained reward model is Proximal Policy Optimization (PPO). PPO is a popular reinforcement technique that optimizes the instruction-tuned LLM by making sure that the LLM does not deviate too much from the expected rewards.21 It was even used to train the original ChatGPT released in November 2022.

# Training No Reward Model

A disadvantage of PPO is that it is a complex method that needs to train at least two models, the reward model and the LLM, which can be more costly than perhaps necessary.

Direct Preference Optimization (DPO) is an alternative to PPO and does away with the reinforcement-based learning procedure.22 Instead of using the reward model to judge the quality of a generation, we let the LLM itself do that. As illustrated in Figure 12-32, we use a copy of the LLM as the reference model to judge the shift between the reference and trainable model in the quality of the accepted generation and rejected generation.

![## Image Analysis: 9168d5f45593ba621566b38ea2c9564a2e2d7e2a18014f9c6348bbd2d6df65d8.jpg

**Conceptual Understanding:**
This image conceptually illustrates a training methodology for Large Language Models (LLMs) that uses a self-referential comparison mechanism for preference tuning, rather than relying on an external reward model. The main purpose is to demonstrate how a trainable LLM can learn to generate more preferred outputs by comparing its own output scores against those of a frozen, reference version of itself, calculating a 'shift' in generation preferences, and then adjusting its parameters based on this feedback loop. Key concepts communicated include iterative model refinement, preference-based learning, and an efficient approach to fine-tuning LLMs by leveraging their internal scoring capabilities.

**Content Interpretation:**
The image shows a process for training or fine-tuning a Large Language Model (LLM) using a preference-based learning approach, where the LLM itself functions as a pseudo-reward model. It illustrates how a "Trainable model" is iteratively adjusted by comparing its output scores for accepted and rejected generations against those of a "Reference model (Frozen)." The process highlights the generation of two types of outputs (rejected and accepted) by both models, their subsequent scoring by their respective LLMs, and the calculation of a "shift" based on these scores. This calculated "shift" then serves as feedback to update the trainable model, aiming to increase the likelihood of accepted generations and decrease the likelihood of rejected generations.

**Key Insights:**
1. **Self-Supervision/Pseudo-Reward:** The diagram demonstrates a method where the LLM itself acts as the 'reward model' by generating scores for its own outputs (rejected and accepted), eliminating the need for an external reward model. This is evidenced by the 'LLM' boxes producing both 'Rejected score' and 'Accepted score' for both the reference and trainable models, and the document context stating, 'Use the LLM itself as the reward model'.
2. **Comparative Tuning:** Training is achieved through a comparison between a dynamic 'Trainable model' and a static 'Reference model (Frozen)'. The 'Calculate shift in: - Rejected generation - Accepted generation' step explicitly highlights this comparative approach.
3. **Preference Alignment:** The primary goal of the training loop is to steer the 'Trainable model' towards producing more desired (accepted) outputs and fewer undesired (rejected) outputs. This is directly stated by the feedback arrows labeled 'Increase likelihood of accepted generation' and 'Increase likelihood of rejected generation' influencing the trainable LLM.

**Document Context:**
This image is highly relevant to the document's narrative on "Training No Reward Model" as explicitly stated in the surrounding text "Figure 12-32. Use the LLM itself as the reward model by comparing the output of a frozen model with the trainable model." It visually demonstrates the conceptual framework for how an LLM can be fine-tuned through self-comparison against a frozen version of itself, effectively removing the need for a separate, explicit reward model. It fits into a broader discussion about alternative and efficient methods for LLM preference tuning and reinforcement learning from human feedback (RLHF) without direct human labeling at every step.

**Summary:**
This diagram illustrates a method for training a Large Language Model (LLM) without an external reward model, instead leveraging the LLM itself to guide its own improvement through a process called "preference tuning." The diagram presents two main sections: a "Reference model (Frozen)" and a "Trainable model (To be preference tuned)." For the "Reference model (Frozen)", an "Input prompt" is provided, leading to two parallel "Generation" steps. These generations are fed into the "LLM" component, which then assigns a "Rejected score" and an "Accepted score." Since this model is "Frozen," these scores serve as a static baseline. Similarly, for the "Trainable model (To be preference tuned)", an "Input prompt" is provided, leading to two parallel "Generation" steps, which are fed into its "LLM" component, also resulting in a "Rejected score" and an "Accepted score." This trainable model is the one that will be adjusted. The core of the tuning process involves comparing the "Rejected score" from both models and the "Accepted score" from both models. This comparison is used to "Calculate shift in: - Rejected generation - Accepted generation." This calculated "shift" provides feedback to the "Trainable model" LLM. The objective of this feedback is to "Increase likelihood of accepted generation" (meaning the model should produce more outputs deemed 'accepted') and to "Increase likelihood of rejected generation" (meaning the model should produce fewer outputs deemed 'rejected'). This iterative process adjusts the trainable LLM's parameters to align its output preferences more closely with the desired behavior, as guided by the comparison against the frozen reference model.](images/9168d5f45593ba621566b38ea2c9564a2e2d7e2a18014f9c6348bbd2d6df65d8.jpg)
Figure 12-32. Use the LLM itself as the reward model by comparing the output of a frozen model with the trainable model.

By calculating this shift during training, we can optimize the likelihood of accepted generations over rejected generations by tracking the difference in the reference model and the trainable model.

To calculate this shift and its related scores, the log probabilities of the rejected generations and accepted generations are extracted from both models. As illustrated in Figure 12-33, this process is performed at a token level where the probabilities are combined to calculate the shift between the reference and trainable models.

![## Image Analysis: 1e40df169bd737e7c59f1a7ba78a660d429d1b93faf62bdf4af0e78c884f6bb1.jpg

**Conceptual Understanding:**
This image conceptually represents a process for evaluating and potentially training a large language model (LLM) by analyzing its response to a 'Rejected generation'. The main purpose is to demonstrate how a 'Trainable model' can be tuned or optimized by comparing its token probabilities for a rejected output against a 'Reference model'. The core idea is to measure the 'shift' in how the trainable model processes undesirable outputs, using token-level probabilities as the metric, thereby implicitly guiding the model away from such generations.

**Content Interpretation:**
The image depicts a method for evaluating a 'Rejected generation' by comparing how a 'Reference model' (frozen) and a 'Trainable model' (preference tuned) assign token probabilities to it. The process shows that the initial 'Prompt' and the 'Rejected generation' serve as inputs to both model types. The core of the process is the calculation of 'token probability' for a given output sequence (e.g., 'I have no idea !'), which is then aggregated to form a 'Rejected score'. The ultimate goal is to 'Calculate shift in rejected scores', implying an optimization objective where the trainable model's scores for rejected generations are compared against a baseline established by the reference model. This system aims to understand and potentially reduce the likelihood of models generating undesirable or 'rejected' outputs by analyzing the probability shifts.

**Key Insights:**
The main takeaway is that evaluating and improving large language models, particularly in avoiding undesirable outputs, can involve comparing the token probabilities assigned by different model states. Specifically, a 'Reference model' (frozen) acts as a baseline, while a 'Trainable model' (to be preference tuned) is the subject of optimization. The process involves: 1. Inputting a 'Prompt' and a 'Rejected generation' into both models. 2. Calculating the 'token probability' for each token in the rejected generation for both models. 3. Aggregating these token probabilities to form a 'Rejected score'. 4. The critical insight for training is to 'Calculate shift in rejected scores', suggesting that the objective is to minimize or alter this shift to make the trainable model less likely to generate (or assign high probability to) 'rejected' outputs. The textual evidence like 'Reference model (Frozen)', 'Trainable model (To be preference tuned)', 'Calculate token probability (Per model)', 'Rejected score is the aggregation of token probabilities.', and 'Calculate shift in rejected scores' all support the understanding that this is a comparison and optimization mechanism based on probabilistic token generation for undesirable outputs.

**Document Context:**
This image directly supports the document's section on 'Training No Reward Model' by visually explaining the mechanism behind calculating scores for rejected generations. The accompanying text states, 'Scores are calculated by taking the probabilities of generation on a token level. The shift in probabilities between the reference model and the trainable model is optimized.' The diagram perfectly illustrates this, showing the token-level probability calculation, the aggregation into a rejected score, and the final calculation of the shift between the two models. This visual reinforces how a model might be implicitly trained against producing undesirable outputs without an explicit reward function, by observing the 'shift' in how a 'Trainable model' treats a 'Rejected generation' compared to a 'Reference model'. The mention of 'preference tuned' for the trainable model further aligns with methods of guiding model behavior based on desired outcomes.

**Summary:**
The image illustrates a process for calculating and comparing 'rejected scores' between a 'Reference model' (Frozen) and a 'Trainable model' (To be preference tuned) based on token probabilities of a 'Rejected generation'. The process begins with a 'Prompt' (comprising tokens 'What', 'are', 'LLMs', '?') and a 'Rejected generation' (comprising tokens 'I', 'got', 'no', 'clue', '!'). Both the 'Prompt' and the 'Rejected generation' are fed into two distinct Large Language Models (LLMs): a 'Reference model' which is explicitly labeled as '(Frozen)', and a 'Trainable model' which is labeled as '(To be preference tuned)'. Following this, a crucial step involves calculating the 'token probability' for each token, and this calculation is performed '(Per model)'. Below this step, the output tokens are shown as 'I', 'have', 'no', 'idea', '!' each with a corresponding bar chart representing its probability distribution. An annotation clarifies that the 'Rejected score is the aggregation of token probabilities.' The final step in the process is to 'Calculate shift in rejected scores', indicating a comparison or difference between the scores derived from the Reference and Trainable models for the rejected generation.](images/1e40df169bd737e7c59f1a7ba78a660d429d1b93faf62bdf4af0e78c884f6bb1.jpg)
Figure 12-33. Scores are calculated by taking the probabilities of generation on a token level. The shift in probabilities between the reference model and the trainable model is optimized. The accepted generation follows the same procedure.

Using these scores, we can optimize the parameters of the trainable model to be more confident of generating the accepted generations and less confident of generating the rejected generations. Compared to PPO, the authors found DPO to be more stable during training and more accurate. Due to its stability, we will be using it as our primary model for preference tuning our previously instruction-tuned model.

# Preference Tuning with DPO

When we use the Hugging Face stack, preference tuning is eerily similar to the instruction tuning we covered before with some slight differences. We will still be using TinyLlama but this time an instruction-tuned version that was first trained using full fine-tuning and then further aligned with DPO. Compared to our initial instruction-tuned model, this LLM was trained on much larger datasets.

In this section, we will demonstrate how you can further align this model using DPO with reward-based datasets.

# Templating Alignment Data

We will use a dataset that for each prompt contains an accepted generation and a rejected generation. This dataset was in part generated by ChatGPT with scores on which output should be accepted and which rejected:

from datasets import load_dataset   
def format_prompt(example): """Format the prompt to using the <|user|> template TinyLLama is using""" # Format answers system $=$ "<|system|>\n" $^ +$ example["system"] + "</s>\n" prompt $=$ "<|user|>\n" $^ +$ example["input"] $^ +$ "</s>\n<|assistant|>\n" chosen $=$ example["chosen"] $^ +$ "</s>\n" rejected $=$ example["rejected"] $^ +$ "</s>\n" return { "prompt": system $^ +$ prompt, "chosen": chosen, "rejected": rejected, }   
# Apply formatting to the dataset and select relatively short answers   
dpo_dataset $=$ load_dataset( "argilla/distilabel-intel-orca-dpo-pairs", split $: =$ "train"   
)   
dpo_dataset $=$ dpo_dataset.filter( lambda r: r["status"] $\downarrow =$ "tie" and r["chosen_score"] $\scriptstyle > = 8$ and not r["in_gsm8k_train"]   
)   
dpo_dataset $=$ dpo_dataset.map( format_prompt, remove_columns $: =$ dpo_dataset.column_names   
)   
dpo_dataset

Note that we apply additional filtering to further reduce the size of the data to roughly 6,000 examples from the original 13,000 examples.

# Model Quantization

We load our base model and load it with the LoRA we created previously. As before, we quantize the model to reduce the necessary VRAM for training:

from peft import AutoPeftModelForCausalLM   
from transformers import BitsAndBytesConfig, AutoTokenizer   
# 4-bit quantization configuration - Q in QLoRA   
bnb_config $=$ BitsAndBytesConfig( load_in_4bit=True, # Use 4-bit precision model loading

bnb_4bit_quant_type $\ast =$ "nf4", # Quantization type bnb_4bit_compute_dtype $\Bumpeq$ "float16", # Compute dtype bnb_4bit_use_double_quant=True, # Apply nested quantization )

# Merge LoRA and base model   
model $=$ AutoPeftModelForCausalLM.from_pretrained( "TinyLlama-1.1B-qlora", low_cpu_mem_usage=True, device_map="auto", quantization_config=bnb_config,   
)   
merged_model $=$ model.merge_and_unload()   
# Load LLaMA tokenizer   
model_name $=$ "TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T"   
tokenizer $=$ AutoTokenizer.from_pretrained(model_name, trust_remote_code $=$ True)   
tokenizer.pad_token $=$ "<PAD>"   
tokenizer.padding_side $=$ "left"

Next, we use the same LoRA configuration as before to perform the DPO training:

from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

# Prepare LoRA configuration   
peft_config $=$ LoraConfig( lora_alpha $\begin{array} { r l } { \mathbf { \Psi } } & { { } = \mathbf { \Psi } } \end{array}$ , # LoRA Scaling lora_dropout $= 0 . 1$ , # Dropout for LoRA Layers ${ \sf \Gamma } \Gamma = { \mathrm {  ~ \Gamma ~ } }$ , # Rank bias $=$ "none", task_type $\ast =$ "CAUSAL_LM", target_modules $=$ # Layers to target ["k_proj", "gate_proj", "v_proj", "up_proj", "q_proj", "o_proj",   
"down_proj"]   
)

# prepare model for training model $=$ prepare_model_for_kbit_training(model) model $=$ get_peft_model(model, peft_config)

# Training Configuration

For the sake of simplicity, we will use the same training arguments as we did before with one difference. Instead of running for a single epoch (which can take up to two hours), we run for 200 steps instead for illustration purposes. Moreover, we added the warmup_ratio parameter, which increases the learning rate from 0 to the learning_rate value we set for the first $1 0 \%$ of steps. By maintaining a small learning rate at the start (i.e., warmup period), we allow the model to adjust to the data before applying larger learning rates, therefore avoiding harmful divergence:

from trl import DPOConfig output_dir $=$ "./results"

# Training arguments   
training_arguments $=$ DPOConfig( output_dir $=$ output_dir, per_device_train_batch_size $^ { = 2 }$ , gradient_accumulation_steps $\mathbf { \Psi } = \mathbf { \Psi }$ , optim="paged_adamw_32bit", learning_rate $\iota =$ 1e-5, lr_scheduler_type $\Bumpeq$ "cosine", max_steps $\begin{array} { r l } { \mathbf { \Psi } } & { { } = } \end{array} .$ , logging_steps $\begin{array} { r l } { \mathit { \Pi } } & { { } = } \\ { \mathit { \Pi } } & { { } = } \end{array} .$ , fp16=True, gradient_checkpointing=True, warmup_ratio=0.1   
)

# Training

Now that we have prepared all our models and parameters, we can start fine-tuning our model:

from trl import DPOTrainer

# Create DPO trainer   
dpo_trainer $=$ DPOTrainer( model, args $=$ training_arguments, train_dataset=dpo_dataset, tokenizer=tokenizer, peft_config=peft_config, beta=0.1, max_prompt_length $1 = 5 1 2$ , max_length=512,   
)

# Fine-tune model with DPO dpo_trainer.train()

# Save adapter dpo_trainer.model.save_pretrained("TinyLlama-1.1B-dpo-qlora")

We have created a second adapter. To merge both adapters, we iteratively merge the adapters with the base model:

from peft import PeftModel

# Merge LoRA and base model   
model $=$ AutoPeftModelForCausalLM.from_pretrained( "TinyLlama-1.1B-qlora", low_cpu_mem_usage=True, device_map="auto",   
)   
sft_model $=$ model.merge_and_unload()   
# Merge DPO LoRA and SFT model   
dpo_model $=$ PeftModel.from_pretrained( sft_model, "TinyLlama-1.1B-dpo-qlora", device_map="auto",   
)   
dpo_model $=$ dpo_model.merge_and_unload()

This combination of $\mathrm { S F T + D P O }$ is a great way to first fine-tune your model to perform basic chatting and then align its answers with human preference. However, it does come at a cost since we need to perform two training loops and potentially tweak the parameters in two processes.

Since the release of DPO, new methods of aligning preferences have been developed. Of note is Odds Ratio Preference Optimization (ORPO), a process that combines SFT and DPO into a single training process.23 It removes the need to perform two separate training loops, further simplifying the training process while allowing for the use of QLoRA.

# Summary

In this chapter, we explored different steps of fine-tuning pretrained LLMs. We per‐ formed fine-tuning by making use of parameter-efficient fine-tuning (PEFT) through the low-rank adaptation (LoRA) technique. We explained how LoRA can be extended through quantization, a technique for reducing memory constraints when represent‐ ing the parameters of the model and adapters.

The fine-tuning process we explored has two steps. In the first step, we performed supervised fine-tuning using instruction data on a pretrained LLM, often called instruction tuning. This resulted in a model that has chat-like behavior and could closely follow instructions.

In the second step, we further improved the model by fine-tuning it on alignment data, data that represents what type of answers are preferred over others. This pro‐ cess, referred to as preference tuning, distills human preference to the previously instruction-tuned model.

Overall, this chapter has shown the two major steps of fine-tuning a pretrained LLM and how that could lead to more accurate and informative outputs.

# Afterword

Thank you to all who joined us on this fascinating journey through the world of large language models. We are grateful for your dedication to learning about these powerful models that have revolutionized language processing.

Throughout this book, we have seen how LLMs work and how they can be used to create a wide range of applications, from simple chatbots to more complex sys‐ tems like search engines. We have also explored various methods for fine-tuning pretrained LLMs on specific tasks, including classification, generation, and language representation. By mastering these techniques, readers will be able to unlock the potential of LLMs and create innovative solutions that can benefit from their capabili‐ ties. This knowledge will enable readers to stay ahead of the curve and adapt to new developments in the field.

As we come to the end of this book, we want to emphasize that our exploration of LLMs is only just the beginning. There are many more exciting developments on the horizon, and we encourage you to continue following the advancements in the field. To help with this process, keep an eye out on the repository of this book as we continue to add resources.

We hope that by reading this book, you gained a deeper understanding of how LLMs can be used in various applications and how they have the potential to transform industries.

With this book as your guide, we believe that you will be well-equipped to navigate the exciting landscape of LLMs and make meaningful contributions to this rapidly advancing field.

# Index

# A

accuracy confusion matrices, 120 output verification, 192   
adaptive pretraining, 320   
agents, 218-223 agentic RAG, 256 ReAct in LangChain, 221-223 step-by-step reasoning, 219-221   
AI (artificial intelligence) accelerated development of, 3 defined, 4   
ALBERT, 115   
align_labels function, 350   
all-MiniLM-L6-v2 model, 309   
all-mpnet-base-v2 model, 337   
Annoy, 239   
Anthropic Claude, 29   
APIs (application programming interfaces), 30 Cohere, xv, 230 external, 134 generating embeddings, 123 OpenAI, xv, 133   
artificial intelligence (see AI)   
ArXiv, 138   
attention, 11-18 overview of, 11-14 Transformer architecture, 15-18 attention calculation, 91-93 attention layer, 79, 86, 88, 106 Flash Attention, 100 grouped-query attention, 98-100 local attention, 96 multi-query attention, 98-100 optimizing attention, 98 self-attention and relevance scoring, 93-94 sparse attention, 96   
attention heads, 91-93, 107   
audience, in text-generation prompts, 178   
Augmented SBERT, 311-315   
autoregressive architecture, 12, 15, 22, 76, 97

# B

bag-of-words model, 6-7 embeddings, 10 topic modeling, 148-150, 156   
benchmarks, in generative model evaluation, 374   
BERT (Bidirectional Encoder Representations from Transformers), 18-20 adoption by search engines, 225 BERT-like models, 115 comparing to other trained tokenizers, 47 fine-tuning pretrained BERT models, 325-328 masked language modeling, 311 Transformer blocks versus, 97   
BERTopic, 148-155 algorithmic variants, 152 modularity of, 151 representation blocks, 156-163 KeyBERTInspired, 158 maximal marginal relevance, 159 text generation, 160-163   
BERTScore, 374   
bias and fairness, 28   
Bidirectional Encoder Representations from Transformers (see BERT)   
bitsandbytes package, 369   
BLEU, 374   
BLIP-2 (Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation 2) chat-based prompting, 283-286 image captioning, 280 preprocessing text, 280 Q-Former, 273-277   
BM25 algorithm, 233   
BPE (byte pair encoding), 43, 55   
byte tokens, 45

# C

c-TF-IDF, 149, 158, 161   
capitalization, 56   
captioning, 280-283   
centroid-based algorithms, 143   
chains, 182-184, 202-209 chain-of-thought, 185-188 chaining single prompt, 203-205 sequential chaining of multiple prompts, 206-209   
character tokens, 45   
chat tokens, 54   
chat-based prompting, 283-286   
Chatbot Arena, 376   
ChatGPT, 202, 273, 383, 386 release of, 3 text classification, 132-135   
chatgpt_generation function, 133   
chat_history input variable, 211   
classification reports, 119-120   
classification step, embedding model, 121   
CLIP, 265-268 connecting text and images, 265 generating multimodal embeddings, 265-268 OpenCLIP, 268-272   
closed-source LLMs, 29   
[CLS] token, 18, 48, 60, 270, 294, 318, 351   
cluster model, 142-144   
clustering (see text clustering)   
CNNs (convolutional neural networks), 260   
Cohere, 252 Command R+, 30, 256 creating accounts, xv, 230

generating embeddings, 123 query rewriting, 255 Rerank endpoint, 241 completion models, 22 compute_metrics function, 351 confusion matrices, 119 CoNLL-2003 dataset, 347 constrained sampling, 194-197 context attention and, 88 prompt engineering, 178 training datasets, 64 context length completion models, 22 token processing limits, 81 context window, in completion models, 22 contrastive learning text embedding models, 291-293, 296 word2vec algorithm and, 64-67 Contrastive Tension (CT), 316 conversation buffer memory, 217 overview of, 210-212 windowed, 212-214 conversation summary memory, 214-217 convert_ids_to_tokens function, 269 convolutional neural networks (CNNs), 260 cosine similarity, 125, 302-304 CountVectorizer, 150 cross-encoders, 244 cross-entropy loss, 305 CT (Contrastive Tension), 316

# D

data outliers, 143   
DataCollator class, 326, 342   
datamapplot package, 163   
DBSCAN (Density-Based Spatial Clustering), 143   
DeBERTa, 59, 115   
decoder-only models (see generative models)   
decoding strategy, 79-81, 106   
dense retrieval, 226, 228-240 caveats of, 234 example of, 230-234 fine-tuning embedding models for, 239 nearest neighbor search versus vector data‐ bases, 238 text chunking, 235-237   
density-based algorithms, 143   
Density-Based Spatial Clustering (DBSCAN), 143   
dimensionality reduction model, 140-142   
DistilBERT, 115, 120   
domain adaptation, 320   
do_sample parameter, 34   
DPO (Direct Preference Optimization), 384-389 fine-tuning, 388 model quantization, 386 templating alignment data, 386 training configuration, 387   
DSPy, 200

#

asy negatives, 308   
lo rating system, 377   
mbeddings, 8-10, 37, 57-70 dense retrieval, 226, 228-240 embedding models, defined, 114, 290 multimodality, 263-272 CLIP, 265-268 OpenCLIP, 268-272 overview of, 8-10, 289-291 positional embeddings, 102-104 recommendation systems, 67-70 text classification tasks that leverage, 120-126 supervised classification, 121-123 zero-shot classification, 123-126 text clustering pipeline, 139-146 cluster model, 142-144 dimensionality reduction model, 140-142 embedding model, 139 inspecting clusters, 144-146 text embedding models, 61-63, 289-321 contrastive learning, 291-293 creating, 296-308 fine-tuning, 309-315 SBERT, 293-296 unsupervised learning, 316 token embeddings, 57-61 creating contextualized word embed‐ dings, 58-61 tokenizer’s vocabulary and, 57 types of, 10 word embeddings, 63-67 pretrained, 63

word2vec algorithm and contrastive training, 64-67 encoder-decoder models, 128 encoder-only models (see representation mod‐ els) ethics, validating output, 192 exponential backoff, 134

#

F   
F1 score, confusion matrices, 120   
FAISS, 239   
Falcon, 168   
feature extraction step, embedding model, 121   
feedforward layer, 86   
feedforward neural networks, 15, 87, 106, 326   
few-shot classification, 333-339 fine-tuning for classification, 337-339 SetFit, 333-336   
few-shot prompting, 181-182, 192-194   
find_topics() function, BERTopic, 154   
fine-tuning embedding models for dense retrieval, 239 generative models, 192, 355-389 evaluating, 373-377 preference tuning, 356, 378-389 supervised fine-tuning, 356-373 training steps, 355-357 overview of, 26 representation models, 323-354 few-shot classification, 333-339 masked language modeling, 340-345 named-entity recognition, 345-353 supervised classification, 323-332 T5 model, 129 text embedding models, 309-315 Augmented SBERT, 311-315 supervised, 309-311   
Flan-T5 model, 50, 130-131   
Flash Attention, 100, 107   
forward pass, 106 components of, 76-79 defined, 74   
foundation models, 23   
fp16 parameter, 299   
freezing layers, 298, 328-332   
frozen (nontrainable) models, 114, 121, 324

# G Ġ symbol, 280

Galactica, 52   
General Language Understanding Evaluation (GLUE) benchmark, 297, 300, 359, 374   
generated_text variable, 281, 283   
generation_output variable, 42   
generative models, 20-22 evaluating, 373-377 automated evaluation, 376 benchmarks, 374 human evaluation, 376 leaderboards, 376 word-level metrics, 374 fine-tuning, 355-389 evaluation, 373-377 preference tuning, 356, 378-389 supervised fine-tuning, 356-373 training steps, 355-357 reasoning, 184-191 self-consistency, 188 tree-of-thought, 189-191 representation models versus, 20 text classification, 127-135 ChatGPT, 132-135 T5, 128-131   
generative pre-trained transformers (see GPTs)   
Generative Pseudo-Labeling (GPL), 316   
Gensim library, 63   
get_topic function, 153   
get_topic_info() method, 152   
GGUF model, 195, 200   
GitHub, xvi   
GloVe, 294   
GLUE (General Language Understanding Eval‐ uation) benchmark, 297, 300, 359, 374   
gold datasets, 312   
Goodhart’s Law, 377   
Google Colab, xii, xiv, 29, 33, 85, 372   
Google Gemini, 250   
Google Search, 225   
GPL (Generative Pseudo-Labeling), 316   
GPT2Tokenizer, 280   
GPTs (generative pre-trained transformers), 20, 167 (see also text generation) GPT-1, 20 GPT-2, 3, 20, 46, 49 GPT-3, 20, 87, 96, 364 GPT-3.5, 23, 132, 205, 221 GPT-4, 23, 29, 38, 50, 87   
GPUs Flash Attention, 100 requirements, xiv, 28 SRAM and HBM, 100   
grammar, 192, 194-197   
greedy decoding, 80   
grounded generation, 250-252   
grouped-query attention, 98-100, 107   
GSM8k, 374   
Guardrails, 194   
Guidance, 194

# H

hallucination avoiding in in instruction-based prompting, 177 text generation models, 225   
hard negatives, 308   
harmful content, generating, 28   
Haystack, 200   
HBM (high bandwidth memory), 100   
HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise), 143, 153   
HellaSwag, 374   
high bandwidth memory (HBM), 100   
Hugging Face, 32, 112 creating accounts, xv evaluate package, 351 tokenizers, 56   
human evaluation, 145, 376   
HumanEval, 375   
hybrid search, 235, 242

# I

Idefics 2, 277   
images (see multimodality)   
in-context learning, 180-182   
indexes, 232   
InfoNCE, 304   
input_ids variable, 40   
instruction-based prompting, 175-177   
intellectual property, 28   
intuition-first philosophy, xi   
invoke function, 204

# J

Jupyter, 85

#

k-means algorithm, 143, 151, 153   
KeyBERTInspired, 158, 160   
keyword search reranking, 242-243 verifying semantic search with, 233   
kv (keys and values) cache, 83-85   
L   
LangChain, 200 (see also chains) loading quantized models with, 200-202 ReAct in, 221-223   
Language AI (Language Artificial Intelligence), 3-24 defining, 4 recent history of, 5-24 attention, 11-18 bag-of-words model, 6-7 embeddings, 8-10 generative models, 20-22 representation models, 18-20 Year of Generative AI, 23-24   
language modeling, 355   
language modeling head (LM head), 76-79   
large language models (see LLMs)   
latent Dirichlet allocation, 147   
LayerNorm, 101   
leaderboards, in generative model evaluation, 376   
learning_rate parameter, 371   
Llama, 168   
Llama 2, xv, 29, 53, 98, 168, 273   
llama-cpp-python library, 195   
LLaVA, 277   
LLM-as-a-judge, 257   
LLMs (large language models), 3-35 code examples and exercises, xvi embeddings, 37, 57-70 recommendation systems, 67-70 text embeddings, 61-63 token embeddings, 57-61 word embeddings, 63-67 fine-tuning generative models, 355-389 evaluation, 373-377 preference tuning, 356, 378-389 supervised fine-tuning, 356-373 training steps, 355-357 fine-tuning representation models, 323-354 few-shot classification, 333-339 masked language modeling, 340-345 named-entity recognition, 345-353 supervised classification, 323-332   
generating text, 32-34   
generative models, 20-22   
hardware and software requirements, xiv, 2   
high-level view, 38   
history of Language AI, 5-24   
interfacing with, 29-32 closed-source models, 29 open models, 30-32   
intuition-first philosophy, xi   
moving definition of, 25   
multimodality, 259-286 embedding models, 263-272 text generation models, 273-286 Vision Transformer, 260-262   
prompt engineering, 167-198 chain prompting, 182-184 in-context learning, 180-182 instruction-based prompting, 175-177 output verification, 191-197 potential complexity of prompts, 177-179 prompt components, 173-175 reasoning with generative models, 184-191 text generation models, 167-172   
representation models, 18-20   
responsible development and usage of, 28   
retrieval-augmented generation, 227, 249-257 agentic RAG, 256 converting search system to, 250 evaluating results, 257 grounded generation, 252 multi-hop RAG, 256 multi-query RAG, 255 query rewriting, 255 query routing, 256 with local models, 252, 254   
semantic search, 225-249 dense retrieval, 226, 228-240 reranking, 226, 240-244 retrieval evaluation metrics, 244-249   
text classification, 111-135 with generative models, 127-135 movie reviews, 112, 113

with representation models, 113-126 text clustering, 137-146 text embedding models, 289-321 contrastive learning, 291-293 creating, 296-308 fine-tuning, 309-315 SBERT, 293-296 unsupervised learning, 316 text generation, 199-224 agents, 218-223 chains, 202-209 memory of conversations, 209-217 model I/O, 200-202 tokens and tokenizers, 37-61 comparing trained tokenizers, 46-54 downloading and running LLMs, 39-42 input preparation, 38 text breakdown, 43 token embeddings, 57-61 tokenization schemes, 44-46 tokenizer properties, 55-56 topic modeling, 138, 146-163 training paradigm of, 25-26 Transformer architecture, 73-107 decoding strategy, 79-81 forward pass components, 76-79 inputs and outputs of, 74-76 keys and values cache, 83-85 parallel token processing and context size, 81-83 recent improvements to, 95-105 Transformer blocks, 85-94 utility of, 27 M head (language modeling head), 76-79 MQL, 194 ocal attention, 96 oRA (low-rank adaptation), 361-364, 387 (see also QLoRA) ora_alpha parameter, 370 oss functions, 301-308 cosine similarity loss, 302-304 multiple negatives ranking loss, 304-308 _scheduler_type parameter, 371

# M

Mamba, 24   
MAP (mean average precision), 244-249   
MarginMSE loss, 302   
masked language modeling (MLM), 340-345

mask_token [MASK], 48   
Massive Text Embedding Benchmark (MTEB), 116, 140, 253, 300   
matplotlib library, 145   
maximal marginal relevance (MMR), 159   
max_new_tokens parameter, 34   
McCarthy, John, 4   
mean average precision (MAP)   
memory of conversations, 209-217 conversation buffer, 210-212 conversation summary, 214-217 windowed conversation buffer, 212-214   
Meta Llama model, 30   
Microsoft Bing, 225, 241   
Microsoft Bing AI, 250   
Microsoft Phi model, 30   
microsoft/mpnet-base model, 297   
min_cluster_size parameter, 144   
min_dist parameter, 142   
MIRACL, 243   
Mistral, 30, 168, 277   
MLM (masked language modeling), 340-345   
MMLU, 374, 376   
MMR (maximal marginal relevance), 159   
MNLI (Multi-Genre Natural Language Infer‐ ence) corpus, 297, 305   
MNR (multiple negatives ranking) loss, 304-308   
model I/O, 200-202   
monoBERT, 244   
MTEB (Massive Text Embedding Benchmark), 116, 140, 253, 300   
Multi-Genre Natural Language Inference (MNLI) corpus, 297, 305   
multi-hop RAG, 256   
multi-query attention, 98-100   
multi-query RAG, 255   
multilevel perceptrons, 79 (see also feedforward neural networks)   
multimodality, 259-286 defined, 259 embedding models, 263-272 CLIP, 265-268 OpenCLIP, 268-272 text generation models, 273-286 BLIP-2, 273-277 chat-based prompting, 283-286 image captioning, 280-283 preprocessing images, 278

preprocessing text, 279 Vision Transformer, 260-262 multiple negatives ranking (MNR) loss, 304-308

# N

named-entity recognition (see NER)   
natural language inference (NLI), 296   
natural language processing (NLP), 4, 293   
nDCG (normalized discounted cumulative gain), 243, 249   
nearest neighbor search pretrained word embeddings, 63 recommendation system embeddings, 68 vector databases versus, 238   
negative sampling, 65   
NER (named-entity recognition), 345-353, 360 fine-tuning for, 352 preparing data for, 347-351   
neural networks, 8   
NLI (natural language inference), 296   
NLP (natural language processing), 4, 293   
noise-contrastive estimation, 65   
nonplayable characters (NPCs), 4   
nontrainable (frozen) models, 114, 121, 324   
normalization, Transformer block, 101   
normalized discounted cumulative gain (nDCG), 243, 249   
NPCs (nonplayable characters), 4   
NTXentLoss, 304   
nucleus sampling, 171   
NumPy, 238   
num_train_epochs parameter, 299, 371   
NVIDIA GPUs, xiv, 33   
n_components parameter, 142   
O   
Odds Ratio Preference Optimization (ORPO), 389   
one-shot prompting, 182 chain-of-thought versus, 186 in-context learning, 181   
Open LLM Leaderboard, 202, 376   
open-source LLMs, 30-32   
OpenAI, 132 (see also ChatGPT; GPTs) creating accounts, xv, 133 generating embeddings, 123   
OpenCLIP, 268-272   
optim parameter, 372   
ORPO (Odds Ratio Preference Optimization), 389   
output verification, 191-197 constrained sampling, 194-197 providing examples, 192   
P   
pad_token [PAD], 47   
parallel processing, 91, 106   
parallel prompts, 184   
PCA (Principal Component Analysis), 141   
PEFT (parameter-efficient fine-tuning), 359-367 adapters, 359-361 compression, 364-367 LoRA, 361-364   
peft library, 370   
peft_config parameter, 371   
Perplexity, 250   
persona, in text-generation prompts, 178   
per_device_eval_batch_size argument, 299   
per_device_train_batch_size argument, 299   
Phi-3 comparing to other trained tokenizers, 53 forward pass, 79 loading quantized models, 202 prompt template, 204 quantization, 201   
Phi-3-mini, 32, 168   
Pinecone, 239   
positional embeddings, 102-104   
PPO (Proximal Policy Optimization), 383   
precision predictions, confusion matrices, 119   
predictions, task-specific model, 118   
preference tuning, 132, 356, 378-389 Direct Preference Optimization, 384-389 fine-tuning, 388 model quantization, 386 templating alignment data, 386 training configuration, 387 reward models, 379-383 inputs and outputs of, 380 training, 380-383   
pretraining, defined, 26   
primacy effect, 177   
Principal Component Analysis (PCA), 141   
projection matrices, 92   
prompt engineering, 127, 167-198

chain prompting, 182-184 in-context learning, 180-182 instruction-based prompting, 175-177 output verification, 191-197 constrained sampling, 194-197 providing examples, 192 potential complexity of prompts, 177-179 prompt components, 173-175 reasoning with generative models, 184-191 text generation models, 167-172 choosing, 167 controlling output, 170-172 loading, 168-169 roximal Policy Optimization (PPO), 383 ython, learning about, xii

Q   
Q-Former (Querying Transformer), 274-277   
Q8 model, 201   
QLoRA (quantized low-rank adaptation), 367-373 fine-tuning, 372 LoRA configuration, 370 merging weights, 373 model quantization, 369 templating instruction data, 367 training configuration, 371   
quantization, 201, 364-367   
quantization_config parameter, 371   
quantized low-rank adaptation (see QLoRA)   
Querying Transformer (Q-Former), 274-277

# R

r parameter, 370   
RAG (retrieval-augmented generation), 57, 63, 227, 249-257 agentic RAG, 256 basic pipeline, 249 converting search system to, 250 evaluating results, 257 grounded generation, 252 with local models, 252-254 multi-hop RAG, 256 multi-query RAG, 255 query rewriting, 255 query routing, 256   
Ragas, 257   
random_state parameter, 142   
rate limit errors, 134   
ReAct in LangChain, 221-223 step-by-step reasoning, 219   
reasoning with generative models, 184-191 chain-of-thought, 185-188 self-consistency, 188 tree-of-thought, 189-191 step-by-step, 219, 221   
recall predictions, confusion matrices, 119   
recency effect, 177   
recommendation systems, 67-70   
recurrent neural networks (RNNs), 11   
reduce_outliers() function, 153   
regulation, 28   
relevance scoring, 90, 92-94, 104   
repository, xiv   
representation models, 18-20 defined, 7 fine-tuning for classification, 323-354 few-shot classification, 333-339 masked language modeling, 340-345 named-entity recognition, 345-353 supervised classification, 323-332 generative models versus, 20 text classification, 113-126 classification tasks that leverage embed‐ dings, 120-126 model selection, 115-116 task-specific models, 116   
representation_model parameter, 161   
reranking, 226, 240-244 BERTopic, 156 example of, 241-243 function of reranking models, 244 sentence transformers, 243   
response validation, in chain prompting, 184   
retrieval evaluation metrics, 244-249 scoring multiple queries with mean average precision, 248 scoring single queries with average preci‐ sion, 247   
retrieval-augmented generation (see RAG)   
return_full_text parameter, 33   
reward models, 379-383 inputs and outputs of, 380 training, 380-383   
RMSNorm, 101   
RNNs (recurrent neural networks), 11

RoBERTa, 46, 115   
RoPE (rotary positional embeddings), 102-104   
Rorschach test, 282   
Rotten Tomatoes dataset, 112, 325   
ROUGE, 374   
RWKV, 24

S   
SBERT, 293-296, 311-315   
self-attention, 15, 93-94   
self-consistency, 188   
semantic search, 225-249 defined, 225 dense retrieval, 226, 228-240 caveats of, 234 example of, 230-234 fine-tuning embedding models for, 239 nearest neighbor search versus vector databases, 238 text chunking, 235-237 reranking, 226, 240-244 example of, 241-243 function of reranking models, 244 sentence transformers, 243 retrieval evaluation metrics, 244-249 scoring multiple queries with mean aver‐ age precision, 248 scoring single queries with average pre‐ cision, 247   
Semantic Textual Similarity Benchmark (STSB), 298   
semi-hard negatives, 308   
sentence-transformers, 62, 122, 151, 243, 272, 293-295, 309, 333   
SentencePiece, 50   
[SEP] token, 47, 60, 351   
sequence-to-sequence models, 127, 128   
SetFit, 323, 333-336   
SFT (supervised fine-tuning), 356-373 full fine-tuning, 357 parameter-efficient fine-tuning, 359-367 adapters, 359-361 compression, 364-367 LoRA, 361-364 QLoRA, 367-373 fine-tuning, 372 LoRA configuration, 370 merging weights, 373 model quantization, 369

templating instruction data, 367 training configuration, 371 shared memory (SRAM), 100 shortlisting, 242 silver datasets, 312 SimCSE (Simple Contrastive Learning of Sen‐ tence Embeddings), 316 skip-gram, 65 softmax loss function, 301 song recommendation systems, 67-70 sparse attention, 96 special tokens, 47, 55 specificity, in instruction-based prompting, 176 SRAM (shared memory), 100 StableLM, 168 StarCoder2, 51 step-by-step reasoning, 219-221 structured output, validating, 191 STSB (Semantic Textual Similarity Bench‐ mark), 298 subword tokens, 44 supervised classification, 121-123 fine-tuning representation models for, 323-332 freezing layers, 328-332 pretrained BERT models, 325-328 supervised fine-tuning (see SFT) system 1 and 2 thinking processes, 185

T   
T5 (Text-to-Text Transfer Transformer), 128-131   
target_modules parameter, 370   
task-specific models, 113   
temperature parameter, 171-172, 188   
Tesla T4, 372   
test splits, 113, 118   
text chunking, 230, 235-237 approaches for, 237 multiple vectors per document, 236 one vector per document, 236   
text classification, 111-135 with generative models, 127-135 ChatGPT, 132-135 T5, 128-131 movie reviews, 112-113 with representation models, 113-126 classification tasks that leverage embed‐ dings, 120-126 model selection, 115-116 task-specific models, 116-120   
text clustering, 137-146 CLIP embedding model and, 265 common pipeline for, 139-146 cluster model, 142-144 dimensionality reduction model, 140-142 embedding model, 139 inspecting clusters, 144-146   
text embedding models, 61-63, 289-321 contrastive learning, 291-293 creating, 296-308 evaluating, 300 generating contrastive examples, 296 loss functions, 301-308 training, 297-300 fine-tuning, 309 Augmented SBERT, 311-315 supervised, 309-311 SBERT, 293-296 unsupervised learning, 316   
text generation, 32-34, 199-224 agents, 218-223 ReAct in LangChain, 221-223 step-by-step reasoning, 219-221 chains, 202-209 chaining single prompt, 203-205 sequential chaining of multiple prompts, 206-209 memory of conversations, 209-217 conversation buffer, 210-212 conversation summary, 214-217 windowed conversation buffer, 212-214 model I/O, 200-202 multimodality, 273-286 BLIP-2, 273-277 chat-based prompting, 283-286 image captioning, 280-283 preprocessing images, 278 preprocessing text, 279 prompt engineering, 167-172 choosing models, 167 controlling output, 170-172 loading models, 168-169 topic modeling, 160-163   
text-in-text-out model, 74   
Text-to-Text Transfer Transformer (T5), 128-131   
enlper/gte-small model, 140   
%timeit magic command, 85   
inyLlama, 367, 385   
okenization-free encoding, 45   
okens and tokenizers, 33, 37-61 bag-of-words model, 6 comparing trained tokenizers, 46-54 BERT base model (cased), 48 BERT base model (uncased), 47 Flan-T5, 50 Galactica, 52 GPT-2, 49 GPT-4, 50 Phi-3 and Llama 2, 53 StarCoder2, 51 decoding strategy, 79-81 downloading and running LLMs, 39-42 forward pass, 76-77 input preparation, 38 masked language modeling, 129 parallel token processing and context size, 81-83 special tokens, 47 task-specific representation model, 117, 127 text breakdown, 43 text-focused versus code-focused models, 56 token embeddings, 57-61, 77, 106 creating contextualized word embed‐ dings, 58-61 tokenizer’s vocabulary and, 57 token spans, 129 tokenization schemes, 44-46 byte tokens, 45 character tokens, 45 subword tokens, 44 word tokens, 44 tokenizer properties, 55-56 datasets, 56 methods, 55 parameters, 55 white space characters, 50   
one of voice, in text-generation prompts, 178   
opic modeling, 138, 146-163 BERTopic, 148-155 representation blocks, 156-163   
op_k parameter, 172   
op_p parameter, 171, 188   
rain splits, 113   
rainingArguments class, 327   
transfer learning, 19   
Transformer architecture, 15-18, 73-107 attention layer, 79, 86 decoding strategy, 79-81 feedforward layer, 86 forward pass components, 76-79 inputs and outputs of, 74-76 keys and values cache, 83-85 optimizing attention, 98 parallel token processing and context size, 81-83 recent improvements to, 95-105 more efficient attention, 96-100 positional embeddings, 102-104 Transformer blocks, 101 Transformer blocks, 85-94 attention calculation, 91-93 attention layer, 88 attention mechanism, 89-91 feedforward neural networks, 87 self-attention and relevance scoring, 93-94 Vision Transformer, 260-262   
transparency and accountability, 28   
tree-of-thought, 189-191   
TruthfulQA, 374, 376   
TSDAE (Transformer-Based Sequential Denois‐ ing Auto-Encoder) for domain adaptation, 320 overview of, 316-320

# U

UltraChat dataset, 368   
UMAP (Uniform Manifold Approximation and   
Projection), 141   
unigram language model, 50   
unk_token [UNK], 47   
use_cache parameter, 84

# V

valid output, verifying, 192 validation splits, 113 vector databases dense retrieval, 229

nearest neighbor search versus, 238 retrieval-augmented generation, 253   
video random-access memory (VRAM), xiv, 29   
visualization BERTopic, 155 cluster analysis, 144 dimensionality reduction and, 145   
ViT (Vision Transformer), 260-262, 274   
vocabulary, of tokenizers, 55, 57, 77, 106   
VRAM (video random-access memory), xiv, 29

# W

warmup_ratio parameter, 387   
warmup_steps argument, 299   
Weaviate, 239   
whitespace characters, 50   
windowed conversation buffer memory, 212-214, 217   
word embeddings, 63-67 pretrained, 63 word2vec algorithm and contrastive train‐ ing, 64-67   
word tokens, 44   
word-level metrics, in generative model evalua‐ tion, 374   
word2vec algorithm, 8, 10-12 contrastive training and, 64-67, 293 embedding songs, 67   
WordPiece, 43 cased BERT base model, 48 uncased BERT base model, 47   
<work> token, 53

# Y Year of Generative AI, 23-24

Z   
zero-shot classification, 123-126 CLIP, 265 SetFit, 339   
zero-shot prompting chain-of-thought, 187 in-context learning, 181

# About the Authors

Jay Alammar is Director and Engineering Fellow at Cohere (pioneering provider of large language models as an API). In this role, he advises and educates enterprises and the developer community on using language models for practical use cases. Through his popular AI/ML blog, Jay has helped millions of researchers and engi‐ neers visually understand machine learning tools and concepts from the basic (end‐ ing up in the documentation of packages like NumPy and pandas) to the cutting-edge (Transformers, BERT, GPT-3, Stable Diffusion). Jay is also a co-creator of popular machine learning and natural language processing courses on Deeplearning.ai and Udacity.

Maarten Grootendorst is a Senior Clinical Data Scientist at IKNL (Netherlands Comprehensive Cancer Organization). He holds master’s degrees in organizational psychology, clinical psychology, and data science, which he leverages to communicate complex machine learning concepts to a wide audience. With his popular blogs, he has reached millions of readers by explaining the fundamentals of artificial intelli‐ gence—often from a psychological point of view. He is the author and maintainer of several open source packages that rely on the strength of large language models, such as BERTopic, PolyFuzz, and KeyBERT. His packages are downloaded millions of times and used by data professionals and organizations worldwide.

# Colophon

The animal on the cover of Hands-On Large Language Models is a red kangaroo (Osphranter rufus). They are the largest of all kangaroos, with a body length that can get up to a little over 5 feet and a tail as long as 3 feet. They are very fast and can hop to speeds over 35 miles per hour. They can jump 6 feet high and leap a distance of 25 feet in a single bound. The position of their eyes allows them see up to 300 degrees.

Red kangaroos are named after the color of their fur. While the name makes sense for the males—they have short, red-brown fur—females are typically more of a blue-grey color with a tinge of brown throughout. The red color in their fur comes from a red oil excreted from the glands in their skin. Because of their color, Australians refer to male red kangaroos as “big reds.” However, because females are faster than males, they are often called “blue fliers.”

Preferring open, dry areas with some trees for shade, red kangaroos can be found across Australia’s mainland except in the upper north, lower southwest, and east coast regions of the country. Surrounding environmental conditions can affect repro‐ duction. Because of this, females can pause or postpone pregnancy or birth until conditions are better. They often use this ability to delay birth of a new baby (joey) until the previous one has left their pouch.

The cover illustration is by Karen Montgomery, based on an antique line engraving from Cassell’s Popular Natural History. The series design is by Edie Freedman, Ellie Volckhausen, and Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono.

O'REILLY?

# Learn from experts. Become one yourself.

Books | Live online courses Instant answers | Virtual events Videos Interactive learning

Get started at oreilly.com.