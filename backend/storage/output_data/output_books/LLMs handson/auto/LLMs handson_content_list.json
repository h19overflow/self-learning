[
    {
        "type": "text",
        "text": "Hands-On Large Language Models ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Language Understanding and Generation ",
        "page_idx": 0
    },
    {
        "type": "image",
        "img_path": "images/84cefeebe1d95d704bc6a77518329e97a55bea30393e4aa2c99fa20f16a5e840.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Hands-On Large Language Models ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "AI has acquired startling new language capabilities in just the past few years. Driven by rapid advances in deep learning, language AI systems are able to write and understand text better than ever before. This trend is enabling new features, products, and entire industries. Through this book’s visually educational nature, readers will learn practical tools and concepts they need to use these capabilities today. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "You’ll understand how to use pretrained large language models for use cases like copywriting and summarization; create semantic search systems that go beyond keyword matching; and use existing libraries and pretrained models for text classification, search, and clusterings. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "This book also helps you: ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "• Understand the architecture of Transformer language models that excel at text generation and representation   \n• Build advanced LLM pipelines to cluster text documents and explore the topics they cover   \n• Build semantic search engines that go beyond keyword search, using methods like dense retrieval and rerankers   \n• Explore how generative models can be used, from prompt engineering all the way to retrieval-augmented generation   \n• Gain a deeper understanding of how to train LLMs and optimize them for specific applications using generative model fine-tuning, contrastive fine-tuning, and in-context learning ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "“Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics. Their book is a valuable resource for anyone looking to understand the main techniques behind how large language models are built.” ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "—Andrew Ng founder of DeepLearning.AI “I can’t think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models.” —Josh Starmer, StatQuest ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Jay Alammar is director and engineering fellow at Cohere. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Maarten Grootendorst is senior clinical data scientist at the Netherlands Comprehensive Cancer Organization (IKNL). ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Praise for Hands-On Large Language Models ",
        "text_level": 1,
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "This is an exceptional guide to the world of language models and their practical   \napplications in industry. Its highly-visual coverage of generative, representational, and   \nretrieval applications of language models empowers readers to quickly understand, use, and refine LLMs. Highly recommended! ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "—Nils Reimers, Director of Machine Learning at Cohere | creator of sentence-transformers ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "Jay and Maarten have continued their tradition of providing beautifully illustrated and insightful descriptions of complex topics in their new book. Bolstered with working code, timelines, and references to key papers, their book is a valuable resource for anyone looking to understand the main techniques behind how Large Language Models are built. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "—Andrew Ng, founder of DeepLearning.AI ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "I can’t think of another book that is more important to read right now. On every single page, I learned something that is critical to success in this era of language models. ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "—Josh Starmer, StatQuest ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "If you’re looking to get up to speed in everything regarding LLMs, look no further! In this wonderful book, Jay and Maarten will take you from zero to expert in the history and latest advances in large language models. With very intuitive explanations, great real-life examples, clear illustrations, and comprehensive code labs, this book lifts the curtain on the complexities of transformer models, tokenizers, semantic search, RAG, and many other cutting-edge technologies. A must read for anyone interested in the latest AI technology! ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "—Luis Serrano, PhD, Founder and CEO of Serrano Academy ",
        "page_idx": 2
    },
    {
        "type": "text",
        "text": "This book is a must-read for anyone interested in the rapidly-evolving field of generative AI. With a focus on both text and visual embeddings, it’s a great blend of algorithmic evolution, theoretical rigor, and practical guidance. Whether you are a student, researcher, or industry professional, this book will equip you with the use cases and solutions needed to level-up your knowledge of generative AI. Well done! ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "—Chris Fregly, Principal Solution Architect, Generative AI at AWS ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "In the heart of the GenAI revolution, this indispensable guide masterfully balances theory and practice, navigating the vast landscape of large language models to equip readers with the knowledge needed for immediate and transformative impact in the field of AI. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "—Tarun Narayanan Venkatachalam, AI Researcher, University of Washington ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Timely reading to get hands-on experience with language models. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "—Emir Muñoz, Genesys ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Hands-On Large Language Models brings clarity and practical examples to cut through the hype of AI. It provides a wealth of great diagrams and visual aids to supplement the clear explanations. The worked examples and code make concrete what other books   \nleave abstract. The book starts with simple introductory beginnings, and steadily builds in   \nscope. By the final chapters, you will be fine-tuning and building your own large language models with confidence. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "—Leland McInnes, Researcher at the Tutte Institute for Mathematics and Computing ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Finally, a book that not only avoids superficial coverage of large language models but also thoroughly explores the background in a way that is both accessible and engaging. The authors have masterfully created a definitive guide that will remain essential reading despite the fast-paced advancements in the field. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "—Prof. DDr. Roman Egger, CEO of Smartvisions.at and Modul University Vienna ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Hands-On Large Language Models Language Understanding and Generation ",
        "text_level": 1,
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Jay Alammar and Maarten Grootendorst ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Hands-On Large Language Models ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "by Jay Alammar and Maarten Grootendorst ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Copyright $^ ©$ 2024 Jay Alammar and Maarten Pieter Grootendorst. All rights reserved. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Printed in the United States of America. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://oreilly.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Acquisitions Editor: Nicole Butterfield Development Editor: Michele Cronin Production Editor: Ashley Stussy Copyeditor: Charles Roumeliotis Proofreader: Kim Cofer ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Indexer: BIM Creatives, LLC Interior Designer: David Futato Cover Designer: Karen Montgomery Illustrator: Kate Dullea ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "September 2024: First Edition ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Revision History for the First Edition 2024-09-10: First Release ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "See http://oreilly.com/catalog/errata.csp?isbn 9781098150969 for release details. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-On Large Language Models, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "The views expressed in this work are those of the authors and do not represent the publisher’s views. While the publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of or reliance on this work. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open source licenses or the intellectual property rights of others, it is your responsibility to ensure that your use thereof complies with such licenses and/or rights. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Table of Contents ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Part I. Understanding Language Models ",
        "text_level": 1,
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "1. An Introduction to Large Language Models. . . 3 ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "What Is Language AI? 4   \nA Recent History of Language AI 5   \nRepresenting Language as a Bag-of-Words 6   \nBetter Representations with Dense Vector Embeddings 8   \nTypes of Embeddings 10   \nEncoding and Decoding Context with Attention 11   \nAttention Is All You Need 15   \nRepresentation Models: Encoder-Only Models 18   \nGenerative Models: Decoder-Only Models 20   \nThe Year of Generative AI 23   \nThe Moving Definition of a “Large Language Model” 25   \nThe Training Paradigm of Large Language Models 25   \nLarge Language Model Applications: What Makes Them So Useful? 27   \nResponsible LLM Development and Usage 28   \nLimited Resources Are All You Need 28   \nInterfacing with Large Language Models 29   \nProprietary, Private Models 29   \nOpen Models 30   \nOpen Source Frameworks 31   \nGenerating Your First Text 32   \nSummary 34 ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2. Tokens and Embeddings. . . . . . . . 37 ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "LLM Tokenization 38   \nHow Tokenizers Prepare the Inputs to the Language Model 38   \nDownloading and Running an LLM 39   \nHow Does the Tokenizer Break Down Text? 43   \nWord Versus Subword Versus Character Versus Byte Tokens 44   \nComparing Trained LLM Tokenizers 46   \nTokenizer Properties 55   \nToken Embeddings 57   \nA Language Model Holds Embeddings for the Vocabulary of Its Tokenizer 57   \nCreating Contextualized Word Embeddings with Language Models 58   \nText Embeddings (for Sentences and Whole Documents) 61   \nWord Embeddings Beyond LLMs 63   \nUsing pretrained Word Embeddings 63   \nThe Word2vec Algorithm and Contrastive Training 64   \nEmbeddings for Recommendation Systems 67   \nRecommending Songs by Embeddings 67   \nTraining a Song Embedding Model 69   \nSummary 71 ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "3. Looking Inside Large Language Models. . . . . 73 ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "An Overview of Transformer Models 74   \nThe Inputs and Outputs of a Trained Transformer LLM 74   \nThe Components of the Forward Pass 76   \nChoosing a Single Token from the Probability Distribution (Sampling/   \nDecoding) 79   \nParallel Token Processing and Context Size 81   \nSpeeding Up Generation by Caching Keys and Values 83   \nInside the Transformer Block 85   \nRecent Improvements to the Transformer Architecture 95   \nMore Efficient Attention 96   \nThe Transformer Block 101   \nPositional Embeddings (RoPE) 102   \nOther Architectural Experiments and Improvements 105   \nSummary 106 ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "Part II. Using Pretrained Language Models ",
        "text_level": 1,
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "4. Text Classification. . . . 111   \nThe Sentiment of Movie Reviews 112   \nText Classification with Representation Models 113   \nModel Selection 115   \nUsing a Task-Specific Model 116   \nClassification Tasks That Leverage Embeddings 120   \nSupervised Classification 121   \nWhat If We Do Not Have Labeled Data? 123   \nText Classification with Generative Models 127   \nUsing the Text-to-Text Transfer Transformer 128   \nChatGPT for Classification 132   \nSummary 135 ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "5. Text Clustering and Topic Modeling. . . . . 137 ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "ArXiv’s Articles: Computation and Language 138   \nA Common Pipeline for Text Clustering 139   \nEmbedding Documents 139   \nReducing the Dimensionality of Embeddings 140   \nCluster the Reduced Embeddings 142   \nInspecting the Clusters 144   \nFrom Text Clustering to Topic Modeling 146   \nBERTopic: A Modular Topic Modeling Framework 148   \nAdding a Special Lego Block 156   \nThe Text Generation Lego Block 160   \nSummary 164 ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "6. Prompt Engineering. . . . . 167 ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Using Text Generation Models 167   \nChoosing a Text Generation Model 167   \nLoading a Text Generation Model 168   \nControlling Model Output 170   \nIntro to Prompt Engineering 173   \nThe Basic Ingredients of a Prompt 173   \nInstruction-Based Prompting 175   \nAdvanced Prompt Engineering 177   \nThe Potential Complexity of a Prompt 177   \nIn-Context Learning: Providing Examples 180   \nChain Prompting: Breaking up the Problem 182   \nReasoning with Generative Models 184   \nChain-of-Thought: Think Before Answering 185   \nSelf-Consistency: Sampling Outputs 188   \nTree-of-Thought: Exploring Intermediate Steps 189   \nOutput Verification 191   \nProviding Examples 192   \nGrammar: Constrained Sampling 194 ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "7. Advanced Text Generation Techniques and Tools. . . . 199 ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Model I/O: Loading Quantized Models with LangChain 200   \nChains: Extending the Capabilities of LLMs 202   \nA Single Link in the Chain: Prompt Template 203   \nA Chain with Multiple Prompts 206   \nMemory: Helping LLMs to Remember Conversations 209   \nConversation Buffer 210   \nWindowed Conversation Buffer 212   \nConversation Summary 214   \nAgents: Creating a System of LLMs 218   \nThe Driving Power Behind Agents: Step-by-step Reasoning 219   \nReAct in LangChain 221   \nSummary 224 ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "8. Semantic Search and Retrieval-Augmented Generation. . . . . 225 ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Overview of Semantic Search and RAG 226   \nSemantic Search with Language Models 228   \nDense Retrieval 228   \nReranking 240   \nRetrieval Evaluation Metrics 244   \nRetrieval-Augmented Generation (RAG) 249   \nFrom Search to RAG 250   \nExample: Grounded Generation with an LLM API 252   \nExample: RAG with Local Models 252   \nAdvanced RAG Techniques 255   \nRAG Evaluation 257   \nSummary 258 ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "9. Multimodal Large Language Models. . . 259 ",
        "text_level": 1,
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Transformers for Vision 260   \nMultimodal Embedding Models 263   \nCLIP: Connecting Text and Images 265   \nHow Can CLIP Generate Multimodal Embeddings? 265   \nOpenCLIP 268   \nMaking Text Generation Models Multimodal 273   \nBLIP-2: Bridging the Modality Gap 273   \nPreprocessing Multimodal Inputs 278   \nUse Case 1: Image Captioning 280   \nUse Case 2: Multimodal Chat-Based Prompting 283   \nSummary 286 ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Part III. Training and Fine-Tuning Language Models ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "10. Creating Text Embedding Models. . . . 289 ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Embedding Models 289   \nWhat Is Contrastive Learning? 291   \nSBERT 293   \nCreating an Embedding Model 296   \nGenerating Contrastive Examples 296   \nTrain Model 297   \nIn-Depth Evaluation 300   \nLoss Functions 301   \nFine-Tuning an Embedding Model 309   \nSupervised 309   \nAugmented SBERT 311   \nUnsupervised Learning 316   \nTransformer-Based Sequential Denoising Auto-Encoder 316   \nUsing TSDAE for Domain Adaptation 320   \nSummary 321 ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "11. Fine-Tuning Representation Models for Classification. . . . . 323 ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Supervised Classification 323   \nFine-Tuning a Pretrained BERT Model 325   \nFreezing Layers 328   \nFew-Shot Classification 333   \nSetFit: Efficient Fine-Tuning with Few Training Examples 333   \nFine-Tuning for Few-Shot Classification 337   \nContinued Pretraining with Masked Language Modeling 340   \nNamed-Entity Recognition 345   \nPreparing Data for Named-Entity Recognition 347   \nFine-Tuning for Named-Entity Recognition 352   \nSummary 353 ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "12. Fine-Tuning Generation Models. . . 355 ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and   \nPreference Tuning 355   \nSupervised Fine-Tuning (SFT) 357   \nFull Fine-Tuning 357   \nParameter-Efficient Fine-Tuning (PEFT) 359   \nInstruction Tuning with QLoRA 367   \nTemplating Instruction Data 367   \nModel Quantization 369   \nLoRA Configuration 370   \nTraining Configuration 371   \nTraining 372   \nMerge Weights 373   \nEvaluating Generative Models 373   \nWord-Level Metrics 374   \nBenchmarks 374   \nLeaderboards 376   \nAutomated Evaluation 376   \nHuman Evaluation 376   \nPreference-Tuning / Alignment / RLHF 378   \nAutomating Preference Evaluation Using Reward Models 379   \nThe Inputs and Outputs of a Reward Model 380   \nTraining a Reward Model 380   \nTraining No Reward Model 384   \nPreference Tuning with DPO 385   \nTemplating Alignment Data 386   \nModel Quantization 386   \nTraining Configuration 387   \nTraining 388   \nSummary 389   \nAfterword. . 391   \nIndex. . 393 ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Large language models (LLMs) have had a profound and far-reaching impact on the world. By enabling machines to better understand and generate human-like language, LLMs have opened new possibilities in the field of AI and impacted entire industries. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "This book provides a comprehensive and highly visual introduction to the world of LLMs, covering both the conceptual foundations and practical applications. From word representations that preceded deep learning to the cutting-edge (at the time of this writing) Transformer architecture, we will explore the history and evolution of LLMs. We delve into the inner workings of LLMs, exploring their architectures, training methods, and fine-tuning techniques. We also examine various applications of LLMs in text classification, clustering, topic modeling, chatbots, search engines, and more. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "With its unique blend of intuition-building, applications, and illustrative style, we hope that this book provides the ideal foundation for those looking to explore the exciting world of LLMs. Whether you are a beginner or an expert, we invite you to join us on this journey to start building with LLMs. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "An Intuition-First Philosophy ",
        "text_level": 1,
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "The main goal of this book is to provide an intuition into the field of LLMs. The pace of development in the Language AI field is incredibly fast and frustration can build trying to keep up with the latest technologies. Instead, we focus on the fundamentals of LLMs and intend to provide a fun and easy learning process. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "To achieve this intuition-first philosophy we liberally make use of visual language. Illustrations will help give a visual identity to major concepts and processes involved in the learning process of LLMs.1 With our illustrative method of storytelling, we want to take you on a journey to this exciting and potentially world-changing field. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Throughout the book, we make a clear distinction between representation and gener‐ ative language models. Representation models are LLMs that do not generate text but are commonly used for task-specific use cases, like classification, whereas generation models are LLMs that generate text, like GPT models. Although generative models are typically the first thing that comes to mind when thinking about LLMs, there is still much use for representation models. We are also loosely using the word “large” in large language models and often elect to simply call them language models as size descriptions are often rather arbitrary and not always indicative of capability. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Prerequisites ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "This book assumes that you have some experience programming in Python and are familiar with the fundamentals of machine learning. The focus will be on building a strong intuition rather than deriving mathematical equations. As such, illustrations combined with hands-on examples will drive the examples and learning through this book. This book assumes no prior knowledge of popular deep learning frameworks such as PyTorch or TensorFlow nor any prior knowledge of generative modeling. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "If you are not familiar with Python, a great place to start is Learn Python, where you will find many tutorials on the basics of the language. To further ease the learning process, we made all the code available on Google Colab, a platform where you can run all of the code without the need to install anything locally. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Book Structure ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "The book is broadly divided into three parts. They are illustrated in Figure P-1 to give you a full view of the book. Note that each chapter can be read independently, so feel free to skim chapters you are already familiar with. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Part I: Understanding Language Models ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "In Part I of the book, we explore the inner workings of language models both small and large. We start with an overview of the field and common techniques (see Chap‐ ter 1) before moving over to two central components of these models, tokenization and embeddings (see Chapter 2). We finish this part of the book with an updated and expanded version of Jay’s well-known Illustrated Transformer, which dives into the architecture of these models (see Chapter 3). Many terms and definitions will be introduced that are used throughout the book. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 14
    },
    {
        "type": "image",
        "img_path": "images/2b4dc2d8a7cfc77660f474400b99b3c2247b9c14e1ea5eba5facb6b0e69431c3.jpg",
        "image_caption": [
            "Figure P-1. All parts and chapters of the book. "
        ],
        "image_footnote": [],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Part II: Using Pretrained Language Models ",
        "text_level": 1,
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "In Part II of the book, we explore how LLMs can be used through common use cases. We use pretrained models and demonstrate their capabilities without the need to fine-tune them. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "You learn how to use language models for supervised classification (see Chapter 4), text clustering and topic modeling (see Chapter 5), leveraging embedding models for semantic search (see Chapter 6), generating text (see Chapters 7 and 8), and extending the capabilities of text generation to the visual domain (see Chapter 9). ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Learning these individual language model capabilities will equip you with the skill set to problem-solve with LLMs and build more and more advanced systems and pipelines. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Part III: Training and Fine-Tuning Language Models ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "In Part III of the book, we explore advanced concepts through training and finetuning all kinds of language models. We will explore how to create and fine-tune an embedding model (see Chapter 10), review how to fine-tune BERT for classification (see Chapter 11), and end the book with several methods for fine-tuning generation models (see Chapter 12). ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Hardware and Software Requirements ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Running generative models is generally a compute-intensive task that requires a com‐ puter with a strong GPU. Since those are not available to every reader, all examples in this book are made to run using an online platform, namely Google Colaboratory, often shortened to “Google Colab.” At the time of writing, this platform allows you to use an NVIDIA GPU (T4) for free to run your code. This GPU has 16 GB of VRAM (which is the memory of your GPU), which is the minimum amount of VRAM we expect for the examples throughout the book. ",
        "page_idx": 15
    },
    {
        "type": "image",
        "img_path": "images/e1700df727ecc305cb9b2115230a6286757935ff5aaf06419cb60a3972f12552.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Not all chapters require a minimum of 16 GB VRAM as some examples, like training and fine-tuning, are more computeintensive than others, such as prompt engineering. In the repos‐ itory, you will find the minimum GPU requirements for each chapter. ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "All code, requirements, and additional tutorials are available in this book’s repository. If you want to run the examples locally, we recommend access to an NVIDIA GPU with a minimum of $1 6 \\mathrm { G B }$ of VRAM. For a local installation, for example with conda, you can follow this setup to create your environment: ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "conda create -n thellmbook pytho $\\ d _ { 1 } = 3 . 1 \\ d 6$ conda activate thellmbook ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "You can install all the necessary dependencies by forking or cloning the repository and then running the following in your newly created Python 3.10 environment: ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "pip install -r requirements.txt ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "API Keys ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "We use both open source and proprietary models throughout the examples to demonstrate the advantages and disadvantages of both. For the proprietary models, using OpenAI and Cohere’s offering, you will need to create a free account: ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "OpenAI ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Click “sign up” on the site to create a free account. This account allows you to create an API key, which can be used to access GPT-3.5. Then, go to “API keys” to create a secret key. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Cohere ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Register a free account on the website. Then, go to “API keys” to create a secret key. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Note that with both accounts, rate limits apply and that these free API keys only allow for a limited number of calls per minute. Throughout all examples, we have taken that into account and provided local alternatives if necessary. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "For the open source models, you do not need to create an account with the exception of the Llama 2 model in Chapter 2. To use that model, you will need a Hugging Face account: ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Hugging Face ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Click “sign up” on the Hugging Face website to create a free account. Then, in “Settings” go to “Access Tokens” to create a token that you can use to download certain LLMs. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Conventions Used in This Book ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "The following typographical conventions are used in this book: ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Italic ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Indicates new terms, URLs, email addresses, filenames, and file extensions. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Constant width ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Used for program listings, as well as within paragraphs to refer to program elements such as variable or function names, databases, data types, environment variables, statements, and keywords. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Constant width bold ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Shows commands or other text that should be typed literally by the user. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Constant width italic Shows text that should be replaced with user-supplied values or by values deter‐ mined by context. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "This element signifies a tip or suggestion. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "This element signifies a general note. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Using Code Examples ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Supplemental material (code examples, exercises, etc.) is available for download at https://github.com/HandsOnLLM/Hands-On-Large-Language-Models. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "If you have a technical question or a problem using the code examples, please send email to support@oreilly.com. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "This book is here to help you get your job done. In general, if example code is offered with this book, you may use it in your programs and documentation. You do not need to contact us for permission unless you’re reproducing a significant portion of the code. For example, writing a program that uses several chunks of code from this book does not require permission. Selling or distributing examples from O’Reilly books does require permission. Answering a question by citing this book and quoting example code does not require permission. Incorporating a significant amount of example code from this book into your product’s documentation does require permission. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "We appreciate, but generally do not require, attribution. An attribution usually includes the title, author, publisher, and ISBN. For example: “Hands-On Large Lan‐ guage Models by Jay Alammar and Maarten Grootendorst (O’Reilly). Copyright 2024 Jay Alammar and Maarten Pieter Grootendorst, 978-1-098-15096-9.” ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "If you feel your use of code examples falls outside fair use or the permission given above, feel free to contact us at permissions@oreilly.com. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "O’Reilly Online Learning ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "For more than 40 years, O’Reilly Media has provided technol‐ ogy and business training, knowledge, and insight to help companies succeed. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Our unique network of experts and innovators share their knowledge and expertise through books, articles, and our online learning platform. O’Reilly’s online learning platform gives you on-demand access to live training courses, in-depth learning paths, interactive coding environments, and a vast collection of text and video from O’Reilly and $^ { 2 0 0 + }$ other publishers. For more information, visit https://oreilly.com. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "How to Contact Us ",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Please address comments and questions concerning this book to the publisher: ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "O’Reilly Media, Inc.   \n1005 Gravenstein Highway North   \nSebastopol, CA 95472   \n800-889-8969 (in the United States or Canada) 707-827-7019 (international or local)   \n707-829-0104 (fax)   \nsupport@oreilly.com   \nhttps://www.oreilly.com/about/contact.html ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "We have a web page for this book, where we list errata, examples, and any additional information. You can access this page at https://oreil.ly/hands_on_LLMs_1e. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "For news and information about our books and courses, visit https://oreilly.com. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Find us on LinkedIn: https://linkedin.com/company/oreilly-media. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Watch us on YouTube: https://youtube.com/oreillymedia. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Acknowledgments ",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Writing this book has been an incredible experience, collaboration, and journey for us. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "The field of (large) language models is one of the most dynamic areas in technology today, and within the span of writing this book, we have witnessed extraordinary advancements. Yet, despite the rapid pace of change, the fundamental principles remain strikingly consistent which made the writing process particularly intriguing. We are grateful to have had the opportunity to explore this field in-depth at such a pivotal moment. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Working with our O’Reilly team was incredible! Special thanks to Michele Cronin for her amazing feedback, support, and enthusiasm for this book from day one. We could not have asked for a better editor—you are amazing! Thank you, Nicole Butterfield, for kicking off this book and helping us maintain a structured approach throughout the writing. Thank you to Karen Montgomery for creating our wonderful cover, we love the kangaroo! Big thanks to Kate Dullea for being so patient with us having to go through hundreds of illustrations many times over. The timely early releases by Clare Laylock helped us see our work grow which was a big motivator, thank you. Thanks to Ashley Stussy and Charles Roumeliotis for the development in the final stages of the book and everyone else at O’Reilly who contributed. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Thanks to our amazing crew of technical reviewers. Invaluable feedback was given by Harm Buisman, Emir Muñoz, Luba Elliott, Guarav Chawla, Rafael V. Pierre, Luba Elliott, Tarun Narayanan, Nikhil Buduma, and Patrick Harrison. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "I’d love to extend my deepest gratitude to my family for their unwavering support and inspiration. I would like to specifically acknowledge my parents, Abdullah and Mishael, and my aunts, Hussah and Aljoharah. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "I’m grateful to the friends, colleagues, and collaborators who helped me understand and explain the tricky concepts covered in this book as well as to the Cohere folks who cultivate a supporting learning and sharing environment. Thank you to Adrien Morisot, Aidan Gomez, Andy Toulis, Anfal Alatawi, Arash Ahmadian, Bharat Venki‐ tesh, Edward Grefenstette, Ivan Zhang, Joao Araújo, Luis Serrano, Matthias Gallé, Meor Amer, Nick Frosst, Patrick Lewis, Phil Blunsom, Sara Hooker, and Suhas Pai. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "I couldn’t conceive of this project getting accomplished to the level it has without the extraordinary talent and tireless effort of Maarten, my coauthor. Your ability to repeatedly nail the technical details (from the pinned version of the nth import dependency to the latest in LLM quantization) while weaving some of the world’s best visual narratives is absolutely breathtaking. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Lastly, a tip of the hat to the incredible coffee shop scene of Riyadh, Saudi Arabia for supplying me with caffeine and a good place to focus from dawn until midnight. It’s where I read most of these papers and worked out my understanding (looking at you, Elixir Bunn). ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Maarten ",
        "text_level": 1,
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "I want to begin by expressing my heartfelt appreciation to my coauthor, Jay. Your insights have made this not only possible but incredibly fulfilling. This journey has been nothing short of amazing and collaborating with you has been an absolute joy. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "I want to sincerely thank my wonderful colleagues at IKNL for their continued support throughout this journey. A special mention goes to Harm—our Monday morning coffee breaks discussing this book were a constant source of encouragement. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Thank you to my family and friends for their unwavering support, and to my parents in particular. Pap, despite the challenges you faced, you always found a way to be there for me when I needed it most, thank you. Mam, the conversations we had as aspiring writers were wonderful and motivated me more than you could ever imagine. Thank you both for your endless support and encouragement. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Finally, I am at a loss for words to adequately express my gratitude to my wonderful wife, Ilse. Lieverd, your boundless enthusiasm and patience have been legendary, especially when I droned on about the latest LLM developments for hours on end. You are my greatest support. My apologies to my amazing daughter, Sarah. At just two years old, you already have listened to more about large language models than anyone should have to endure in a lifetime! I promise we’ll make up for it with endless playtime and adventures together. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Understanding Language Models ",
        "text_level": 1,
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "An Introduction to Large Language Models ",
        "text_level": 1,
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Humanity is at an inflection point. From 2012 onwards, developments in building AI systems (using deep neural networks) accelerated so that by the end of the decade, they yielded the first software system able to write articles indiscernible from those written by humans. This system was an AI model called Generative Pre-trained Transformer 2, or GPT-2. 2022 marked the release of ChatGPT, which demonstrated how profoundly this technology was poised to revolutionize how we interact with technology and information. Reaching one million active users in five days and then one hundred million active users in two months, the new breed of AI models started out as human-like chatbots but quickly evolved into a monumental shift in our approach to common tasks, like translation, text generation, summarization, and more. It became an invaluable tool for programmers, educators, and researchers. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "The success of ChatGPT was unprecedented and popularized more research into the technology behind it, namely large language models (LLMs). Both proprietary and public models were being released at a steady pace, closing in on, and eventually catching up to the performance of ChatGPT. It is not an exaggeration to state that almost all attention was on LLMs. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "As a result, 2023 will always be known, at least to us, as the year that drastically changed our field, Language Artificial Intelligence (Language AI), a field character‐ ized by the development of systems capable of understanding and generating human language. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "However, LLMs have been around for a while now and smaller models are still rele‐ vant to this day. LLMs are much more than just a single model and there are many other techniques and models in the field of language AI that are worth exploring. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "In this book, we aim to give readers a solid understanding of the fundamentals of both LLMs and the field of Language AI in general. This chapter serves as the scaffolding for the rest of the book and will introduce concepts and terms that we will use throughout the chapters. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "But mostly, we intend to answer the following questions in this chapter: ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "• What is Language AI?   \n• What are large language models?   \n• What are the common use cases and applications of large language models?   \n• How can we use large language models ourselves? ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "What Is Language AI? ",
        "text_level": 1,
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "The term artificial intelligence (AI) is often used to describe computer systems dedica‐ ted to performing tasks close to human intelligence, such as speech recognition, lan‐ guage translation, and visual perception. It is the intelligence of software as opposed to the intelligence of humans. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Here is a more formal definition by one of the founders of the artificial intelligence discipline: ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "[Artificial intelligence is] the science and engineering of making intelligent machines, especially intelligent computer programs. It is related to the similar task of using computers to understand human intelligence, but AI does not have to confine itself to methods that are biologically observable. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "—John McCarthy, $2 0 0 7 ^ { 1 }$ ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Due to the ever-evolving nature of AI, the term has been used to describe a wide variety of systems, some of which might not truly embody intelligent behavior. For instance, characters in computer games (NPCs [nonplayable characters]) have often been referred to as AI even though many are nothing more than if-else statements. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "Language AI refers to a subfield of AI that focuses on developing technologies capable of understanding, processing, and generating human language. The term Language AI can often be used interchangeably with natural language processing (NLP) with the continued success of machine learning methods in tackling language processing problems. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "We use the term Language AI to encompass technologies that technically might not be LLMs but still have a significant impact on the field, like how retrieval systems can give LLMs superpowers (see Chapter 8). ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Throughout this book, we want to focus on the models that have had a major role in shaping the field of Language AI. This means exploring more than just LLMs in isolation. That, however, brings us to the question: what are large language models? To begin answering this question in this chapter, let’s first explore the history of Language AI. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "A Recent History of Language AI ",
        "text_level": 1,
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "The history of Language AI encompasses many developments and models aiming to represent and generate language, as illustrated in Figure 1-1. ",
        "page_idx": 26
    },
    {
        "type": "image",
        "img_path": "images/b207fa42974dee7806a8447996f24a0915123885fce362a4956384ec29fa120f.jpg",
        "image_caption": [
            "Figure 1-1. A peek into the history of Language AI. "
        ],
        "image_footnote": [],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "Language, however, is a tricky concept for computers. Text is unstructured in nature and loses its meaning when represented by zeros and ones (individual characters). As a result, throughout the history of Language AI, there has been a large focus on representing language in a structured manner so that it can more easily be used by computers. Examples of these Language AI tasks are provided in Figure 1-2. ",
        "page_idx": 26
    },
    {
        "type": "image",
        "img_path": "images/0786527f72737086cdfb55ac634c0d7bc49950a25bed0ffec04b40d1ce37cc2a.jpg",
        "image_caption": [
            "Figure 1-2. Language AI is capable of many tasks by processing textual input. "
        ],
        "image_footnote": [],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Representing Language as a Bag-of-Words ",
        "text_level": 1,
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Our history of Language AI starts with a technique called bag-of-words, a method for representing unstructured text.2 It was first mentioned around the 1950s but became popular around the 2000s. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "Bag-of-words works as follows: let’s assume that we have two sentences for which we want to create numerical representations. The first step of the bag-of-words model is tokenization, the process of splitting up the sentences into individual words or subwords (tokens), as illustrated in Figure 1-3. ",
        "page_idx": 27
    },
    {
        "type": "image",
        "img_path": "images/989d490a958de17822c93eba848b39b68db6a9d32f0f819da7943ed77daba793.jpg",
        "image_caption": [
            "Figure 1-3. Each sentence is split into words (tokens) by splitting on a whitespace. "
        ],
        "image_footnote": [],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "The most common method for tokenization is by splitting on a whitespace to create individual words. However, this has its disadvantages as some languages, like Man‐ darin, do not have whitespaces around individual words. In the next chapter, we will go in depth about tokenization and how that technique influences language models. As illustrated in Figure 1-4, after tokenization, we combine all unique words from each sentence to create a vocabulary that we can use to represent the sentences. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/5f64cb4255c817dbd83633b9b94ba6fa08bc3c8603ca4e069d6f291b47e148a1.jpg",
        "image_caption": [
            "Figure 1-4. A vocabulary is created by retaining all unique words across both sentences. "
        ],
        "image_footnote": [],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Using our vocabulary, we simply count how often a word in each sentence appears, quite literally creating a bag of words. As a result, a bag-of-words model aims to create representations of text in the form of numbers, also called vectors or vector representations, observed in Figure 1-5. Throughout the book, we refer to these kinds of models as representation models. ",
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/1c4a71cce28f9d871cf81a14d6eb667f5d5bbd6cffb8690d472ce23f31ddd45c.jpg",
        "image_caption": [
            "Figure 1-5. A bag-of-words is created by counting individual words. These values are referred to as vector representations. "
        ],
        "image_footnote": [],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Although bag-of-words is a classic method, it is by no means completely obsolete. In Chapter 5, we will explore how it can still be used to complement more recent language models. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Better Representations with Dense Vector Embeddings ",
        "text_level": 1,
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Bag-of-words, although an elegant approach, has a flaw. It considers language to be nothing more than an almost literal bag of words and ignores the semantic nature, or meaning, of text. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Released in 2013, word2vec was one of the first successful attempts at capturing the meaning of text in embeddings.3 Embeddings are vector representations of data that attempt to capture its meaning. To do so, word2vec learns semantic representations of words by training on vast amounts of textual data, like the entirety of Wikipedia. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "To generate these semantic representations, word2vec leverages neural networks. These networks consist of interconnected layers of nodes that process information. As illustrated in Figure 1-6, neural networks can have many layers where each connection has a certain weight depending on the input. These weights are often referred to as the parameters of the model. ",
        "page_idx": 29
    },
    {
        "type": "image",
        "img_path": "images/da8cf7c8c75474b6ff272fb6f631b6a1fcdc8b9a2af6b87243fb0afadc34d744.jpg",
        "image_caption": [
            "Figure 1-6. A neural network consists of interconnected layers of nodes where each connection is a linear equation. "
        ],
        "image_footnote": [],
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Using these neural networks, word2vec generates word embeddings by looking at which other words they tend to appear next to in a given sentence. We start by assigning every word in our vocabulary with a vector embedding, say of 50 values for each word initialized with random values. Then in every training step, as illustrated in Figure 1-7, we take pairs of words from the training data and a model attempts to predict whether or not they are likely to be neighbors in a sentence. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "During this training process, word2vec learns the relationship between words and distills that information into the embedding. If the two words tend to have the same neighbors, their embeddings will be closer to one another and vice versa. In Chapter 2, we will look closer at word2vec’s training procedure. ",
        "page_idx": 30
    },
    {
        "type": "image",
        "img_path": "images/6d4e9f5f423bcc52ce1e36ffc94970ebcf6ecfde8b605502d46afa1a0f9b5f59.jpg",
        "image_caption": [
            "Figure 1-7. A neural network is trained to predict if two words are neighbors. During this process, the embeddings are updated to be in line with the ground truth. "
        ],
        "image_footnote": [],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "The resulting embeddings capture the meaning of words but what exactly does that mean? To illustrate this phenomenon, let’s somewhat oversimplify and imagine we have embeddings of several words, namely “apple” and “baby.” Embeddings attempt to capture meaning by representing the properties of words. For instance, the word “baby” might score high on the properties “newborn” and “human” while the word “apple” scores low on these properties. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 1-8, embeddings can have many properties to represent the meaning of a word. Since the size of embeddings is fixed, their properties are chosen to create a mental representation of the word. ",
        "page_idx": 30
    },
    {
        "type": "image",
        "img_path": "images/6c867208a0b31e397d1d43704b46ba5c756c35afb5da7387f2a5a6e4b8f92e3f.jpg",
        "image_caption": [
            "Figure 1-8. The values of embeddings represent properties that are used to represent words. We may oversimplify by imagining that dimensions represent concepts (which they don’t), but it helps express the idea. "
        ],
        "image_footnote": [],
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "In practice, these properties are often quite obscure and seldom relate to a single entity or humanly identifiable concept. However, together, these properties make sense to a computer and serve as a good way to translate human language into computer language. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Embeddings are tremendously helpful as they allow us to measure the semantic similarity between two words. Using various distance metrics, we can judge how close one word is to another. As illustrated in Figure 1-9, if we were to compress these embeddings into a two-dimensional representation, you would notice that words with similar meaning tend to be closer. In Chapter 5, we will explore how to compress these embeddings into $n$ -dimensional space. ",
        "page_idx": 31
    },
    {
        "type": "image",
        "img_path": "images/e4f420cf6bc2ed6312e0a73fef21f57d6047d63a3ac0ad7371e1d0c1f0edb3ca.jpg",
        "image_caption": [
            "Figure 1-9. Embeddings of words that are similar will be close to each other in dimen‐ sional space. "
        ],
        "image_footnote": [],
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Types of Embeddings ",
        "text_level": 1,
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "There are many types of embeddings, like word embeddings and sentence embed‐ dings that are used to indicate different levels of abstractions (word versus sentence), as illustrated in Figure 1-10. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Bag-of-words, for instance, creates embeddings at a document level since it repre‐ sents the entire document. In contrast, word2vec generates embeddings for words only. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Throughout the book, embeddings will take on a central role as they are utilized in many use cases, such as classification (see Chapter 4), clustering (see Chapter 5), and semantic search and retrieval-augmented generation (see Chapter 8). In Chapter 2, we will take our first deep dive into token embeddings. ",
        "page_idx": 31
    },
    {
        "type": "image",
        "img_path": "images/57e990ffd4aff594696292bb523fceb29d393916990026448bc1f4acd67bbf97.jpg",
        "image_caption": [
            "Figure 1-10. Embeddings can be created for different types of input. "
        ],
        "image_footnote": [],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Encoding and Decoding Context with Attention ",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "The training process of word2vec creates static, downloadable representations of words. For instance, the word “bank” will always have the same embedding regardless of the context in which it is used. However, “bank” can refer to both a financial bank as well as the bank of a river. Its meaning, and therefore its embeddings, should change depending on the context. ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "A step in encoding this text was achieved through recurrent neural networks (RNNs). These are variants of neural networks that can model sequences as an additional input. ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "To do so, these RNNs are used for two tasks, encoding or representing an input sentence and decoding or generating an output sentence. Figure 1-11 illustrates this concept by showing how a sentence like $^ { \\mathfrak { c } } \\mathrm { I }$ love llamas” gets translated to the Dutch “Ik hou van lama’s.” ",
        "page_idx": 32
    },
    {
        "type": "image",
        "img_path": "images/6ec142c6658af6aeddafdd21c0e1f4010268032b7e0cdeb1fe8c94e46dd8a9be.jpg",
        "image_caption": [
            "Figure 1-11. Two recurrent neural networks (decoder and encoder) translating an input sequence from English to Dutch. "
        ],
        "image_footnote": [],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "Each step in this architecture is autoregressive. When generating the next word, this architecture needs to consume all previously generated words, as shown in Figure 1-12. ",
        "page_idx": 33
    },
    {
        "type": "image",
        "img_path": "images/bc98f2d5410ab7b5a98bfc6bd307e7221a8df8e2f70a2aa376a13a87224f1a18.jpg",
        "image_caption": [
            "Figure 1-12. Each previous output token is used as input to generate the next token. "
        ],
        "image_footnote": [],
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "The encoding step aims to represent the input as well as possible, generating the context in the form of an embedding, which serves as the input for the decoder. To generate this representation, it takes embeddings as its inputs for words, which means we can use word2vec for the initial representations. In Figure 1-13, we can observe this process. Note how the inputs are processed sequentially, one at a time, as well as the output. ",
        "page_idx": 33
    },
    {
        "type": "image",
        "img_path": "images/f549a5155a751b84e4daa94ab82979215ec41d5cb450f31e2ec021adc9bd2905.jpg",
        "image_caption": [
            "Figure 1-13. Using word2vec embeddings, a context embedding is generated that repre‐ sents the entire sequence. "
        ],
        "image_footnote": [],
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "This context embedding, however, makes it difficult to deal with longer sentences since it is merely a single embedding representing the entire input. In 2014, a solution called attention was introduced that highly improved upon the original architecture.4 Attention allows a model to focus on parts of the input sequence that are relevant to one another (“attend” to each other) and amplify their signal, as shown in Fig‐ ure 1-14. Attention selectively determines which words are most important in a given sentence. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "For instance, the output word “lama’s” is Dutch for “llamas,” which is why the atten‐ tion between both is high. Similarly, the words “lama’s” and “I” have lower attention since they aren’t as related. In Chapter 3, we will go more in depth on the attention mechanism. ",
        "page_idx": 34
    },
    {
        "type": "image",
        "img_path": "images/b69b051fdefe0464c5e4c8d4f47c8ab3d1c32b1850b836069b439eecd65cbbcd.jpg",
        "image_caption": [
            "Figure 1-14. Attention allows a model to “attend” to certain parts of sequences that might relate more or less to one another. "
        ],
        "image_footnote": [],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "By adding these attention mechanisms to the decoder step, the RNN can generate signals for each input word in the sequence related to the potential output. Instead of passing only a context embedding to the decoder, the hidden states of all input words are passed. This process is demonstrated in Figure 1-15. ",
        "page_idx": 35
    },
    {
        "type": "image",
        "img_path": "images/656f282dd4254190db4561e7d9ad3a8e89c4386136512e08466998ea96cf6f8e.jpg",
        "image_caption": [
            "Figure 1-15. After generating the words “Ik,” “hou,” and “van,” the attention mechanism of the decoder enables it to focus on the word “llamas” before it generates the Dutch translation (“lama’s”). "
        ],
        "image_footnote": [],
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "As a result, during the generation of “Ik hou van lama’s,” the RNN keeps track of the words it mostly attends to perform the translation. Compared to word2vec, this architecture allows for representing the sequential nature of text and the context in which it appears by “attending” to the entire sentence. This sequential nature, however, precludes parallelization during training of the model. ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "Attention Is All You Need ",
        "text_level": 1,
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "The true power of attention, and what drives the amazing abilities of large lan‐ guage models, was first explored in the well-known “Attention is all you need” paper released in 2017.5 The authors proposed a network architecture called the Transformer, which was solely based on the attention mechanism and removed the recurrence network that we saw previously. Compared to the recurrence network, the Transformer could be trained in parallel, which tremendously sped up training. ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "In the Transformer, encoding and decoder components are stacked on top of each other, as illustrated in Figure 1-16. This architecture remains autoregressive, needing to consume each generated word before creating a new word. ",
        "page_idx": 36
    },
    {
        "type": "image",
        "img_path": "images/9582283a3adc69fb053cbb4b25b5b3e3e2acc46f5dd3632b37d8e9a174d5b648.jpg",
        "image_caption": [
            "Figure 1-16. The Transformer is a combination of stacked encoder and decoder blocks where the input flows through each encoder and decoder. "
        ],
        "image_footnote": [],
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Now, both the encoder and decoder blocks would revolve around attention instead of leveraging an RNN with attention features. The encoder block in the Transformer consists of two parts, self-attention and a feedforward neural network, which are shown in Figure 1-17. ",
        "page_idx": 36
    },
    {
        "type": "image",
        "img_path": "images/ed2a35607fe98e41bf1dcf244df130916a43f8eb6599a4c233954da6646302e5.jpg",
        "image_caption": [
            "Figure 1-17. An encoder block revolves around self-attention to generate intermediate representations. "
        ],
        "image_footnote": [],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "Compared to previous methods of attention, self-attention can attend to different positions within a single sequence, thereby more easily and accurately representing the input sequence as illustrated in Figure 1-18. Instead of processing one token at a time, it can be used to look at the entire sequence in one go. ",
        "page_idx": 37
    },
    {
        "type": "image",
        "img_path": "images/a92160e8cdedf7984266d79b61fe6a94808c0341ea0327d727d7cda1612664f5.jpg",
        "image_caption": [
            "Figure 1-18. Self-attention attends to all parts of the input sequence so that it can “look” both forward and back in a single sequence. "
        ],
        "image_footnote": [],
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "Compared to the encoder, the decoder has an additional layer that pays attention to the output of the encoder (to find the relevant parts of the input). As demonstrated in Figure 1-19, this process is similar to the RNN attention decoder that we discussed previously. ",
        "page_idx": 37
    },
    {
        "type": "image",
        "img_path": "images/bbb6aebb2c924e4dea92005b1cb73414d3318ba121b0d6bb2e356611c977d84c.jpg",
        "image_caption": [
            "Figure 1-19. The decoder has an additional attention layer that attends to the output of the encoder. "
        ],
        "image_footnote": [],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "As shown in Figure 1-20, the self-attention layer in the decoder masks future posi‐ tions so it only attends to earlier positions to prevent leaking information when generating the output. ",
        "page_idx": 38
    },
    {
        "type": "image",
        "img_path": "images/b25f25af01ac6b1d57c685151b0632c129249f52727e65b3e78a2010aa56492a.jpg",
        "image_caption": [
            "Figure 1-20. Only attend to previous tokens to prevent “looking into the future.” "
        ],
        "image_footnote": [],
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Together, these building blocks create the Transformer architecture and are the foun‐ dation of many impactful models in Language AI, such as BERT and GPT-1, which we cover later in this chapter. Throughout this book, most models that we will use are Transformer-based models. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "There is much more to the Transformer architecture than what we explored thus far. In Chapters 2 and 3, we will go through the many reasons why Transformer models work so well, including multi-head attention, positional embeddings, and layer normalization. ",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "Representation Models: Encoder-Only Models ",
        "text_level": 1,
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "The original Transformer model is an encoder-decoder architecture that serves trans‐ lation tasks well but cannot easily be used for other tasks, like text classification. ",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "In 2018, a new architecture called Bidirectional Encoder Representations from Trans‐ formers (BERT) was introduced that could be leveraged for a wide variety of tasks and would serve as the foundation of Language AI for years to come.6 BERT is an encoder-only architecture that focuses on representing language, as illustrated in Figure 1-21. This means that it only uses the encoder and removes the decoder entirely. ",
        "page_idx": 39
    },
    {
        "type": "image",
        "img_path": "images/e48deaa01c1dcb4aa9131bcc30341cc657733765439db583485b5fce88f42a4e.jpg",
        "image_caption": [
            "Figure 1-21. The architecture of a BERT base model with 12 encoders. "
        ],
        "image_footnote": [],
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "These encoder blocks are the same as we saw before: self-attention followed by feedforward neural networks. The input contains an additional token, the [CLS] or classification token, which is used as the representation for the entire input. Often, we use this [CLS] token as the input embedding for fine-tuning the model on specific tasks, like classification. ",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "Training these encoder stacks can be a difficult task that BERT approaches by adopt‐ ing a technique called masked language modeling (see Chapters 2 and 11). As shown in Figure 1-22, this method masks a part of the input for the model to predict. This prediction task is difficult but allows BERT to create more accurate (intermediate) representations of the input. ",
        "page_idx": 40
    },
    {
        "type": "image",
        "img_path": "images/bb69d94d14e53592bc227a447097ef71c4253d733731b259debe9f3fb56f538a.jpg",
        "image_caption": [
            "Figure 1-22. Train a BERT model by using masked language modeling. "
        ],
        "image_footnote": [],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "This architecture and training procedure makes BERT and related architectures incredible at representing contextual language. BERT-like models are commonly used for transfer learning, which involves first pretraining it for language modeling and then fine-tuning it for a specific task. For instance, by training BERT on the entirety of Wikipedia, it learns to understand the semantic and contextual nature of text. Then, as shown in Figure 1-23, we can use that pretrained model to fine-tune it for a specific task, like text classification. ",
        "page_idx": 40
    },
    {
        "type": "image",
        "img_path": "images/7818123a929ecf880c56c204b09b1f1ef31ad4fdacab211e9e7e6c0e61399e08.jpg",
        "image_caption": [
            "Figure 1-23. After pretraining BERT on masked language model, we fine-tune it for specific tasks. "
        ],
        "image_footnote": [],
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "A huge benefit of pretrained models is that most of the training is already done for us. Fine-tuning on specific tasks is generally less compute-intensive and requires less data. Moreover, BERT-like models generate embeddings at almost every step in their architecture. This also makes BERT models feature extraction machines without the need to fine-tune them on a specific task. ",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Encoder-only models, like BERT, will be used in many parts of the book. For years, they have been and are still used for common tasks, including classification tasks (see Chapter 4), clustering tasks (see Chapter 5), and semantic search (see Chapter 8). ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Throughout the book, we will refer to encoder-only models as representation models to differentiate them from decoder-only, which we refer to as generative models. Note that the main distinction does not lie between the underlying architecture and the way these models work. Representation models mainly focus on representing language, for instance, by creating embeddings, and typically do not generate text. In contrast, generative models focus primarily on generating text and typically are not trained to generate embeddings. ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "The distinction between representation and generative models and components will also be shown in most images. Representation models are teal with a small vector icon (to indicate its focus on vectors and embeddings) whilst generative models are pink with a small chat icon (to indicate its generative capabilities). ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Generative Models: Decoder-Only Models ",
        "text_level": 1,
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "Similar to the encoder-only architecture of BERT, a decoder-only architecture was proposed in 2018 to target generative tasks.7 This architecture was called a Generative Pre-trained Transformer (GPT) for its generative capabilities (it’s now known as GPT-1 to distinguish it from later versions). As shown in Figure 1-24, it stacks decoder blocks similar to the encoder-stacked architecture of BERT. ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "GPT-1 was trained on a corpus of 7,000 books and Common Crawl, a large dataset of web pages. The resulting model consisted of 117 million parameters. Each parameter is a numerical value that represents the model’s understanding of language. ",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "If everything remains the same, we expect more parameters to greatly influence the capabilities and performance of language models. Keeping this in mind, we saw larger and larger models being released at a steady pace. As illustrated in Figure 1-25, GPT-2 had 1.5 billion parameters8 and GPT-3 used 175 billion parameters9 quickly followed. ",
        "page_idx": 41
    },
    {
        "type": "image",
        "img_path": "images/b5c32087cb63d999f866a528b2bcea837a1ba5f90428119dc1fa3f4d2015e7ac.jpg",
        "image_caption": [
            "Figure 1-24. The architecture of a GPT-1. It uses a decoder-only architecture and removes the encoder-attention block. "
        ],
        "image_footnote": [],
        "page_idx": 42
    },
    {
        "type": "image",
        "img_path": "images/086aca0ca5c28d176eb1674b09a518ba7d6212ad3a4ab9d4b5f1f5fee130a0f1.jpg",
        "image_caption": [
            "Figure 1-25. GPT models quickly grew in size with each iteration. "
        ],
        "image_footnote": [],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "These generative decoder-only models, especially the “larger” models, are commonly referred to as large language models (LLMs). As we will discuss later in this chapter, the term LLM is not only reserved for generative models (decoder-only) but also representation models (encoder-only). ",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "Generative LLMs, as sequence-to-sequence machines, take in some text and attempt to autocomplete it. Although a handy feature, their true power shone from being trained as a chatbot. Instead of completing a text, what if they could be trained to answer questions? By fine-tuning these models, we can create instruct or chat models that can follow directions. ",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 1-26, the resulting model could take in a user query (prompt) and output a response that would most likely follow that prompt. As such, you will often hear that generative models are completion models. ",
        "page_idx": 43
    },
    {
        "type": "image",
        "img_path": "images/7753b7595e3b41cf99e08d95960b3cd7816d01cb299455201d4eebcad5bc3330.jpg",
        "image_caption": [
            "Figure 1-26. Generative LLMs take in some input and try to complete it. With instruct models, this is more than just autocomplete and attempts to answer the question. "
        ],
        "image_footnote": [],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "A vital part of these completion models is something called the context length or context window. The context length represents the maximum number of tokens the model can process, as shown in Figure 1-27. A large context window allows entire documents to be passed to the LLM. Note that due to the autoregressive nature of these models, the current context length will increase as new tokens are generated. ",
        "page_idx": 43
    },
    {
        "type": "image",
        "img_path": "images/53df94a10a0bf9bd286d8143c49b811e2fda72449627c0e8b0bea89f8639dfa7.jpg",
        "image_caption": [
            "Figure 1-27. The context length is the maximum context an LLM can handle. "
        ],
        "image_footnote": [],
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "The Year of Generative AI ",
        "text_level": 1,
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "LLMs had a tremendous impact on the field and led some to call 2023 The Year of Generative AI with the release, adoption, and media coverage of ChatGPT (GPT-3.5). When we refer to ChatGPT, we are actually talking about the product and not the underlying model. When it was first released, it was powered by the GPT-3.5 LLM and has since then grown to include several more performant variants, such as GPT-4.10 ",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "GPT-3.5 was not the only model that made its impact in the Year of Generative AI. As illustrated in Figure 1-28, both open source and proprietary LLMs have made their way to the people at an incredible pace. These open source base models are often referred to as foundation models and can be fine-tuned for specific tasks, like following instructions. ",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Proprietary models ",
        "text_level": 1,
        "page_idx": 45
    },
    {
        "type": "image",
        "img_path": "images/89c4348b4591c275044513787ec9a7906c24a259f6e712fe8aa6d6881e8f5c2b.jpg",
        "image_caption": [
            "Figure 1-28. A comprehensive view into the Year of Generative AI. Note that many models are still missing from this overview! "
        ],
        "image_footnote": [],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "Apart from the widely popular Transformer architecture, new promising architec‐ tures have emerged such as Mamba11,12 and RWKV.13 These novel architectures attempt to reach Transformer-level performance with additional advantages, like larger context windows or faster inference. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "These developments exemplify the evolution of the field and showcase 2023 as a truly hectic year for AI. It took all we had to just keep up with the many developments, both within and outside of Language AI. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "As such, this book explores more than just the latest LLMs. We will explore how other models, such as embedding models, encoder-only models, and even bag-ofwords can be used to empower LLMs. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "The Moving Definition of a “Large Language Model” ",
        "text_level": 1,
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "In our travels through the recent history of Language AI, we observed that primarily generative decoder-only (Transformer) models are commonly referred to as large language models. Especially if they are considered to be “large.” In practice, this seems like a rather constrained description! ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "What if we create a model with the same capabilities as GPT-3 but 10 times smaller? Would such a model fall outside the “large” language model categorization? ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "Similarly, what if we released a model as big as GPT-4 that can perform accurate text classification but does not have any generative capabilities? Would it still qualify as a large “language model” if its primary function is not language generation, even though it still represents text? ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "The problem with these kinds of definitions is that we exclude capable models. What name we give one model or the other does not change how it behaves. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "Since the definition of the term “large language model” tends to evolve with the release of new models, we want to be explicit in what it means for this book. “Large” is arbitrary and what might be considered a large model today could be small tomor‐ row. There are currently many names for the same thing and to us, “large language models” are also models that do not generate text and can be run on consumer hardware. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "As such, aside from covering generative models, this book will also cover models with fewer than 1 billion parameters that do not generate text. We will explore how other models, such as embedding models, representation models, and even bag-of-words can be used to empower LLMs. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "The Training Paradigm of Large Language Models ",
        "text_level": 1,
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "Traditional machine learning generally involves training a model for a specific task, like classification. As shown in Figure 1-29, we consider this to be a one-step process. ",
        "page_idx": 46
    },
    {
        "type": "image",
        "img_path": "images/57e62396ee56555eed5ddcccfebbde257437301b9a97628da56a8683ace6da8d.jpg",
        "image_caption": [
            "Figure 1-29. Traditional machine learning involves a single step: training a model for a specific target task, like classification or regression. "
        ],
        "image_footnote": [],
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "Creating LLMs, in contrast, typically consists of at least two steps: ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "Language modeling ",
        "text_level": 1,
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "The first step, called pretraining, takes the majority of computation and training time. An LLM is trained on a vast corpus of internet text allowing the model to learn grammar, context, and language patterns. This broad training phase is not yet directed toward specific tasks or applications beyond predicting the next word. The resulting model is often referred to as a foundation model or base model. These models generally do not follow instructions. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "Fine-tuning ",
        "text_level": 1,
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "The second step, fine-tuning or sometimes post-training, involves using the previ‐ ously trained model and further training it on a narrower task. This allows the LLM to adapt to specific tasks or to exhibit desired behavior. For example, we could fine-tune a base model to perform well on a classification task or to follow instructions. It saves massive amounts of resources because the pretraining phase is quite costly and generally requires data and computing resources that are out of the reach of most people and organizations. For instance, Llama 2 has been trained on a dataset containing 2 trillion tokens.14 Imagine the compute necessary to create that model! In Chapter 12, we will go over several methods for fine-tuning foundation models on your dataset. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "Any model that goes through the first step, pretraining, we consider a pretrained model, which also includes fine-tuned models. This two-step approach of training is visualized in Figure 1-30. ",
        "page_idx": 47
    },
    {
        "type": "image",
        "img_path": "images/5671cea3e05231a6aff4c3d51abb8d733e7e83faf11e458339e446cd05a8bb21.jpg",
        "image_caption": [
            "Figure 1-30. Compared to traditional machine learning, LLM training takes a multistep approach. "
        ],
        "image_footnote": [],
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "Additional fine-tuning steps can be added to further align the model with the user’s preferences, as we will explore in Chapter 12. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "Large Language Model Applications: What Makes Them So Useful? ",
        "text_level": 1,
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "The nature of LLMs makes them suitable for a wide range of tasks. With text genera‐ tion and prompting, it almost seems as if your imagination is the limit. To illustrate, let’s explore some common tasks and techniques: ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Detecting whether a review left by a customer is positive or negative This is (supervised) classification and can be handled with both encoder- and decoder-only models either with pretrained models (see Chapter 4) or by finetuning models (see Chapter 11). ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Developing a system for finding common topics in ticket issues This is (unsupervised) classification for which we have no predefined labels. We can leverage encoder-only models to perform the classification itself and decoder-only models for labeling the topics (see Chapter 5). ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Building a system for retrieval and inspection of relevant documents ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "A major component of language model systems is their ability to add external resources of information. Using semantic search, we can build systems that allow us to easily access and find information for an LLM to use (see Chapter 8). Improve your system by creating or fine-tuning a custom embedding model (see Chapter 12). ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Constructing an LLM chatbot that can leverage external resources, such as tools and documents ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "This is a combination of techniques that demonstrates how the true power of LLMs can be found through additional components. Methods such as prompt engineering (see Chapter 6), retrieval-augmented generation (see Chapter 8), and fine-tuning an LLM (see Chapter 12) are all pieces of the LLM puzzle. ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Constructing an LLM capable of writing recipes based on a picture showing the products in your fridge ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "This is a multimodal task where the LLM takes in an image and reasons about what it sees (see Chapter 9). LLMs are being adapted to other modalities, such as Vision, which opens a wide variety of interesting use cases. ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "LLM applications are incredibly satisfying to create since they are partially bounded by the things you can imagine. As these models grow more accurate, using them in practice for creative use cases such as role-playing and writing children’s books simply becomes more and more fun. ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Responsible LLM Development and Usage ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "The impact of LLMs has been and likely continues to be significant due to their wide‐ spread adoption. As we explore the incredible capabilities of LLMs it is important to keep their societal and ethical implications in mind. Several key points to consider: ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Bias and fairness ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "LLMs are trained on large amounts of data that might contain biases. LLMs might learn from these biases, start to reproduce them, and potentially amplify them. Since the data on which LLMs are trained are seldom shared, it remains unclear what potential biases they might contain unless you try them out. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Transparency and accountability ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Due to LLMs’ incredible capabilities, it is not always clear when you are talking with a human or an LLM. As such, the usage of LLMs when interacting with humans can have unintended consequences when there is no human in the loop. For instance, LLM-based applications used in the medical field might be regulated as medical devices since they could affect a patient’s well-being. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Generating harmful content ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "An LLM does not necessarily generate ground-truth content and might confi‐ dently output incorrect text. Moreover, they can be used to generate fake news, articles, and other misleading sources of information. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Intellectual property ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Is the output of an LLM your intellectual property or that of the LLM’s creator? When the output is similar to a phrase in the training data, does the intellectual property belong to the author of that phrase? Without access to the training data it remains unclear when copyrighted material is being used by the LLM. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Regulation ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Due to the enormous impact of LLMs, governments are starting to regulate commercial applications. An example is the European AI Act, which regulates the development and deployment of foundation models including LLMs. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "As you develop and use LLMs, we want to stress the importance of ethical considera‐ tions and urge you to learn more about the safe and responsible use of LLMs and AI systems in general. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "Limited Resources Are All You Need ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "The compute resources that we have referenced several times thus far generally relate to the GPU(s) you have available on your system. A powerful GPU (graphics card) will make both training and using LLMs much more efficient and faster. ",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "In choosing a GPU, an important component is the amount of VRAM (video random-access memory) you have available. This refers to the amount of memory you have available on your GPU. In practice, the more VRAM you have the better. The reason for this is that some models simply cannot be used at all if you do not have sufficient VRAM. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Because training and fine-tuning LLMs can be an expensive process, GPU-wise, those without a powerful GPU have often been referred to as the GPU-poor. This illustrates the battle for computing resources to train these huge models. To create the Llama 2 family of models, for example, Meta used A100-80 GB GPUs. Assuming renting such a GPU would cost $\\$ 1.50/\\mathrm { h r }$ the total costs of creating these models would exceed $\\$ 5,000,000$ 5 ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Unfortunately, there is no single rule to determine exactly how much VRAM you need for a specific model. It depends on the model’s architecture and size, compres‐ sion technique, context size, backend for running the model, etc. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "This book is for the GPU-poor! We will use models that users can run without the most expensive GPU(s) available or a big budget. To do so, we will make all the code available in Google Colab instances. At the time of writing, a free instance of Google Colab will net you a T4 GPU with 16 GB VRAM, which is the minimum amount of VRAM that we suggest. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Interfacing with Large Language Models ",
        "text_level": 1,
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Interfacing with LLMs is a vital component of not only using them but also develop‐ ing an understanding of their inner workings. Due to the many developments in the field, there has been an abundance of techniques, methods, and packages for communicating with LLMs. Throughout the book, we intend to explore the most common techniques for doing so, including using both proprietary (closed source) and publicly available open models. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Proprietary, Private Models ",
        "text_level": 1,
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Closed source LLMs are models that do not have their weights and architecture shared with the public. They are developed by specific organizations with their underlying code being kept secret. Examples of such models include OpenAI’s GPT-4 and Anthropic’s Claude. These proprietary models are generally backed by significant commercial support and have been developed and integrated within their services. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "You can access these models through an interface that communicates with the LLM, called an API (application programming interface), as illustrated in Figure 1-31. For instance, to use ChatGPT in Python you can use OpenAI’s package to interface with the service without directly accessing it. ",
        "page_idx": 51
    },
    {
        "type": "image",
        "img_path": "images/b8a3787fdf3f5bae26705f128201cbaa943314e0fe56f54ad5280cdc44b3eb91.jpg",
        "image_caption": [
            "Figure 1-31. Closed source LLMs are accessed by an interface (API). As a result, details of the LLM itself, including its code and architecture are not shared with the user. "
        ],
        "image_footnote": [],
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "A huge benefit of proprietary models is that the user does not need to have a strong GPU to use the LLM. The provider takes care of hosting and running the model and generally has more computing available. There is no expertise necessary concerning hosting and using the model, which lowers the barrier to entry significantly. More‐ over, these models tend to be more performant than their open source counterparts due to the significant investment from these organizations. ",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "A downside to this is that it can be a costly service. The provider manages the risk and costs of hosting the LLM, which often translates to a paid service. Moreover, since there is no direct access to the model, there is no method to fine-tune it yourself. Lastly, your data is shared with the provider, which is not desirable in many common use cases, such as sharing patient data. ",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "Open Models ",
        "text_level": 1,
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "Open LLMs are models that share their weights and architecture with the public to use. They are still developed by specific organizations but often share their code for creating or running the model locally—with varying levels of licensing that may or may not allow commercial usage of the model. Cohere’s Command R, the Mistral models, Microsoft’s Phi, and Meta’s Llama models are all examples of open models. ",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "There are ongoing discussions as to what truly represents an open source model. For instance, some publicly shared models have a permissive commercial license, which means that the model cannot be used for commercial purposes. For many, this is not the true definition of open source, which states that using these models should not have any restrictions. Similarly, the data on which a model is trained as well as its source code are seldom shared. ",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "You can download these models and use them on your device as long as you have a powerful GPU that can handle these kinds of models, as shown in Figure 1-32. ",
        "page_idx": 52
    },
    {
        "type": "image",
        "img_path": "images/e97ad625074d9c3510b8ae3b60a8bce801a9e6b5395e72d04fdf3ba26216f24e.jpg",
        "image_caption": [
            "Figure 1-32. Open source LLMs are directly by the user. As a result, details of the LLM itself including its code and architecture are shared with the user. "
        ],
        "image_footnote": [],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "A major advantage of these local models is that you, the user, have complete control over the model. You can use the model without depending on the API connection, fine-tune it, and run sensitive data through it. You are not dependent on any service and have complete transparency of the processes that lead to the output of the model. This benefit is enhanced by the large communities that enable these processes, such as Hugging Face, demonstrating the possibilities of collaborative efforts. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "A downside is that you need powerful hardware to run these models and even more when training or fine-tuning them. Moreover, it requires specific knowledge to set up and use these models (which we will cover throughout this book). ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "We generally prefer using open source models wherever we can. The freedom this gives to play around with options, explore the inner workings, and use the model locally arguably provides more benefits than using proprietary LLMs. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Open Source Frameworks ",
        "text_level": 1,
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Compared to closed source LLMs, open source LLMs require you to use certain pack‐ ages to run them. In 2023, many different packages and frameworks were released that, each in their own way, interact with and make use of LLMs. Wading through hundreds upon hundreds of potentially worthwhile frameworks is not the most enjoyable experience. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "As a result, you might even miss your favorite framework in this book! ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Instead of attempting to cover every LLM framework in existence (there are too many, and they continue to grow in number), we aim to provide you with a solid foundation for leveraging LLMs. The idea is that after reading this book, you can easily pick up most other frameworks as they all work in a very similar manner. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "The intuition that we attempt to realize is an important component of this. If you have an intuitive understanding of not only LLMs but also using them in practice with common frameworks, branching out to others should be a straightforward task. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "More specifically, we focus on backend packages. These are packages without a GUI (graphical user interface) that are created for efficiently loading and running any LLM on your device, such as llama.cpp, LangChain, and the core of many frameworks, Hugging Face Transformers. ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "We will mostly cover frameworks for interacting with large lan‐ guage models through code. Although it helps you learn the fundamentals of these frameworks, sometimes you just want a ChatGPT-like interface with a local LLM. Fortunately, there are many incredible frameworks that allow for this. A few examples include text-generation-webui, KoboldCpp, and LM Studio. ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "Generating Your First Text ",
        "text_level": 1,
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "An important component of using language models is selecting them. The main source for finding and downloading LLMs is the Hugging Face Hub. Hugging Face is the organization behind the well-known Transformers package, which for years has driven the development of language models in general. As the name implies, the package was built on top of the transformers framework that we discussed in “A Recent History of Language AI” on page 5. ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "At the time of writing, you will find more than 800,000 models on Hugging Face’s platform for many different purposes, from LLMs and computer vision models to models that work with audio and tabular data. Here, you can find almost any open source LLM. ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "Although we will explore all kinds of models throughout this book, let’s start our first lines of code with a generative model. The main generative model we use throughout the book is Phi-3-mini, which is a relatively small (3.8 billion parameters) but quite performant model.16 Due to its small size, the model can be run on devices with less than 8 GB of VRAM. If you perform quantization, a type of compression that we will further discuss in Chapters 7 and 12, you can use even less than 6 GB of VRAM. Moreover, the model is licensed under the MIT license, which allows the model to be used for commercial purposes without constraints! ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "Keep in mind that new and improved LLMs are frequently released. To ensure this book remains current, most examples are designed to work with any LLM. We’ll also highlight different models in the repository associated with this book for you to try out. ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "Let’s get started! When you use an LLM, two models are loaded: ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "• The generative model itself • Its underlying tokenizer ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "The tokenizer is in charge of splitting the input text into tokens before feeding it to the generative model. You can find the tokenizer and model on the Hugging Face site and only need the corresponding IDs to be passed. In this case, we use “microsoft/ Phi-3-mini-4k-instruct” as the main path to the model. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "We can use transformers to load both the tokenizer and model. Note that we assume you have an NVIDIA GPU (device_map $| =$ \"cuda\") but you can choose a different device instead. If you do not have access to a GPU you can use the free Google Colab notebooks we made available in the repository of this book: ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "from transformers import AutoModelForCausalLM, AutoTokenizer   \n# Load model and tokenizer   \nmodel $=$ AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype $=$ \"auto\", trust_remote_code=True,   \n)   \ntokenizer $=$ AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Running the code will start downloading the model and depending on your internet connection can take a couple of minutes. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Although we now have enough to start generating text, there is a nice trick in trans‐ formers that simplifies the process, namely transformers.pipeline. It encapsulates the model, tokenizer, and text generation process into a single function: ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "from transformers import pipeline ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "# Create a pipeline   \ngenerator $=$ pipeline( \"text-generation\", model=model, tokenizer $=$ tokenizer, return_full_text=False, max_new_tokens $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = } \\end{array} .$ , do_sample=False   \n) ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "The following parameters are worth mentioning: ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "return_full_text By setting this to False, the prompt will not be returned but merely the output of the model. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "max_new_tokens ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "The maximum number of tokens the model will generate. By setting a limit, we prevent long and unwieldy output as some models might continue generating output until they reach their context window. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "do_sample ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Whether the model uses a sampling strategy to choose the next token. By setting this to False, the model will always select the next most probable token. In Chapter 6, we explore several sampling parameters that invoke some creativity in the model’s output. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "To generate our first text, let’s instruct the model to tell a joke about chickens. To do so, we format the prompt in a list of dictionaries where each dictionary relates to an entity in the conversation. Our role is that of “user” and we use the “content” key to define our prompt: ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "# The prompt (user input / query)   \nmessages $= [$ {\"role\": \"user\", \"content\": \"Create a funny joke about chickens.\"}   \n] ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "# Generate output output $=$ generator(messages) print(output[0][\"generated_text\"]) ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Why don't chickens like to go to the gym? Because they can't crack the eggsistence of it! ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "And that is it! The first text generated in this book was a decent joke about chickens. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "In this first chapter of the book, we delved into the revolutionary impact LLMs have had on the Language AI field. It has significantly changed our approach to tasks such as translation, classification, summarization, and more. Through a recent history of Language AI, we explored the fundamentals of several types of LLMs, from a simple bag-of-words representation to more complex representations using neural networks. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "We discussed the attention mechanism as a step toward encoding context within models, a vital component of what makes LLMs so capable. We touched on two main categories of models that use this incredible mechanism: representation models (encoder-only) like BERT and generative models (decoder-only) like the GPT family of models. Both categories are considered large language models throughout this book. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Overall, the chapter provided an overview of the landscape of Language AI, including its applications, societal and ethical implications, and the resources needed to run such models. We ended by generating our first text using Phi-3, a model that will be used throughout the book. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "In the next two chapters, you will learn about some underlying processes. We start by exploring tokenization and embeddings in Chapter 2, two often underestimated but vital components of the Language AI field. What follows in Chapter 3 is an in-depth look into language models where you will discover the precise methods used for generating text. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Tokens and Embeddings ",
        "text_level": 1,
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "Tokens and embeddings are two of the central concepts of using large language models (LLMs). As we’ve seen in the first chapter, they’re not only important to understanding the history of Language AI, but we cannot have a clear sense of how LLMs work, how they’re built, and where they will go in the future without a good sense of tokens and embeddings, as we can see in Figure 2-1. ",
        "page_idx": 58
    },
    {
        "type": "image",
        "img_path": "images/ea7aeb828f6dfe1bdd68262c540876e8e46bbbbbcd51e01b0217fed1ec7f6d51.jpg",
        "image_caption": [
            "Figure 2-1. Language models deal with text in small chunks called tokens. For the lan‐ guage model to compute language, it needs to turn tokens into numeric representations called embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "In this chapter, we look more closely at what tokens are and the tokenization meth‐ ods used to power LLMs. We will then dive into the famous word2vec embedding method that preceded modern-day LLMs and see how it’s extending the concept of token embeddings to build commercial recommendation systems that power a lot of the apps you use. Finally, we go from token embeddings into sentence or text embeddings, where a whole sentence or document can have one vector that represents it—enabling applications like semantic search and topic modeling that we see in Part II of this book. ",
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "LLM Tokenization ",
        "text_level": 1,
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "The way the majority of people interact with language models, at the time of this writing, is through a web playground that presents a chat interface between the user and a language model. You may notice that a model does not produce its output response all at once; it actually generates one token at a time. ",
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "But tokens aren’t only the output of a model, they’re also the way in which the model sees its inputs. A text prompt sent to the model is first broken down into tokens, as we’ll now see. ",
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "How Tokenizers Prepare the Inputs to the Language Model ",
        "text_level": 1,
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "Viewed from the outside, generative LLMs take an input prompt and generate a response, as we can see in Figure 2-2. ",
        "page_idx": 59
    },
    {
        "type": "image",
        "img_path": "images/1448a94c81f8a98443ca26958b65a46ea6ebeefc472d2de3635f922b598c23d5.jpg",
        "image_caption": [
            "Figure 2-2. High-level view of a language model and its input prompt. "
        ],
        "image_footnote": [],
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "Before the prompt is presented to the language model, however, it first has to go through a tokenizer that breaks it into pieces. You can find an example showing the tokenizer of GPT-4 on the OpenAI Platform. If we feed it the input text, it shows the output in Figure 2-3, where each token is shown in a different color. ",
        "page_idx": 59
    },
    {
        "type": "image",
        "img_path": "images/7a5939372ff6b4e9c4c5f53cd99caf221c7f7c458e20fb0de666854bdd37a000.jpg",
        "image_caption": [
            "Figure 2-3. A tokenizer breaks down text into words or parts of words before the model processes the text. It does so according to a specific method and training procedure (from https://oreil.ly/ovUWO). "
        ],
        "image_footnote": [],
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "Let’s look at a code example and interact with these tokens ourselves. Here we’ll be downloading an LLM and seeing how to tokenize the input before generating text with the LLM. ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "Downloading and Running an LLM ",
        "text_level": 1,
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "Let’s start by loading our model and its tokenizer as we’ve done in Chapter 1: ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "from transformers import AutoModelForCausalLM, AutoTokenizer ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "# Load model and tokenizer   \nmodel $=$ AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype $=$ \"auto\", trust_remote_code=True,   \n)   \ntokenizer $=$ AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\") ",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "We can then proceed to the actual generation. We first declare our prompt, then tokenize it, then pass those tokens to the model, which generates its output. In this case, we’re asking the model to only generate 20 new tokens: ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "prompt $=$ \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant|>\" ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "# Tokenize the input prompt input_ids $=$ tokenizer(prompt, return_tensors $=$ \"pt\").input_ids.to(\"cuda\") ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "# Generate the text   \ngeneration_output $=$ model.generate( input_ids $\\mathbf { \\Psi } =$ input_ids, max_new_tokens $\\scriptstyle = 2 \\Theta$   \n) ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "# Print the output print(tokenizer.decode(generation_output[0])) ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "Output: ",
        "text_level": 1,
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "<s> Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.<|assistant $| >$ Subject: My Sincere Apologies for the Gardening Mishap   \nDear ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "The text in bold is the 20 tokens generated by the model. ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "Looking at the code, we can see that the model does not in fact receive the text prompt. Instead, the tokenizers processed the input prompt, and returned the infor‐ mation the model needed in the variable input_ids, which the model used as its input. ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "Let’s print input_ids to see what it holds inside: ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "tensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420, 920, 372, 9559, 29889, 32001]], device='cuda:0') ",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "This reveals the inputs that LLMs respond to, a series of integers as shown in Figure 2-4. Each one is the unique ID for a specific token (character, word, or part of a word). These IDs reference a table inside the tokenizer containing all the tokens it knows. ",
        "page_idx": 61
    },
    {
        "type": "image",
        "img_path": "images/cab60c394682bd7e5c9c4bf1b3b33134f4dd3493f7321c930d859671b3b3a15b.jpg",
        "image_caption": [
            "Figure 2-4. A tokenizer processes the input prompt and prepares the actual input into the language model: a list of token IDs. The specific token IDs in the figure are just demonstrative. "
        ],
        "image_footnote": [],
        "page_idx": 62
    },
    {
        "type": "text",
        "text": "If we want to inspect those IDs, we can use the tokenizer’s decode method to translate the IDs back into text that we can read: ",
        "page_idx": 62
    },
    {
        "type": "text",
        "text": "for id in input_ids[0]: print(tokenizer.decode(id)) ",
        "page_idx": 62
    },
    {
        "type": "text",
        "text": "This prints (each token is on a separate line): ",
        "page_idx": 62
    },
    {
        "type": "table",
        "img_path": "images/85724baff7ae3788748d3f14133f50ef33ddc4f521b437ae3dbad9489a6564c6.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>&lt;S&gt;</td></tr><tr><td>Write</td></tr><tr><td>an</td></tr><tr><td>email</td></tr><tr><td>apolog</td></tr><tr><td>izing</td></tr><tr><td>to</td></tr><tr><td> Sarah</td></tr><tr><td>for the</td></tr><tr><td>trag</td></tr><tr><td>ic</td></tr><tr><td> garden</td></tr><tr><td>ing</td></tr><tr><td>m</td></tr><tr><td>ish</td></tr></table>",
        "page_idx": 62
    },
    {
        "type": "table",
        "img_path": "images/b60a52261ac3368095323b9907704d6603aeed7a2a9b5a8f1bf95406918d83bd.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td></tr><tr><td>ap</td></tr><tr><td></td></tr><tr><td>.</td></tr><tr><td>Exp</td></tr><tr><td>lain</td></tr><tr><td>how</td></tr><tr><td>it</td></tr><tr><td>happened</td></tr><tr><td>. &lt;|assistant|&gt;</td></tr></table>",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "This is how the tokenizer broke down our input prompt. Notice the following: ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "• The first token is ID 1 $( < s > )$ , a special token indicating the beginning of the text.   \n• Some tokens are complete words (e.g., Write, an, email).   \n• Some tokens are parts of words (e.g., apolog, izing, trag, ic).   \n• Punctuation characters are their own token. ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "Notice how the space character does not have its own token. Instead, partial tokens (like “izing” and “ic”) have a special hidden character at their beginning that indicates that they’re connected with the token that precedes them in the text. Tokens without that special character are assumed to have a space before them. ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "On the output side, we can also inspect the tokens generated by the model by printing the generation_output variable. This shows the input tokens as well as the output tokens (we’ll highlight the new tokens in bold): ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "tensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278,   \n25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420,   \n920, 372, 9559, 29889, 32001, 3323, 622, 29901, 1619, 317,   \n3742, 406, 6225, 11763, 363, 278, 19906, 292, 341, 728,   \n481, 13, 13, 29928, 799]], device $\\mathbf { = }$ 'cuda:0') ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "This shows us the model generated the token 3323, 'Sub', followed by token 622, 'ject'. Together they formed the word 'Subject'. They were then followed by token 29901, which is the colon ':'...and so on. Just like on the input side, we need the tokenizer on the output side to translate the token ID into the actual text. We do that using the tokenizer’s decode method. We can pass it an individual token ID or a list of them: ",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "print(tokenizer.decode(3323)) print(tokenizer.decode(622)) print(tokenizer.decode([3323, 622])) print(tokenizer.decode(29901)) ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "This outputs: ",
        "page_idx": 64
    },
    {
        "type": "table",
        "img_path": "images/6047ec817496cd8a317500c7618ab974a43b40707c9062846b807eaa58fcabcd.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Sub</td></tr><tr><td>ject</td></tr><tr><td> Subject</td></tr><tr><td>：</td></tr></table>",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "How Does the Tokenizer Break Down Text? ",
        "text_level": 1,
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "There are three major factors that dictate how a tokenizer breaks down an input prompt. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "First, at model design time, the creator of the model chooses a tokenization method. Popular methods include byte pair encoding (BPE) (widely used by GPT models) and WordPiece (used by BERT). These methods are similar in that they aim to optimize an efficient set of tokens to represent a text dataset, but they arrive at it in different ways. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "Second, after choosing the method, we need to make a number of tokenizer design choices like vocabulary size and what special tokens to use. More on this in “Compar‐ ing Trained LLM Tokenizers” on page 46. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "Third, the tokenizer needs to be trained on a specific dataset to establish the best vocabulary it can use to represent that dataset. Even if we set the same methods and parameters, a tokenizer trained on an English text dataset will be different from another trained on a code dataset or a multilingual text dataset. ",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "In addition to being used to process the input text into a language model, tokenizers are used on the output of the language model to turn the resulting token ID into the output word or token associated with it, as Figure 2-5 shows. ",
        "page_idx": 64
    },
    {
        "type": "image",
        "img_path": "images/927c59356ecc0c6986dda851a6f8fd0a0fd063048004bd235fbcc1b8cf882e11.jpg",
        "image_caption": [
            "Figure 2-5. Tokenizers are also used to process the output of the model by converting the output token ID into the word or token associated with that ID. "
        ],
        "image_footnote": [],
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "Word Versus Subword Versus Character Versus Byte Tokens ",
        "text_level": 1,
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "The tokenization scheme we just discussed is called subword tokenization. It’s the most commonly used tokenization scheme but not the only one. The four notable ways to tokenize are shown in Figure 2-6. Let’s go over them: ",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "Word tokens ",
        "text_level": 1,
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "This approach was common with earlier methods like word2vec but is being used less and less in NLP. Its usefulness, however, led it to be used outside of NLP for use cases such as recommendation systems, as we’ll see later in the chapter. ",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "One challenge with word tokenization is that the tokenizer may be unable to deal with new words that enter the dataset after the tokenizer was trained. This also results in a vocabulary that has a lot of tokens with minimal differences between them (e.g., apology, apologize, apologetic, apologist). This latter chal‐ lenge is resolved by subword tokenization as it has a token for apolog, and then suffix tokens (e.g., -y, -ize, -etic, -ist) that are common with many other tokens, resulting in a more expressive vocabulary. ",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "Subword tokens ",
        "text_level": 1,
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "This method contains full and partial words. In addition to the vocabulary expressivity mentioned earlier, another benefit of the approach is its ability to represent new words by breaking down the new token into smaller characters, which tend to be a part of the vocabulary. ",
        "page_idx": 65
    },
    {
        "type": "image",
        "img_path": "images/a7a1b2c125cd9a7a1c17542d713ec78f3421a0fe27b95d9df52a52c1217cea4e.jpg",
        "image_caption": [
            "Figure 2-6. There are multiple methods of tokenization that break down the text to different sizes of components (words, subwords, characters, and bytes). "
        ],
        "image_footnote": [],
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "Character tokens ",
        "text_level": 1,
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "This is another method that can deal successfully with new words because it has the raw letters to fall back on. While that makes the representation easier to tokenize, it makes the modeling more difficult. Where a model with subword tokenization can represent “play” as one token, a model using character-level tokens needs to model the information to spell out “p-l-a-y” in addition to modeling the rest of the sequence. ",
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "Subword tokens present an advantage over character tokens in the ability to fit more text within the limited context length of a Transformer model. So with a model with a context length of 1,024, you may be able to fit about three times as much text using subword tokenization than using character tokens (subword tokens often average three characters per token). ",
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "Byte tokens ",
        "text_level": 1,
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "One additional tokenization method breaks down tokens into the individual bytes that are used to represent unicode characters. Papers like “CANINE: Pretraining an efficient tokenization-free encoder for language representation” out‐ line methods like this, which are also called “tokenization-free encoding.” Other works like “ByT5: Towards a token-free future with pre-trained byte-to-byte models” show that this can be a competitive method, especially in multilingual scenarios. ",
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "One distinction to highlight here: some subword tokenizers also include bytes as tokens in their vocabulary as the final building block to fall back to when they encounter characters they can’t otherwise represent. The GPT-2 and RoBERTa token‐ izers do this, for example. This doesn’t make them tokenization-free byte-level token‐ izers, because they don’t use these bytes to represent everything, only a subset, as we’ll see in the next section. ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "If you want to go deeper into tokenizers, they are discussed in more detail in Design‐ ing Large Language Model Applications. ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "Comparing Trained LLM Tokenizers ",
        "text_level": 1,
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "We’ve pointed out earlier three major factors that dictate the tokens that appear within a tokenizer: the tokenization method, the parameters and special tokens we use to initialize the tokenizer, and the dataset the tokenizer is trained on. Let’s compare and contrast a number of actual, trained tokenizers to see how these choices change their behavior. This comparison will show us that newer tokenizers have changed their behavior to improve model performance, and we’ll also see how spe‐ cialized models (like code generation models, for example) often need specialized tokenizers. ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "We’ll use a number of tokenizers to encode the following text: ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "text $=$ \"\"\"   \nEnglish and CAPITALIZATION   \n??鸟   \nshow_tokens False None elif $= = > =$ else: two tabs:\" \" Three tabs:   \n$1 2 . 0 ^ { \\ast } 5 0 = 6 0 0$ ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "This will allow us to see how each tokenizer deals with a number of different kinds of tokens: ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "• Capitalization.   \n• Languages other than English.   \n• Emojis.   \n• Programming code with keywords and whitespaces often used for indentation (in languages like Python for example).   \n• Numbers and digits.   \n• Special tokens. These are unique tokens that have a role other than representing text. They include tokens that indicate the beginning of the text, or the end of the text (which is the way the model signals to the system that it has completed this generation), or other functions as we’ll see. ",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Let’s go from older to newer tokenizers to see how they tokenize this text and what that might say about the language model. We’ll tokenize the text, and then print each token with a color background color using this function: ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "colors_list $=$ [ '102;194;165', '252;141;98', '141;160;203', '231;138;195', '166;216;84', '255;217;47'   \n] ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "def show_tokens(sentence, tokenizer_name): tokenizer $=$ AutoTokenizer.from_pretrained(tokenizer_name) token_ids $=$ tokenizer(sentence).input_ids for idx, t in enumerate(token_ids): print( f'\\x1b[0;30;48;2;{colors_list[idx % len(colors_list)]}m' + tokenizer.decode(t) + '\\x1b[0m', end=' ) ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "BERT base model (uncased) (2018) ",
        "text_level": 1,
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Link to the model on the HuggingFace model hub ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Tokenization method: WordPiece, introduced in “Japanese and Korean voice search”: ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Vocabulary size: 30,522 ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "Special tokens: ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "unk_token [UNK] An unknown token that the tokenizer has no specific encoding for. ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "sep_token [SEP] ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "A separator that enables certain tasks that require giving the model two texts (in these cases, the model is called a cross-encoder). One example is reranking, as we’ll see in Chapter 8. ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "pad_token [PAD] ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "A padding token used to pad unused positions in the model’s input (as the model expects a certain length of input, its context-size). ",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "cls_token [CLS] A special classification token for classification tasks, as we’ll see in Chapter 4. ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "mask_token [MASK] ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "A masking token used to hide tokens during the training process. ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "Tokenized text: ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "[CLS] english and capital ##ization [UNK] [UNK] show _ token ##s false none eli ##f = = > = else : two tab ##s : \" \" three tab ##s : \" \" 12 . 0 \\* 50 = 600 [SEP] ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "BERT was released in two major flavors: cased (where the capitalization is kept) and uncased (where all capital letters are first turned into small cap letters). With the uncased (and more popular) version of the BERT tokenizer, we notice the following: ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "• The newline breaks are gone, which makes the model blind to information encoded in newlines (e.g., a chat log when each turn is in a new line).   \n• All the text is in lowercase.   \n• The word “capitalization” is encoded as two subtokens: capital ##ization. The ## characters are used to indicate this token is a partial token connected to the token that precedes it. This is also a method to indicate where the spaces are, as it is assumed tokens without ## in front have a space before them.   \n• The emoji and Chinese characters are gone and replaced with the [UNK] special token indicating an “unknown token.” ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "BERT base model (cased) (2018) ",
        "text_level": 1,
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "Link to the model on the HuggingFace model hub ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "Tokenization method: WordPiece ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "Vocabulary size: 28,996 ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "Special tokens: Same as the uncased version ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "Tokenized text: ",
        "page_idx": 69
    },
    {
        "type": "image",
        "img_path": "images/ce8618a64547f1d3755b2a9cb719fafd4aa1f81389d52a5c796bd1be036eafaa.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "The cased version of the BERT tokenizer differs mainly in including uppercase tokens. ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "• Notice how “CAPITALIZATION” is now represented as eight tokens: CA ##PI ##TA ##L ##I ##Z ##AT ##ION. ",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "• Both BERT tokenizers wrap the input within a starting [CLS] token and a closing [SEP] token. [CLS] and [SEP] are utility tokens used to wrap the input text and they serve their own purposes. [CLS] stands for classification as it’s a token used at times for sentence classification. [SEP] stands for separator, as it’s used to separate sentences in some applications that require passing two sentences to a model (For example, in Chapter 8, we will use a [SEP] token to separate the text of the query and a candidate result.) ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "GPT-2 (2019) ",
        "text_level": 1,
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "Link to the model on the HuggingFace model hub ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "Tokenization method: Byte pair encoding (BPE), introduced in “Neural machine translation of rare words with subword units”. ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "Vocabulary size: 50,257 ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "Special tokens: <|endoftext|> ",
        "page_idx": 70
    },
    {
        "type": "image",
        "img_path": "images/ae0e85f9efce8d1a29dcf5a347effc1d2d51148aebc3c1554db1cdcf877a8cb4.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "� � ",
        "text_level": 1,
        "page_idx": 70
    },
    {
        "type": "image",
        "img_path": "images/4b0c87aaf92321b8df01c6c5a81eaea0d03925b0a65981217744e90fe36813dc.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "12 . 0 \\* 50 = 600 ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "With the GPT-2 tokenizer, we notice the following: ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "• The newline breaks are represented in the tokenizer.   \n• Capitalization is preserved, and the word “CAPITALIZATION” is represented in four tokens.   \n• The $\\pmb { \\triangleright }$ 鸟 characters are now represented by multiple tokens each. While we see these tokens printed as the $\\spadesuit$ character, they actually stand for different tokens. For example, the $\\pmb { \\triangleright }$ emoji is broken down into the tokens with token IDs 8582, 236, and 113. The tokenizer is successful in reconstructing the original character from these tokens. We can see that by printing tokenizer.decode([8582, 236, 113]), which prints out $\\pmb { \\triangleright }$ .   \n• The two tabs are represented as two tokens (token number 197 in that vocabu‐ lary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character.   \n• The two tabs are represented as two tokens (token number 197 in that vocabu‐ lary) and the four spaces are represented as three tokens (number 220) with the final space being a part of the token for the closing quote character. ",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "What is the significance of whitespace characters? These are important for models to understand or generate code. A model that uses a single token to represent four consecutive whitespace characters is more tuned to a Python code dataset. While a model can live with representing it as four different tokens, it does make the modeling more difficult as the model needs to keep track of the indentation level, which often leads to worse performance. This is an example of where tokenization choices can help the model improve on a certain task. ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Flan-T5 (2022) ",
        "text_level": 1,
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Tokenization method: Flan-T5 uses a tokenizer implementation called SentencePiece, introduced in “SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing”, which supports BPE and the unigram language model (described in “Subword regularization: Improving neural network translation models with multiple subword candidates”). ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Vocabulary size: 32,100 ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Special tokens: ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "• unk_token <unk> • pad_token <pad> ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Tokenized text: ",
        "page_idx": 71
    },
    {
        "type": "image",
        "img_path": "images/6e6f7317991ab3ea96d71075b33bb0d735eefbaa09fe051975b719ed42d20ed6.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "The Flan-T5 family of models use the SentencePiece method. We notice the following: ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "• No newline or whitespace tokens; this would make it challenging for the model to work with code.   \n• The emoji and Chinese characters are both replaced by the <unk> token, making the model completely blind to them. ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "GPT-4 (2023) ",
        "text_level": 1,
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Tokenization method: BPE ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Vocabulary size: A little over 100,000 ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "Special tokens: ",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "• <|endoftext|> ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "• Fill in the middle tokens. These three tokens enable the LLM to generate a completion given not only the text before it but also considering the text after it. This method is explained in more detail in the paper “Efficient training of language models to fill in the middle”; its exact details are beyond the scope of this book. These special tokens are: — <|fim_prefix|> — <|fim_middle|> — <|fim_suffix|> ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "Tokenized text: ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "English and CAPITAL IZATION ",
        "page_idx": 72
    },
    {
        "type": "image",
        "img_path": "images/f52c45a26de5ca908ba71eb36f221250fd5b51e8b7e2b4e68a5c354a0c60eb30.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "$1 2 \\cdot \\odot ^ { \\star } 5 \\Theta = 6 \\odot \\Theta$ ",
        "text_level": 1,
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "The GPT-4 tokenizer behaves similarly to its ancestor, the GPT-2 tokenizer. Some differences are: ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "• The GPT-4 tokenizer represents the four spaces as a single token. In fact, it has a specific token for every sequence of whitespaces up to a list of 83 whitespaces.   \n• The Python keyword elif has its own token in GPT-4. Both this and the previ‐ ous point stem from the model’s focus on code in addition to natural language.   \n• The GPT-4 tokenizer uses fewer tokens to represent most words. Examples here include “CAPITALIZATION” (two tokens versus four) and “tokens” (one token versus three).   \n• Refer back to what we said about the GPT-2 tokenizer with regards to the Ł tokens. ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "StarCoder2 (2024) ",
        "text_level": 1,
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "StarCoder2 is a 15-billion parameter model focused on generating code described in the paper “StarCoder 2 and the stack v2: The next generation”, which continues the work from the original StarCoder described in “StarCoder: May the source be with you!”. ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "Tokenization method: Byte pair encoding (BPE) ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "Vocabulary size: 49,152 ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "Example special tokens: ",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "• <|endoftext|> • Fill in the middle tokens: ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "— <fim_prefix> — <fim_middle> — <fim_suffix> — <fim_pad> ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "• When representing code, managing the context is important. One file might make a function call to a function that is defined in a different file. So the model needs some way of being able to identify code that is in different files in the same code repository, while making a distinction between code in different repos. That’s why StarCoder2 uses special tokens for the name of the repository and the filename: ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "— <filename> — <reponame> — <gh_stars> ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "Tokenized text: ",
        "page_idx": 73
    },
    {
        "type": "image",
        "img_path": "images/38197bac9c6c23e69869f0550be82dda34aface86b53ed6e0a9937b3d31dea9a.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "� � ",
        "text_level": 1,
        "page_idx": 73
    },
    {
        "type": "image",
        "img_path": "images/ad7f19be6e1b77b94d396d7f894713a6226de45f39f1be103acc87c62f013d48.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "1 2 . 0 \\* 5 0 = 6 0 0 ",
        "text_level": 1,
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "This is an encoder that focuses on code generation: ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "• Similar to GPT-4, it encodes the list of whitespaces as a single token. • A major difference here to everything we’ve seen so far is that each digit is assigned its own token (so 600 becomes $6 \\odot \\Theta$ ). The hypothesis here is that this would lead to better representation of numbers and mathematics. In GPT-2, for example, the number 870 is represented as a single token. But 871 is represented as two tokens (8 and 71). You can intuitively see how that might be confusing to the model and how it represents numbers. ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "Galactica ",
        "text_level": 1,
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "The Galactica model described in “Galactica: A large language model for science” is focused on scientific knowledge and is trained on many scientific papers, reference materials, and knowledge bases. It pays extra attention to tokenization that makes it more sensitive to the nuances of the dataset it’s representing. For example, it includes special tokens for citations, reasoning, mathematics, amino acid sequences, and DNA sequences. ",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Tokenization method: Byte pair encoding (BPE) ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Vocabulary size: 50,000 ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Special tokens: ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "• <s>   \n• <pad>   \n• </s>   \n• <unk>   \n• References: Citations are wrapped within the two special tokens: — [START_REF] — [END_REF] — One example of usage from the paper is: Recurrent neural net works, long short-term memory [START_REF]Long Short-Term Memory, Hochreiter[END_REF]   \n• Step-by-step reasoning: — <work> is an interesting token that the model uses for chain-of-thought rea‐ soning. ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Tokenized text: ",
        "page_idx": 74
    },
    {
        "type": "image",
        "img_path": "images/c01181d3a1ad27307bbed0678341b04467c74335d13f82becc39b969b2b7dc90.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 74
    },
    {
        "type": "image",
        "img_path": "images/e8b24d311fbb798fd2edd9f6eda243a22a295098c20af38ce465bcdba63b6dc1.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "1 2 . 0 \\* 5 0 = 6 0 0 ",
        "text_level": 1,
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "The Galactica tokenizer behaves similar to StarCoder2 in that it has code in mind. It also encodes whitespaces in the same way: assigning a single token to sequences of whitespace of different lengths. It differs in that it also does that for tabs, though. So from all the tokenizers we’ve seen so far, it’s the only one that assigns a single token to the string made up of two tabs $( \" \\backslash \\ t \\backslash \\ t ^ { \\prime } )$ . ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Phi-3 (and Llama 2) ",
        "text_level": 1,
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "The Phi-3 model we look at in this book reuses the tokenizer of Llama 2 yet adds a number of special tokens. ",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "Tokenization method: Byte pair encoding (BPE) ",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "Vocabulary size: 32,000 ",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "Special tokens: ",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "• <|endoftext|> ",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "• Chat tokens: As chat LLMs rose to popularity in 2023, the conversational nature of LLMs started to be a leading use case. Tokenizers have been adapted to this direction by the addition of tokens that indicate the turns in a conversation and the roles of each speaker. These special tokens include: ",
        "page_idx": 75
    },
    {
        "type": "equation",
        "img_path": "images/5ea2c304d369472d10d5db1ec930cb43c562854fb61db97c7732048b7b11a441.jpg",
        "text": "$$\n\\begin{array} { l } { - < | \\mathsf { u s e r } | > } \\\\ { - < | \\mathsf { a s s i s t a n t } | > } \\\\ { - < | \\mathsf { s y s t e m } | > } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "We can now recap our tour by looking at all these examples side by side: ",
        "page_idx": 75
    },
    {
        "type": "table",
        "img_path": "images/434a734dcddd750f5712a1f40997a0b0671b20850a8030dc7a349b970011ed12.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>BERT base model (uncased)</td><td>[CLS]english and capital ##ization [UNk][UNk] showtoken ##s false none eli ##f==&gt;=else:twotab ##s:&quot;&quot;three tab##s:&quot;12.0*50=600[SEP]</td></tr><tr><td>BERT base model (cased)</td><td>[CLS] English and CA ##PI ##TA ##L ##I ##Z ##AT ##ION [UNK][UNK] shoWtoken ##sF##als ##e Noneel##if =&gt;=else :two ta ##bs :&quot; Three ta ##bs:12. *50=600[SEP]</td></tr><tr><td>GPT-2</td><td>Englishand CAP ITAL IZATION showtok ens False Noneelif==&gt;=else:two tabs:&quot;&quot; Three tabs : 12.0*50=600</td></tr><tr><td>FLAN-T5 GPT-4</td><td>English and CA PI TAL IZ ATION &lt;unk&gt;&lt;unk&gt; showto ken SFal s e NoneeLif ==&gt; =else:twotabs:&quot; Three tabs:&quot;&quot;12.0*50=600&lt;/s&gt; EnglishandCAPITALIZATION</td></tr><tr><td></td><td>日日日 show_tokens FalseNoneelif==&gt;=else:twotabs:&quot;&quot;Threetabs:&quot; 12.0*50=600</td></tr><tr><td>StarCoder</td><td>English and CAPITAL IZATION 自0000 showtokens FalseNoneelif==&gt;=else:two tabs :&quot;&quot; Three tabs:&quot; 12.050=600</td></tr></table>",
        "page_idx": 75
    },
    {
        "type": "image",
        "img_path": "images/91a1aa85a754170ae8c4b7c64a80ae0a82e62f2bc7ff1cc306c8607db5799ffa.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "Tokenizer Properties ",
        "text_level": 1,
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "The preceding guided tour of trained tokenizers showed a number of ways in which actual tokenizers differ from each other. But what determines their tokenization behavior? There are three major groups of design choices that determine how the tokenizer will break down text: the tokenization method, the initialization parame‐ ters, and the domain of the data the tokenizer targets. ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "Tokenization methods ",
        "text_level": 1,
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "As we’ve seen, there are a number of tokenization methods with byte pair encoding (BPE) being the more popular one. Each of these methods outlines an algorithm for how to choose an appropriate set of tokens to represent a dataset. You can find a great overview of all these methods on the Hugging Face page that summarizes tokenizers. ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "Tokenizer parameters ",
        "text_level": 1,
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "After choosing a tokenization method, an LLM designer needs to make some deci‐ sions about the parameters of the tokenizer. These include: ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "Vocabulary size ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "How many tokens to keep in the tokenizer’s vocabulary? (30K and 50K are often used as vocabulary size values, but more and more we’re seeing larger sizes like 100K.) ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "Special tokens ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "What special tokens do we want the model to keep track of? We can add as many of these as we want, especially if we want to build an LLM for special use cases. Common choices include: ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "• Beginning of text token (e.g., $\\tt { < S > }$ )   \n• End of text token   \n• Padding token ",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "• Unknown token • CLS token • Masking token ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "Aside from these, the LLM designer can add tokens that help better model the domain of the problem they’re trying to focus on, as we’ve seen with Galactica’s <work> and [START_REF] tokens. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "Capitalization ",
        "text_level": 1,
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "In languages such as English, how do we want to deal with capitalization? Should we convert everything to lowercase? (Name capitalization often carries useful information, but do we want to waste token vocabulary space on all-caps versions of words?) ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "The domain of the data ",
        "text_level": 1,
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "Even if we select the same method and parameters, tokenizer behavior will be differ‐ ent based on the dataset it was trained on (before we even start model training). The tokenization methods mentioned previously work by optimizing the vocabulary to represent a specific dataset. From our guided tour we’ve seen how that has an impact on datasets like code and multilingual text. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "For code, for example, we’ve seen that a text-focused tokenizer may tokenize the indentation spaces like this (we’ll highlight some tokens in color): ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "def add_numbers(a, b): ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "....\"\"\"Add the two numbers \\`a\\` and \\`b\\`.\"\"\" ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "....return a $^ +$ b ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "This may be suboptimal for a code-focused model. Code-focused models are often improved by making different tokenization choices: ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "def add_numbers(a, b): ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "\"\"\"Add the two numbers \\`a\\` and \\`b\\`.\"\"\" ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": ".return a + b ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "These tokenization choices make the model’s job easier and thus its performance has a higher probability of improving. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "You can find a more detailed tutorial on training tokenizers in the Tokenizers section of the Hugging Face course and in Natural Language Processing with Transformers, Revised Edition. ",
        "page_idx": 77
    },
    {
        "type": "text",
        "text": "Token Embeddings ",
        "text_level": 1,
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "Now that we understand tokenization, we have solved one part of the problem of representing language to a language model. In this sense, language is a sequence of tokens. And if we train a good-enough model on a large-enough set of tokens, it starts to capture the complex patterns that appear in its training dataset: ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "• If the training data contains a lot of English text, that pattern reveals itself as a model capable of representing and generating the English language. • If the training data contains factual information (Wikipedia, for example), the model would have the ability to generate some factual information (see the following note). ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "The next piece of the puzzle is finding the best numerical representation for these tokens that the model can use to calculate and properly model the patterns in the text. These patterns reveal themselves to us as a model’s coherence in a specific language, or capability to code, or any of the growing list of capabilities we expect from language models. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "As we’ve seen in Chapter 1, that is what embeddings are. They are the numeric representation space utilized to capture the meanings and patterns in language. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "Oops: Achieving a good threshold of language coherence and better-than-average factual generation, however, starts to present a new problem. Some users start to trust the model’s fact generation ability (e.g., at the beginning of 2023 some language models were being dubbed “Google killers”). It didn’t take long for advanced users to recognize that generation models alone aren’t reliable search engines. This led to the rise of retrieval-augmented genera‐ tion (RAG), which combines search and LLMs. We cover RAG in more detail in Chapter 8. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "A Language Model Holds Embeddings for the Vocabulary of Its Tokenizer ",
        "text_level": 1,
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "After a tokenizer is initialized and trained, it is then used in the training process of its associated language model. This is why a pretrained language model is linked with its tokenizer and can’t use a different tokenizer without training. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "The language model holds an embedding vector for each token in the tokenizer’s vocabulary, as we can see in Figure 2-7. When we download a pretrained language model, a portion of the model is this embeddings matrix holding all of these vectors. ",
        "page_idx": 78
    },
    {
        "type": "text",
        "text": "Before the beginning of the training process, these vectors are randomly initialized like the rest of the model’s weights, but the training process assigns them the values that enable the useful behavior they’re trained to perform. ",
        "page_idx": 79
    },
    {
        "type": "image",
        "img_path": "images/bf8d8dff490607a8a8870775c773622166b6f2c008cf85c052e283259c01d9fd.jpg",
        "image_caption": [
            "Figure 2-7. A language model holds an embedding vector associated with each token in its tokenizer. "
        ],
        "image_footnote": [],
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "Creating Contextualized Word Embeddings with Language Models ",
        "text_level": 1,
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "Now that we’ve covered token embeddings as the input to a language model, let’s look at how language models can create better token embeddings. This is one of the primary ways to use language models for text representation. This empowers applications like named-entity recognition or extractive text summarization (which summarizes a long text by highlighting the most important parts of it, instead of generating new text as a summary). ",
        "page_idx": 79
    },
    {
        "type": "text",
        "text": "Instead of representing each token or word with a static vector, language models create contextualized word embeddings (shown in Figure 2-8) that represent a word with a different token based on its context. These vectors can then be used by other systems for a variety of tasks. In addition to the text applications we mentioned in the previous paragraph, these contextualized vectors, for example, are what powers AI image generation systems like DALL·E, Midjourney, and Stable Diffusion, for example. ",
        "page_idx": 79
    },
    {
        "type": "image",
        "img_path": "images/19e30f05984b3df5cbf5d0cbd394839da6a3040fe4af012f006d16296262d05c.jpg",
        "image_caption": [
            "Figure 2-8. Language models produce contextualized token embeddings that improve on raw, static token embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "Let’s look at how we can generate contextualized word embeddings; the majority of this code should be familiar to you by now: ",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "from transformers import AutoModel, AutoTokenizer   \n# Load a tokenizer   \ntokenizer $=$ AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")   \n# Load a language model   \nmodel $=$ AutoModel.from_pretrained(\"microsoft/deberta-v3-xsmall\")   \n# Tokenize the sentence   \ntokens $=$ tokenizer('Hello world', return_tensors $: =$ 'pt')   \n# Process the tokens   \noutput $=$ model(\\*\\*tokens)[0] ",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "The model we’re using here is called DeBERTa v3, which at the time of writing is one of the best-performing language models for token embeddings while being small and highly efficient. It is described in the paper “DeBERTaV3: Improving DeBERTa using ELECTRA-style pre-training gradient-disentangled embedding sharing”. ",
        "page_idx": 80
    },
    {
        "type": "text",
        "text": "This code downloads a pretrained tokenizer and model, then uses them to process the string “Hello world”. The output of the model is then saved in the output variable. Let’s inspect that variable by first printing its dimensions (we expect it to be a multidimensional array): ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "output.shape ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "This prints out: ",
        "page_idx": 81
    },
    {
        "type": "table",
        "img_path": "images/8b47d5219ad0514722b5c299ed43d2c6677f69aff91616ce6e1813382e98adf3.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>torch.Size([1,4,384])</td></tr></table>",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "Skipping the first dimension, we can read this as four tokens, each one embedded in a vector of 384 values. The first dimension is the batch dimension used in cases (like training) when we want to send multiple input sentences to the model at the same time (they’re processed at the same time, which speeds up the process). ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "But what are these four vectors? Did the tokenizer break the two words into four tokens, or is something else happening here? We can use what we’ve learned about tokenizers to inspect them: ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "for token in tokens['input_ids'][0]: print(tokenizer.decode(token)) ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "This prints out: ",
        "page_idx": 81
    },
    {
        "type": "table",
        "img_path": "images/f7b188247effb4d9adfa0c0805ed05be976459ffed479744002463c9d52d9b1a.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>[CLs]</td><td></td></tr><tr><td>Hello</td><td></td></tr><tr><td>world</td><td></td></tr><tr><td>[SEP]</td><td></td></tr></table>",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "This particular tokenizer and model operate by adding the [CLS] and [SEP] tokens to the beginning and end of a string. ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "Our language model has now processed the text input. The result of its output is the following: ",
        "page_idx": 81
    },
    {
        "type": "table",
        "img_path": "images/b20b76e47d8975269aea2a91dd91875c30999e9dad4f2ea38486cfe15bd87bf4.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\">tensor([[</td></tr><tr><td>[-3.3060，-0.0507，-0.1098，...， -0.1704，-0.1618,0.6932],</td><td></td></tr><tr><td>[0.8918，0.0740，-0.1583，...，0.1869，1.4760,0.0751],</td><td></td></tr><tr><td></td><td>0.0871，0.6364，-0.3050，...，0.4729，-0.1829，1.0157]，</td></tr><tr><td></td><td>[-3.1624，-0.1436，-0.0941， ..., -0.0290，-0.1265，0.7954]</td></tr></table>",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "This is the raw output of a language model. The applications of large language models build on top of outputs like this. ",
        "page_idx": 81
    },
    {
        "type": "text",
        "text": "We recap the input tokenization and resulting outputs of a language model in Fig‐ ure 2-9. Technically, the switch from token IDs into raw embeddings is the first step that occurs inside a language model. ",
        "page_idx": 81
    },
    {
        "type": "image",
        "img_path": "images/8b794c0fb2a149a5c15d06dbcd9be1bcdccded8b7de6e0a78921c7ea11196ab6.jpg",
        "image_caption": [
            "Figure 2-9. A language model operates on raw, static embeddings as its input and produces contextual text embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "A visual like this is essential for the next chapter when we start to look at how Transformer-based LLMs work. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "Text Embeddings (for Sentences and Whole Documents) ",
        "text_level": 1,
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "While token embeddings are key to how LLMs operate, a number of LLM applica‐ tions require operating on entire sentences, paragraphs, or even text documents. This has led to special language models that produce text embeddings—a single vector that represents a piece of text longer than just one token. ",
        "page_idx": 82
    },
    {
        "type": "text",
        "text": "We can think of text embedding models as taking a piece of text and ultimately producing a single vector that represents that text and captures its meaning in some useful form. Figure 2-10 shows that process. ",
        "page_idx": 82
    },
    {
        "type": "image",
        "img_path": "images/8bb0fa202f350b81897ee9b3edb1d43347d5e496d888f59982069cbb2e517c40.jpg",
        "image_caption": [
            "Figure 2-10. In step 1, we use the embedding model to extract the features and convert the input text to embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "There are multiple ways of producing a text embedding vector. One of the most common ways is to average the values of all the token embeddings produced by the model. Yet high-quality text embedding models tend to be trained specifically for text embedding tasks. ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "We can produce text embeddings with sentence-transformers, a popular package for leveraging pretrained embedding models.1 The package, like transformers in the previous chapter, can be used to load publicly available models. To illustrate creating embeddings, we use the all-mpnet-base-v2 model. Note that in Chapter 4, we will further explore how you can choose an embedding model for your task. ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "from sentence_transformers import SentenceTransformer # Load model model $=$ SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\") ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "# Convert text to text embeddings vector $=$ model.encode(\"Best movie ever!\") ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "The number of values, or the dimensions, of the embedding vector depends on the underlying embedding model. Let’s explore that for our model: ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "vector.shape ",
        "page_idx": 83
    },
    {
        "type": "text",
        "text": "This sentence is now encoded in this one vector with a dimension of 768 numerical values. In Part II of this book, once we start looking at applications, we’ll start to see the immense usefulness of these text embeddings vectors in powering everything from categorization to semantic search to RAG. ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "Word Embeddings Beyond LLMs ",
        "text_level": 1,
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "Embeddings are useful even outside of text and language generation. Embeddings, or assigning meaningful vector representations to objects, turns out to be useful in many domains, including recommender engines and robotics. In this section, we’ll look at how to use pretrained word2vec embeddings and touch on how the method creates word embeddings. Seeing how word2vec is trained will prime you to learn about contrastive training in Chapter 10. Then in the following section, we’ll see how those embeddings can be used for recommendation systems. ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "Using pretrained Word Embeddings ",
        "text_level": 1,
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "Let’s look at how we can download pretrained word embeddings (like word2vec or GloVe) using the Gensim library: ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "import gensim.downloader as api ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)   \n# Other options include \"word2vec-google-news-300\"   \n# More options at https://github.com/RaRe-Technologies/gensim-data   \nmodel $=$ api.load(\"glove-wiki-gigaword-50\") ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "Here, we’ve downloaded the embeddings of a large number of words trained on Wikipedia. We can then explore the embedding space by seeing the nearest neighbors of a specific word, “king” for example: ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "model.most_similar([model['king']], topn $\\scriptstyle 1 = 1 1$ ) ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "This outputs: ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "[('king', 1.0000001192092896), ('prince', 0.8236179351806641), ('queen', 0.7839043140411377), ('ii', 0.7746230363845825), ('emperor', 0.7736247777938843), ('son', 0.766719400882721), ('uncle', 0.7627150416374207), ('kingdom', 0.7542161345481873), ('throne', 0.7539914846420288), ('brother', 0.7492411136627197), ('ruler', 0.7434253692626953)] ",
        "page_idx": 84
    },
    {
        "type": "text",
        "text": "The Word2vec Algorithm and Contrastive Training ",
        "text_level": 1,
        "page_idx": 85
    },
    {
        "type": "text",
        "text": "The word2vec algorithm described in the paper “Efficient estimation of word repre‐ sentations in vector space” is described in detail in The Illustrated Word2vec. The central ideas are condensed here as we build on them when discussing one method for creating embeddings for recommendation engines in the following section. ",
        "page_idx": 85
    },
    {
        "type": "text",
        "text": "Just like LLMs, word2vec is trained on examples generated from text. Let’s say, for example, we have the text “Thou shalt not make a machine in the likeness of a human mind” from the Dune novels by Frank Herbert. The algorithm uses a sliding window to generate training examples. We can, for example, have a window size two, meaning that we consider two neighbors on each side of a central word. ",
        "page_idx": 85
    },
    {
        "type": "text",
        "text": "The embeddings are generated from a classification task. This task is used to train a neural network to predict if words commonly appear in the same context or not (context here means in many sentences in the training dataset we’re modeling). We can think of this as a neural network that takes two words and outputs 1 if they tend to appear in the same context, and 0 if they do not. ",
        "page_idx": 85
    },
    {
        "type": "text",
        "text": "In the first position for the sliding window, we can generate four training examples, as we can see in Figure 2-11. ",
        "page_idx": 85
    },
    {
        "type": "image",
        "img_path": "images/8367a80f3698c85e4eb390b56bfd2be7c619e868af6d2d5f77896144fc060591.jpg",
        "image_caption": [
            "Figure 2-11. A sliding window is used to generate training examples for the word2vec algorithm to later predict if two words are neighbors or not. "
        ],
        "image_footnote": [],
        "page_idx": 85
    },
    {
        "type": "text",
        "text": "In each of the produced training examples, the word in the center is used as one input, and each of its neighbors is a distinct second input in each training example. We expect the final trained model to be able to classify this neighbor relationship and output 1 if the two input words it receives are indeed neighbors. These training examples are visualized in Figure 2-12. ",
        "page_idx": 85
    },
    {
        "type": "image",
        "img_path": "images/e2cf1a30129ea7ba3d5814a312c0e3450e235bc615fdaea69df5ac3daac2d3a8.jpg",
        "image_caption": [
            "Figure 2-12. Each generated training example shows a pair of neighboring words. ",
            "Figure 2-13. We need to present our models with negative examples: words that are not usually neighbors. A better model is able to better distinguish between the positive and negative examples. "
        ],
        "image_footnote": [],
        "page_idx": 86
    },
    {
        "type": "text",
        "text": "If, however, we have a dataset of only a target value of 1, then a model can cheat and ace it by outputting 1 all the time. To get around this, we need to enrich our training dataset with examples of words that are not typically neighbors. These are called negative examples and are shown in Figure 2-13. ",
        "page_idx": 86
    },
    {
        "type": "table",
        "img_path": "images/6bf41699f435d483c3f6442af0961e5f8176e8e7d24820cabcf6c5e650369c3d.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Word 1</td><td>Word 2</td><td>Target</td><td rowspan=\"6\">Positive examples</td></tr><tr><td>not</td><td>thou</td><td>1</td></tr><tr><td>not</td><td>shalt</td><td>1</td></tr><tr><td>not</td><td>make</td><td>1</td></tr><tr><td>not</td><td>a</td><td>1</td></tr><tr><td></td><td>apothecary</td><td>0</td></tr><tr><td>thou</td><td>sublime</td><td>0</td></tr><tr><td>not make</td><td>def</td><td>0</td></tr><tr><td>a</td><td>playback</td><td>0</td></tr></table>",
        "page_idx": 86
    },
    {
        "type": "text",
        "text": "It turns out that we don’t have to be too scientific in how we choose the negative examples. A lot of useful models result from the simple ability to detect positive examples from randomly generated examples (inspired by an important idea called noise-contrastive estimation and described in “Noise-contrastive estimation: A new estimation principle for unnormalized statistical models”). So in this case, we get random words and add them to the dataset and indicate that they are not neighbors (and thus the model should output 0 when it sees them). ",
        "page_idx": 86
    },
    {
        "type": "text",
        "text": "With this, we’ve seen two of the main concepts of word2vec (Figure 2-14): skip-gram, the method of selecting neighboring words, and negative sampling, adding negative examples by random sampling from the dataset. ",
        "page_idx": 86
    },
    {
        "type": "image",
        "img_path": "images/95bf00bf86c65b9710d203f02ff0cf09627c35f0de581920d1338a0dc17681f5.jpg",
        "image_caption": [
            "Figure 2-14. Skip-gram and negative sampling are two of the main ideas behind the word2vec algorithm and are useful in many other problems that can be formulated as token sequence problems. "
        ],
        "image_footnote": [],
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "We can generate millions and even billions of training examples like this from running text. Before proceeding to train a neural network on this dataset, we need to make a couple of tokenization decisions, which, just like we’ve seen with LLM tokenizers, include how to deal with capitalization and punctuation and how many tokens we want in our vocabulary. ",
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "We then create an embedding vector for each token, and randomly initialize them, as can be seen in Figure 2-15. In practice, this is a matrix of dimensions vocab_size x embedding_dimensions. ",
        "page_idx": 87
    },
    {
        "type": "image",
        "img_path": "images/05a46f18c7828bc51940761de5de7988d40d8feafececf2c32b01f25e1ebe256.jpg",
        "image_caption": [
            "Figure 2-15. A vocabulary of words and their starting, random, uninitialized embedding vectors. "
        ],
        "image_footnote": [],
        "page_idx": 87
    },
    {
        "type": "text",
        "text": "A model is then trained on each example to take in two embedding vectors and predict if they’re related or not. We can see what this looks like in Figure 2-16. ",
        "page_idx": 87
    },
    {
        "type": "image",
        "img_path": "images/bfd26649c94f256a10262b431f5e0281610cc03618b0989e32b2d0a9e88a11c0.jpg",
        "image_caption": [
            "Figure 2-16. A neural network is trained to predict if two words are neighbors. It updates the embeddings in the training process to produce the final, trained embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "Based on whether its prediction was correct or not, the typical machine learning training step updates the embeddings so that the next time the model is presented with those two vectors, it has a better chance of being more correct. And by the end of the training process, we have better embeddings for all the tokens in our vocabulary. ",
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "This idea of a model that takes two vectors and predicts if they have a certain relation is one of the most powerful ideas in machine learning, and time after time has proven to work very well with language models. This is why we’re dedicating Chapter 10 to this concept and how it optimizes language models for specific tasks (like sentence embeddings and retrieval). ",
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "The same idea is also central to bridging modalities like text and images, which is key to AI image generation models, as we’ll see in Chapter 9 on multimodal models. In that formulation, a model is presented with an image and a caption, and it should predict whether that caption describes the image or not. ",
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "Embeddings for Recommendation Systems ",
        "text_level": 1,
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "As we’ve mentioned, the concept of embeddings is useful in so many other domains.   \nIn industry, it’s widely used for recommendation systems, for example. ",
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "Recommending Songs by Embeddings ",
        "text_level": 1,
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "In this section we’ll use the word2vec algorithm to embed songs using human-made music playlists. Imagine if we treated each song as we would a word or token, and we treated each playlist like a sentence. These embeddings can then be used to recommend similar songs that often appear together in playlists. ",
        "page_idx": 88
    },
    {
        "type": "text",
        "text": "The dataset we’ll use was collected by Shuo Chen from Cornell University. It contains playlists from hundreds of radio stations around the US. Figure 2-17 demonstrates this dataset. ",
        "page_idx": 88
    },
    {
        "type": "image",
        "img_path": "images/a43c14d4c7692996fc743ff255aadaef1a31a36b94342bebc63a568596a9ce57.jpg",
        "image_caption": [
            "Figure 2-17. For song embeddings that capture song similarity we’ll use a dataset made up of a collection of playlists, each containing a list of songs. "
        ],
        "image_footnote": [],
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "Let’s demonstrate the end product before we look at how it’s built. So let’s give it a few songs and see what it recommends in response. ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "Let’s start by giving it Michael Jackson’s “Billie Jean,” the song with ID 3822: ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "# We will define and explore this function in detail below print_recommendations(3822) ",
        "page_idx": 89
    },
    {
        "type": "table",
        "img_path": "images/9b729a69cc28a08b54c04fb67265ed1c86e0b9dfa17ba1fe07c50c61cad9e2dd.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>id Title</td><td>artist</td></tr><tr><td>4181 Kiss</td><td>Prince &amp; The Revolution</td></tr><tr><td>12749 Wanna Be Startin&#x27; Somethin&#x27;</td><td>Michael Jackson</td></tr><tr><td>1506</td><td>The Way You Make Me Feel Michael Jackson</td></tr><tr><td>3396 Holiday</td><td>Madonna</td></tr><tr><td>500</td><td>Don&#x27;t Stop&#x27;Til You Get Enough Michael Jackson</td></tr></table>",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "That looks reasonable. Madonna, Prince, and other Michael Jackson songs are the nearest neighbors. ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "Let’s step away from pop and into rap, and see the neighbors of 2Pac’s “California Love”: ",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "print_recommendations(842) ",
        "page_idx": 89
    },
    {
        "type": "table",
        "img_path": "images/6ab1d7a7312246031d398c0e244b4ff6dc28f776db60aa09d39c889fc5f0f222.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>id Title</td><td>artist</td></tr><tr><td>413 IfIRuledthe World (ImagineThat)(wVLauryHill)</td><td>Nas</td></tr><tr><td>196 I&#x27;ll Be Missing You</td><td> Puff Daddy &amp; The Family</td></tr><tr><td>330 Hate It or Love It (wV 50 Cent)</td><td>The Game</td></tr><tr><td>211 Hypnotize</td><td>The Notorious B.I.G.</td></tr><tr><td>5788 Drop It Like It&#x27;s Hot (wV/ Pharrell)</td><td>Snoop Dogg</td></tr></table>",
        "page_idx": 89
    },
    {
        "type": "text",
        "text": "Another quite reasonable list! Now that we know it works, let’s see how to build such a system. ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "Training a Song Embedding Model ",
        "text_level": 1,
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "We’ll start by loading the dataset containing the song playlists as well as each song’s metadata, such as its title and artist: ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "import pandas as pd from urllib import request ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "# Get the playlist dataset file   \ndata $=$ request.urlopen('https://storage.googleapis.com/maps-premium/data   \nset/yes_complete/train.txt') ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "# Parse the playlist dataset file. Skip the first two lines as # they only contain metadata lines $=$ data.read().decode(\"utf-8\").split('\\n')[2:] ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "# Remove playlists with only one song playlists $=$ [s.rstrip().split() for s in lines if len(s.split()) > 1] ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "# Load song metadata   \nsongs_file $=$ request.urlopen('https://storage.googleapis.com/maps-premium/data   \nset/yes_complete/song_hash.txt')   \nsongs_file $=$ songs_file.read().decode(\"utf-8\").split( $\" \\mathrm { \\Delta } \\mathrm { \\backslash n \" }$ )   \nsongs $=$ [s.rstrip().split('\\t') for s in songs_file]   \nsongs_df $=$ pd.DataFrame(data $=$ songs, columns $=$ ['id', 'title', 'artist'])   \nsongs_df $=$ songs_df.set_index('id') ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "Now that we’ve saved them, let’s inspect the playlists list. Each element inside it is a playlist containing a list of song IDs: ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "print( 'Playlist #1:\\n ', playlists[0], '\\n') print( 'Playlist #2:\\n ', playlists[1]) ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "Playlist #1: Playlist #2: $\\begin{array} { r l } & { [ { ^ { \\dag } \\Theta ^ { \\dag } } , { ^ { \\dag } \\Theta ^ { \\dag } } , { ^ { \\dag } \\Theta ^ { \\dag } } , { ^ { \\dag } 2 ^ { \\dag } } , { ^ { \\dag } 3 ^ { \\dag } } , { ^ { \\dag } 4 ^ { \\dag } } , { ^ { \\dag } 5 ^ { \\dag } } , { ^ { \\dag } \\dots } , { ^ { \\dag } 4 3 ^ { \\dag } } ] } \\\\ & { [ { ^ { \\dag } 7 8 ^ { \\dag } } , { ^ { \\dag } 7 9 ^ { \\dag } } , { ^ { \\dag } 8 \\Theta ^ { \\dag } } , { ^ { \\dag } 3 ^ { \\dag } } , { ^ { \\dag } } , { ^ { \\dag } 6 2 ^ { \\dag } } , { ^ { \\dag } \\dots } , { ^ { \\dag } 2 1 \\Theta ^ { \\dag } } ] } \\end{array}$ ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "Let’s train the model: ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "from gensim.models import Word2Vec   \n# Train our Word2Vec model   \nmodel $=$ Word2Vec( playlists, vector_size $= 3 2$ , windo $\\scriptstyle 1 = 2 \\Theta$ , negative $\\scriptstyle \\mathbf { \\varepsilon = } 5 \\Theta$ , min_count $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ , workers=4   \n) ",
        "page_idx": 90
    },
    {
        "type": "text",
        "text": "That takes a minute or two to train and results in embeddings being calculated for each song that we have. Now we can use those embeddings to find similar songs exactly as we did earlier with words: ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "song_id $=$ 2172 ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "# Ask the model for songs similar to song #2172 model.wv.most_similar(positive $\\mathrel { \\mathop : } =$ str(song_id)) ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "This outputs: ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "[('2976', 0.9977465271949768), ('3167', 0.9977430701255798), ('3094', 0.9975950717926025), ('2640', 0.9966474175453186), ('2849', 0.9963167905807495)] ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "That is the list of the songs whose embeddings are most similar to song 2172. In this case, the song is: ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "print(songs_df.iloc[2172]) ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "title Fade To Black artist Metallica Name: 2172 , dtype: object ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "This results in recommendations that are all in the same heavy metal and hard rock genre: ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "import numpy as np   \ndef print_recommendations(song_id): similar_songs $=$ np.array( model.wv.most_similar(positive $\\iota =$ str(song_id),topn $\\scriptstyle 1 = \\atop \\left. \\begin{array} { r l } \\end{array} \\right.$ ) )[:,0] return songs_df.iloc[similar_songs] ",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "# Extract recommendations print_recommendations(2172) ",
        "page_idx": 91
    },
    {
        "type": "table",
        "img_path": "images/c5b913ce5e19db383c1e919930ba9a11a722e402f022bfb0123301027bd3a6a8.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>id</td><td>Title</td><td>artist</td></tr><tr><td>11473</td><td>Little Guitars</td><td>Van Halen</td></tr><tr><td>3167</td><td> Unchained</td><td>Van Halen</td></tr><tr><td>5586</td><td> The Last in Line</td><td>Dio</td></tr><tr><td>5634</td><td>Mr. Brownstone</td><td> Guns N&#x27; Roses</td></tr><tr><td>3094</td><td> Breaking the Law</td><td> Judas Priest</td></tr></table>",
        "page_idx": 91
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "In this chapter, we have covered LLM tokens, tokenizers, and useful approaches to using token embeddings. This prepares us to start looking closer at language models in the next chapter, and also opens the door to learn about how embeddings are used beyond language models. ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "We explored how tokenizers are the first step in processing input to an LLM, trans‐ forming raw textual input into token IDs. Common tokenization schemes include breaking text down into words, subword tokens, characters, or bytes, depending on the specific requirements of a given application. ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "A tour of real-world pretrained tokenizers (from BERT to GPT-2, GPT-4, and other models) showed us areas where some tokenizers are better (e.g., preserving informa‐ tion like capitalization, newlines, or tokens in other languages) and other areas where tokenizers are just different from each other (e.g., how they break down certain words). ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "Three of the major tokenizer design decisions are the tokenizer algorithm (e.g., BPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size, special tokens, capitalization, treatment of capitalization and different languages), and the dataset the tokenizer is trained on. ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "Language models are also creators of high-quality contextualized token embeddings that improve on raw static embeddings. Those contextualized token embeddings are what’s used for tasks including named-entity recognition (NER), extractive text summarization, and text classification. In addition to producing token embeddings, language models can produce text embeddings that cover entire sentences or even documents. This empowers plenty of applications that will be shown in Part II of this book covering language model applications ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "Before LLMs, word embedding methods like word2vec, GloVe, and fastText were popular. In language processing, this has largely been replaced with contextualized word embeddings produced by language models. The word2vec algorithm relies on two main ideas: skip-gram and negative sampling. It also uses contrastive training similar to the type we’ll see in Chapter 10. ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "Embeddings are useful for creating and improving recommender systems as we discussed in the music recommender we built from curated song playlists. ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "In the next chapter, we will take a deep dive into the process after tokenization: how does an LLM process these tokens and generate text? We will look at some of the main intuitions of how LLMs that use the Transformer architecture work. ",
        "page_idx": 92
    },
    {
        "type": "text",
        "text": "Looking Inside Large Language Models ",
        "text_level": 1,
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "Now that we have a sense of tokenization and embeddings, we’re ready to dive deeper into the language model and see how it works. In this chapter, we’ll look at some of the main intuitions of how Transformer language models work. Our focus will be on text generation models so we get a deeper sense for generative LLMs in particular. ",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "We’ll be looking at both the concepts and some code examples that demonstrate them. Let’s start by loading a language model and getting it ready for generation by declaring a pipeline. In your first read, feel free to skip the code and focus on grasping the concepts involved. Then in a second read, the code will get you to start applying these concepts. ",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "import torch   \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline   \n# Load model and tokenizer   \ntokenizer $=$ AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")   \nmodel $=$ AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype $=$ \"auto\", trust_remote_code=True,   \n)   \n# Create a pipeline   \ngenerator $=$ pipeline( \"text-generation\", model=model, tokenizer $=$ tokenizer, return_full_text $=$ False, max_new_tokens $= 5 \\Theta$ , do_sample=False,   \n) ",
        "page_idx": 94
    },
    {
        "type": "text",
        "text": "An Overview of Transformer Models ",
        "text_level": 1,
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "Let’s begin our exploration with a high-level overview of the model, and then we’ll see how later work has improved upon the Transformer model since its introduction in 2017. ",
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "The Inputs and Outputs of a Trained Transformer LLM ",
        "text_level": 1,
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "The most common picture of understanding the behavior of a Transformer LLM is to think of it as a software system that takes in text and generates text in response. Once a large enough text-in-text-out model is trained on a large enough high-quality dataset, it becomes able to generate impressive and useful outputs. Figure 3-1 shows one such model used to author an email. ",
        "page_idx": 95
    },
    {
        "type": "image",
        "img_path": "images/a219dbef7dda246e0c9e56e71849cd59e84ca1a98e1975a40e42623d3dbe0562.jpg",
        "image_caption": [
            "Figure 3-1. At a high level of abstraction, Transformer LLMs take a text prompt and output generated text. "
        ],
        "image_footnote": [],
        "page_idx": 95
    },
    {
        "type": "text",
        "text": "The model does not generate the text all in one operation; it actually generates one token at a time. Figure 3-2 shows four steps of token generation in response to the input prompt. Each token generation step is one forward pass through the model (that’s machine-learning speak for the inputs going into the neural network and flowing through the computations it needs to produce an output on the other end of the computation graph). ",
        "page_idx": 95
    },
    {
        "type": "image",
        "img_path": "images/2d163b099067903081df6e2c0ec5859c449d3c80abed783e4ea74eeeec53ec9b.jpg",
        "image_caption": [
            "Figure 3-2. Transformer LLMs generate one token at a time, not the entire text at once. "
        ],
        "image_footnote": [],
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "After each token generation, we tweak the input prompt for the next generation step by appending the output token to the end of the input prompt. We can see this in Figure 3-3. ",
        "page_idx": 96
    },
    {
        "type": "image",
        "img_path": "images/e2c555804ce8bb4a1c7f5d7d7d7fd2980bcbcdfbf0f7a8614c2dd8379a8ad6c7.jpg",
        "image_caption": [
            "Figure 3-3. An output token is appended to the prompt, then this new text is presented to the model again for another forward pass to generate the next token. "
        ],
        "image_footnote": [],
        "page_idx": 96
    },
    {
        "type": "text",
        "text": "This gives us a more accurate picture of the model as it is simply predicting the next token based on an input prompt. Software around the neural network basically runs it in a loop to sequentially expand the generated text until completion. ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "There’s a specific word used in machine learning to describe models that consume their earlier predictions to make later predictions (e.g., the model’s first generated token is used to generate the second token). They’re called autoregressive models. That is why you’ll hear text generation LLMs being called autoregressive models. This is often used to differentiate text generation models from text representation models like BERT, which are not autoregressive. ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "This autoregressive, token-by-token generation is what happens under the hood when we generate text with the LLM like we see here: ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "prompt $=$ \"Write an email apologizing to Sarah for the tragic gardening mishap. Explain how it happened.\" ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "output $=$ generator(prompt) ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "print(output[0]['generated_text']) ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "This generates the text: ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "Solution 1:   \nSubject: My Sincere Apologies for the Gardening Mishap   \nDear Sarah,   \nI hope this message finds you well. I am writing to express my deep ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "We can see the model begin to write the email starting with the subject. It stopped abruptly because it reached the token limit we established by setting max_new_tokens to 50 tokens. If we increase that, it will continue until concluding the email. ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "The Components of the Forward Pass ",
        "text_level": 1,
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "In addition to the loop, two key internal components are the tokenizer and the language modeling head (LM head). Figure 3-4 shows where these components lie in the system. We saw in the previous chapter how tokenizers break down the text into a sequence of token IDs that then become the input to the model. ",
        "page_idx": 97
    },
    {
        "type": "text",
        "text": "The tokenizer is followed by the neural network: a stack of Transformer blocks that do all of the processing. That stack is then followed by the LM head, which translates the output of the stack into probability scores for what the most likely next token is. ",
        "page_idx": 97
    },
    {
        "type": "image",
        "img_path": "images/4a57dedec0a651884f6232ba1e56e2cb5e277e3e7d6fbee7415a4782492c2c2a.jpg",
        "image_caption": [
            "Figure 3-4. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks, and a language modeling head. "
        ],
        "image_footnote": [],
        "page_idx": 98
    },
    {
        "type": "text",
        "text": "Recall from Chapter 2 that the tokenizer contains a table of tokens—the tokenizer’s vocabulary. The model has a vector representation associated with each of these tokens in the vocabulary (token embeddings). Figure 3-5 shows both the vocabulary and associated token embeddings for a model with a vocabulary of 50,000 tokens. ",
        "page_idx": 98
    },
    {
        "type": "image",
        "img_path": "images/30fbc386165244294f83d86d9873153b21f7fa809c4d0f569741bd16a17ef8ca.jpg",
        "image_caption": [
            "Figure 3-5. The tokenizer has a vocabulary of 50,000 tokens. The model has token embeddings associated with those embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 98
    },
    {
        "type": "text",
        "text": "The flow of the computation follows the direction of the arrow from top to bottom. For each generated token, the process flows once through each of the Transformer blocks in the stack in order, then to the LM head, which finally outputs the probabil‐ ity distribution for the next token, seen in Figure 3-6. ",
        "page_idx": 98
    },
    {
        "type": "image",
        "img_path": "images/00dced44467d08bcbb4da1a0e2fd77cd3556da55d0863510c33be8031eea6eb4.jpg",
        "image_caption": [
            "Figure 3-6. At the end of the forward pass, the model predicts a probability score for each token in the vocabulary. "
        ],
        "image_footnote": [],
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "The LM head is a simple neural network layer itself. It is one of multiple possible “heads” to attach to a stack of Transformer blocks to build different kinds of systems. Other kinds of Transformer heads include sequence classification heads and token classification heads. ",
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "We can display the order of the layers by simply printing out the model variable. For this model, we have: ",
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "Phi3ForCausalLM( (model): Phi3Model( (embed_tokens): Embedding(32064, 3072, padding_id ${ \\it \\Omega } = 3 2 0 0 0$ ) (embed_dropout): Dropout $\\cdot P = 0 . 0$ , inplace $\\iota =$ False) (layers): ModuleList( (0-31): $3 2 \\ \\times$ Phi3DecoderLayer( (self_attn): Phi3Attention( (o_proj): Linear(in_features $\\ B { = } 3 \\Theta 7 2$ , out_features $\\mathtt { \\_ 3 0 7 2 }$ , bias=False) (qkv_proj): Linear(in_features $\\mathtt { \\_ 3 0 7 2 }$ , out_features ${ \\tt = } 9 2 1 6$ , bias $=$ False) (rotary_emb): Phi3RotaryEmbedding() ) (mlp): Phi3MLP( (gate_up_proj): Linear(in_feature $\\mathord {  } 3 \\Theta 7 2$ , out_feature $\\scriptstyle \\sum 1 6 3 8 4$ ,   \nbias=False) (down_proj): Linear(in_feature ${ \\tt s } = 8 1 9 2$ , out_feature ${ \\tt s } = 3 0 7 2$ , bias $\\mathbf { \\Psi } _ { 1 } =$ False) (activation_fn): SiLU() ) (input_layernorm): Phi3RMSNorm() (resid_attn_dropout): Dropout( $\\mathsf { p } { = } \\Theta . \\Theta$ , inplace=False) (resid_mlp_dropout): Dropout $\\mathsf { p } { = } \\Theta . \\Theta$ , inplace=False) (post_attention_layernorm): Phi3RMSNorm() ) ) (norm): Phi3RMSNorm() (lm_head): Linear(in_feature ${ \\tt S } = 3 \\Theta 7 2$ , out_feature $s { = } 3 2 \\Theta 6 4$ , bias=False)   \n) ",
        "page_idx": 99
    },
    {
        "type": "text",
        "text": "Looking at this structure, we can notice the following highlights: ",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "• This shows us the various nested layers of the model. The majority of the model is labeled model, followed by lm_head.   \n• Inside the Phi3Model model, we see the embeddings matrix embed_tokens and its dimensions. It has 32,064 tokens each with a vector size of 3,072.   \n• Skipping the dropout layer for now, we can see the next major component is the stack of Transformer decoder layers. It contains 32 blocks of type Phi3Deco derLayer.   \n• Each of these Transformer blocks includes an attention layer and a feedforward neural network (also known as an mlp or multilevel perceptron). We’ll cover these in more detail later in the chapter.   \n• Finally, we see the lm_head taking a vector of size 3,072 and outputting a vector equivalent to the number of tokens the model knows. That output is the proba‐ bility score for each token that helps us select the output token. ",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "Choosing a Single Token from the Probability Distribution (Sampling/ Decoding) ",
        "text_level": 1,
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "At the end of processing, the output of the model is a probability score for each token in the vocabulary, as we saw previously in Figure 3-6. The method of choosing a sin‐ gle token from the probability distribution is called the decoding strategy. Figure 3-7 shows how this leads to picking the token “Dear” in one example. ",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "The easiest decoding strategy would be to always pick the token with the highest probability score. In practice, this doesn’t tend to lead to the best outputs for most use cases. A better approach is to add some randomness and sometimes choose the second or third highest probability token. The idea here is to basically sample from the probability distribution based on the probability score, as the statisticians would say. ",
        "page_idx": 100
    },
    {
        "type": "text",
        "text": "What this means for the example in Figure 3-7 is that if the token “Dear” has a $4 0 \\%$ probability of being the next token, then it has a $4 0 \\%$ chance of being picked (instead of greedy search, which would pick it directly for having the highest score). So with this method, all the other tokens have a chance of being picked according to their score. ",
        "page_idx": 100
    },
    {
        "type": "image",
        "img_path": "images/39be2d51e721c4cb0c8227a78e14cfa6362cbe6e4c9b07207c52d3bdc5914210.jpg",
        "image_caption": [
            "Figure 3-7. The tokens with the highest probability after the model’s forward pass. Our decoding strategy decides which of the tokens to output by sampling based on the probabilities. "
        ],
        "image_footnote": [],
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "Choosing the highest scoring token every time is called greedy decoding. It’s what happens if you set the temperature parameter to zero in an LLM. We cover the concept of temperature in Chapter 6. ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "Let’s look more closely at the code that demonstrates this process. In this code block, we pass the input tokens through the model, and then lm_head: ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "prompt $=$ \"The capital of France is\"   \n# Tokenize the input prompt   \ninput_ids $=$ tokenizer(prompt, return_tensors $=$ \"pt\").input_ids   \n# Tokenize the input prompt   \ninput_ids $=$ input_ids.to(\"cuda\")   \n# Get the output of the model before the lm_head   \nmodel_output $=$ model.model(input_ids)   \n# Get the output of the lm_head   \nlm_head_output $=$ model.lm_head(model_output[0]) ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "Now, lm_head_output is of the shape [1, 6, 32064]. We can access the token proba‐ bility scores for the last generated token using lm_head_output[0,-1], which uses the index 0 across the batch dimension; the index $^ { - 1 }$ gets us the last token in the sequence. This is now a list of probability scores for all 32,064 tokens. We can get the top scoring token ID, and then decode it to arrive at the text of the generated output token: ",
        "page_idx": 101
    },
    {
        "type": "text",
        "text": "In this case this turns out to be: ",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "Parallel Token Processing and Context Size ",
        "text_level": 1,
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "One of the most compelling features of Transformers is that they lend themselves better to parallel computing than previous neural network architectures in language processing. In text generation, we get a first glance at this when looking at how each token is processed. We know from the previous chapter that the tokenizer will break down the text into tokens. Each of these input tokens then flows through its own computation path (that’s a good first intuition, at least). We can see these individual processing tracks or streams in Figure 3-8. ",
        "page_idx": 102
    },
    {
        "type": "image",
        "img_path": "images/ceed25cd93f15e6cc6f8c4f00f6ad787fbe68738bf5c72128a42d3a36dec88ae.jpg",
        "image_caption": [
            "Figure 3-8. Each token is processed through its own stream of computation (with some interaction between them in attention steps, as we’ll later see). "
        ],
        "image_footnote": [],
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "Current Transformer models have a limit for how many tokens they can process at once. That limit is called the model’s context length. A model with 4K context length can only process 4K tokens and would only have 4K of these streams. ",
        "page_idx": 102
    },
    {
        "type": "text",
        "text": "Each of the token streams starts with an input vector (the embedding vector and some positional information; we’ll discuss positional embeddings later in the chap‐ ter). At the end of the stream, another vector emerges as the result of the model’s processing, as shown in Figure 3-9. ",
        "page_idx": 103
    },
    {
        "type": "image",
        "img_path": "images/83880c51e73af413158219042f6b9ab259b5f4a3d5b78e5c7280ce9c97d72dc2.jpg",
        "image_caption": [
            "Figure 3-9. Each processing stream takes a vector as input and produces a final resulting vector of the same size (often referred to as the model dimension). "
        ],
        "image_footnote": [],
        "page_idx": 103
    },
    {
        "type": "text",
        "text": "For text generation, only the output result of the last stream is used to predict the next token. That output vector is the only input into the LM head as it calculates the probabilities of the next token. ",
        "page_idx": 103
    },
    {
        "type": "text",
        "text": "You may wonder why we go through the trouble of calculating all the token streams if we’re discarding the outputs of all but the last token. The answer is that the calculations of the previous streams are required and used in calculating the final stream. Yes, we’re not using their final output vector, but we use earlier outputs (in each Transformer block) in the Transformer block’s attention mechanism. ",
        "page_idx": 103
    },
    {
        "type": "text",
        "text": "If you’re following along with the code examples, recall that the output of lm_head was of the shape [1, 6, 32064]. That was because the input to it was of the shape [1, 6, 3072], which is a batch of one input string, containing six tokens, each of them represented by a vector of size 3,072 corresponding to the output vectors after the stack of Transformer blocks. ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "We can access these matrices and view their dimensions by printing: ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "model_output[0].shape ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "This outputs: ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "torch.Size([1, 6, 3072])",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "Similarly, we can print the output of the LM head: ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "lm_head_output.shape ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "This outputs: ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "Speeding Up Generation by Caching Keys and Values ",
        "text_level": 1,
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "Recall that when generating the second token, we simply append the output token to the input and do another forward pass through the model. If we give the model the ability to cache the results of the previous calculation (especially some of the specific vectors in the attention mechanism), we no longer need to repeat the calculations of the previous streams. This time the only needed calculation is for the last stream. This is an optimization technique called the keys and values (kv) cache and it provides a significant speedup of the generation process. Keys and values are some of the central components of the attention mechanism, as we’ll see later in this chapter. ",
        "page_idx": 104
    },
    {
        "type": "text",
        "text": "Figure 3-10 shows how when generating the second token, only one processing stream is active as we cache the results of the previous streams. ",
        "page_idx": 104
    },
    {
        "type": "image",
        "img_path": "images/89afe8e619172242dc9fe03b2e0e914315f11ccab30b67143bc0ec529b306bc3.jpg",
        "image_caption": [
            "Figure 3-10. When generating text, it’s important to cache the computation results of previous tokens instead of repeating the same calculation over and over again. "
        ],
        "image_footnote": [],
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "In Hugging Face Transformers, cache is enabled by default. We can disable it by setting use_cache to False. We can see the difference in speed by asking for a long generation, and timing the generation with and without caching: ",
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "prompt $=$ \"Write a very long email apologizing to Sarah for the tragic gardening   \nmishap. Explain how it happened.\"   \n# Tokenize the input prompt   \ninput_ids $=$ tokenizer(prompt, return_tensors $=$ \"pt\").input_ids   \ninput_ids $=$ input_ids.to(\"cuda\") ",
        "page_idx": 105
    },
    {
        "type": "text",
        "text": "Then we time how long it takes to generate 100 tokens with caching. We can use the %%timeit magic command in Jupyter or Colab to time how long the execution takes (it runs the command several times and gets the average): ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "%%timeit -n 1   \n# Generate the text   \ngeneration_output $=$ model.generate( input_ids $\\mathbf { \\Psi } =$ input_ids, max_new_tokens $\\begin{array} { r l } { \\mathrm { ~  ~ \\tau ~ } } & { { } = } \\\\ { \\mathrm { ~  ~ \\tau ~ } } & { { } = } \\end{array}$ , use_cache=True   \n) ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "On a Colab with a T4 GPU, this comes to 4.5 seconds. How long would that take if we disable the cache, however? ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "%%timeit -n 1   \n# Generate the text   \ngeneration_output $=$ model.generate( input_ids $\\cdot = \\cdot$ input_ids, max_new_tokens $\\begin{array} { r l } { \\mathrm { ~  ~ \\tau ~ } } & { { } = } \\\\ { \\mathrm { ~  ~ \\tau ~ } } & { { } = } \\end{array}$ , use_cache=False   \n) ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "This comes out to 21.8 seconds. A dramatic difference. In fact, from a user experience standpoint, even the four-second generation time tends to be a long time to wait for a user that’s staring at a screen and waiting for an output from the model. This is one reason why LLM APIs stream the output tokens as the model generates them instead of waiting for the entire generation to be completed. ",
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "Inside the Transformer Block ",
        "text_level": 1,
        "page_idx": 106
    },
    {
        "type": "text",
        "text": "We can now talk about where the vast majority of processing happens: the Trans‐ former blocks. As Figure 3-11 shows, Transformer LLMs are composed of a series Transformer blocks (often in the range of six in the original Transformer paper, to over a hundred in many large LLMs). Each block processes its inputs, then passes the results of its processing to the next block. ",
        "page_idx": 106
    },
    {
        "type": "image",
        "img_path": "images/fea40327e414a5b1a63ae445e4a7e2571e4dcb1358713e366cb36c0bd29c9c96.jpg",
        "image_caption": [
            "Figure 3-11. The bulk of the Transformer LLM processing happens inside a series of Transformer blocks, each handing the result of its processing as input to the subsequent block. "
        ],
        "image_footnote": [],
        "page_idx": 107
    },
    {
        "type": "text",
        "text": "A Transformer block (Figure 3-12) is made up of two successive components: ",
        "page_idx": 107
    },
    {
        "type": "text",
        "text": "1. The attention layer is mainly concerned with incorporating relevant information from other input tokens and positions 2. The feedforward layer houses the majority of the model’s processing capacity ",
        "page_idx": 107
    },
    {
        "type": "image",
        "img_path": "images/700350d7b2dba288d0012aa998158165b5a0d298ddd2182736749183f4b93427.jpg",
        "image_caption": [
            "Figure 3-12. A Transformer block is made up of a self-attention layer and a feedforward neural network. "
        ],
        "image_footnote": [],
        "page_idx": 107
    },
    {
        "type": "text",
        "text": "The feedforward neural network at a glance ",
        "text_level": 1,
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "A simple example giving the intuition of the feedforward neural network would be if we pass the simple input “The Shawshank” to a language model, with the expectation that it will generate “Redemption” as the most probable next word (in reference to the film from 1994). ",
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "The feedforward neural network (collectively in all the model layers) is the source of this information, as Figure 3-13 shows. When the model was successfully trained to model a massive text archive (which included many mentions of “The Shawshank Redemption”), it learned and stored the information (and behaviors) that make it succeed at this task. ",
        "page_idx": 108
    },
    {
        "type": "image",
        "img_path": "images/1b58ad45b968389ea057eec5206259cc78c3ae63f7be473feae3d3e944d9b8e0.jpg",
        "image_caption": [
            "Figure 3-13. The feedforward neural network component of a Transformer block likely does the majority of the model’s memorization and interpolation. "
        ],
        "image_footnote": [],
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "For an LLM to be successfully trained, it needs to memorize a lot of information. But it is not simply a large database. Memorization is only one ingredient in the recipe of impressive text generation. The model is able to use this same machinery to interpolate between data points and more complex patterns to be able to generalize— which means doing well on inputs it hadn’t seen in the past and were not in its training dataset. ",
        "page_idx": 108
    },
    {
        "type": "text",
        "text": "When you use a modern commercial LLM, the outputs you get are not the ones mentioned earlier in the strict meaning of a “lan‐ guage model.” Passing “The Shawshank” to a chat LLM like GPT-4 produces an output: ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "\"The Shawshank Redemption\" is a 1994 film directed by Frank Darabont and is based on the novella \"Rita Hayworth and Shawshank Redemption\" written by Stephen King. ...etc. ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "This is because raw language models (like GPT-3) are difficult for people to properly utilize. This is why the language model is then trained on instruction-tuning and human preference and feedback fine-tuning to match people’s expectations of what the model should output. ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "The attention layer at a glance ",
        "text_level": 1,
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "Context is vital in order to properly model language. Simple memorization and interpolation based on the previous token can only take us so far. We know that because this was one of the leading approaches to build language models before neural networks (see Chapter 3, “N-gram Language Models” of Speech and Language Processing by Daniel Jurafsky and James H. Martin). ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "Attention is a mechanism that helps the model incorporate context as it’s processing a specific token. Think of the following prompt: ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "“The dog chased the squirrel because it” ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "For the model to predict what comes after “it,” it needs to know what “it” refers to. Does it refer to the dog or the squirrel? ",
        "page_idx": 109
    },
    {
        "type": "text",
        "text": "In a trained Transformer LLM, the attention mechanism makes that determination.   \nAttention adds information from the context into the representation of the “it” token.   \nWe can see a simple version of that in Figure 3-14. ",
        "page_idx": 109
    },
    {
        "type": "image",
        "img_path": "images/59e2d0f7b480a5f91b1a7d285d50024302bbb2d9992830d6a5011af440718c2b.jpg",
        "image_caption": [
            "Figure 3-14. The self-attention layer incorporates relevant information from previous positions that help process the current token. "
        ],
        "image_footnote": [],
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "The model does that based on the patterns seen and learned from the training dataset. Perhaps previous sentences also give more clues, like, for example, referring to the dog as “she” thus making it clear that “it” refers to the squirrel. ",
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "Attention is all you need ",
        "text_level": 1,
        "page_idx": 110
    },
    {
        "type": "text",
        "text": "It is worth diving deeper into the attention mechanism. The most stripped-down version of the mechanism is shown in Figure 3-15. It shows multiple token positions going into the attention layer; the final one is the one being currently processed (the pink arrow). The attention mechanism operates on the input vector at that position. It incorporates relevant information from the context into the vector it produces as the output for that position. ",
        "page_idx": 110
    },
    {
        "type": "image",
        "img_path": "images/70f0c775463a46c3cdd599293c95a6ff88a2e9c35503060a10e074dc618f7368.jpg",
        "image_caption": [
            "Figure 3-15. A simplified framing of attention: an input sequence and a current position being processed. As we’re mainly concerned with this position, the figure shows an input vector and an output vector that incorporates information from the previous elements in the sequence according to the attention mechanism. "
        ],
        "image_footnote": [],
        "page_idx": 111
    },
    {
        "type": "text",
        "text": "Two main steps are involved in the attention mechanism: ",
        "page_idx": 111
    },
    {
        "type": "text",
        "text": "1. A way to score how relevant each of the previous input tokens are to the current token being processed (in the pink arrow).   \n2. Using those scores, we combine the information from the various positions into a single output vector. ",
        "page_idx": 111
    },
    {
        "type": "image",
        "img_path": "images/b449d378851b5121f78d3c19b007f18e1ba0ad3fe4547e9e62a0121a4d4903e1.jpg",
        "image_caption": [
            "Figure 3-16 shows these two steps. ",
            "Figure 3-16. Attention is made up of two major steps: relevance scoring for each posi‐ tion, then a step where we combine the information based on those scores. "
        ],
        "image_footnote": [],
        "page_idx": 111
    },
    {
        "type": "text",
        "text": "To give the Transformer more extensive attention capability, the attention mecha‐ nism is duplicated and executed multiple times in parallel. Each of these parallel applications of attention is conducted into an attention head. This increases the model’s capacity to model complex patterns in the input sequence that require paying attention to different patterns at once. ",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "Figure 3-17 shows the intuition of how attention heads run in parallel with a preced‐ ing step of splitting information and a later step of combining the results of all the heads. ",
        "page_idx": 112
    },
    {
        "type": "image",
        "img_path": "images/4551b14641d1997930f5bdf18c4c470dfef8aefd525ff85de8219733b4dae5a3.jpg",
        "image_caption": [
            "Figure 3-17. We get better LLMs by doing attention multiple times in parallel, increasing the model’s capacity to attend to different types of information. "
        ],
        "image_footnote": [],
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "How attention is calculated ",
        "text_level": 1,
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "Let’s look at how attention is calculated inside a single attention head. Before we start the calculation, let’s observe the following as the starting position: ",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "• The attention layer (of a generative LLM) is processing attention for a single position.   \n• The inputs to the layer are: — The vector representation of the current position or token — The vector representations of the previous tokens   \n• The goal is to produce a new representation of the current position that incorpo‐ rates relevant information from the previous tokens: — For example, if we’re processing the last position in the sentence “Sarah fed the cat because it,” we want “it” to represent the cat—so attention bakes in “cat information” from the cat token. ",
        "page_idx": 112
    },
    {
        "type": "text",
        "text": "• The training process produces three projection matrices that produce the com‐ ponents that interact in this calculation: ",
        "page_idx": 113
    },
    {
        "type": "text",
        "text": "— A query projection matrix — A key projection matrix — A value projection matrix ",
        "page_idx": 113
    },
    {
        "type": "image",
        "img_path": "images/f5a36e31bd25981bcfc64232a1646a93ec4f979198acf86d62f06ee7055589c9.jpg",
        "image_caption": [
            "Figure 3-18 shows the starting position for all of these components before the atten‐ tion calculations start. For simplicity, let’s look at only one attention head because the other heads have identical calculations but with their individual projection matrices. ",
            "Figure 3-18. Before starting the self-attention calculation, we have the inputs to the layer and projection matrices for queries, keys, and values. "
        ],
        "image_footnote": [],
        "page_idx": 113
    },
    {
        "type": "text",
        "text": "Attention starts by multiplying the inputs by the projection matrices to create three new matrices. These are called the queries, keys, and values matrices. These matrices contain the information of the input tokens projected to three different spaces that help carry out the two steps of attention: ",
        "page_idx": 113
    },
    {
        "type": "text",
        "text": "1. Relevance scoring   \n2. Combining information ",
        "page_idx": 113
    },
    {
        "type": "image",
        "img_path": "images/44aa82f0ccfb3769585e478ed200de68c70da90ea8baff999a1496a87e4831bc.jpg",
        "image_caption": [
            "Figure 3-19 shows these three new matrices, and how the bottom row of all three matrices is associated with the current position while the rows above it are associated with the previous positions. ",
            "Figure 3-19. Attention is carried out by the interaction of the queries, keys, and val‐ ues matrices. Those are produced by multiplying the layer’s inputs with the projection matrices. "
        ],
        "image_footnote": [],
        "page_idx": 114
    },
    {
        "type": "text",
        "text": "Self-attention: Relevance scoring ",
        "text_level": 1,
        "page_idx": 114
    },
    {
        "type": "text",
        "text": "In a generative Transformer, we’re generating one token at a time. This means we’re processing one position at a time. So the attention mechanism here is only concerned with this one position, and how information from other positions can be pulled in to inform this position. ",
        "page_idx": 114
    },
    {
        "type": "text",
        "text": "The relevance scoring step of attention is conducted by multiplying the query vector of the current position with the keys matrix. This produces a score stating how relevant each previous token is. Passing that by a softmax operation normalizes these scores so they sum up to 1. Figure 3-20 shows the relevance score resulting from this calculation. ",
        "page_idx": 115
    },
    {
        "type": "image",
        "img_path": "images/293a6aac8341cc87de98135e66bfbc74e09d88d54fc2d45ce27b15745a1fe43c.jpg",
        "image_caption": [
            "Figure 3-20. Scoring the relevance of previous tokens is accomplished by multiplying the query associated with the current position with the keys matrix. "
        ],
        "image_footnote": [],
        "page_idx": 115
    },
    {
        "type": "text",
        "text": "Self-attention: Combining information ",
        "text_level": 1,
        "page_idx": 115
    },
    {
        "type": "text",
        "text": "Now that we have the relevance scores, we multiply the value vector associated with each token by that token’s score. Summing up those resulting vectors produces the output of this attention step, as we see in Figure 3-21. ",
        "page_idx": 115
    },
    {
        "type": "image",
        "img_path": "images/2de435cc46defbf70190806dd33841a35a9c73d10fe89ba63b9ba47077951154.jpg",
        "image_caption": [
            "Figure 3-21. Attention combines the relevant information of previous positions by multi‐ plying their relevance scores by their respective value vectors. "
        ],
        "image_footnote": [],
        "page_idx": 116
    },
    {
        "type": "text",
        "text": "Recent Improvements to the Transformer Architecture ",
        "text_level": 1,
        "page_idx": 116
    },
    {
        "type": "text",
        "text": "Since the release of the Transformer architecture, much work has been done to improve it and create better models. This spans training on larger datasets and opti‐ mizations for the training process and learning rates to use, but it also extends to the architecture itself. At the time of writing, a lot of the ideas of the original Transformer stand unchanged. There are a few architectural ideas that have proved to be valuable. They contribute to the performance of more recent Transformer models like Llama 2. In this final section of the chapter, we go over a number of the important recent developments of the Transformer architecture. ",
        "page_idx": 116
    },
    {
        "type": "text",
        "text": "More Efficient Attention ",
        "text_level": 1,
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "The area that gets the most focus from the research community is the attention layer of the Transformer. This is because the attention calculation is the most computation‐ ally expensive part of the process. ",
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "Local/sparse attention ",
        "text_level": 1,
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "As Transformers started getting larger, ideas like sparse attention (“Generating long sequences with sparse transformers”) and sliding window attention (“Longformer: The long-document transformer”) provided improvements for the efficiency of the attention calculation. Sparse attention limits the context of previous tokens that the model can attend to, as we can see in Figure 3-22. ",
        "page_idx": 117
    },
    {
        "type": "image",
        "img_path": "images/2ad8d0eb903c340914ab315fa960646d034e2ba12f38bfb42277fc8e0601690c.jpg",
        "image_caption": [
            "Figure 3-22. Local attention boosts performance by only paying attention to a small number of previous positions. "
        ],
        "image_footnote": [],
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "One model that incorporates such a mechanism is GPT-3. But it does not use that for all the Transformer blocks—the quality of the generation would vastly degrade if the model could only see a small number of previous tokens. The GPT-3 architec‐ ture interweaved full-attention and efficient-attention Transformer blocks. So the Transformer blocks alternate between full attention (e.g., blocks 1 and 3) and sparse attention (e.g., blocks 2 and 4). ",
        "page_idx": 117
    },
    {
        "type": "text",
        "text": "To demonstrate different kinds of attention, review Figure 3-23, which shows how different attention mechanisms work. Each figure shows which previous tokens (light blue) can be attended to when processing the current token (in dark blue). ",
        "page_idx": 117
    },
    {
        "type": "image",
        "img_path": "images/cc688c0bfd489ffca33e6e87c748f251ab10b29228841989186447421350258f.jpg",
        "image_caption": [
            "Figure 3-23. Full attention versus sparse attention. Figure 3-24 explains the coloring. (Source: “Generating long sequences with sparse transformers”.) "
        ],
        "image_footnote": [],
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "Each row corresponds to a token being processed. The color coding indicates which tokens the model is able to pay attention to while it’s processing the token in the dark blue cell. Figure 3-24 describes this with more clarity. ",
        "page_idx": 118
    },
    {
        "type": "image",
        "img_path": "images/dc17de364261ccd0a453c6a02e81b2d6060c41e44195d11bc7b9663a8e5e46d6.jpg",
        "image_caption": [
            "Figure 3-24. Attention figures show which token is being processed, and which previous tokens an attention mechanism allows it to attend to. "
        ],
        "image_footnote": [],
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "This figure also shows the autoregressive nature of decoder Transformer blocks (which make up most text generation models); they can only pay attention to previ‐ ous tokens. Contrast this to BERT, which can pay attention to both sides (hence the B in BERT stands for bidirectional). ",
        "page_idx": 118
    },
    {
        "type": "text",
        "text": "Multi-query and grouped-query attention ",
        "text_level": 1,
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "A more recent efficient attention tweak to the Transformer is grouped-query atten‐ tion (“GQA: Training generalized multi-query transformer models from multi-head checkpoints”), which is used by models like Llama 2 and 3. Figure 3-25 shows these different types of attention, and the next section continues to explain them. ",
        "page_idx": 119
    },
    {
        "type": "image",
        "img_path": "images/b630ffa4d3a130cdd018c661c87c2a04ebc79c2eb211e89cab7a496acfad0d0d.jpg",
        "image_caption": [
            "Figure 3-25. A comparison of different kinds of attention: the original multi-head, grouped-query attention, and multi-query attention (source: “Fast transformer decod‐ ing: One write-head is all you need”). "
        ],
        "image_footnote": [],
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "Grouped-query attention builds on multi-query attention (“Fast transformer decod‐ ing: One write-head is all you need”). These methods improve inference scalability of larger models by reducing the size of the matrices involved. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "Optimizing attention: From multi-head to multi-query to grouped query ",
        "text_level": 1,
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "Earlier in the chapter we showed how the Transformer paper described multi-headed attention. The Illustrated Transformer discusses in detail how the queries, keys, and values matrices are used to conduct the attention operation. Figure 3-26 shows how each “attention head” has its own distinct query, key, and value matrices calculated for a given input. ",
        "page_idx": 119
    },
    {
        "type": "text",
        "text": "The way that multi-query attention optimizes this is to share the keys and values matrices between all the heads. So the only unique matrices for each head would be the queries matrices, as we can see in Figure 3-27. ",
        "page_idx": 119
    },
    {
        "type": "image",
        "img_path": "images/d79e2d7fbcde597e6c08eb784667514ebd5cedc2278c8d9f93ba9986a9cb4636.jpg",
        "image_caption": [
            "Figure 3-26. Attention is conducted using matrices of queries, keys, and values. In multi-head attention, each head has a distinct version of each of these matrices. "
        ],
        "image_footnote": [],
        "page_idx": 120
    },
    {
        "type": "image",
        "img_path": "images/252017fce5a41978b890befec909ece0093f74bf520c9f0999c026e59fd746a5.jpg",
        "image_caption": [
            "Figure 3-27. Multi-query attention presents a more efficient attention mechanism by sharing the keys and values matrices across all the attention heads. "
        ],
        "image_footnote": [],
        "page_idx": 120
    },
    {
        "type": "text",
        "text": "As model sizes grow, however, this optimization can be too punishing and we can afford to use a little more memory to improve the quality of the models. This is where grouped-query attention comes in. Instead of cutting the number of keys and values matrices to one of each, it allows us to use more (but less than the number of heads). Figure 3-28 shows these groups and how each group of attention heads shares keys and values matrices. ",
        "page_idx": 121
    },
    {
        "type": "image",
        "img_path": "images/e2089894abfedbd869c76960112cf7433de6b6eb1ee2e85f754d08cf67ec2ff4.jpg",
        "image_caption": [
            "Figure 3-28. Grouped-query attention sacrifices a little bit of the efficiency of multiquery attention in return for a large improvement in quality by allowing multiple groups of shared key/value matrices; each group has its respective set of attention heads. "
        ],
        "image_footnote": [],
        "page_idx": 121
    },
    {
        "type": "text",
        "text": "Flash Attention ",
        "text_level": 1,
        "page_idx": 121
    },
    {
        "type": "text",
        "text": "Flash Attention is a popular method and implementation that provides significant speedups for both training and inference of Transformer LLMs on GPUs. It speeds up the attention calculation by optimizing what values are loaded and moved between a GPU’s shared memory (SRAM) and high bandwidth memory (HBM). It is described in detail in the papers “FlashAttention: Fast and memory-efficient exact attention with IO-awareness” and the subsequent “FlashAttention-2: Faster attention with bet‐ ter parallelism and work partitioning”. ",
        "page_idx": 121
    },
    {
        "type": "text",
        "text": "The Transformer Block ",
        "text_level": 1,
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "Recall that the two major components of a Transformer block are an attention layer and a feedforward neural network. A more detailed view of the block would also reveal the residual connections and layer-normalization operations that we can see in Figure 3-29. ",
        "page_idx": 122
    },
    {
        "type": "image",
        "img_path": "images/d26cd2e93bd9399e9a2a4b6e13ca6a0227999d1c625a381fa7a3ecb5787de649.jpg",
        "image_caption": [
            "Figure 3-29. A Transformer block from the original Transformer paper. "
        ],
        "image_footnote": [],
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "The latest Transformer models at the time of this writing still retain the major components, yet make a number of tweaks as we can see in Figure 3-30. ",
        "page_idx": 122
    },
    {
        "type": "text",
        "text": "One of the differences we see in this version of the Transformer block is that nor‐ malization happens prior to attention and the feedforward layers. This has been reported to reduce the required training time (read: “On layer normalization in the Transformer architecture”). Another improvement in normalization here is using RMSNorm, which is simpler and more efficient than the LayerNorm used in the original Transformer (read: “Root mean square layer normalization”). Lastly, instead of the original Transformer’s ReLU activation function, newer variants like SwiGLU (described in “GLU Variants Improve Transformer”) are now more common. ",
        "page_idx": 122
    },
    {
        "type": "image",
        "img_path": "images/332630301ed2a8de0b27f04cae8794a04ad230035429f872ae9cbc4c8577564f.jpg",
        "image_caption": [
            "Figure 3-30. The Transformer block of a 2024-era Transformer like Llama 3 features some tweaks like pre-normalization and an attention optimized with grouped-query attention and rotary embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 123
    },
    {
        "type": "text",
        "text": "Positional Embeddings (RoPE) ",
        "text_level": 1,
        "page_idx": 123
    },
    {
        "type": "text",
        "text": "Positional embeddings have been a key component since the original Transformer. They enable the model to keep track of the order of tokens/words in a sequence/ sentence, which is an indispensable source of information in language. From the many positional encoding schemes proposed in the past years, rotary positional embeddings (or “RoPE,” introduced in “RoFormer: Enhanced Transformer with rotary position embedding”) is especially important to point out. ",
        "page_idx": 123
    },
    {
        "type": "text",
        "text": "The original Transformer paper and some of the early variants had absolute posi‐ tional embeddings that, in essence, marked the first token as position 1, the second as position 2...etc. These could either be static methods (where the positional vectors are generated using geometric functions) or learned (where the model training assigns them their values during the learning process). Some challenges arise from such methods when we scale up models, which requires us to find ways to improve their efficiency. ",
        "page_idx": 123
    },
    {
        "type": "text",
        "text": "For example, one challenge in efficiently training models with large context is that a lot of documents in the training set are much shorter than that context. It would be inefficient to allocate the entire, say, 4K context to a short 10-word sentence. So during model training, documents are packed together into each context in the training batch, as Figure 3-31 shows. ",
        "page_idx": 123
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 124
    },
    {
        "type": "image",
        "img_path": "images/2c9abbab95a3089c5762402d2c5d56f2433d653ae453689dbd8d2324b5408261.jpg",
        "image_caption": [
            "Figure 3-31. Packing is the process of efficiently organizing short training documents into the context. It includes grouping multiple documents in a single context while minimizing the padding at the end of the context. "
        ],
        "image_footnote": [],
        "page_idx": 124
    },
    {
        "type": "text",
        "text": "Learn more about packing by reading “Efficient sequence packing without crosscontamination: Accelerating large language models without impacting performance” and watching the great visuals in “Introducing packed BERT for 2X training speed-up in natural language processing”. ",
        "page_idx": 124
    },
    {
        "type": "text",
        "text": "Positional embedding methods have to adapt to this and other practical considera‐ tions. If Document 50, for example, starts at position 50, then we’d be misinforming the model if we tell it that that first token is number 50 and that would affect its performance (because it would assume there’s previous context while in reality the earlier tokens belong to a different and unrelated document the model should ignore). ",
        "page_idx": 124
    },
    {
        "type": "text",
        "text": "Instead of the static, absolute embeddings that are added in the beginning of the forward pass, rotary embeddings are a method to encode positional information in a way that captures absolute and relative token position information. It is based on the idea of rotating vectors in their embeddings space. In the forward pass, they are added in the attention step, as Figure 3-32 shows. ",
        "page_idx": 124
    },
    {
        "type": "image",
        "img_path": "images/1141d2d1ec78f3985b067bc2447721b379d0ea4c480a8b3b1aa61efaffa27ca0.jpg",
        "image_caption": [
            "Figure 3-32. Rotary embeddings are applied in the attention step, not at the start of the forward pass. "
        ],
        "image_footnote": [],
        "page_idx": 125
    },
    {
        "type": "text",
        "text": "During the attention process, the positional information is mixed in specifically to the queries and keys matrices just before we multiply them for relevance scoring, as we can see in Figure 3-33. ",
        "page_idx": 125
    },
    {
        "type": "image",
        "img_path": "images/d20be1d048e576d1b5aae1dc537558a9ec41c1a3b09998844cf8289c1c972be1.jpg",
        "image_caption": [
            "Figure 3-33. Rotary positional embeddings are added to the representation of tokens just before the relevance scoring step in self-attention. "
        ],
        "image_footnote": [],
        "page_idx": 126
    },
    {
        "type": "text",
        "text": "Other Architectural Experiments and Improvements ",
        "text_level": 1,
        "page_idx": 126
    },
    {
        "type": "text",
        "text": "Many tweaks of the Transformer are proposed and researched on a continuous basis. “A Survey of Transformers” highlights a few of the main directions. Transformer architectures are also constantly adapted to domains beyond LLMs. Computer vision is an area where a lot of Transformer architecture research is happening (see: “Trans‐ formers in vision: A survey” and “A survey on vision transformer”). Other domains include robotics (see “Open X-Embodiment: Robotic learning datasets and RT-X models”) and time series (see “Transformers in time series: A survey”). ",
        "page_idx": 126
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 127
    },
    {
        "type": "text",
        "text": "In this chapter we discussed the main intuitions of Transformers and recent develop‐ ments that enable the latest Transformer LLMs. We went over many new concepts, so let’s break down the key concepts that we discussed in this chapter: ",
        "page_idx": 127
    },
    {
        "type": "text",
        "text": "• A Transformer LLM generates one token at a time.   \n• That output token is appended to the prompt, then this updated prompt is presen‐ ted to the model again for another forward pass to generate the next token.   \n• The three major components of the Transformer LLM are the tokenizer, a stack of Transformer blocks, and a language modeling head.   \n• The tokenizer contains the token vocabulary for the model. The model has token embeddings associated with those tokens. Breaking the text into tokens and then using the embeddings of these tokens is the first step in the token generation process.   \n• The forward pass flows through all the stages once, one by one.   \n• Near the end of the process, the LM head scores the probabilities of the next possible token. Decoding strategies inform which actual token to pick as the output for this generation step (sometimes it’s the most probable next token, but not always).   \n• One reason the Transformer excels is its ability to process tokens in parallel. Each of the input tokens flow into their individual tracks or streams of processing. The number of streams is the model’s “context size” and this represents the max number of tokens the model can operate on.   \n• Because Transformer LLMs loop to generate the text one token at a time, it’s a good idea to cache the processing results of each step so we don’t duplicate the processing effort (these results are stored as various matrices within the layers).   \n• The majority of processing happens within Transformer blocks. These are made up of two components. One of them is the feedforward neural network, which is able to store information and make predictions and interpolations from data it was trained on.   \n• The second major component of a Transformer block is the attention layer. Attention incorporates contextual information to allow the model to better cap‐ ture the nuance of language.   \n• Attention happens in two major steps: (1) scoring relevance and (2) combining information.   \n• A Transformer attention layer conducts several attention operations in parallel, each occurring inside an attention head, and their outputs are aggregated to make up the output of the attention layer.   \n• Attention can be accelerated via sharing the keys and values matrices between all heads, or groups of heads (grouped-query attention).   \n• Methods like Flash Attention speed up the attention calculation by optimizing how the operation is done on the different memory systems of a GPU. ",
        "page_idx": 127
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 128
    },
    {
        "type": "text",
        "text": "Transformers continue to see new developments and proposed tweaks to improve them in different scenarios, including language models and other domains and applications. ",
        "page_idx": 128
    },
    {
        "type": "text",
        "text": "In Part II of the book, we will cover some of these practical applications of LLMs. In Chapter 4, we start with text classification, a common task in Language AI. This next chapter serves as an introduction to applying both generative and representation models. ",
        "page_idx": 128
    },
    {
        "type": "text",
        "text": "Using Pretrained Language Models ",
        "text_level": 1,
        "page_idx": 130
    },
    {
        "type": "text",
        "text": "Text Classification ",
        "text_level": 1,
        "page_idx": 132
    },
    {
        "type": "text",
        "text": "A common task in natural language processing is classification. The goal of the task is to train a model to assign a label or class to some input text (see Figure 4-1). Classifying text is used across the world for a wide range of applications, from sentiment analysis and intent detection to extracting entities and detecting language. The impact of language models, both representative and generative, on classification cannot be understated. ",
        "page_idx": 132
    },
    {
        "type": "image",
        "img_path": "images/affaa0c44bf69e6a44aef23721e2ce5bc3e8e2cd49ccb92e89bc0fbff4c75954.jpg",
        "image_caption": [
            "Figure 4-1. Using a language model to classify text. "
        ],
        "image_footnote": [],
        "page_idx": 132
    },
    {
        "type": "text",
        "text": "In this chapter, we will discuss several ways to use language models for classifying text. It will serve as an accessible introduction to using language models that already have been trained. Due to the broad field of text classification, we will discuss several techniques and use them to explore the field of language models: ",
        "page_idx": 132
    },
    {
        "type": "text",
        "text": "“Text Classification with Representation Models” on page 113 demonstrates the flexibility of nongenerative models for classification. We will cover both taskspecific models and embedding models. “Text Classification with Generative Models” on page 127 is an introduction to generative language models as most of them can be used for classification. We will cover both an open source as well as a closed source language model. ",
        "page_idx": 132
    },
    {
        "type": "text",
        "text": "In this chapter, we will focus on leveraging pretrained language models, models that already have been trained on large amounts of data that can be used for classifying text. As illustrated in Figure 4-2, we will examine both representation and language models and explore their differences. ",
        "page_idx": 133
    },
    {
        "type": "image",
        "img_path": "images/8905badb807d8e635b7b0ae2fd86a5dea0f3fdc51f9cd8396b773d67ba95239b.jpg",
        "image_caption": [
            "Figure 4-2. Although both representation and generative models can be used for classifi‐ cation, their approaches differ. "
        ],
        "image_footnote": [],
        "page_idx": 133
    },
    {
        "type": "text",
        "text": "This chapter serves as an introduction to a variety of language models, both genera‐ tive and nongenerative. We will encounter common packages for loading and using these models. ",
        "page_idx": 133
    },
    {
        "type": "image",
        "img_path": "images/54e19cbc15e9e17d6a31d2df862701d9226acd37ea8b43714fba9341e3172809.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 133
    },
    {
        "type": "text",
        "text": "Although this book focuses on LLMs, it is highly advised to com‐ pare these examples against classic, but strong baselines such as representing text with TF-IDF and training a logistic regression classifier on top of that. ",
        "page_idx": 133
    },
    {
        "type": "text",
        "text": "The Sentiment of Movie Reviews ",
        "text_level": 1,
        "page_idx": 133
    },
    {
        "type": "text",
        "text": "You can find the data we use to explore techniques for classifying text on the Hugging Face Hub, a platform for hosting models but also data. We will use the well-known “rotten_tomatoes” dataset to train and evaluate our models.1 It contains 5,331 positive and 5,331 negative movie reviews from Rotten Tomatoes. ",
        "page_idx": 133
    },
    {
        "type": "text",
        "text": "To load this data, we make use of the datasets package, which will be used through‐ out the book: ",
        "page_idx": 133
    },
    {
        "type": "text",
        "text": "from datasets import load_dataset ",
        "text_level": 1,
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "# Load our data   \ndata $=$ load_dataset(\"rotten_tomatoes\")   \ndata   \nDatasetDict({ train: Dataset({ features: ['text', 'label'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label'], num_rows: 1066 })   \n}) ",
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "The data is split up into train, test, and validation splits. Throughout this chapter, we will use the train split when we train a model and the test split for validating the results. Note that the additional validation split can be used to further validate generalization if you used the train and test splits to perform hyperparameter tuning. ",
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "Let’s take a look at some examples in our train split: ",
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "{'text': ['the rock is destined to be the 21st century\\'s new \" conan \" and that he\\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',   \n'things really get weird , though not particularly scary : the movie is all portent and no content .'],   \n'label': [1, 0]} ",
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "These short reviews are either labeled as positive (1) or negative (0). This means that we will focus on binary sentiment classification. ",
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "Text Classification with Representation Models ",
        "text_level": 1,
        "page_idx": 134
    },
    {
        "type": "text",
        "text": "Classification with pretrained representation models generally comes in two flavors, either using a task-specific model or an embedding model. As we explored in the previous chapter, these models are created by fine-tuning a foundation model, like BERT, on a specific downstream task as illustrated in Figure 4-3. ",
        "page_idx": 134
    },
    {
        "type": "image",
        "img_path": "images/e9c3fefb2276cf31e09c71c648bff47d2dd858d52a5afd0b830ae49e5c4f6c2e.jpg",
        "image_caption": [
            "Figure 4-3. A foundation model is fine-tuned for specific tasks; for instance, to perform classification or generate general-purpose embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 135
    },
    {
        "type": "text",
        "text": "A task-specific model is a representation model, such as BERT, trained for a specific task, like sentiment analysis. As we explored in Chapter 1, an embedding model generates general-purpose embeddings that can be used for a variety of tasks not limited to classification, like semantic search (see Chapter 8). ",
        "page_idx": 135
    },
    {
        "type": "text",
        "text": "The process of fine-tuning a BERT model for classification is covered in Chapter 11 while creating an embedding model is covered in Chapter 10. In this chapter, we keep both models frozen (nontrainable) and only use their output as shown in Figure 4-4. ",
        "page_idx": 135
    },
    {
        "type": "image",
        "img_path": "images/b77dfa0e94159e1e44d55850b3b0d686e3d6f74ef7bd450d52093a946125f8ae.jpg",
        "image_caption": [
            "Figure 4-4. Perform classification directly with a task-specific model or indirectly with general-purpose embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 135
    },
    {
        "type": "text",
        "text": "We will leverage pretrained models that others have already fine-tuned for us and explore how they can be used to classify our selected movie reviews. ",
        "page_idx": 135
    },
    {
        "type": "text",
        "text": "Model Selection ",
        "text_level": 1,
        "page_idx": 136
    },
    {
        "type": "text",
        "text": "Choosing the right models is not as straightforward as you might think with over 60,000 models on the Hugging Face Hub for text classification and more than 8,000 models that generate embeddings at the moment of writing. Moreover, it’s crucial to select a model that fits your use case and consider its language compatibility, the underlying architecture, size, and performance. ",
        "page_idx": 136
    },
    {
        "type": "text",
        "text": "Let’s start with the underlying architecture. As we explored in Chapter 1, BERT, a well-known encoder-only architecture, is a popular choice for creating task-specific and embedding models. While generative models, like the GPT family, are incredible models, encoder-only models similarly excel in task-specific use cases and tend to be significantly smaller in size. ",
        "page_idx": 136
    },
    {
        "type": "text",
        "text": "Over the years, many variations of BERT have been developed, including RoBERTa,2 DistilBERT,3 ALBERT,4 and DeBERTa,5 each trained in various contexts. You can find an overview of some well-known BERT-like models in Figure 4-5. ",
        "page_idx": 136
    },
    {
        "type": "image",
        "img_path": "images/3fe335633ef31f65c040b1ea635967717fcc3275f84f258bcf61a57846b5af59.jpg",
        "image_caption": [
            "Figure 4-5. A timeline of common BERT-like model releases. These are considered foundation models and are mostly intended to be fine-tuned on a downstream task. "
        ],
        "image_footnote": [],
        "page_idx": 136
    },
    {
        "type": "text",
        "text": "Selecting the right model for the job can be a form of art in itself. Trying thousands of pretrained models that can be found on Hugging Face’s Hub is not feasible so we need to be efficient with the models that we choose. Having said that, several models are great starting points and give you an idea of the base performance of these kinds of models. Consider them solid baselines: ",
        "page_idx": 136
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "• BERT base model (uncased)   \n• RoBERTa base model   \n• DistilBERT base model (uncased)   \n• DeBERTa base model   \n• bert-tiny   \n• ALBERT base v2 ",
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "For the task-specific model, we are choosing the Twitter-RoBERTa-base for Senti‐ ment Analysis model. This is a RoBERTa model fine-tuned on tweets for sentiment analysis. Although this was not trained specifically for movie reviews, it is interesting to explore how this model generalizes. ",
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "When selecting models to generate embeddings from, the MTEB leaderboard is a great place to start. It contains open and closed source models benchmarked across several tasks. Make sure to not only take performance into account. The importance of inference speed should not be underestimated in real-life solutions. As such, we will use sentence-transformers/all-mpnet-base-v2 as the embedding throughout this section. It is a small but performant model. ",
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "Using a Task-Specific Model ",
        "text_level": 1,
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "Now that we have selected our task-specific representation model, let’s start by load‐ ing our model: ",
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "from transformers import pipeline   \n# Path to our HF model   \nmodel_path $=$ \"cardiffnlp/twitter-roberta-base-sentiment-latest\"   \n# Load model into pipeline   \npipe $=$ pipeline( model=model_path, tokenizer=model_path, return_all_scores ${ } = { }$ True, device=\"cuda:0\"   \n) ",
        "page_idx": 137
    },
    {
        "type": "text",
        "text": "As we load our model, we also load the tokenizer, which is responsible for converting input text into individual tokens, as illustrated in Figure 4-6. Although that parameter is not needed as it is loaded automatically, it illustrates what is happening under the hood. ",
        "page_idx": 138
    },
    {
        "type": "image",
        "img_path": "images/c472dec498dfb0544fee126ffc1ed695270a4835c2ece95ca7e833abe30d34cc.jpg",
        "image_caption": [
            "Figure 4-6. An input sentence is first fed to a tokenizer before it can be processed by the task-specific model. "
        ],
        "image_footnote": [],
        "page_idx": 138
    },
    {
        "type": "text",
        "text": "These tokens are at the core of most language models, as explored in depth in Chapter 2. A major benefit of these tokens is that they can be combined to generate representations even if they were not in the training data, as shown in Figure 4-7. ",
        "page_idx": 138
    },
    {
        "type": "image",
        "img_path": "images/662f4a53dfd6944e42c4314c1110c4853e24d28c73c94d18dde3bef34ff6a5e8.jpg",
        "image_caption": [
            "Figure 4-7. By breaking down an unknown word into tokens, word embeddings can still be generated. "
        ],
        "image_footnote": [],
        "page_idx": 139
    },
    {
        "type": "text",
        "text": "After loading all the necessary components, we can go ahead and use our model on the test split of our data: ",
        "page_idx": 139
    },
    {
        "type": "text",
        "text": "import numpy as np   \nfrom tqdm import tqdm   \nfrom transformers.pipelines.pt_utils import KeyDataset   \n# Run inference   \ny_pred $= \\ [ ]$   \nfor output in tqdm(pipe(KeyDataset(data[\"test\"], \"text\")),   \ntotal $\\cdot ^ { = }$ len(data[\"test\"])): negative_score $=$ output[0][\"score\"] positive_score $=$ output[2][\"score\"] assignment $=$ np.argmax([negative_score, positive_score]) y_pred.append(assignment) ",
        "page_idx": 139
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 139
    },
    {
        "type": "text",
        "text": "Now that we have generated our predictions, all that is left is evaluation. We create a small function that we can easily use throughout this chapter: ",
        "page_idx": 139
    },
    {
        "type": "text",
        "text": "from sklearn.metrics import classification_report   \ndef evaluate_performance(y_true, y_pred): \"\"\"Create and print the classification report\"\"\" performance $=$ classification_report( y_true, y_pred, target_names $\\mathbf { \\Psi } _ { 1 } =$ [\"Negative Review\", \"Positive Review\"] ",
        "page_idx": 139
    },
    {
        "type": "text",
        "text": ") print(performance) ",
        "page_idx": 140
    },
    {
        "type": "text",
        "text": "Next, let’s create our classification report: ",
        "page_idx": 140
    },
    {
        "type": "text",
        "text": "evaluate_performance(data[\"test\"][\"label\"], y_pred) ",
        "page_idx": 140
    },
    {
        "type": "table",
        "img_path": "images/bae6024e4a5d04cb67044c960ebc635477fe6993360a5b3a1e3e0897f9e077a1.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.76</td><td>0.88</td><td>0.81</td><td>533</td></tr><tr><td>Positive Review</td><td>0.86</td><td>0.72</td><td>0.78</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.80</td><td>1066</td></tr><tr><td>macro avg</td><td>0.81</td><td>0.80</td><td>0.80</td><td>1066</td></tr><tr><td> weighted avg</td><td>0.81</td><td>0.80</td><td>0.80</td><td>1066</td></tr></table>",
        "page_idx": 140
    },
    {
        "type": "text",
        "text": "To read the resulting classification report, let’s first start by exploring how we can identify correct and incorrect predictions. There are four combinations depending on whether we predict something correctly (True) versus incorrectly (False) and whether we predict the correct class (Positive) versus incorrect class (Negative). We can illustrate these combinations as a matrix, commonly referred to as a confusion matrix, in Figure 4-8. ",
        "page_idx": 140
    },
    {
        "type": "image",
        "img_path": "images/a4dd0c74760d3bb047455a47f89b7662abb519c126b11d6f844f208d2188e2a6.jpg",
        "image_caption": [
            "Figure 4-8. The confusion matrix describes four types of predictions we can make. "
        ],
        "image_footnote": [],
        "page_idx": 140
    },
    {
        "type": "text",
        "text": "Using the confusion matrix, we can derive several formulas to describe the quality of the model. In the previously generated classification report we can see four such methods, namely precision, recall, accuracy, and the F1 score: ",
        "page_idx": 140
    },
    {
        "type": "text",
        "text": "• Precision measures how many of the items found are relevant, which indicates the accuracy of the relevant results.   \n• Recall refers to how many relevant classes were found, which indicates its ability to find all relevant results. ",
        "page_idx": 140
    },
    {
        "type": "text",
        "text": "• Accuracy refers to how many correct predictions the model makes out of all predictions, which indicates the overall correctness of the model. • The F1 score balances both precision and recall to create a model’s overall performance. ",
        "page_idx": 141
    },
    {
        "type": "text",
        "text": "These four metrics are illustrated in Figure 4-9, which describes them using the aforementioned classification report. ",
        "page_idx": 141
    },
    {
        "type": "image",
        "img_path": "images/f7e331d4de3b35bc6015f78d796f1cd777d7d23145b684ddfe040e362c2bbd24.jpg",
        "image_caption": [
            "Figure 4-9. The classification report describes several metrics for evaluating a model’s performance. "
        ],
        "image_footnote": [],
        "page_idx": 141
    },
    {
        "type": "text",
        "text": "We will consider the weighted average of the F1 score throughout the examples in this book to make sure each class is treated equally. Our pretrained BERT model gives us an F1 score of 0.80 (we are reading this from the weighted avg row and the $f { \\cal I }$ -score column), which is great for a model not trained specifically on our domain data! ",
        "page_idx": 141
    },
    {
        "type": "text",
        "text": "To improve the performance of our selected model, we could do a few different things including selecting a model trained on our domain data, movie reviews in this case, like DistilBERT base uncased finetuned SST-2. We could also shift our focus to another flavor of representation models, namely embedding models. ",
        "page_idx": 141
    },
    {
        "type": "text",
        "text": "Classification Tasks That Leverage Embeddings ",
        "text_level": 1,
        "page_idx": 141
    },
    {
        "type": "text",
        "text": "In the previous example, we used a pretrained task-specific model for sentiment analysis. However, what if we cannot find a model that was pretrained for this specific task? Do we need to fine-tune a representation model ourselves? The answer is no! ",
        "page_idx": 141
    },
    {
        "type": "text",
        "text": "There might be times when you want to fine-tune the model yourself if you have sufficient computing available (see Chapter 11). However, not everyone has access to extensive computing. This is where general-purpose embedding models come in. ",
        "page_idx": 142
    },
    {
        "type": "text",
        "text": "Supervised Classification ",
        "text_level": 1,
        "page_idx": 142
    },
    {
        "type": "text",
        "text": "Unlike the previous example, we can perform part of the training process ourselves by approaching it from a more classical perspective. Instead of directly using the rep‐ resentation model for classification, we will use an embedding model for generating features. Those features can then be fed into a classifier, thereby creating a two-step approach as shown in Figure 4-10. ",
        "page_idx": 142
    },
    {
        "type": "image",
        "img_path": "images/f267c56ab8b160fd5dcb7d1234c4337729b477ed1cc61ea2b4885153211fc69a.jpg",
        "image_caption": [
            "Figure 4-10. The feature extraction step and classification steps are separated. "
        ],
        "image_footnote": [],
        "page_idx": 142
    },
    {
        "type": "text",
        "text": "A major benefit of this separation is that we do not need to fine-tune our embedding model, which can be costly. In contrast, we can train a classifier, like a logistic regression, on the CPU instead. ",
        "page_idx": 142
    },
    {
        "type": "text",
        "text": "In the first step, we convert our textual input to embeddings using the embedding model as shown in Figure 4-11. Note that this model is similarly kept frozen and is not updated during the training process. ",
        "page_idx": 142
    },
    {
        "type": "image",
        "img_path": "images/8e457f93ab024a5bd264725e3e53c74ceae577356b0b573990e28b39e750931c.jpg",
        "image_caption": [
            "Figure 4-11. In step 1, we use the embedding model to extract the features and convert the input text to embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 142
    },
    {
        "type": "text",
        "text": "We can perform this step with sentence-transformer, a popular package for lever‐ aging pretrained embedding models.6 Creating the embeddings is straightforward: ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "from sentence_transformers import SentenceTransformer # Load model model $=$ SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\") ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "# Convert text to embeddings train_embeddings $=$ model.encode(data[\"train\"][\"text\"], show_progress_bar=True) test_embeddings $=$ model.encode(data[\"test\"][\"text\"], show_progress_bar $=$ True) ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "As we covered in Chapter 1, these embeddings are numerical representations of the input text. The number of values, or dimension, of the embedding depends on the underlying embedding model. Let’s explore that for our model: ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "train_embeddings.shape (8530, 768) ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "This shows that each of our 8,530 input documents has an embedding dimension of 768 and therefore each embedding contains 768 numerical values. ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "In the second step, these embeddings serve as the input features to the classifier illus‐ trated in Figure 4-12. The classifier is trainable and not limited to logistic regression and can take on any form as long as it performs classification. ",
        "page_idx": 143
    },
    {
        "type": "image",
        "img_path": "images/75e79fbb7394f92632023fa7298659c53def1154fadebf8a300d8cac3005880a.jpg",
        "image_caption": [
            "Figure 4-12. Using the embeddings as our features, we train a logistic regression model on our training data. "
        ],
        "image_footnote": [],
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "We will keep this step straightforward and use a logistic regression as the classifier. To train it, we only need to use the generated embeddings together with our labels: ",
        "page_idx": 143
    },
    {
        "type": "text",
        "text": "from sklearn.linear_model import LogisticRegression # Train a logistic regression on our train embeddings clf $=$ LogisticRegression(random_state $= 4 2$ ) clf.fit(train_embeddings, data[\"train\"][\"label\"]) ",
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "Next, let’s evaluate our model: ",
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "# Predict previously unseen instances y_pred $=$ clf.predict(test_embeddings) evaluate_performance(data[\"test\"][\"label\"], y_pred) ",
        "page_idx": 144
    },
    {
        "type": "table",
        "img_path": "images/eafa16b88b7c267859601ddfac94129d080499f52fdd93a3709dd8248f0d0594.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td> precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.85</td><td>0.86</td><td>0.85 533</td></tr><tr><td>Positive Review</td><td>0.86</td><td>0.85</td><td>0.85 533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.85 1066</td></tr><tr><td>.macro avg</td><td>0.85</td><td>0.85</td><td>0.85 1066</td></tr><tr><td> weighted avg</td><td>0.85</td><td>0.85</td><td>0.85 1066</td></tr></table>",
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "By training a classifier on top of our embeddings, we managed to get an F1 score of 0.85! This demonstrates the possibilities of training a lightweight classifier while keeping the underlying embedding model frozen. ",
        "page_idx": 144
    },
    {
        "type": "image",
        "img_path": "images/7f9f2b5ea5af1fa8ff1ce5fbe949db02e5d73bf21c1e2bbd877ef25a5b1dcafe.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "In this example, we used sentence-transformers to extract our embeddings, which benefits from a GPU to speed up inference. However, we can remove this GPU dependency by using an exter‐ nal API to create the embeddings. Popular choices for generating embeddings are Cohere’s and OpenAI’s offerings. As a result, this would allow the pipeline to run entirely on the CPU. ",
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "What If We Do Not Have Labeled Data? ",
        "text_level": 1,
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "In our previous example, we had labeled data that we could leverage, but this might not always be the case in practice. Getting labeled data is a resource-intensive task that can require significant human labor. Moreover, is it actually worthwhile to collect these labels? ",
        "page_idx": 144
    },
    {
        "type": "text",
        "text": "To test this, we can perform zero-shot classification, where we have no labeled data to explore whether the task seems feasible. Although we know the definition of the labels (their names), we do not have labeled data to support them. Zero-shot classification attempts to predict the labels of input text even though it was not trained on them, as shown in Figure 4-13. ",
        "page_idx": 144
    },
    {
        "type": "image",
        "img_path": "images/265bd0d259380b761bd1a21bd7d9342b925527b78320c1853d92e2e4fa49c63a.jpg",
        "image_caption": [
            "Figure 4-13. In zero-shot classification, we have no labeled data, only the labels them‐ selves. The zero-shot model decides how the input is related to the candidate labels. "
        ],
        "image_footnote": [],
        "page_idx": 145
    },
    {
        "type": "text",
        "text": "To perform zero-shot classification with embeddings, there is a neat trick that we can use. We can describe our labels based on what they should represent. For example, a negative label for movie reviews can be described as “This is a negative movie review.” By describing and embedding the labels and documents, we have data that we can work with. This process, as illustrated in Figure 4-14, allows us to generate our own target labels without the need to actually have any labeled data. ",
        "page_idx": 145
    },
    {
        "type": "image",
        "img_path": "images/920bc9dfeeb24b102e3f920b0068561b50ac97cbf9ff2b189312bd7b85663271.jpg",
        "image_caption": [
            "Figure 4-14. To embed the labels, we first need to give them a description, such as “a negative movie review.” This can then be embedded through sentence-transformers. "
        ],
        "image_footnote": [],
        "page_idx": 145
    },
    {
        "type": "text",
        "text": "We can create these label embeddings using the .encode function as we did earlier: ",
        "page_idx": 146
    },
    {
        "type": "text",
        "text": "# Create embeddings for our labels label_embeddings $=$ model.encode([\"A negative review\", \"A positive review\"]) ",
        "page_idx": 146
    },
    {
        "type": "text",
        "text": "To assign labels to documents, we can apply cosine similarity to the document label pairs. This is the cosine of the angle between vectors, which is calculated through the dot product of the embeddings and divided by the product of their lengths, as illustrated in Figure 4-15. ",
        "page_idx": 146
    },
    {
        "type": "image",
        "img_path": "images/448abda12fb4e8be27942313f3ac3377d6175e583c8f250ad79efb22c7401875.jpg",
        "image_caption": [
            "Figure 4-15. The cosine similarity is the angle between two vectors or embeddings. In this example, we calculate the similarity between a document and the two possible labels, positive and negative. "
        ],
        "image_footnote": [],
        "page_idx": 146
    },
    {
        "type": "text",
        "text": "We can use cosine similarity to check how similar a given document is to the descrip‐ tion of the candidate labels. The label with the highest similarity to the document is chosen as illustrated in Figure 4-16. ",
        "page_idx": 146
    },
    {
        "type": "image",
        "img_path": "images/f3d7f1218d13a2d81dd3b286f11e482cc0b055c1c6e0973aa70e405dd306001c.jpg",
        "image_caption": [
            "Figure 4-16. After embedding the label descriptions and the documents, we can use cosine similarity for each label document pair. "
        ],
        "image_footnote": [],
        "page_idx": 146
    },
    {
        "type": "text",
        "text": "To perform cosine similarity on the embeddings, we only need to compare the document embeddings with the label embeddings and get the best matching pairs: ",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "from sklearn.metrics.pairwise import cosine_similarity # Find the best matching label for each document sim_matrix $=$ cosine_similarity(test_embeddings, label_embeddings) y_pred $=$ np.argmax(sim_matrix, axis $^ { , = 1 }$ ) ",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "And that is it! We only needed to come up with names for our labels to perform our classification tasks. Let’s see how well this method works: ",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "evaluate_performance(data[\"test\"][\"label\"], y_pred) ",
        "page_idx": 147
    },
    {
        "type": "table",
        "img_path": "images/2fe946a2bd2255f56304f2b215adad2251ece8b70a28777a987c90e9d312ddbd.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.78</td><td>0.77</td><td>0.78</td><td>533</td></tr><tr><td>Positive Review</td><td>0.77</td><td>0.79</td><td>0.78</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.78</td><td>1066</td></tr><tr><td>macro avg</td><td>0.78</td><td>0.78</td><td>0.78</td><td>1066</td></tr><tr><td>weighted avg</td><td>0.78</td><td>0.78</td><td>0.78</td><td>1066</td></tr></table>",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "If you are familiar with zero-shot classification with Transformerbased models, you might wonder why we choose to illustrate this with embeddings instead. Although natural language inference models are amazing for zero-shot classification, the example here demonstrates the flexibility of embeddings for a variety of tasks. As you will see throughout the book, embeddings can be found in most Language AI use cases and are often an underestimated but incredibly vital component. ",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "An F1 score of 0.78 is quite impressive considering we did not use any labeled data at all! This just shows how versatile and useful embeddings are, especially if you are a bit creative with how they are used. ",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "Let’s put that creativity to the test. We decided upon “A nega‐ tive/positive review” as the name of our labels but that can be improved. Instead, we can make them a bit more concrete and specific toward our data by using “A very negative/positive movie review” instead. This way, the embedding will capture that it is a movie review and will focus a bit more on the extremes of the two labels. Try it out and explore how it affects the results. ",
        "page_idx": 147
    },
    {
        "type": "text",
        "text": "Text Classification with Generative Models ",
        "text_level": 1,
        "page_idx": 148
    },
    {
        "type": "text",
        "text": "Classification with generative language models, such as OpenAI’s GPT models, works a bit differently from what we have done thus far. These models take as input some text and generative text and are thereby aptly named sequence-to-sequence models. This is in stark contrast to our task-specific model, which outputs a class instead, as illustrated in Figure 4-17. ",
        "page_idx": 148
    },
    {
        "type": "image",
        "img_path": "images/01e0ed2ef86c77b6f4267f8f67e29dcaa15578d5da6b6973adec85d95f2d1307.jpg",
        "image_caption": [
            "Figure 4-17. A task-specific model generates numerical values from sequences of tokens while a generative model generates sequences of tokens from sequences of tokens. "
        ],
        "image_footnote": [],
        "page_idx": 148
    },
    {
        "type": "text",
        "text": "These generative models are generally trained on a wide variety of tasks and usually do not perform your use case out of the box. For instance, if we give a generative model a movie review without any context, it has no idea what to do with it. ",
        "page_idx": 148
    },
    {
        "type": "text",
        "text": "Instead, we need to help it understand the context and guide it toward the answers that we are looking for. As demonstrated in Figure 4-18, this guiding process is done mainly through the instruction, or prompt, that you give such a model. Iteratively improving your prompt to get your preferred output is called prompt engineering. ",
        "page_idx": 148
    },
    {
        "type": "image",
        "img_path": "images/8b816ac90a8a3cb0f362bd74b9d7722ecb9d8d633e4f22703079f6ca76264bdf.jpg",
        "image_caption": [
            "Figure 4-18. Prompt engineering allows prompts to be updated to improve the output generated by the model. "
        ],
        "image_footnote": [],
        "page_idx": 149
    },
    {
        "type": "text",
        "text": "In this section, we will demonstrate how we can leverage different types of generative models to perform classification without our Rotten Tomatoes dataset. ",
        "page_idx": 149
    },
    {
        "type": "text",
        "text": "Using the Text-to-Text Transfer Transformer ",
        "text_level": 1,
        "page_idx": 149
    },
    {
        "type": "text",
        "text": "Throughout this book, we will explore mostly encoder-only (representation) mod‐ els like BERT and decoder-only (generative) models like ChatGPT. However, as discussed in Chapter 1, the original Transformer architecture actually consists of an encoder-decoder architecture. Like the decoder-only models, these encoder-decoder models are sequence-to-sequence models and generally fall in the category of genera‐ tive models. ",
        "page_idx": 149
    },
    {
        "type": "text",
        "text": "An interesting family of models that leverage this architecture is the Text-to-Text Transfer Transformer or T5 model. Illustrated in Figure 4-19, its architecture is similar to the original Transformer where 12 decoders and 12 encoders are stacked together.7 ",
        "page_idx": 149
    },
    {
        "type": "image",
        "img_path": "images/5b87a0bb7c5b457a86bc54a66ecb21989643c31729160d4a03a7a27a0466df67.jpg",
        "image_caption": [
            "Figure 4-19. The T5 architecture is similar to the original Transformer model, a decoderencoder architecture. "
        ],
        "image_footnote": [],
        "page_idx": 150
    },
    {
        "type": "text",
        "text": "With this architecture, these models were first pretrained using masked language modeling. In the first step of training, illustrated in Figure 4-20, instead of masking individual tokens, sets of tokens (or token spans) were masked during pretraining. ",
        "page_idx": 150
    },
    {
        "type": "image",
        "img_path": "images/87099f7601ca611279f0ee88345374c662c7619ae4b4c61448f758b0c4c8d12f.jpg",
        "image_caption": [
            "Figure 4-20. In the first step of training, namely pretraining, the T5 model needs to predict masks that could contain multiple tokens. "
        ],
        "image_footnote": [],
        "page_idx": 150
    },
    {
        "type": "text",
        "text": "The second step of training, namely fine-tuning the base model, is where the real magic happens. Instead of fine-tuning the model for one specific task, each task is converted to a sequence-to-sequence task and trained simultaneously. As illustrated in Figure 4-21, this allows the model to be trained on a wide variety of tasks. ",
        "page_idx": 150
    },
    {
        "type": "image",
        "img_path": "images/5ccca55ec9c3bf6b9b918141effc47f7d6c25ee88a14780fbd5a3f36f76e3e3e.jpg",
        "image_caption": [
            "Figure 4-21. By converting specific tasks to textual instructions, the T5 model can be trained on a variety of tasks during fine-tuning. "
        ],
        "image_footnote": [],
        "page_idx": 151
    },
    {
        "type": "text",
        "text": "This method of fine-tuning was extended in the paper “Scaling instruction-finetuned language models”, which introduced more than a thousand tasks during fine-tuning that more closely follow instructions as we know them from GPT models.8 This resulted in the Flan-T5 family of models that benefit from this large variety of tasks. ",
        "page_idx": 151
    },
    {
        "type": "text",
        "text": "To use this pretrained Flan-T5 model for classification, we will start by loading it through the \"text2text-generation\" task, which is generally reserved for these encoder-decoder models: ",
        "page_idx": 151
    },
    {
        "type": "text",
        "text": "# Load our model   \npipe $=$ pipeline( \"text2text-generation\", model \"google/flan-t5-small\", device=\"cuda:0\"   \n) ",
        "page_idx": 151
    },
    {
        "type": "text",
        "text": "The Flan-T5 model comes in various sizes (flan-t5-small/base/large/xl/xxl) and we will use the smallest to speed things up a bit. However, feel free to play around with larger models to see if you can improve the results. ",
        "page_idx": 151
    },
    {
        "type": "text",
        "text": "Compared to our task-specific model, we cannot just give the model some text and hope it will output the sentiment. Instead, we will have to instruct the model to do so. ",
        "page_idx": 151
    },
    {
        "type": "text",
        "text": "Thus, we prefix each document with the prompt “Is the following sentence positive or negative?”: ",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "# Prepare our data   \nprompt $=$ \"Is the following sentence positive or negative? \"   \ndata $=$ data.map(lambda example: {\"t5\": prompt $^ +$ example['text']})   \ndata   \nDatasetDict({ train: Dataset({ features: ['text', 'label', 't5'], num_rows: 8530 }) validation: Dataset({ features: ['text', 'label', 't5'], num_rows: 1066 }) test: Dataset({ features: ['text', 'label', 't5'], num_rows: 1066 })   \n}) ",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "After creating our updated data, we can run the pipeline similar to the task-specific example: ",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "# Run inference   \ny_pred $= \\ [ ]$   \nfor output in tqdm(pipe(KeyDataset(data[\"test\"], \"t5\")),   \ntotal $\\cdot ^ { = }$ len(data[\"test\"])): text $=$ output[0][\"generated_text\"] y_pred.append(0 if text $= =$ \"negative\" else 1) ",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "Since this model generates text, we did need to convert the textual output to numer‐ ical values. The output word “negative” was mapped to 0 whereas “positive” was mapped to 1. ",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "These numerical values now allow us to test the quality of the model in the same way we have done before: ",
        "page_idx": 152
    },
    {
        "type": "table",
        "img_path": "images/707dcaf767588ad301ddce0ffa1ff1f2dc6aaf4df52fd04a5de8628e50426bdf.jpg",
        "table_caption": [
            "evaluate_performance(data[\"test\"][\"label\"], y_pred) "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td> support</td></tr><tr><td>Negative Review</td><td>0.83</td><td>0.85</td><td>0.84</td><td>533</td></tr><tr><td>Positive Review</td><td>0.85</td><td>0.83</td><td>0.84</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.84</td><td>1066</td></tr><tr><td>macro avg</td><td>0.84</td><td>0.84</td><td>0.84</td><td>1066</td></tr><tr><td> weighted avg</td><td>0.84</td><td>0.84</td><td>0.84</td><td>1066</td></tr></table>",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "With an F1 score of 0.84, it is clear this Flan-T5 model is an amazing first look into the capabilities of generative models. ",
        "page_idx": 152
    },
    {
        "type": "text",
        "text": "ChatGPT for Classification ",
        "text_level": 1,
        "page_idx": 153
    },
    {
        "type": "text",
        "text": "Although we focus throughout the book on open source models, another major com‐ ponent of the Language AI field is closed sourced models; in particular, ChatGPT. ",
        "page_idx": 153
    },
    {
        "type": "text",
        "text": "Although the underlying architecture of the original ChatGPT model (GPT-3.5) is not shared, we can assume from its name that it is based on the decoder-only architecture that we have seen in the GPT models thus far. ",
        "page_idx": 153
    },
    {
        "type": "text",
        "text": "Fortunately, OpenAI shared an overview of the training procedure that involved an important component, namely preference tuning. As illustrated in Figure 4-22, OpenAI first manually created the desired output to an input prompt (instruction data) and used that data to create a first variant of its model. ",
        "page_idx": 153
    },
    {
        "type": "image",
        "img_path": "images/485f4111e4333807e1921a55f0d79ad850c9b924d6b78110b1cf123a94f0e5ec.jpg",
        "image_caption": [
            "Figure 4-22. Manually labeled data consisting of an instruction (prompt) and output was used to perform fine-tuning (instruction-tuning). "
        ],
        "image_footnote": [],
        "page_idx": 153
    },
    {
        "type": "text",
        "text": "OpenAI used the resulting model to generate multiple outputs that were manually ranked from best to worst. As shown in Figure 4-23, this ranking demonstrates a preference for certain outputs (preference data) and was used to create its final model, ChatGPT. ",
        "page_idx": 153
    },
    {
        "type": "image",
        "img_path": "images/18b619d2b164713fc5381e3d728ddc9e8dba1bdf0cb4277800c0b9266555a037.jpg",
        "image_caption": [
            "Figure 4-23. Manually ranked preference data was used to generate the final model, ChatGPT. "
        ],
        "image_footnote": [],
        "page_idx": 153
    },
    {
        "type": "text",
        "text": "A major benefit of using preference data over instruction data is the nuance it represents. By demonstrating the difference between a good and better output the generative model learns to generate text that resembles human preference. In Chap‐ ter 12, we will explore how these fine-tuning and preference-tuning methodologies work and how you can perform them yourself. ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "The process of using a closed sourced model is quite different from the open sourced examples we have seen thus far. Instead of loading the model, we can access the model through OpenAI’s API. ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "Before we go into the classification example, you will first need to create a free account on https://oreil.ly/AEXvA and create an API key here: https://oreil.ly/lrTXl. After doing so, you can use your API to communicate with OpenAI’s servers. ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "We can use this key to create a client: ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "import openai ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "# Create client client $=$ openai.OpenAI(api_key $\\mathbf {  }$ \"YOUR_KEY_HERE\") ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "Using this client, we create the chatgpt_generation function, which allows us to generate some text based on a specific prompt, input document, and the selected model: ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "def chatgpt_generation(prompt, document, model $=$ \"gpt-3.5-turbo-0125\" \"\"\"Generate an output based on a prompt and an input document.\"\" messages=[ { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" }, { \"role\": \"user\", \"content\": prompt.replace(\"[DOCUMENT]\", document) } ] chat_completion $=$ client.chat.completions.create( messages=messages, model $\\cdot =$ model, temperature $\\scriptstyle = 0$ ) return chat_completion.choices[0].message.content ",
        "page_idx": 154
    },
    {
        "type": "text",
        "text": "Next, we will need to create a template to ask the model to perform the classification: ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "# Define a prompt template as a base   \nprompt $=$ \"\"\"Predict whether the following document is a positive or negative   \nmovie review: ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "[DOCUMENT] ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "If it is positive return 1 and if it is negative return 0. Do not give any other answers. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "# Predict the target using GPT document $=$ \"unpretentious , charming , quirky , original\" chatgpt_generation(prompt, document) ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "This template is merely an example and can be changed however you want. For now, we kept it as simple as possible to illustrate how to use such a template. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "Before you use this over a potentially large dataset, it is important to always keep track of your usage. External APIs such as OpenAI’s offering can quickly become costly if you perform many requests. At the time of writing, running our test dataset using the “gpt-3.5-turbo- $. 0 1 2 5 ^ { \\mathrm { { \\circ } } }$ model costs 3 cents, which is covered by the free account, but this might change in the future. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "When dealing with external APIs, you might run into rate limit errors. These appear when you call the API too often as some APIs might limit the rate with which you can use it per minute or hour. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "To prevent these errors, we can implement several methods for retrying the request, including something referred to as exponential backoff. It performs a short sleep each time we hit a rate limit error and then retries the unsuccessful request. Whenever it is unsuccessful again, the sleep length is increased until the request is successful or we hit a maximum number of retries. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "To use it with OpenAI, there is a great guide that can help you get started. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "Next, we can run this for all reviews in the test dataset to get its predictions. You can skip this if you want to save your (free) credits for other tasks. ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "# You can skip this if you want to save your (free) credits   \npredictions $=$ [ chatgpt_generation(prompt, doc) for doc in tqdm(data[\"test\"][\"text\"])   \n] ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "Like the previous example, we need to convert the output from strings to integers to evaluate its performance: ",
        "page_idx": 155
    },
    {
        "type": "text",
        "text": "# Extract predictions y_pred $=$ [int(pred) for pred in predictions] ",
        "text_level": 1,
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "# Evaluate performance evaluate_performance(data[\"test\"][\"label\"], y_pred) ",
        "page_idx": 156
    },
    {
        "type": "table",
        "img_path": "images/d8987056cbefb474e0f898955ebdd0e8e6127c94c7119e6f594e9cd215231037.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td>precision</td><td>recall</td><td>f1-score</td><td>support</td></tr><tr><td>Negative Review</td><td>0.87</td><td>0.97</td><td>0.92</td><td>533</td></tr><tr><td>Positive Review</td><td>0.96</td><td>0.86</td><td>0.91</td><td>533</td></tr><tr><td>accuracy</td><td></td><td></td><td>0.91</td><td>1066</td></tr><tr><td>macro avg</td><td>0.92</td><td>0.91</td><td>0.91</td><td>1066</td></tr><tr><td> weighted avg</td><td>0.92</td><td>0.91</td><td>0.91</td><td>1066</td></tr></table>",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "The F1 score of 0.91 already gives a glimpse into the performance of the model that brought generative AI to the masses. However, since we do not know what data the model was trained on, we cannot easily use these kinds of metrics for evaluating the model. For all we know, it might have actually been trained on our dataset! ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "In Chapter 12, we will explore how we can evaluate both open source and closed source models on more generalized tasks. ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "In this chapter, we discussed many different techniques for performing a wide variety of classification tasks, from fine-tuning your entire model to no tuning at all! Classi‐ fying textual data is not as straightforward as it may seem on the surface and there is an incredible amount of creative techniques for doing so. ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "In this chapter, we explored text classification using both generative and representa‐ tion language models. Our goal was to assign a label or class to input text for the classification of a review’s sentiment. ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "We explored two types of representation models, a task-specific model and an embedding model. The task-specific model was pretrained on a large dataset specif‐ ically for sentiment analysis and showed us that pretrained models are a great technique for classifying documents. The embedding model was used to generate multipurpose embeddings that we used as the input to train a classifier. ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "Similarly, we explored two types of generative models, an open source encoderdecoder model (Flan-T5) and a closed source decoder-only model (GPT-3.5). We used these generative models in text classification without requiring specific (addi‐ tional) training on domain data or labeled datasets. ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "In the next chapter, we will continue with classification but focus instead on unsuper‐ vised classification. What can we do if we have textual data without any labels? What information can we extract? We will focus on clustering our data as well as naming the clusters with topic modeling techniques. ",
        "page_idx": 156
    },
    {
        "type": "text",
        "text": "Text Clustering and Topic Modeling ",
        "text_level": 1,
        "page_idx": 158
    },
    {
        "type": "text",
        "text": "Although supervised techniques, such as classification, have reigned supreme over the last few years in the industry, the potential of unsupervised techniques such as text clustering cannot be understated. ",
        "page_idx": 158
    },
    {
        "type": "text",
        "text": "Text clustering aims to group similar texts based on their semantic content, meaning, and relationships. As illustrated in Figure 5-1, the resulting clusters of semantically similar documents not only facilitate efficient categorization of large volumes of unstructured text but also allow for quick exploratory data analysis. ",
        "page_idx": 158
    },
    {
        "type": "image",
        "img_path": "images/ca165fafe03da19c20cafc8ae5a93bc41f1b1d6193b277ceb21caf009b1b7f18.jpg",
        "image_caption": [
            "Figure 5-1. Clustering unstructured textual data. "
        ],
        "image_footnote": [],
        "page_idx": 158
    },
    {
        "type": "text",
        "text": "The recent evolution of language models, which enable contextual and semantic representations of text, has enhanced the effectiveness of text clustering. Language is more than a bag of words, and recent language models have proved to be quite capable of capturing that notion. Text clustering, unbound by supervision, allows for creative solutions and diverse applications, such as finding outliers, speedup labeling, and finding incorrectly labeled data. ",
        "page_idx": 158
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "Text clustering has also found itself in the realm of topic modeling, where we want to discover (abstract) topics that appear in large collections of textual data. As shown in Figure 5-2, we generally describe a topic using keywords or keyphrases and, ideally, have a single overarching label. ",
        "page_idx": 159
    },
    {
        "type": "image",
        "img_path": "images/d4f3598c5badde0a43af58c22cc4afac68733048928eafc5ada838654179a1ee.jpg",
        "image_caption": [
            "Figure 5-2. Topic modeling is a way to give meaning to clusters of textual documents. "
        ],
        "image_footnote": [],
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "In this chapter, we will first explore how to perform clustering with embedding models and then transition to a text-clustering-inspired method of topic modeling, namely BERTopic. ",
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "Text clustering and topic modeling have an important role in this book as they explore creative ways to combine a variety of different language models. We will explore how combining encoder-only (embeddings), decoder-only (generative), and even classical methods (bag-of-words) can result in amazing new techniques and pipelines. ",
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "ArXiv’s Articles: Computation and Language ",
        "text_level": 1,
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "Throughout this chapter, we will be running clustering and topic modeling algo‐ rithms on ArXiv articles. ArXiv is an open-access platform for scholarly articles, mostly in the fields of computer science, mathematics, and physics. We will explore articles in the field of Computation and Language to keep with the theme of this book. The dataset contains 44,949 abstracts between 1991 and 2024 from ArXiv’s cs.CL (Computation and Language) section. ",
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "We load the data and create separate variables for the abstracts, titles, and years of each article: ",
        "page_idx": 159
    },
    {
        "type": "text",
        "text": "# Load data from Hugging Face from datasets import load_dataset dataset $=$ load_dataset(\"maartengr/arxiv_nlp\")[\"train\"] ",
        "text_level": 1,
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "# Extract metadata abstracts $=$ dataset[\"Abstracts\"] titles $=$ dataset[\"Titles\"] ",
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "A Common Pipeline for Text Clustering ",
        "text_level": 1,
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "Text clustering allows for discovering patterns in data that you may or may not be familiar with. It allows for getting an intuitive understanding of the task, for example, a classification task, but also of its complexity. As a result, text clustering can become more than just a quick method for exploratory data analysis. ",
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "Although there are many methods for text clustering, from graph-based neural net‐ works to centroid-based clustering techniques, a common pipeline that has gained popularity involves three steps and algorithms: ",
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "1. Convert the input documents to embeddings with an embedding model.   \n2. Reduce the dimensionality of embeddings with a dimensionality reduction model.   \n3. Find groups of semantically similar documents with a cluster model. ",
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "Embedding Documents ",
        "text_level": 1,
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "The first step is to convert our textual data to embeddings, as illustrated in Figure 5-3. Recall from previous chapters that embeddings are numerical representations of text that attempt to capture its meaning. ",
        "page_idx": 160
    },
    {
        "type": "image",
        "img_path": "images/60d2282659142a3ce17385f05d00f232ac9ac5fecd64977e96110de0e165d385.jpg",
        "image_caption": [
            "Figure 5-3. Step 1: We convert documents to embeddings using an embedding model. "
        ],
        "image_footnote": [],
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "Choosing embedding models optimized for semantic similarity tasks is especially important for clustering as we attempt to find groups of semantically similar documents. Fortunately, most embedding models at the time of writing focus on just that, semantic similarity. ",
        "page_idx": 160
    },
    {
        "type": "text",
        "text": "As we did in the previous chapter, we will use the MTEB leaderboard to select an embedding model. We will need an embedding model that has a decent score on clustering tasks but also is small enough to run quickly. Instead of using the “sentence-transformers/all-mpnet-base-v2” model we used in the previous chapter, we use the “thenlper/gte-small” model instead. It is a more recent model that outper‐ forms the previous model on clustering tasks and due to its small size is even faster for inference. However, feel free to play around with newer models that have been released since! ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "from sentence_transformers import SentenceTransformer # Create an embedding for each abstract embedding_model $=$ SentenceTransformer(\"thenlper/gte-small\") embeddings $=$ embedding_model.encode(abstracts, show_progress_bar=True) ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "Let’s check how many values each document embedding contains: ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "# Check the dimensions of the resulting embeddings embeddings.shape ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "(44949, 384) ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "Each embedding has 384 values that together represent the semantic representation of the document. You can view these embeddings as the features that we want to cluster. ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "Reducing the Dimensionality of Embeddings ",
        "text_level": 1,
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "Before we cluster the embeddings, we will first need to take their high dimensionality into account. As the number of dimensions increases, there is an exponential growth in the number of possible values within each dimension. Finding all subspaces within each dimension becomes increasingly complex. ",
        "page_idx": 161
    },
    {
        "type": "text",
        "text": "As a result, high-dimensional data can be troublesome for many clustering tech‐ niques as it gets more difficult to identify meaningful clusters. Instead, we can make use of dimensionality reduction. As illustrated in Figure 5-4, this technique allows us to reduce the size of the dimensional space and represent the same data with fewer dimensions. Dimensionality reduction techniques aim to preserve the global structure of high-dimensional data by finding low-dimensional representations. ",
        "page_idx": 161
    },
    {
        "type": "image",
        "img_path": "images/180fbe51fd25a709a2f317ea5f670e0aedf185bfabf5d90cb7ed84fe997d958c.jpg",
        "image_caption": [
            "Figure 5-4. Dimensionality reduction allows data in high-dimensional space to be com‐ pressed to a lower-dimensional representation. "
        ],
        "image_footnote": [],
        "page_idx": 162
    },
    {
        "type": "text",
        "text": "Note that this is a compression technique and that the underlying algorithm is not arbitrarily removing dimensions. To help the cluster model create meaningful clus‐ ters, the second step in our clustering pipeline is therefore dimensionality reduction, as shown in Figure 5-5. ",
        "page_idx": 162
    },
    {
        "type": "image",
        "img_path": "images/464dda49be096fa6b350f9d83be9763c57f05da5494ef4789a23a18c27fe894c.jpg",
        "image_caption": [
            "Figure 5-5. Step 2: The embeddings are reduced to a lower-dimensional space using dimensionality reduction. "
        ],
        "image_footnote": [],
        "page_idx": 162
    },
    {
        "type": "text",
        "text": "Well-known methods for dimensionality reduction are Principal Component Analy‐ sis (PCA)1 and Uniform Manifold Approximation and Projection (UMAP).2 For this pipeline, we are going with UMAP as it tends to handle nonlinear relationships and structures a bit better than PCA. ",
        "page_idx": 162
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 163
    },
    {
        "type": "image",
        "img_path": "images/e56cbb42f47136c33f3355f7aa55ebaeb8e2002de1d787a499936f144d71517e.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "Dimensionality reduction techniques, however, are not flawless. They do not perfectly capture high-dimensional data in a lowerdimensional representation. Information will always be lost with this procedure. There is a balance between reducing dimensionality and keeping as much information as possible. ",
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "To perform dimensionality reduction, we need to instantiate our UMAP class and pass the generated embeddings to it: ",
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "from umap import UMAP   \n# We reduce the input embeddings from 384 dimensions to 5 dimensions   \numap_model $=$ UMAP( n_components $\\scriptstyle \\mathbf { \\alpha = { \\begin{array} { l } { \\mathbf { \\alpha } } \\\\ { \\mathbf { \\beta = { \\frac { } { } } } } \\end{array} } }$ , min_dist $= 0 . 6$ , metric $= ^ { \\prime }$ 'cosine', random_state $= 4 2$   \n)   \nreduced_embeddings $=$ umap_model.fit_transform(embeddings) ",
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "We can use the n_components parameter to decide the shape of the lowerdimensional space, namely 5 dimensions. Generally, values between 5 and 10 work well to capture high-dimensional global structures. ",
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "The min_dist parameter is the minimum distance between embedded points. We are setting this to 0 as that generally results in tighter clusters. We set metric to 'cosine' as Euclidean-based methods have issues dealing with high-dimensional data. ",
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "Note that setting a random_state in UMAP will make the results reproducible across sessions but will disable parallelism and therefore slow down training. ",
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "Cluster the Reduced Embeddings ",
        "text_level": 1,
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "The third step is to cluster the reduced embeddings, as illustrated in Figure 5-6. ",
        "page_idx": 163
    },
    {
        "type": "image",
        "img_path": "images/c111dcf13f7d4ce6d0246f52d2b28d44b44e6268f81fd78c79c26381727202a2.jpg",
        "image_caption": [
            "Figure 5-6. Step 3: We cluster the documents using the embeddings with reduced dimensionality. "
        ],
        "image_footnote": [],
        "page_idx": 163
    },
    {
        "type": "text",
        "text": "Although a common choice is a centroid-based algorithm like k-means, which requires a set of clusters to be generated, we do not know the number of clusters beforehand. Instead, a density-based algorithm freely calculates the number of clus‐ ters and does not force all data points to be part of a cluster, as illustrated in Figure 5-7. ",
        "page_idx": 164
    },
    {
        "type": "image",
        "img_path": "images/1c16aa3c8bb2447624a11ca732bb1bfa44a162e4d0df87fe97762a5d6f79123e.jpg",
        "image_caption": [
            "Figure 5-7. The clustering algorithm not only impacts how clusters are generated but also how they are viewed. "
        ],
        "image_footnote": [],
        "page_idx": 164
    },
    {
        "type": "text",
        "text": "A common density-based model is Hierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN).3 HDBSCAN is a hierarchical variation of a clustering algorithm called DBSCAN that allows for dense (micro)-clusters to be found without having to explicitly specify the number of clusters.4 As a density-based method, HDBSCAN can also detect outliers in the data, which are data points that do not belong to any cluster. These outliers will not be assigned or forced to belong to any cluster. In other words, they are ignored. Since ArXiv articles might contain some niche papers, using a model that detects outliers could be helpful. ",
        "page_idx": 164
    },
    {
        "type": "text",
        "text": "As with the previous packages, using HDBSCAN is straightforward. We only need to instantiate the model and pass our reduced embeddings to it: ",
        "page_idx": 164
    },
    {
        "type": "text",
        "text": "# We fit the model and extract the clusters   \nhdbscan_model $=$ HDBSCAN( min_cluster_size $\\mathtt { \\Gamma } = 5 \\Theta$ , metric $=$ \"euclidean\", cluster_selection_method=\"eom\"   \n).fit(reduced_embeddings)   \nclusters $=$ hdbscan_model.labels_ ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "# How many clusters did we generate? len(set(clusters)) ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "156 ",
        "text_level": 1,
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "With HDBSCAN, we generated 156 clusters in our dataset. To create more clusters, we will need to reduce the value of min_cluster_size as it represents the minimum size that a cluster can take. ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "Inspecting the Clusters ",
        "text_level": 1,
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "Now that we have generated our clusters, we can inspect each cluster manually and explore the assigned documents to get an understanding of its content. For example, let us take a few random documents from cluster 0: ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "import numpy as np   \n# Print first three documents in cluster 0   \ncluster $\\mathit { \\Theta } = \\mathit { \\Theta } \\Theta$   \nfor index in np.where(clusters $= =$ cluster)[0][:3]: print(abstracts[index][:300] $^ +$ 1 \\n\") ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "This works aims to design a statistical machine translation from English text to American Sign Language (ASL). The system is based on Moses tool with some modifications and the results are synthesized through a 3D avatar for interpretation. First, we translate the input text to gloss, a written fo... ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "Researches on signed languages still strongly dissociate lin- guistic issues related on phonological and phonetic aspects, and gesture studies for recognition and synthesis purposes. This paper focuses on the imbrication of motion and meaning for the analysis, synthesis and evaluation of sign lang... ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "Modern computational linguistic software cannot produce important aspects of sign language translation. Using some researches we deduce that the majority of automatic sign language translation systems ignore many aspects when they generate animation; therefore the interpretation lost the truth inf... ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "From these documents, it seems that this cluster contains documents mostly about translation from and to sign language, interesting! ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "We can take this one step further and attempt to visualize our results instead of going through all documents manually. To do so, we will need to reduce our document embeddings to two dimensions, as that allows us to plot the documents on an $\\mathrm { x / y }$ plane: ",
        "page_idx": 165
    },
    {
        "type": "text",
        "text": "# Reduce 384-dimensional embeddings to two dimensions for easier visualization   \nreduced_embeddings $=$ UMAP( n_components $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ , min_dist $= 0 . 6$ , metric $=$ \"cosine\", random_state=42   \n).fit_transform(embeddings)   \n# Create dataframe   \ndf $=$ pd.DataFrame(reduced_embeddings, columns $=$ [\"x\", \"y\"])   \ndf[\"title\"] $=$ titles   \ndf[\"cluster\"] $=$ [str(c) for c in clusters] ",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "# Select outliers and non-outliers (clusters) to_plot $=$ df.loc[df.cluster $\\ : : = \\ : \\ \" - 1 \\ \" \\ : $ , :] outliers $=$ df.loc[df.cluster $\\ = \\begin{array} { r l } { \\frac { \\ d } { \\ d t } = } & { { } \\frac { \\ d H } { \\ d t } - 1 \\frac { \\ d H } { \\ d t } } \\end{array}$ , :] ",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "We also created a dataframe for our clusters (clusters_df) and for the outliers (out liers_df) separately since we generally want to focus on the clusters and highlight those. ",
        "page_idx": 166
    },
    {
        "type": "image",
        "img_path": "images/89ff4ab06921a661fd470a44d106a2bd0e7c36b5cd844051e5468b741e86eeb6.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "Using any dimensionality reduction technique for visualization purposes creates information loss. It is merely an approximation of what our original embeddings look like. Although it is informa‐ tive, it might push clusters together and drive them further apart than they actually are. Human evaluation, inspecting the clusters ourselves, is therefore a key component of cluster analysis! ",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "To generate a static plot, we will use the well-known plotting library, matplotlib: ",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "import matplotlib.pyplot as plt ",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "# Plot outliers and non-outliers separately   \nplt.scatter(outliers_df.x, outliers_df.y, alpha $\\scriptstyle 1 = \\atop -$ .05, $s = 2$ , $\\mathbf { c } { = }$ \"grey\")   \nplt.scatter( clusters_df.x, clusters_df.y, ${ \\mathsf { C } } { \\mathsf { = } }$ clusters_df.cluster.astype(int), alpha $1 { = } 0 \\cdot 6$ , $s = 2$ , cmap $\\mid =$ \"tab20b\"   \n)   \nplt.axis(\"off\") ",
        "page_idx": 166
    },
    {
        "type": "text",
        "text": "As we can see in Figure 5-8, it tends to capture major clusters quite well. Note how clusters of points are colored in the same color, indicating that HDBSCAN put them in a group together. Since we have a large number of clusters, the plotting library cycles the colors between clusters, so don’t think that all green points are one cluster, for example. ",
        "page_idx": 166
    },
    {
        "type": "image",
        "img_path": "images/10fd2649719a93e52c0855efea98bdac67a5445d66c6795cc31e102d9f2db734.jpg",
        "image_caption": [
            "Figure 5-8. The generated clusters (colored) and outliers (gray) are represented as a 2D visualization. "
        ],
        "image_footnote": [],
        "page_idx": 167
    },
    {
        "type": "text",
        "text": "This is visually appealing but does not yet allow us to see what is happening inside the clusters. Instead, we can extend this visualization by going from text clustering to topic modeling. ",
        "page_idx": 167
    },
    {
        "type": "text",
        "text": "From Text Clustering to Topic Modeling ",
        "text_level": 1,
        "page_idx": 167
    },
    {
        "type": "text",
        "text": "Text clustering is a powerful tool for finding structure among large collections of documents. In our previous example, we could manually inspect each cluster and identify them based on their collection of documents. For instance, we explored a cluster that contained documents about sign language. We could say that the topic of that cluster is “sign language.” ",
        "page_idx": 167
    },
    {
        "type": "text",
        "text": "This idea of finding themes or latent topics in a collection of textual data is often referred to as topic modeling. Traditionally, it involves finding a set of keywords or phrases that best represent and capture the meaning of the topic, as we illustrate in Figure 5-9. ",
        "page_idx": 167
    },
    {
        "type": "image",
        "img_path": "images/e69b4f6ea61df008d80abf7140d90f170eaec9b559c325bb6f5e020e4947cf8a.jpg",
        "image_caption": [
            "Figure 5-9. Traditionally, topics are represented by a number of keywords but can take other forms. "
        ],
        "image_footnote": [],
        "page_idx": 168
    },
    {
        "type": "text",
        "text": "Instead of labeling a topic as “sign language,” these techniques use keywords such as “sign,” “language,” and “translation” to describe the topic. As such, this does not give a single label to a topic and instead requires the user to understand the meaning of the topic through those keywords. ",
        "page_idx": 168
    },
    {
        "type": "text",
        "text": "Classic approaches, like latent Dirichlet allocation, assume that each topic is charac‐ terized by a probability distribution of words in a corpus’s vocabulary.5 Figure 5-10 demonstrates how each word in a vocabulary is scored against its relevance to each topic. ",
        "page_idx": 168
    },
    {
        "type": "image",
        "img_path": "images/95642e7afcd933a607c85960cf4cf4dc93bb50cb3347f1abedaffd43b6976cf2.jpg",
        "image_caption": [
            "Figure 5-10. Keywords are extracted based on their distribution over a single topic. "
        ],
        "image_footnote": [],
        "page_idx": 168
    },
    {
        "type": "text",
        "text": "These approaches generally use a bag-of-words technique for the main features of the textual data, which does not take the context nor the meaning of words and phrases into account. In contrast, our text clustering example does take both into account as it relies on Transformer-based embeddings that are optimized for semantic similarity and contextual meaning through attention. ",
        "page_idx": 169
    },
    {
        "type": "text",
        "text": "In this section, we will extend text clustering into the realm of topic modeling through a highly modular text clustering and topic modeling framework, namely BERTopic. ",
        "page_idx": 169
    },
    {
        "type": "text",
        "text": "BERTopic: A Modular Topic Modeling Framework ",
        "text_level": 1,
        "page_idx": 169
    },
    {
        "type": "text",
        "text": "BERTopic is a topic modeling technique that leverages clusters of semantically similar texts to extract various types of topic representations.6 The underlying algorithm can be thought of in two steps. ",
        "page_idx": 169
    },
    {
        "type": "text",
        "text": "First, as illustrated in Figure 5-11, we follow the same procedure as we did before in our text clustering example. We embed documents, reduce their dimensionality, and finally cluster the reduced embedding to create groups of semantically similar documents. ",
        "page_idx": 169
    },
    {
        "type": "image",
        "img_path": "images/0b4470d1361a453f999c70126f721d4df27f33dc90fc03254fbd4da95336eeea.jpg",
        "image_caption": [
            "Figure 5-11. The first part of BERTopic’s pipeline is to create clusters of semantically similar documents. "
        ],
        "image_footnote": [],
        "page_idx": 169
    },
    {
        "type": "text",
        "text": "Second, it models a distribution over words in the corpus’s vocabulary by leveraging a classic method, namely bag-of-words. The bag-of-words, as we discussed briefly in Chapter 1 and illustrate in Figure 5-12, does exactly what its name implies, counting the number of times each word appears in a document. The resulting representation could be used to extract the most frequent words inside a document. ",
        "page_idx": 169
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 170
    },
    {
        "type": "image",
        "img_path": "images/bc512871d6b383d4a247118e5a2e3ec3f81acadb65da201e660df5442f048d3f.jpg",
        "image_caption": [
            "Figure 5-12. A bag-of-words counts the number of times each word appears inside a document. "
        ],
        "image_footnote": [],
        "page_idx": 170
    },
    {
        "type": "text",
        "text": "There are two caveats, however. First, this is a representation on a document level and we are interested in a cluster-level perspective. To address this, the frequency of words is calculated within the entire cluster instead of only the document, as illustrated in Figure 5-13. ",
        "page_idx": 170
    },
    {
        "type": "image",
        "img_path": "images/3621062c1a92da5bbfc5cc5b4a1413b7ea4299d7c2158b2ce88fc14c633a291e.jpg",
        "image_caption": [
            "Figure 5-13. Generating $c$ -TF by counting the frequency of words per cluster instead of per document. "
        ],
        "image_footnote": [],
        "page_idx": 170
    },
    {
        "type": "text",
        "text": "Second, stop words like “the” and “I” tend to appear often in documents and provide little meaning to the actual documents. BERTopic uses a class-based variant of term frequency–inverse document frequency (c-TF-IDF) to put more weight on words that are more meaningful to a cluster and put less weight on words that are used across all clusters. ",
        "page_idx": 170
    },
    {
        "type": "text",
        "text": "Each word in the bag-of-words, the c-TF in c-TF-IDF, is multiplied by the IDF value of each word. As shown in Figure 5-14, the IDF value is calculated by taking the logarithm of the average frequency of all words across all clusters divided by the total frequency of each word. ",
        "page_idx": 170
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 171
    },
    {
        "type": "image",
        "img_path": "images/6bc49b7424ea588dda739e0722f1bbd5e9b8d6bb10baebbc2a1fdedc28b8f3a9.jpg",
        "image_caption": [
            "Figure 5-14. Creating a weighting scheme. "
        ],
        "image_footnote": [],
        "page_idx": 171
    },
    {
        "type": "text",
        "text": "The result is a weight (“IDF”) for each word that we can multiply with their fre‐ quency $( ^ { \\alpha } \\mathrm { { c - T F } ^ { \\alpha } ) }$ to get the weighted values $\\overset { \\ast } { \\boldsymbol { \\mathbf { \\mathit { c } } } }$ -TF-IDF”). ",
        "page_idx": 171
    },
    {
        "type": "text",
        "text": "This second part of the procedure, as shown in Figure 5-15, allows for generating a distribution over words as we have seen before. We can use scikit-learn’s CountVec torizer to generate the bag-of-words (or term frequency) representation. Here, each cluster is considered a topic that has a specific ranking of the corpus’s vocabulary. ",
        "page_idx": 171
    },
    {
        "type": "image",
        "img_path": "images/40f9913acba484ab58662b104ee45c858bb88127c0f21b580192a60c0d665aa3.jpg",
        "image_caption": [
            "Figure 5-15. The second part of BERTopic’s pipeline is representing the topics: the calcu‐ lation of the weight of term $^ * { x ^ { * } }$ in a class $^ * { c } ^ { * }$ . "
        ],
        "image_footnote": [],
        "page_idx": 171
    },
    {
        "type": "text",
        "text": "Putting the two steps together, clustering and representing topics, results in the full pipeline of BERTopic, as illustrated in Figure 5-16. With this pipeline, we can cluster semantically similar documents and from the clusters generate topics represented by several keywords. The higher a word’s weight in a topic, the more representative it is of that topic. ",
        "page_idx": 171
    },
    {
        "type": "image",
        "img_path": "images/f5b4b2832e127050156cac2e417147d9260c97996b9b4489867ec5dc76b3f645.jpg",
        "image_caption": [
            "Figure 5-16. The full pipeline of BERTopic, roughly, consists of two steps, clustering and topic representation. "
        ],
        "image_footnote": [],
        "page_idx": 172
    },
    {
        "type": "text",
        "text": "A major advantage of this pipeline is that the two steps, clustering and topic repre‐ sentation, are largely independent of one another. For instance, with c-TF-IDF, we are not dependent on the models used in clustering the documents. This allows for significant modularity throughout every component of the pipeline. And as we will explore later in this chapter, it is a great starting point to fine-tune the topic representations. ",
        "page_idx": 172
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 5-17, although sentence-transformers is used as the default embedding model, we can swap it with any other embedding technique. The same applies to all other steps. If you do not want outliers generated with HDBSCAN, you can use k-means instead. ",
        "page_idx": 172
    },
    {
        "type": "image",
        "img_path": "images/6a5c0dac9d0afdfa997ee7f0ffc4d051cfa3afdb72564804322c8cbe8c6e21f2.jpg",
        "image_caption": [
            "Figure 5-17. The modularity of BERTopic is a key component and allows you to build your own topic model however you want. "
        ],
        "image_footnote": [],
        "page_idx": 172
    },
    {
        "type": "text",
        "text": "You can think of this modularity as building with Lego blocks; each part of the pipeline is completely replaceable with another, similar algorithm. Through this modularity, newly released models can be integrated within its architecture. As the field of Language AI grows, so does BERTopic! ",
        "page_idx": 172
    },
    {
        "type": "text",
        "text": "The Modularity of BERTopic ",
        "text_level": 1,
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "The modularity of BERTopic has another advantage: it allows it to be used and adapted to different use cases using the same base model. For instance, BERTopic supports a wide variety of algorithmic variants: ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "• Guided topic modeling   \n• (Semi-)supervised topic modeling   \n• Hierarchical topic modeling   \n• Dynamic topic modeling   \n• Multimodal topic modeling   \n• Multi-aspect topic modeling   \n• Online and incremental topic modeling • Zero-shot topic modeling   \n• Etc. ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "The modularity and algorithmic flexibility are the foundation of the author’s aim to make BERTopic the one-stop-shop for topic modeling. You can find a full overview of its capabilities in the documentation or the repository. ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "To run BERTopic with our ArXiv dataset, we can use our previously defined models and embeddings (although it is not mandatory): ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "from bertopic import BERTopic ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "# Train our model with our previously defined models   \ntopic_model $=$ BERTopic( embedding_model=embedding_model, umap_model=umap_model, hdbscan_model=hdbscan_model, verbose=True   \n).fit(abstracts, embeddings) ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "Let us start by exploring the topics that were created. The get_topic_info() method is useful to get a quick description of the topics that we found: ",
        "page_idx": 173
    },
    {
        "type": "text",
        "text": "topic_model.get_topic_info() ",
        "page_idx": 173
    },
    {
        "type": "table",
        "img_path": "images/734985d39264586146ceeb75e04223212e8abeac010f4d870bc13e3223af29d6.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\">Topic Count Name</td><td>Representation</td><td></td></tr><tr><td>-1</td><td>14520</td><td>-1_the_of_and_to</td><td>[the, of, and,to, in, we, that, language,fo...</td><td rowspan=\"5\"></td></tr><tr><td>0</td><td>2290</td><td>0_speech_asr_recognition_end</td><td>[speech,asr, recognition,end,acoustic, spea...</td></tr><tr><td>1</td><td>1403</td><td>1_medical_clinical_biomedical_patient</td><td>[medical, clinical, biomedical, patient, healt...</td></tr><tr><td>2</td><td>1156</td><td> 2_sentiment_aspect_analysis_reviews</td><td>[sentiment,aspect,analysis, reviews, opinion...</td></tr><tr><td>3</td><td>986</td><td>3_translation_nmt_machine_neural</td><td>[translation, nmt, machine, neural, bleu, engl...</td></tr><tr><td></td><td>…</td><td>…</td><td></td></tr><tr><td>150</td><td>54</td><td>150_coherence_discourse_paragraph_text</td><td>[coherence,discourse,paragraph, text, cohes...</td></tr><tr><td>151</td><td>54</td><td>151_prompt_prompts_optimization_prompting[prompt, prompts,optimization,prompting. lm..</td><td></td></tr><tr><td>152</td><td>53</td><td>152_sentence_sts_embeddings_similarity</td><td>[sentence,sts,embeddings,similarity,embed...</td></tr><tr><td>153</td><td>53</td><td>153_counseling_mental_health_therapy</td><td>[counseling, mental, health, therapy, psychoth...</td></tr><tr><td>154</td><td>50</td><td>154_backdoor_attacs_attack_triggers</td><td>[backdoor,attacks,attack, triggers, poisoned...</td></tr></table>",
        "page_idx": 174
    },
    {
        "type": "text",
        "text": "Each of these topics is represented by several keywords, which are concatenated with a “_” in the Name column. This Name column allows us to quickly get a feeling of what the topic is about as it shows the four keywords that best represent it. ",
        "page_idx": 174
    },
    {
        "type": "text",
        "text": "You might also have noticed that the very first topic is labeled -1. That topic contains all documents that could not be fitted within a topic and are considered outliers. This is a result of the clustering algorithm, HDBSCAN, which does not force all points to be clus‐ tered. To remove outliers, we could either use a non-outlier algo‐ rithm like k-means or use BERTopic’s reduce_outliers() function to reassign the outliers to topics. ",
        "page_idx": 174
    },
    {
        "type": "text",
        "text": "We can inspect individual topics and explore which keywords best represent them with the get_topic function. For example, topic 0 contains the following keywords: ",
        "page_idx": 174
    },
    {
        "type": "text",
        "text": "topic_model.get_topic(0) ",
        "page_idx": 174
    },
    {
        "type": "text",
        "text": "[('speech', 0.028177697715245358), ('asr', 0.018971184497453525), ('recognition', 0.013457745472471012), ('end', 0.00980445092749381), ('acoustic', 0.009452082794507863), ('speaker', 0.0068822647060204885), ('audio', 0.006807649923681604), ('the', 0.0063343444687017645), ('error', 0.006320144717019838), ('automatic', 0.006290216996043161)] ",
        "page_idx": 174
    },
    {
        "type": "text",
        "text": "For example, topic 0 contains the keywords “speech,” “asr,” and “recognition.” Based on these keywords, it seems that the topic is about automatic speech recognition (ASR). ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "We can use the find_topics() function to search for specific topics based on a search term. Let’s search for a topic about topic modeling: ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "topic_model.find_topics(\"topic modeling\") ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "([22, -1, 1, 47, 32], [0.95456535, 0.91173744, 0.9074769, 0.9067007, 0.90510106]) ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "This returns that topic 22 has a relatively high similarity (0.95) with our search term. If we then inspect the topic, we can see that it is indeed a topic about topic modeling: ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "topic_model.get_topic(22) ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "[('topic', 0.06634619076655907), ('topics', 0.035308535091932707), ('lda', 0.016386314730705634), ('latent', 0.013372311924864435), ('document', 0.012973600191120576), ('documents', 0.012383715497143821), ('modeling', 0.011978375291037142), ('dirichlet', 0.010078277589545706), ('word', 0.008505619415413312), ('allocation', 0.007930890698168108)] ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "Although we know that this topic is about topic modeling, let’s see if the BERTopic abstract is also assigned to this topic: ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "topic_model.topics_[titles.index(\"BERTopic: Neural topic modeling with a classbased TF-IDF procedure\")] ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "It is! These functionalities allow us to quickly find the topics that we are interested in. ",
        "page_idx": 175
    },
    {
        "type": "image",
        "img_path": "images/fb5a6a627925aeff69f28933610954ed587b3a7282f7ec02218ab2b771bf8825.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "The modularity of BERTopic gives you a lot of choices, which can be overwhelming. For that purpose, the author created a best practices guide that goes through common practices to speed up training, improve representations, and more. ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "To make exploration of the topics a bit easier, we can look back at our text clustering example. There, we created a static visualization to see the general structure of the created topic. With BERTopic, we can create an interactive variant that allows us to quickly explore which topics exist and which documents they contain. ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "Doing so requires us to use the two-dimensional embeddings, reduced_embeddings, that we created with UMAP. Moreover, when we hover over documents, we will show the title instead of the abstract to quickly get an understanding of the documents in a topic: ",
        "page_idx": 175
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 176
    },
    {
        "type": "text",
        "text": "# Visualize topics and documents   \nfig $=$ topic_model.visualize_documents( titles, reduced_embeddings $\\mathbf { \\equiv }$ reduced_embeddings, width $\\scriptstyle 1 = 1 2 0 0$ , hide_annotations=True   \n)   \n# Update fonts of legend for easier visualization   \nfig.update_layout(font $\\Bumpeq$ dict(size=16)) ",
        "page_idx": 176
    },
    {
        "type": "text",
        "text": "As we can see in Figure 5-18, this interactive plot quickly gives us a sense of the created topics. You can zoom in to view individual documents or double-click a topic on the righthand side to only view it. ",
        "page_idx": 176
    },
    {
        "type": "image",
        "img_path": "images/c898502a99eb5b6f57789b06f53cf452f2a20512a5caf8223263ee330b9cbfac.jpg",
        "image_caption": [
            "Figure 5-18. The output when we visualize documents and topics. "
        ],
        "image_footnote": [],
        "page_idx": 176
    },
    {
        "type": "text",
        "text": "There is a wide range of visualization options in BERTopic. There are three that are worthwhile to explore to get an idea of the relationships between topics: ",
        "page_idx": 176
    },
    {
        "type": "text",
        "text": "# Visualize barchart with ranked keywords topic_model.visualize_barchart() ",
        "page_idx": 176
    },
    {
        "type": "text",
        "text": "# Visualize relationships between topics # Visualize the potential hierarchical structure of topics topic_model.visualize_hierarchy() ",
        "page_idx": 176
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 177
    },
    {
        "type": "text",
        "text": "Adding a Special Lego Block ",
        "text_level": 1,
        "page_idx": 177
    },
    {
        "type": "text",
        "text": "The pipeline in BERTopic that we have explored thus far, albeit fast and modular, has a disadvantage: it still represents a topic through a bag-of-words without taking into account semantic structures. ",
        "page_idx": 177
    },
    {
        "type": "text",
        "text": "The solution is to leverage the strength of the bag-of-words representation, which is its speed to generate a meaningful representation. We can use this first meaningful representation and tweak it using more powerful but slower techniques, like embed‐ ding models. As shown in Figure 5-19, we can rerank the initial distribution of words to improve the resulting representation. Note that this idea of reranking an initial set of results is a main staple in neural search, a subject that we cover in Chapter 8. ",
        "page_idx": 177
    },
    {
        "type": "image",
        "img_path": "images/5ec7d9aa29a98bda87190a24b579d75b601bd76dd48b7fb61c4bdef239c07946.jpg",
        "image_caption": [
            "Figure 5-19. Fine-tune the topic representations by reranking the original c-TF-IDF distributions. "
        ],
        "image_footnote": [],
        "page_idx": 177
    },
    {
        "type": "text",
        "text": "As a result, we can design a new Lego block, as shown in Figure 5-20, that takes in this first topic representation and spits out an improved representation. ",
        "page_idx": 177
    },
    {
        "type": "text",
        "text": "In BERTopic, such reranker models are referred to as representation models. A major benefit of this approach is that the optimization of topic representations only needs to be done as many times as we have topics. For instance, if we have millions of documents and a hundred topics, the representation block only needs to be applied once for every topic instead of for every document. ",
        "page_idx": 177
    },
    {
        "type": "text",
        "text": "As shown in Figure 5-21, a wide variety of representation blocks have been designed for BERTopic that allows you to fine-tune the representations. The representation block can even be stacked multiple times to fine-tune representations using different methodologies. ",
        "page_idx": 177
    },
    {
        "type": "image",
        "img_path": "images/ddf83756ff5f49549aeb4834e0a71f14a6997abfa15f8bb96402fb8cf41a4302.jpg",
        "image_caption": [
            "Figure 5-20. The reranker (representation) block sits on top of the c-TF-IDF representation. "
        ],
        "image_footnote": [],
        "page_idx": 178
    },
    {
        "type": "image",
        "img_path": "images/c04ae98b4904f231e98f1f805ede28a77db05b039efa79ce47cd8bf7610c3abd.jpg",
        "image_caption": [
            "Figure 5-21. After applying the $c$ -TF-IDF weighting, topics can be fine-tuned with a wide variety of representation models, many of which are large language models. "
        ],
        "image_footnote": [],
        "page_idx": 178
    },
    {
        "type": "text",
        "text": "Before we explore how we can use these representation blocks, we first need to do two things. First, we are going to save our original topic representations so that it will be much easier to compare with and without representation models: ",
        "page_idx": 178
    },
    {
        "type": "text",
        "text": "# Save original representations   \nfrom copy import deepcopy   \noriginal_topics $=$ deepcopy(topic_model.topic_representations_) ",
        "page_idx": 178
    },
    {
        "type": "text",
        "text": "Second, let’s create a short wrapper that we can use to quickly visualize the differences in topic words to compare with and without representation models: ",
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "def topic_differences(model, original_topics, nr_topics $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ ): \"\"\"Show the differences in topic representations between two models \"\"\" df $=$ pd.DataFrame(columns=[\"Topic\", \"Original\", \"Updated\"]) for topic in range(nr_topics): ",
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "# Extract top 5 words per topic per model   \nog_words $=$ \" | \".join(list(zip(\\*original_topics[topic]))[0][:5]) new_words $=$ \" | \".join(list(zip(\\*model.get_topic(topic)))[0][:5]) df.loc[len(df)] $=$ [topic, og_words, new_words] ",
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "return df ",
        "text_level": 1,
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "KeyBERTInspired ",
        "text_level": 1,
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "The first representation block that we are going to explore is KeyBERTInspired. KeyBERTInspired is, as you might have guessed, a method inspired by the keyword extraction package, KeyBERT.7 KeyBERT extracts keywords from texts by comparing word and document embeddings through cosine similarity. ",
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "BERTopic uses a similar approach. KeyBERTInspired uses c-TF-IDF to extract the most representative documents per topic by calculating the similarity between a document’s c-TF-IDF values and those of the topic they correspond to. As shown in Figure 5-22, the average document embedding per topic is calculated and compared to the embeddings of candidate keywords to rerank the keywords. ",
        "page_idx": 179
    },
    {
        "type": "image",
        "img_path": "images/0eb33e3125dd0b63ad4d62ff48056aeecce8cf3e15d9b430b9b62fde964d1944.jpg",
        "image_caption": [
            "Figure 5-22. KeyBERTInspired representation model procedure. "
        ],
        "image_footnote": [],
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "Due to the modular nature of BERTopic, we can update our initial topic representa‐ tions with KeyBERTInspired without needing to perform the dimensionality reduc‐ tion and clustering steps: ",
        "page_idx": 179
    },
    {
        "type": "text",
        "text": "from bertopic.representation import KeyBERTInspired ",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "# Update our topic representations using KeyBERTInspired   \nrepresentation_model $=$ KeyBERTInspired()   \ntopic_model.update_topics(abstracts, representation_model $\\cdot ^ { = }$ representation_model) ",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "# Show topic differences topic_differences(topic_model, original_topics) ",
        "page_idx": 180
    },
    {
        "type": "table",
        "img_path": "images/b922b33400539239dedaf99d80cb17431b7bc2d884f3039eeebf8cb4fd2b90b7.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Topic Original</td><td></td><td>Updated</td></tr><tr><td>0</td><td>speech |asr| recognition |end |acoustic</td><td>speech |encoder| phonetic|language |trans...</td></tr><tr><td>1</td><td>medical |clinical biomedical | patient | he..</td><td> nlp |ehr | cinical | biomedical | language</td></tr><tr><td>2</td><td>sentiment |aspect |analysis | reviews|opinion</td><td>aspect |sentiment | aspects |sentiments |cl...</td></tr><tr><td>3</td><td>translation |nmt | machine | neural | bleu</td><td>translation |translating|translate |transl...</td></tr><tr><td>4</td><td></td><td> summarization |summaries |summary | abstract.. summarization |summarizers | summaries |sum..</td></tr></table>",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "The updated model shows that the topics are easier to read compared to the original model. It also demonstrates the downside of using embedding-based techniques. Words in the original model, like nmt (topic 3), which stands for neural machine translation, are removed as the model could not properly represent the entity. For domain experts, these abbreviations are highly informative. ",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "Maximal marginal relevance ",
        "text_level": 1,
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "With c-TF-IDF and the previously shown KeyBERTInspired techniques, we still have significant redundancy in the resulting topic representations. For instance, having both the words “summaries” and “summary” in a topic representation introduces redundancy as they are quite similar. ",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "We can use maximal marginal relevance (MMR) to diversify our topic representa‐ tions. The algorithm attempts to find a set of keywords that are diverse from one another but still relate to the documents they are compared to. It does so by embed‐ ding a set of candidate keywords and iteratively calculating the next best keyword to add. Doing so requires setting a diversity parameter, which indicates how diverse keywords need to be. ",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "In BERTopic, we use MMR to go from a set of initial keywords, let’s say 30, to a smaller but more diverse set of keywords, let’s say 10. It filters out redundant words and only keeps words that contribute something new to the topic representation. ",
        "page_idx": 180
    },
    {
        "type": "text",
        "text": "Doing so is rather straightforward: ",
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "from bertopic.representation import MaximalMarginalRelevance # Update our topic representations to MaximalMarginalRelevance representation_model $=$ MaximalMarginalRelevance(diversity $= 0 . 2$ ) topic_model.update_topics(abstracts, representation_model $\\cdot ^ { = }$ representation_model) ",
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "# Show topic differences topic_differences(topic_model, original_topics) ",
        "page_idx": 181
    },
    {
        "type": "table",
        "img_path": "images/d34eec71bd6ada3c4b8ba948f1dbe1996d6b5d0ae51213559307991c0d30b37f.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Topic Original</td><td></td><td>Updated</td></tr><tr><td>0</td><td>speech |asr|recognition| end|acoustic</td><td>speech | asr | error| model | training</td></tr><tr><td>1</td><td>medical | clinical | biomedical | patient |he..</td><td>clinical | biomedical | patient | healthcare |..</td></tr><tr><td>2</td><td> sentiment |aspect |analysis reviews |opinion</td><td> sentiment |analysis reviews |absa | polarity</td></tr><tr><td>3</td><td>translation | nmt |machine| neural |bleu</td><td>translation | nmt|bleu |parallel | multili..</td></tr><tr><td>4</td><td> summarization |summaries | summary |abstract...</td><td> summarization |document |extractive |rouge .</td></tr></table>",
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "The resulting topics demonstrate more diversity in their representations. For instance, topic 4 only shows one “summary”-like word and instead adds other words that might contribute more to the overall representation. ",
        "page_idx": 181
    },
    {
        "type": "image",
        "img_path": "images/2ebbcbd5ba6769fe692b9a756d8e217953907f2a54710156479b9768ba49be77.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "Both KeyBERTInspired and MMR are amazing techniques for improving the first set of topic representations. KeyBERTInspired especially tends to remove nearly all stop words since it focuses on the semantic relationships between words and documents. ",
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "The Text Generation Lego Block ",
        "text_level": 1,
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "The representation block in BERTopic has been acting as a reranking block in our previous examples. However, as we already explored in the previous chapter, genera‐ tive models have great potential for a wide variety of tasks. ",
        "page_idx": 181
    },
    {
        "type": "text",
        "text": "We can use generative models in BERTopic quite efficiently by following a part of the reranking procedure. Instead of using a generative model to identify the topic of all documents, of which there can potentially be millions, we will use the model to generate a label for our topic. As illustrated in Figure 5-23, instead of generating or reranking keywords, we ask the model to generate a short label based on keywords that were previously generated and a small set of representative documents. ",
        "page_idx": 181
    },
    {
        "type": "image",
        "img_path": "images/96245ab4502220e340231197a480152fa77eca532706c4b3aa2f9b10d714c001.jpg",
        "image_caption": [
            "Figure 5-23. Use text generative LLMs and prompt engineering to create labels for topics from keywords and documents related to each topic. "
        ],
        "image_footnote": [],
        "page_idx": 182
    },
    {
        "type": "text",
        "text": "There are two components to the illustrated prompt. First, the documents that are inserted using the [DOCUMENTS] tag are a small subset of documents, typically four, that best represent the topic. The documents with the highest cosine similarity of their c-TF-IDF values with those of the topic are selected. Second, the keywords that make up a topic are also passed to the prompt and referenced using the [KEYWORDS] tag. The keywords could be generated by c-TF-IDF or any of the other representa‐ tions we discussed thus far. ",
        "page_idx": 182
    },
    {
        "type": "text",
        "text": "As a result, we only need to use the generative model once for every topic, of which there could be potentially hundreds, instead of once for each document, of which there could potentially be millions. There are many generative models that we can choose from, both open source and proprietary. Let’s start with a model that we have explored in the previous chapter, the Flan-T5 model. ",
        "page_idx": 182
    },
    {
        "type": "text",
        "text": "We create a prompt that works well with the model and use it in BERTopic through the representation_model parameter: ",
        "page_idx": 182
    },
    {
        "type": "text",
        "text": "from transformers import pipeline   \nfrom bertopic.representation import TextGeneration   \nprompt $=$ \"\"\"I have a topic that contains the following documents:   \n[DOCUMENTS]   \nThe topic is described by the following keywords: '[KEYWORDS]'.   \nBased on the documents and keywords, what is this topic about?\"\"\"   \n# Update our topic representations using Flan-T5   \ngenerator $=$ pipeline(\"text2text-generation\", model \"google/flan-t5-small\") representation_model $=$ TextGeneration( ",
        "page_idx": 182
    },
    {
        "type": "text",
        "text": "generator, prompt=prompt, doc_length $\\begin{array} { r l } { | = { } } & { { } } \\end{array}$ , tokenizer $=$ \"whitespace\" ) topic_model.update_topics(abstracts, representation_model $\\cdot ^ { = }$ representation_model) # Show topic differences topic_differences(topic_model, original_topics) ",
        "page_idx": 183
    },
    {
        "type": "table",
        "img_path": "images/933d2ad64299e29520715a2da0679d6a6b6daaa621658bcaba41cc968e4d536d.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\">Topic 0riginal</td><td>Updated</td></tr><tr><td>0</td><td>speech | asr |recognition | end|acoustic</td><td> Speech-to-description</td></tr><tr><td>1</td><td>medical | clinical | biomedical | patient | he...</td><td>Science/Tech</td></tr><tr><td>2</td><td>sentiment | aspect | analysis | reviews | opinion</td><td>Review</td></tr><tr><td>3</td><td>translation | nmt |machine| neural | bleu</td><td>Attention-based neural machine translation</td></tr><tr><td>4</td><td>summarization | summaries |summary|abstract.. Summarization</td><td></td></tr></table>",
        "page_idx": 183
    },
    {
        "type": "text",
        "text": "Some of these labels, like “Summarization” seem to be logical when comparing them to the original representations. Others, however, like “Science/Tech,” seem quite broad and do not do the original topic justice. Let’s explore instead how OpenAI’s GPT-3.5 would perform considering the model is not only larger but expected to have more linguistic capabilities: ",
        "page_idx": 183
    },
    {
        "type": "text",
        "text": "import openai   \nfrom bertopic.representation import OpenAI   \nprompt = \"\"\"   \nI have a topic that contains the following documents:   \n[DOCUMENTS] ",
        "page_idx": 183
    },
    {
        "type": "text",
        "text": "The topic is described by the following keywords: [KEYWORDS] ",
        "page_idx": 183
    },
    {
        "type": "text",
        "text": "Based on the information above, extract a short topic label in the following   \nformat:   \ntopic: <short topic label>   \n# Update our topic representations using GPT-3.5   \nclient $=$ openai.OpenAI(api_key=\"YOUR_KEY_HERE\")   \nrepresentation_model $=$ OpenAI( client, model $\\ l =$ \"gpt-3.5-turbo\", exponential_backoff=True, chat=True,   \nprompt=prompt   \n)   \ntopic_model.update_topics(abstracts, representation_model $\\ l =$ representation_mode ",
        "page_idx": 183
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 183
    },
    {
        "type": "text",
        "text": "# Show topic differences topic_differences(topic_model, original_topics) ",
        "page_idx": 183
    },
    {
        "type": "table",
        "img_path": "images/ed2cbfb77926ae08e745cf572e5ddacfadde8b443c12e149df6eeaa089b5bfb8.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Topic 0riginal</td><td></td><td>Updated</td></tr><tr><td>0</td><td> speech |asr recognition |end|acoustic</td><td>Leveraging External Data for Improving Low-Res...</td></tr><tr><td>1</td><td>medical | clinical | biomedical | patient |he...</td><td>Improved Representation Learning for Biomedica...</td></tr><tr><td>2</td><td> sentiment |aspect |analysis |reviews |opinion</td><td>Advancements in Aspect-Based Sentiment Analys..</td></tr><tr><td>3</td><td>translation |nmt | machine|neural | bleu</td><td>Neural Machine Translation Enhancements</td></tr><tr><td>4</td><td>summarization|summaries | summary |abstract.. Document Summarization Techniques</td><td></td></tr></table>",
        "page_idx": 184
    },
    {
        "type": "text",
        "text": "The resulting labels are quite impressive! We are not even using GPT-4 and the resulting labels seem to be more informative than our previous example. Note that BERTopic is not confined to only using OpenAI’s offering but has local backends as well. ",
        "page_idx": 184
    },
    {
        "type": "image",
        "img_path": "images/65b686226626e0527345c0a007b3babc5b1eebecd2334c5e2a5772586a084350.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 184
    },
    {
        "type": "text",
        "text": "Although it seems like we do not need the keywords anymore, they are still representative of the input documents. No model is perfect and it is generally advised to generate multiple topic representa‐ tions. BERTopic allows for all topics to be represented by different representations. You could, for example, use KeyBERTInspired, MMR, and GPT-3.5 side by side to get different perspectives on the same topic. ",
        "page_idx": 184
    },
    {
        "type": "text",
        "text": "With these GPT-3.5 generated labels, we can create beautiful illustrations using the datamapplot package (Figure 5-24): ",
        "page_idx": 184
    },
    {
        "type": "text",
        "text": "# Visualize topics and documents   \nfig $=$ topic_model.visualize_document_datamap( titles, topics=list(range(20)), reduced_embeddings $\\mathbf { \\sigma } =$ reduced_embeddings, width=1200, label_font_size=11, label_wrap_width $\\begin{array} { r l } { | = { } } & { { } } \\end{array}$ , use_medoids=True,   \n) ",
        "page_idx": 184
    },
    {
        "type": "image",
        "img_path": "images/a319a7aaeb41ed90734be69259234e87bae8505b114e710c055514f011639df2.jpg",
        "image_caption": [
            "Figure 5-24. The top 20 topics visualized. "
        ],
        "image_footnote": [],
        "page_idx": 185
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 185
    },
    {
        "type": "text",
        "text": "In this chapter, we explored how LLMs, both generative and representative, can be used in the domain of unsupervised learning. Despite supervised methods like classification being prevalent in recent years, unsupervised approaches such as text clustering hold immense potential due to their ability to group texts based on seman‐ tic content without prior labeling. ",
        "page_idx": 185
    },
    {
        "type": "text",
        "text": "We covered a common pipeline for clustering textual documents that starts with converting input text into numerical representations, which we call embeddings. Then, dimensionality reduction is applied to these embeddings to simplify highdimensional data for better clustering outcomes. Finally, a clustering algorithm on the dimensionality-reduced embeddings is applied to cluster the input text. Manually inspecting the clusters helped us understand which documents they contained and how to interpret these clusters. ",
        "page_idx": 185
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 186
    },
    {
        "type": "text",
        "text": "To transition away from this manual inspection, we explored how BERTopic extends this text clustering pipeline with a method for automatically representing the clusters. This methodology is often referred to as topic modeling, which attempts to uncover themes within large amounts of documents. BERTopic generates these topic repre‐ sentations through a bag-of-words approach enhanced with c-TF-IDF, which weighs words based on their cluster relevance and frequency across all clusters. ",
        "page_idx": 186
    },
    {
        "type": "text",
        "text": "A major benefit of BERTopic is its modular nature. In BERTopic, you can choose any model in the pipeline, which allows for additional representations of topics that create multiple perspectives of the same topic. We explored maximal marginal rele‐ vance and KeyBERTInspired as methodologies to fine-tune the topic representations generated with c-TF-IDF. Additionally, we used the same generative LLMs as in the previous chapter (Flan-T5 and GPT-3.5) to further improve the interpretability of topics by generating highly interpretable labels. ",
        "page_idx": 186
    },
    {
        "type": "text",
        "text": "In the next chapter, we shift focus and explore a common method for improving the output of generative models, namely prompt engineering. ",
        "page_idx": 186
    },
    {
        "type": "text",
        "text": "Prompt Engineering ",
        "text_level": 1,
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "In the first chapters of this book, we took our first steps into the world of large language models (LLMs). We delved into various applications, such as supervised and unsupervised classification, employing models that focus on representing text, like BERT and its derivatives. ",
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "As we progressed, we used models trained primarily for text generation, models that are often referred to as generative pre-trained transformers (GPT). These models have the remarkable ability to generate text in response to prompts from the user. Through prompt engineering, we can design these prompts in a way that enhances the quality of the generated text. ",
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "In this chapter, we will explore these generative models in more detail and dive into the realm of prompt engineering, reasoning with generative models, verification, and even evaluating their output. ",
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "Using Text Generation Models ",
        "text_level": 1,
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "Before we start with the fundamentals of prompt engineering, it is essential to explore the basics of utilizing a text generation model. How do we select the model to use? Do we use a proprietary or open source model? How can we control the generated output? These questions will serve as our stepping stones into using text generation models. ",
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "Choosing a Text Generation Model ",
        "text_level": 1,
        "page_idx": 188
    },
    {
        "type": "text",
        "text": "Choosing a text generation model starts with choosing between proprietary models or open source models. Although proprietary models are generally more performant, we focus in this book more on open source models as they offer more flexibility and are free to use. ",
        "page_idx": 188
    },
    {
        "type": "image",
        "img_path": "images/7381ea2ca06ae0a7813becdd70154f919efe68360dbec3027feef73f3cb72034.jpg",
        "image_caption": [
            "Figure 6-1 shows a small selection of impactful foundation models, LLMs that have been pretrained on vast amounts of text data and are often fine-tuned for specific applications. ",
            "Figure 6-1. Foundation models are often released in several different sizes. "
        ],
        "image_footnote": [],
        "page_idx": 189
    },
    {
        "type": "text",
        "text": "From those foundation models, hundreds if not thousands of models have been fine-tuned, one more suitable for certain tasks than another. Choosing the model to use can be a daunting task! ",
        "page_idx": 189
    },
    {
        "type": "text",
        "text": "We advise starting with a small foundation model. So let’s continue using Phi-3-mini, which has 3.8 billion parameters. This makes it suitable for running with devices up to 8 GB of VRAM. Overall, scaling up to larger models tends to be a nicer experience than scaling down. Smaller models provide a great introduction and lay a solid foundation for progressing to larger models. ",
        "page_idx": 189
    },
    {
        "type": "text",
        "text": "Loading a Text Generation Model ",
        "text_level": 1,
        "page_idx": 189
    },
    {
        "type": "text",
        "text": "The most straightforward method of loading a model, as we have done in previous chapters, is by leveraging the transformers library: ",
        "page_idx": 189
    },
    {
        "type": "text",
        "text": "import torch   \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, pipeline   \n# Load model and tokenizer   \nmodel $=$ AutoModelForCausalLM.from_pretrained( \"microsoft/Phi-3-mini-4k-instruct\", device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True,   \n)   \ntokenizer $=$ AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\")   \n# Create a pipeline   \npipe $=$ pipeline( \"text-generation\", model=model, ",
        "page_idx": 189
    },
    {
        "type": "text",
        "text": "tokenizer $=$ tokenizer, return_full_text $=$ False, max_new_tokens $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = } \\end{array} .$ , do_sample=False, ) ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "Compared to previous chapters, we will take a closer look at developing and using the prompt template. ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "To illustrate, let’s revisit the example from Chapter 1 where we asked the LLM to make a joke about chickens: ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "# Prompt   \nmessages $=$ [ {\"role\": \"user\" \"content\": \"Create a funny joke about chickens.\"}   \n] ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "# Generate the output output $=$ pipe(messages) print(output[0][\"generated_text\"]) ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "Why don't chickens like to go to the gym? Because they can't crack the eggsistence of it! ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "Under the hood, transformers.pipeline first converts our messages into a specific prompt template. We can explore this process by accessing the underlying tokenizer: ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "# Apply prompt template   \nprompt $=$ pipe.tokenizer.apply_chat_template(messages, tokenize=False)   \nprint(prompt)   \n<s><|user|>   \nCreate a funny joke about chickens.<|end|>   \n<|assistant|> ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "You may recognize the special tokens <|user $| >$ and $<$ |assistant $| >$ from Chapter 2. This prompt template, further illustrated in Figure 6-2, was used during the training of the model. Not only does it provide information about who said what, but it is also used to indicate when the model should stop generating text (see the $< | \\mathsf { e n d } | >$ token). This prompt is passed directly to the LLM and processed all at once. ",
        "page_idx": 190
    },
    {
        "type": "text",
        "text": "In the next chapter, we will customize parts of this template ourselves. Throughout this chapter, we can use transformers.pipeline to handle chat template processing for us. Next, let us explore how we can control the output of the model. ",
        "page_idx": 190
    },
    {
        "type": "image",
        "img_path": "images/b06da65385db7c21e67da812ce8fea23bdf71ceafe8e64fdfdc789b370c379ee.jpg",
        "image_caption": [
            "Figure 6-2. The template Phi-3 expects when interacting with the model. "
        ],
        "image_footnote": [],
        "page_idx": 191
    },
    {
        "type": "text",
        "text": "Controlling Model Output ",
        "text_level": 1,
        "page_idx": 191
    },
    {
        "type": "text",
        "text": "Other than prompt engineering, we can control the kind of output we want by adjusting the model parameters. In our previous example, you might have noticed that we used several parameters in the pipe function, including temperature and top_p. ",
        "page_idx": 191
    },
    {
        "type": "text",
        "text": "These parameters control the randomness of the output. A part of what makes LLMs exciting technology is that it can generate different responses for the exact same prompt. Each time an LLM needs to generate a token, it assigns a likelihood number to each possible token. ",
        "page_idx": 191
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 6-3, in the sentence $^ { \\mathfrak { c } } \\mathrm { I }$ am driving a…” the likelihood of that sentence being followed by tokens like “car” or “truck” is generally higher than a token like “elephant.” However, there is still a possibility of “elephant” being generated but it is much lower. ",
        "page_idx": 191
    },
    {
        "type": "image",
        "img_path": "images/a41521dc37eafd73056865f9d1d392c5870300f28d3508e28348d3a93827b354.jpg",
        "image_caption": [
            "Figure 6-3. The model chooses the next token to generate based on their likelihood scores. "
        ],
        "image_footnote": [],
        "page_idx": 191
    },
    {
        "type": "text",
        "text": "When we loaded our model, we purposefully set do_sample $\\mathsf { \\Pi } = \\mathsf { F }$ alse to make sure the output is somewhat consistent. This means that no sampling will be done and only the most probable next token is selected. However, to use the temperature and top_p parameters, we will set do_sample=True in order to make use of them. ",
        "page_idx": 191
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "Temperature ",
        "text_level": 1,
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "The temperature controls the randomness or creativity of the text generated. It defines how likely it is to choose tokens that are less probable. The underlying idea is that a temperature of 0 generates the same response every time because it always chooses the most likely word. As illustrated in Figure 6-4, a higher value allows less probable words to be generated. ",
        "page_idx": 192
    },
    {
        "type": "image",
        "img_path": "images/e2efe06f62f4af368f26d7831668fd95dad3fda0682fc49d7a9919fab0f47843.jpg",
        "image_caption": [
            "Figure 6-4. A higher temperature increases the likelihood that less probable tokens are generated and vice versa. "
        ],
        "image_footnote": [],
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "As a result, a higher temperature (e.g., 0.8) generally results in a more diverse output while a lower temperature (e.g., 0.2) creates a more deterministic output. ",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "You can use temperature in your pipeline as follows: ",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "# Using a high temperature   \noutput $=$ pipe(messages, do_sample $\\ast =$ True, temperature $^ { = 1 }$ )   \nprint(output[0][\"generated_text\"]) ",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "Why don't chickens like to go on a rollercoaster? Because they're afraid they might suddenly become chicken-soup! ",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "Note that every time you rerun this piece of code, the output will change! tempera ture introduces stochastic behavior since the model now randomly selects tokens. ",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "top_p, also known as nucleus sampling, is a sampling technique that controls which subset of tokens (the nucleus) the LLM can consider. It will consider tokens until it reaches their cumulative probability. If we set top_p to 0.1, it will consider tokens until it reaches that value. If we set top_p to 1, it will consider all tokens. ",
        "page_idx": 192
    },
    {
        "type": "text",
        "text": "As shown in Figure 6-5, by lowering the value, it will consider fewer tokens and generally give less “creative” output, while increasing the value allows the LLM to choose from more tokens. ",
        "page_idx": 193
    },
    {
        "type": "image",
        "img_path": "images/71627ad212fb92280bda686652820aeed5e88d22f90f51175020bacd0725dd88.jpg",
        "image_caption": [
            "Figure 6-5. A higher top_p increases the number of tokens that can be selected to generate and vice versa. "
        ],
        "image_footnote": [],
        "page_idx": 193
    },
    {
        "type": "text",
        "text": "Similarly, the top_k parameter controls exactly how many tokens the LLM can consider. If you change its value to 100, the LLM will only consider the top 100 most probable tokens. ",
        "page_idx": 193
    },
    {
        "type": "text",
        "text": "You can use top_p in your pipeline as follows: ",
        "page_idx": 193
    },
    {
        "type": "text",
        "text": "# Using a high top_p output $=$ pipe(messages, do_sample=True, top_p $^ { 1 = 1 }$ ) print(output[0][\"generated_text\"]) ",
        "page_idx": 193
    },
    {
        "type": "text",
        "text": "Why don't chickens make good comedians? Because their 'jokes' always 'feather' the truth! ",
        "page_idx": 193
    },
    {
        "type": "text",
        "text": "As shown in Table 6-1, these parameters allow the user to have a sliding scale between being creative (high temperature and top_p) and being predictable (lower temperature and top_p). ",
        "page_idx": 193
    },
    {
        "type": "table",
        "img_path": "images/3f0c205419c9922c30619c612455b540617e6189e5fa2ec442308d7e1d95501c.jpg",
        "table_caption": [
            "Table 6-1. Use case examples when selecting values for temperature and top_p. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Example use case</td><td>ture</td><td>Tempera top_p Description</td><td></td></tr><tr><td>Brainstorming session</td><td>High</td><td>High</td><td>High randomness with large pol of potential tokens.The results will be highly diverse, often leading to very creative and unexpected results.</td></tr><tr><td>Email generation</td><td>Low</td><td>Low</td><td>Deterministic output with high probable predicted tokens.This results in predictable, focused,and conservative outputs.</td></tr><tr><td>Creative writing</td><td>High</td><td>Low</td><td>High randomness with a small pool of potential tokens.This combination produces creative outputs but still remains coherent.</td></tr><tr><td>Translation</td><td>Low</td><td>High</td><td>Deterministic output with high probable predicted tokens.Produces coherent output with a wider range of vocabulary,leading to outputs with linguistic variety.</td></tr></table>",
        "page_idx": 193
    },
    {
        "type": "text",
        "text": "Intro to Prompt Engineering ",
        "text_level": 1,
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "An essential part of working with text-generative LLMs is prompt engineering. By carefully designing our prompts we can guide the LLM to generate desired responses. Whether the prompts are questions, statements, or instructions, the main goal of prompt engineering is to elicit a useful response from the model. ",
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "Prompt engineering is more than designing effective prompts. It can be used as a tool to evaluate the output of a model as well as to design safeguards and safety mitigation methods. This is an iterative process of prompt optimization and requires experimentation. There is not and unlikely will ever be a perfect prompt design. ",
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "In this section, we will go through common methods for prompt engineering, and small tips and tricks to understand what the effect is of certain prompts. These skills allow us to understand the capabilities of LLMs and lie at the foundation of interfacing with these kinds of models. ",
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "We begin by answering the question: what should be in a prompt? ",
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "The Basic Ingredients of a Prompt ",
        "text_level": 1,
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "An LLM is a prediction machine. Based on a certain input, the prompt, it tries to predict the words that might follow it. At its core (illustrated in Figure 6-6), the prompt does not need to be more than a few words to elicit a response from the LLM. ",
        "page_idx": 194
    },
    {
        "type": "image",
        "img_path": "images/46db636099c91689cbc67eb0b310410198e0c399d6c39d1086a2ee7111776d03.jpg",
        "image_caption": [
            "Figure 6-6. A basic example of a prompt. No instruction is given so the LLM will simply try to complete the sentence. "
        ],
        "image_footnote": [],
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "However, although the illustration works as a basic example, it fails to complete a specific task. Instead, we generally approach prompt engineering by asking a specific question or task the LLM should complete. To elicit the desired response, we need a more structured prompt. ",
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "For example, and as shown in Figure 6-7, we could ask the LLM to classify a sentence into either having positive or negative sentiment. This extends the most basic prompt to one consisting of two components—the instruction itself and the data that relates to the instruction. ",
        "page_idx": 194
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 195
    },
    {
        "type": "image",
        "img_path": "images/07b8b96f26df00e19653a466f0957b630ee905b7d5852dfc46ae40c937d7aa74.jpg",
        "image_caption": [
            "Figure 6-7. Two components of a basic instruction prompt: the instruction itself and the data it refers to. "
        ],
        "image_footnote": [],
        "page_idx": 195
    },
    {
        "type": "text",
        "text": "More complex use cases might require more components in a prompt. For instance, to make sure the model only outputs “negative” or “positive” we can introduce output indicators that help guide the model. In Figure 6-8, we prefix the sentence with “Text:” and add “Sentiment:” to prevent the model from generating a complete sen‐ tence. Instead, this structure indicates that we expect either “negative” or “positive.” Although the model might not have been trained on these components directly, it was fed enough instructions to be able to generalize to this structure. ",
        "page_idx": 195
    },
    {
        "type": "image",
        "img_path": "images/0955c18da2f57b01eba1487d7cdcb78f3bd805b787470eab3ee388e1ac6b63cc.jpg",
        "image_caption": [
            "Figure 6-8. Extending the prompt with an output indicator that allows for a specific output. "
        ],
        "image_footnote": [],
        "page_idx": 195
    },
    {
        "type": "text",
        "text": "We can continue adding or updating the elements of a prompt until we elicit the response we are looking for. We could add additional examples, describe the use case in more detail, provide additional context, etc. These components are merely exam‐ ples and not a limited set of possibilities. The creativity that comes with designing these components is key. ",
        "page_idx": 196
    },
    {
        "type": "text",
        "text": "Although a prompt is a single piece of text, it is tremendously helpful to think of prompts as pieces of a larger puzzle. Have I described the context of my question? Does the prompt have an example of the output? ",
        "page_idx": 196
    },
    {
        "type": "text",
        "text": "Instruction-Based Prompting ",
        "text_level": 1,
        "page_idx": 196
    },
    {
        "type": "text",
        "text": "Although prompting comes in many flavors, from discussing philosophy with the LLM to role-playing with your favorite superhero, prompting is often used to have the LLM answer a specific question or resolve a certain task. This is referred to as instruction-based prompting. ",
        "page_idx": 196
    },
    {
        "type": "text",
        "text": "Figure 6-9 illustrates a number of use cases in which instruction-based prompting plays an important role. We already did one of these in the previous example, namely supervised classification. ",
        "page_idx": 196
    },
    {
        "type": "image",
        "img_path": "images/b104fd3874ad04b5e382133e3588a92af735fd962f9d026d7bece1d4de3a4a96.jpg",
        "image_caption": [
            "Figure 6-9. Use cases for instruction-based prompting. "
        ],
        "image_footnote": [],
        "page_idx": 196
    },
    {
        "type": "text",
        "text": "Each of these tasks requires different prompting formats and more specifically, asking different questions of the LLM. Asking the LLM to summarize a piece of text will not suddenly result in classification. To illustrate, examples of prompts for some of these use cases can be found in Figure 6-10. ",
        "page_idx": 196
    },
    {
        "type": "image",
        "img_path": "images/c19a7e44985bf2dd57564e430035276d3cc87a7025ec2370efa0c3bd65b8791e.jpg",
        "image_caption": [
            "Figure 6-10. Prompt examples of common use cases. Notice how within a use case, the structure and location of the instruction can be changed. "
        ],
        "image_footnote": [],
        "page_idx": 197
    },
    {
        "type": "text",
        "text": "Although these tasks require different instructions, there is actually a lot of overlap in the prompting techniques used to improve the quality of the output. A nonexhaustive list of these techniques includes: ",
        "page_idx": 197
    },
    {
        "type": "text",
        "text": "Specificity ",
        "page_idx": 197
    },
    {
        "type": "text",
        "text": "Accurately describe what you want to achieve. Instead of asking the LLM to “Write a description for a product” ask it to “Write a description for a product in less than two sentences and use a formal tone.” ",
        "page_idx": 197
    },
    {
        "type": "text",
        "text": "Hallucination ",
        "text_level": 1,
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "LLMs may generate incorrect information confidently, which is referred to as hallucination. To reduce its impact, we can ask the LLM to only generate an answer if it knows the answer. If it does not know the answer, it can respond with “I don’t know.” ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "Order ",
        "text_level": 1,
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "Either begin or end your prompt with the instruction. Especially with long prompts, information in the middle is often forgotten.1 LLMs tend to focus on information either at the beginning of a prompt (primacy effect) or the end of a prompt (recency effect). ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "Here, specificity is arguably the most important aspect. By restricting and specifying what the model should generate, there is a smaller chance of having it generate something not related to your use case. For instance, if we were to skip the instruc‐ tion “in two to three sentences” it might generate complete paragraphs. Like human conversations, without any specific instructions or additional context, it is difficult to derive what the task at hand actually is. ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "Advanced Prompt Engineering ",
        "text_level": 1,
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "On the surface, creating a good prompt might seem straightforward. Ask a specific question, be accurate, add some examples, and you are done! However, prompting can grow complex quite quickly and as a result is an often-underestimated compo‐ nent of leveraging LLMs. ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "Here, we will go through several advanced techniques for building up your prompts, starting with the iterative workflow of building up complex prompts all the way to using LLMs sequentially to get improved results. Eventually, we will even build up to advanced reasoning techniques. ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "The Potential Complexity of a Prompt ",
        "text_level": 1,
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "As we explored in the intro to prompt engineering, a prompt generally consists of multiple components. In our very first example, our prompt consisted of instruction, data, and output indicators. As we mentioned before, no prompt is limited to just these three components and you can build it up to be as complex as you want. ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "These advanced components can quickly make a prompt quite complex. Some com‐ mon components are: ",
        "page_idx": 198
    },
    {
        "type": "text",
        "text": "Persona ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Describe what role the LLM should take on. For example, use “You are an expert in astrophysics” if you want to ask a question about astrophysics. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Instruction ",
        "text_level": 1,
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "The task itself. Make sure this is as specific as possible. We do not want to leave much room for interpretation. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Context ",
        "text_level": 1,
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Additional information describing the context of the problem or task. It answers questions like “What is the reason for the instruction?” ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Format ",
        "text_level": 1,
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "The format the LLM should use to output the generated text. Without it, the LLM will come up with a format itself, which is troublesome in automated systems. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Audience ",
        "text_level": 1,
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "The target of the generated text. This also describes the level of the generated output. For education purposes, it is often helpful to use ELI5 (“Explain it like $\\Gamma { \\bf m } 5 ^ { \\mathrm { \\scriptscriptstyle ~ . } }$ ). ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Tone ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "The tone of voice the LLM should use in the generated text. If you are writing a formal email to your boss, you might not want to use an informal tone of voice. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "Data ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "The main data related to the task itself. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "To illustrate, let us extend the classification prompt we had earlier and use all of the preceding components. This is demonstrated in Figure 6-11. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "This complex prompt demonstrates the modular nature of prompting. We can add and remove components freely and judge their effect on the output. As illustrated in Figure 6-12, we can slowly build up our prompt and explore the effect of each change. ",
        "page_idx": 199
    },
    {
        "type": "text",
        "text": "The changes are not limited to simply introducing or removing components. Their order, as we saw before with the recency and primacy effects, can affect the quality of the LLM’s output. In other words, experimentation is vital when finding the best prompt for your use case. With prompting, we essentially have ourselves in an iterative cycle of experimentation. ",
        "page_idx": 199
    },
    {
        "type": "image",
        "img_path": "images/5c830f81a10aae883ca8b200b50873b6cc9e310a0c4e34819516969841487190.jpg",
        "image_caption": [
            "Figure 6-11. An example of a complex prompt with many components. "
        ],
        "image_footnote": [],
        "page_idx": 200
    },
    {
        "type": "image",
        "img_path": "images/31f73f200831cb3c38871f316a3af52762984dcc40bf6cdb0768027e1bae5d24.jpg",
        "image_caption": [
            "Figure 6-12. Iterating over modular components is a vital part of prompt engineering. "
        ],
        "image_footnote": [],
        "page_idx": 200
    },
    {
        "type": "text",
        "text": "Try it out yourself! Use the complex prompt to add and/or remove parts to observe its impact on the generated output. You will quickly notice when pieces of the puzzle are worth keeping. You can use your own data by adding it to the data variable: ",
        "page_idx": 200
    },
    {
        "type": "text",
        "text": "# Prompt components ",
        "text_level": 1,
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "persona $=$ \"You are an expert in Large Language models. You excel at breaking   \ndown complex papers into digestible summaries.\\n\"   \ninstruction $=$ \"Summarize the key findings of the paper provided.\\n\"   \ncontext $=$ \"Your summary should extract the most crucial points that can help   \nresearchers quickly understand the most vital information of the paper.\\n\"   \ndata_format $=$ \"Create a bullet-point summary that outlines the method. Follow   \nthis up with a concise paragraph that encapsulates the main results.\\n\"   \naudience $=$ \"The summary is designed for busy researchers that quickly need to   \ngrasp the newest trends in Large Language Models.\\n\"   \ntone $=$ \"The tone should be professional and clear.\\n\"   \ntext $=$ \"MY TEXT TO SUMMARIZE\"   \ndata $=$ f\"Text to summarize: {text}\" ",
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "# The full prompt - remove and add pieces to view its impact on the generated output query $=$ persona $^ +$ instruction $^ +$ context $^ +$ data_format $^ +$ audience $^ +$ tone $^ +$ data ",
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "There is all manner of components that we could add and creative components like using emotional stimuli (e.g., “This is very impor‐ tant for my career.”2). Part of the fun in prompt engineering is that you can be as creative as possible to figure out which combination of prompt components contribute to your use case. There are few constraints to developing a format that works for you. ",
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "In a way, it is an attempt to reverse engineer what the model has learned and how it responds to certain prompts. However, note that some prompts work better for certain models compared to others as their training data might be different or they are trained for different purposes. ",
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "In-Context Learning: Providing Examples ",
        "text_level": 1,
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "In the previous sections, we tried to accurately describe what the LLM should do. Although accurate and specific descriptions help the LLM to understand the use case, we can go one step further. Instead of describing the task, why do we not just show the task? ",
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "We can provide the LLM with examples of exactly the thing that we want to achieve. This is often referred to as in-context learning, where we provide the model with correct examples.3 ",
        "page_idx": 201
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 6-13, this comes in a number of forms depending on how many examples you show the LLM. Zero-shot prompting does not leverage examples, one-shot prompts use a single example, and few-shot prompts use two or more examples. ",
        "page_idx": 202
    },
    {
        "type": "image",
        "img_path": "images/1df0008f70bb07b20d199b41e7e1823bc613c3291bdc0444b6264d52f9402762.jpg",
        "image_caption": [
            "Figure 6-13. An example of a complex prompt with many components. "
        ],
        "image_footnote": [],
        "page_idx": 202
    },
    {
        "type": "text",
        "text": "Adopting the original phrase, we believe that “an example is worth a thousand words.” These examples provide a direct example of what and how the LLM should achieve. ",
        "page_idx": 202
    },
    {
        "type": "text",
        "text": "We can illustrate this method with a simple example taken from the original paper describing this method.4 The goal of the prompt is to generate a sentence with a made-up word. To improve the quality of the resulting sentence, we can show the generative model an example of what a proper sentence with a made-up word would be. ",
        "page_idx": 202
    },
    {
        "type": "text",
        "text": "To do so, we will need to differentiate between our question (user) and the answers that were provided by the model (assistant). We additionally showcase how this interaction is processed using the template: ",
        "page_idx": 202
    },
    {
        "type": "text",
        "text": "# Use a single example of using the made-up word in a sentence   \none_shot_prompt $=$ [ { \"role\": \"user\", \"content\": \"A 'Gigamuru' is a type of Japanese musical instrument. An   \nexample of a sentence that uses the word Gigamuru is:\" ",
        "page_idx": 202
    },
    {
        "type": "text",
        "text": "},{ \"role\": \"assistant\", \"content\": \"I have a Gigamuru that my uncle gave me as a gift. I love to play it at home.\" },{ \"role\": \"user\" \"content\": \"To 'screeg' something is to swing a sword at it. An example of a sentence that uses the word screeg is:\" } ] print(tokenizer.apply_chat_template(one_shot_prompt, tokenize $=$ False)) ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "<s><|user|>   \nA 'Gigamuru' is a type of Japanese musical instrument. An example of a sen  \ntence that uses the word Gigamuru is:<|end|>   \n<|assistant|>   \nI have a Gigamuru that my uncle gave me as a gift. I love to play it at home.<|   \nend|>   \n<|user|>   \nTo 'screeg' something is to swing a sword at it. An example of a sentence that   \nuses the word screeg is:<|end|>   \n<|assistant|> ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "The prompt illustrates the need to differentiate between the user and the assistant. If we did not, it would seem as if we were talking to ourselves. Using these interactions, we can generate output as follows: ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "# Generate the output outputs $=$ pipe(one_shot_prompt) print(outputs[0][\"generated_text\"]) ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "During the intense duel, the knight skillfully screeged his opponent's shield, forcing him to defend himself. ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "It correctly generated the answer! ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "As with all prompt components, one- or few-shot prompting is not the be all and end all of prompt engineering. We can use it as one piece of the puzzle to further enhance the descriptions that we gave it. The model can still “choose,” through random sampling, to ignore the instructions. ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "Chain Prompting: Breaking up the Problem ",
        "text_level": 1,
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "In previous examples, we explored splitting up prompts into modular components to improve the performance of LLMs. Although this works well for many use cases, this might not be feasible for highly complex prompts or use cases. ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "Instead of breaking the problem within a prompt, we can do so between prompts. Essentially, we take the output of one prompt and use it as input for the next, thereby creating a continuous chain of interactions that solves our problem. ",
        "page_idx": 203
    },
    {
        "type": "text",
        "text": "To illustrate, let us say we want to use an LLM to create a product name, slogan, and sales pitch for us based on a number of product features. Although we can ask the LLM to do this in one go, we can instead break up the problem into pieces. ",
        "page_idx": 204
    },
    {
        "type": "text",
        "text": "As a result, and as illustrated in Figure 6-14, we get a sequential pipeline that first creates the product name, uses that with the product features as input to create the slogan, and finally, uses the features, product name, and slogan to create the sales pitch. ",
        "page_idx": 204
    },
    {
        "type": "image",
        "img_path": "images/20103bd01ed933ddde7d9f96541f8eaaaed5bc94a073f662d54716bc87a1682b.jpg",
        "image_caption": [
            "Figure 6-14. Using a description of a product’s features, chain prompts to create a suitable name, slogan, and sales pitch. "
        ],
        "image_footnote": [],
        "page_idx": 204
    },
    {
        "type": "text",
        "text": "This technique of chaining prompts allows the LLM to spend more time on each individual question instead of tackling the whole problem. Let us illustrate this with a small example. We first create a name and slogan for a chatbot: ",
        "page_idx": 204
    },
    {
        "type": "text",
        "text": "# Create name and slogan for a product   \nproduct_prompt $= [$ {\"role\": \"user\", \"content\": \"Create a name and slogan for a chatbot that   \nleverages LLMs.\"}   \n]   \noutputs $=$ pipe(product_prompt)   \nproduct_description $=$ outputs[0][\"generated_text\"]   \nprint(product_description) ",
        "page_idx": 204
    },
    {
        "type": "text",
        "text": "Name: 'MindMeld Messenger' ",
        "page_idx": 204
    },
    {
        "type": "text",
        "text": "Slogan: 'Unleashing Intelligent Conversations, One Response at a Time' ",
        "page_idx": 204
    },
    {
        "type": "text",
        "text": "Then, we can use the generated output as input for the LLM to generate a sales pitch: ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "# Based on a name and slogan for a product, generate a sales pitch   \nsales_prompt $= [$ {\"role\": \"user\", \"content\": f\"Generate a very short sales pitch for the   \nfollowing product: '{product_description}'\"}   \n]   \noutputs $=$ pipe(sales_prompt)   \nsales_pitch $=$ outputs[0][\"generated_text\"]   \nprint(sales_pitch) ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Introducing MindMeld Messenger - your ultimate communication partner! Unleash intelligent conversations with our innovative AI-powered messaging platform. With MindMeld Messenger, every response is thoughtful, personalized, and timely. Say goodbye to generic replies and hello to meaningful interactions. Elevate your communication game with MindMeld Messenger - where every message is a step toward smarter conversations. Try it now and experience the future of messaging! ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Although we need two calls to the model, a major benefit is that we can give each call different parameters. For instance, the number of tokens created was relatively small for the name and slogan whereas the pitch can be much longer. ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "This can be used for a variety of use cases, including: ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Response validation ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Ask the LLM to double-check previously generated outputs. ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Parallel prompts ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Create multiple prompts in parallel and do a final pass to merge them. For example, ask multiple LLMs to generate multiple recipes in parallel and use the combined result to create a shopping list. ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Writing stories ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Leverage the LLM to write books or stories by breaking down the problem into components. For example, by first writing a summary, developing characters, and building the story beats before diving into creating the dialogue. ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "In the next chapter, we will automate this process and go beyond chaining LLMs. We will chain other pieces of technology together, like memory, tool use, and more! Before that, this idea of prompt chaining will be explored further in the following sec‐ tions, which describe more complex prompt chaining methods like self-consistency, chain-of-thought, and tree-of-thought. ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "Reasoning with Generative Models ",
        "text_level": 1,
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "In the previous sections, we focused mostly on the modular component of prompts, building them up through iteration. These advanced prompt engineering techniques, like prompt chaining, proved to be the first step toward enabling complex reasoning with generative models. ",
        "page_idx": 205
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "Reasoning is a core component of human intelligence and is often compared to the emergent behavior of LLMs that often resembles reasoning. We highlight “resemble” as these models, at the time of writing, are generally considered to demonstrate this behavior through memorization of training data and pattern matching. ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "The output that they showcase, however, can demonstrate complex behavior and although it might not be “true” reasoning, they are still referred to as reasoning capa‐ bilities. In other words, we work together with the LLM through prompt engineering so we can mimic reasoning processes in order to improve the output of the LLM. ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "To allow for this reasoning behavior, it is a good moment to step back and explore what reasoning entails in human behavior. To simplify, our methods of reasoning can be divided into system 1 and 2 thinking processes. ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "System 1 thinking represents an automatic, intuitive, and near-instantaneous pro‐ cess. It shares similarities with generative models that automatically generate tokens without any self-reflective behavior. In contrast, system 2 thinking is a conscious, slow, and logical process, akin to brainstorming and self-reflection.5 ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "If we could give a generative model the ability to mimic a form of self-reflection, we would essentially be emulating the system 2 way of thinking, which tends to produce more thoughtful responses than system 1 thinking. In this section, we will explore several techniques that attempt to mimic these kinds of thought processes of human reasoners with the aim of improving the output of the model. ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "Chain-of-Thought: Think Before Answering ",
        "text_level": 1,
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "The first and major step toward complex reasoning in generative models was through a method called chain-of-thought. Chain-of-thought aims to have the gen‐ erative model “think” first rather than answering the question directly without any reasoning.6 ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 6-15, it provides examples in a prompt that demonstrate the reasoning the model should do before generating its response. These reasoning processes are referred to as “thoughts.” This helps tremendously for tasks that involve a higher degree of complexity, like mathematical questions. Adding this reasoning step allows the model to distribute more compute over the reasoning process. Instead of calculating the entire solution based on a few tokens, each additional token in this reasoning process allows the LLM to stabilize its output. ",
        "page_idx": 206
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 207
    },
    {
        "type": "image",
        "img_path": "images/55bc92a9b27a22acda65213a70cc4dfda365615ec205b6945f984b58820d8614.jpg",
        "image_caption": [
            "Figure 6-15. Chain-of-thought prompting uses reasoning examples to persuade the gen‐ erative model to use reasoning in its answer. "
        ],
        "image_footnote": [],
        "page_idx": 207
    },
    {
        "type": "text",
        "text": "We use the example the authors used in their paper to demonstrate this phenomenon: ",
        "page_idx": 207
    },
    {
        "type": "text",
        "text": "# Answering with chain-of-thought   \ncot_prompt $=$ [ {\"role\": \"user\", \"content\": \"Roger has 5 tennis balls. He buys 2 more cans   \nof tennis balls. Each can has 3 tennis balls. How many tennis balls does he   \nhave now?\"}, {\"role\": \"assistant\", \"content\": \"Roger started with 5 balls. 2 cans of 3   \ntennis balls each is 6 tennis balls. $5 ~ + ~ 6 ~ = ~ 1 1$ . The answer is 11.\"}, {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20   \nto make lunch and bought 6 more, how many apples do they have?\"}   \n] ",
        "page_idx": 207
    },
    {
        "type": "text",
        "text": "# Generate the output outputs $=$ pipe(cot_prompt) print(outputs[0][\"generated_text\"]) ",
        "page_idx": 207
    },
    {
        "type": "text",
        "text": "The cafeteria started with 23 apples. They used 20 apples, so they had 23 - 20 $= 3$ apples left. Then they bought 6 more apples, so they now have $3 ~ + ~ 6 ~ = ~ 9$ apples. The answer is 9. ",
        "page_idx": 207
    },
    {
        "type": "text",
        "text": "Note how the model doesn’t generate only the answer but provides an explanation before doing so. By doing so, it can leverage the knowledge it has generated thus far to compute the final answer. ",
        "page_idx": 208
    },
    {
        "type": "text",
        "text": "Although chain-of-thought is a great method for enhancing the output of a genera‐ tive model, it does require one or more examples of reasoning in the prompt, which the user might not have access to. Instead of providing examples, we can simply ask the generative model to provide the reasoning (zero-shot chain-of-thought). There are many different forms that work but a common and effective method is to use the phrase “Let’s think step-by-step,” which is illustrated in Figure 6-16.7 ",
        "page_idx": 208
    },
    {
        "type": "image",
        "img_path": "images/d820b8d101cde759e9b9facbc03e15f17f5fc243781898f4f4952bc18bd34dea.jpg",
        "image_caption": [
            "Figure 6-16. Chain-of-thought prompting without using examples. Instead, it uses the phrase “Let’s think step-by-step” to prime reasoning in its answer. "
        ],
        "image_footnote": [],
        "page_idx": 208
    },
    {
        "type": "text",
        "text": "Using the example we used before, we can simply append that phrase to the prompt to enable chain-of-thought-like reasoning: ",
        "page_idx": 208
    },
    {
        "type": "text",
        "text": "# Zero-shot chain-of-thought   \nzeroshot_cot_prompt $=$ [ {\"role\": \"user\", \"content\": \"The cafeteria had 23 apples. If they used 20   \nto make lunch and bought 6 more, how many apples do they have? Let's think   \nstep-by-step.\"}   \n] ",
        "page_idx": 208
    },
    {
        "type": "text",
        "text": "# Generate the output outputs $=$ pipe(zeroshot_cot_prompt) print(outputs[0][\"generated_text\"]) ",
        "page_idx": 208
    },
    {
        "type": "text",
        "text": "Step 1: Start with the initial number of apples, which is 23. Step 2: Subtract the number of apples used to make lunch, which is 20. So, 23 - $2 \\Theta = 3$ apples remaining. Step 3: Add the number of apples bought, which is 6. So, $3 + 6 = 9$ apples. ",
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "The cafeteria now has 9 apples. ",
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "Without needing to provide examples, we again got the same reasoning behavior. This is why it is so important to “show your work” when doing calculations. By addressing the reasoning process the LLM can use the previously generated informa‐ tion as a guide through generating the final answer. ",
        "page_idx": 209
    },
    {
        "type": "image",
        "img_path": "images/27c942dbd868306f7671a81fa28c07688c39ad4f1338b23b179fd7f2642c7cad.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "Although the prompt “Let’s think step by step” can improve the output, you are not constrained by this exact formulation. Alterna‐ tives exist like “Take a deep breath and think step-by-step” and “Let’s work through this problem step-by-step.”8 ",
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "Self-Consistency: Sampling Outputs ",
        "text_level": 1,
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "Using the same prompt multiple times can lead to different results if we allow for a degree of creativity through parameters like temperature and top_p. As a result, the quality of the output might improve or degrade depending on the random selection of tokens. In other words, luck! ",
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "To counteract this degree of randomness and improve the performance of generative models, self-consistency was introduced. This method asks the generative model the same prompt multiple times and takes the majority result as the final answer.9 During this process, each answer can be affected by different temperature and top_p values to increase the diversity of sampling. ",
        "page_idx": 209
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 6-17, this method can further be improved by adding chainof-thought prompting to improve its reasoning while only using the answer for the voting procedure. ",
        "page_idx": 209
    },
    {
        "type": "image",
        "img_path": "images/ece89d8bb15c0e598ae76c65a2ab120244bfd8b4cd1a129299e436cad6458463.jpg",
        "image_caption": [
            "Figure 6-17. By sampling from multiple reasoning paths, we can use majority voting to extract the most likely answer. "
        ],
        "image_footnote": [],
        "page_idx": 210
    },
    {
        "type": "text",
        "text": "However, this does require a single question to be asked multiple times. As a result, although the method can improve performance, it becomes $n$ times slower where $n$ is the number of output samples. ",
        "page_idx": 210
    },
    {
        "type": "text",
        "text": "Tree-of-Thought: Exploring Intermediate Steps ",
        "text_level": 1,
        "page_idx": 210
    },
    {
        "type": "text",
        "text": "The ideas of chain-of-thought and self-consistency are meant to enable more com‐ plex reasoning. By sampling from multiple “thoughts” and making them more thoughtful, we aim to improve the output of generative models. ",
        "page_idx": 210
    },
    {
        "type": "text",
        "text": "These techniques only scratch the surface of what is currently being done to mimic complex reasoning. An improvement to these approaches can be found in tree-ofthought, which allows for an in-depth exploration of several ideas. ",
        "page_idx": 210
    },
    {
        "type": "text",
        "text": "The method works as follows. When faced with a problem that requires multiple reasoning steps, it often helps to break it down into pieces. At each step, and as illustrated in Figure 6-18, the generative model is prompted to explore different solutions to the problem at hand. It then votes for the best solution and continues to the next step.10 ",
        "page_idx": 210
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 211
    },
    {
        "type": "image",
        "img_path": "images/074f0be04329c403933b838666619b7969f63d5f4c1633bf764d58c19218274e.jpg",
        "image_caption": [
            "Figure 6-18. By leveraging a tree-based structure, generative models can generate inter‐ mediate thoughts to be rated. The most promising thoughts are kept and the lowest are pruned. "
        ],
        "image_footnote": [],
        "page_idx": 211
    },
    {
        "type": "text",
        "text": "This method is tremendously helpful when needing to consider multiple paths, like when writing a story or coming up with creative ideas. A disadvantage of this method is that it requires many calls to the generative models, which slows the application significantly. Fortunately, there has been a successful attempt to convert the tree-ofthought framework into a simple prompting technique.11 ",
        "page_idx": 211
    },
    {
        "type": "text",
        "text": "Instead of calling the generative model multiple times, we ask the model to mimic that behavior by emulating a conversation between multiple experts. These experts will question each other until they reach a consensus. An example of a tree-ofthought prompt is: ",
        "page_idx": 211
    },
    {
        "type": "text",
        "text": "# Zero-shot tree-of-thought zeroshot_tot_prompt $=$ [ ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "{\"role\": \"user\", \"content\": \"Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all experts will go on to the next step, etc. If any expert realizes they're wrong at any point then they leave. The question is 'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, how many apples do they have?' Make sure to discuss the results.\"} ] ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "We can use this prompt to explore how an LLM might respond to complex questions: ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "# Generate the output outputs $=$ pipe(zeroshot_tot_prompt) print(outputs[0][\"generated_text\"]) ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Expert 1: Step 1 - Start with the initial number of apples: 23 apples. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Expert 2: Step 1 - Subtract the apples used for lunch: $2 3 \\ - \\ 2 0 \\ = \\ 3$ apples remaining. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Expert 3: Step 1 - Add the newly bought apples: $3 + 6 = 9$ apples. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Expert 1: Step 2 - Confirm the final count: The cafeteria has 9 apples. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Expert 2: Step 2 - Review the calculations: 23 - $2 \\Theta = 3$ , then $3 + 6 = 9$ . The calculations are correct. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Expert 3: Step 2 - Agree with the result: The cafeteria indeed has 9 apples. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "All experts agree on the final count: The cafeteria has 9 apples. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "We again get the correct answer but instead through a “discussion between experts.” It is interesting to see such a conservation between “experts” that demonstrates the creativity that comes with prompt engineering. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Output Verification ",
        "text_level": 1,
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Systems and applications built with generative models might eventually end up in production. When that happens, it is important that we verify and control the output of the model to prevent breaking the application and to create a robust generative AI application. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Reasons for validating the output might include: ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Structured output ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "By default, most generative models create free-form text without adhering to specific structures other than those defined by natural language. Some use cases require their output to be structured in certain formats, like JSON. ",
        "page_idx": 212
    },
    {
        "type": "text",
        "text": "Valid output ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Even if we allow the model to generate structured output, it still has the capability to freely generate its content. For instance, when a model is asked to output either one of two choices, it should not come up with a third. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Ethics ",
        "text_level": 1,
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Some open source generative models have no guardrails and will generate out‐ puts that do not consider safety or ethical considerations. For instance, use cases might require the output to be free of profanity, personally identifiable information (PII), bias, cultural stereotypes, etc. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Accuracy ",
        "text_level": 1,
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Many use cases require the output to adhere to certain standards or performance. The aim is to double-check whether the generated information is factually accu‐ rate, coherent, or free from hallucination. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Controlling the output of a generative model, as we explored with parameters like top_p and temperature, is not an easy feat. These models require help to generate consistent output conforming to certain guidelines. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Generally, there are three ways of controlling the output of a generative model: ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Examples ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Provide a number of examples of the expected output. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Grammar ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Control the token selection process. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Fine-tuning ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Tune a model on data that contains the expected output. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "In this section, we will go through the first two methods. The third, fine-tuning a model, is left for Chapter 12 where we will go in depth into fine-tuning methods. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "Providing Examples ",
        "text_level": 1,
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "A simple and straightforward method to fix the output is to provide the generative model with examples of what the output should look like. As we explored before, few-shot learning is a helpful technique that guides the output of the generative model. This method can be generalized to guide the structure of the output as well. ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "For example, let us consider an example where we want the generative model to create a character profile for an RPG game. We start by using no examples: ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "# Zero-shot learning: Providing no examples   \nzeroshot_prompt $=$ [ {\"role\": \"user\", \"content\": \"Create a character profile for an RPG game in   \nJSON format.\"} ",
        "page_idx": 213
    },
    {
        "type": "text",
        "text": "# Generate the output outputs $=$ pipe(zeroshot_prompt) print(outputs[0][\"generated_text\"]) ",
        "page_idx": 214
    },
    {
        "type": "text",
        "text": "\\`\\`\\`json   \n{   \n\"characterProfile\": { \"name\": \"Eldrin Stormbringer\", \"class\": \"Warlock\", \"race\": \"Half-Elf\", \"age\": 27, \"gender\": \"Male\", \"alignment\": \"Chaotic Good\", \"background\": \"Rogue\", }, \"attributes\": { \"strength\": 10, \"dexterity\": 17, \"constitution\": 12, \"intelligence\": 12, \"wisdom\": 10, \"charisma ",
        "page_idx": 214
    },
    {
        "type": "text",
        "text": "The preceding truncated output is not valid JSON since the model stopped gener‐ ating tokens after starting the “charisma” attribute. Moreover, we might not want certain attributes. Instead, we can provide the model with a number of examples that indicate the expected format: ",
        "page_idx": 214
    },
    {
        "type": "text",
        "text": "# One-shot learning: Providing an example of the output structure   \none_shot_template $=$ \"\"\"Create a short character profile for an RPG game. Make   \nsure to only use this format:   \n{ \"description\": \"A SHORT DESCRIPTION\", \"name\": \"THE CHARACTER'S NAME\", \"armor\": \"ONE PIECE OF ARMOR\", \"weapon\": \"ONE OR MORE WEAPONS\"   \n}   \n\"\"\"   \none_shot_prompt $=$ [ {\"role\": \"user\", \"content\": one_shot_template}   \n] ",
        "page_idx": 214
    },
    {
        "type": "text",
        "text": "# Generate the output outputs $=$ pipe(one_shot_prompt) print(outputs[0][\"generated_text\"]) ",
        "page_idx": 214
    },
    {
        "type": "text",
        "text": "The model perfectly followed the example we gave it, which allows for more consis‐ tent behavior. This also demonstrates the importance of leveraging few-shot learning to improve the structure of the output and not only its content. ",
        "page_idx": 215
    },
    {
        "type": "text",
        "text": "An important note here is that it is still up to the model whether it will adhere to your suggested format or not. Some models are better than others at following instructions. ",
        "page_idx": 215
    },
    {
        "type": "text",
        "text": "Grammar: Constrained Sampling ",
        "text_level": 1,
        "page_idx": 215
    },
    {
        "type": "text",
        "text": "Few-shot learning has a big disadvantage: we cannot explicitly prevent certain output from being generated. Although we guide the model and give it instructions, it might still not follow it entirely. ",
        "page_idx": 215
    },
    {
        "type": "text",
        "text": "Instead, packages have been rapidly developed to constrain and validate the output of generative models, like Guidance, Guardrails, and LMQL. In part, they leverage generative models to validate their own output, as illustrated in Figure 6-19. The generative models retrieve the output as new prompts and attempt to validate it based on a number of predefined guardrails. ",
        "page_idx": 215
    },
    {
        "type": "image",
        "img_path": "images/623acde3e0490376e47a1e1f2a48a213588cf517ef46c744abfd0248d52c8102.jpg",
        "image_caption": [
            "Figure 6-19. Use an LLM to check whether the output correctly follows our rules. "
        ],
        "image_footnote": [],
        "page_idx": 215
    },
    {
        "type": "text",
        "text": "Similarly, as illustrated in Figure 6-20, this validation process can also be used to control the formatting of the output by generating parts of its format ourselves as we already know how it should be structured. ",
        "page_idx": 215
    },
    {
        "type": "image",
        "img_path": "images/923584a2aa89b99f0ddf335f8c0de8f2ba5fd3aedcc73758a3b47c9db203ede7.jpg",
        "image_caption": [
            "Figure 6-20. Use an LLM to generate only the pieces of information we do not know beforehand. "
        ],
        "image_footnote": [],
        "page_idx": 216
    },
    {
        "type": "text",
        "text": "This process can be taken one step further and instead of validating the output we can already perform validation during the token sampling process. When sampling tokens, we can define a number of grammars or rules that the LLM should adhere to when choosing its next token. For instance, if we ask the model to either return “positive,” “negative,” or “neutral” when performing sentiment classification, it might still return something else. As illustrated in Figure 6-21, by constraining the sampling process, we can have the LLM only output what we are interested in. Note that this is still affected by parameters such as top_p and temperature. ",
        "page_idx": 216
    },
    {
        "type": "image",
        "img_path": "images/2b4b32c2720ee40f380ce3a7d26c3e475e3aa5aedf671e408704877d5ba7caf3.jpg",
        "image_caption": [
            "Figure 6-21. Constrain the token selection to only three possible tokens: “positive,” “neutral,” and “negative.” "
        ],
        "image_footnote": [],
        "page_idx": 216
    },
    {
        "type": "text",
        "text": "Let us illustrate this phenomenon with llama-cpp-python, a library similar to trans formers that we can use to load in our language model. It is generally used to efficiently load and use compressed models (through quantization; see Chapter 12) but we can also use it to apply a JSON grammar. ",
        "page_idx": 216
    },
    {
        "type": "text",
        "text": "We load the same model we used throughout this chapter but use a different format instead, namely GGUF. llama-cpp-python expects this format, which is generally used for compressed (quantized) models. ",
        "page_idx": 216
    },
    {
        "type": "text",
        "text": "Since we are loading a new model, it is advised to restart the notebook. That will clear any previous models and empty the VRAM. You can also run the following to empty the VRAM: ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "import gc   \nimport torch   \ndel model, tokenizer, pipe   \n# Flush memory   \ngc.collect()   \ntorch.cuda.empty_cache() ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "Now that we have cleared the memory, we can load Phi-3. We set n_gpu_layers to -1 to indicate that we want all layers of the model to be run from the GPU. The n_ctx refers to the context size of the model. The repo_id and filename refer to the Hugging Face repository where the model resides: ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "from llama_cpp.llama import Llama ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "# Load Phi-3   \nllm $=$ Llama.from_pretrained( repo_id=\"microsoft/Phi-3-mini-4k-instruct-gguf\", filename=\"\\*fp16.gguf\", n_gpu_layers $\\ n = -$ , n_ctx $\\varXi$ 2048, verbose=False   \n) ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "To generate the output using the internal JSON grammar, we only need to specify the response_format as a JSON object. Under the hood, it will apply a JSON grammar to make sure the output adheres to that format. ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "To illustrate, let’s ask the model to create an RPG character in JSON format to be used in a Dungeons & Dragons session: ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "# Generate output   \noutput $=$ llm.create_chat_completion( messages=[ {\"role\": \"user\", \"content\": \"Create a warrior for an RPG in JSON for   \nmat.\"}, ], response_format={\"type\": \"json_object\"}, temperature $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ ,   \n)['choices'][0]['message'][\"content\"] ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "To check whether the output actually is JSON, we can attempt to process it as such: ",
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "import json ",
        "text_level": 1,
        "page_idx": 217
    },
    {
        "type": "text",
        "text": "# Format as json   \njson_output $=$ json.dumps(json.loads(output), indent ${ = } 4$ )   \nprint(json_output) war-torn land. Witnessing the brutality and suffering caused by conflict, he dedicated his life to becoming a formidable warrior who could protect those unable to defend themselves.\"   \n} ",
        "page_idx": 217
    },
    {
        "type": "image",
        "img_path": "images/983db9a812b9c4dd9211445d5cbcfe95394338d894a5575907baec0d56c54816.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 218
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 218
    },
    {
        "type": "text",
        "text": "The output is properly formatted as JSON. This allows us to more confidently use generative models in applications where we expect the output to adhere to certain formats. ",
        "page_idx": 218
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 219
    },
    {
        "type": "text",
        "text": "In this chapter, we explored the basics of using generative models through prompt engineering and output verification. We focused on the creativity and potential com‐ plexity that comes with prompt engineering. These components of a prompt are key in generating and optimizing output appropriate for different use cases. ",
        "page_idx": 219
    },
    {
        "type": "text",
        "text": "We further explored advanced prompt engineering techniques such as in-context learning and chain-of-thought. These methods involve guiding generative models to reason through complex problems by providing examples or phrases that encourage step-by-step thinking thereby mimicking human reasoning processes. ",
        "page_idx": 219
    },
    {
        "type": "text",
        "text": "Overall, this chapter demonstrated that prompt engineering is a crucial aspect of working with LLMs, as it allows us to effectively communicate our needs and prefer‐ ences to the model. By mastering prompt engineering techniques, we can unlock some of the potential of LLMs and generate high-quality responses that meet our requirements. ",
        "page_idx": 219
    },
    {
        "type": "text",
        "text": "The next chapter will build upon these concepts by exploring more advanced tech‐ niques for leveraging generative models. We will go beyond prompt engineering and explore how LLMs can use external memory and tools. ",
        "page_idx": 219
    },
    {
        "type": "text",
        "text": "Advanced Text Generation Techniques and Tools ",
        "text_level": 1,
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "In the previous chapter, we saw how prompt engineering can do wonders for the accuracy of your text-generation large language model (LLM). With just a few small tweaks, these LLMs are guided toward more purposeful and accurate answers. This showed how much there is to gain using techniques that do not fine-tune the LLM but instead use the LLM more efficiently, such as the relatively straightforward prompt engineering. ",
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "In this chapter, we will continue this train of thought. What can we do to further enhance the experience and output that we get from the LLM without needing to fine-tune the model itself? ",
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "Fortunately, a great deal of methods and techniques allow us to further improve what we started with in the previous chapter. These more advanced techniques lie at the foundation of numerous LLM-focused systems and are, arguably, one of the first things users implement when designing such systems. ",
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "In this chapter, we will explore several such methods and concepts for improving the quality of the generated text: ",
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "Model I/O Loading and working with LLMs ",
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "Memory Helping LLMs to remember ",
        "page_idx": 220
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Combining complex behavior with external tools ",
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Chains ",
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Connecting methods and modules ",
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "These methods are all integrated with the LangChain framework that will help us easily use these advanced techniques throughout this chapter. LangChain is one of the earlier frameworks that simplify working with LLMs through useful abstractions. Newer frameworks of note are DSPy and Haystack. Some of these abstractions are illustrated in Figure 7-1. Note that retrieval will be discussed in the next chapter. ",
        "page_idx": 221
    },
    {
        "type": "image",
        "img_path": "images/6e51b28a08b83e316bcc0a0e1567fa40fe430e22570ec3425164cccc1d8715fd.jpg",
        "image_caption": [
            "Figure 7-1. LangChain is a complete framework for using LLMs. It has modular compo‐ nents that can be chained together to allow for complex LLM systems. "
        ],
        "image_footnote": [],
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Each of these techniques has significant strengths by themselves but their true value does not exist in isolation. It is when you combine all of these techniques that you get an LLM-based system with incredible performance. The culmination of these techniques is truly where LLMs shine. ",
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Model I/O: Loading Quantized Models with LangChain ",
        "text_level": 1,
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Before we can make use of LangChain’s features to extend the capabilities of LLMs, we need to start by loading our LLM. As in previous chapters, we will be using Phi-3 but with a twist; we will use a GGUF model variant instead. A GGUF model represents a compressed version of its original counterpart through a method called quantization, which reduces the number of bits needed to represent the parameters of an LLM. ",
        "page_idx": 221
    },
    {
        "type": "text",
        "text": "Bits, a series of 0s and 1s, represent values by encoding them in binary form. More bits result in a wider range of values but requires more memory to store those values, as shown in Figure 7-2. ",
        "page_idx": 221
    },
    {
        "type": "image",
        "img_path": "images/82a58fffa294dcba2d1f03eb9265ebb6a931f541d05668988c5183c9d08de5b5.jpg",
        "image_caption": [
            "Figure 7-2. Attempting to represent pi with float 32-bit and float 16-bit representations. Notice the lowered accuracy when we halve the number of bits. "
        ],
        "image_footnote": [],
        "page_idx": 222
    },
    {
        "type": "text",
        "text": "Quantization reduces the number of bits required to represent the parameters of an LLM while attempting to maintain most of the original information. This comes with some loss in precision but often makes up for it as the model is much faster to run, requires less VRAM, and is often almost as accurate as the original. ",
        "page_idx": 222
    },
    {
        "type": "text",
        "text": "To illustrate quantization, consider this analogy. If asked what the time is, you might say “14:16,” which is correct but not a fully precise answer. You could have said it is $^ { \\infty } 1 4 ! 1 6$ and 12 seconds” instead, which would have been more accurate. However, mentioning seconds is seldom helpful and we often simply put that in discrete numbers, namely full minutes. Quantization is a similar process that reduces the precision of a value (e.g., removing seconds) without removing vital information (e.g., retaining hours and minutes). ",
        "page_idx": 222
    },
    {
        "type": "text",
        "text": "In Chapter 12, we will further discuss how quantization works under the hood. You can also see a full visual guide to quantization in $\\mathrm { { ^ { * } A } }$ Visual Guide to Quantization” by Maarten Grootendorst. For now, it is important to know that we will use an 8-bit variant of Phi-3 compared to the original 16-bit variant, cutting the memory requirements almost in half. ",
        "page_idx": 222
    },
    {
        "type": "image",
        "img_path": "images/1ac4802e6f22a7602ed19fd4729af718e9e830e584e52d723bde55ea6b8fa850.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 222
    },
    {
        "type": "text",
        "text": "As a rule of thumb, look for at least 4-bit quantized models. These models have a good balance between compression and accuracy. Although it is possible to use 3-bit or even 2-bit quantized mod‐ els, the performance degradation becomes noticeable and it would instead be preferable to choose a smaller model with a higher precision. ",
        "page_idx": 222
    },
    {
        "type": "text",
        "text": "First, we will need to download the model. Note that the link contains multiple files with different bit-variants. FP16, the model we choose, represents the 16-bit variant: ",
        "page_idx": 222
    },
    {
        "type": "text",
        "text": "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/ Phi-3-mini-4k-instruct-fp16.gguf ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "We use llama-cpp-python together with LangChain to load the GGUF file: ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "from langchain import LlamaCpp ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "# Make sure the model path is correct for your system!   \nllm $=$ LlamaCpp( model_path $=$ \"Phi-3-mini-4k-instruct-fp16.gguf\", n_gpu_layer $\\mathsf { S } \\mathsf { = } \\mathsf { - } \\mathsf { 1 }$ , max_tokens $= 5 \\Theta \\Theta$ , n_ct ${ \\it \\Omega } = 2 \\Theta 4 8$ , seed $= 4 2$ , verbose=False   \n) ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "In LangChain, we use the invoke function to generate output: ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "llm.invoke(\"Hi! My name is Maarten. What is $1 + 1 ?$ ) ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "Unfortunately, we get no output! As we have seen in previous chapters, Phi-3 requires a specific prompt template. Compared to our examples with transformers, we will need to explicitly use a template ourselves. Instead of copy-pasting this template each time we use Phi-3 in LangChain, we can use one of LangChain’s core functionalities, namely “chains.” ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "All examples in this chapter can be run with any LLM. This means that you can choose whether to use Phi-3, ChatGPT, Llama 3 or anything else when going through the examples. We will use Phi-3 as a default throughout, but the state-of-the-art changes quickly, so consider using a newer model instead. You can use the Open LLM Leaderboard (a ranking of open source LLMs) to choose whichever works best for your use case. ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "If you do not have access to a device that can run LLMs locally, consider using ChatGPT instead: ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "from langchain.chat_models import ChatOpenAI # Create a chat-based LLM chat_model $=$ ChatOpenAI(openai_api_key $\\mathbf { \\tilde { \\mathbf { \\tilde { \\mathbf { \\tilde { \\mathbf { \\tilde { \\tilde { \\tilde { \\mathbf { \\tilde } } } } } } } } } } }$ \"MY_KEY\") ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "Chains: Extending the Capabilities of LLMs ",
        "text_level": 1,
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "LangChain is named after one of its main methods, chains. Although we can run LLMs in isolation, their power is shown when used with additional components or even when used in conjunction with each other. Chains not only allow for extending the capabilities of LLMs but also for multiple chains to be connected together. ",
        "page_idx": 223
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 224
    },
    {
        "type": "text",
        "text": "The most basic form of a chain in LangChain is a single chain. Although a chain can take many forms, each with a different complexity, it generally connects an LLM with some additional tool, prompt, or feature. This idea of connecting a component to an LLM is illustrated in Figure 7-3. ",
        "page_idx": 224
    },
    {
        "type": "image",
        "img_path": "images/f34a8e929c9d5de227f7fba41a912680373792724191ef0eb9829f32b9ff1b70.jpg",
        "image_caption": [
            "Figure 7-3. A single chain connects some modular component, like a prompt template or external memory, to the LLM. "
        ],
        "image_footnote": [],
        "page_idx": 224
    },
    {
        "type": "text",
        "text": "In practice, chains can become complex quite quickly. We can extend the prompt template however we want and we can even combine several separate chains together to create intricate systems. In order to thoroughly understand what is happening in a chain, let’s explore how we can add Phi-3’s prompt template to the LLM. ",
        "page_idx": 224
    },
    {
        "type": "text",
        "text": "A Single Link in the Chain: Prompt Template ",
        "text_level": 1,
        "page_idx": 224
    },
    {
        "type": "text",
        "text": "We start with creating our first chain, namely the prompt template that Phi-3 expects. In the previous chapter, we explored how transformers.pipeline applies the chat template automatically. This is not always the case with other packages and they might need the prompt template to be explicitly defined. With LangChain, we will use chains to create and use a default prompt template. It also serves as a nice hands-on experience with using prompt templates. ",
        "page_idx": 224
    },
    {
        "type": "text",
        "text": "The idea, as illustrated in Figure 7-4, is that we chain the prompt template together with the LLM to get the output we are looking for. Instead of having to copy-paste the prompt template each time we use the LLM, we would only need to define the user and system prompts. ",
        "page_idx": 224
    },
    {
        "type": "image",
        "img_path": "images/1df6a201fdd57e850ffd256ca3873da3dc1b87ef84db15e58080c206cd04a636.jpg",
        "image_caption": [
            "Figure 7-4. By chaining a prompt template with an LLM, we only need to define the input prompts. The template will be constructed for you. "
        ],
        "image_footnote": [],
        "page_idx": 224
    },
    {
        "type": "text",
        "text": "The template for Phi-3 is comprised of four main components: ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "• $< \\mathsf { S } >$ to indicate when the prompt starts   \n• <|user|> to indicate the start of the user’s prompt   \n• <|assistant $| >$ to indicate the start of the model’s output   \n• <|end|> to indicate the end of either the prompt or the model’s output ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "These are further illustrated in Figure 7-5 with an example. ",
        "page_idx": 225
    },
    {
        "type": "image",
        "img_path": "images/ee50c4b553c2cf0524b41c8bdb5e01b91d859ec12621abea72688099e93843d3.jpg",
        "image_caption": [
            "Figure 7-5. The prompt template Phi-3 expects. "
        ],
        "image_footnote": [],
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "To generate our simple chain, we first need to create a prompt template that adheres to Phi-3’s expected template. Using this template, the model takes in a system_prompt, which generally describes what we expect from the LLM. Then, we can use the input_prompt to ask the LLM specific questions: ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "from langchain import PromptTemplate ",
        "text_level": 1,
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "# Create a prompt template with the \"input_prompt\" variable   \ntemplate $=$ \"\"\"<s><|user|>   \n{input_prompt} $\\lnot$ |end|>   \n<|assistant|>\"\"\"   \nprompt $=$ PromptTemplate( template=template, input_variables=[\"input_prompt\"]   \n) ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "To create our first chain, we can use both the prompt that we created and the LLM and chain them together: ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "basic_chain $=$ prompt | llm ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "To use the chain, we need to use the invoke function and make sure that we use the input_prompt to insert our question: ",
        "page_idx": 225
    },
    {
        "type": "text",
        "text": "# Use the chain   \nbasic_chain.invoke( { \"input_prompt\": \"Hi! My name is Maarten. What is $1 + 1 2 ^ { 1 1 }$ , }   \n) ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "The answer to $1 ~ + ~ 1$ is 2. It's a basic arithmetic operation where you add one unit to another, resulting in two units altogether. ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "The output gives us the response without any unnecessary tokens. Now that we have created this chain, we do not have to create the prompt template from scratch each time we use the LLM. Note that we did not disable sampling as before, so your output might differ. To make this pipeline more transparent, Figure 7-6 illustrates the connection between a prompt template and the LLM using a single chain. ",
        "page_idx": 226
    },
    {
        "type": "image",
        "img_path": "images/9a73fbc4295e72d63d3aad574b042e00385508bf0b9f4009f7f2cccd1af3f330.jpg",
        "image_caption": [
            "Figure 7-6. An example of a single chain using Phi-3’s template. "
        ],
        "image_footnote": [],
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "The example assumes that the LLM needs a specific template. This is not always the case. With OpenAI’s GPT-3.5, its API handles the underlying template. ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "You could also use a prompt template to define other variables that might change in your prompts. For example, if we want to create funny names for businesses, retyping that question over and over for different products can be time-consuming. ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "Instead, we can create a prompt that is reusable: ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "# Create a Chain that creates our business' name   \ntemplate $=$ \"Create a funny name for a business that   \nsells {product}.\"   \nname_prompt $=$ PromptTemplate( template $\\iota =$ template, input_variables=[\"product\"]   \n) ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "Adding a prompt template to the chain is just the very first step you need to enhance the capabilities of your LLM. Throughout this chapter, we will see many ways in which we can add additional modular components to existing chains, starting with memory. ",
        "page_idx": 226
    },
    {
        "type": "text",
        "text": "A Chain with Multiple Prompts ",
        "text_level": 1,
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "In our previous example, we created a single chain consisting of a prompt template and an LLM. Since our example was quite straightforward, the LLM had no issues dealing with the prompt. However, some applications are more involved and require lengthy or complex prompts to generate a response that captures those intricate details. ",
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "Instead, we could break this complex prompt into smaller subtasks that can be run sequentially. This would require multiple calls to the LLM but with smaller prompts and intermediate outputs as shown in Figure 7-7. ",
        "page_idx": 227
    },
    {
        "type": "image",
        "img_path": "images/70cbaae3e16e98144727557adab34bf51b7ebb59f03ec3c5c2bc4f901f4cbdce.jpg",
        "image_caption": [
            "Figure 7-7. With sequential chains, the output of a prompt is used as the input for the next prompt. "
        ],
        "image_footnote": [],
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "This process of using multiple prompts is an extension of our previous example. Instead of using a single chain, we link chains where each link deals with a specific subtask. ",
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "For instance, consider the process of generating a story. We could ask the LLM to generate a story along with complex details like the title, a summary, a description of the characters, etc. Instead of trying to put all of that information into a single prompt, we could dissect this prompt into manageable smaller tasks instead. ",
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "Let’s illustrate with an example. Assume that we want to generate a story that has three components: ",
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "• A title • A description of the main character • A summary of the story ",
        "page_idx": 227
    },
    {
        "type": "text",
        "text": "Instead of generating everything in one go, we create a chain that only requires a single input by the user and then sequentially generates the three components. This process is illustrated in Figure 7-8. ",
        "page_idx": 227
    },
    {
        "type": "image",
        "img_path": "images/8c924e35f3999105c584c0e0b9651e3a1f74fe6c1d31d8214b638c564fe2c79a.jpg",
        "image_caption": [
            "Figure 7-8. The output of the title prompt is used as the input of the character prompt. To generate the story, the output of all previous prompts is used. "
        ],
        "image_footnote": [],
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "To generate that story, we use LangChain to describe the first component, namely the title. This first link is the only component that requires some input from the user. We define the template and use the \"summary\" variable as the input variable and \"title\" as the output. ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "We ask the LLM to “Create a title for a story about {summary}” where “{summary}” will be our input: ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "from langchain import LLMChain ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "# Create a chain for the title of our story   \ntemplate $=$ \"\"\"<s><|user|>   \nCreate a title for a story about {summary}. Only return the title.<|end|>   \n<|assistant|>\"\"\"   \ntitle_prompt $=$ PromptTemplate(template $\\iota =$ template, input_variables $=$ [\"summary\"])   \ntitle $=$ LLMChain(llm=llm, prompt $: =$ title_prompt, output_key=\"title\") ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "Let’s run an example to showcase these variables: ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "title.invoke({\"summary\": \"a girl that lost her mother\"}) ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "{'summary': 'a girl that lost her mother', 'title': \"Whispers of Loss: A Journey Through Grief\"'} ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "This already gives us a great title for the story! Note that we can see both the input (\"summary\") as well as the output (\"title\"). ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "Let’s generate the next component, namely the description of the character. We generate this component using both the summary as well as the previously generated title. Making sure that the chain uses those components, we create a new prompt with the {summary} and {title} tags: ",
        "page_idx": 228
    },
    {
        "type": "text",
        "text": "# Create a chain for the character description using the summary and title template $=$ \"\"\"<s><|user|>   \nDescribe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>   \n<|assistant|>\"\"\"   \ncharacter_prompt $=$ PromptTemplate(   \ntemplate $\\iota =$ template, input_variables $\\mathbf { \\sigma } =$ [\"summary\", \"title\"]   \n)   \ncharacter $=$ LLMChain(llm $\\mid =$ llm, prompt $=$ character_prompt, output_key=\"character\") ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "Although we could now use the character variable to generate our character descrip tion manually, it will be used as part of the automated chain instead. ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "Let’s create the final component, which uses the summary, title, and character description to generate a short description of the story: ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "# Create a chain for the story using the summary, title, and character descrip tion   \ntemplate $=$ \"\"\"<s><|user|>   \nCreate a story about {summary} with the title {title}. The main character is: {character}. Only return the story and it cannot be longer than one paragraph. <|end|>   \n<|assistant|>\"\"\"   \nstory_prompt $=$ PromptTemplate(   \ntemplate=template, input_variables $\\mathbf { \\equiv }$ [\"summary\", \"title\", \"character\"] )   \nstory $=$ LLMChain(llm $\\mid =$ llm, prompt $\\Bumpeq$ story_prompt, output_key $^ { \\prime } =$ \"story\") ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "Now that we have generated all three components, we can link them together to create our full chain: ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "# Combine all three components to create the full chain llm_chain $=$ title | character | story ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "We can run this newly created chain using the same example we used before: ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "llm_chain.invoke(\"a girl that lost her mother\") ",
        "page_idx": 229
    },
    {
        "type": "text",
        "text": "Running this chain gives us all three components. This only required us to input a single short prompt, the summary. Another advantage of dividing the problem into smaller tasks is that we now have access to these individual components. We can easily extract the title; that might not have been the case if we were to use a single prompt. ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "Memory: Helping LLMs to Remember Conversations ",
        "text_level": 1,
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "When we are using LLMs out of the box, they will not remember what was being said in a conversation. You can share your name in one prompt but it will have forgotten it by the next prompt. ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "Let’s illustrate this phenomenon with an example using the basic_chain we created before. First, we tell the LLM our name: ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "# Let's give the LLM our name basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is $1 ~ + ~ 1 ? ~ \\}$ ) ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "Hello Maarten! The answer to $1 ~ + ~ 1$ is 2. ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "Next, we ask it to reproduce the name we have given it: ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "# Next, we ask the LLM to reproduce the name basic_chain.invoke({\"input_prompt\": \"What is my name?\"}) ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "I'm sorry, but as a language model, I don't have the ability to know personal information about individuals. You can provide the name you'd like to know more about, and I can help you with information or general inquiries related to it. ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "Unfortunately, the LLM does not know the name we gave it. The reason for this forgetful behavior is that these models are stateless—they have no memory of any previous conversation! ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 7-9, conversing with an LLM that does not have any memory is not the greatest experience. ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "To make these models stateful, we can add specific types of memory to the chain that we created earlier. In this section, we will go through two common methods for helping LLMs to remember conservations: ",
        "page_idx": 230
    },
    {
        "type": "text",
        "text": "• Conversation buffer • Conversation summary ",
        "page_idx": 230
    },
    {
        "type": "image",
        "img_path": "images/ff151c589887b9ba8eb5fcf99b10b2ff9d03658a5ad4028610f84c1609088d19.jpg",
        "image_caption": [
            "Figure 7-9. An example of a conversation between an LLM with memory and without memory. "
        ],
        "image_footnote": [],
        "page_idx": 231
    },
    {
        "type": "text",
        "text": "Conversation Buffer ",
        "text_level": 1,
        "page_idx": 231
    },
    {
        "type": "text",
        "text": "One of the most intuitive forms of giving LLMs memory is simply reminding them exactly what has happened in the past. As illustrated in Figure 7-10, we can achieve this by copying the full conversation history and pasting that into our prompt. ",
        "page_idx": 231
    },
    {
        "type": "image",
        "img_path": "images/071b13a5819683a5d33216d2c7e0ed0f357862ce771f91d843790fdb9cd9aa61.jpg",
        "image_caption": [
            "Figure 7-10. We can remind an LLM of what previously happened by simply appending the entire conversation history to the input prompt. "
        ],
        "image_footnote": [],
        "page_idx": 231
    },
    {
        "type": "text",
        "text": "In LangChain, this form of memory is called a ConversationBufferMemory. Its implementation requires us to update our previous prompt to hold the history of the chat. ",
        "page_idx": 231
    },
    {
        "type": "text",
        "text": "We’ll start by creating this prompt: ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "# Create an updated prompt template to include a chat history   \ntemplate $=$ \"\"\"<s><|user|>Current conversation:{chat_history}   \n{input_prompt}<|end|>   \n<|assistant|>\"\"\"   \nprompt $=$ PromptTemplate( template=template, input_variables=[\"input_prompt\", \"chat_history\"]   \n) ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "Notice that we added an additional input variable, namely chat_history. This is where the conversation history will be given before we ask the LLM our question. ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "Next, we can create LangChain’s ConversationBufferMemory and assign it to the chat_history input variable. ConversationBufferMemory will store all the conversa‐ tions we have had with the LLM thus far. ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "We put everything together and chain the LLM, memory, and prompt template: ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "from langchain.memory import ConversationBufferMemory   \n# Define the type of memory we will use   \nmemory $=$ ConversationBufferMemory(memory_key=\"chat_history\")   \n# Chain the LLM, prompt, and memory together   \nllm_chain $=$ LLMChain( prompt=prompt, llm=llm, memory=memory   \n) ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "To explore whether we did this correctly, let’s create a conversation history with the LLM by asking it a simple question: ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "# Generate a conversation and ask a basic question llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is $1 ~ + ~ 1 ? ~ \\}$ ) ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "{'input_prompt': 'Hi! My name is Maarten. What is $1 { \\ : } + { \\ : } 1 ? { } ^ { \\prime }$ ,   \n'chat_history': ',   \n'text': \" Hello Maarten! The answer to $1 ~ + ~ 1$ is 2. Hope you're having a great   \nday!\"} ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "You can find the generated text in the 'text' key, the input prompt in 'in put_prompt', and the chat history in 'chat_history'. Note that since this is the first time we used this specific chain, there is no chat history. ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "Next, let’s follow up by asking the LLM if it remembers the name we used: ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "# Does the LLM remember the name we gave it? llm_chain.invoke({\"input_prompt\": \"What is my name?\"}) ",
        "page_idx": 232
    },
    {
        "type": "text",
        "text": "{'input_prompt': 'What is my name?',   \n'chat_history': \"Human: Hi! My name is Maarten. What is $1 ~ + ~ 1 ? \\backslash \\{ \\mathsf { n A I }$ : Hello   \nMaarten! The answer to $1 ~ + ~ 1$ is 2. Hope you're having a great day!\",   \n'text': ' Your name is Maarten.'} ",
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "By extending the chain with memory, the LLM was able to use the chat history to find the name we gave it previously. This more complex chain is illustrated in Figure 7-11 to give an overview of this additional functionality. ",
        "page_idx": 233
    },
    {
        "type": "image",
        "img_path": "images/60c11470427be92ed73144500bc7940ead8b05c89cfacea6fee714c46a0775f8.jpg",
        "image_caption": [
            "Figure 7-11. We extend the LLM chain with memory by appending the entire conversa‐ tion history to the input prompt. "
        ],
        "image_footnote": [],
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "Windowed Conversation Buffer ",
        "text_level": 1,
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "In our previous example, we essentially created a chatbot. You could talk to it and it remembers the conversation you had thus far. However, as the size of the conversa‐ tion grows, so does the size of the input prompt until it exceeds the token limit. ",
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "One method of minimizing the context window is to use the last $k$ conversations instead of maintaining the full chat history. In LangChain, we can use Conversation BufferWindowMemory to decide how many conversations are passed to the input prompt: ",
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "from langchain.memory import ConversationBufferWindowMemory   \n# Retain only the last 2 conversations in memory   \nmemory $=$ ConversationBufferWindowMemory( $k = 2$ , memory_key=\"chat_history\")   \n# Chain the LLM, prompt, and memory together   \nllm_chain $=$ LLMChain( prompt=prompt, llm=llm, memory=memory   \n) ",
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "Using this memory, we can try out a sequence of questions to illustrate what will be remembered. We start with two conversations: ",
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "# Ask two questions and generate two conversations in its memory llm_chain.predict(input_prompt $: =$ \"Hi! My name is Maarten and I am 33 years old. ",
        "page_idx": 233
    },
    {
        "type": "text",
        "text": "{'input_prompt': 'What is $3 + 3 ?$ ,   \n'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is $\\textit { 1 + 1 2 }$ \\nAI: Hello Maarten! It's nice to meet you. Regarding your question, $^ { 1 ~ + }$ 1 equals 2. If you have any other questions or need further assistance, feel free to ask! $\\mathsf { \\backslash n \\backslash n }$ (Note: This response answers the provided mathematical query while maintaining politeness and openness for additional inquiries.)\",   \n'text': \" Hello Maarten! It's nice to meet you as well. Regarding your new question, $3 + 3$ equals 6. If there's anything else you need help with or more questions you have, $\\tau ^ { \\prime } { \\mathfrak { n } }$ here for you!\"} ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "The interaction we had thus far is shown in \"chat_history\". Note that under the hood, LangChain saves it as an interaction between you (indicated with Human) and the LLM (indicated with AI). ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "Next, we can check whether the model indeed knows the name we gave it: ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "# Check whether it knows the name we gave it llm_chain.invoke({\"input_prompt\":\"What is my name?\"}) ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "{'input_prompt': 'What is my name?', 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is $\\textit { 1 + 1 2 }$ \\nAI: Hello Maarten! It's nice to meet you. Regarding your question, $^ { 1 ~ + }$ 1 equals 2. If you have any other questions or need further assistance, feel free to ask!\\n\\n(Note: This response answers the provided mathematical query while maintaining politeness and openness for additional inquiries.)\\nHuman: What is $3 + 3 2$ \\nAI: Hello Maarten! It's nice to meet you as well. Regarding your new question, $3 + 3$ equals 6. If there's anything else you need help with or more questions you have, 'text': ' Your name is Maart $\\tau ^ { \\prime } \\mathfrak { n }$ here for you!\", as mentioned at the beginning of our conversation. Is there anything else you would like to know or discuss?'} ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "Based on the output in 'text' it correctly remembers the name we gave it. Note that the chat history is updated with the previous question. ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "Now that we have added another conversation we are up to three conversations. Con‐ sidering the memory only retains the last two conversations, our very first question is not remembered. ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "Since we provided an age in our first interaction, we check whether the LLM indeed does not know the age anymore: ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "# Check whether it knows the age we gave it llm_chain.invoke({\"input_prompt\":\"What is my age?\"}) ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "{'input_prompt': 'What is my age?',   \n'chat_history': \"Human: What is $3 + 3 ?$ \\nAI: Hello again! $3 + 3$ equals 6. If there's anything else I can help you with, just let me know!\\nHuman: What is my name?\\nAI: Your name is Maarten.\",'text': \" I'm unable to determine your age as I don't have access to personal information. Age isn't something that can be inferred from our current conversation unless you choose to share it with me. How else may I assist you today?\"} ",
        "page_idx": 234
    },
    {
        "type": "text",
        "text": "The LLM indeed has no access to our age since that was not retained in the chat history. ",
        "page_idx": 235
    },
    {
        "type": "text",
        "text": "Although this method reduces the size of the chat history, it can only retain the last few conversations, which is not ideal for lengthy conversations. Let’s explore how we can summarize the chat history instead. ",
        "page_idx": 235
    },
    {
        "type": "text",
        "text": "Conversation Summary ",
        "text_level": 1,
        "page_idx": 235
    },
    {
        "type": "text",
        "text": "As we have discussed previously, giving your LLM the ability to remember conver‐ sations is vital for a good interactive experience. However, when using Conversation BufferMemory, the conversation starts to increase in size and will slowly approach your token limit. Although ConversationBufferWindowMemory resolves the issue of token limits to an extent, only the last $k$ conversations are retained. ",
        "page_idx": 235
    },
    {
        "type": "text",
        "text": "Although a solution would be to use an LLM with a larger context window, these tokens still need to be processed before generation tokens, which can increase compute time. Instead, let’s look toward a more sophisticated technique, Conversa tionSummaryMemory. As the name implies, this technique summarizes an entire con‐ versation history to distill it into the main points. ",
        "page_idx": 235
    },
    {
        "type": "text",
        "text": "This summarization process is enabled by another LLM that is given the conversation history as input and asked to create a concise summary. A nice advantage of using an external LLM is that we are not confined to using the same LLM during conversation. The summarization process is illustrated in Figure 7-12. ",
        "page_idx": 235
    },
    {
        "type": "image",
        "img_path": "images/b0dad392dec2cca381261ffeb14e55af88624becab925feea80fd0fe4337f3b6.jpg",
        "image_caption": [
            "Figure 7-12. Instead of passing the conversation history directly to the prompt, we use another LLM to summarize it first. "
        ],
        "image_footnote": [],
        "page_idx": 235
    },
    {
        "type": "text",
        "text": "This means that whenever we ask the LLM a question, there are two calls: ",
        "page_idx": 236
    },
    {
        "type": "text",
        "text": "• The user prompt • The summarization prompt ",
        "page_idx": 236
    },
    {
        "type": "text",
        "text": "To use this in LangChain, we first need to prepare a summarization template that we will use as the summarization prompt: ",
        "page_idx": 236
    },
    {
        "type": "text",
        "text": "# Create a summary prompt template   \nsummary_prompt_template $=$ \"\"\"<s><|user|>Summarize the conversations and update with the new lines.   \nCurrent summary:   \n{summary}   \nnew lines of conversation:   \n{new_lines}   \nNew summary:<|end|>   \n<|assistant|>\"\"\"   \nsummary_prompt $=$ PromptTemplate(   \ninput_variables $=$ [\"new_lines\", \"summary\"],   \ntemplate $\\iota =$ summary_prompt_template   \n) ",
        "page_idx": 236
    },
    {
        "type": "text",
        "text": "Using ConversationSummaryMemory in LangChain is similar to what we did with the previous examples. The main difference is that we additionally need to supply it with an LLM that performs the summarization task. Although we use the same LLM for both summarizing and user prompting, you could use a smaller LLM for the summarization task to speed up computation: ",
        "page_idx": 236
    },
    {
        "type": "text",
        "text": "from langchain.memory import ConversationSummaryMemory   \n# Define the type of memory we will use   \nmemory $=$ ConversationSummaryMemory( llm $\\mid =$ llm, memory_key=\"chat_history\", prompt=summary_prompt   \n)   \n# Chain the LLM, prompt, and memory together   \nllm_chain $=$ LLMChain( prompt $: =$ prompt, llm $\\mid =$ llm, memory=memory   \n) ",
        "page_idx": 236
    },
    {
        "type": "text",
        "text": "Having created our chain, we can test out its summarization capabilities by creating a short conversation: ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "# Generate a conversation and ask for the name   \nllm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is $1 ~ + ~ 1 ? ~ \\}$ )   \nllm_chain.invoke({\"input_prompt\": \"What is my name?\"}) {'input_prompt': 'What is my name?',   \n'chat_history': ' Summary: Human, identified as Maarten, asked the AI about the sum of $1 ~ + ~ 1$ , which was correctly answered by the AI as 2 and offered additional assistance if needed.',   \n'text': ' Your name in this context was referred to as \"Maarten\". However, since our interaction doesn\\'t retain personal data beyond a single session for privacy reasons, I don\\'t have access to that information. How can I assist you further today?'} ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "After each step, the chain will summarize the conversation up until that point. Note how the first conversation was summarized in 'chat_history' by creating a description of the conversation. ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "We can continue the conversation and at each step, the conversation will be summar‐ ized and new information will be added as necessary: ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "# Check whether it has summarized everything thus far llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"}) ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "{'input_prompt': 'What was the first question I asked?',   \n'chat_history': ' Summary: Human, identified as Maarten in the context of this   \nconversation, first asked about the sum of $1 ~ + ~ 1$ and received an answer of   \n2 from the AI. Later, Maarten inquired about their name but the AI clarified   \nthat personal data is not retained beyond a single session for privacy rea  \nsons. The AI offered further assistance if needed.',   \n'text': ' The first question you asked was \"what\\'s $1 ~ + ~ 1 ? \" ~ \\}$ ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "After asking another question, the LLM updated the summary to include the previ‐ ous conversation and correctly inferred the original question. ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "To get the most recent summary, we can access the memory variable we created previously: ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "# Check what the summary is thus far memory.load_memory_variables({}) ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "{'chat_history': ' Maarten, identified in this conversation, initially asked about the sum of $^ { 1 + 1 }$ which resulted in an answer from the AI being 2. Subsequently, he sought clarification on his name but the AI informed him that no personal data is retained beyond a single session due to privacy reasons. The AI then offered further assistance if required. Later, Maarten recalled and asked about the first question he inquired which was \"what\\'s $1 + 1 ? \\ \" \\cdot \\}$ ",
        "page_idx": 237
    },
    {
        "type": "text",
        "text": "This more complex chain is illustrated in Figure 7-13 to give an overview of this additional functionality. ",
        "page_idx": 237
    },
    {
        "type": "image",
        "img_path": "images/8400c00d42d92d824baf823c11857add26da2b988c8efb60547b4bb58e737de3.jpg",
        "image_caption": [
            "Figure 7-13. We extend the LLM chain with memory by summarizing the entire conver‐ sation history before giving it to the input prompt. "
        ],
        "image_footnote": [],
        "page_idx": 238
    },
    {
        "type": "text",
        "text": "This summarization helps keep the chat history relatively small without using too many tokens during inference. However, since the original question was not explicitly saved in the chat history, the model needed to infer it based on the context. This is a disadvantage if specific information needs to be stored in the chat history. Moreover, multiple calls to the same LLM are needed, one for the prompt and one for the summarization. This can slow down computing time. ",
        "page_idx": 238
    },
    {
        "type": "text",
        "text": "Often, it is a trade-off between speed, memory, and accuracy. Where Conversation BufferMemory is instant but hogs tokens, ConversationSummaryMemory is slow but frees up tokens to use. Additional pros and cons of the memory types we have explored thus far are described in Table 7-1. ",
        "page_idx": 238
    },
    {
        "type": "table",
        "img_path": "images/2303168bdcd5dcd236af427b61bc3ec111147e1f39d368ec8279c7836c056b0d.jpg",
        "table_caption": [
            "Table 7-1. The pros and cons of different memory types. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Memory type Pros</td><td></td><td>Cons</td></tr><tr><td>Conversation Buffer</td><td>·Easiest implementation ·Ensures no information loss within context window</td><td>· Slower generation speed as more tokens are needed ·Only suitable for large-context LLMs ·Larger chat histories make information retrieval difficult</td></tr><tr><td>Windowed Conversation Buffer</td><td>· Large-context LLMs are not needed unless chat history is large ·No information loss over the last k interactions</td><td>· Only captures the last k interactions ·No compression of the last k interactions</td></tr><tr><td>Conversation Summary</td><td>· Captures the fll hstory ·Enables long conversations ·Reduces tokens needed to capture full history</td><td>·An additional callis necessary for each interaction · Quality is reliant on the LLM&#x27;s summarization capabilities</td></tr></table>",
        "page_idx": 238
    },
    {
        "type": "text",
        "text": "Agents: Creating a System of LLMs ",
        "text_level": 1,
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "Thus far, we have created systems that follow a user-defined set of steps to take. One of the most promising concepts in LLMs is their ability to determine the actions they can take. This idea is often called agents, systems that leverage a language model to determine which actions they should take and in what order. ",
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "Agents can make use of everything we have seen thus far, such as model I/O, chains, and memory, and extend it further with two vital components: ",
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "• Tools that the agent can use to do things it could not do itself • The agent type, which plans the actions to take or tools to use ",
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "Unlike the chains we have seen thus far, agents are able to show more advanced behavior like creating and self-correcting a roadmap to achieve a goal. They can interact with the real world through the use of tools. As a result, these agents can perform a variety of tasks that go beyond what an LLM is capable of in isolation. ",
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "For example, LLMs are notoriously bad at mathematical problems and often fail at solving simple math-based tasks but they could do much more if we provide access to a calculator. As illustrated in Figure 7-14, the underlying idea of agents is that they utilize LLMs not only to understand our query but also to decide which tool to use and when. ",
        "page_idx": 239
    },
    {
        "type": "image",
        "img_path": "images/c302c1e4d5e6c7c5be3136a1d16e2279166beaab42bcdc09af3d1326ea915289.jpg",
        "image_caption": [
            "Figure 7-14. Giving LLMs the ability to choose which tools they use for a particular problem results in more complex and accurate behavior. "
        ],
        "image_footnote": [],
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "In this example, we would expect the LLM to use the calculator when it faces a math‐ ematical task. Now imagine we extend this with dozens of other tools, like a search engine or a weather API. Suddenly, the capabilities of LLMs increase significantly. ",
        "page_idx": 239
    },
    {
        "type": "text",
        "text": "In other words, agents that make use of LLMs can be powerful general problem solv‐ ers. Although the tools they use are important, the driving force of many agent-based systems is the use of a framework called Reasoning and Acting (ReAct1). ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "The Driving Power Behind Agents: Step-by-step Reasoning ",
        "text_level": 1,
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "ReAct is a powerful framework that combines two important concepts in behavior: reasoning and acting. LLMs are exceptionally powerful when it comes to reasoning as we explored in detail in Chapter 5. ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "Acting is a bit of a different story. LLMs are not able to act like you and I do. To give them the ability to act, we could tell an LLM that it can use certain tools, like a weather forecasting API. However, since LLMs can only generate text, they would need to be instructed to use specific queries to trigger the forecasting API. ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "ReAct merges these two concepts and allows reasoning to affect acting and actions to affect reasoning. In practice, the framework consists of iteratively following these three steps: ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "• Thought • Action • Observation ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "Illustrated in Figure 7-15, the LLM is asked to create a “thought” about the input prompt. This is similar to asking the LLM what it thinks it should do next and why. Then, based on the thought, an “action” is triggered. The action is generally an external tool, like a calculator or a search engine. Finally, after the results of the “action” are returned to the LLM it “observes” the output, which is often a summary of whatever result it retrieved. ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "To illustrate with an example, imagine you are on holiday in the United States and interested in buying a MacBook Pro. Not only do you want to know the price but you need it converted to EUR as you live in Europe and are more comfortable with those prices. ",
        "page_idx": 240
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 7-16, the agent will first search the web for current prices. It might find one or more prices depending on the search engine. After retrieving the price, it will use a calculator to convert USD to EUR assuming we know the exchange rate. ",
        "page_idx": 240
    },
    {
        "type": "image",
        "img_path": "images/345c8fc54a1ca2c795359a9addfb3f7552e7fa95637a689d3c7680972c840bbb.jpg",
        "image_caption": [
            "Figure 7-15. An example of a ReAct prompt template. "
        ],
        "image_footnote": [],
        "page_idx": 241
    },
    {
        "type": "image",
        "img_path": "images/29e7ad7c3eaa9881dcc33c50943fdf54c952fb7082bd6bf2ba9a59ec68cb66cd.jpg",
        "image_caption": [
            "Figure 7-16. An example of two cycles in a ReAct pipeline. "
        ],
        "image_footnote": [],
        "page_idx": 241
    },
    {
        "type": "text",
        "text": "During this process, the agent describes its thoughts (what it should do), its actions (what it will do), and its observations (the results of the action). It is a cycle of thoughts, actions, and observations that results in the agent’s output. ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "ReAct in LangChain ",
        "text_level": 1,
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "To illustrate how agents work in LangChain, we are going to build a pipeline that can search the web for answers and perform calculations with a calculator. These autonomous processes generally require an LLM that is powerful enough to properly follow complex instructions. ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "The LLM that we used thus far is relatively small and not sufficient to run these examples. Instead, we will be using OpenAI’s GPT-3.5 model as it follows these complex instructions more closely: ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "import os from langchain_openai import ChatOpenAI ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "# Load OpenAI's LLMs with LangChain   \nos.environ[\"OPENAI_API_KEY\"] $=$ \"MY_KEY\"   \nopenai_llm $=$ ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature $\\scriptstyle 1 = 0$ ) ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "Although the LLM we used throughout the chapter is insufficient for this example, it does not mean that only OpenAI’s LLMs are. Larger useful LLMs exist but they require significantly more com‐ pute and VRAM. For instance, local LLMs often come in different sizes and within a family of models, increasing a model’s size leads to better performance. To keep the necessary compute at a mini‐ mum, we choose a smaller LLM throughout the examples in this chapter. ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "However, as the field of generative models evolves, so do these smaller LLMs. We would be anything but surprised if eventually smaller LLMs, like the one used in this chapter, would be capable enough to run this example. ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "After doing so, we will define the template for our agent. As we have shown before, it describes the ReAct steps it needs to follow: ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "# Create the ReAct template   \nreact_template $=$ \"\"\"Answer the following questions as best you can. You have   \naccess to the following tools: ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "{tools} ",
        "text_level": 1,
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "Use the following format: ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "Question: the input question you must answer Thought: you should always think about what to do ",
        "page_idx": 242
    },
    {
        "type": "text",
        "text": "Action: the action to take, should be one of [{tool_names}]   \nAction Input: the input to the action   \nObservation: the result of the action   \n... (this Thought/Action/Action Input/Observation can repeat N times)   \nThought: I now know the final answer   \nFinal Answer: the final answer to the original input question ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "Begin! ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "Question: {input} Thought:{agent_scratchpad}\" ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "prompt $=$ PromptTemplate( template=react_template, input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]   \n) ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "This template illustrates the process of starting with a question and generating inter‐ mediate thoughts, actions, and observations. ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "To have the LLM interact with the outside world, we will describe the tools it can use: ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "from langchain.agents import load_tools, Tool from langchain.tools import DuckDuckGoSearchResults ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "# You can create the tool to pass to an agent   \nsearch $=$ DuckDuckGoSearchResults()   \nsearch_tool $=$ Tool( name=\"duckduck\", description=\"A web search engine. Use this to as a search engine for gen   \neral queries.\", func search.run,   \n)   \n# Prepare tools   \ntools $=$ load_tools([\"llm-math\"], llm $\\left| = \\right.$ openai_llm)   \ntools.append(search_tool) ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "The tools include the DuckDuckGo search engine and a math tool that allows it to access a basic calculator. ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "Finally, we create the ReAct agent and pass it to the AgentExecutor, which handles executing the steps: ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "from langchain.agents import AgentExecutor, create_react_agent ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "# Construct the ReAct agent   \nagent $=$ create_react_agent(openai_llm, tools, prompt)   \nagent_executor $=$ AgentExecutor( agent $\\ l =$ agent, tools $\\mathbf { \\sigma } =$ tools, verbose=True, handle_parsing_error $: =$ True   \n) ",
        "page_idx": 243
    },
    {
        "type": "text",
        "text": "To test whether the agent works, we use the previous example, namely finding the price of a MacBook Pro: ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "# What is the price of a MacBook Pro? agent_executor.invoke( { \"input\": \"What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.\" } ) While executing, the model generates multiple intermediate steps similar to the steps illustrated in Figure 7-17. ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": ">Entering new AgentExecutor chain...   \nI need to find the current price of a MacBook Pro in USD first before converting it to EUR. Action: duckduck   \nAction Input: \"current price of MacBook Pro in USD\"[snippet: View at Best Buy. The best MacE Action: Calculator   \nAction Input: \\$2,249.00 \\* 0.85Answer: 1911.649999999999I now know the final answer Final Answer: The current price of α MacBook Pro in USD is \\$2,249.00. It would cost approxin ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "Figure 7-17. An example of the ReAct process in LangChain. ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "These intermediate steps illustrate how the model processes the ReAct template and what tools it accesses. This allows us to debug issues and explore whether the agent uses the tools correctly. ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "When finished, the model gives us an output like this: ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "{'input': 'What is the current price of a MacBook Pro in USD? How much would it cost in EUR if the exchange rate is 0.85 EUR for 1 USD?', 'output': 'The current price of a MacBook Pro in USD is $\\$ 2$ ,249.00. It would cost approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.'} ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "Considering the limited tools the agent has, this is quite impressive! Using just a search engine and a calculator the agent could give us an answer. ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "Whether that answer is actually correct should be taken into account. By creating this relatively autonomous behavior, we are not involved in the intermediate steps. As such, there is no human in the loop to judge the quality of the output or reasoning process. ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "This double-edged sword requires a careful system design to improve its reliability. For instance, we could have the agent return the website’s URL where it found the MacBook Pro’s price or ask whether the output is correct at each step. ",
        "page_idx": 244
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 245
    },
    {
        "type": "text",
        "text": "In this chapter, we explored several ways to extend the capabilities of LLMs by adding modular components. We began by creating a simple but reusable chain that connected the LLM with a prompt template. We then expanded on this concept by adding memory to the chain, which allowed the LLM to remember conversations. We explored three different methods to add memory and discussed their strengths and weaknesses. ",
        "page_idx": 245
    },
    {
        "type": "text",
        "text": "We then delved into the world of agents that leverage LLMs to determine their actions and make decisions. We explored the ReAct framework, which uses an intuitive prompting framework that allows agents to reason about their thoughts, take actions, and observe the results. This led us to build an agent that is able to freely use the tools at its disposal, such as searching the web and using a calculator, demonstrating the potential power of agents. ",
        "page_idx": 245
    },
    {
        "type": "text",
        "text": "With this foundation in place, we are now poised to explore ways in which LLMs can be used to improve existing search systems and even become the core of new, more powerful search systems, as discussed in the next chapter. ",
        "page_idx": 245
    },
    {
        "type": "text",
        "text": "Semantic Search and Retrieval-Augmented Generation ",
        "text_level": 1,
        "page_idx": 246
    },
    {
        "type": "text",
        "text": "Search was one of the first language model applications to see broad industry adop‐ tion. Months after the release of the seminal “BERT: Pre-training of deep bidirec‐ tional transformers for language understanding” (2018) paper, Google announced it was using it to power Google Search and that it represented “one of the biggest leaps forward in the history of Search.” Not to be outdone, Microsoft Bing also stated that “Starting from April of this year, we used large transformer models to deliver the largest quality improvements to our Bing customers in the past year.” ",
        "page_idx": 246
    },
    {
        "type": "text",
        "text": "This is a clear testament to the power and usefulness of these models. Their addi‐ tion instantly and dramatically improves some of the most mature, well-maintained systems that billions of people around the planet rely on. The ability they add is called semantic search, which enables searching by meaning, and not simply keyword matching. ",
        "page_idx": 246
    },
    {
        "type": "text",
        "text": "On a separate track, the fast adoption of text generation models led many users to ask the models questions and expect factual answers. And while the models were able to answer fluently and confidently, their answers were not always correct or up-to-date. This problem grew to be known as model “hallucinations,” and one of the leading ways to reduce it is to build systems that can retrieve relevant information and provide it to the LLM to aid it in generating more factual answers. This method, called RAG, is one of the most popular applications of LLMs. ",
        "page_idx": 246
    },
    {
        "type": "text",
        "text": "Overview of Semantic Search and RAG ",
        "text_level": 1,
        "page_idx": 247
    },
    {
        "type": "text",
        "text": "There’s a lot of research on how to best use language models for search. Three broad categories of these models are dense retrieval, reranking, and RAG. Here is an overview of these three categories that the rest of the chapter will then explain in more detail: ",
        "page_idx": 247
    },
    {
        "type": "text",
        "text": "Dense retrieval ",
        "text_level": 1,
        "page_idx": 247
    },
    {
        "type": "text",
        "text": "Dense retrieval systems rely on the concept of embeddings, the same concept we’ve encountered in the previous chapters, and turn the search problem into retrieving the nearest neighbors of the search query (after both the query and the documents are converted into embeddings). Figure 8-1 shows how dense retrieval takes a search query, consults its archive of texts, and outputs a set of relevant results. ",
        "page_idx": 247
    },
    {
        "type": "image",
        "img_path": "images/9959b6e4a675814181b201b2baa937740d6dfbfc693bc4fe741c75fc8fb8b9dc.jpg",
        "image_caption": [
            "Figure 8-1. Dense retrieval is one of the key types of semantic search, relying on the similarity of text embeddings to retrieve relevant results. "
        ],
        "image_footnote": [],
        "page_idx": 247
    },
    {
        "type": "text",
        "text": "Reranking ",
        "text_level": 1,
        "page_idx": 247
    },
    {
        "type": "text",
        "text": "Search systems are often pipelines of multiple steps. A reranking language model is one of these steps and is tasked with scoring the relevance of a subset of results against the query; the order of results is then changed based on these scores. Figure 8-2 shows how rerankers are different from dense retrieval in that they take an additional input: a set of search results from a previous step in the search pipeline. ",
        "page_idx": 247
    },
    {
        "type": "image",
        "img_path": "images/f0f3dea6cdf050ea0a5be5968bd42fc273da324acc9bc78b7c7150c94a097739.jpg",
        "image_caption": [
            "Figure 8-2. Rerankers, the second key type of semantic search, take a search query and a collection of results, and reorder them by relevance, often resulting in vastly improved results. "
        ],
        "image_footnote": [],
        "page_idx": 248
    },
    {
        "type": "text",
        "text": "RAG ",
        "text_level": 1,
        "page_idx": 248
    },
    {
        "type": "text",
        "text": "The growing LLM capability of text generation led to a new type of search systems that include a model that generates an answer in response to a query. Figure 8-3 shows an example of such a generative search system. ",
        "page_idx": 248
    },
    {
        "type": "text",
        "text": "Generative search is a subset of a broader type of category of systems better called RAG systems. These are text generation systems that incorporate search capabilities to reduce hallucinations, increase factuality, and/or ground the gen‐ eration model on a specific dataset. ",
        "page_idx": 248
    },
    {
        "type": "image",
        "img_path": "images/fd42be46f4e8426028a36f157e36ac7af2652cfdd8cf5838f0448ddfb8a08510.jpg",
        "image_caption": [
            "Figure 8-3. A RAG system formulates an answer to a question and (preferably) cites its information sources. "
        ],
        "image_footnote": [],
        "page_idx": 248
    },
    {
        "type": "text",
        "text": "The rest of the chapter covers these three types of systems in more detail. While these are the major categories, they are not the only LLM applications in the domain of search. ",
        "page_idx": 248
    },
    {
        "type": "text",
        "text": "Semantic Search with Language Models ",
        "text_level": 1,
        "page_idx": 249
    },
    {
        "type": "text",
        "text": "Let’s now dive into more detail on the major categories of systems that can upgrade the search capabilities of our language models. We’ll start with dense retrieval and then move on through reranking and RAG. ",
        "page_idx": 249
    },
    {
        "type": "text",
        "text": "Dense Retrieval ",
        "text_level": 1,
        "page_idx": 249
    },
    {
        "type": "text",
        "text": "Recall that embeddings turn text into numeric representations. Those can be thought of as points in space, as we can see in Figure 8-4. Points that are close together mean that the text they represent is similar. So in this example, text 1 and text 2 are more similar to each other (because they are near each other) than text 3 (because it’s farther away). ",
        "page_idx": 249
    },
    {
        "type": "image",
        "img_path": "images/a1743747801cfd67862b6acc13ca870ed0c5bf3e3fe750dd6fd08be14b197f1d.jpg",
        "image_caption": [
            "Figure 8-4. The intuition of embeddings: each text is a point and texts with similar meaning are close to each other. "
        ],
        "image_footnote": [],
        "page_idx": 249
    },
    {
        "type": "text",
        "text": "This is the property that is used to build search systems. In this scenario, when a user enters a search query, we embed the query, thus projecting it into the same space as our text archive. Then we simply find the nearest documents to the query in that space, and those would be the search results (Figure 8-5). ",
        "page_idx": 249
    },
    {
        "type": "image",
        "img_path": "images/e9b9dfc8d3c488049df4aa80d13a7cc70212ccb452d4c699e1e1773239021246.jpg",
        "image_caption": [
            "Figure 8-5. Dense retrieval relies on the property that search queries will be close to their relevant results. "
        ],
        "image_footnote": [],
        "page_idx": 250
    },
    {
        "type": "text",
        "text": "Judging by the distances in Figure 8-5, “text $2 ^ { \\mathfrak { v } }$ is the best result for this query, followed by “text 1.” Two questions could arise here, however: ",
        "page_idx": 250
    },
    {
        "type": "text",
        "text": "• Should text 3 even be returned as a result? That’s a decision for you, the system designer. It’s sometimes desirable to have a max threshold of similarity score to filter out irrelevant results (in case the corpus has no relevant results for the query). • Are a query and its best result semantically similar? Not always. This is why language models need to be trained on question-answer pairs to become better at retrieval. This process is explained in more detail in Chapter 10. ",
        "page_idx": 250
    },
    {
        "type": "text",
        "text": "Figure 8-6 shows how we chunk a document before proceeding to embed each chunk. Those embedding vectors are then stored in the vector database and are ready for retrieval. ",
        "page_idx": 250
    },
    {
        "type": "image",
        "img_path": "images/ec1783752746a642dc8329d3dd0b0b245e54d8018dd8bb0d1b5d7552cf6ef0ce.jpg",
        "image_caption": [
            "Figure 8-6. Convert some external knowledge base to a vector database. We can then query this vector database for information about the knowledge base. "
        ],
        "image_footnote": [],
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "Dense retrieval example ",
        "text_level": 1,
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "Let’s take a look at a dense retrieval example by using Cohere to search the Wikipedia page for the film Interstellar. In this example, we will do the following: ",
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "1. Get the text we want to make searchable and apply some light processing to   \nchunk it into sentences.   \n2. Embed the sentences.   \n3. Build the search index.   \n4. Search and see the results. ",
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "Get your Cohere API key by signing up at https://oreil.ly/GxrQ1. Paste it in the following code. You will not have to pay anything to run through this example. ",
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "Let’s import the libraries we’ll need: ",
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "import cohere   \nimport numpy as np   \nimport pandas as pd   \nfrom tqdm import tqdm   \n# Paste your API key here. Remember to not share publicly   \napi_key $=$ ''   \n# Create and retrieve a Cohere API key from os.cohere.ai   \nco $=$ cohere.Client(api_key) ",
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "Getting the text archive and chunking it. Let’s use the first section of the Wikipedia arti‐ cle on the film Interstellar. We’ll get the text, then break it into sentences: ",
        "page_idx": 251
    },
    {
        "type": "text",
        "text": "text $=$ ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "Interstellar is a 2014 epic science fiction film co-written, directed, and pro duced by Christopher Nolan.   \nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine.   \nSet in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind. Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007.   \nCaltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar.   \nCinematographer Hoyte van Hoytema shot it on $3 5 \\ \\mathrm { m m }$ movie film in the Panavision anamorphic format and IMAX $7 6 m m$ .   \nPrincipal photography began in late 2013 and took place in Alberta, Iceland, and Los Angeles.   \nInterstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects. Interstellar premiered on October 26, 2014, in Los Angeles.   \nIn the United States, it was first released on film stock, expanding to venues using digital projectors.   \nThe film had a worldwide gross over $\\$ 677$ million (and $\\$ 773$ million with subse quent re-releases), making it the tenth-highest grossing film of 2014.   \nIt received acclaim for its performances, direction, screenplay, musical score, visual effects, ambition, themes, and emotional weight.   \nIt has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics. Since its premiere, Interstellar gained a cult following,[5] and now is regarded by many sci-fi experts as one of the best science-fiction films of all time.   \nInterstellar was nominated for five awards at the 87th Academy Awards, winning Best Visual Effects, and received numerous other accolades\"\"\" ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "# Split into a list of sentences texts $=$ text.split('.') ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "# Clean up to remove empty spaces and new lines texts $=$ [t.strip(' \\n') for t in texts] ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "Embedding the text chunks. Let’s now embed the texts. We’ll send them to the Cohere API, and get back a vector for each text: ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "# Get the embeddings   \nresponse $=$ co.embed( texts $\\mathbf { \\equiv }$ texts, input_type $\\iota =$ \"search_document\",   \n).embeddings ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "embeds $=$ np.array(response) print(embeds.shape) ",
        "page_idx": 252
    },
    {
        "type": "text",
        "text": "This outputs (15, 4096), which indicates that we have 15 vectors, each one of size 4,096. ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "Building the search index. Before we can search, we need to build a search index. An index stores the embeddings and is optimized to quickly retrieve the nearest neighbors even if we have a very large number of points: ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "import faiss dim $=$ embeds.shape[1] index $=$ faiss.IndexFlatL2(dim) print(index.is_trained) index.add(np.float32(embeds)) ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "Search the index. We can now search the dataset using any query we want. We simply embed the query and present its embedding to the index, which will retrieve the most similar sentence from the Wikipedia article. ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "Let’s define our search function: ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "def search(query, number_of_results $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ ): ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "# 1. Get the query's embedding query_embed $=$ co.embed(texts $, =$ [query], input_type $\\Bumpeq$ \"search_query\",).embeddings[0] ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "# 2. Retrieve the nearest neighbors distances , similar_item_ids $=$ index.search(np.float32([query_embed]), num ber_of_results) ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "# 3. Format the results   \ntexts_np $=$ np.array(texts) # Convert texts list to numpy for easier indexing   \nresults $=$ pd.DataFrame(data $=$ {'texts': texts_np[similar_item_ids[0]], 'distance': distances[0]}) ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "# 4. Print and return the results print(f\"Query:'{query}'\\nNearest neighbors:\") return results ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "We are now ready to write a query and search the texts! ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "query $=$ \"how precise was the science\"   \nresults $=$ search(query)   \nresults ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "This produces the following output: ",
        "page_idx": 253
    },
    {
        "type": "text",
        "text": "Query: 'how precise was the science' Nearest neighbors: ",
        "page_idx": 254
    },
    {
        "type": "table",
        "img_path": "images/2d6277d611dfbe5abbe1fcb4b64cf74502ac1006d0aa5a01db6c83c22088241e.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>texts</td><td></td><td>distance</td></tr><tr><td>0</td><td>It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics</td><td>10757.379883</td></tr><tr><td>1.</td><td>Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer,actedasascientific consultant,and wrotea tie-in book,The Science of Interstellar</td><td>11566.131836</td></tr><tr><td>2.</td><td>Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects</td><td>11922.833008</td></tr></table>",
        "page_idx": 254
    },
    {
        "type": "text",
        "text": "The first result has the least distance, and so is the most similar to the query. Looking at it, it answers the question perfectly. Notice that this wouldn’t have been possible if we were only doing keyword search because the top result did not include the same keywords in the query. ",
        "page_idx": 254
    },
    {
        "type": "text",
        "text": "We can actually verify that by defining a keyword search function to compare the two. We’ll use the BM25 algorithm, which is one of the leading lexical search meth‐ ods. See this notebook for the source of these code snippets: ",
        "page_idx": 254
    },
    {
        "type": "text",
        "text": "from rank_bm25 import BM25Okapi   \nfrom sklearn.feature_extraction import _stop_words   \nimport string   \ndef bm25_tokenizer(text): tokenized_doc $=$ [] for token in text.lower().split(): token $=$ token.strip(string.punctuation) if len(token) $> ~ \\Theta$ and token not in _stop_words.ENGLISH_STOP_WORDS: tokenized_doc.append(token) return tokenized_doc   \ntokenized_corpus $= \\ [ ]$   \nfor passage in tqdm(texts): tokenized_corpus.append(bm25_tokenizer(passage))   \nbm25 $=$ BM25Okapi(tokenized_corpus)   \ndef keyword_search(query, top_ $\\mathbf { k } = \\mathbf { \\Gamma }$ , num_candidates $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ ): print(\"Input question:\", query) ##### BM25 search (lexical search) ##### bm25_scores $=$ bm25.get_scores(bm25_tokenizer(query)) top_n $=$ np.argpartition(bm25_scores, -num_candidates)[-num_candidates:] bm25_hits $=$ [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n] bm25_hits $=$ sorted(bm25_hits, key=lambda x: x['score'], reverse=True) ",
        "page_idx": 254
    },
    {
        "type": "text",
        "text": "print(f\"Top-3 lexical search (BM25) hits\") for hit in bm25_hits[0:top_k]: print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['cor pus_id']].replace(\"\\n\", \" \"))) ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "Now when we search for the same query, we get a different set of results from the dense retrieval search: ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "keyword_search(query $=$ \"how precise was the science\") ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "Results: ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "Input question: how precise was the science   \nTop-3 lexical search (BM25) hits 1.789 Interstellar is a 2014 epic science fiction film co-written, direc  \nted, and produced by Christopher Nolan 1.373 Caltech theoretical physicist and 2017 Nobel laureate in Phys  \nics[4] Kip Thorne was an executive producer, acted as a scientific consultant,   \nand wrote a tie-in book, The Science of Interstellar 0.000 It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain,   \nBill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "Note that the first result does not really answer the question despite it sharing the word “science” with the query. In the next section, we’ll see how adding a reranker can improve this search system. But before that, let’s complete our overview of dense retrieval by looking at its caveats and go over some methods of breaking down texts into chunks. ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "Caveats of dense retrieval ",
        "text_level": 1,
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "It’s useful to be aware of some of the drawbacks of dense retrieval and how to address them. What happens, for example, if the texts don’t contain the answer? We still get results and their distances. For example: ",
        "page_idx": 255
    },
    {
        "type": "table",
        "img_path": "images/4abb68d2fe7d420828974ba7af9f05637259f3fa602945fbe5798109002386db.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td colspan=\"2\">Query:&#x27;What is the mass of the moon?&#x27; Nearest neighbors:</td></tr><tr><td>texts</td><td>distance</td></tr><tr><td>0 The flm had a worldwide gross over $677 milon (and $773 million with subsequent re-releases), making it the tenth-highest grossing film of 2014</td><td>1.298275</td></tr><tr><td>It ha also received prase frommany astronomersfor tsscientficaccuacyandportryal of theoretical1.324389</td><td></td></tr><tr><td>astrophysics 2 Cinematographer Hoyte van Hoytema shot it on 35 mm movie flm in the Panavision anamorphic format and IMAx 70 mm</td><td>1.328375</td></tr></table>",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "In cases like this, one possible heuristic is to set a threshold level—a maximum distance for relevance, for example. A lot of search systems present the user with the best info they can get and leave it up to the user to decide if it’s relevant or not. ",
        "page_idx": 255
    },
    {
        "type": "text",
        "text": "Tracking the information of whether the user clicked on a result (and were satisfied by it) can improve future versions of the search system. ",
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "Another caveat of dense retrieval is when a user wants to find an exact match for a specific phrase. That’s a case that’s perfect for keyword matching. That’s one reason why hybrid search, which includes both semantic search and keyword search, is advised instead of relying solely on dense retrieval. ",
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "Dense retrieval systems also find it challenging to work properly in domains other than the ones that they were trained on. So, for example, if you train a retrieval model on internet and Wikipedia data, and then deploy it on legal texts (without having enough legal data as part of the training set), the model will not work as well in that legal domain. ",
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "The final thing we’d like to point out is that this is a case where each sentence contained a piece of information, and we showed queries that specifically ask for that information. What about questions whose answers span multiple sentences? This highlights one of the important design parameters of dense retrieval systems: what is the best way to chunk long texts? And why do we need to chunk them in the first place? ",
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "Chunking long texts ",
        "text_level": 1,
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "One limitation of Transformer language models is that they are limited in context sizes, meaning we cannot feed them very long texts that go above the number of words or tokens that the model supports. So how do we embed long texts? ",
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "There are several possible ways, and two possible approaches shown in Figure 8-7 include indexing one vector per document and indexing multiple vectors per document. ",
        "page_idx": 256
    },
    {
        "type": "image",
        "img_path": "images/9c580e3c42d1714831c1f140dafacdf807df74a751ad25fc5e25e3d572881fe3.jpg",
        "image_caption": [
            "Figure 8-7. It’s possible to create one vector representing an entire document, but it’s bet‐ ter for longer documents to be split into smaller chunks that get their own embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 256
    },
    {
        "type": "text",
        "text": "One vector per document. In this approach, we use a single vector to represent the whole document. The possibilities here include: ",
        "page_idx": 257
    },
    {
        "type": "text",
        "text": "• Embedding only a representative part of the document and ignoring the rest of the text. This may mean embedding only the title, or only the beginning of the document. This is useful to get quickly started with building a demo but it leaves a lot of information unindexed and therefore unsearchable. As an approach, it may work better for documents where the beginning captures the main points of a document (think: Wikipedia article). But it’s really not the best approach for a real system because a lot of information would be left out of the index and would be unsearchable. • Embedding the document in chunks, embedding those chunks, and then aggre‐ gating those chunks into a single vector. The usual method of aggregation here is to average those vectors. A downside of this approach is that it results in a highly compressed vector that loses a lot of the information in the document. ",
        "page_idx": 257
    },
    {
        "type": "text",
        "text": "This approach can satisfy some information needs, but not others. A lot of the time, a search is for a specific piece of information contained in an article, which is better captured if the concept had its own vector. ",
        "page_idx": 257
    },
    {
        "type": "text",
        "text": "Multiple vectors per document. In this approach, we chunk the document into smaller pieces, and embed those chunks. Our search index then becomes that of chunk embeddings, not entire document embeddings. Figure 8-8 shows a number of possi‐ ble text chunking approaches. ",
        "page_idx": 257
    },
    {
        "type": "image",
        "img_path": "images/4c471e00148ace74824a9301374163ca19b0ce003ab649e76880da6031277cb7.jpg",
        "image_caption": [
            "Figure 8-8. Several chunking methods and their effects on the input text. Overlapping chunks can be important to prevent the absence of context. "
        ],
        "image_footnote": [],
        "page_idx": 257
    },
    {
        "type": "text",
        "text": "The chunking approach is better because it has full coverage of the text and because the vectors tend to capture individual concepts inside the text. This leads to a more expressive search index. Figure 8-9 shows a number of possible approaches. ",
        "page_idx": 258
    },
    {
        "type": "image",
        "img_path": "images/0d8326cbfac2f083421b17fe134d5d35053e14c9b2a6cf2b18bb04382e07f7f9.jpg",
        "image_caption": [
            "Figure 8-9. A number of possible options for chunking a document for embedding. "
        ],
        "image_footnote": [],
        "page_idx": 258
    },
    {
        "type": "text",
        "text": "The best way of chunking a long text will depend on the types of texts and queries your system anticipates. Approaches include: ",
        "page_idx": 258
    },
    {
        "type": "text",
        "text": "• Each sentence is a chunk. The issue here is this could be too granular and the vectors don’t capture enough of the context.   \n• Each paragraph is a chunk. This is great if the text is made up of short para‐ graphs. Otherwise, it may be that every 3–8 sentences is a chunk.   \n• Some chunks derive a lot of their meaning from the text around them. So we can incorporate some context via: — Adding the title of the document to the chunk. — Adding some of the text before and after them to the chunk. This way, the chunks can overlap so they include some surrounding text that also appears in adjacent chunks. This is what we can see in Figure 8-10. ",
        "page_idx": 258
    },
    {
        "type": "text",
        "text": "Expect more chunking strategies to arise as the field develops—some of which may even use LLMs to dynamically split a text into meaningful chunks. ",
        "page_idx": 258
    },
    {
        "type": "image",
        "img_path": "images/f4dce5e596ef68c9b5b2d62ace7758036a2d724589a4ccd4d29bd5be38599c0d.jpg",
        "image_caption": [
            "Figure 8-10. Chunking the text into overlapping segments is one strategy to retain more of the context around different segments. "
        ],
        "image_footnote": [],
        "page_idx": 259
    },
    {
        "type": "text",
        "text": "Nearest neighbor search versus vector databases ",
        "text_level": 1,
        "page_idx": 259
    },
    {
        "type": "text",
        "text": "Once the query is embedded, we need to find the nearest vectors to it from our text archive as we can see in Figure 8-11. The most straightforward way to find the nearest neighbors is to calculate the distances between the query and the archive. That can easily be done with NumPy and is a reasonable approach if you have thousands or tens of thousands of vectors in your archive. ",
        "page_idx": 259
    },
    {
        "type": "image",
        "img_path": "images/4cdc4f539bdad8a45193cfd33e00cb78b4edcc6b403ffc6da1ba77d48a73e125.jpg",
        "image_caption": [
            "Figure 8-11. As we saw in Chapter 3, we can compare embeddings to quickly find the most similar documents to a query. "
        ],
        "image_footnote": [],
        "page_idx": 259
    },
    {
        "type": "text",
        "text": "As you scale beyond to the millions of vectors, an optimized approach for retrieval is to rely on approximate nearest neighbor search libraries like Annoy or FAISS. These allow you to retrieve results from massive indexes in milliseconds and some of them can improve their performance by utilizing GPUs and scaling to clusters of machines to serve very large indices. ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "Another class of vector retrieval systems are vector databases like Weaviate or Pine‐ cone. A vector database allows you to add or delete vectors without having to rebuild the index. They also provide ways to filter your search or customize it in ways beyond merely vector distances. ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "Fine-tuning embedding models for dense retrieval ",
        "text_level": 1,
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "Just as we discussed in Chapter 4 on text classification, we can improve the perfor‐ mance of an LLM on a task using fine-tuning. As in that case, retrieval needs to optimize text embeddings and not simply token embeddings. The process for this fine-tuning is to get training data composed of queries and relevant results. ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "Let’s look at one example from our dataset, the sentence “Interstellar premiered on October 26, 2014, in Los Angeles.” Two possible queries where this is a relevant result are: ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "• Relevant query 1: “Interstellar release date” • Relevant query 2: “When did Interstellar premier” ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "The fine-tuning process aims to make the embeddings of these queries close to the embedding of the resulting sentence. It also needs to see negative examples of queries that are not relevant to the sentence, for example: ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "• Irrelevant query: “Interstellar cast” ",
        "page_idx": 260
    },
    {
        "type": "text",
        "text": "With these examples, we now have three pairs—two positive pairs and one negative pair. Let’s assume, as we can see in Figure 8-12, that before fine-tuning, all three queries have the same distance from the result document. That’s not far-fetched because they all talk about Interstellar. ",
        "page_idx": 260
    },
    {
        "type": "image",
        "img_path": "images/bca6220c405bf1117b82511b9ff7914f9eba86479e46f12d4c1ed41ad12527ba.jpg",
        "image_caption": [
            "Figure 8-12. Before fine-tuning, the embeddings of both relevant and irrelevant queries may be close to a particular document. "
        ],
        "image_footnote": [],
        "page_idx": 261
    },
    {
        "type": "text",
        "text": "The fine-tuning step works to make the relevant queries closer to the document and at the same time make irrelevant queries farther from the document. We can see this effect in Figure 8-13. ",
        "page_idx": 261
    },
    {
        "type": "image",
        "img_path": "images/50d79a1e83bc31dac31a89134d5eb0f49971e5132289ac0d74efcc8302dc0030.jpg",
        "image_caption": [
            "Figure 8-13. After the fine-tuning process, the text embedding model becomes better at this search task by incorporating how we define relevance on our dataset using the examples we provided of relevant and irrelevant documents. "
        ],
        "image_footnote": [],
        "page_idx": 261
    },
    {
        "type": "text",
        "text": "Reranking ",
        "text_level": 1,
        "page_idx": 261
    },
    {
        "type": "text",
        "text": "A lot of organizations have already built search systems. For those organizations, an easier way to incorporate language models is as a final step inside their search pipeline. This step is tasked with changing the order of the search results based on relevance to the search query. This one step can vastly improve search results and it’s in fact what Microsoft Bing added to achieve the improvements to search results using BERT-like models. Figure 8-14 shows the structure of a rerank search system serving as the second stage in a two-stage search system. ",
        "page_idx": 261
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 262
    },
    {
        "type": "image",
        "img_path": "images/8d5d6228d61534d07a267c859c6215862294a9eb29f287f2ea841fa737bceb64.jpg",
        "image_caption": [
            "Figure 8-14. LLM rerankers operate as part of a search pipeline with the goal of reordering a number of shortlisted search results by relevance. "
        ],
        "image_footnote": [],
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "Reranking example ",
        "text_level": 1,
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "A reranker takes in the search query and a number of search results, and returns the optimal ordering of these documents so the most relevant ones to the query are higher in ranking. Cohere’s Rerank endpoint is a simple way to start using a first reranker. We simply pass it the query and texts and get the results back. We don’t need to train or tune it: ",
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "query $=$ \"how precise was the science\"   \nresults $=$ co.rerank(query=query, documents ${ } = { }$ texts, top_ $\\harpoonright$ , return_docu   \nments $\\varprojlim \\varprojlim 2$ True)   \nresults.results ",
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "We can print these results: ",
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "for idx, result in enumerate(results.results): print(idx, result.relevance_score , result.document.text) ",
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "Output: ",
        "text_level": 1,
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "0 0.1698185 It has also received praise from many astronomers for its scientific accuracy and portrayal of theoretical astrophysics   \n1 0.07004896 The film had a worldwide gross over $\\$ 677$ million (and $\\$ 773$ million with subsequent re-releases), making it the tenth-highest grossing film of 2014   \n2 0.0043994132 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar ",
        "page_idx": 262
    },
    {
        "type": "text",
        "text": "This shows the reranker is much more confident about the first result, assigning it a relevance score of 0.16, while the other results are scored much lower in relevance. ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "In this basic example, we passed our reranker all 15 of our documents. More often, however, our index would have thousands or millions of entries, and we need to shortlist, say one hundred or one thousand results and then present those to the reranker. This shortlisting step is called the first stage of the search pipeline. ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "The first-stage retriever can be keyword search, dense retrieval, or better yet—hybrid search that uses both of them. We can revisit our previous example to see how adding a reranker after a keyword search system improves its performance. ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "Let’s tweak our keyword search function so it retrieves a list of the top 10 results using keyword search, then use rerank to choose the top 3 results from those 10: ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "def keyword_and_reranking_search(query, top_ $k = 3$ , num_candidates $\\begin{array} { r l } { \\mathit { \\Pi } } & { { } = } \\\\ { \\mathit { \\Pi } } & { { } = } \\end{array} .$ ): print(\"Input question:\", query) ##### BM25 search (lexical search) ##### bm25_scores $=$ bm25.get_scores(bm25_tokenizer(query)) top_n $=$ np.argpartition(bm25_scores, -num_candidates)[-num_candidates:] bm25_hits $=$ [{'corpus_id': idx, 'score': bm25_scores[idx]} for idx in top_n] bm25_hits $=$ sorted(bm25_hits, key=lambda x: x['score'], reverse=True) print(f\"Top-3 lexical search (BM25) hits\") for hit in bm25_hits[0:top_k]: print(\"\\t{:.3f}\\t{}\".format(hit['score'], texts[hit['cor   \npus_id']].replace(\"\\n\", \" \"))) #Add re-ranking docs $=$ [texts[hit['corpus_id']] for hit in bm25_hits] print(f\"\\nTop-3 hits by rank-API ({len(bm25_hits)} BM25 hits re-ranked)\") results $=$ co.rerank(query=query, documents $\\mathbf { \\Psi } _ { 1 } =$ docs, top_n=top_k, return_docu   \nments=True) # print(results.results) for hit in results.results: # print(hit) print(\"\\t{:.3f}\\t{}\".format(hit.relevance_score, hit.docu   \nment.text.replace(\"\\n\", \" \"))) ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "Now we can send our query and check the results of keyword search and then the result of keyword search shortlisting its top 10 results, then pass them on to the reranker: ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "keyword_and_reranking_search(query $=$ \"how precise was the science\") ",
        "page_idx": 263
    },
    {
        "type": "text",
        "text": "Results: ",
        "text_level": 1,
        "page_idx": 264
    },
    {
        "type": "text",
        "text": "Input question: how precise was the science   \nTop-3 lexical search (BM25) hits   \n1.789 Interstellar is a 2014 epic science fiction film co-written, directed, and produced by Christopher Nolan   \n1.373 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar   \n0.000 Interstellar uses extensive practical and miniature effects and the company Double Negative created additional digital effects   \nTop-3 hits by rank-API (10 BM25 hits re-ranked)   \n0.004 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive producer, acted as a scientific consultant, and wrote a tie-in book, The Science of Interstellar   \n0.004 Set in a dystopian future where humanity is struggling to survive, the film follows a group of astronauts who travel through a wormhole near Saturn in search of a new home for mankind   \n0.003 Brothers Christopher and Jonathan Nolan wrote the screenplay, which had its origins in a script Jonathan developed in 2007 ",
        "page_idx": 264
    },
    {
        "type": "text",
        "text": "We see that keyword search assigns scores to only two results that share some of the keywords. In the second set of results, the reranker elevates the second result appropriately as the most relevant result for the query. This is a toy example that gives us a glimpse of the effect, but in practice, such a pipeline significantly improves search quality. On a multilingual benchmark like MIRACL, a reranker can boost performance from 36.5 to 62.8, measured as nDCG $@ 1 0$ (more on evaluation later in this chapter). ",
        "page_idx": 264
    },
    {
        "type": "text",
        "text": "Open source retrieval and reranking with sentence transformers ",
        "text_level": 1,
        "page_idx": 264
    },
    {
        "type": "text",
        "text": "If you want to locally set up retrieval and reranking on your own machine, then you can use the Sentence Transformers library. Refer to the documentation at https:// oreil.ly/jJOhV for setup. Check the “Retrieve & Re-Rank” section for instructions and code examples for how to conduct these steps in the library. ",
        "page_idx": 264
    },
    {
        "type": "text",
        "text": "How reranking models work ",
        "text_level": 1,
        "page_idx": 265
    },
    {
        "type": "text",
        "text": "One popular way of building LLM search rerankers is to present the query and each result to an LLM working as a cross-encoder. This means that a query and possible result are presented to the model at the same time allowing the model to view both these texts before it assigns a relevance score, as we can see in Figure 8-15. All of the documents are processed simultaneously as a batch yet each document is evaluated against the query independently. The scores then determine the new order of the results. This method is described in more detail in a paper titled “Multi-stage document ranking with BERT” and is sometimes referred to as monoBERT. ",
        "page_idx": 265
    },
    {
        "type": "image",
        "img_path": "images/51c2f0a43488095d340c3edf1d1c001538f0d5079104511c1304195a5528f6bc.jpg",
        "image_caption": [
            "Figure 8-15. A reranker assigns a relevance score to each document by looking at the document and the query at the same time. "
        ],
        "image_footnote": [],
        "page_idx": 265
    },
    {
        "type": "text",
        "text": "This formulation of search as relevance scoring basically boils down to being a classification problem. Given those inputs, the model outputs a score from 0–1 where 0 is irrelevant and 1 is highly relevant. This should be familiar from our classification discussions in Chapter 4. ",
        "page_idx": 265
    },
    {
        "type": "text",
        "text": "To learn more about the development of using LLMs for search, \"Pretrained trans‐ formers for text tanking: BERT and beyond\" is a highly recommended look at the developments of these models until about 2021. ",
        "page_idx": 265
    },
    {
        "type": "text",
        "text": "Retrieval Evaluation Metrics ",
        "text_level": 1,
        "page_idx": 265
    },
    {
        "type": "text",
        "text": "Semantic search is evaluated using metrics from the Information Retrieval (IR) field.   \nLet’s discuss one of these popular metrics: mean average precision (MAP). ",
        "page_idx": 265
    },
    {
        "type": "text",
        "text": "Evaluating search systems needs three major components: a text archive, a set of queries, and relevance judgments indicating which documents are relevant for each query. We see these components in Figure 8-16. ",
        "page_idx": 265
    },
    {
        "type": "image",
        "img_path": "images/457e261628f6ac1a47662bde131437b87dc9dea25b0aeaefd34396f2d2fbecf4.jpg",
        "image_caption": [
            "Figure 8-16. To evaluate search systems, we need a test suite including queries and relevance judgments indicating which documents in our archive are relevant for each query. "
        ],
        "image_footnote": [],
        "page_idx": 266
    },
    {
        "type": "text",
        "text": "Using this test suite, we can proceed to explore evaluating search systems. Let’s start with a simple example. Let’s assume we pass query 1 to two different search systems. And get two sets of results. Say we limit the number of results to three, as we can see in Figure 8-17. ",
        "page_idx": 266
    },
    {
        "type": "image",
        "img_path": "images/3f3e21983c639c92125aebe6772542ec767c630eca3abbd6082bc71536eaa8ae.jpg",
        "image_caption": [
            "Figure 8-17. To compare two search systems, we pass the same query from our test suite to both systems and look at their top results. "
        ],
        "image_footnote": [],
        "page_idx": 266
    },
    {
        "type": "text",
        "text": "To tell which is a better system, we turn to the relevance judgments that we have for the query. Figure 8-18 shows which of the returned results are relevant. ",
        "page_idx": 266
    },
    {
        "type": "image",
        "img_path": "images/104824366b9bf133333d09ff446016df94f4efca3b31fa8ff5b947ef33bd1437.jpg",
        "image_caption": [
            "Figure 8-18. Looking at the relevance judgments from our test suite, we can see that system 1 did a better job than system 2. "
        ],
        "image_footnote": [],
        "page_idx": 267
    },
    {
        "type": "text",
        "text": "This shows us a clear case where system 1 is better than system 2. Intuitively, we may just count how many relevant results each system retrieved. System 1 got two out of three correct, and system 2 got only one out of three correct. But what about a case like Figure 8-19 where both systems only get one relevant result out of three, but they’re in different positions? ",
        "page_idx": 267
    },
    {
        "type": "image",
        "img_path": "images/dd96673b06494286470d26405ec1d619c949e920c13c36b8a9f29f618fcddc80.jpg",
        "image_caption": [
            "Figure 8-19. We need a scoring system that rewards system 1 for assigning a high position to a relevant result—even though both systems retrieved only one relevant result in their top three results. "
        ],
        "image_footnote": [],
        "page_idx": 267
    },
    {
        "type": "text",
        "text": "In this case, we can intuit that system 1 did a better job than system 2 because the result in the first position (the most important position) is correct. But how can we assign a number or score to how much better that result is? Mean average precision is a measure that is able to quantify this distinction. ",
        "page_idx": 267
    },
    {
        "type": "text",
        "text": "One common way to assign numeric scores in this scenario is average precision, which evaluates system 1’s result for the query to be 1 and system 2’s to be 0.3. So let’s see how average precision is calculated to evaluate one set of results, and then how it’s aggregated to evaluate a system across all the queries in the test suite. ",
        "page_idx": 268
    },
    {
        "type": "text",
        "text": "Scoring a single query with average precision ",
        "text_level": 1,
        "page_idx": 268
    },
    {
        "type": "text",
        "text": "To score a search system on this query, we can focus on scoring the relevant docu‐ ments. Let’s start by looking at a query that only has one relevant document in the test suite. ",
        "page_idx": 268
    },
    {
        "type": "text",
        "text": "The first one is easy: the search system placed the relevant result (the only available one for this query) at the top. This gets the system the perfect score of 1. Figure 8-20 shows this calculation: looking at the first position, we have a relevant result leading to a precision at position 1 of 1.0 (calculated as the number of relevant results at position 1, divided by the position we’re currently looking at). ",
        "page_idx": 268
    },
    {
        "type": "image",
        "img_path": "images/53b4ac896fb7e4050f94ae4bad7d08b65a765f49d0a7bf5f0f908ebae940dda5.jpg",
        "image_caption": [
            "Figure 8-20. To calculate mean average precision, we start by calculating precision at each position, starting with position 1. "
        ],
        "image_footnote": [],
        "page_idx": 268
    },
    {
        "type": "text",
        "text": "Since we’re only scoring relevant documents we can ignore the scores of nonrelevant documents and stop our calculation here. What if the system actually placed the only relevant result at the third position, however? How would that affect the score? Figure 8-21 shows how that results in a penalty. ",
        "page_idx": 268
    },
    {
        "type": "image",
        "img_path": "images/212c75a55b3a89b9e99403a1aa54424a90809002084c0fe7306eca40497291be.jpg",
        "image_caption": [
            "Figure 8-21. If the system places nonrelevant documents ahead of a relevant document, its precision score is penalized. "
        ],
        "image_footnote": [],
        "page_idx": 269
    },
    {
        "type": "text",
        "text": "Let’s now look at a query with more than one relevant document. Figure 8-22 shows that calculation and how averaging now comes into the picture. ",
        "page_idx": 269
    },
    {
        "type": "image",
        "img_path": "images/408e13e96c40de045645fa9f63d8ff0e59184a59cef98756545a5bb6fb43a748.jpg",
        "image_caption": [
            "Figure 8-22. Average precision of a document with multiple relevant documents consid‐ ers the precision at $k$ results of all the relevant documents. "
        ],
        "image_footnote": [],
        "page_idx": 269
    },
    {
        "type": "text",
        "text": "Scoring across multiple queries with mean average precision ",
        "text_level": 1,
        "page_idx": 269
    },
    {
        "type": "text",
        "text": "Now that we’re familiar with precision at k and average precision, we can extend this knowledge to a metric that can score a search system against all the queries in our test suite. That metric is called mean average precision. Figure 8-23 shows how to calculate this metric by taking the mean of the average precisions of each query. ",
        "page_idx": 269
    },
    {
        "type": "image",
        "img_path": "images/e370d1973404fc314a46e1c1ba2162984eb3bc4619a2bf17ee3d924e31822450.jpg",
        "image_caption": [
            "Figure 8-23. The mean average precision takes into consideration the average precision score of a system for every query in the test suite. By averaging them, it produces a single metric that we can use to compare a search system against another. "
        ],
        "image_footnote": [],
        "page_idx": 270
    },
    {
        "type": "text",
        "text": "You may be wondering why the same operation is called “mean” and “average.” It’s likely an aesthetic choice because MAP sounds better than average average precision. ",
        "page_idx": 270
    },
    {
        "type": "text",
        "text": "Now we have a single metric that we can use to compare different systems. If you want to learn more about evaluation metrics, see the “Evaluation in Information Retrieval” chapter of Introduction to Information Retrieval (Cambridge University Press) by Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze. ",
        "page_idx": 270
    },
    {
        "type": "text",
        "text": "In addition to mean average precision, another metric commonly used for search systems is normalized discounted cumulative gain (nDCG), which is more nuanced in that the relevance of documents is not binary (relevant versus not relevant) and one document can be labeled as more relevant than another in the test suite and scoring mechanism. ",
        "page_idx": 270
    },
    {
        "type": "text",
        "text": "Retrieval-Augmented Generation (RAG) ",
        "text_level": 1,
        "page_idx": 270
    },
    {
        "type": "text",
        "text": "The mass adoption of LLMs quickly led to people asking them questions and expecting factual answers. While the models can answer some questions correctly, they also confidently answer lots of questions incorrectly. The leading method the industry turned to remedy this behavior is RAG, described in the paper “RetrievalAugmented Generation for Knowledge-Intensive NLP Tasks” (2020)1 and illustrated in Figure 8-24. ",
        "page_idx": 270
    },
    {
        "type": "image",
        "img_path": "images/c5b242c007ebb673a1a3f72d53490c988aa888a2e2792ce8d156f721ed29285a.jpg",
        "image_caption": [
            "Figure 8-24. A basic RAG pipeline is made up of a search step followed by a grounded generation step where the LLM is prompted with the question and the information retrieved from the search step. "
        ],
        "image_footnote": [],
        "page_idx": 271
    },
    {
        "type": "text",
        "text": "RAG systems incorporate search capabilities in addition to generation capabilities. They can be seen as an improvement to generation systems because they reduce their hallucinations and improve their factuality. They also enable use cases of “chat with my data” that consumers and companies can use to ground an LLM on internal company data, or a specific data source of interest (e.g., chatting with a book). ",
        "page_idx": 271
    },
    {
        "type": "text",
        "text": "This also extends to search systems. More search engines are incorporating an LLM to summarize results or answer questions submitted to the search engine. Examples include Perplexity, Microsoft Bing AI, and Google Gemini. ",
        "page_idx": 271
    },
    {
        "type": "text",
        "text": "From Search to RAG ",
        "text_level": 1,
        "page_idx": 271
    },
    {
        "type": "text",
        "text": "Let’s now turn our search system into a RAG system. We do that by adding an LLM to the end of the search pipeline. We present the question and the top retrieved documents to the LLM, and ask it to answer the question given the context provided by the search results. We can see an example in Figure 8-25. ",
        "page_idx": 271
    },
    {
        "type": "text",
        "text": "This generation step is called grounded generation because the retrieved relevant information we provide the LLM establishes a certain context that grounds the LLM in the domain we’re interested in. Figure 8-26 shows how grounded generation fits after search if we continue our embeddings search example from earlier. ",
        "page_idx": 271
    },
    {
        "type": "image",
        "img_path": "images/e7cd7c5922267fb1968e49817daace0c407cf4c1030eba92000293ff0cbe03b7.jpg",
        "image_caption": [
            "Figure 8-25. Generative search formulates answers and summaries at the end of a search pipeline while citing its sources (returned by the previous steps in the search system). "
        ],
        "image_footnote": [],
        "page_idx": 272
    },
    {
        "type": "image",
        "img_path": "images/852e69a91bdde87aeaeefbd1f73f4a16f4a3b34ea0483091576f5d499437fd1d.jpg",
        "image_caption": [
            "Figure 8-26. Find the most relevant information to an input prompt by comparing the similarities between embeddings. The most relevant information is added to the prompt before giving it to the LLM. "
        ],
        "image_footnote": [],
        "page_idx": 272
    },
    {
        "type": "text",
        "text": "Example: Grounded Generation with an LLM API ",
        "text_level": 1,
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "Let’s look at how to add a grounded generation step after the search results to create our first RAG system. For this example, we’ll use Cohere’s managed LLM, which builds on the search systems we’ve seen earlier in the chapter. We’ll use embedding search to retrieve the top documents, then we’ll pass those to the co.chat endpoint along with the questions to provide a grounded answer: ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "query $=$ \"income generated\" ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "# 1- Retrieval   \n# We $^ { \\prime } \\mathbb { \\mathbb { Z } } \\mathbb { \\mathbb { Z } }$ use embedding search. But ideally we'd do hybrid   \nresults $=$ search(query)   \n# 2- Grounded Generation   \ndocs_dict $=$ [{'text': text} for text in results['texts']]   \nresponse $=$ co.chat( message $=$ query, documents $=$ docs_dict   \n) ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "print(response.text) ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "Result: ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "The film generated a worldwide gross of over $\\$ 677$ million, or $\\$ 773$ million with subsequent re-releases. ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "We are highlighting some of the text because the model indicated the source for these spans of text to be the first document we passed in: ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "citations $\\mathbf { \\Psi } _ { 1 } =$ [ChatCitation(star $\\scriptstyle \\mathtt { t } = 2 1$ , end $= 3 6$ , text='worldwide gross', document_ids $=$ ['doc_0']), ChatCitation(star $\\tan \\Theta$ , end $\\scriptstyle 1 = 5 7$ , text='over $\\$ 677$ million', document_ $\\scriptstyle \\mathbf { \\cdot d } \\mathbf { s } = [ \\mathbf { \\sigma } ^ { \\prime } \\mathbf { d } { \\mathsf { o c } } _ { - } { \\mathsf { o } } ^ { \\prime } ] )$ , ChatCitation(start $\\scriptstyle = 6 2$ , end $\\mathtt { \\Pi } = 1 \\Theta 3$ , text='\\$773 million with subsequent re-releases.', document_ids $\\mathbf { \\equiv } =$ ['doc_0'])]   \ndocuments=[{'id': 'doc_0', 'text': 'The film had a worldwide gross over $\\$ 677$ million (and $\\$ 773$ million with subsequent re-releases), making it the tenthhighest grossing film of $2 \\Theta 1 4 ^ { \\prime } \\rbrace ]$ ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "Example: RAG with Local Models ",
        "text_level": 1,
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "Let us now replicate this basic functionality with local models. We will lose the ability to do span citations and the smaller local model isn’t going to work as well as the larger managed model, but it’s useful to demonstrate the flow. We’ll start by downloading a quantized model. ",
        "page_idx": 273
    },
    {
        "type": "text",
        "text": "Loading the generation model ",
        "text_level": 1,
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "We start by downloading our model: ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/ Phi-3-mini-4k-instruct-fp16.gguf ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "Using llama.cpp, llama-cpp-python, and LangChain, we load the text generation model: ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "from langchain import LlamaCpp ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "# Make sure the model path is correct for your system!   \nllm $=$ LlamaCpp( model_path $=$ \"Phi-3-mini-4k-instruct-fp16.gguf\", n_gpu_layers $\\ c = - 1$ , max_tokens $\\begin{array} { r l } { : = } & { { } } \\end{array}$ , n_ctx $= 2 \\Theta 4 8$ , seed $= 4 2$ , verbose=False   \n) ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "Loading the embedding model ",
        "text_level": 1,
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "Let’s now load an embedding language model. In this example, we will choose the BAAI/bge-small-en-v1.5 model. At the time of writing, it is high on the MTEB leaderboard for embedding models and relatively small: ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "from langchain.embeddings.huggingface import HuggingFaceEmbeddings   \n# Embedding model for converting text to numerical representations   \nembedding_model $=$ HuggingFaceEmbeddings( model_name='thenlper/gte-small'   \n) ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "We can now use the embedding model to set up our vector database: ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "from langchain.vectorstores import FAISS # Create a local vector database db $=$ FAISS.from_texts(texts, embedding_model) ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "The RAG prompt ",
        "text_level": 1,
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "A prompt template plays a vital part in the RAG pipeline. It is the central place where we communicate the relevant documents to the LLM. To do so, we will create an additional input variable named context that can provide the LLM with the retrieved documents: ",
        "page_idx": 274
    },
    {
        "type": "text",
        "text": "from langchain import PromptTemplate ",
        "text_level": 1,
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "# Create a prompt template template $=$ \"\"\"<|user|> Relevant information: {context} ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "Provide a concise answer the following question using the relevant information   \nprovided above:   \n{question} $\\mathit { \\Pi } _ { < }$ |end|>   \n<|assistant|>\"\"\"   \nprompt $=$ PromptTemplate( template=template, input_variables=[\"context\", \"question\"]   \n) ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "from langchain.chains import RetrievalQA ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "# RAG pipeline   \nrag $=$ RetrievalQA.from_chain_type( llm=llm, chain_type='stuff', retriever $=$ db.as_retriever(), chain_type_kwargs={ \"prompt\": prompt }, verbose=True   \n) ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "Now we’re ready to call the model and ask it a question: ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "rag.invoke('Income generated') ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "Result: ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "The Income generated by the film in 2014 was over $\\$ 677$ million worldwide. This made it the tenth-highest grossing film of that year. It should be noted, however, this figure includes both initial ticket sales as well as any subsequent re-releases. With these additional releases, total earnings surged to approximately $\\$ 773$ million. The release format transitioned from traditional film stock projection in theaters to digital projectors once it was expanded to various venues in the United States. This shift might have contributed to wider audience reach and potentially higher grossing figures over time. However, specific data on how this affected total earnings isn't provided in the information above. ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "As always, we can adjust the prompt to control the model’s generation (e.g., answer length and tone). ",
        "page_idx": 275
    },
    {
        "type": "text",
        "text": "Advanced RAG Techniques ",
        "text_level": 1,
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "There are several additional techniques to improve the performance of RAG systems.   \nSome of them are laid out here. ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "Query rewriting ",
        "text_level": 1,
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "If the RAG system is a chatbot, the preceding simple RAG implementation would likely struggle with the search step if a question is too verbose, or to refer to context in previous messages in the conversation. This is why it’s a good idea to use an LLM to rewrite the query into one that aids the retrieval step in getting the right information. An example of this is a message such as: ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "User Question: “We have an essay due tomorrow. We have to write about some animal. I love penguins. I could write about them. But I could also write about dolphins. Are they animals? Maybe. Let’s do dolphins. Where do they live for example?” ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "This should actually be rewritten into a query like: ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "Query: “Where do dolphins live” ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "This rewriting behavior can be done through a prompt (or through an API call).   \nCohere’s API, for example, has a dedicated query-rewriting mode for co.chat. ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "Multi-query RAG ",
        "text_level": 1,
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "The next improvement we can introduce is to extend the query rewriting to be able to search multiple queries if more than one is needed to answer a specific question. Take for example: ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "User Question: “Compare the financial results of Nvidia in 2020 vs. 2023” ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "We may find one document that contains the results for both years, but more likely, we’re better off making two search queries: ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "Query 1: “Nvidia 2020 financial results” Query 2: “Nvidia 2023 financial results” ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "We then present the top results of both queries to the model for grounded generation. An additional small improvement here is to also give the query rewriter the option to determine if no search is required and if it can directly generate a confident answer without searching. ",
        "page_idx": 276
    },
    {
        "type": "text",
        "text": "Multi-hop RAG ",
        "text_level": 1,
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "A more advanced question may require a series of sequential queries. Take for example a question like: ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "User Question: “Who are the largest car manufacturers in 2023? Do they each make EVs or not?” ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "To answer this, the system must first search for: ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "Step 1, Query 1: “largest car manufacturers $2 0 2 3 ^ { \\mathfrak { n } }$ ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "Then after it gets this information (the result being Toyota, Volkswagen, and Hyun‐ dai), it should ask follow-up questions: ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "Step 2, Query 1: “Toyota Motor Corporation electric vehicles” Step 2, Query 2: “Volkswagen AG electric vehicles” Step 2, Query 3: “Hyundai Motor Company electric vehicles” ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "Query routing ",
        "text_level": 1,
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "An additional enhancement is to give the model the ability to search multiple data sources. We can, for example, specify for the model that if it gets a question about HR, it should search the company’s HR information system (e.g., Notion) but if the question is about customer data, that it should search the customer relationship management (CRM) (e.g., Salesforce). ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "Agentic RAG ",
        "text_level": 1,
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "You may be able to now see that the list of previous enhancements slowly delegates more and more responsibility to the LLM to solve more and more complex problems. This relies on the LLM’s capability to gauge the required information needs as well as its ability to utilize multiple data sources. This new nature of the LLM starts to become closer and closer to an agent that acts on the world. The data sources can also now be abstracted into tools. We saw, for example, that we can search Notion, but by the same token, we should be able to post to Notion as well. ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "Not all LLMs will have the RAG capabilities mentioned here. At the time of writing, likely only the largest managed models may be able to attempt this behavior. Thank‐ fully, Cohere’s Command $\\mathrm { R } +$ excels at these tasks and is available as an open-weights model as well. ",
        "page_idx": 277
    },
    {
        "type": "text",
        "text": "RAG Evaluation ",
        "text_level": 1,
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "There are still ongoing developments in how to evaluate RAG models. A good paper to read on this topic is “Evaluating verifiability in generative search engines” (2023), which runs human evaluations on different generative search systems.2 ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "It evaluates results along four axes: ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Fluency Whether the generated text is fluent and cohesive. ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Perceived utility Whether the generated answer is helpful and informative. ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Citation recall ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "The proportion of generated statements about the external world that are fully supported by their citations. ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Citation precision ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "The proportion of generated citations that support their associated statements. ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "While human evaluation is always preferred, there are approaches that attempt to automate these evaluations by having a capable LLM act as a judge (called LLM-as-ajudge) and score the different generations along the different axes. Ragas is a software library that does exactly this. It also scores some additional useful metrics like: ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Faithfulness ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Whether the answer is consistent with the provided context ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Answer relevance How relevant the answer is to the question ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "The Ragas documentation site provides more details about the formulas to actually calculate these metrics. ",
        "page_idx": 278
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 279
    },
    {
        "type": "text",
        "text": "In this chapter, we looked at different ways of using language models to improve existing search systems and even be the core of new, more powerful search systems. These include: ",
        "page_idx": 279
    },
    {
        "type": "text",
        "text": "• Dense retrieval, which relies on the similarity of text embeddings. These are systems that embed a search query and retrieve the documents with the nearest embeddings to the query’s embedding.   \n• Rerankers, systems (like monoBERT) that look at a query and candidate results and score the relevance of each document to that query. These relevance scores are then used to order the shortlisted results according to their relevance to the query, often producing an improved results ranking.   \n• RAG, where search systems have a generative LLM at the end of the pipeline to formulate an answer based on retrieved documents while citing sources. ",
        "page_idx": 279
    },
    {
        "type": "text",
        "text": "We also looked at one of the possible methods of evaluating search systems. Mean average precision allows us to score search systems to be able to compare across a test suite of queries and their known relevance to the test queries. Evaluating RAG systems requires multiple axes, however, like faithfulness, fluency, and others that can be evaluated by humans or by LLM-as-a-judge. ",
        "page_idx": 279
    },
    {
        "type": "text",
        "text": "In the next chapter, we will explore how language models can be made multimodal and reason not just about text but images as well. ",
        "page_idx": 279
    },
    {
        "type": "text",
        "text": "Multimodal Large Language Models ",
        "text_level": 1,
        "page_idx": 280
    },
    {
        "type": "text",
        "text": "When you think about large language models (LLMs), multimodality might not be the first thing that comes to mind. After all, they are language models! But we can quickly see that models can be much more useful if they’re able to handle types of data other than text. It’s very useful, for example, if a language model is able to glance at a picture and answer questions about it. A model that is able to handle text and images (each of which is called a modality) is said to be multimodal, as we can see in Figure 9-1. ",
        "page_idx": 280
    },
    {
        "type": "image",
        "img_path": "images/5bbc40d784b2d5990a878ef3eb1976cf8547e79ac19c6d0bf4fc248438663d08.jpg",
        "image_caption": [
            "Figure 9-1. Models that are able to deal with different types (or modalities) of data, such as images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to accept a modality as input yet not be able to generate in that modality. "
        ],
        "image_footnote": [],
        "page_idx": 280
    },
    {
        "type": "text",
        "text": "We have seen all manner of emerging behaviors rising from LLMs, from generaliza‐ tion capabilities and reasoning to arithmetic and linguistics. As models grow larger and smarter, so do their skill sets.1 ",
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "The ability to receive and reason with multimodal input might further increase and help emerge capabilities that were previously locked. In practice, language does not solely live in a vacuum. As an example, your body language, facial expressions, intonation, etc. are all methods of communication that enhance the spoken word. ",
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "The same thing applies to LLMs; if we can enable them to reason about multimodal information, their capabilities might increase and we become able to deploy them to solve new kinds of problems. ",
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "In this chapter, we will explore a number of different LLMs that have multimodal capabilities and what that means for practical use cases. We will start by exploring how images are converted to numerical representations using an adaptation of the original Transformer technique. Then, we will show how LLMs can be extended to include vision tasks using this Transformer. ",
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "Transformers for Vision ",
        "text_level": 1,
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "Throughout the chapters of this book, we have seen the success of using Transformerbased models for a variety of language modeling tasks, from classification and clustering to search and generative modeling. So it might not be surprising that researchers have been looking at a way to generalize some of the Transformer’s success to the field of computer vision. ",
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "The method they came up with is called the Vision Transformer (ViT), which has been shown to do tremendously well on image recognition tasks compared to the previously default convolutional neural networks (CNNs).2 Like the original Trans‐ former, ViT is used to transform unstructured data, an image, into representations that can be used for a variety of tasks, like classification, as illustrated in Figure 9-2. ",
        "page_idx": 281
    },
    {
        "type": "text",
        "text": "ViT relies on an important component of the Transformer architecture, namely the encoder. As we saw in Chapter 1, the encoder is responsible for converting textual input into numerical representations before being passed to the decoder. However, before the encoder can perform its duties, the textual input needs to be tokenized first, as is illustrated in Figure 9-3. ",
        "page_idx": 281
    },
    {
        "type": "image",
        "img_path": "images/89b2e4baab70bd68d9e4aad174f04afc319989576b2239f816492a21b5751c98.jpg",
        "image_caption": [
            "Figure 9-2. Both the original Transformer as well as the Vision Transformer take unstructured data, convert it to numerical representations, and finally use that for tasks like classification. "
        ],
        "image_footnote": [],
        "page_idx": 282
    },
    {
        "type": "image",
        "img_path": "images/0ac32d0c67121f85cf6d7e342948eff598ff02bcdc9d43df90ad3bb34288a271.jpg",
        "image_caption": [
            "Figure 9-3. Text is passed to one or multiple encoders by first tokenizing it using a tokenizer. "
        ],
        "image_footnote": [],
        "page_idx": 282
    },
    {
        "type": "text",
        "text": "Since an image does not consist of words this tokenization process cannot be used for visual data. Instead, the authors of ViT came up with a method for tokenizing images into “words,” which allowed them to use the original encoder structure. ",
        "page_idx": 282
    },
    {
        "type": "text",
        "text": "Imagine that you have an image of a cat. This image is represented by a number of pixels, let’s say $5 1 2 \\times 5 1 2$ pixels. Each individual pixel does not convey much information but when you combine patches of pixels, you slowly start to see more information. ",
        "page_idx": 282
    },
    {
        "type": "text",
        "text": "ViT uses a principle much like that. Instead of splitting up text into tokens, it converts the original image into patches of images. In other words, it cuts the image into a number of pieces horizontally and vertically as illustrated in Figure 9-4. ",
        "page_idx": 283
    },
    {
        "type": "image",
        "img_path": "images/18216c84992965f3241c72220ba6f4d6deeef8964974b1a4df1d9e0e826508d3.jpg",
        "image_caption": [
            "Figure 9-4. The “tokenization” process for image input. It converts an image into patches of subimages. "
        ],
        "image_footnote": [],
        "page_idx": 283
    },
    {
        "type": "text",
        "text": "Just like we are converting text into tokens of text, we are converting an image into patches of images. The flattened input of image patches can be thought of as the tokens in a piece of text. However, unlike tokens, we cannot just assign each patch with an ID since these patches will rarely be found in other images, unlike the vocabulary of a text. ",
        "page_idx": 283
    },
    {
        "type": "text",
        "text": "Instead, the patches are linearly embedded to create numerical representations, namely embeddings. These can then be used as the input of a Transformer model. That way, the patches of images are treated the same way as tokens. The full process is illustrated in Figure 9-5. ",
        "page_idx": 283
    },
    {
        "type": "text",
        "text": "For illustrative purposes, the images in the examples were patched into $3 \\times 3$ patches but the original implementation used $1 6 \\times 1 6$ patches. After all, the paper is called “An Image is Worth 16x16 Words.” ",
        "page_idx": 283
    },
    {
        "type": "text",
        "text": "What is so interesting about this approach is that the moment the embeddings are passed to the encoder, they are treated as if they were textual tokens. From that point forward, there is no difference in how a text or image trains. ",
        "page_idx": 283
    },
    {
        "type": "text",
        "text": "Due to these similarities, the ViT is often used to make all kinds of language models multimodal. One of the most straightforward ways to use it is during the training of embedding models. ",
        "page_idx": 283
    },
    {
        "type": "image",
        "img_path": "images/9db92eb5b04c517046fa926456e2bee5bf4d2bf51b716717cff95c95adea146f.jpg",
        "image_caption": [
            "Figure 9-5. The main algorithm behind ViT. After patching the images and linearly projecting them, the patch embeddings are passed to the encoder and treated as if they were textual tokens. "
        ],
        "image_footnote": [],
        "page_idx": 284
    },
    {
        "type": "text",
        "text": "Multimodal Embedding Models ",
        "text_level": 1,
        "page_idx": 284
    },
    {
        "type": "text",
        "text": "In previous chapters, we used embedding models to capture the semantic content of textual representations, such as papers and documents. We saw that we could use these embeddings or numerical representations to find similar documents, apply classification tasks, and even perform topic modeling. ",
        "page_idx": 284
    },
    {
        "type": "text",
        "text": "As we have seen many times before, embeddings often are an important driver behind LLM applications. They are an efficient method for capturing large-scale information and searching for the needle in the haystack of information. ",
        "page_idx": 284
    },
    {
        "type": "text",
        "text": "That said, we have looked at text-only embedding models thus far, which focus on generating embeddings for textual representations. Although embedding models exist for solely embedding imagery, we will look at embedding models that can capture both textual as well as visual representations. We illustrate this in Figure 9-6. ",
        "page_idx": 284
    },
    {
        "type": "image",
        "img_path": "images/e98dc77b9c0b561bd8fb0e945b6cd257ca8cc5f85f67d5066247279e299f1251.jpg",
        "image_caption": [
            "Figure 9-6. Multimodal embedding models can create embeddings for multiple modali‐ ties in the same vector space. "
        ],
        "image_footnote": [],
        "page_idx": 285
    },
    {
        "type": "text",
        "text": "An advantage is that this allows for comparing multimodal representations since the resulting embeddings lie in the same vector space (Figure 9-7). For instance, using such a multimodal embedding model, we can find images based on input text. What images would we find if we search for images similar to “pictures of a puppy”? Vice versa would also be possible. Which documents are best related to this question? ",
        "page_idx": 285
    },
    {
        "type": "image",
        "img_path": "images/167baa9faf32c88f27f2fe360a2c0df430a9a100d1dcf1319af1eca690ca4d11.jpg",
        "image_caption": [
            "Figure 9-7. Despite having coming from different modalities, embeddings with similar meaning will be close to each other in vector space. "
        ],
        "image_footnote": [],
        "page_idx": 285
    },
    {
        "type": "text",
        "text": "There are a number of multimodal embedding models, but the most well-known and currently most-used model is Contrastive Language-Image Pre-training (CLIP). ",
        "page_idx": 285
    },
    {
        "type": "text",
        "text": "CLIP: Connecting Text and Images ",
        "text_level": 1,
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "CLIP is an embedding model that can compute embeddings of both images and texts. The resulting embeddings lie in the same vector space, which means that the embed‐ dings of images can be compared with the embeddings of text.3 This comparison capability makes CLIP, and similar models, usable for tasks such as: ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Zero-shot classification ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "We can compare the embedding of an image with that of the description of its possible classes to find which class is most similar. ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Clustering ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Cluster both images and a collection of keywords to find which keywords belong to which sets of images. ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Search ",
        "text_level": 1,
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Across billions of texts or images, we can quickly find what relates to an input text or image. ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Generation ",
        "text_level": 1,
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "Use multimodal embeddings to drive the generation of images (e.g., stable diffusion4). ",
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "How Can CLIP Generate Multimodal Embeddings? ",
        "text_level": 1,
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "The procedure of CLIP is actually quite straightforward. Imagine that you have a dataset with millions of images alongside captions as we illustrate in Figure 9-8. ",
        "page_idx": 286
    },
    {
        "type": "image",
        "img_path": "images/35712f27ced4e39109355f24117339c80fad096b6f286d6b9b8cd1530aab0bba.jpg",
        "image_caption": [
            "Figure 9-8. The type of data that is needed to train a multimodal embedding model. "
        ],
        "image_footnote": [],
        "page_idx": 286
    },
    {
        "type": "text",
        "text": "This dataset can be used to create two representations for each pair, the image and its caption. To do so, CLIP uses a text encoder to embed text and an image encoder to embed images. As is shown in Figure 9-9, the result is an embedding for both the image and its corresponding caption. ",
        "page_idx": 287
    },
    {
        "type": "image",
        "img_path": "images/ca9d8c416252bbdae4b786e901b5f3f70300bebd4f9ce72af8d3c4743e01af26.jpg",
        "image_caption": [
            "Figure 9-9. In the first step of training CLIP, both images and text are embedded using an image and text encoder, respectively. "
        ],
        "image_footnote": [],
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "The pair of embeddings that are generated are compared through cosine similarity. As we saw in Chapter 4, cosine similarity is the cosine of the angle between vectors, which is calculated through the dot product of the embeddings and divided by the product of their lengths. ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "When we start training, the similarity between the image embedding and text embed‐ ding will be low as they are not yet optimized to be within the same vector space. During training, we optimize for the similarity between the embeddings and want to maximize them for similar image/caption pairs and minimize them for dissimilar image/caption pairs (Figure 9-10). ",
        "page_idx": 287
    },
    {
        "type": "text",
        "text": "After calculating their similarity, the model is updated and the process starts again with new batches of data and updated representations (Figure 9-11). This method is called contrastive learning, and we will go in depth into its inner workings in Chapter 10 where we will create our own embedding model. ",
        "page_idx": 287
    },
    {
        "type": "image",
        "img_path": "images/8637a103827942b551b5c714d2e4d852c460b718a2851cddf47d98f1d44e9387.jpg",
        "image_caption": [
            "Figure 9-10. In the second step of training CLIP, the similarity between the sentence and image embedding is calculated using cosine similarity. "
        ],
        "image_footnote": [],
        "page_idx": 288
    },
    {
        "type": "image",
        "img_path": "images/8b1e62fee2b4380c8ccfb6142c6d7a83b92f04c0eeacc597751954d3144d4c28.jpg",
        "image_caption": [
            "Figure 9-11. In the third step of training CLIP, the text and image encoders are updated to match what the intended similarity should be. This updates the embeddings such that they are closer in vector space if the inputs are similar. "
        ],
        "image_footnote": [],
        "page_idx": 288
    },
    {
        "type": "text",
        "text": "Eventually, we expect the embedding of an image of a cat would be similar to the embedding of the phrase “a picture of a cat.” As we will see in Chapter 10, to make sure the representations are as accurate as possible, negative examples of images and captions that are not related should also be included in the training process. Modeling similarity is not only knowing what makes things similar to one another, but also what makes them different and dissimilar. ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "OpenCLIP ",
        "text_level": 1,
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "For our next example, we are going to be using models from the open source variant of CLIP, namely OpenCLIP. Using OpenCLIP, or any CLIP model, boils down to two things: processing the textual and image inputs before passing them to the main model. ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "Before doing so, let’s take a look at a small example where we will be using one of the images we have seen before, namely, an AI-generated image (through stable diffusion) of a puppy playing in the snow, as illustrated in Figure 9-12: ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "from urllib.request import urlopen from PIL import Image ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "# Load an AI-generated image of a puppy playing in the snow   \npuppy_path $=$ \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large  \nLanguage-Models/main/chapter09/images/puppy.png\"   \nimage $=$ Image.open(urlopen(puppy_path)).convert(\"RGB\") ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "caption $=$ \"a puppy playing in the snow\" ",
        "page_idx": 289
    },
    {
        "type": "image",
        "img_path": "images/63199d0510623f89a7d11a556ddb5a15ef1e1f88c3a84a0732304eb9feb35e5a.jpg",
        "image_caption": [
            "Figure 9-12. An AI-generated image of a puppy playing in the snow. "
        ],
        "image_footnote": [],
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "Since we have a caption for this image, we can use OpenCLIP to generate embeddings for both. ",
        "page_idx": 289
    },
    {
        "type": "text",
        "text": "To do so, we load in three models: ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "• A tokenizer for tokenizing the textual input • A preprocessor to preprocess and resize the image • The main model that converts the previous outputs to embeddings from transformers import CLIPTokenizerFast, CLIPProcessor, CLIPModel model_id $=$ \"openai/clip-vit-base-patch32\" ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "# Load a tokenizer to preprocess the text clip_tokenizer $=$ CLIPTokenizerFast.from_pretrained(model_id) ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "# Load a processor to preprocess the images clip_processor $=$ CLIPProcessor.from_pretrained(model_id) ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "# Main model for generating text and image embeddings model $=$ CLIPModel.from_pretrained(model_id) ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "After having loaded in the models, preprocessing our input is straightforward. Let’s start with the tokenizer and see what happens if we preprocess our input: ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "# Tokenize our input   \ninputs $=$ clip_tokenizer(caption, return_tensors $=$ \"pt\")   \ninputs ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "This outputs a dictionary that contains the IDs of the input: ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "{'input_ids': tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])} ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "To see what those IDs represent, we can convert them to tokens using the aptly named convert_ids_to_tokens function: ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "# Convert our input back to tokens clip_tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "This gives us the following output: ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "['<|startoftext|>',   \n'a</w>',   \n'puppy</w>',   \n'playing</w>',   \n'in</w>',   \n'the</w>',   \n'snow</w>',   \n'<|endoftext|>'] ",
        "page_idx": 290
    },
    {
        "type": "text",
        "text": "As we often have seen before, the text is split up into tokens. Additionally, we now also see that the start and end of the text is indicated to separate it from a potential image embedding. You might also notice that the [CLS] token is missing. In CLIP, the [CLS] token is actually used to represent the image embedding. ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "Now that we have preprocessed our caption, we can create the embedding: ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "# Create a text embedding   \ntext_embedding $=$ model.get_text_features(\\*\\*inputs)   \ntext_embedding.shape ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "This results in an embedding that has 512 values for this single string: ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "torch.Size([1, 512])",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "Before we can create our image embedding, like the text embedding, we will need to preprocess it as the model expects the input image to have certain characteristics, like its size and shape. ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "To do so, we can use the processor that we created before: ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "# Preprocess image   \nprocessed_image $=$ clip_processor( text=None, images $\\mathbf { \\Psi } =$ image, return_tensors $: =$ \"pt\"   \n)[\"pixel_values\"] ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "processed_image.shape ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "The original image was $5 1 2 \\times 5 1 2$ pixels. Notice that the preprocessing of this image reduced its size to $2 2 4 \\times 2 2 4$ pixels as that is its expected size: ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "torch.Size([1, 3, 224, 224])",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "Let’s visualize the results of this preprocessing as shown in Figure 9-13: ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "import torch   \nimport numpy as np   \nimport matplotlib.pyplot as plt   \n# Prepare image for visualization   \nimg $=$ processed_image.squeeze(0)   \nimg $=$ img.permute(\\*torch.arange(img.ndim - 1, -1, -1))   \nimg $=$ np.einsum(\"ijk->jik\", img)   \n# Visualize preprocessed image   \nplt.imshow(img)   \nplt.axis(\"off\") ",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 291
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 291
    },
    {
        "type": "image",
        "img_path": "images/a88cee8fb10bbef79dd611379eaf97a93d9950c1646f83a664f317870f56d6db.jpg",
        "image_caption": [
            "Figure 9-13. The preprocessed input image by CLIP. "
        ],
        "image_footnote": [],
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "To convert this preprocessed image into embeddings, we can call the model as we did before and explore what shape it returns: ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "# Create the image embedding   \nimage_embedding $=$ model.get_image_features(processed_image)   \nimage_embedding.shape ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "This gives us the following shape: ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "torch.Size([1, 512])",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "Notice that the shape of the resulting image embedding is the same as that of the text embedding. This is important as it allows us to compare their embeddings and see if they are similar. ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "We can use these embeddings to calculate how similar they are. To do so, we normal‐ ize the embeddings first before calculating the dot product to give us a similarity score: ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "# Normalize the embeddings text_embedding $/ =$ text_embedding.norm(dim=-1, keepdim=True) image_embedding $/ =$ image_embedding.norm(dim=-1, keepdim=True) ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "# Calculate their similarity   \ntext_embedding $=$ text_embedding.detach().cpu().numpy()   \nimage_embedding $=$ image_embedding.detach().cpu().numpy()   \nscore $=$ np.dot(text_embedding, image_embedding.T)   \nscore ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "This gives us the following score: ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "array([[0.33149648]], dtype=float32) ",
        "page_idx": 292
    },
    {
        "type": "text",
        "text": "We get a similarity score of 0.33, which is difficult to interpret considering we don’t know what the model considers a low versus a high similarity score. Instead, let’s extend the example with more images and captions as illustrated in Figure 9-14. ",
        "page_idx": 293
    },
    {
        "type": "table",
        "img_path": "images/1ebd224e2558b8c54307b79863db94703ab60c2c6d2e838c6b04ec5ada8408e9.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td rowspan=\"4\">A puppy playing in the snow A pixelated image of a cute cat</td><td></td><td></td><td></td></tr><tr><td>0.33</td><td>0.19</td><td>0.11</td></tr><tr><td>0.15</td><td>0.35</td><td>0.09</td></tr><tr><td>A supercar on the road with 0.08 the sunset in the background</td><td>0.13</td><td>0.31</td></tr></table>",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "It seems that a score of 0.33 is indeed high considering the similarities with other images are quite a bit lower. ",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "Using sentence-transformers to Load CLIP ",
        "text_level": 1,
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "sentence-transformers implements a few CLIP-based models that make it much easier to create embeddings. It only takes a few lines of code: ",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "from sentence_transformers import SentenceTransformer, util # Load SBERT-compatible CLIP model model $=$ SentenceTransformer(\"clip-ViT-B-32\") ",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "# Encode the images image_embeddings $=$ model.encode(images) ",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "# Encode the captions text_embeddings $=$ model.encode(captions) ",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "#Compute cosine similarities sim_matrix $=$ util.cos_sim( image_embeddings, text_embeddings ) ",
        "page_idx": 293
    },
    {
        "type": "text",
        "text": "Making Text Generation Models Multimodal ",
        "text_level": 1,
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "Traditionally, text generation models have been, as you might expect, models that interpret textual representations. Models like Llama 2 and ChatGPT excel at reason‐ ing about textual information and responding with natural language. ",
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "They are, however, limited to the modality they were trained in, namely text. As we have seen before with multimodal embedding models, the addition of vision can enhance the capabilities of a model. ",
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "In the case of text generation models, we would like it to reason about certain input images. For example, we could give it an image of a pizza and ask it what ingredients it contains. You could show it a picture of the Eiffel Tower and ask when it was built or where it is located. This conversational ability is further illustrated in Figure 9-15. ",
        "page_idx": 294
    },
    {
        "type": "image",
        "img_path": "images/cfdb037a3af13fdd2453beacfd8ecdae9f59f7288a69592ba8e2e9895da72e15.jpg",
        "image_caption": [
            "Figure 9-15. An example of a multimodal text generation model (BLIP-2) that can reason about input images. "
        ],
        "image_footnote": [],
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "To bridge the gap between these two domains, attempts have been made to introduce a form of multimodality to existing models. One such method is called BLIP-2: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understand‐ ing and Generation 2. BLIP-2 is an easy-to-use and modular technique that allows for introducing vision capabilities to existing language models. ",
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "BLIP-2: Bridging the Modality Gap ",
        "text_level": 1,
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "Creating a multimodal language model from scratch requires significant computing power and data. We would have to use billions of images, text, and image-text pairs to create such a model. As you can imagine, this is not easily feasible! ",
        "page_idx": 294
    },
    {
        "type": "text",
        "text": "Instead of building the architecture from scratch, BLIP-2 bridges the vision-language gap by building a bridge, named the Querying Transformer (Q-Former), that con‐ nects a pretrained image encoder and a pretrained LLM.5 ",
        "page_idx": 295
    },
    {
        "type": "text",
        "text": "By leveraging pretrained models, BLIP-2 only needs to train the bridge without needing to train the image encoder and LLM from scratch. It makes great use of the technology and models that are already out there! This bridge is illustrated in Figure 9-16. ",
        "page_idx": 295
    },
    {
        "type": "image",
        "img_path": "images/8fb6049e10eddaf68db6b740453e1dfac353a10364c92850c11c217342f6f958.jpg",
        "image_caption": [
            "Figure 9-16. The Querying Transformer is the bridge between vision (ViT) and text (LLM) that is the only trainable component of the pipeline. "
        ],
        "image_footnote": [],
        "page_idx": 295
    },
    {
        "type": "text",
        "text": "To connect the two pretrained models, the Q-Former mimics their architectures. It has two modules that share their attention layers: ",
        "page_idx": 295
    },
    {
        "type": "text",
        "text": "• An Image Transformer to interact with the frozen Vision Transformer for feature extraction   \n• A Text Transformer that can interact with the LLM ",
        "page_idx": 295
    },
    {
        "type": "text",
        "text": "The Q-Former is trained in two stages, one for each modality, as illustrated in Figure 9-17. ",
        "page_idx": 295
    },
    {
        "type": "text",
        "text": "In step 1, image-document pairs are used to train the Q-Former to represent both images and text. These pairs are generally captions of images, as we have seen before when training CLIP. ",
        "page_idx": 295
    },
    {
        "type": "text",
        "text": "The images are fed to the frozen ViT to extract vision embeddings. These embed‐ dings are used as the input of Q-Former’s ViT. The captions are used as the input of Q-Former’s Text Transformer. ",
        "page_idx": 295
    },
    {
        "type": "image",
        "img_path": "images/e4e835ef97746cea118d624b68c6e6eb74067fc322d7c4cf79f51ef059aea370.jpg",
        "image_caption": [
            "Figure 9-17. In step 1, representation learning is applied to learn representations for vision and language simultaneously. In step 2, these representations are converted to soft visual prompts to feed the LLM. "
        ],
        "image_footnote": [],
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "With these inputs, the Q-Former is then trained on three tasks: ",
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "Image-text contrastive learning This task attempts to align pairs of image and text embeddings such that they maximize their mutual information. ",
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "Image-text matching ",
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "A classification task to predict whether an image and text pair is positive (matched) or negative (unmatched). ",
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "Image-grounded text generation ",
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "Trains the model to generate text based on information extracted from the input image. ",
        "page_idx": 296
    },
    {
        "type": "text",
        "text": "These three objectives are jointly optimized to improve the visual representations that are extracted from the frozen ViT. In a way, we are trying to inject textual information into the embeddings of the frozen ViT so that we can use them in the LLM. This first step of BLIP-2 is illustrated in Figure 9-18. ",
        "page_idx": 296
    },
    {
        "type": "image",
        "img_path": "images/ad775f33b3c7cad5f73f27b0a08b16f004f78b9b2ff497508003db716fdbc4ca.jpg",
        "image_caption": [
            "Figure 9-18. In step 1, the output of the frozen ViT is used together with its caption and trained on three contrastive-like tasks to learn visual-text representations. "
        ],
        "image_footnote": [],
        "page_idx": 297
    },
    {
        "type": "text",
        "text": "In step 2, the learnable embeddings derived from step 1 now contain visual informa‐ tion in the same dimensional space as the corresponding textual information. The learnable embeddings are then passed to the LLM. In a way, these embeddings serve as soft visual prompts that condition the LLM on the visual representations that were extracted by the Q-Former. ",
        "page_idx": 297
    },
    {
        "type": "text",
        "text": "There is also a fully connected linear layer in between them to make sure that the learnable embeddings have the same shape as the LLM expects. This second step of converting vision to language is represented in Figure 9-19. ",
        "page_idx": 297
    },
    {
        "type": "image",
        "img_path": "images/dbc6c57749d508af91ebc056a48819d8b9e3dad6e5fbb8e85c40e9ca5e88a97d.jpg",
        "image_caption": [
            "Figure 9-19. In step 2, the learned embeddings from the Q-Former are passed to the LLM through a projection layer. The projected embeddings serve as a soft visual prompt. "
        ],
        "image_footnote": [],
        "page_idx": 297
    },
    {
        "type": "text",
        "text": "When we put these steps together, they make it possible for the Q-Former to learn visual and textual representations in the same dimensional space, which can be used as a soft prompt to the LLM. As a result, the LLM will be given information about the image in a similar manner to the context you would provide an LLM when prompting. The full in-depth process is illustrated in Figure 9-20. ",
        "page_idx": 298
    },
    {
        "type": "image",
        "img_path": "images/d86d1435890ab71ec39ae3d19ea2d4bc38cbe1835ebef9dfcd17d132d7dc31af.jpg",
        "image_caption": [
            "Figure 9-20. The full BLIP-2 procedure. "
        ],
        "image_footnote": [],
        "page_idx": 298
    },
    {
        "type": "text",
        "text": "Since BLIP-2, many other visual LLMs have been released that have similar processes, like LLaVA, a framework for making textual LLMs multimodal6 or Idefics 2, an efficient visual LLM based on the Mistral 7B LLM.7 Both visual LLMs, although having different architectures, connect pretrained CLIP-like visual encoders with textual LLMs. The goal of these architectures is to project visual features from the input images to language embeddings such that they can be used as the input for an LLM. Similar to the Q-Former, they attempt to bridge the gap between images and text. ",
        "page_idx": 298
    },
    {
        "type": "text",
        "text": "Preprocessing Multimodal Inputs ",
        "text_level": 1,
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "Now that we know how BLIP-2 is created, there are a number of interesting use cases for such a model, not limited to captioning images, answering visual questions, and even performing prompting. ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "Before we go through some use cases, let’s first load the model and explore how you can use it: ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "from transformers import AutoProcessor, Blip2ForConditionalGeneration import torch ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "# Load processor and main model   \nblip_processor $=$ AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")   \nmodel $=$ Blip2ForConditionalGeneration.from_pretrained( \"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16   \n) ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "# Send the model to GPU to speed up inference device $=$ \"cuda\" if torch.cuda.is_available() else \"cpu\" model.to(device) ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "Using model.vision_model and model.language_model, we can see which ViT and generative model are used, respectively, in the BLIP-2 model we loaded. ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "We loaded two components that make up our full pipeline: a processor and a model. The processor can be compared to the tokenizer of language models. It converts unstructured input, such as images and text, to representations that the model gener‐ ally expects. ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "Preprocessing images ",
        "text_level": 1,
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "Let’s start by exploring what the processor does to images. We start by loading the picture of a very wide image for illustration purposes: ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "# Load image of a supercar   \ncar_path $=$ \"https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large  \nLanguage-Models/main/chapter09/images/car.png\"   \nimage $=$ Image.open(urlopen(car_path)).convert(\"RGB\") ",
        "page_idx": 299
    },
    {
        "type": "text",
        "text": "image ",
        "page_idx": 299
    },
    {
        "type": "image",
        "img_path": "images/c4cbc941d4752291d94b584453db0f887bb3193afd6faa041cd30a4d94d76bff.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "The image has $5 2 0 \\times 4 9 2$ pixels, which is generally an unusual format. So let’s see what our processor does to it: ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "# Preprocess the image   \ninputs $=$ blip_processor(image, return_tensors $=$ \"pt\").to(device, torch.float16)   \ninputs[\"pixel_values\"].shape ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "This gives us the following shape: ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "torch.Size([1, 3, 224, 224])",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "The result is a $2 2 4 \\times 2 2 4$ -sized image. Quite a bit smaller than we initially had! This also means that all the original different shapes of the image will be processed into squares. So be careful inputting very wide or tall images as they might get distorted. ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "Preprocessing text ",
        "text_level": 1,
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "Let’s continue this exploration of the processor with text instead. First, we can access the tokenizer used to tokenize the input text: ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "blip_processor.tokenizer ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "This gives us the following output: ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "GPT2TokenizerFast(name_or_path $=$ 'Salesforce/blip2-opt-2.7b', vocab_siz $= 5 \\Theta 2 6 5$ ,   \nmodel_max_length $| =$ 1000000000000000019884624838656, is_fast $=$ True, pad  \nding_side='right', truncation_side='right', special_tokens={'bos_token': '</   \n$S > \"$ , 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'},   \nclean_up_tokenization_spaces $=$ True), added_tokens_decoder={   \n1: AddedToken(\"<pad>\", rstrip $\\backsimeq$ False, lstrip $| =$ False, single_word=False, normal  \nized=True, special $\\ l =$ True),   \n2: AddedToken(\"</s>\", rstrip $\\ulcorner$ False, lstrip $| =$ False, single_word $=$ False, normal  \nized=True, special=True),   \n} ",
        "page_idx": 300
    },
    {
        "type": "text",
        "text": "The BLIP-2 model here uses a GPT2Tokenizer. As we explored in Chapter 2, how tokenizers deal with input text can differ greatly. ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "To explore how GPT2Tokenizer works, we can try it out with a small sentence. We start by converting the sentence to token IDs before converting them back to tokens: ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "# Preprocess the text   \ntext $=$ \"Her vocalization was remarkably melodic\"   \ntoken_ids $=$ blip_processor(image, text=text, return_tensors $=$ \"pt\") token_ids $=$ token_ids.to(device, torch.float16)[\"input_ids\"][0]   \n# Convert input ids back to tokens   \ntokens $=$ blip_processor.tokenizer.convert_ids_to_tokens(token_ids)   \ntokens ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "This gives us the following tokens: ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "['</s>', 'Her', 'Ġvocal', 'ization', 'Ġwas', 'Ġremarkably', 'Ġmel', 'odic'] ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "When we inspect the tokens, you might notice a strange symbol at the beginning of some tokens, namely, the $\\dot { \\mathrm { ~ G ~ } }$ symbol. This is actually supposed to be a space. However, an internal function takes characters in certain code points and moves them up by 256 to make them printable. As a result, the space (code point 32) becomes $\\dot { \\mathrm { ~ G ~ } }$ (code point 288). ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "We will convert them to underscores for illustrative purposes: ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "# Replace the space token with an underscore tokens $=$ [token.replace(\"Ġ\", \"_\") for token in tokens] tokens ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "This gives us a nicer output: ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "['</s>', 'Her', '_vocal', 'ization', '_was', '_remarkably', '_mel', 'odic'] ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "The output shows that the underscore indicates the beginning of a word. That way, words that are made up of multiple tokens can be recognized. ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "Use Case 1: Image Captioning ",
        "text_level": 1,
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "The most straightforward usage of a model like BLIP-2 is to create captions of images that you have in your data. You might be a store that wants to create descriptions of its clothing or perhaps you are a photographer that does not have the time to manually label the $1 { , } 0 0 0 +$ pictures of a wedding. ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "The process of captioning an image closely follows the processing. An image is converted to pixel values that the model can read. These pixel values are passed to BLIP-2 to be converted into soft visual prompts that the LLM can use to decide on a proper caption. ",
        "page_idx": 301
    },
    {
        "type": "text",
        "text": "Let’s take the image of a supercar and use the processor to derive pixels in the expected shape: ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "# Load an AI-generated image of a supercar image $=$ Image.open(urlopen(car_path)).convert(\"RGB\") ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "# Convert an image into inputs and preprocess it   \ninputs $=$ blip_processor(image, return_tensors $=$ \"pt\").to(device, torch.float16)   \nimage ",
        "page_idx": 302
    },
    {
        "type": "image",
        "img_path": "images/6a4bdaa0e247a1c0299818e247cc45f459963e9ce27862fb75f8f3498b1e8fbd.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "The next step is converting the image into token IDs using the BLIP-2 model. After doing so, we can convert the IDs into text (the generated caption): ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "# Generate image ids to be passed to the decoder (LLM)   \ngenerated_ids $=$ model.generate(\\*\\*inputs, max_new_tokens $\\begin{array} { r l } { : = { } } & { { } } \\end{array}$ )   \n# Generate text from the image ids   \ngenerated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   \ngenerated_text $=$ generated_text[0].strip()   \ngenerated_text ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "generated_text contains the caption: ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "an orange supercar driving on the road at sunset ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "This seems like a perfect description for this image! ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "Image captioning is a great way to get to learn this model before stepping into more complex use cases. Try it out with a few images yourself and see where it performs well and where it performs poorly. Domain-specific images, like pictures of specific cartoon characters or imaginary creations, may fail as the model was trained on largely public data. ",
        "page_idx": 302
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 303
    },
    {
        "type": "text",
        "text": "Let’s end this use case with a fun example, namely an image from the Rorschach test, which is illustrated in Figure 9-21. It is part of an old psychological experiment that tests the individual’s perception of inkblots.8 What someone sees in such an inkblot supposedly tells you something about a person’s personality characteristics. It is quite a subjective test but that just makes it more fun! ",
        "page_idx": 303
    },
    {
        "type": "image",
        "img_path": "images/f96ac1ff7268d609091578e49a5c2797d9127b695a102f3a4263f31b89cf25a3.jpg",
        "image_caption": [
            "Figure 9-21. An image from the Rorschach test. What do you see in it? "
        ],
        "image_footnote": [],
        "page_idx": 303
    },
    {
        "type": "text",
        "text": "Let’s take the image illustrated in Figure 9-21 and use that as our input: ",
        "page_idx": 303
    },
    {
        "type": "text",
        "text": "# Load Rorschach image   \nurl $=$ \"https://upload.wikimedia.org/wikipedia/commons/7/70/Ror   \nschach_blot_01.jpg\"   \nimage $=$ Image.open(urlopen(url)).convert(\"RGB\")   \n# Generate caption   \ninputs $=$ blip_processor(image, return_tensors $=$ \"pt\").to(device, torch.float16)   \ngenerated_ids $=$ model.generate(\\*\\*inputs, max_new_tokens $\\begin{array} { r l } { : = { } } & { { } } \\end{array}$ )   \ngenerated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   \n) ",
        "page_idx": 303
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 303
    },
    {
        "type": "text",
        "text": "generated_text $=$ generated_text[0].strip() generated_text ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "As before, when we inspect the generated_text variable, we can take a look at the caption: ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "a black and white ink drawing of a bat ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "I can definitely see how the model would caption this image using such a description. Since this is a Rorschach test, what do you think it says about the model? ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "Use Case 2: Multimodal Chat-Based Prompting ",
        "text_level": 1,
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "Although captioning is an important task, we can extend its use case even further. In the previous example, we showed going from one modality, vision (image), to another, text (caption). ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "Instead of following this linear structure, we can try to present both modalities simul‐ taneously by performing what is called visual question answering. In this particular use case, we give the model an image along with a question about that specific image for it to answer. The model needs to process both the image as well as the question at once. ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "To demonstrate, let’s start with the picture of a car and ask BLIP-2 to describe the image. To do so, we first need to preprocess the image as we did a few times before: ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "# Load an AI-generated image of a supercar image $=$ Image.open(urlopen(car_path)).convert(\"RGB\") ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "To perform our visual question answering we need to give BLIP-2 more than just the image, namely the prompt. Without it, the model would generate a caption as it did before. We will ask the model to describe the image we just processed: ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "# Visual question answering prompt $=$ \"Question: Write down what you see in this picture. Answer:\" ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "# Process both the image and the prompt   \ninputs $=$ blip_processor(image, text=prompt, return_tensors $\\mathbf { \\equiv }$ \"pt\").to(device,   \ntorch.float16)   \n# Generate text   \ngenerated_ids $=$ model.generate(\\*\\*inputs, max_new_tokens $= 3 \\Theta$ )   \ngenerated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens $\\mathbf { \\sigma } =$ True   \n)   \ngenerated_text $=$ generated_text[0].strip()   \ngenerated_text ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "This gives us the following output: ",
        "page_idx": 304
    },
    {
        "type": "text",
        "text": "It correctly describes the image. However, this is a rather simple example since our question is essentially asking the model to create a caption. Instead, we can ask follow-up questions in a chat-based manner. ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "To do so, we can give the model our previous conversation, including its answer to our question. We then ask it a follow-up question: ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "# Chat-like prompting   \nprompt $=$ \"Question: Write down what you see in this picture. Answer: A sports car driving on the road at sunset. Question: What would it cost me to drive that car? Answer:\"   \n# Generate output   \ninputs $=$ blip_processor(image, text=prompt, return_tensors $\\mathbf { \\sigma } =$ \"pt\").to(device,   \ntorch.float16)   \ngenerated_ids $=$ model.generate(\\*\\*inputs, max_new_tokens $\\begin{array} { r l } { : = { } } & { { } } \\end{array}$ )   \ngenerated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   \n)   \ngenerated_text $=$ generated_text[0].strip()   \ngenerated_text ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "This gives us the following answer: ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "\\$1,000,000 ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "$\\$ 1,000,000$ is highly specific! This shows more chat-like behavior from BLIP-2, which allows for some interesting conversations. ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "Finally, we can make this process a bit smoother by creating an interactive chatbot using ipywidgets, an extension for Jupyter notebooks that allows us to make interac‐ tive buttons, input text, etc: ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "from IPython.display import HTML, display   \nimport ipywidgets as widgets   \ndef text_eventhandler(\\*args): question $=$ args[0][\"new\"] if question: args[0][\"owner\"].value $=$ # Create prompt if not memory: prompt $=$ \" Question: \" $^ +$ question $^ +$ \" 1 Answer:\" else: template $=$ \"Question: {} Answer: {}.\" prompt $=$ \" \".join( [ template.format(memory[i][0], memory[i][1]) ",
        "page_idx": 305
    },
    {
        "type": "text",
        "text": "for i in range(len(memory)) ] ) + \" Question: \" $^ +$ question $^ +$ \" Answer:\" ",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "# Generate text   \ninputs $=$ blip_processor(image, text $\\equiv$ prompt, return_tensors $=$ \"pt\")   \ninputs $=$ inputs.to(device, torch.float16)   \ngenerated_ids $=$ model.generate(\\*\\*inputs, max_new_tokens $= 1 \\Theta \\Theta$ )   \ngenerated_text $=$ blip_processor.batch_decode( generated_ids, skip_special_tokens=True   \n)   \ngenerated_text $=$ generated_text[0].strip().split(\"Question\")[0] ",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "# Update memory memory.append((question, generated_text)) ",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "# Assign to output   \noutput.append_display_data(HTML(\"<b>USER:</b> \" $^ +$ question))   \noutput.append_display_data(HTML(\"<b>BLIP-2:</b> \" $^ +$ generated_text))   \noutput.append_display_data(HTML(\"<br>\"))   \n# Prepare widgets   \nin_text $=$ widgets.Text()   \nin_text.continuous_update $=$ False   \nin_text.observe(text_eventhandler, \"value\")   \noutput $=$ widgets.Output()   \nmemory $=$ []   \n# Display chat box   \ndisplay( widgets.VBox( children $| =$ [output, in_text], layout=widgets.Layout(display $^ { \\prime } =$ \"inline-flex\", flex_flow=\"column  \nreverse\"), )   \n) UsER: Write down what you see in this picture. BLIP-2: A sports car driving on the road at sunset USER:What would it cost me to drive that car? BLIP-2: \\$1,000,000   \nUSER: Why are sports cars expensive?   \nBLIP-2: Because they're fast. ",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 306
    },
    {
        "type": "text",
        "text": "It seems that we can continue the conversation and ask a bunch of questions. Using this chat-based approach, we essentially created a chatbot that can reason about images! ",
        "page_idx": 307
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 307
    },
    {
        "type": "text",
        "text": "In this chapter, we explored various methods for making LLMs multimodal by bridging the gap between textual and visual representations. We started by discus‐ sing Transformers for vision, which are models that convert images into numerical representations. This was achieved through the use of image encoders and patch embeddings, which allow the model to process images at various scales. ",
        "page_idx": 307
    },
    {
        "type": "text",
        "text": "We then explored the creation of embedding models that can convert both images and text to numerical representations using CLIP. We saw how CLIP uses contrastive learning to align image and text embeddings in a shared space, allowing for tasks like zero-shot classification, clustering, and search. The chapter also introduced Open‐ CLIP, an open source variant of CLIP that is easy to use for multimodal embedding tasks. ",
        "page_idx": 307
    },
    {
        "type": "text",
        "text": "Finally, we explored how text generation models could be made multimodal and dived into the BLIP-2 model. The core idea of these multimodal text generation models involves projecting visual features from input images to text embeddings that can be used by LLMs. We saw how this model could be used for image captioning and multimodal chat-based prompting, where both modalities are combined to generate responses. Overall, this chapter highlighted the power of multimodality in LLMs and demonstrated its applications in various areas such as image captioning, search, and chat-based prompting. ",
        "page_idx": 307
    },
    {
        "type": "text",
        "text": "In Part III of the book, we will cover training and fine-tuning techniques. In Chap‐ ter 10, we will explore how to create and fine-tune a text embedding model, which is a core technology that drives many language modeling applications. This next chapter serves as an introduction into both training and fine-tuning language models. ",
        "page_idx": 307
    },
    {
        "type": "text",
        "text": "Training and Fine-Tuning Language Models ",
        "text_level": 1,
        "page_idx": 308
    },
    {
        "type": "text",
        "text": "Creating Text Embedding Models ",
        "text_level": 1,
        "page_idx": 310
    },
    {
        "type": "text",
        "text": "Text embedding models lie at the foundation of many powerful natural language processing applications. They lay the groundwork for empowering already impres‐ sive technologies such as text generation models. We have already used embedding models throughout this book in a number of applications, such as supervised classifi‐ cation, unsupervised classification, semantic search, and even giving memory to text generation models like ChatGPT. ",
        "page_idx": 310
    },
    {
        "type": "text",
        "text": "It is nearly impossible to overstate the importance of embedding models in the field as they are the driving power behind so many applications. As such, in this chapter, we will discuss a variety of ways that we can create and fine-tune an embedding model to increase its representative and semantic power. ",
        "page_idx": 310
    },
    {
        "type": "text",
        "text": "Let’s start by discovering what embedding models are and how they generally work. ",
        "page_idx": 310
    },
    {
        "type": "text",
        "text": "Embedding Models ",
        "text_level": 1,
        "page_idx": 310
    },
    {
        "type": "text",
        "text": "Embeddings and embedding models have already been discussed in quite a number of chapters (Chapters 4, 5, and 8) thereby demonstrating their usefulness. Before going into training such a model, let’s recap what we have learned with embedding models. ",
        "page_idx": 310
    },
    {
        "type": "text",
        "text": "Unstructured textual data by itself is often quite hard to process. They are not values we can directly process, visualize, and create actionable results from. We first have to convert this textual data to something that we can easily process: numeric representations. This process is often referred to as embedding the input to output usable vectors, namely embeddings, as shown in Figure 10-1. ",
        "page_idx": 310
    },
    {
        "type": "image",
        "img_path": "images/19cdb3b3d3ecdd0195aecb0dcf193a03ed19c48fb15742f8a76fc43ac9487b06.jpg",
        "image_caption": [
            "Figure 10-1. We use an embedding model to convert textual input, such as documents, sentences, and phrases, to numerical representations, called embeddings. "
        ],
        "image_footnote": [],
        "page_idx": 311
    },
    {
        "type": "text",
        "text": "This process of embedding the input is typically performed by an LLM, which we refer to as an embedding model. The main purpose of such a model is to be as accurate as possible in representing the textual data as an embedding. ",
        "page_idx": 311
    },
    {
        "type": "text",
        "text": "However, what does it mean to be accurate in representation? Typically, we want to capture the semantic nature—the meaning—of documents. If we can capture the core of what the document communicates, we hope to have captured what the document is about. In practice, this means that we expect vectors of documents that are similar to one another to be similar, whereas the embeddings of documents that each discuss something entirely different should be dissimilar. We’ve seen this idea of semantic similarity several times already in this book, and it is visualized in Figure 10-2. This figure is a simplified example. While two-dimensional visualization helps illustrate the proximity and similarity of embeddings, these embeddings typically reside in high-dimensional spaces. ",
        "page_idx": 311
    },
    {
        "type": "image",
        "img_path": "images/594a37f946c963cbc4d050d932575da43fa013e4d5be43d2d1e75e2a9943288b.jpg",
        "image_caption": [
            "Figure 10-2. The idea of semantic similarity is that we expect textual data with similar meanings to be closer to each other in $n$ -dimensional space (two dimensions are illustra‐ ted here). "
        ],
        "image_footnote": [],
        "page_idx": 311
    },
    {
        "type": "text",
        "text": "An embedding model, however, can be trained for a number of purposes. For example, when we are building a sentiment classifier, we are more interested in the sentiment of texts than their semantic similarity. As illustrated in Figure 10-3, we can fine-tune the model such that documents are closer in n-dimensional space based on their sentiment rather than their semantic nature. ",
        "page_idx": 312
    },
    {
        "type": "text",
        "text": "Either way, an embedding model aims to learn what makes certain documents similar to one another and we can guide this process. By presenting the model with enough examples of semantically similar documents, we can steer toward semantics whereas using examples of sentiment would steer it in that direction. ",
        "page_idx": 312
    },
    {
        "type": "image",
        "img_path": "images/007d466cd083af023b5ea2e45744797488b178ba90b12b334fb546ee1be70887.jpg",
        "image_caption": [
            "Figure 10-3. In addition to semantic similarity, an embedding model can be trained to focus on sentiment similarity. In this figure, negative reviews (red) are close to one another and dissimilar to positive reviews (green). "
        ],
        "image_footnote": [],
        "page_idx": 312
    },
    {
        "type": "text",
        "text": "There are many ways in which we can train, fine-tune, and guide embedding mod‐ els, but one of the strongest and most widely used techniques is called contrastive learning. ",
        "page_idx": 312
    },
    {
        "type": "text",
        "text": "What Is Contrastive Learning? ",
        "text_level": 1,
        "page_idx": 312
    },
    {
        "type": "text",
        "text": "One major technique for both training and fine-tuning text embedding models is called contrastive learning. Contrastive learning is a technique that aims to train an embedding model such that similar documents are closer in vector space while dissimilar documents are further apart. If this sounds familiar, it’s because it’s very similar to the word2vec method from Chapter 2. We have seen this notion previously in Figures 10-2 and 10-3. ",
        "page_idx": 312
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 313
    },
    {
        "type": "text",
        "text": "The underlying idea of contrastive learning is that the best way to learn and model similarity/dissimilarity between documents is by feeding a model examples of similar and dissimilar pairs. In order to accurately capture the semantic nature of a docu‐ ment, it often needs to be contrasted with another document for a model to learn what makes it different or similar. This contrasting procedure is quite powerful and relates to the context in which documents are written. This high-level procedure is demonstrated in Figure 10-4. ",
        "page_idx": 313
    },
    {
        "type": "image",
        "img_path": "images/583d4cd94b8f6b0a5a40bfc205103be02798a184c877898cf69a292fa17816f6.jpg",
        "image_caption": [
            "Figure 10-4. Contrastive learning aims to teach an embedding model whether docu‐ ments are similar or dissimilar. It does so by presenting groups of documents to a model that are similar or dissimilar to a certain degree. "
        ],
        "image_footnote": [],
        "page_idx": 313
    },
    {
        "type": "text",
        "text": "Another way to look at contrastive learning is through the nature of explanations. A nice example of this is an anecdotal story of a reporter asking a robber “Why did you rob a bank?” to which he answers, “Because that is where the money is.”1 Although a factually correct answer, the intent of the question was not why he robs banks specifically but why he robs at all. This is called contrastive explanation and refers to understanding a particular case, “Why P?” in contrast to alternatives, “Why P and not $\\mathbf { Q } ? ^ { \\mathfrak { n } _ { 2 } }$ In the example, the question could be interpreted in a number of ways and may be best modeled by providing an alternative: “Why did you rob a bank (P) instead of obeying the law (Q)?” ",
        "page_idx": 313
    },
    {
        "type": "text",
        "text": "The importance of alternatives to the understanding of a question also applies to how an embedding learns through contrastive learning. By showing a model similar and dissimilar pairs of documents, it starts to learn what makes something similar/dissim‐ ilar and more importantly, why. ",
        "page_idx": 313
    },
    {
        "type": "text",
        "text": "For example, you could teach a model to understand what a dog is by letting it find features such as “tail,” “nose,” “four legs,” etc. This learning process can be quite difficult since features are often not well-defined and can be interpreted in a number of ways. A being with a “tail,” “nose,” and “four legs” can also be a cat. To help the model steer toward what we are interested in, we essentially ask it, “Why is this a dog and not a cat?” By providing the contrast between two concepts, it starts to learn the features that define the concept but also the features that are not related. We get more information when we frame a question as a contrast. We further illustrate this concept of contrastive explanation in Figure 10-5. ",
        "page_idx": 313
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 314
    },
    {
        "type": "image",
        "img_path": "images/549eeeec4a81d134c751b43d2d97be27034a67e800558c337bb0bf906ccba24e.jpg",
        "image_caption": [
            "Figure 10-5. When we feed an embedding model different contrasts (degrees of similar‐ ity), it starts to learn what makes things different from one another and thereby the distinctive characteristics of concepts. "
        ],
        "image_footnote": [],
        "page_idx": 314
    },
    {
        "type": "text",
        "text": "One of the earliest and most popular examples of contrastive learn‐ ing in NLP is actually word2vec, as we discussed in Chapters 1 and 2. The model learns word representations by training on individual words in a sentence. A word close to a target word in a sentence will be constructed as a positive pair whereas randomly sampled words constitute dissimilar pairs. In other words, positive examples of neighboring words are contrasted with randomly selected words that are not neighbors. Although not widely known, it is one of the first major breakthroughs in NLP that leverages contrastive learning with neural networks. ",
        "page_idx": 314
    },
    {
        "type": "text",
        "text": "There are many ways we can apply contrastive learning to create text embed‐ ding models but the most well-known technique and framework is sentencetransformers. ",
        "page_idx": 314
    },
    {
        "type": "text",
        "text": "SBERT ",
        "text_level": 1,
        "page_idx": 314
    },
    {
        "type": "text",
        "text": "Although there are many forms of contrastive learning, one framework that has popularized the technique within the natural language processing community is sentence-transformers.3 Its approach fixes a major problem with the original BERT implementation for creating sentence embeddings, namely its computational overhead. Before sentence-transformers, sentence embeddings often used an archi‐ tectural structure called cross-encoders with BERT. ",
        "page_idx": 314
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 315
    },
    {
        "type": "text",
        "text": "A cross-encoder allows two sentences to be passed to the Transformer network simultaneously to predict the extent to which the two sentences are similar. It does so by adding a classification head to the original architecture that can output a similarity score. However, the number of computations rises quickly when you want to find the highest pair in a collection of 10,000 sentences. That would require $\\mathrm { n } { \\cdot } ( \\mathrm { n } { - } 1 ) / 2$ $= 4 9 , 9 9 5 , 0 0 0$ inference computations and therefore generates significant overhead. Moreover, a cross-encoder generally does not generate embeddings, as shown in Figure 10-6. Instead, it outputs a similarity score between the input sentences. ",
        "page_idx": 315
    },
    {
        "type": "text",
        "text": "A solution to this overhead is to generate embeddings from a BERT model by averaging its output layer or using the [CLS] token. This, however, has shown to be worse than simply averaging word vectors, like GloVe.4 ",
        "page_idx": 315
    },
    {
        "type": "image",
        "img_path": "images/f8878b1561c06088bf1757067bb03b3effc446630bdfcc757c13a0090f96da60.jpg",
        "image_caption": [
            "Figure 10-6. The architecture of a cross-encoder. Both sentences are concatenated, sepa‐ rated with $\\alpha < S E P >$ token, and fed to the model simultaneously. "
        ],
        "image_footnote": [],
        "page_idx": 315
    },
    {
        "type": "text",
        "text": "Instead, the authors of sentence-transformers approached the problem differently and searched for a method that is fast and creates embeddings that can be compared semantically. The result is an elegant alternative to the original cross-encoder archi‐ tecture. Unlike a cross-encoder, in sentence-transformers the classification head is dropped, and instead mean pooling is used on the final output layer to generate an embedding. This pooling layer averages the word embeddings and gives back a fixed dimensional output vector. This ensures a fixed-size embedding. ",
        "page_idx": 315
    },
    {
        "type": "text",
        "text": "The training for sentence-transformers uses a Siamese architecture. In this archi‐ tecture, as visualized in Figure 10-7, we have two identical BERT models that share the same weights and neural architecture. These models are fed the sentences from which embeddings are generated through the pooling of token embeddings. Then, models are optimized through the similarity of the sentence embeddings. Since the weights are identical for both BERT models, we can use a single model and feed it the sentences one after the other. ",
        "page_idx": 316
    },
    {
        "type": "image",
        "img_path": "images/b45b1ff0888784dfa58438ac50b74a6504486d823b39361c8f49bfad4bb6eeab.jpg",
        "image_caption": [
            "Figure 10-7. The architecture of the original sentence-transformers model, which leverages a Siamese network, also called a bi-encoder. "
        ],
        "image_footnote": [],
        "page_idx": 316
    },
    {
        "type": "text",
        "text": "The optimization process of these pairs of sentences is done through loss functions, which can have a major impact on the model’s performance. During training, the embeddings for each sentence are concatenated together with the difference between the embeddings. Then, this resulting embedding is optimized through a softmax classifier. ",
        "page_idx": 316
    },
    {
        "type": "text",
        "text": "The resulting architecture is also referred to as a bi-encoder or SBERT for sentenceBERT. Although a bi-encoder is quite fast and creates accurate sentence representa‐ tions, cross-encoders generally achieve better performance than a bi-encoder but do not generate embeddings. ",
        "page_idx": 316
    },
    {
        "type": "text",
        "text": "The bi-encoder, like a cross-encoder, leverages contrastive learning; by optimizing the (dis)similarity between pairs of sentences, the model will eventually learn the things that make the sentences what they are. ",
        "page_idx": 316
    },
    {
        "type": "text",
        "text": "To perform contrastive learning, we need two things. First, we need data that consti‐ tutes similar/dissimilar pairs. Second, we will need to define how the model defines and optimizes similarity. ",
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "Creating an Embedding Model ",
        "text_level": 1,
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "There are many methods through which an embedding model can be created but generally, we look toward contrastive learning. This is an important aspect of many embedding models as the process allows it to efficiently learn semantic representations. ",
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "However, this is not a free process. We will need to understand how to generate contrastive examples, how to train the model, and how to properly evaluate it. ",
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "Generating Contrastive Examples ",
        "text_level": 1,
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "When pretraining your embedding model, you will often see data being used from natural language inference (NLI) datasets. NLI refers to the task of investigating whether, for a given premise, it entails the hypothesis (entailment), contradicts it (contradiction), or neither (neutral). ",
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "For example, when the premise is $^ { \\mathfrak { e } } \\mathrm { H e }$ is in the cinema watching Coco” and the hypothesis $^ { \\mathfrak { e } } \\mathrm { H e }$ is watching Frozen at home,” then these statements are contradictions. In contrast, when the premise is $^ { \\mathfrak { e } } \\mathrm { H e }$ is in the cinema watching Coco” and the hypothesis “In the movie theater he is watching the Disney movie Coco,” then these statements are considered entailment. This principle is illustrated in Figure 10-8. ",
        "page_idx": 317
    },
    {
        "type": "image",
        "img_path": "images/9634d1a72674ca0e4c0c3b0341f4fca1e824448bc6895efd62ff02314e064366.jpg",
        "image_caption": [
            "Figure 10-8. We can leverage the structure of NLI datasets to generate negative examples (contradiction) and positive examples (entailments) for contrastive learning. "
        ],
        "image_footnote": [],
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "If you look closely at entailment and contradiction, then they describe the extent to which two inputs are similar to one another. As such, we can use NLI datasets to generate negative examples (contradictions) and positive examples (entailments) for contrastive learning. ",
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "The data that we are going to be using throughout creating and fine-tuning embedding models is derived from the General Language Understanding Evaluation benchmark (GLUE). This GLUE benchmark consists of nine language understanding tasks to evaluate and analyze model performance. ",
        "page_idx": 317
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "One of these tasks is the Multi-Genre Natural Language Inference (MNLI) corpus, which is a collection of 392,702 sentence pairs annotated with entailment (contradic‐ tion, neutral, entailment). We will be using a subset of the data, 50,000 annotated sentence pairs, to create a minimal example that does not need to be trained for hours on end. Do note, though, that the smaller the dataset, the more unstable training or fine-tuning an embedding model is. If possible, larger datasets are preferred assuming it is still quality data: ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "from datasets import load_dataset ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "# Load MNLI dataset from GLUE   \n# $\\Theta =$ entailment, $ { 1 } =$ neutral, $2 \\ =$ contradiction   \ntrain_dataset $=$ load_dataset( \"glue\", \"mnli\", split=\"train\"   \n).select(range(50_000))   \ntrain_dataset $=$ train_dataset.remove_columns(\"idx\") ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "Next, we take a look at an example: ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "dataset[2] ",
        "text_level": 1,
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "{'premise': 'One of our number will carry out your instructions minutely.', 'hypothesis': 'A member of my team will execute your orders with immense precision.   \n'label': 0} ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "This shows an example of an entailment between the premise and the hypothesis as they are positively related and have near identical meanings. ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "Train Model ",
        "text_level": 1,
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "Now that we have our dataset with training examples, we will need to create our embedding model. We typically choose an existing sentence-transformers model and fine-tune that model, but in this example, we are going to train an embedding from scratch. ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "This means that we will have to define two things. First, a pretrained Transformer model that serves as embedding individual words. We will use the BERT base model (uncased) as it is a great introduction model. However, many others exist that also have been evaluated using sentence-transformers. Most notably, microsoft/ mpnet-base often gives good results when used as a word embedding model. ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "from sentence_transformers import SentenceTransformer ",
        "page_idx": 318
    },
    {
        "type": "text",
        "text": "By default, all layers of an LLM in sentence-transformers are trainable. Although it is possible to freeze certain layers, it is generally not advised since the performance is often better when unfreezing all layers. ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "Next, we will need to define a loss function over which we will optimize the model. As mentioned at the beginning of this section, one of the first instances of sentencetransformers uses softmax loss. For illustrative purposes, we are going to be using that for now, but we will go into more performant losses later on: ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "from sentence_transformers import losses ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "# Define the loss function. In softmax loss, we will also need to explicitly   \nset the number of labels.   \ntrain_loss $=$ losses.SoftmaxLoss( model $=$ embedding_model, sentence_embedding_dimension=embedding_model.get_sentence_embedding_dimen   \nsion(), num_label $; = 3$   \n) ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "Before we train our model, we define an evaluator to evaluate the model’s perfor‐ mance during training, which also determines the best model to save. ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "We can perform evaluation of the performance of our model using the Semantic Textual Similarity Benchmark (STSB). It is a collection of human-labeled sentence pairs, with similarity scores between 1 and 5. ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "We use this dataset to explore how well our model scores on this semantic similarity task. Moreover, we process the STSB data to make sure all values are between 0 and 1: ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "# Create an embedding similarity evaluator for STSB   \nval_sts $=$ load_dataset(\"glue\", \"stsb\", split=\"validation\")   \nevaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores $\\mathbf { \\Psi } _ { 1 } =$ [score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\",   \n) ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "Now that we have our evaluator, we create SentenceTransformerTrainingArgu ments, similar to training with Hugging Face Transformers (as we will explore in the next chapter): ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "# Define the training arguments ",
        "page_idx": 319
    },
    {
        "type": "text",
        "text": "args $=$ SentenceTransformerTrainingArguments( output_dir $\\ ' =$ \"base_embedding_model\", num_train_epochs $\\scriptstyle \\mathbf { \\alpha } = { \\mathbf { \\beta } }$ , per_device_train_batch_size $= 3 2$ , per_device_eval_batch_size $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , warmup_steps $\\mathord { \\left. \\vert { \\left. ^ { } = 1 \\Theta \\Theta \\right.}  \\right. }$ , fp1 $\\hat { \\bf \\Phi } = \\overrightharpoon { \\bf { \\Phi } }$ True, eval_step ${ \\displaystyle \\ i = 1 0 0 }$ , logging_step $\\begin{array} { r l } { : = } & { { } } \\end{array}$ ,   \n) ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "Of note are the following arguments: ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "num_train_epochs The number of training rounds. We keep this at 1 for faster training but it is generally advised to increase this value. ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "per_device_train_batch_size The number of samples to process simultaneously on each device (e.g., GPU or CPU) during evaluation. Higher values generally means faster training. ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "per_device_eval_batch_size ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "The number of samples to process simultaneously on each device (e.g., GPU or CPU) during evaluation. Higher values generally means faster evaluation. ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "warmup_steps ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "The number of steps during which the learning rate will be linearly increased from zero to the initial learning rate defined for the training process. Note that we did not specify a custom learning rate for this training process. ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "fp16",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "By enabling this parameter we allow for mixed precision training, where compu‐ tations are performed using 16-bit floating-point numbers (FP16) instead of the default 32-bit (FP32). This reduces memory usage and potentially increases the training speed. ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "Now that we have defined our data, embedding model, loss, and evaluator, we can start training our model. We can do that using SentenceTransformerTrainer: ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "from sentence_transformers.trainer import SentenceTransformerTrainer ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "# Train embedding model   \ntrainer $=$ SentenceTransformerTrainer( model embedding_model, args $=$ args, train_dataset $\\Bumpeq$ train_dataset, loss $=$ train_loss, evaluator $=$ evaluator   \n)   \ntrainer.train() ",
        "page_idx": 320
    },
    {
        "type": "text",
        "text": "After training our model, we can use the evaluator to get the performance on this single task: ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "# Evaluate our trained model evaluator(embedding_model) ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "{'pearson_cosine': 0.5982288436666162, 'spearman_cosine': 0.6026682018489217, 'pearson_manhattan': 0.6100690915500567, 'spearman_manhattan': 0.617732600131989, 'pearson_euclidean': 0.6079280934202278, 'spearman_euclidean': 0.6158926913905742, 'pearson_dot': 0.38364924527804595, 'spearman_dot': 0.37008497926991796, 'pearson_max': 0.6100690915500567, 'spearman_max': 0.617732600131989} ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "We get several different distance measures. The one we are interested in most is 'pearson_cosine', which is the cosine similarity between centered vectors. It is a value between 0 and 1 where a higher value indicates higher degrees of similarity. We get a value of 0.59, which we consider a baseline throughout this chapter. ",
        "page_idx": 321
    },
    {
        "type": "image",
        "img_path": "images/02fe52845e5d0f3c695678a26727075f428c0943fc52f29d64a31b8e42f572ac.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "Larger batch sizes tend to work better with multiple negative rank‐ ings (MNR) loss as a larger batch makes the task more difficult. The reason for this is that the model needs to find the best match‐ ing sentence from a larger set of potential pairs of sentences. You can adapt the code to try out different batch sizes and get a feeling of its effects. ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "In-Depth Evaluation ",
        "text_level": 1,
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "A good embedding model is more than just a good score on the STSB benchmark! As we observed earlier, the GLUE benchmark has a number of tasks for which we can evaluate our embedding model. However, there exist many more benchmarks that allow for the evaluation of embedding models. To unify this evaluation procedure, the Massive Text Embedding Benchmark (MTEB) was developed.5 The MTEB spans 8 embedding tasks that cover 58 datasets and 112 languages. ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "To publicly compare state-of-the-art embedding models, a leaderboard was created with the scores of each embedding model across all tasks: ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "from mteb import MTEB",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "# Choose evaluation task evaluation $=$ MTEB(tasks $=$ [\"Banking77Classification\"]) ",
        "page_idx": 321
    },
    {
        "type": "text",
        "text": "{'Banking77Classification': {'mteb_version': '1.1.2',   \n'dataset_revision': '0fd18e25b25c072e09e0d92ab615fda904d66300',   \n'mteb_dataset_name': 'Banking77Classification',   \n'test': {'accuracy': 0.4926298701298701,   \n'f1': 0.49083335791288685,   \n'accuracy_stderr': 0.010217785746224237,   \n'f1_stderr': 0.010265814957074591,   \n'main_score': 0.4926298701298701,   \n'evaluation_time': 31.83}}} ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "This gives us several evaluation metrics for this specific task that we can use to explore its performance. ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "The great thing about this evaluation benchmark is not only the diversity of the tasks and languages but that even the evaluation time is saved. Although many embedding models exist, we typically want those that are both accurate and have low latency. The tasks for which embedding models are used, like semantic search, often benefit from and require fast inference. ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "Since testing your model on the entire MTEB can take a couple of hours depending on your GPU, we will use the STSB benchmark throughout this chapter instead for illustration purposes. ",
        "page_idx": 322
    },
    {
        "type": "image",
        "img_path": "images/e0587241a1f1dc64d7a5a0e89ec5ccf107ec0936a73bcec4ef285784f0f96018.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "Whenever you are done training and evaluating your model, it is important to restart the notebook. This will clear your VRAM up for the next training examples throughout this chapter. By restart‐ ing the notebook, we can be sure that all VRAM is cleared. ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "Loss Functions ",
        "text_level": 1,
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "We trained our model using softmax loss to illustrate how one of the first sentencetransformers models was trained. However, not only is there a large variety of loss functions to choose from, but softmax loss is generally not advised as there are more performant losses. ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "Instead of going through every single loss function out there, there are two loss functions that are typically used and seem to perform generally well, namely: ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "• Cosine similarity • Multiple negatives ranking (MNR) loss ",
        "page_idx": 322
    },
    {
        "type": "text",
        "text": "There are many more loss functions to choose from than just those discussed here. For example, a loss like MarginMSE works great for training or fine-tuning a cross-encoder. There are a num‐ ber of interesting loss functions implemented in the sentencetransformers framework. ",
        "page_idx": 323
    },
    {
        "type": "text",
        "text": "Cosine similarity ",
        "text_level": 1,
        "page_idx": 323
    },
    {
        "type": "text",
        "text": "The cosine similarity loss is an intuitive and easy-to-use loss that works across many different use cases and datasets. It is typically used in semantic textual similarity tasks. In these tasks, a similarity score is assigned to the pairs of texts over which we optimize the model. ",
        "page_idx": 323
    },
    {
        "type": "text",
        "text": "Instead of having strictly positive or negative pairs of sentences, we assume pairs of sentences that are similar or dissimilar to a certain degree. Typically, this value lies between 0 and 1 to indicate dissimilarity and similarity, respectively (Figure 10-9). ",
        "page_idx": 323
    },
    {
        "type": "image",
        "img_path": "images/3c95c96682f96d245e44e6b766a720a399c0f7f26bc4af91a88dd2d20edbdf5a.jpg",
        "image_caption": [
            "Figure 10-9. Cosine similarity loss aims to minimize the cosine distance between seman‐ tically similar sentences and to maximize the distance between semantically dissimilar sentences. "
        ],
        "image_footnote": [],
        "page_idx": 323
    },
    {
        "type": "text",
        "text": "Cosine similarity loss is straightforward—it calculates the cosine similarity between the two embeddings of the two texts and compares that to the labeled similarity score. The model will learn to recognize the degree of similarity between sentences. ",
        "page_idx": 323
    },
    {
        "type": "text",
        "text": "Cosine similarity loss intuitively works best using data where you have pairs of sen‐ tences and labels that indicate their similarity between 0 and 1. To use this loss with our NLI dataset, we need to convert the entailment (0), neutral (1), and contradiction (2) labels to values between 0 and 1. The entailment represents a high similarity between the sentences, so we give it a similarity score of 1. In contrast, since both neutral and contradiction represent dissimilarity, we give these labels a similarity score of 0: ",
        "page_idx": 323
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "from datasets import Dataset, load_dataset   \n# Load MNLI dataset from GLUE   \n# $\\Theta =$ entailment, $ { 1 } =$ neutral, $2 \\ =$ contradiction   \ntrain_dataset $=$ load_dataset( \"glue\", \"mnli\", split $=$ \"train\"   \n).select(range(50_000))   \ntrain_dataset $=$ train_dataset.remove_columns(\"idx\")   \n# (neutral/contradiction) $\\scriptstyle = { \\boldsymbol { \\mathcal { O } } }$ and (entailment) $= \\mathcal { I }$   \nmapping $=$ {2: 0, 1: 0, 0:1}   \ntrain_dataset $=$ Dataset.from_dict({ \"sentence1\": train_dataset[\"premise\"], \"sentence2\": train_dataset[\"hypothesis\"], \"label\": [float(mapping[label]) for label in train_dataset[\"label\"]]   \n}) ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "As before, we create our evaluator: ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "# Create an embedding similarity evaluator for stsb   \nval_sts $=$ load_dataset(\"glue\", \"stsb\", split=\"validation\")   \nevaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores $=$ [score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\"   \n) ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "Then, we follow the same steps as before but select a different loss instead: ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "# Define model embedding_model $=$ SentenceTransformer(\"bert-base-uncased\") ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "# Loss function train_loss $=$ losses.CosineSimilarityLoss(model $. =$ embedding_model) ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "# Define the training arguments   \nargs $=$ SentenceTransformerTrainingArguments( output_dir=\"cosineloss_embedding_model\", num_train_epochs $^ { , = 1 }$ , per_device_train_batch_siz $\\begin{array} { r l } { \\mathbf { \\epsilon } } & { { } : \\mathbf { \\sigma } } \\\\ { \\mathbf { \\epsilon } } & { { } : \\mathbf { \\sigma } } \\end{array}$ , per_device_eval_batch_size $= 3 2$ , ",
        "page_idx": 324
    },
    {
        "type": "text",
        "text": "warmup_steps $\\mathord { \\left. \\vert { \\left. ^ { } = 1 \\Theta \\Theta \\right.}  \\right. }$ , fp1 $\\dot { \\bf \\nu } =$ True, eval_steps ${ \\tt \\Gamma } = 1 0 0$ , logging_step $\\quad : = \\quad$ , ) ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "# Train model   \ntrainer $=$ SentenceTransformerTrainer( model $=$ embedding_model, args $=$ args, train_dataset=train_dataset, loss $=$ train_loss, evaluator=evaluator   \n)   \ntrainer.train() ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "Evaluating the model after training gives us the following score: ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "# Evaluate our trained model evaluator(embedding_model) ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "{'pearson_cosine': 0.7222322163831805, 'spearman_cosine': 0.7250508271229599, 'pearson_manhattan': 0.7338163436711481, 'spearman_manhattan': 0.7323479193408869, 'pearson_euclidean': 0.7332716434966307, 'spearman_euclidean': 0.7316999722750905, 'pearson_dot': 0.660366792336156, 'spearman_dot': 0.6624167554844425, 'pearson_max': 0.7338163436711481, 'spearman_max': 0.7323479193408869} ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "A Pearson cosine score of 0.72 is a big improvement compared to the softmax loss example, which scored 0.59. This demonstrates the impact the loss function can have on performance. ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "Make sure to restart your notebook so we can explore a more common and perform‐ ant loss, namely multiple negatives ranking loss. ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "Multiple negatives ranking loss ",
        "text_level": 1,
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "Multiple negatives ranking (MNR) loss,6 often referred to as InfoNCE7 or NTXen‐ tLoss,8 is a loss that uses either positive pairs of sentences or triplets that contain a pair of positive sentences and an additional unrelated sentence. This unrelated sentence is called a negative and represents the dissimilarity between the positive sentences. ",
        "page_idx": 325
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 326
    },
    {
        "type": "text",
        "text": "For example, you might have pairs of question/answer, image/image caption, paper title/paper abstract, etc. The great thing about these pairs is that we can be confident they are hard positive pairs. In MNR loss (Figure 10-10), negative pairs are construc‐ ted by mixing a positive pair with another positive pair. In the example of a paper title and abstract, you would generate a negative pair by combining the title of a paper with a completely different abstract. These negatives are called in-batch negatives and can also be used to generate the triplets. ",
        "page_idx": 326
    },
    {
        "type": "image",
        "img_path": "images/081ec99e11d77a7aeb81036ea404436f86df48ce9744c87f67c11845ace76cbc.jpg",
        "image_caption": [
            "Figure 10-10. Multiple negatives ranking loss aims to minimize the distance between related pairs of text, such as questions and answers, and maximize the distance between unrelated pairs, such as questions and unrelated answers. "
        ],
        "image_footnote": [],
        "page_idx": 326
    },
    {
        "type": "text",
        "text": "After having generated these positive and negative pairs, we calculate their embed‐ dings and apply cosine similarity. These similarity scores are then used to answer the question, are these pairs negative or positive? In other words, it is treated as a classification task and we can use cross-entropy loss to optimize the model. ",
        "page_idx": 326
    },
    {
        "type": "text",
        "text": "To make these triplets we start with an anchor sentence (i.e., labeled as the “prem‐ ise”), which is used to compare other sentences. Then, using the MNLI dataset, we only select sentence pairs that are positive (i.e., labeled as “entailment”). To add negative sentences, we randomly sample sentences as the “hypothesis.” ",
        "page_idx": 326
    },
    {
        "type": "text",
        "text": "import random   \nfrom tqdm import tqdm   \nfrom datasets import Dataset, load_dataset   \n# # Load MNLI dataset from GLUE   \nmnli $=$ load_dataset(\"glue\", \"mnli\", split $=$ \"train\").select(range(50_000))   \nmnli $=$ mnli.remove_columns(\"idx\") ",
        "page_idx": 326
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 326
    },
    {
        "type": "text",
        "text": "mnli $=$ mnli.filter(lambda x: True if $\\times [ \\mathrm {  ~ \\bar { ~ } { ~ \\bf ~ \\alpha ~ } ~ } ] = \\mathrm {  ~ \\bar { ~ } { ~ \\bf ~ \\alpha ~ } ~ }$ else False) ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "# Prepare data and add a soft negative   \ntrain_dataset $=$ {\"anchor\": [], \"positive\": [], \"negative\": []}   \nsoft_negatives $=$ mnli[\"hypothesis\"]   \nrandom.shuffle(soft_negatives)   \nfor row, soft_negative in tqdm(zip(mnli, soft_negatives)): train_dataset[\"anchor\"].append(row[\"premise\"]) train_dataset[\"positive\"].append(row[\"hypothesis\"]) train_dataset[\"negative\"].append(soft_negative)   \ntrain_dataset $=$ Dataset.from_dict(train_dataset) ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "Since we only selected sentences labeled with “entailment,” the number of rows reduced quite a a bit from 50,000 to 16,875 rows. ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "Let’s define the evaluator: ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator   \n# Create an embedding similarity evaluator for stsb   \nval_sts $=$ load_dataset(\"glue\", \"stsb\", split=\"validation\")   \nevaluator $=$ EmbeddingSimilarityEvaluator( sentences $1 =$ val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores $\\mathbf { \\Psi } _ { 1 } =$ [score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\"   \n) ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "We then train as before but with MNR loss instead: ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "# Define model embedding_model $=$ SentenceTransformer('bert-base-uncased') ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "# Loss function train_loss $=$ losses.MultipleNegativesRankingLoss(model=embedding_model) ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "# Define the training arguments   \nargs $=$ SentenceTransformerTrainingArguments( output_dir $=$ \"mnrloss_embedding_model\", num_train_epochs $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ , per_device_train_batch_size $= 3 2$ , per_device_eval_batch_size $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , warmup_steps $\\begin{array} { r l } { \\mathrm { ~  ~ \\tau ~ } } & { { } = } \\\\ { \\mathrm { ~  ~ \\tau ~ } } & { { } = } \\end{array}$ , fp16=True, eval_steps ${ \\tt \\Gamma } = 1 0 0$ , logging_step $\\begin{array} { r } { \\ \\vdots = \\ } \\\\ { \\mathfrak { d } \\mathfrak { d } } \\end{array}$ ,   \n) ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "# Train model ",
        "page_idx": 327
    },
    {
        "type": "text",
        "text": "trainer $=$ SentenceTransformerTrainer( model embedding_model, args $=$ args, train_dataset $\\Bumpeq$ train_dataset, loss $=$ train_loss, evaluator=evaluator   \n)   \ntrainer.train() ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "Let’s see how this dataset and loss function compare to our previous examples: ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "# Evaluate our trained model evaluator(embedding_model) ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "{'pearson_cosine': 0.8093892326162132, 'spearman_cosine': 0.8121064796503025, 'pearson_manhattan': 0.8215001523827565, 'spearman_manhattan': 0.8172161486524246, 'pearson_euclidean': 0.8210391407846718, 'spearman_euclidean': 0.8166537141010816, 'pearson_dot': 0.7473360302629125, 'spearman_dot': 0.7345184137194012, 'pearson_max': 0.8215001523827565, 'spearman_max': 0.8172161486524246} ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "Compared to our previously trained model with softmax loss (0.72), our model with MNR loss (0.80) seems to be much more accurate! ",
        "page_idx": 328
    },
    {
        "type": "image",
        "img_path": "images/0d645c4bf91765b5c1e2d41d45620f6400a6c52be4bd356c2323ac400f5594f5.jpg",
        "image_caption": [],
        "image_footnote": [],
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "Larger batch sizes tend to be better with MNR loss as a larger batch makes the task more difficult. The reason for this is that the model needs to find the best matching sentence from a larger set of potential pairs of sentences. You can adapt the code to try out different batch sizes and get a feeling of the effects. ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "There is a downside to how we used this loss function. Since negatives are sampled from other question/answer pairs, these in-batch or “easy” negatives that we used could potentially be completely unrelated to the question. As a result, the embedding model’s task of then finding the right answer to a question becomes quite easy. Instead, we would like to have negatives that are very related to the question but not the right answer. These negatives are called hard negatives. Since this would make the task more difficult for the embedding model as it has to learn more nuanced representations, the embedding model’s performance generally improves quite a bit. ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "A good example of a hard negative is the following. Let’s assume we have the follow‐ ing question: “How many people live in Amsterdam?” A related answer to this ques‐ tion would be: “Almost a million people live in Amsterdam.” To generate a good hard negative, we ideally want the answer to contain something about Amsterdam and the number of people living in this city. For example: “More than a million people live in Utrecht, which is more than Amsterdam.” This answer relates to the question but is not the actual answer, so this would be a good hard negative. Figure 10-11 illustrates the differences between easy and hard negatives. ",
        "page_idx": 328
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 329
    },
    {
        "type": "image",
        "img_path": "images/7614ab02926e990ef35f433bf92279bb37a45f966f6feec7ad002afd7191aae0.jpg",
        "image_caption": [
            "Figure 10-11. An easy negative is typically unrelated to both the question and answer. A semi-hard negative has some similarities to the topic of the question and answer but is somewhat unrelated. A hard negative is very similar to the question but is generally the wrong answer. "
        ],
        "image_footnote": [],
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Gathering negatives can roughly be divided into the following three processes: ",
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Easy negatives Through randomly sampling documents as we did before. ",
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Semi-hard negatives ",
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Using a pretrained embedding model, we can apply cosine similarity on all sentence embeddings to find those that are highly related. Generally, this does not lead to hard negatives since this method merely finds similar sentences, not question/answer pairs. ",
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Hard negatives ",
        "text_level": 1,
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "These often need to be either manually labeled (for instance, by generating semihard negatives) or you can use a generative model to either judge or generate sentence pairs. ",
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Make sure to restart your notebook so we can explore the different methods of fine-tuning embedding models. ",
        "page_idx": 329
    },
    {
        "type": "text",
        "text": "Fine-Tuning an Embedding Model ",
        "text_level": 1,
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "In the previous section, we went through the basics of training an embedding model from scratch and saw how we could leverage loss functions to further optimize its performance. This approach, although quite powerful, requires creating an embed‐ ding model from scratch. This process can be quite costly and time-consuming. ",
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "Instead, the sentence-transformers framework allows nearly all embedding models to be used as a base for fine-tuning. We can choose an embedding model that was already trained on a large amount of data and fine-tune it for our specific data or purpose. ",
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "There are several ways to fine-tune your model, depending on the data availability and domain. We will go through two such methods and demonstrate the strength of leveraging pretrained embedding models. ",
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "Supervised ",
        "text_level": 1,
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "The most straightforward way to fine-tune an embedding model is to repeat the process of training our model as we did before but replace the 'bert-base-uncased' with a pretrained sentence-transformers model. There are many to choose from but generally, all-MiniLM-L6-v2 performs well across many use cases and due to its small size is quite fast. ",
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "We use the same data as we used to train our model in the MNR loss example but instead use a pretrained embedding model to fine-tune. As always, let’s start by loading the data and creating the evaluator: ",
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "from datasets import load_dataset   \nfrom sentence_transformers.evaluation import EmbeddingSimilarityEvaluator   \n# Load MNLI dataset from GLUE   \n# $\\Theta =$ entailment, $ { 1 } =$ neutral, $2 \\ =$ contradiction   \ntrain_dataset $=$ load_dataset( \"glue\", \"mnli\", split=\"train\"   \n).select(range(50_000))   \ntrain_dataset $=$ train_dataset.remove_columns(\"idx\")   \n# Create an embedding similarity evaluator for stsb   \nval_sts $=$ load_dataset(\"glue\", \"stsb\", split $\\equiv$ \"validation\")   \nevaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences $2 =$ val_sts[\"sentence2\"], scores $=$ [score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\"   \n) ",
        "page_idx": 330
    },
    {
        "type": "text",
        "text": "The training steps are similar to our previous examples but instead of using 'bertbase-uncased', we can use a pretrained embedding model instead: ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "# Define model embedding_model $=$ SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2') ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "# Loss function train_loss $=$ losses.MultipleNegativesRankingLoss(model=embedding_model) ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "# Define the training arguments   \nargs $=$ SentenceTransformerTrainingArguments( output_dir $\\ ' =$ \"finetuned_embedding_model\", num_train_epochs $^ { - 1 }$ , per_device_train_batch_size $= 3 2$ , per_device_eval_batch_size $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , warmup_steps $\\mathord { \\left. \\vert { \\left. ^ { } = 1 \\Theta \\Theta \\right.}  \\right. }$ , fp1 $\\hat { \\bf \\Phi } =$ True, eval_steps ${ \\tt \\Gamma } = 1 0 0$ , logging_step $\\mathord { \\left. = \\right.} 1 0 0 $ ,   \n)   \n# Train model   \ntrainer $=$ SentenceTransformerTrainer( model $=$ embedding_model, args $=$ args, train_dataset $\\Bumpeq$ train_dataset, loss $=$ train_loss, evaluator=evaluator   \n)   \ntrainer.train() ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "Evaluating this model gives us the following score: ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "# Evaluate our trained model evaluator(embedding_model) ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "{'pearson_cosine': 0.8509553350510896, 'spearman_cosine': 0.8484676559567688, 'pearson_manhattan': 0.8503896832470704, 'spearman_manhattan': 0.8475760325664419, 'pearson_euclidean': 0.8513115442079158, 'spearman_euclidean': 0.8484676559567688, 'pearson_dot': 0.8489553386816947, 'spearman_dot': 0.8484676559567688, 'pearson_max': 0.8513115442079158, 'spearman_max': 0.8484676559567688} ",
        "page_idx": 331
    },
    {
        "type": "text",
        "text": "Although a score of 0.85 is the highest we have seen thus far, the pretrained model that we used for fine-tuning was already trained on the full MNLI dataset, whereas we only used 50,000 examples. It might seem redundant but this example demonstrates how to fine-tune a pretrained embedding model on your own data. ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "Instead of using a pretrained BERT model like 'bert-baseuncased' or a possible out-of-domain model like 'all-mpnetbase-v2', you can also perform masked language modeling on the pretrained BERT model to first adapt it to your domain. Then, you can use this fine-tuned BERT model as the base for training your embedding model. This is a form of domain adaptation. In the next chapter, we will apply masked language modeling on a pretrained model. ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "Note that the main difficulty of training or fine-tuning your model is finding the right data. With these models, we not only want to have very large datasets, but the data in itself needs to be of high quality. Developing positive pairs is generally straightforward but adding hard negative pairs significantly increases the difficulty of creating quality data. ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "As always, restart your notebook to free up VRAM for the following examples. ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "Augmented SBERT ",
        "text_level": 1,
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "A disadvantage of training or fine-tuning these embedding models is that they often require substantial training data. Many of these models are trained with more than a billion sentence pairs. Extracting such a high number of sentence pairs for your use case is generally not possible as in many cases, there are only a couple of thousand labeled data points available. ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "Fortunately, there is a way to augment your data such that an embedding model can be fine-tuned when there is only a little labeled data available. This procedure is referred to as Augmented SBERT.9 ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "In this procedure, we aim to augment the small amount of labeled data such that they can be used for regular training. It makes use of the slow and more accurate cross-encoder architecture (BERT) to augment and label a larger set of input pairs. These newly labeled pairs are then used for fine-tuning a bi-encoder (SBERT). ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "As shown in Figure 10-12, Augmented SBERT involves the following steps: ",
        "page_idx": 332
    },
    {
        "type": "text",
        "text": "1. Fine-tune a cross-encoder (BERT) using a small, annotated dataset (gold   \ndataset).   \n2. Create new sentence pairs.   \n3. Label new sentence pairs with the fine-tuned cross-encoder (silver dataset).   \n4. Train a bi-encoder (SBERT) on the extended dataset (gold $^ +$ silver dataset). ",
        "page_idx": 333
    },
    {
        "type": "text",
        "text": "Here, a gold dataset is a small but fully annotated dataset that holds the ground truth. A silver dataset is also fully annotated but is not necessarily the ground truth as it was generated through predictions of the cross-encoder. ",
        "page_idx": 333
    },
    {
        "type": "image",
        "img_path": "images/da89cd23f7ccc533f40c37faf10b95e02b9ad85645e23c2f2c643db4f308fbf6.jpg",
        "image_caption": [
            "Figure 10-12. Augmented SBERT works through training a cross-encoder on a small gold dataset, then using that to label an unlabeled dataset to generate a larger silver dataset. Finally, both the gold and silver datasets are used to train the bi-encoder. "
        ],
        "image_footnote": [],
        "page_idx": 333
    },
    {
        "type": "text",
        "text": "Before we get into the preceding steps, let’s first prepare the data. Instead of our original 50,000 documents, we take a subset of 10,000 documents to simulate a setting where we have limited annotated data. As we did in our example with cosine similarity loss, give entailment a score of 1 whereas neutral and contradiction get a score of 0: ",
        "page_idx": 333
    },
    {
        "type": "text",
        "text": "import pandas as pd   \nfrom tqdm import tqdm   \nfrom datasets import load_dataset, Dataset   \nfrom sentence_transformers import InputExample   \nfrom sentence_transformers.datasets import NoDuplicatesDataLoader   \n# Prepare a small set of 10000 documents for the cross-encoder   \ndataset $=$ load_dataset(\"glue\", \"mnli\", split $\\mathbf { \\varepsilon } =$ \"train\").select(range(10_000))   \nmapping $=$ {2: 0, 1: 0, 0:1}   \n# Data loader   \ngold_examples $=$ [ InputExample(texts $\\mathbf { \\equiv }$ [row[\"premise\"], row[\"hypothesis\"]], label=map   \nping[row[\"label\"]]) for row in tqdm(dataset)   \n] ",
        "page_idx": 333
    },
    {
        "type": "text",
        "text": "gold_dataloader $=$ NoDuplicatesDataLoader(gold_examples, batch_size $= 3 2$ ) ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "# Pandas DataFrame for easier data handling   \ngold $=$ pd.DataFrame( { \"sentence1\": dataset[\"premise\"], \"sentence2\": dataset[\"hypothesis\"], \"label\": [mapping[label] for label in dataset[\"label\"]] }   \n) ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "This is the gold dataset since it is labeled and represents our ground truth. ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "Using this gold dataset, we train our cross-encoder (step 1): ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "from sentence_transformers.cross_encoder import CrossEncoder   \n# Train a cross-encoder on the gold dataset   \ncross_encoder $=$ CrossEncoder(\"bert-base-uncased\", num_labels $\\scriptstyle : = \\ .$ )   \ncross_encoder.fit( train_dataloader $: =$ gold_dataloader, epochs $^ { - 1 }$ , show_progress_bar $: =$ True, warmup_steps $\\mathord { \\left. \\vert { \\left. ^ { } = 1 \\Theta \\Theta \\right.}  \\right. }$ , use_amp=False   \n) ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "After training our cross-encoder, we use the remaining 400,000 sentence pairs (from our original dataset of 50,000 sentence pairs) as our silver dataset (step 2): ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "# Prepare the silver dataset by predicting labels with the cross-encoder   \nsilver $=$ load_dataset( \"glue\", \"mnli\", split $=$ \"train\"   \n).select(range(10_000, 50_000))   \npairs $=$ list(zip(silver[\"premise\"], silver[\"hypothesis\"])) ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "If you do not have any additional unlabeled sentence pairs, you can randomly sample them from your original gold dataset. To illustrate, you can create a new sentence pair by taking the premise from one row and the hypothesis from another. This allows you to easily generate 10 times as many sentence pairs that can be labeled with the cross-encoder. ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "This strategy, however, likely generates significantly more dissimi‐ lar than similar pairs. Instead, we can use a pretrained embedding model to embed all candidate sentence pairs and retrieve the top-k sentences for each input sentence using semantic search. This rough reranking process allows us to focus on sentence pairs that are likely to be more similar. Although the sentences are still chosen based on an approximation since the pretrained embedding model was not trained on our data, it is much better than random sampling. ",
        "page_idx": 334
    },
    {
        "type": "text",
        "text": "Note that we assume that these sentence pairs are unlabeled in this example. We will use our fine-tuned cross-encoder to label these sentence pairs (step 3): ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "import numpy as np ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "# Label the sentence pairs using our fine-tuned cross-encoder   \noutput $=$ cross_encoder.predict( pairs, apply_softmax $\\equiv$ True,   \nshow_progress_bar=True   \n)   \nsilver $=$ pd.DataFrame( { \"sentence1\": silver[\"premise\"], \"sentence2\": silver[\"hypothesis\"], \"label\": np.argmax(output, axis $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ ) }   \n) ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "Now that we have a silver and gold dataset, we simply combine them and train our embedding model as we did before: ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "# Combine gold $^ +$ silver data $=$ pd.concat([gold, silver], ignore_index $\\equiv$ True, axis $\\scriptstyle = 0$ ) data $=$ data.drop_duplicates(subset=[\"sentence1\", \"sentence2\"], keep $^ { 1 = }$ \"first\") train_dataset $=$ Dataset.from_pandas(data, preserve_index=False) ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "As always, we need to define our evaluator: ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "# Create an embedding similarity evaluator for stsb   \nval_sts $=$ load_dataset(\"glue\", \"stsb\", split=\"validation\")   \nevaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores $=$ [score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\"   \n) ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "We train the model the same as before except now we use the augmented dataset: ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "from sentence_transformers import losses, SentenceTransformer from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "# Define model embedding_model $=$ SentenceTransformer(\"bert-base-uncased\") ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "# Loss function train_loss $=$ losses.CosineSimilarityLoss(model $. =$ embedding_model) ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "# Define the training arguments args $=$ SentenceTransformerTrainingArguments( ",
        "page_idx": 335
    },
    {
        "type": "text",
        "text": "output_dir $=$ \"augmented_embedding_model\", num_train_epochs $^ { = 1 }$ , per_device_train_batch_size $: = 3 2$ , per_device_eval_batch_size $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , warmup_steps $\\mathord { \\left. \\vert { \\left. ^ { } = 1 \\Theta \\Theta \\right.}  \\right. }$ , fp1 $\\mathrm { { } } ; = \\mathrm { { } }$ True, eval_steps ${ \\tt \\Gamma } = 1 0 0$ , logging_step ${ \\displaystyle \\langle = 1 0 0 }$ , ) ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "# Train model   \ntrainer $=$ SentenceTransformerTrainer( model embedding_model, args $=$ args, train_dataset=train_dataset, loss $=$ train_loss, evaluator=evaluator   \n)   \ntrainer.train() ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "Finally, we evaluate the model: ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "evaluator(embedding_model) ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "{'pearson_cosine': 0.7101597020018693, 'spearman_cosine': 0.7210536464320728, 'pearson_manhattan': 0.7296749443525249, 'spearman_manhattan': 0.7284184255293913, 'pearson_euclidean': 0.7293097297208753, 'spearman_euclidean': 0.7282830906742256, 'pearson_dot': 0.6746605824703588, 'spearman_dot': 0.6754486790570754, 'pearson_max': 0.7296749443525249, 'spearman_max': 0.7284184255293913} ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "The original cosine similarity loss example had a score of 0.72 with the full dataset. Using only $2 0 \\%$ of that data, we managed to get a score of 0.71! ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "This method allows us to increase the size of datasets that you already have available without the need to manually label hundreds of thousands of sentence pairs. You can test the quality of your silver data by also training your embedding model only on the gold dataset. The difference in performance indicates how much your silver dataset potentially adds to the quality of the model. ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "You can restart your notebook a final time for the last example, namely unsupervised learning. ",
        "page_idx": 336
    },
    {
        "type": "text",
        "text": "Unsupervised Learning ",
        "text_level": 1,
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "To create an embedding model, we typically need labeled data. However, not all real-world datasets come with a nice set of labels that we can use. We instead look for techniques to train the model without any predetermined labels—unsuper‐ vised learning. Many approaches exist, like Simple Contrastive Learning of Sentence Embeddings (SimCSE),10 Contrastive Tension (CT),11 Transformer-based Sequential Denoising Auto-Encoder (TSDAE),12 and Generative Pseudo-Labeling (GPL).13 ",
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "In this section, we will focus on TSDAE, as it has shown great performance on unsupervised tasks as well as domain adaptation. ",
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "Transformer-Based Sequential Denoising Auto-Encoder ",
        "text_level": 1,
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "TSDAE is a very elegant approach to creating an embedding model with unsuper‐ vised learning. The method assumes that we have no labeled data at all and does not require us to artificially create labels. ",
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "The underlying idea of TSDAE is that we add noise to the input sentence by remov‐ ing a certain percentage of words from it. This “damaged” sentence is put through an encoder, with a pooling layer on top of it, to map it to a sentence embedding. From this sentence embedding, a decoder tries to reconstruct the original sentence from the “damaged” sentence but without the artificial noise. The main concept here is that the more accurate the sentence embedding is, the more accurate the reconstructed sentence will be. ",
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "This method is very similar to masked language modeling, where we try to recon‐ struct and learn certain masked words. Here, instead of reconstructing masked words, we try to reconstruct the entire sentence. ",
        "page_idx": 337
    },
    {
        "type": "text",
        "text": "After training, we can use the encoder to generate embeddings from text since the decoder is only used for judging whether the embeddings can accurately reconstruct the original sentence (Figure 10-13). ",
        "page_idx": 337
    },
    {
        "type": "image",
        "img_path": "images/bbda92faf7ca0ae117173787cb75f00e3ea9727aa7539108cc0f7cbf8d6d6c2a.jpg",
        "image_caption": [
            "Figure 10-13. TSDAE randomly removes words from an input sentence that is passed through an encoder to generate a sentence embedding. From this sentence embedding, the original sentence is reconstructed. "
        ],
        "image_footnote": [],
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "Since we only need a bunch of sentences without any labels, training this model is straightforward. We start by downloading an external tokenizer, which is used for the denoising procedure: ",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "# Download additional tokenizer import nltk nltk.download(\"punkt\") ",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "Then, we create flat sentences from our data and remove any labels that we have to mimic an unsupervised setting: ",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "from tqdm import tqdm   \nfrom datasets import Dataset, load_dataset   \nfrom sentence_transformers.datasets import DenoisingAutoEncoderDataset   \n# Create a flat list of sentences   \nmnli $=$ load_dataset(\"glue\", \"mnli\", split $=$ \"train\").select(range(25_000))   \nflat_sentences $=$ mnli[\"premise\"] $^ +$ mnli[\"hypothesis\"] ",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "# Add noise to our input data damaged_data $=$ DenoisingAutoEncoderDataset(list(set(flat_sentences))) ",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "# Create dataset train_dataset $=$ {\"damaged_sentence\": [], \"original_sentence\": []} ",
        "page_idx": 338
    },
    {
        "type": "text",
        "text": "for data in tqdm(damaged_data): train_dataset[\"damaged_sentence\"].append(data.texts[0]) train_dataset[\"original_sentence\"].append(data.texts[1])   \ntrain_dataset $=$ Dataset.from_dict(train_dataset) ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "This creates a dataset of 50,000 sentences. When we inspect the data, notice that the first sentence is the damaged sentence and the second sentence the original: ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "train_dataset[0] ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "{'damaged_sentence': 'Grim jaws are. 'original_sentence': 'Grim faces and hardened jaws are not people-friendly.'} ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "The first sentence shows the “noisy” data whereas the second shows the original input sentence. After creating our data, we define our evaluator as before: ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "# Create an embedding similarity evaluator for stsb   \nval_sts $=$ load_dataset(\"glue\", \"stsb\", split=\"validation\")   \nevaluator $=$ EmbeddingSimilarityEvaluator( sentences1=val_sts[\"sentence1\"], sentences2=val_sts[\"sentence2\"], scores $=$ [score/5 for score in val_sts[\"label\"]], main_similarity=\"cosine\"   \n) ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "Next, we run the training as before but with the [CLS] token as the pooling strategy instead of the mean pooling of the token embeddings. In the TSDAE paper, this was shown to be more effective since mean pooling loses the position information, which is not the case when using the [CLS] token: ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "from sentence_transformers import models, SentenceTransformer ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "# Create your embedding model   \nword_embedding_model $=$ models.Transformer(\"bert-base-uncased\")   \npooling_model $=$ models.Pooling(word_embedding_model.get_word_embedding_dimen   \nsion(), \"cls\")   \nembedding_model $=$ SentenceTransformer(modules $=$ [word_embedding_model, pool   \ning_model]) ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "Using our sentence pairs, we will need a loss function that attempts to reconstruct the original sentence using the noise sentence, namely DenoisingAutoEncoderLoss. By doing so, it will learn how to accurately represent the data. It is similar to masking but without knowing where the actual masks are. ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "Moreover, we tie the parameters of both models. Instead of having separate weights for the encoder’s embedding layer and the decoder’s output layer, they share the same weights. This means that any updates to the weights in one layer will be reflected in the other layer as well: ",
        "page_idx": 339
    },
    {
        "type": "text",
        "text": "from sentence_transformers import losses ",
        "text_level": 1,
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "# Use the denoising auto-encoder loss train_loss $=$ losses.DenoisingAutoEncoderLoss( embedding_model, tie_encoder_decoder=True ) train_loss.decoder $=$ train_loss.decoder.to(\"cuda\") ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "Finally, training our model works the same as we have seen several times before but we lower the batch size as memory increases with this loss function: ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "from sentence_transformers.trainer import SentenceTransformerTrainer from sentence_transformers.training_args import SentenceTransformerTrainingArgu ments ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "# Define the training arguments   \nargs $=$ SentenceTransformerTrainingArguments( output_dir=\"tsdae_embedding_model\", num_train_epochs $^ { = 1 }$ , per_device_train_batch_size $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , per_device_eval_batch_size $= 1 6$ , warmup_steps $\\mathord { \\left. \\vert { \\left. ^ { } = 1 \\Theta \\Theta \\right.}  \\right. }$ , fp1 ${ \\sf 6 = }$ True, eval_step ${ \\displaystyle \\ i = 1 0 0 }$ , logging_step $\\quad : = \\quad$ ,   \n)   \n# Train model   \ntrainer $=$ SentenceTransformerTrainer( model $=$ embedding_model, args $=$ args, train_dataset=train_dataset, loss $=$ train_loss, evaluator $=$ evaluator   \n)   \ntrainer.train() ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "After training, we evaluate our model to explore how well such an unsupervised technique performs: ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "# Evaluate our trained model evaluator(embedding_model) ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "{'pearson_cosine': 0.6991809700971775, 'spearman_cosine': 0.713693213167873, 'pearson_manhattan': 0.7152343356643568, 'spearman_manhattan': 0.7201441944880915, 'pearson_euclidean': 0.7151142243297436, 'spearman_euclidean': 0.7202291660769805, 'pearson_dot': 0.5198066451871277, 'spearman_dot': 0.5104025515225046, 'pearson_max': 0.7152343356643568, 'spearman_max': 0.7202291660769805} ",
        "page_idx": 340
    },
    {
        "type": "text",
        "text": "After fitting our model, we got a score of 0.70, which is quite impressive considering we did all this training with unlabeled data. ",
        "page_idx": 341
    },
    {
        "type": "text",
        "text": "Using TSDAE for Domain Adaptation ",
        "text_level": 1,
        "page_idx": 341
    },
    {
        "type": "text",
        "text": "When you have very little or no labeled data available, you typically use unsupervised learning to create your text embedding model. However, unsupervised techniques are generally outperformed by supervised techniques and have difficulty learning domain-specific concepts. ",
        "page_idx": 341
    },
    {
        "type": "text",
        "text": "This is where domain adaptation comes in. Its goal is to update existing embedding models to a specific textual domain that contains different subjects from the source domain. Figure 10-14 demonstrates how domains can differ in content. The target domain, or out-domain, generally contains words and subjects that were not found in the source domain or in-domain. ",
        "page_idx": 341
    },
    {
        "type": "image",
        "img_path": "images/3be549cc4c2e7abbc6daf6a6937f3519915fcd2bbd945437b447da7ec7475268.jpg",
        "image_caption": [
            "Figure 10-14. In domain adaptation, the aim is to create and generalize an embedding model from one domain to another. "
        ],
        "image_footnote": [],
        "page_idx": 341
    },
    {
        "type": "text",
        "text": "One method for domain adaptation is called adaptive pretraining. You start by pre‐ training your domain-specific corpus using an unsupervised technique, such as the previously discussed TSDAE or masked language modeling. Then, as illustrated in Figure 10-15, you fine-tune that model using a training dataset that can be either outside or in your target domain. Although data from the target domain is preferred, out-domain data also works since we started with unsupervised training on the target domain. ",
        "page_idx": 341
    },
    {
        "type": "image",
        "img_path": "images/24d52c947bfd367159748ea17225ab9c6b18662d1e58cc55ffc3d5853be8578a.jpg",
        "image_caption": [
            "Figure 10-15. Domain adaptation can be performed with adaptive pretraining and adaptive fine-tuning. "
        ],
        "image_footnote": [],
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "Using everything you have learned in this chapter, you should be able to reproduce this pipeline! First, you can start with TSDAE to train an embedding model on your target domain and then fine-tune it using either general supervised training or Augmented SBERT. ",
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "In this chapter, we looked at creating and fine-tuning embedding models through various tasks. We discussed the concept of embeddings and their role in representing textual data in a numerical format. We then explored the foundational technique of many embedding models, namely contrastive learning, which learns primarily from (dis)similar pairs of documents. ",
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "Using a popular embedding framework, sentence-transformers, we then created embedding models using a pretrained BERT model while exploring different loss functions, such as cosine similarity loss and MNR loss. We discussed how the collec‐ tion of (dis)similar pairs or triples of documents is vital to the performance of the resulting model. ",
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "In the sections that followed, we explored techniques for fine-tuning embedding models. Both supervised and unsupervised techniques were discussed such as Aug‐ mented SBERT and TSDAE for domain adaptation. Compared to creating an embed‐ ding model, fine-tuning generally needs less data and is a great way to adapt existing embedding models to your domain. ",
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "In the next chapter, methods for fine-tuning representations for classification will be discussed. Both BERT models and embedding models will make an appearance as well as a wide range of fine-tuning techniques. ",
        "page_idx": 342
    },
    {
        "type": "text",
        "text": "Fine-Tuning Representation Models for Classification ",
        "text_level": 1,
        "page_idx": 344
    },
    {
        "type": "text",
        "text": "In Chapter 4, we used pretrained models to classify our text. We kept the pretrained models as they were without any modifications to them. This might make you wonder, what happens if we were to fine-tune them? ",
        "page_idx": 344
    },
    {
        "type": "text",
        "text": "If we have sufficient data, fine-tuning tends to lead to some of the best-performing models possible. In this chapter, we will go through several methods and applications for fine-tuning BERT models. “Supervised Classification” on page 323 demonstrates the general process of fine-tuning a classification model. Then, in “Few-Shot Classifi‐ cation” on page 333, we look at SetFit, which is a method for efficiently fine-tuning a high-performing model using a small number of training examples. In “Continued Pretraining with Masked Language Modeling” on page 340, we will explore how to continue training a pretrained model. Lastly, classification on a token level is explored in “Named-Entity Recognition” on page 345. ",
        "page_idx": 344
    },
    {
        "type": "text",
        "text": "We will focus on nongenerative tasks, as generative models will be covered in Chapter 12. ",
        "page_idx": 344
    },
    {
        "type": "text",
        "text": "Supervised Classification ",
        "text_level": 1,
        "page_idx": 344
    },
    {
        "type": "text",
        "text": "In Chapter 4, we explored supervised classification tasks by leveraging pretrained representation models that were either trained to predict sentiment (task-specific model) or to generate embeddings (embedding model), as shown in Figure 11-1. ",
        "page_idx": 344
    },
    {
        "type": "image",
        "img_path": "images/ff3ee819dc97446e0f37a71de6d656c83cc4efb3cf9cc2f508e88e454b445cf3.jpg",
        "image_caption": [
            "Figure 11-1. In Chapter 4, we used pretrained models to perform classification without updating their weight. These models were kept “frozen.” "
        ],
        "image_footnote": [],
        "page_idx": 345
    },
    {
        "type": "text",
        "text": "Both models were kept frozen (nontrainable) to showcase the potential of leveraging pretrained models for classification tasks. The embedding model uses a separate trainable classification head (classifier) to predict the sentiment of movie reviews. ",
        "page_idx": 345
    },
    {
        "type": "text",
        "text": "In this section, we will take a similar approach but allow both the model and the classification head to be updated during training. As shown in Figure 11-2, instead of using an embedding model, we will fine-tune a pretrained BERT model to create a task-specific model similar to the one we used in Chapter 2. Compared to the embedding model approach, we will fine-tune both the representation model and the classification head as a single architecture. ",
        "page_idx": 345
    },
    {
        "type": "image",
        "img_path": "images/da47e0c7bdceb4968afc62f38ff2c4974e427753b17e2e49f17f0a32ab903e90.jpg",
        "image_caption": [
            "Figure 11-2. Compared to the “frozen” architecture, we instead train both the pretrained BERT model and the classification head. A backward pass will start at the classification head and go through BERT. "
        ],
        "image_footnote": [],
        "page_idx": 345
    },
    {
        "type": "text",
        "text": "To do so, instead of freezing the model, we allow it to be trainable and update its parameters during training. As illustrated in Figure 11-3, we will use a pretrained BERT model and add a neural network as a classification head, both of which will be fine-tuned for classification. ",
        "page_idx": 346
    },
    {
        "type": "image",
        "img_path": "images/44737b1db3c6fc0c15654da8c2d6357b8a0de9d2c3e2439ba7d5879735b938f1.jpg",
        "image_caption": [
            "Figure 11-3. The architecture of a task-specific model. It contains a pretrained represen‐ tation model (e.g., BERT) with an additional classification head for the specific task. "
        ],
        "image_footnote": [],
        "page_idx": 346
    },
    {
        "type": "text",
        "text": "In practice, this means that the pretrained BERT model and the classification head are updated jointly. Instead of independent processes, they learn from one another and allow for more accurate representations. ",
        "page_idx": 346
    },
    {
        "type": "text",
        "text": "Fine-Tuning a Pretrained BERT Model ",
        "text_level": 1,
        "page_idx": 346
    },
    {
        "type": "text",
        "text": "We will be using the same dataset we used in Chapter 4 to fine-tune our model, namely the Rotten Tomatoes dataset, which contains 5,331 positive and 5,331 nega‐ tive movie reviews from Rotten Tomatoes: ",
        "page_idx": 346
    },
    {
        "type": "text",
        "text": "from datasets import load_dataset   \n# Prepare data and splits   \ntomatoes $=$ load_dataset(\"rotten_tomatoes\")   \ntrain_data, test_data $=$ tomatoes[\"train\"], tomatoes[\"test\"] ",
        "page_idx": 346
    },
    {
        "type": "text",
        "text": "The first step in our classification task is to select the underlying model we want to use. We use \"bert-base-cased\", which was pretrained on the English Wikipedia as well as a large dataset consisting of unpublished books.1 ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "We define the number of labels that we want to predict beforehand. This is necessary to create the feedforward neural network that is applied on top of our pretrained model: ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "from transformers import AutoTokenizer, AutoModelForSequenceClassification   \n# Load model and tokenizer   \nmodel_id $=$ \"bert-base-cased\"   \nmodel $=$ AutoModelForSequenceClassification.from_pretrained( model_id, num_labels $\\ : = \\ 1$   \n)   \ntokenizer $=$ AutoTokenizer.from_pretrained(model_id) ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "Next, we will tokenize our data: ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "from transformers import DataCollatorWithPadding   \n# Pad to the longest sequence in the batch   \ndata_collator $=$ DataCollatorWithPadding(tokenizer $=$ tokenizer)   \ndef preprocess_function(examples): \"\"\"Tokenize input data\"\"\" return tokenizer(examples[\"text\"], truncation=True)   \n# Tokenize train/test data   \ntokenized_train $=$ train_data.map(preprocess_function, batched=True)   \ntokenized_test $=$ test_data.map(preprocess_function, batched=True) ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "Before creating the Trainer, we will want to prepare a special DataCollator. A DataCollator is a class that helps us build batches of data but also allows us to apply data augmentation. ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "During this process of tokenization, and as shown in Chapter 9, we will add padding to the input text to create equally sized representations. We use DataCollatorWith Padding for that. ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "Of course, an example would not be complete without defining some metrics: ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "import numpy as np   \nfrom datasets import load_metric   \ndef compute_metrics(eval_pred): \"\"\"Calculate F1 score\"\"\" logits, labels $=$ eval_pred   \nload_f1 $=$ load_metric(\"f1\")   \n$\\textsf { f } 1 \\ =$ load_f1.compute(predictions $=$ predictions, references $=$ labels)[\"f1\"]   \nreturn {\"f1\": f1} ",
        "page_idx": 347
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "With compute_metrics we can define any number of metrics that we are interested in and that can be printed out or logged during training. This is especially helpful during training as it allows for detecting overfitting behavior. ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "Next, we instantiate our Trainer: ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "from transformers import TrainingArguments, Trainer   \n# Training arguments for parameter tuning   \ntraining_args $=$ TrainingArguments( \"model\", learning_rate $\\begin{array} { r l } { \\mathbf { \\Psi } : } & { { } \\mathbf { \\Psi } : \\mathbf { \\Psi } } \\end{array}$ , per_device_train_batch_size $= 1 6$ , per_device_eval_batch_size $= 1 6$ , num_train_epochs $^ { = 1 }$ , weight_decay ${ \\tt = } \\Theta$ .01, save_strategy=\"epoch\", report_to=\"none\"   \n)   \n# Trainer which executes the training process   \ntrainer $=$ Trainer( model=model, args=training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer $\\mathbf { \\tilde { \\mathbf { \\tilde { \\mathbf { \\tilde { \\mathbf { \\tilde { \\tilde { \\tilde { \\mathbf { \\tilde } } } } } } } } } } }$ tokenizer, data_collator $=$ data_collator, compute_metrics $=$ compute_metrics,   \n) ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "The TrainingArguments class defines hyperparameters we want to tune, such as the learning rate and how many epochs (rounds) we want to train. The Trainer is used to execute the training process. ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "Finally, we can train our model and evaluate it: ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "trainer.evaluate() ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "{'eval_loss': 0.3663691282272339, 'eval_f1': 0.8492366412213741, 'eval_runtime': 4.5792, 'eval_samples_per_second': 232.791, 'eval_steps_per_second': 14.631, 'epoch': 1.0} ",
        "page_idx": 348
    },
    {
        "type": "text",
        "text": "We get an F1 score of 0.85, which is quite a bit higher than the task-specific model we used in Chapter 4, which resulted in an F1 score of 0.80. It shows that fine-tuning a model yourself can be more advantageous than using a pretrained model. It only costs us a couple of minutes to train. ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "Freezing Layers ",
        "text_level": 1,
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "To further showcase the importance of training the entire network, the next example will demonstrate how you can use Hugging Face Transformers to freeze certain layers of your network. ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "We will freeze the main BERT model and allow only updates to pass through the classification head. This will be a great comparison as we will keep everything the same, except for freezing specific layers. ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "To start, let’s reinitialize our model so we can start from scratch: ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "# Load model and tokenizer   \nmodel $=$ AutoModelForSequenceClassification.from_pretrained( model_id, num_labels $: = 2$   \n)   \ntokenizer $=$ AutoTokenizer.from_pretrained(model_id) ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "Our pretrained BERT model contains a lot of layers that we can potentially freeze. Inspecting these layers gives insight into the structure of the network and what we might want to freeze: ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "# Print layer names   \nfor name, param in model.named_parameters(): print(name)   \nbert.embeddings.word_embeddings.weight   \nbert.embeddings.position_embeddings.weight   \nbert.embeddings.token_type_embeddings.weight   \nbert.embeddings.LayerNorm.weight   \nbert.embeddings.LayerNorm.bias   \nbert.encoder.layer.0.attention.self.query.weight   \nbert.encoder.layer.0.attention.self.query.bias   \nbert.encoder.layer.11.output.LayerNorm.weight   \nbert.encoder.layer.11.output.LayerNorm.bias   \nbert.pooler.dense.weight   \nbert.pooler.dense.bias   \nclassifier.weight   \nclassifier.bias ",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 349
    },
    {
        "type": "text",
        "text": "There are 12 (0–11) encoder blocks consisting of attention heads, dense networks, and layer normalization. We further illustrate this architecture in Figure 11-4 to demonstrate everything that could be potentially frozen. On top of that, we have our classification head. ",
        "page_idx": 350
    },
    {
        "type": "image",
        "img_path": "images/d97e9205a204b23d81aaebb3cdd276f75bb1b1f755e37b8df0b3b3a59236f663.jpg",
        "image_caption": [
            "Figure 11-4. The basic architecture of BERT with the additional classification head. "
        ],
        "image_footnote": [],
        "page_idx": 350
    },
    {
        "type": "text",
        "text": "We could choose to only freeze certain layers to speed up computing but still allow the main model to learn from the classification task. Generally, we want frozen layers to be followed by trainable layers. ",
        "page_idx": 350
    },
    {
        "type": "text",
        "text": "We are going to freeze everything except for the classification head as we did in Chapter 2: ",
        "page_idx": 350
    },
    {
        "type": "text",
        "text": "for name, param in model.named_parameters(): # Trainable classification head if name.startswith(\"classifier\"): param.requires_grad $=$ True # Freeze everything else else: param.requires_grad $=$ False ",
        "page_idx": 350
    },
    {
        "type": "text",
        "text": "As shown in Figure 11-5, we have frozen everything except for the feedforward neural network, which is our classification head. ",
        "page_idx": 350
    },
    {
        "type": "image",
        "img_path": "images/e94c0e3bde96225a4ef1a466fa2279d43d1001753835c76a294357a9dde5b589.jpg",
        "image_caption": [
            "Figure 11-5. We fully freeze all encoder blocks and embedding layers such that the BERT model does not learn new representations during fine-tuning. "
        ],
        "image_footnote": [],
        "page_idx": 351
    },
    {
        "type": "text",
        "text": "Now that we have successfully frozen everything but the classification head, we can move on to train our model: ",
        "page_idx": 351
    },
    {
        "type": "text",
        "text": "from transformers import TrainingArguments, Trainer   \n# Trainer which executes the training process   \ntrainer $=$ Trainer( model=model, args $\\mathbf { \\equiv }$ training_args, train_dataset=tokenized_train, eval_dataset=tokenized_test, tokenizer=tokenizer, data_collator $\\mathbf { \\equiv } =$ data_collator, compute_metrics $=$ compute_metrics,   \n)   \ntrainer.train() ",
        "page_idx": 351
    },
    {
        "type": "text",
        "text": "You might notice that training has become much faster. That is because we are only training the classification head, which provides us with a significant speedup compared to fine-tuning the entire model: ",
        "page_idx": 351
    },
    {
        "type": "text",
        "text": "trainer.evaluate() ",
        "page_idx": 351
    },
    {
        "type": "text",
        "text": "{'eval_loss': 0.6821751594543457, 'eval_f1': 0.6331058020477816, 'eval_runtime': 4.0175, 'eval_samples_per_second': 265.337, 'eval_steps_per_second': 16.677, 'epoch': 1.0} ",
        "page_idx": 351
    },
    {
        "type": "text",
        "text": "When we evaluate the model, we only get an F1 score of 0.63, which is quite a bit lower compared to our original 0.85 score. Instead of freezing nearly all layers, let’s freeze everything up until encoder block 10 as illustrated in Figure 11-6, and see how it affects performance. A major benefit is that this reduces computation but still allows updates to flow through part of the pretrained model: ",
        "page_idx": 352
    },
    {
        "type": "image",
        "img_path": "images/40f99b2f062eac6a13fefd6288bcb4c51676eeaf354aa949b8e961216ae48e72.jpg",
        "image_caption": [
            "Figure 11-6. We freeze the first 10 encoder blocks of our BERT model. Everything else is trainable and will be fine-tuned. "
        ],
        "image_footnote": [],
        "page_idx": 352
    },
    {
        "type": "text",
        "text": "# Load model   \nmodel_id $=$ \"bert-base-cased\"   \nmodel $=$ AutoModelForSequenceClassification.from_pretrained( model_id, num_labels $\\scriptstyle : = \\ .$   \n)   \ntokenizer $=$ AutoTokenizer.from_pretrained(model_id)   \n# Encoder block 11 starts at index 165 and   \n# we freeze everything before that block   \nfor index, (name, param) in enumerate(model.named_parameters()): if index $<$ 165: param.requires_grad $=$ False   \n# Trainer which executes the training process   \ntrainer $=$ Trainer( model=model, args $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ training_args, train_dataset $: =$ tokenized_train, eval_dataset=tokenized_test, ",
        "page_idx": 352
    },
    {
        "type": "text",
        "text": "tokenizer $\\mathbf { \\tilde { \\mathbf { \\tilde { \\mathbf { \\tilde { \\mathbf { \\tilde { \\tilde { \\tilde { \\mathbf { \\tilde } } } } } } } } } } }$ tokenizer, data_collator $=$ data_collator, compute_metrics $=$ compute_metrics, ) trainer.train() ",
        "page_idx": 353
    },
    {
        "type": "text",
        "text": "After training, we evaluate the results: ",
        "page_idx": 353
    },
    {
        "type": "text",
        "text": "trainer.evaluate()   \n{'eval_loss': 0.40812647342681885, 'eval_f1': 0.8, 'eval_runtime': 3.7125, 'eval_samples_per_second': 287.137, 'eval_steps_per_second': 18.047, 'epoch': 1.0} ",
        "page_idx": 353
    },
    {
        "type": "text",
        "text": "We got an F1 score of 0.8, which is much higher than our previous score of 0.63 when freezing all layers. It demonstrates that although we generally want to train as many layers as possible, you can get away with training less if you do not have the necessary computing power. ",
        "page_idx": 353
    },
    {
        "type": "text",
        "text": "To further illustrate this effect, we tested the effect of iteratively freezing encoder blocks and fine-tuning them as we did thus far. As shown in Figure 11-7, training only the first five encoder blocks (red vertical line) is enough to almost reach the performance of training all encoder blocks. ",
        "page_idx": 353
    },
    {
        "type": "image",
        "img_path": "images/2d2a2f55867a8fe0f84fa5397a091f7c65a03726ad22166d15bf3026005800e5.jpg",
        "image_caption": [
            "Figure 11-7. The effect of freezing certain encoder blocks on the performance of the model. Training more blocks leads to improved performance but stabilizes early on. "
        ],
        "image_footnote": [],
        "page_idx": 353
    },
    {
        "type": "text",
        "text": "When you are training for multiple epochs, the difference (in training time and resources) between freezing and not freezing often becomes larger. It is therefore advised to play around with a balance that works for you. ",
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "Few-Shot Classification ",
        "text_level": 1,
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "Few-shot classification is a technique within supervised classification where you have a classifier learn target labels based on only a few labeled examples. This technique is great when you have a classification task but do not have many labeled data points readily available. In other words, this method allows you to label a few high-quality data points per class on which to train the model. This idea of using a few labeled data points for training your model is shown in Figure 11-8. ",
        "page_idx": 354
    },
    {
        "type": "image",
        "img_path": "images/d1505ea7432063fbdf4c5a499d2e2da7e084c14d464b7d2f4ee54ac0490bb20e.jpg",
        "image_caption": [
            "Figure 11-8. In few-shot classification, we only use a few labeled data points to learn from. "
        ],
        "image_footnote": [],
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "SetFit: Efficient Fine-Tuning with Few Training Examples ",
        "text_level": 1,
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "To perform few-shot text classification, we use an efficient framework called SetFit.2 It is built on top of the architecture of sentence-transformers to generate high-quality textual representations that are updated during training. Only a few labeled examples are needed for this framework to be competitive with fine-tuning a BERT-like model on a large, labeled dataset as we explored in the previous example. ",
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "The underlying algorithm of SetFit consists of three steps: ",
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "1. Sampling training data ",
        "text_level": 1,
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "Based on in-class and out-class selection of labeled data it generates positive (similar) and negative (dissimilar) pairs of sentences ",
        "page_idx": 354
    },
    {
        "type": "text",
        "text": "2. Fine-tuning embeddings ",
        "text_level": 1,
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "Fine-tuning a pretrained embedding model based on the previously generated training data ",
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "3. Training a classifier Create a classification head on top of the embedding model and train it using the previously generated training data ",
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "Before fine-tuning an embedding model, we need to generate training data. The model assumes the training data to be samples of positive (similar) and negative (dissimilar) pairs of sentences. However, when we are dealing with a classification task, our input data is generally not labeled as such. ",
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "Say, for example, we have the training dataset in Figure 11-9 that classifies text into two categories: text about programming languages, and text about pets. ",
        "page_idx": 355
    },
    {
        "type": "table",
        "img_path": "images/1b05778f1e568cfa37ebf6cd49eae366ae89ac1e1e67eb1a3b60d4f60ce93b16.jpg",
        "table_caption": [
            "Figure 11-9. Data in two classes: text about programming languages and text about pets. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Text</td><td>Class</td></tr><tr><td>I write my codein Python</td><td>Programming languages</td></tr><tr><td>I should practice SQL</td><td>Programming languages</td></tr><tr><td> My dog is a labrador</td><td>Pets</td></tr><tr><td>Ihave a Siamese cat</td><td>Pets</td></tr></table>",
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "In step 1, SetFit handles this problem by generating the necessary data based on in-class and out-class selection as we illustrate in Figure 11-10. For example, when we have 16 sentences about sports, we can create $1 6 ^ { * } \\left( 1 6 - 1 \\right) / 2 = 1 2 0$ pairs that we label as positive pairs. We can use this process to generate negative pairs by collecting pairs from different classes. ",
        "page_idx": 355
    },
    {
        "type": "table",
        "img_path": "images/0166e074985f13aeda94f6f3ae5501990988671157c27d0a59383066eff0dd37.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td>Text1</td><td>Text 2</td><td>Pair type</td></tr><tr><td>I write my code in Python</td><td> I should practice SQL</td><td>Positive</td></tr><tr><td> My dog is a labrador</td><td>I have a Siamese cat</td><td>Positive</td></tr><tr><td>I write my codein Python</td><td> My dog is a labrador</td><td>Negative</td></tr><tr><td> Ihave a Siamese cat</td><td>I should practice SQL</td><td>Negative</td></tr></table>",
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "Figure 11-10. Step 1: sampling training data. We assume sentences within a class are similar and create positive pairs while sentences in different classes become negative pairs. ",
        "page_idx": 355
    },
    {
        "type": "text",
        "text": "In step 2, we can use the generated sentence pairs to fine-tune the embedding model. This leverages a method called contrastive learning to fine-tune a pretrained BERT model. As we reviewed in Chapter 10, contrastive learning allows accurate sentence embeddings to be learned from pairs of similar (positive) and dissimilar (negative) sentences. ",
        "page_idx": 356
    },
    {
        "type": "text",
        "text": "Since we generated these pairs in the previous step, we can use them to fine-tune a SentenceTransformers model. Although we have discussed contrastive learning before, we again illustrate the method in Figure 11-11 as a refresher. ",
        "page_idx": 356
    },
    {
        "type": "image",
        "img_path": "images/5b9c11e14f61a4a8ebeaaa573eeec97b7204fffe17b0f7f794bc7328dacb33bd.jpg",
        "image_caption": [
            "Figure 11-11. Step 2: Fine-tuning a SentenceTransformers model. Using contrastive learning, embeddings are learned from positive and negative sentence pairs. "
        ],
        "image_footnote": [],
        "page_idx": 356
    },
    {
        "type": "text",
        "text": "The goal of fine-tuning this embedding model is that it can create embeddings that are tuned to the classification task. The relevance of the classes, and their rela‐ tive meaning, are distilled into the embeddings through fine-tuning the embedding model. ",
        "page_idx": 356
    },
    {
        "type": "text",
        "text": "In step 3, we generate embeddings for all sentences and use those as the input of a classifier. We can use the fine-tuned SentenceTransformers model to convert our sentences into embeddings that we can use as features. The classifier learns from our fine-tuned embeddings to accurately predict unseen sentences. This last step is illustrated in Figure 11-12. ",
        "page_idx": 357
    },
    {
        "type": "image",
        "img_path": "images/2ce2f6d9bcd1d6a4c45e18660fc42ea76b85fd460594ca098586253bcd4ca4df.jpg",
        "image_caption": [
            "Figure 11-12. Step 3: Training a classifier. The classifier can be any scikit-learn model or a classification head. "
        ],
        "image_footnote": [],
        "page_idx": 357
    },
    {
        "type": "text",
        "text": "When we put all the steps together, we get an efficient and elegant pipeline for performing classification when you only have a few labels per class. It cleverly makes use of the idea that we have labeled data, although not in the way that we would like it. The three steps together are illustrated in Figure 11-13 to give a single overview of the entire procedure. ",
        "page_idx": 357
    },
    {
        "type": "text",
        "text": "First, sentence pairs are generated based on in-class and out-class selection. Second, the sentence pairs are used to fine-tune a pretrained SentenceTransformer model. Third, the sentences are embedded with the fine-tuned model on which a classifier is trained to predict the classes. ",
        "page_idx": 357
    },
    {
        "type": "image",
        "img_path": "images/107272a49ae6bde9fdc4dcab843e51b2d7f5874efd5b9f5a5c13e73139a72af4.jpg",
        "image_caption": [
            "Figure 11-13. The three main steps of SetFit. "
        ],
        "image_footnote": [],
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "Fine-Tuning for Few-Shot Classification ",
        "text_level": 1,
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "We previously trained on a dataset containing roughly 8,500 movie reviews. However, since this is a few-shot setting, we will only sample 16 examples per class. With two classes, we will only have 32 documents to train on compared to the 8,500 movie reviews we used before! ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "from setfit import sample_dataset ",
        "text_level": 1,
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "# We simulate a few-shot setting by sampling 16 examples per class sampled_train_data $=$ sample_dataset(tomatoes[\"train\"], num_samples ${ \\tt = } 1 6$ ) ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "After sampling the data, we choose a pretrained SentenceTransformer model to finetune. The official documentation contains an overview of pretrained SentenceTrans former models from which we are going to be using \"sentence-transformers/ all-mpnet-base-v2\". It is one of the best-performing models on the MTEB leader‐ board, which shows the performance of embedding models across a variety of tasks: ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "from setfit import SetFitModel ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "# Load a pretrained SentenceTransformer model model $=$ SetFitModel.from_pretrained(\"sentence-transformers/all-mpnet-base-v2\") ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "After loading in the pretrained SentenceTransformer model, we can start defining our SetFitTrainer. By default, a logistic regression model is chosen as the classifier to train. ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "Similar to what we did with Hugging Face Transformers, we can use the trainer to define and play around with relevant parameters. For example, we set the num_epochs to 3 so that contrastive learning will be performed for three epochs: ",
        "page_idx": 358
    },
    {
        "type": "text",
        "text": "from setfit import TrainingArguments as SetFitTrainingArguments from setfit import Trainer as SetFitTrainer ",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "# Define training arguments   \nargs $=$ SetFitTrainingArguments( num_epochs $^ { = 3 }$ , # The number of epochs to use for contrastive learning num_iterations $= 2 0$ # The number of text pairs to generate   \n)   \nargs.eval_strategy $=$ args.evaluation_strategy   \n# Create trainer   \ntrainer $=$ SetFitTrainer( model=model, args $=$ args, train_dataset $\\Bumpeq$ sampled_train_data, eval_dataset $=$ test_data, metric=\"f1\"   \n) ",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "We only need to call train to start the training loop. When we do, we should get the following output: ",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "# Training loop trainer.train() ",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "\\*\\*\\*\\*\\* Running training \\*\\*\\*\\*\\* Num unique pairs $=$ 1280 Batch size $\\ l = \\ 1 6$ Num epochs $= 3$ Total optimization steps $=$ 240 ",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "Notice that the output mentions that 1,280 sentence pairs were generated for finetuning the SentenceTransformer model. As a default, 20 sentence pair combinations are generated for each sample in our data, which would be $2 0 \\div 3 2 = 6 8 0$ samples. We will have to multiply this value by 2 for each positive and negative pair generated, $6 8 0 \\div 2 = 1 . 2 8 0$ sentence pairs. Generating 1,280 sentence pairs is quite impressive considering we only had 32 labeled sentences to start with! ",
        "page_idx": 359
    },
    {
        "type": "text",
        "text": "When we do not specifically define a classification head, by default a logistic regression is used. If we would like to specify a classifi‐ cation head ourselves, we can do so by specifying the following model in SetFitTrainer: ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "# Load a SetFit model from Hub   \nmodel $=$ SetFitModel.from_pretrained( \"sentence-transformers/all-mpnet-base-v2\", use_differentiable_head=True, head_params={\"out_features\": num_classes},   \n)   \n# Create trainer   \ntrainer $=$ SetFitTrainer( model $=$ model,   \n) ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "Here, num_classes refers to the number of classes that we want to predict. ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "Next, let’s evaluate the model to get a feeling of its performance: ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "# Evaluate the model on our test data trainer.evaluate() ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "With only 32 labeled documents, we get an F1 score of 0.85. Considering that the model was trained on a tiny subset of the original data, this is very impressive! Moreover, in Chapter 2, we got the same performance but instead trained a logistic regression model on the embeddings of the full data. Thus, this pipeline demonstrates the potential of taking the time to label just a few instances. ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "Not only can SetFit perform few-shot classification tasks, but it also has support for when you have no labels at all, also called zero-shot classification. SetFit generates synthetic examples from the label names to resemble the classification task and then trains a SetFit model on them. For example, if the target labels are “happy” and “sad,” then synthetic data could be “The example is happy” and “This example is sad.” ",
        "page_idx": 360
    },
    {
        "type": "text",
        "text": "Continued Pretraining with Masked Language Modeling ",
        "text_level": 1,
        "page_idx": 361
    },
    {
        "type": "text",
        "text": "In the examples thus far, we leveraged a pretrained model and fine-tuned it to perform classification. This process describes a two-step process: first pretraining a model (which was already done for us) and then fine-tuning it for a particular task. We illustrate this process in Figure 11-14. ",
        "page_idx": 361
    },
    {
        "type": "image",
        "img_path": "images/dbbb3266f1204e3fa8798ae05ee7fc916f364b8408af1c8d3f5f1c7daca70052.jpg",
        "image_caption": [
            "Figure 11-14. To fine-tune the model on a target task—for example, classification—we either start with pretraining a BERT model or use a pretrained one. "
        ],
        "image_footnote": [],
        "page_idx": 361
    },
    {
        "type": "text",
        "text": "This two-step approach is typically used throughout many applications. It has its limitations when faced with domain-specific data. The pretrained model is often trained on very general data, like Wikipedia pages, and might not be tuned to your domain-specific words. ",
        "page_idx": 361
    },
    {
        "type": "text",
        "text": "Instead of adopting this two-step approach, we can squeeze another step between them, namely continue pretraining an already pretrained BERT model. In other words, we can simply continue training the BERT model using masked language modeling (MLM) but instead use data from our domain. It is like going from a general BERT model to a BioBERT model specialized for the medical domain, to a fine-tuned BioBERT model to classify medication. ",
        "page_idx": 361
    },
    {
        "type": "text",
        "text": "This will update the subword representations to be more tuned toward words it would not have seen before. This process is illustrated in Figure 11-15 and dem‐ onstrates how this additional step updates a masked language modeling task. Con‐ tinuing pretraining on a pretrained BERT model has been shown to improve the performance of models in classification tasks and is a worthwhile addition to the fine-tuning pipeline.3 ",
        "page_idx": 362
    },
    {
        "type": "image",
        "img_path": "images/2d3a2abea1cb3de55e024ee237366ce841388c88589efdfe546f751e632d25f3.jpg",
        "image_caption": [
            "Figure 11-15. Instead of a two-step approach, we can add another step that continues to pretrain the pretrained model before fine-tuning it on the target task. Notice how the masks were filled with abstract concepts in 1 while they were filled with movie-specific concepts in 2. "
        ],
        "image_footnote": [],
        "page_idx": 362
    },
    {
        "type": "text",
        "text": "Instead of having to pretrain an entire model from scratch, we can simply continue pretraining before fine-tuning it for classification. This also helps the model to adapt to a certain domain or even the lingo of a specific organization. The genealogy of models a company might want to adopt is further illustrated in Figure 11-16. ",
        "page_idx": 362
    },
    {
        "type": "image",
        "img_path": "images/2943378f91e01a9c2f4d913dbed5d1cfc77ec4963ef28f87879963062fcf567a.jpg",
        "image_caption": [
            "Figure 11-16. The three-step approach illustrated for specific use cases. "
        ],
        "image_footnote": [],
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "In this example, we will demonstrate how to apply step 2 and continue pretraining an already pretrained BERT model. We use the same data that we started with, namely the Rotten Tomatoes reviews. ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "We start by loading the \"bert-base-cased\" model we have used thus far and prepare it for MLM: ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "from transformers import AutoTokenizer, AutoModelForMaskedLM # Load model for masked language modeling (MLM) model $=$ AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\") tokenizer $=$ AutoTokenizer.from_pretrained(\"bert-base-cased\") ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "We need to tokenize the raw sentences. We will also remove the labels since this is not a supervised task: ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "def preprocess_function(examples): return tokenizer(examples[\"text\"], truncation=True) ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "# Tokenize data tokenized_train $=$ train_data.map(preprocess_function, batched=True) tokenized_train $=$ tokenized_train.remove_columns(\"label\") tokenized_test $=$ test_data.map(preprocess_function, batched=True) tokenized_test $=$ tokenized_test.remove_columns(\"label\") ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "Previously, we used DataCollatorWithPadding, which dynamically pads the input it receives. ",
        "page_idx": 363
    },
    {
        "type": "text",
        "text": "Instead, we will have a DataCollator that will perform the masking of tokens for us. There are two methods that are generally used for this: token and whole-word masking. With token masking, we randomly mask $1 5 \\%$ of the tokens in a sentence. It might happen that part of a word will be masked. To enable masking of the entire word, we could apply whole-word masking, as illustrated in Figure 11-17. ",
        "page_idx": 363
    },
    {
        "type": "image",
        "img_path": "images/28ba4d16c1ecbd5ac7b2dff5098b7793e41f10f8174d532cf4ef0a373cbbf2e9.jpg",
        "image_caption": [
            "Figure 11-17. Different methods for randomly masking tokens. "
        ],
        "image_footnote": [],
        "page_idx": 364
    },
    {
        "type": "text",
        "text": "Generally, predicting whole words tends to be more complicated than tokens, which makes the model perform better as it needs to learn more accurate and precise repre‐ sentations during training. However, it tends to take a bit more time to converge. We will be going with token masking in this example using DataCollatorForLan guageModeling for faster convergence. However, we can use whole-word masking by replacing DataCollatorForLanguageModeling with DataCollatorForWholeWord Mask. Lastly, we set the probability that a token is masked in a given sentence to $1 5 \\%$ (mlm_probability): ",
        "page_idx": 364
    },
    {
        "type": "text",
        "text": "from transformers import DataCollatorForLanguageModeling ",
        "page_idx": 364
    },
    {
        "type": "text",
        "text": "# Masking Tokens   \ndata_collator $=$ DataCollatorForLanguageModeling( tokenizer $=$ tokenizer, mlm=True, mlm_probability=0.15   \n) ",
        "page_idx": 364
    },
    {
        "type": "text",
        "text": "Next, we will create the Trainer for running the MLM task and specify certain parameters: ",
        "page_idx": 364
    },
    {
        "type": "text",
        "text": "# Training arguments for parameter tuning   \ntraining_args $=$ TrainingArguments( \"model\", learning_rate $\\begin{array} { r l } { \\mathbf { \\Psi } : } & { { } \\mathbf { \\Psi } : \\mathbf { \\Psi } } \\end{array}$ , per_device_train_batch_size $= 1 6$ , per_device_eval_batch_size $= 1 6$ , num_train_epochs $= 1 0$ , weight_decay $\\mathord {  = } 0 \\mathrm { ~ , ~ } 0 1$ , save_strategy $^ { \\prime } =$ \"epoch\", report_to=\"none\"   \n)   \n# Initialize Trainer   \ntrainer $=$ Trainer( model=model, args $=$ training_args, train_dataset=tokenized_train, eval_dataset $: =$ tokenized_test, tokenizer $=$ tokenizer, data_collato $\\ ' =$ data_collator   \n) ",
        "page_idx": 364
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "Several parameters are worth noting. We train for 20 epochs and keep the task short. You can experiment with the learning rate and weight decay to ascertain whether they assist in fine-tuning the model. ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "Before we start our training loop we will first save our pretrained tokenizer. The tokenizer is not updated during training so there is no need to save it after training. We will, however, save our model after we continue pretraining: ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "# Save pre-trained tokenizer tokenizer.save_pretrained(\"mlm\") ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "# Train model trainer.train() ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "# Save updated model model.save_pretrained(\"mlm\") ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "This gives us an updated model in the mlm folder. To evaluate its performance we would normally fine-tune the model on a variety of tasks. For our purposes, however, we can run some masking tasks to see if it has learned from its continued training. ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "We will do so by loading in our pretrained model before we continue pretraining. Using the sentence \"What a horrible [MASK]!\" the model will predict which word would be in place of \"[MASK]\": ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "from transformers import pipeline # Load and create predictions mask_filler $=$ pipeline(\"fill-mask\", model $\\cdot ^ { = }$ \"bert-base-cased\") preds $=$ mask_filler(\"What a horrible [MASK]!\") ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "# Print results   \nfor pred in preds: print $f ^ { \\prime \\prime } > > >$ {pred[\"sequence\"]}\") ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "$> > >$ What a horrible idea! $> > >$ What a horrible dream! >>> What a horrible thing! >>> What a horrible day! >>> What a horrible thought! ",
        "page_idx": 365
    },
    {
        "type": "text",
        "text": "The output demonstrates concepts like “idea,” “dream,” and “day,” which definitely make sense. Next, let’s see what our updated model predicts: ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "# Load and create predictions mask_filler $=$ pipeline(\"fill-mask\", model $\\cdot ^ { = }$ \"mlm\") preds $=$ mask_filler(\"What a horrible [MASK]!\") ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "# Print results   \nfor pred in preds: print $f ^ { \\prime \\prime } > > >$ {pred[\"sequence\"]}\") ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "$> > >$ What a horrible movie! $> > >$ What a horrible film! $> > >$ What a horrible mess! $> > >$ What a horrible comedy! $> > >$ What a horrible story! ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "A horrible movie, film, mess, etc. clearly shows us that the model is more biased toward the data that we fed it compared to the pretrained model. ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "The next step would be to fine-tune this model on the classification task that we did at the beginning of this chapter. Simply load the model as follows and you are good to go: ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "from transformers import AutoModelForSequenceClassification ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "# Fine-tune for classification   \nmodel $=$ AutoModelForSequenceClassification.from_pretrained(\"mlm\", num_labels $^ { , = 2 }$ )   \ntokenizer $=$ AutoTokenizer.from_pretrained(\"mlm\") ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "Named-Entity Recognition ",
        "text_level": 1,
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "In this section, we will delve into the process of fine-tuning a pretrained BERT model specifically for NER (named-entity recognition). Instead of classifying entire documents, this procedure allows for the classification of individual tokens and/or words, including people and locations. This is especially helpful for de-identification and anonymization tasks when there is sensitive data. ",
        "page_idx": 366
    },
    {
        "type": "text",
        "text": "NER shares similarities with the classification example we explored at the beginning of this chapter. Nevertheless, a key distinction lies in the preprocessing and classifica‐ tion of data. Given that we are focusing on classifying individual words instead of entire documents, we must preprocess the data to consider this granular structure. Figure 11-18 provides a visual representation of this word-level approach. ",
        "page_idx": 366
    },
    {
        "type": "image",
        "img_path": "images/2798158f7240f6b7dc0cfc6aaad81685c9ceb015cd24aba1c139ef76aa72a4d3.jpg",
        "image_caption": [
            "Figure 11-18. Fine-tuning a BERT model for NER allows for the detection of named entities, such as people or locations. "
        ],
        "image_footnote": [],
        "page_idx": 367
    },
    {
        "type": "text",
        "text": "Fine-tuning the pretrained BERT model follows a similar architecture akin to what we observed with document classification. However, there is a fundamental shift in the classification approach. Rather than relying on the aggregation or pooling of token embeddings, the model now makes predictions for individual tokens in a sequence. It is crucial to emphasize that our word-level classification task does not entail classifying entire words, but rather the tokens that collectively constitute those words. Figure 11-19 provides a visual representation of this token-level classification. ",
        "page_idx": 367
    },
    {
        "type": "image",
        "img_path": "images/c660820391d613dc87625f2b7dba577a50fb7cc9db7bf1687e301043f69f8420.jpg",
        "image_caption": [
            "Figure 11-19. During the fine-tuning process of a BERT model, individual tokens are classified instead of words or entire documents. "
        ],
        "image_footnote": [],
        "page_idx": 367
    },
    {
        "type": "text",
        "text": "Preparing Data for Named-Entity Recognition ",
        "text_level": 1,
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "In this example, we will use the English version of the CoNLL-2003 dataset, which contains several different types of named entities (person, organization, location, miscellaneous, and no entity) and has roughly 14,000 training samples.4 ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "# The CoNLL-2003 dataset for NER dataset $=$ load_dataset(\"conll2003\", trust_remote_code=True) ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "While researching datasets to use for this example, there were a few more that we wanted to share. wnut_17 is a task that focuses on emerging and rare entities, those that are more difficult to spot. Furthermore, the tner/mit_movie_trivia and tner/mit_res taurant datasets are quite fun to use. tner/mit_movie_trivia is for detecting entities like actor, plot, and soundtrack whereas tner/ mit_restaurant aims to detect entities such as amenity, dish, and cuisine.5 ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "Let’s inspect the structure of the data with an example: ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "example $=$ dataset[\"train\"][848] example ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "{'id': '848',   \n'tokens': ['Dean', 'Palmer', 'hit', 'his', '30th', 'homer', 'for', 'the', 'Rangers', '.'],   \n'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],   \n'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],   \n'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]} ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "This dataset provides us with labels for each word given in a sentence. These labels can be found in the ner_tags key, which refers to the following possible entities: ",
        "page_idx": 368
    },
    {
        "type": "text",
        "text": "label2id $= \\ \\left\\{ \\begin{array} { r l } \\end{array} \\right.$ \"O\": 0, \"B-PER\": 1, \"I-PER\": 2, \"B-ORG\": 3, \"I-ORG\": 4, \"B-LOC\": 5, \"I-LOC\": 6, \"B-MISC\": 7, \"I-MISC\": 8   \n}   \nid2label $=$ {index: label for label, index in label2id.items()}   \nlabel2id ",
        "page_idx": 369
    },
    {
        "type": "text",
        "text": "{'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8} ",
        "page_idx": 369
    },
    {
        "type": "text",
        "text": "These entities correspond to specific categories: a person (PER), organization (ORG), location (LOC), miscellaneous entities (MISC), and no entity (O). Note that these entities are prefixed with either a B (beginning) or an I (inside). If two tokens that follow each other are part of the same phrase, then the start of that phrase is indicated with B, which is followed by an I to show that they belong to each other and are not independent entities. ",
        "page_idx": 369
    },
    {
        "type": "text",
        "text": "This process is further illustrated in Figure 11-20. In the figure, since “Dean” is the start of the phrase and “Palmer” is the end, we know that “Dean Palmer” is a person and that “Dean” and “Palmer” are not individual people. ",
        "page_idx": 369
    },
    {
        "type": "image",
        "img_path": "images/a59533f2d4efa37dd287c99d68567c00bd636a21c58caa14028836eb1e0d8774.jpg",
        "image_caption": [
            "Figure 11-20. By indicating the start and end of the phrase with the same entity, we can recognize entities of entire phrases. "
        ],
        "image_footnote": [],
        "page_idx": 369
    },
    {
        "type": "text",
        "text": "Our data is preprocessed and split up into words but not yet tokens. To do so, we will tokenize it further with the tokenizer of the pretrained model we used throughout this chapter, namely bert-base-cased: ",
        "page_idx": 369
    },
    {
        "type": "text",
        "text": "from transformers import AutoModelForTokenClassification # Load tokenizer tokenizer $=$ AutoTokenizer.from_pretrained(\"bert-base-cased\") ",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "# Load model   \nmodel $=$ AutoModelForTokenClassification.from_pretrained( \"bert-base-cased\", num_labels=len(id2label), id2label $\\ l =$ id2label, label2id=label2id   \n) ",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "Let’s explore how the tokenizer would process our example: ",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "# Split individual tokens into sub-tokens   \ntoken_ids $=$ tokenizer(example[\"tokens\"], is_split_into_words=True)[\"input_ids\"]   \nsub_tokens $=$ tokenizer.convert_ids_to_tokens(token_ids)   \nsub_tokens ['[CLS]',   \n'Dean',   \n'Palmer',   \n'hit',   \n'his',   \n'30th',   \n'home',   \n'##r',   \n'for',   \n'the',   \n'Rangers', ，，   \n.',   \n'[SEP]'] ",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "The tokenizer added the [CLS] and [SEP] tokens as we learned in Chapters 2 and 3. Note that the word 'homer' was further split up into the tokens 'home' and '##r'. This creates a bit of a problem for us since we have labeled data at the word level but not at the token level. This can be resolved by aligning the labels with their subtoken counterparts during tokenization. ",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "Let’s consider the word 'Maarten', which has the label B-PER to signal that this is a person. If we pass that word through the tokenizer, it splits the word up into the tokens 'Ma', '##arte', and '##n'. We cannot use the B-PER entity for all tokens as that would signal that the three tokens are all independent people. Whenever an entity is split into tokens, the first token should have B (for beginning) and the following should be I (for inner). ",
        "page_idx": 370
    },
    {
        "type": "text",
        "text": "Therefore, 'Ma' will get the B-PER to signal the start of a phrase, and '##arte', and '##n' will get the I-PER to signal they belong to a phrase. This alignment process is illustrated in Figure 11-21. ",
        "page_idx": 370
    },
    {
        "type": "image",
        "img_path": "images/871b38f046caebac8d7401cacbe58b3225356792395bbe291a001cb2d6df62ad.jpg",
        "image_caption": [
            "Figure 11-21. The alignment process of labeling tokenized input. "
        ],
        "image_footnote": [],
        "page_idx": 371
    },
    {
        "type": "text",
        "text": "We create a function, align_labels, that will tokenize the input and align these tokens with their updated labels during tokenization: ",
        "page_idx": 371
    },
    {
        "type": "text",
        "text": "def align_labels(examples): token_ids $=$ tokenizer( examples[\"tokens\"], truncation=True, is_split_into_words=True ) labels $=$ examples[\"ner_tags\"] updated_labels $= \\ [ ]$ for index, label in enumerate(labels): # Map tokens to their respective word word_ids $=$ token_ids.word_ids(batch_index $\\ l =$ index) previous_word_idx $=$ None label_ids $=$ [] for word_idx in word_ids: # The start of a new word if word_idx $\\downarrow =$ previous_word_idx: previous_word_idx $=$ word_idx updated_label $= ~ - 1 0 0$ if word_idx is None else label[word_idx] label_ids.append(updated_label) # Special token is -100 elif word_idx is None: label_ids.append(-100) # If the label is B-XXX we change it to I-XXX else: updated_label $=$ label[word_idx] if updated_label $\\% 2 \\ = \\ 1$ : ",
        "page_idx": 371
    },
    {
        "type": "text",
        "text": "updated_labels.append(label_ids) ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "token_ids[\"labels\"] $=$ updated_labels return token_ids ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "tokenized $=$ dataset.map(align_labels, batched=True) ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "Looking at our example, note that additional labels (-100) were added for the [CLS] and [SEP] tokens: ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "# Difference between original and updated labels print(f\"Original: {example[\"ner_tags\"]}\") print(f\"Updated: {tokenized[\"train\"][848][\"labels\"]}\") ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "Original: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0] Updated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100] ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "Now that we have tokenized and aligned the labels, we can start thinking about defining our evaluation metrics. This is also different from what we have seen before. Instead of a single prediction per document, we now have multiple predictions per document, namely per token. ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "We will make use of the evaluate package by Hugging Face to create a compute_met rics function that allows us to evaluate performance on a token level: ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "import evaluate ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "# Load sequential evaluation   \nseqeval $=$ evaluate.load(\"seqeval\")   \ndef compute_metrics(eval_pred): # Create predictions logits, labels $=$ eval_pred predictions $=$ np.argmax(logits, axis $^ { = 2 }$ ) true_predictions $=$ [] true_labels $=$ [] # Document-level iteration for prediction, label in zip(predictions, labels): # Token-level iteration for token_prediction, token_label in zip(prediction, label): # We ignore special tokens if token_label $\\downarrow =$ -100: true_predictions.append([id2label[token_prediction]]) true_labels.append([id2label[token_label]]) ",
        "page_idx": 372
    },
    {
        "type": "text",
        "text": "results $=$ seqeval.compute( predictions=true_predictions, references $=$ true_labels ) return {\"f1\": results[\"overall_f1\"]} ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "Fine-Tuning for Named-Entity Recognition ",
        "text_level": 1,
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "We are nearly there. Instead of DataCollatorWithPadding, we need a collator that works with classification on a token level, namely DataCollatorForTokenClassifica tion: ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "from transformers import DataCollatorForTokenClassification # Token-classification DataCollator data_collator $=$ DataCollatorForTokenClassification(tokenizer=tokenizer) ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "Now that we have loaded our model, the rest of the steps are similar to previous training procedures in this chapter. We define a trainer with specific arguments that we can tune and create a Trainer: ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "# Training arguments for parameter tuning   \ntraining_args $=$ TrainingArguments( \"model\", learning_rate $^ { 1 \\pm }$ 2e-5, per_device_train_batch_size $= 1 6$ , per_device_eval_batch_size $= 1 6$ , num_train_epochs $: = 1$ , weight_decay $\\mathbf { \\Sigma } = \\mathbf { \\Sigma }$ .01, save_strategy=\"epoch\", report_to $1 =$ \"none\"   \n)   \n# Initialize Trainer   \ntrainer $=$ Trainer( model model, args $=$ training_args, train_dataset=tokenized[\"train\"], eval_dataset $: =$ tokenized[\"test\"], tokenizer=tokenizer, data_collato $\\ ' =$ data_collator, compute_metrics $=$ compute_metrics,   \n)   \ntrainer.train() ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "We then evaluate the model that we created: ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "# Evaluate the model on our test data trainer.evaluate() ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "Lastly, let’s save the model and use it in a pipeline for inference. This allows us to check certain data so we can manually inspect what happens during inference and if we are satisfied with the output: ",
        "page_idx": 373
    },
    {
        "type": "text",
        "text": "from transformers import pipeline   \n# Save our fine-tuned model   \ntrainer.save_model(\"ner_model\")   \n# Run inference on the fine-tuned model   \ntoken_classifier $=$ pipeline( \"token-classification\", model=\"ner_model\",   \n)   \ntoken_classifier(\"My name is Maarten.\")   \n[{'entity': 'B-PER', 'score': 0.99534035, 'index': 4, 'word': 'Ma', 'start': 11, 'end': 13},   \n{'entity': 'I-PER', 'score': 0.9928328, 'index': 5, 'word': '##arte', 'start': 13, 'end': 17},   \n{'entity': 'I-PER', 'score': 0.9954301, 'index': 6, 'word': '##n', 'start': 17, 'end': 18}] ",
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "In the sentence \"My name is Maarten\", the word \"Maarten\" and its subtokens were correctly identified as a person! ",
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "In this chapter, we explored several tasks for fine-tuning pretrained representation models on specific classification tasks. We started by demonstrating how to fine-tune a pretrained BERT model and extended the examples by freezing certain layers of its architectures. ",
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "We experimented with a few-shot classification technique called SetFit, which involves fine-tuning a pretrained embedding model together with a classification head using limited labeled data. Using only a few labeled data points, this model generated similar performance to the models we explored in earlier chapters. ",
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "Next, we delved into the concept of continued pretraining, where we used a pre‐ trained BERT model as a starting point and continued training it using different data. The underlying process, masked language modeling, is not only used for creating a representation model but can also be used to continue pretraining models. ",
        "page_idx": 374
    },
    {
        "type": "text",
        "text": "Finally, we looked at named-entity recognition, a task that involves identifying spe‐ cific entities such as people and places in unstructured text. Compared to previous examples, this classification was done on a word level rather than on a document level. ",
        "page_idx": 375
    },
    {
        "type": "text",
        "text": "In the next chapter, we continue with the field of fine-tuning language models but focus on generative models instead. Using a two-step process, we will explore how to fine-tune a generative model to properly follow instructions and then fine-tune it for human preference. ",
        "page_idx": 375
    },
    {
        "type": "text",
        "text": "Fine-Tuning Generation Models ",
        "text_level": 1,
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "In this chapter, we will take a pretrained text generation model and go over the pro‐ cess of fine-tuning it. This fine-tuning step is key in producing high-quality models and an important tool in our toolbox to adapt a model to a specific desired behavior. Fine-tuning allows us to adapt a model to a specific dataset or domain. ",
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "Throughout this chapter, we will guide you among the two most common methods for fine-tuning text generation models, supervised fine-tuning and preference tuning. We will explore the transformative potential of fine-tuning pretrained text generation models to make them more effective tools for your application. ",
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "The Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and Preference Tuning ",
        "text_level": 1,
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "There are three common steps that lead to creating a high-quality LLM: ",
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "1. Language modeling ",
        "text_level": 1,
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "The first step in creating a high-quality LLM is to pretrain it on one or more massive text datasets (Figure 12-1). During training, it attempts to predict the next token to accurately learn linguistic and semantic representations found in the text. As we saw before in Chapters 3 and 11, this is called language modeling and is a self-supervised method. ",
        "page_idx": 376
    },
    {
        "type": "text",
        "text": "This produces a base model, also commonly referred to as a pretrained or founda‐ tion model. Base models are a key artifact of the training process but are harder for the end user to deal with. This is why the next step is important. ",
        "page_idx": 376
    },
    {
        "type": "image",
        "img_path": "images/ea197ac3019334dc171b91ca06af26941c4bb908825ca413033ff1d91c4dbe98.jpg",
        "image_caption": [
            "Figure 12-1. During language modeling, the LLM aims to predict the next token based on an input. This is a process without labels. "
        ],
        "image_footnote": [],
        "page_idx": 377
    },
    {
        "type": "text",
        "text": "2. Fine-tuning 1 (supervised fine-tuning) ",
        "text_level": 1,
        "page_idx": 377
    },
    {
        "type": "text",
        "text": "LLMs are more useful if they respond well to instructions and try to follow them. When humans ask the model to write an article, they expect the model to generate the article and not list other instructions for example (which is what a base model might do). ",
        "page_idx": 377
    },
    {
        "type": "text",
        "text": "With supervised fine-tuning (SFT), we can adapt the base model to follow instructions. During this fine-tuning process, the parameters of the base model are updated to be more in line with our target task, like following instructions. Like a pretrained model, it is trained using next-token prediction but instead of only predicting the next token, it does so based on a user input (Figure 12-2). ",
        "page_idx": 377
    },
    {
        "type": "image",
        "img_path": "images/ab900aa9da61bed8dc156b9b4bc3f6ab0bd69651e23d5b0c4f9dfa8de3a85724.jpg",
        "image_caption": [
            "Figure 12-2. During supervised fine-tuning, the LLM aims to predict the next token based on an input that has additional labels. In a sense, the label is the user’s input. "
        ],
        "image_footnote": [],
        "page_idx": 377
    },
    {
        "type": "text",
        "text": "SFT can also be used for other tasks, like classification, but is often used to go from a base generative model to an instruction (or chat) generative model. ",
        "page_idx": 377
    },
    {
        "type": "text",
        "text": "3. Fine-tuning 2 (preference tuning) ",
        "text_level": 1,
        "page_idx": 377
    },
    {
        "type": "text",
        "text": "The final step further improves the quality of the model and makes it more aligned with the expected behavior of AI safety or human preferences. This is called preference tuning. Preference tuning is a form of fine-tuning and, as the name implies, aligns the output of the model to our preferences, which are defined by the data that we give it. Like SFT, it can improve upon the original model but has the added benefit of distilling preference of output in its training process. These three steps are illustrated in Figure 12-3 and demonstrate the process of starting from an untrained architecture and ending with a preferencetuned LLM. ",
        "page_idx": 377
    },
    {
        "type": "image",
        "img_path": "images/4375c8eda4edede4c6a51d6055b40daf501931d0681f4a9ebd7860985cdf1a0d.jpg",
        "image_caption": [
            "Figure 12-3. The three steps of creating a high-quality LLM. "
        ],
        "image_footnote": [],
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "In this chapter, we use a base model that was already trained on massive datasets and explore how we can fine-tune it using both fine-tuning strategies. For each method, we start with the theoretical underpinnings before using them in practice. ",
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "Supervised Fine-Tuning (SFT) ",
        "text_level": 1,
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "The purpose of pretraining a model on large datasets is that it is able to reproduce language and its meaning. During this process, the model learns to complete input phrases as shown in Figure 12-4. ",
        "page_idx": 378
    },
    {
        "type": "image",
        "img_path": "images/2b4e61435943e3c8d0af0c27f97e696ce0727b7c5caab55a6e38d295e6f0f105.jpg",
        "image_caption": [
            "Figure 12-4. A base or pretrained LLM was trained to predict the next word(s). "
        ],
        "image_footnote": [],
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "This example also illustrates that the model was not trained to follow instructions and instead will attempt to complete a question rather than answer it (Figure 12-5). ",
        "page_idx": 378
    },
    {
        "type": "image",
        "img_path": "images/f8bf31c641be62f9f24ba539908700f9912611460d7b6f2210e7f6592d275772.jpg",
        "image_caption": [
            "Figure 12-5. A base LLM will not follow instructions but instead attempts to predict each next word. It may even create new questions. "
        ],
        "image_footnote": [],
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "We can use this base model and adapt it to certain use cases, such as following instructions, by fine-tuning it. ",
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "Full Fine-Tuning ",
        "text_level": 1,
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "The most common fine-tuning process is full fine-tuning. Like pretraining an LLM, this process involves updating all parameters of a model to be in line with your target task. The main difference is that we now use a smaller but labeled dataset whereas the pretraining process was done on a large dataset without any labels (Figure 12-6). ",
        "page_idx": 378
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 379
    },
    {
        "type": "image",
        "img_path": "images/bd7ad0f59343345438f13e2ddcdc1bbac97193ac4c25a2f2842fc1d265638281.jpg",
        "image_caption": [
            "Figure 12-6. Compared to language modeling (pretraining), full fine-tuning uses a smaller but labeled dataset. "
        ],
        "image_footnote": [],
        "page_idx": 379
    },
    {
        "type": "text",
        "text": "You can use any labeled data for full fine-tuning, making it also a great technique for learning domain-specific representations. To make our LLM follow instructions, we will need question-response data. This data, as shown in Figure 12-7, is queries by the user with corresponding answers. ",
        "page_idx": 379
    },
    {
        "type": "image",
        "img_path": "images/3b8922650b7d09dd17ec684174b08f83a1a6a178d441d864d6f72607601ad977.jpg",
        "image_caption": [
            "Figure 12-7. Instruction data with instructions by a user and corresponding answers. The instructions can contain many different tasks. "
        ],
        "image_footnote": [],
        "page_idx": 379
    },
    {
        "type": "text",
        "text": "During full fine-tuning, the model takes the input (instructions) and applies nexttoken prediction on the output (response). In turn, instead of generating new ques‐ tions, it will follow instructions. ",
        "page_idx": 379
    },
    {
        "type": "text",
        "text": "Parameter-Efficient Fine-Tuning (PEFT) ",
        "text_level": 1,
        "page_idx": 380
    },
    {
        "type": "text",
        "text": "Updating all parameters of a model has a large potential of increasing its performance but comes with several disadvantages. It is costly to train, has slow training times, and requires significant storage. To resolve these issues, attention has been given to parameter-efficient fine-tuning (PEFT) alternatives that focus on fine-tuning pre‐ trained models at higher computational efficiency. ",
        "page_idx": 380
    },
    {
        "type": "text",
        "text": "Adapters ",
        "text_level": 1,
        "page_idx": 380
    },
    {
        "type": "text",
        "text": "Adapters are a core component of many PEFT-based techniques. The method pro‐ poses a set of additional modular components inside the Transformer that can be fine-tuned to improve the model’s performance on a specific task without having to fine-tune all the model weights. This saves a lot of time and compute. ",
        "page_idx": 380
    },
    {
        "type": "text",
        "text": "Adapters are described in the paper “Parameter-efficient transfer learning for NLP”, which showed that fine-tuning $3 . 6 \\%$ of the parameters of BERT for a task can yield comparable performance to fine-tuning all the model’s weights.1 On the GLUE benchmark, the authors show they reach within $0 . 4 \\%$ of the performance of full fine-tuning. In a single Transformer block, the paper’s proposed architecture places adapters after the attention layer and the feedforward neural network as illustrated in Figure 12-8. ",
        "page_idx": 380
    },
    {
        "type": "image",
        "img_path": "images/0f7de0dc9efbf06122ddf4ec2a4a6a140deea5eb952c394876d3c6c7c01f2b8e.jpg",
        "image_caption": [
            "Figure 12-8. Adapters add a small number of weights in certain places in the network that can be fine-tuned efficiently while leaving the majority of model weights frozen. "
        ],
        "image_footnote": [],
        "page_idx": 380
    },
    {
        "type": "text",
        "text": "It’s not enough to only alter one Transformer block, however, so these components are part of every block in the model, as Figure 12-9 shows. ",
        "page_idx": 381
    },
    {
        "type": "image",
        "img_path": "images/ed4def64e89b6130f89272f4e44b33a89f03cb8b7509a05ef1646272bd911702.jpg",
        "image_caption": [
            "Figure 12-9. Adapter components span the various Transformer blocks in the model. "
        ],
        "image_footnote": [],
        "page_idx": 381
    },
    {
        "type": "text",
        "text": "Seeing all the adapter’s components across the model like this enables us to see indi‐ vidual adapters as shown in Figure 12-10, which is a collection of these components spanning all the blocks of the model. Adapter 1 can be a specialist in, say, medical text classification, while Adapter 2 can specialize in named-entity recognition (NER). You can download specialized adapters from https://oreil.ly/XraXg. ",
        "page_idx": 381
    },
    {
        "type": "image",
        "img_path": "images/52683c8739c8721a0173a0b98e333ac4cadf070c001522bc61459b3635a3aafe.jpg",
        "image_caption": [
            "Figure 12-10. Adapters that specialize in specific tasks can be swapped into the same architecture (if they share the same original model architecture and weights). "
        ],
        "image_footnote": [],
        "page_idx": 382
    },
    {
        "type": "text",
        "text": "The paper “AdapterHub: A framework for adapting transformers” introduced the Adapter Hub as a central repository for sharing adapters.2 A lot of these earlier adapt‐ ers were more focused on BERT architectures. More recently, the concept has been applied to text generation Transformers in papers like “LLaMA-Adapter: Efficient fine-tuning of language models with zero-init attention”.3 ",
        "page_idx": 382
    },
    {
        "type": "text",
        "text": "Low-Rank Adaptation (LoRA) ",
        "text_level": 1,
        "page_idx": 382
    },
    {
        "type": "text",
        "text": "As an alternative to adapters, low-rank adaptation (LoRA) was introduced and is at the time of writing is a widely used and effective technique for PEFT. LoRA is a technique that (like adapters) only requires updating a small set of parameters. As ",
        "page_idx": 382
    },
    {
        "type": "image",
        "img_path": "images/0e8d6d754d32a32f16c7cee311436c96fd2676807caaf2ed2ceb29a2e6b24c9b.jpg",
        "image_caption": [
            "illustrated in Figure 12-11, it creates a small subset of the base model to fine-tune instead of adding layers to the model.4 ",
            "Figure 12-11. LoRA requires only fine-tuning a small set of parameters that can be kept separately from the base LLM. "
        ],
        "image_footnote": [],
        "page_idx": 383
    },
    {
        "type": "text",
        "text": "Like adapters, this subset allows for much quicker fine-tuning since we only need to update a small part of the base model. We create this subset of parameters by approximating large matrices that accompany the original LLM with smaller matri‐ ces. We can then use those smaller matrices as a replacement and fine-tune them instead of the original large matrices. Take for example the $1 0 \\times 1 0$ matrix we see in Figure 12-12. ",
        "page_idx": 383
    },
    {
        "type": "image",
        "img_path": "images/65120786edb2e73ff6f385c2f0b7fee2c2e8b46b060e3a2fc890c659392f7370.jpg",
        "image_caption": [
            "Figure 12-12. A major bottleneck of LLMs is their massive weight matrices. Only one of these may have 150 million parameters and each Transformer block would have its version of these. "
        ],
        "image_footnote": [],
        "page_idx": 383
    },
    {
        "type": "text",
        "text": "We can come up with two smaller matrices, which when multiplied, reconstruct a 10 $\\times 1 0$ matrix. This is a major efficiency win because instead of using 100 weights (10 times 10) we now only have 20 weights (10 plus 10), as we can see in Figure 12-13. ",
        "page_idx": 384
    },
    {
        "type": "image",
        "img_path": "images/e23527b1df751e2ae4477ad5a929b41f31234fe4749a542144b5a19405e144b2.jpg",
        "image_caption": [
            "Figure 12-13. Decomposing a large weight matrix into two smaller matrices leads to a compressed, low-rank version of the matrix that can be fine-tuned more efficiently. "
        ],
        "image_footnote": [],
        "page_idx": 384
    },
    {
        "type": "text",
        "text": "During training, we only need to update these smaller matrices instead of the full weight changes. The updated change matrices (smaller matrices) are then combined with the full (frozen) weights as illustrated in Figure 12-14. ",
        "page_idx": 384
    },
    {
        "type": "image",
        "img_path": "images/b9f6eb221d95441d417d6282b6f516bbdeddf3625f82551b7b7810f52911c190.jpg",
        "image_caption": [
            "Figure 12-14. Compared to full fine-tuning, LoRA aims to update a small representation of the original weights during training. "
        ],
        "image_footnote": [],
        "page_idx": 384
    },
    {
        "type": "text",
        "text": "But you might suspect that performance would drop. And you would be right. But where does this trade-off make sense? ",
        "page_idx": 385
    },
    {
        "type": "text",
        "text": "Papers like “Intrinsic dimensionality explains the effectiveness of language model fine-tuning” demonstrate that language models “have a very low intrinsic dimen‐ sion.”5 This means that we can find small ranks that approximate even the massive matrices of an LLM. A 175B model like GPT-3, for example, would have a weight matrix of $1 2 , 2 8 8 \\times 1 2 , 2 8 8$ inside each of its 96 Transformer blocks. That’s 150 million parameters. If we can successfully adapt that matrix into rank 8, that would only require two $1 2 , 2 8 8 \\times 2$ matrices resulting in 197K parameters per block. These are major savings in speed, storage, and compute as explained further in the previously referenced LoRA paper. ",
        "page_idx": 385
    },
    {
        "type": "text",
        "text": "This smaller representation is quite flexible in that you can select which parts of the base model to fine-tune. For instance, we can only fine-tune the Query and Value weight matrices in each Transformer layer. ",
        "page_idx": 385
    },
    {
        "type": "text",
        "text": "Compressing the model for (more) efficient training ",
        "text_level": 1,
        "page_idx": 385
    },
    {
        "type": "text",
        "text": "We can make LoRA even more efficient by reducing the memory requirements of the model’s original weights before projecting them into smaller matrices. The weights of an LLM are numeric values with a given precision, which can be expressed by the number of bits like float64 or float32. As illustrated in Figure 12-15, if we lower the amount of bits to represent a value, we get a less accurate result. However, if we lower the number of bits we also lower the memory requirements of that model. ",
        "page_idx": 385
    },
    {
        "type": "image",
        "img_path": "images/04416080792254b2817cb655f4acea0f11333c848c96454bdb91c18e31c7a55b.jpg",
        "image_caption": [
            "Figure 12-15. Attempting to represent pi with float 32-bit and float 16-bit representa‐ tions. Notice the lowered accuracy when we halve the number of bits. "
        ],
        "image_footnote": [],
        "page_idx": 385
    },
    {
        "type": "text",
        "text": "With quantization, we aim to lower the number of bits while still accurately repre‐ senting the original weight values. However, as shown in Figure 12-16, when directly mapping higher precision values to lower precision values, multiple higher precision values might end up being represented by the same lower precision values. ",
        "page_idx": 386
    },
    {
        "type": "image",
        "img_path": "images/ff963c798d5da3e701a25586858b5edd2e3011b8212e9762c735c98d64da79a0.jpg",
        "image_caption": [
            "Figure 12-16. Quantizing weights that are close to one another results in the same reconstructed weights thereby removing any differentiating factor. "
        ],
        "image_footnote": [],
        "page_idx": 386
    },
    {
        "type": "text",
        "text": "Instead, the authors of QLoRA, a quantized version of LoRA, found a way to go from a higher number of bits to a lower value and vice versa without differentiating too much from the original weights.6 ",
        "page_idx": 386
    },
    {
        "type": "text",
        "text": "They used blockwise quantization to map certain blocks of higher precision values to lower precision values. Instead of directly mapping higher precision to lower preci‐ sion values, additional blocks are created that allow for quantizing similar weights. As shown in Figure 12-17, this results in values that can be accurately represented with lower precision. ",
        "page_idx": 386
    },
    {
        "type": "image",
        "img_path": "images/57175204a571e4d1d68226d9aa885f5b1da6a031bc2a329a1abd03a0567f2da7.jpg",
        "image_caption": [
            "Figure 12-17. Blockwise quantization can accurately represent weights in lower precision through quantization blocks. "
        ],
        "image_footnote": [],
        "page_idx": 387
    },
    {
        "type": "text",
        "text": "A nice property of neural networks is that their values are generally normally dis‐ tributed between $^ { - 1 }$ and 1. This property allows us to bin the original weights to lower bits based on their relative density, as illustrated in Figure 12-18. The mapping between weights is more efficient as it takes into account the relative frequency of weights. This also reduces issues with outliers. ",
        "page_idx": 387
    },
    {
        "type": "image",
        "img_path": "images/798b4419912711ad9df21f4a60aeb02b42be14856ae86c64992a6eabaaef306a.jpg",
        "image_caption": [
            "Figure 12-18. Using distribution-aware blocks we can prevent values close to one another from being represented with the same quantized value. "
        ],
        "image_footnote": [],
        "page_idx": 387
    },
    {
        "type": "text",
        "text": "Combined with the blockwise quantization, this normalization procedure allows for accurate representation of high precision values by low precision values with only a small decrease in the performance of the LLM. As a result, we can go from a 16-bit float representation to a measly 4-bit normalized float representation. A 4-bit representation significantly reduces the memory requirements of the LLM during training. Note that the quantization of LLMs in general is also helpful for inference as quantized LLMs are smaller in size and therefore require less VRAM. ",
        "page_idx": 388
    },
    {
        "type": "text",
        "text": "There are more elegant methods to further optimize this like double quantization and paged optimizers, which you can read about more in the QLoRA paper discussed earlier. For a complete and highly visual guide to quantization, see this blog post. ",
        "page_idx": 388
    },
    {
        "type": "text",
        "text": "Instruction Tuning with QLoRA ",
        "text_level": 1,
        "page_idx": 388
    },
    {
        "type": "text",
        "text": "Now that we have explored how QLoRA works, let us put that knowledge into practice! In this section, we will fine-tune a completely open source and smaller version of Llama, TinyLlama, to follow instructions using the QLoRA procedure. Consider this model a base or pretrained model, one that was trained with language modeling but cannot yet follow instructions. ",
        "page_idx": 388
    },
    {
        "type": "text",
        "text": "Templating Instruction Data ",
        "text_level": 1,
        "page_idx": 388
    },
    {
        "type": "text",
        "text": "To have the LLM follow instructions, we will need to prepare instruction data that follows a chat template. This chat template, as illustrated in Figure 12-19, differenti‐ ates between what the LLM has generated and what the user has generated. ",
        "page_idx": 388
    },
    {
        "type": "image",
        "img_path": "images/4a9f71b279ca575e765813406af4aad6fd6199d6afdd729327497e6348776f13.jpg",
        "image_caption": [
            "Figure 12-19. The chat template that we use throughout this chapter. "
        ],
        "image_footnote": [],
        "page_idx": 388
    },
    {
        "type": "text",
        "text": "We chose this chat template to use throughout the examples since the chat version of TinyLlama uses the same format. The data that we are using is a small subset of the UltraChat dataset.7 This dataset is a filtered version of the original UltraChat dataset that contains almost $2 0 0 \\mathrm { k }$ conversations between a user and an LLM. ",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "We create a function, format_prompt, to make sure that the conversations follow this template: ",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "from transformers import AutoTokenizer   \nfrom datasets import load_dataset   \n# Load a tokenizer to use its chat template   \ntemplate_tokenizer $=$ AutoTokenizer.from_pretrained( \"TinyLlama/TinyLlama-1.1BChat-v1.0\"   \n)   \ndef format_prompt(example): \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\" # Format answers chat $=$ example[\"messages\"] prompt $=$ template_tokenizer.apply_chat_template(chat, tokenize=False) return {\"text\": prompt}   \n# Load and format the data using the template TinyLLama is using   \ndataset $=$ ( load_dataset(\"HuggingFaceH4/ultrachat_200k\", split $\\mathbf { \\varepsilon } =$ \"test_sft\") .shuffle(seed $= 4 2$ ) .select(range(3_000))   \n)   \ndataset $=$ dataset.map(format_prompt) ",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "We select a subset of 3,000 documents to reduce the training time, but you can increase this value to get more accurate results. ",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "Using the \"text\" column, we can explore these formatted prompts: ",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "# Example of formatted promptprint(dataset[\"text\"][2576])",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "<|user|>   \nGiven the text: Knock, knock. Who's there? Hike.   \nCan you continue the joke based on the given text material \"Knock, knock. Who's there? Hike\"?</s>   \n<|assistant|>   \nSure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>   \n<|user|>   \nCan you tell me another knock-knock joke based on the same text material   \n\"Knock, knock. Who's there? Hike\"?</s>   \n<|assistant|>   \nOf course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here   \nand let's go for a walk! $! < / \\mathsf { s } >$ ",
        "page_idx": 389
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 390
    },
    {
        "type": "text",
        "text": "Model Quantization ",
        "text_level": 1,
        "page_idx": 390
    },
    {
        "type": "text",
        "text": "Now that we have our data, we can start loading in our model. This is where we apply the Q in QLoRA, namely quantization. We use the bitsandbytes package to compress the pretrained model to a 4-bit representation. ",
        "page_idx": 390
    },
    {
        "type": "text",
        "text": "In BitsAndBytesConfig, you can define the quantization scheme. We follow the steps used in the original QLoRA paper and load the model in 4-bit (load_in_4bit) with a normalized float representation (bnb_4bit_quant_type) and double quantization (bnb_4bit_use_double_quant): ",
        "page_idx": 390
    },
    {
        "type": "text",
        "text": "import torch   \nfrom transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig   \nmodel_name $=$ \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"   \n# 4-bit quantization configuration - Q in QLoRA   \nbnb_config $=$ BitsAndBytesConfig( load_in_4bit=True, # Use 4-bit precision model loading bnb_4bit_quant_type $\\ast =$ \"nf4\", # Quantization type bnb_4bit_compute_dtype $\\Bumpeq$ \"float16\", # Compute dtype bnb_4bit_use_double_quant=True, # Apply nested quantization   \n)   \n# Load the model to train on the GPU   \nmodel $=$ AutoModelForCausalLM.from_pretrained( model_name, device_map=\"auto\", # Leave this out for regular SFT quantization_config=bnb_config,   \n)   \nmodel.config.use_cache $=$ False   \nmodel.config.pretraining_tp $\\ c = ~ 1$   \n# Load LLaMA tokenizer   \ntokenizer $=$ AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)   \ntokenizer.pad_token $=$ \"<PAD>\"   \ntokenizer.padding_side $=$ \"left\" ",
        "page_idx": 390
    },
    {
        "type": "text",
        "text": "This quantization procedure allows us to decrease the size of the original model while retaining most of the original weights’ precision. Loading the model now only uses ${ \\sim } 1$ GB VRAM compared to the ${ \\sim } 4$ GB of VRAM it would need without quantization. Note that during fine-tuning, more VRAM will be necessary so it does not cap out on the ${ \\sim } 1$ GB VRAM needed to load the model. ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "LoRA Configuration ",
        "text_level": 1,
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "Next, we will need to define our LoRA configuration using the peft library, which represents hyperparameters of the fine-tuning process: ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "# Prepare LoRA Configuration   \npeft_config $=$ LoraConfig( lora_alpha $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , # LoRA Scaling lora_dropout $= 0 . 1$ , # Dropout for LoRA Layers $\\Gamma { = } 6 4$ , # Rank bias $=$ \"none\", task_type $\\ast =$ \"CAUSAL_LM\", target_modules $=$ # Layers to target [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\",   \n\"down_proj\"]   \n) ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "# Prepare model for training model $=$ prepare_model_for_kbit_training(model) model $=$ get_peft_model(model, peft_config) ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "There are several parameters worth mentioning: ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "r ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "This is the rank of the compressed matrices (recall this from Figure 12-13) Increasing this value will also increase the sizes of compressed matrices leading to less compression and thereby improved representative power. Values typically range between 4 and 64. ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "lora_alpha ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "Controls the amount of change that is added to the original weights. In essence, it balances the knowledge of the original model with that of the new task. A rule of thumb is to choose a value twice the size of r. ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "target_modules ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "Controls which layers to target. The LoRA procedure can choose to ignore specific layers, like specific projection layers. This can speed up training but reduce performance and vice versa. ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "Playing around with the parameters is a worthwhile experiment to get an intuitive understanding of values that work and those that do not. You can find an amazing resource of additional tips on LoRA fine-tuning in the Ahead of AI newsletter by Sebastian Raschka. ",
        "page_idx": 391
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "This example demonstrates an efficient form of fine-tuning your model. If you want to perform full fine-tuning instead, you can remove the quantization_config parameter when loading the model and skip the creation of peft_config. By removing those, we would go from “Instruction tuning with QLoRA” to “full instruction tuning.” ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "Training Configuration ",
        "text_level": 1,
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "Lastly, we need to configure our training parameters as we did in Chapter 11: ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "from transformers import TrainingArguments output_dir $=$ \"./results\" ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "# Training arguments   \ntraining_arguments $=$ TrainingArguments( output_dir=output_dir, per_device_train_batch_size $^ { = 2 }$ , gradient_accumulation_steps $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ , optim=\"paged_adamw_32bit\", learning_rate=2e-4, lr_scheduler_type $^ { 1 \\pm }$ \"cosine\", num_train_epochs $^ { - 1 }$ , logging_step $ \\Longrightarrow 1 0$ , fp16=True, gradient_checkpointing=True   \n) ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "There are several parameters worth mentioning: ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "num_train_epochs The total number of training rounds. Higher values tend to degrade performance so we generally like to keep this low. ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "learning_rate ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "Determines the step size at each iteration of weight updates. The authors of QLoRA found that higher learning rates work better for larger models $\\mathrm { \\Omega } > 3 3 \\mathrm { B }$ parameters). ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "lr_scheduler_type ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "A cosine-based scheduler to adjust the learning rate dynamically. It will linearly increase the learning rate, starting from zero, until it reaches the set value. After that, the learning rate is decayed following the values of a cosine function. ",
        "page_idx": 392
    },
    {
        "type": "text",
        "text": "optim ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "The paged optimizers used in the original QLoRA paper. ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "Optimizing these parameters is a difficult task and there are no set guidelines for doing so. It requires experimentation to figure out what works best for specific datasets, model sizes, and target tasks. ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "Although this section describes instruction tuning, we could also use QLoRA to fine-tune an instruction model. For instance, we could fine-tune a chat model to generate specific SQL code or to create JSON output that adheres to a specific format. As long as you have the data available (with appropriate query-response items), QLoRA is a great technique for nudging an existing chat model to be more appropriate for your use case. ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "Training ",
        "text_level": 1,
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "Now that we have prepared all our models and parameters, we can start fine-tuning our model. We load in SFTTrainer and simply run trainer.train(): ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "from trl import SFTTrainer   \n# Set supervised fine-tuning parameters   \ntrainer $=$ SFTTrainer( model=model, train_dataset=dataset, dataset_text_field=\"text\", tokenizer=tokenizer, args $=$ training_arguments, max_seq_length $= 5 1 2$ , # Leave this out for regular SFT peft_config=peft_config,   \n) ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "# Train model trainer.train() ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "# Save QLoRA weights trainer.model.save_pretrained(\"TinyLlama-1.1B-qlora\") ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "During training the loss will be printed every 10 steps according to the log ging_steps parameter. If you are using the free GPU provided by Google Colab, which is the Tesla T4 at the time of writing, then training might take up to an hour. A good time to take a break! ",
        "page_idx": 393
    },
    {
        "type": "text",
        "text": "Merge Weights ",
        "text_level": 1,
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "After we have trained our QLoRA weights, we still need to combine them with the original weights to use them. We reload the model in 16 bits, instead of the quantized 4 bits, to merge the weights. Although the tokenizer was not updated during training, we save it to the same folder as the model for easier access: ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "from peft import AutoPeftModelForCausalLM   \nmodel $=$ AutoPeftModelForCausalLM.from_pretrained( \"TinyLlama-1.1B-qlora\", low_cpu_mem_usage=True, device_map=\"auto\",   \n)   \n# Merge LoRA and base model   \nmerged_model $=$ model.merge_and_unload() ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "After merging the adapter with the base model, we can use it with the prompt template that we defined earlier: ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "from transformers import pipeline   \n# Use our predefined prompt template   \nprompt $=$ \"\"\"<|user|>   \nTell me something about Large Language Models.</s>   \n<|assistant|>   \n# Run our instruction-tuned model   \npipe $=$ pipeline(task $: =$ \"text-generation\", model=merged_model, tokenizer=tokenizer)   \nprint(pipe(prompt)[0][\"generated_text\"]) ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "Large Language Models (LLMs) are artificial intelligence (AI) models that learn language and understand what it means to say things in a particular language. They are trained on huge amounts of text… ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "The aggregate output shows that the model now closely follows our instructions, which is not possible with the base model. ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "Evaluating Generative Models ",
        "text_level": 1,
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "Evaluating generative models poses a significant challenge. Generative models are used across many diverse use cases, making it a challenge to rely on a singular metric for judgment. Unlike more specialized models, a generative model’s ability to solve mathematical questions does not guarantee success in solving coding questions. ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "At the same time, evaluating these models is vital, particularly in production settings where consistency is important. Given their probabilistic nature, generative models do not necessarily generate consistent outputs; there is a need for robust evaluation. ",
        "page_idx": 394
    },
    {
        "type": "text",
        "text": "In this section, we will explore a few common evaluation methods, but we want to emphasize the current lack of golden standards. No one metric is perfect for all use cases. ",
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "Word-Level Metrics ",
        "text_level": 1,
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "One common metrics category for comparing generative models is word-level evalu‐ ation. These classic techniques compare a reference dataset with the generated tokens on a token(set) level. Common word-level metrics include perplexity,8 ROUGE,9 BLEU,10 and BERTScore.11 ",
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "Of note is perplexity, which measures how well a language model predicts a text. Given input text, the model predicts how likely the next token is. With perplexity, we assume a model performs better if it gives the next token a high probability. In other words, the models should not be “perplexed” when presented with a well-written document. ",
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "As illustrated in Figure 12-20, when presented with the input “When a measure becomes a,” the model is asked how probable the word “target” is as the next word. ",
        "page_idx": 395
    },
    {
        "type": "image",
        "img_path": "images/527ba3853577c3105cdfac863060514b22b7a02f35f2b71b176958a0d0c015fa.jpg",
        "image_caption": [
            "Figure 12-20. Next-word prediction is a central feature of many LLMs. "
        ],
        "image_footnote": [],
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "Although perplexity, and other word-level metrics, are useful metrics to understand the confidence of the model, they are not a perfect measure. They do not account for consistency, fluency, creativity, or even correctness of the generated text. ",
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "Benchmarks ",
        "text_level": 1,
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "A common method for evaluating generative models on language generation and understanding tasks is on well-known and public benchmarks, such as MMLU,12 GLUE,13 TruthfulQA,14 GSM8k,15 and HellaSwag.16 These benchmarks give us information about basic language understanding but also complex analytical answer‐ ing, like math problems. ",
        "page_idx": 395
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 396
    },
    {
        "type": "text",
        "text": "Aside from natural language tasks, some models specialize in other domains, like programming. These models tend to be evaluated on different benchmarks, such as HumanEval,17 which consists of challenging programming tasks for the model to solve. Table 12-1 gives an overview of common public benchmarks for generative models. ",
        "page_idx": 396
    },
    {
        "type": "table",
        "img_path": "images/7fd1db34e9bcc391d1971d826c12a96a375c949c95bd5365eced6ef808f2962a.jpg",
        "table_caption": [
            "Table 12-1. Common public benchmarks for generative models "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Benchmark Description</td><td></td><td>Resources</td></tr><tr><td>MMLU</td><td>The Massive Multitask Language Understanding (MMLU) benchmark tests the model on 57 different tas, including clasification,questionanswering,andsentiment analysis.</td><td>https://oreil.ly/ nrG_g</td></tr><tr><td>GLUE</td><td>TheGeneral Language Understanding Evaluation (GLUE)benchmark consists of language understanding tasks covering a wide degree of difficulty.</td><td>https://orel/ LV_fb</td></tr><tr><td></td><td>TruthfulQATruthfulQA measures the truthfulness of a model&#x27;s generated text.</td><td>https://oreil.ly/ i2Br</td></tr><tr><td>GSM8k</td><td>The GSM8k dataset contains grade-schol math word problems.It is linguisticallydiverse and created by human problem writers.</td><td>https://oreil.ly/ 0OBXY</td></tr><tr><td>HellaSswag</td><td>HellSwag is a challenge dataset for evaluating common-sense inference.It consists of multiple-choice questions that the model needs to answer.It can select one offour answer choices for each question.</td><td>https://oreill/ aDvBP</td></tr><tr><td>HumanEval</td><td>The HumanEval benchmark is used for evaluating generated code based on 164 programming problems.</td><td>https://orel/l duix</td></tr></table>",
        "page_idx": 396
    },
    {
        "type": "text",
        "text": "Benchmarks are a great way to get a basic understanding on how well a model performs on a wide variety of tasks. A downside to public benchmarks is that models can be overfitted to these benchmarks to generate the best responses. Moreover, these are still broad benchmarks and might not cover very specific use cases. Lastly, another downside is that some benchmarks require strong GPUs with a long running time (over hours) to compute, which makes iteration difficult. ",
        "page_idx": 396
    },
    {
        "type": "text",
        "text": "Leaderboards ",
        "text_level": 1,
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "With so many different benchmarks, it is hard to choose which benchmark best suits your model. Whenever a model is released, you will often see it evaluated on several benchmarks to showcase how it performs across the board. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "As such, leaderboards were developed containing multiple benchmarks. A common leaderboard is the Open LLM Leaderboard, which, at the time of writing, includes six benchmarks, including HellaSwag, MMLU, TruthfulQA, and GSM8k. Models that top the leaderboard, assuming they were not overfitted on the data, are generally regar‐ ded as the “best” model. However, since these leaderboards often contain publicly available benchmarks, there is a risk of overfitting on the leaderboard. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "Automated Evaluation ",
        "text_level": 1,
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "Part of evaluating a generative output is the quality of its text. For instance, even if two models were to give the same correct answer to a question, the way they derived that answer might be different. It is often not just about the final answer but also the construction of it. Similarly, although two summaries might be similar, one could be significantly shorter than another, which is often important for a good summary. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "To evaluate the quality of the generated text above the correctness of the final answer, LLM-as-a-judge was introduced.18 In essence, a separate LLM is asked to judge the quality of the LLM to be evaluated. An interesting variant of this method is pairwise comparison. Two different LLMs will generate an answer to a question and a third LLM will be the judge to declare which is better. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "As a result, this methodology allows for automated evaluation of open-ended ques‐ tions. A major advantage is that as LLMs improve, so do their capabilities to judge the quality of output. In other words, this evaluation methodology grows with the field. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "Human Evaluation ",
        "text_level": 1,
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "Although benchmarks are important, the gold standard of evaluation is gener‐ ally considered to be human evaluation. Even if an LLM scores well on broad benchmarks, it still might not score well on domain-specific tasks. Moreover, bench‐ marks do not fully capture human preference and all methods discussed before are merely proxies for that. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "A great example of a human-based evaluation technique is the Chatbot Arena.19 When you go to this leaderboard you are shown two (anonymous) LLMs you can interact with. Any question or prompt you ask will be sent to both models and you will receive their output. Then, you can decide which output you prefer. This process allows for the community to vote on which models they prefer without knowing which ones are presented. Only after you vote do you see which model generated which text. ",
        "page_idx": 397
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "At the time of writing, this method has generated over $8 0 0 { , } 0 0 0 { + }$ human votes that were used to compute a leaderboard. These votes are used to calculate the relative skill level of LLMs based on their win rates. For instance, if a low-ranked LLM beats a high-ranked LLM, its ranking changes significantly. In chess, this is referred to as the Elo rating system. ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "This methodology therefore uses crowdsourced votes, which helps us understand the quality of the LLM. However, it is still the aggregated opinion of a wide variety of users, which might not relate to your use case. ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "As a result, there is no one perfect method of evaluating LLMs. All mentioned methodologies and benchmarks provide an important, although limited evaluation perspective. Our advice is to evaluate your LLM based on the intended use case. For coding, HumanEval would be more logical than GSM8k. ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "But most importantly, we believe that you are the best evaluator. Human evaluation remains the gold standard because it is up to you to decide whether the LLM works for your intended use case. As with the examples in this chapter, we highly advise that you also try these models and perhaps develop some questions yourself. For example, the authors of this book are Arabic (Jay Alammar) and Dutch (Maarten Grootendorst), and we often ask questions in our native language when approached with new models. ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "One final note on this topic is a quote we hold dear: ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "When a measure becomes a target, it ceases to be a good measure. —Goodhart’s Law20 ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "In the context of LLMs, when using a specific benchmark, we tend to optimize for that benchmark regardless of the consequences. For instance, if we focus purely on optimizing for generating grammatically correct sentences, the model could learn to only output one sentence: “This is a sentence.” It is grammatically correct but tells you nothing about its language understanding capabilities. Thus, the model may excel at a specific benchmark but potentially at the expense of other useful capabilities. ",
        "page_idx": 398
    },
    {
        "type": "text",
        "text": "Preference-Tuning / Alignment / RLHF ",
        "text_level": 1,
        "page_idx": 399
    },
    {
        "type": "text",
        "text": "Although our model can now follow instructions, we can further improve its behav‐ ior by a final training phase that aligns it to how we expect it to behave in different scenarios. For instance, when asked “What is an LLM?” we might prefer an elaborate answer that describes the internals of an LLM compared to the answer “It is a large language model” without further explanations. How exactly do we align our (human) preference for one answer over the other with the output of an LLM? ",
        "page_idx": 399
    },
    {
        "type": "text",
        "text": "To start with, recall that an LLM takes a prompt and outputs a generation as illustra‐ ted in Figure 12-21. ",
        "page_idx": 399
    },
    {
        "type": "image",
        "img_path": "images/62a223f8c316e75c43506fa91976c192a2298ccb33f05a70ccbe8503ee58ad9a.jpg",
        "image_caption": [
            "Figure 12-21. An LLM takes an input prompt and outputs a generation. "
        ],
        "image_footnote": [],
        "page_idx": 399
    },
    {
        "type": "text",
        "text": "We can ask a person (preference evaluator) to evaluate the quality of that model generation. Say they assign it a certain score, like 4 (see Figure 12-22). ",
        "page_idx": 399
    },
    {
        "type": "image",
        "img_path": "images/f7c500f7504f0e4187d93f4b407c213ec020f0f4ab93675c9cb5c6a419504e7a.jpg",
        "image_caption": [
            "Figure 12-22. Use a preference evaluator (human or otherwise) to evaluate the quality of the generation. "
        ],
        "image_footnote": [],
        "page_idx": 399
    },
    {
        "type": "text",
        "text": "Figure 12-23 shows a preference tuning step updating the model based on that score: ",
        "page_idx": 399
    },
    {
        "type": "text",
        "text": "• If the score is high, the model is updated to encourage it to generate more like this type of generation. • If the score is low, the model is updated to discourage such generations. ",
        "page_idx": 399
    },
    {
        "type": "image",
        "img_path": "images/21939d28e7d57cc4e67db8ce4017f23c3c4934c94b3c4ed01cc2e83d65000dce.jpg",
        "image_caption": [
            "Figure 12-23. Preference tuning methods update the LLM based on the evaluation score. "
        ],
        "image_footnote": [],
        "page_idx": 400
    },
    {
        "type": "text",
        "text": "As always, we need many training examples. So can we automate the preference evaluation? Yes, we can by training a different model called a reward model. ",
        "page_idx": 400
    },
    {
        "type": "text",
        "text": "Automating Preference Evaluation Using Reward Models ",
        "text_level": 1,
        "page_idx": 400
    },
    {
        "type": "text",
        "text": "To automate preference evaluation, we need a step before the preference-tuning step, namely to train a reward model, as shown in Figure 12-24. ",
        "page_idx": 400
    },
    {
        "type": "image",
        "img_path": "images/2d83307b547b3f65cbf1506d8fba7ea3b9eb86cb12d3ac4e5f6374e309a4c1f1.jpg",
        "image_caption": [
            "Figure 12-24. We train a reward model before fine-tuning the LLM. "
        ],
        "image_footnote": [],
        "page_idx": 400
    },
    {
        "type": "text",
        "text": "Figure 12-25 shows that to create a reward model, we take a copy of the instructiontuned model and slightly change it so that instead of generating text, it now outputs a single score. ",
        "page_idx": 400
    },
    {
        "type": "image",
        "img_path": "images/25a6e90c6763ab848bda82fa9b5026c67d0359fe5717098425620f2e45e71c56.jpg",
        "image_caption": [
            "Figure 12-25. The LLM becomes a reward model by replacing its language modeling head with a quality classification head. "
        ],
        "image_footnote": [],
        "page_idx": 400
    },
    {
        "type": "text",
        "text": "The Inputs and Outputs of a Reward Model ",
        "text_level": 1,
        "page_idx": 401
    },
    {
        "type": "text",
        "text": "The way we expect this reward model to work is that we give it a prompt and a generation, and it outputs a single number indicating the preference/quality of that generation in response to that prompt. Figure 12-26 shows the reward model generating this single number. ",
        "page_idx": 401
    },
    {
        "type": "image",
        "img_path": "images/4fc9e76d7f277b5ad177299b50f54b4a4bb537dd9626f511c00c982784c85cf3.jpg",
        "image_caption": [
            "Figure 12-26. Use a reward model trained on human preference to generate the comple‐ tion quality score. "
        ],
        "image_footnote": [],
        "page_idx": 401
    },
    {
        "type": "text",
        "text": "Training a Reward Model ",
        "text_level": 1,
        "page_idx": 401
    },
    {
        "type": "text",
        "text": "We cannot directly use the reward model. It needs to first be trained to properly score generations. So let’s get a preference dataset that the model can learn from. ",
        "page_idx": 401
    },
    {
        "type": "text",
        "text": "Reward model training dataset ",
        "text_level": 1,
        "page_idx": 401
    },
    {
        "type": "text",
        "text": "One common shape for preference datasets is for a training example to have a prompt, with one accepted generation and one rejected generation. (Nuance: it’s not always a good versus bad generation; it can be that the two generations are both good, but that one is better than the other). Figure 12-27 shows an example preference training set with two training examples. ",
        "page_idx": 401
    },
    {
        "type": "image",
        "img_path": "images/e191e2ed73d351836e2f6a239a8a3dcced4c33c46c84bfe1c52f4dabf6caa1b0.jpg",
        "image_caption": [
            "Figure 12-27. Preference tuning datasets are often made up of prompts with accepted and rejected generations. "
        ],
        "image_footnote": [],
        "page_idx": 402
    },
    {
        "type": "text",
        "text": "One way to generate preference data is to present a prompt to the LLM and have it generate two different generations. As shown in Figure 12-28, we can ask human labelers which of the two they prefer. ",
        "page_idx": 402
    },
    {
        "type": "image",
        "img_path": "images/ccedf5de4b7f11e85ba395d5480b11d45e72749ef28a60b6a6bd31ae0a7dd74e.jpg",
        "image_caption": [
            "Figure 12-28. Output two generations and ask a human labeler which one they prefer. "
        ],
        "image_footnote": [],
        "page_idx": 402
    },
    {
        "type": "text",
        "text": "Reward model training step ",
        "text_level": 1,
        "page_idx": 403
    },
    {
        "type": "text",
        "text": "Now that we have the preference training dataset, we can proceed to train the reward model. ",
        "page_idx": 403
    },
    {
        "type": "text",
        "text": "A simple step is that we use the reward model to: ",
        "page_idx": 403
    },
    {
        "type": "text",
        "text": "1. Score the accepted generation   \n2. Score the rejected generation ",
        "page_idx": 403
    },
    {
        "type": "image",
        "img_path": "images/e3f2d9160b96c77c802299adb5651b6eb575535029a6435252ada988025f0548.jpg",
        "image_caption": [
            "Figure 12-29 shows the training objective: to ensure the accepted generation has a higher score than the rejected generation. ",
            "Figure 12-29. The reward model aims to evaluate the quality scores of generations in response to a prompt. "
        ],
        "image_footnote": [],
        "page_idx": 403
    },
    {
        "type": "text",
        "text": "When we combine everything together as shown in Figure 12-30, we get the three stages to preference tuning: ",
        "page_idx": 403
    },
    {
        "type": "text",
        "text": "1. Collect preference data   \n2. Train a reward model   \n3. Use the reward model to fine-tune the LLM (operating as the preference evaluator) ",
        "page_idx": 403
    },
    {
        "type": "image",
        "img_path": "images/7c51557a871cacc654682d702a1c6ca8a032b3f63a0a21bfafac948e9f7aa64f.jpg",
        "image_caption": [
            "Figure 12-30. The three stages of preference tuning: collecting preference data, training a reward model, and finally fine-tuning the LLM. "
        ],
        "image_footnote": [],
        "page_idx": 404
    },
    {
        "type": "text",
        "text": "Reward models are an excellent idea that can be further extended and developed. Llama 2, for example, trains two reward models: one that scores helpfulness and another that scores safety (Figure 12-31). ",
        "page_idx": 404
    },
    {
        "type": "image",
        "img_path": "images/4531625577c074e84d859e0c8f5ba23fa72de8903a0dcee48dbfe4b28693e516.jpg",
        "image_caption": [
            "Figure 12-31. We can use multiple reward models to perform the scoring. "
        ],
        "image_footnote": [],
        "page_idx": 404
    },
    {
        "type": "text",
        "text": "A common method to fine-tune the LLM with the trained reward model is Proximal Policy Optimization (PPO). PPO is a popular reinforcement technique that optimizes the instruction-tuned LLM by making sure that the LLM does not deviate too much from the expected rewards.21 It was even used to train the original ChatGPT released in November 2022. ",
        "page_idx": 404
    },
    {
        "type": "text",
        "text": "Training No Reward Model ",
        "text_level": 1,
        "page_idx": 405
    },
    {
        "type": "text",
        "text": "A disadvantage of PPO is that it is a complex method that needs to train at least two models, the reward model and the LLM, which can be more costly than perhaps necessary. ",
        "page_idx": 405
    },
    {
        "type": "text",
        "text": "Direct Preference Optimization (DPO) is an alternative to PPO and does away with the reinforcement-based learning procedure.22 Instead of using the reward model to judge the quality of a generation, we let the LLM itself do that. As illustrated in Figure 12-32, we use a copy of the LLM as the reference model to judge the shift between the reference and trainable model in the quality of the accepted generation and rejected generation. ",
        "page_idx": 405
    },
    {
        "type": "image",
        "img_path": "images/9168d5f45593ba621566b38ea2c9564a2e2d7e2a18014f9c6348bbd2d6df65d8.jpg",
        "image_caption": [
            "Figure 12-32. Use the LLM itself as the reward model by comparing the output of a frozen model with the trainable model. "
        ],
        "image_footnote": [],
        "page_idx": 405
    },
    {
        "type": "text",
        "text": "By calculating this shift during training, we can optimize the likelihood of accepted generations over rejected generations by tracking the difference in the reference model and the trainable model. ",
        "page_idx": 405
    },
    {
        "type": "text",
        "text": "To calculate this shift and its related scores, the log probabilities of the rejected generations and accepted generations are extracted from both models. As illustrated in Figure 12-33, this process is performed at a token level where the probabilities are combined to calculate the shift between the reference and trainable models. ",
        "page_idx": 405
    },
    {
        "type": "image",
        "img_path": "images/1e40df169bd737e7c59f1a7ba78a660d429d1b93faf62bdf4af0e78c884f6bb1.jpg",
        "image_caption": [
            "Figure 12-33. Scores are calculated by taking the probabilities of generation on a token level. The shift in probabilities between the reference model and the trainable model is optimized. The accepted generation follows the same procedure. "
        ],
        "image_footnote": [],
        "page_idx": 406
    },
    {
        "type": "text",
        "text": "Using these scores, we can optimize the parameters of the trainable model to be more confident of generating the accepted generations and less confident of generating the rejected generations. Compared to PPO, the authors found DPO to be more stable during training and more accurate. Due to its stability, we will be using it as our primary model for preference tuning our previously instruction-tuned model. ",
        "page_idx": 406
    },
    {
        "type": "text",
        "text": "Preference Tuning with DPO ",
        "text_level": 1,
        "page_idx": 406
    },
    {
        "type": "text",
        "text": "When we use the Hugging Face stack, preference tuning is eerily similar to the instruction tuning we covered before with some slight differences. We will still be using TinyLlama but this time an instruction-tuned version that was first trained using full fine-tuning and then further aligned with DPO. Compared to our initial instruction-tuned model, this LLM was trained on much larger datasets. ",
        "page_idx": 406
    },
    {
        "type": "text",
        "text": "In this section, we will demonstrate how you can further align this model using DPO with reward-based datasets. ",
        "page_idx": 406
    },
    {
        "type": "text",
        "text": "Templating Alignment Data ",
        "text_level": 1,
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "We will use a dataset that for each prompt contains an accepted generation and a rejected generation. This dataset was in part generated by ChatGPT with scores on which output should be accepted and which rejected: ",
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "from datasets import load_dataset   \ndef format_prompt(example): \"\"\"Format the prompt to using the <|user|> template TinyLLama is using\"\"\" # Format answers system $=$ \"<|system|>\\n\" $^ +$ example[\"system\"] + \"</s>\\n\" prompt $=$ \"<|user|>\\n\" $^ +$ example[\"input\"] $^ +$ \"</s>\\n<|assistant|>\\n\" chosen $=$ example[\"chosen\"] $^ +$ \"</s>\\n\" rejected $=$ example[\"rejected\"] $^ +$ \"</s>\\n\" return { \"prompt\": system $^ +$ prompt, \"chosen\": chosen, \"rejected\": rejected, }   \n# Apply formatting to the dataset and select relatively short answers   \ndpo_dataset $=$ load_dataset( \"argilla/distilabel-intel-orca-dpo-pairs\", split $: =$ \"train\"   \n)   \ndpo_dataset $=$ dpo_dataset.filter( lambda r: r[\"status\"] $\\downarrow =$ \"tie\" and r[\"chosen_score\"] $\\scriptstyle > = 8$ and not r[\"in_gsm8k_train\"]   \n)   \ndpo_dataset $=$ dpo_dataset.map( format_prompt, remove_columns $: =$ dpo_dataset.column_names   \n)   \ndpo_dataset ",
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "Note that we apply additional filtering to further reduce the size of the data to roughly 6,000 examples from the original 13,000 examples. ",
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "Model Quantization ",
        "text_level": 1,
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "We load our base model and load it with the LoRA we created previously. As before, we quantize the model to reduce the necessary VRAM for training: ",
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "from peft import AutoPeftModelForCausalLM   \nfrom transformers import BitsAndBytesConfig, AutoTokenizer   \n# 4-bit quantization configuration - Q in QLoRA   \nbnb_config $=$ BitsAndBytesConfig( load_in_4bit=True, # Use 4-bit precision model loading ",
        "page_idx": 407
    },
    {
        "type": "text",
        "text": "bnb_4bit_quant_type $\\ast =$ \"nf4\", # Quantization type bnb_4bit_compute_dtype $\\Bumpeq$ \"float16\", # Compute dtype bnb_4bit_use_double_quant=True, # Apply nested quantization ) ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "# Merge LoRA and base model   \nmodel $=$ AutoPeftModelForCausalLM.from_pretrained( \"TinyLlama-1.1B-qlora\", low_cpu_mem_usage=True, device_map=\"auto\", quantization_config=bnb_config,   \n)   \nmerged_model $=$ model.merge_and_unload()   \n# Load LLaMA tokenizer   \nmodel_name $=$ \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"   \ntokenizer $=$ AutoTokenizer.from_pretrained(model_name, trust_remote_code $=$ True)   \ntokenizer.pad_token $=$ \"<PAD>\"   \ntokenizer.padding_side $=$ \"left\" ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "Next, we use the same LoRA configuration as before to perform the DPO training: ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "# Prepare LoRA configuration   \npeft_config $=$ LoraConfig( lora_alpha $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = \\mathbf { \\Psi } } \\end{array}$ , # LoRA Scaling lora_dropout $= 0 . 1$ , # Dropout for LoRA Layers ${ \\sf \\Gamma } \\Gamma = { \\mathrm {  ~ \\Gamma ~ } }$ , # Rank bias $=$ \"none\", task_type $\\ast =$ \"CAUSAL_LM\", target_modules $=$ # Layers to target [\"k_proj\", \"gate_proj\", \"v_proj\", \"up_proj\", \"q_proj\", \"o_proj\",   \n\"down_proj\"]   \n) ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "# prepare model for training model $=$ prepare_model_for_kbit_training(model) model $=$ get_peft_model(model, peft_config) ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "Training Configuration ",
        "text_level": 1,
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "For the sake of simplicity, we will use the same training arguments as we did before with one difference. Instead of running for a single epoch (which can take up to two hours), we run for 200 steps instead for illustration purposes. Moreover, we added the warmup_ratio parameter, which increases the learning rate from 0 to the learning_rate value we set for the first $1 0 \\%$ of steps. By maintaining a small learning rate at the start (i.e., warmup period), we allow the model to adjust to the data before applying larger learning rates, therefore avoiding harmful divergence: ",
        "page_idx": 408
    },
    {
        "type": "text",
        "text": "from trl import DPOConfig output_dir $=$ \"./results\" ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "# Training arguments   \ntraining_arguments $=$ DPOConfig( output_dir $=$ output_dir, per_device_train_batch_size $^ { = 2 }$ , gradient_accumulation_steps $\\mathbf { \\Psi } = \\mathbf { \\Psi }$ , optim=\"paged_adamw_32bit\", learning_rate $\\iota =$ 1e-5, lr_scheduler_type $\\Bumpeq$ \"cosine\", max_steps $\\begin{array} { r l } { \\mathbf { \\Psi } } & { { } = } \\end{array} .$ , logging_steps $\\begin{array} { r l } { \\mathit { \\Pi } } & { { } = } \\\\ { \\mathit { \\Pi } } & { { } = } \\end{array} .$ , fp16=True, gradient_checkpointing=True, warmup_ratio=0.1   \n) ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "Training ",
        "text_level": 1,
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "Now that we have prepared all our models and parameters, we can start fine-tuning our model: ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "from trl import DPOTrainer ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "# Create DPO trainer   \ndpo_trainer $=$ DPOTrainer( model, args $=$ training_arguments, train_dataset=dpo_dataset, tokenizer=tokenizer, peft_config=peft_config, beta=0.1, max_prompt_length $1 = 5 1 2$ , max_length=512,   \n) ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "# Fine-tune model with DPO dpo_trainer.train() ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "# Save adapter dpo_trainer.model.save_pretrained(\"TinyLlama-1.1B-dpo-qlora\") ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "We have created a second adapter. To merge both adapters, we iteratively merge the adapters with the base model: ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "from peft import PeftModel ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "# Merge LoRA and base model   \nmodel $=$ AutoPeftModelForCausalLM.from_pretrained( \"TinyLlama-1.1B-qlora\", low_cpu_mem_usage=True, device_map=\"auto\",   \n)   \nsft_model $=$ model.merge_and_unload()   \n# Merge DPO LoRA and SFT model   \ndpo_model $=$ PeftModel.from_pretrained( sft_model, \"TinyLlama-1.1B-dpo-qlora\", device_map=\"auto\",   \n)   \ndpo_model $=$ dpo_model.merge_and_unload() ",
        "page_idx": 409
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "This combination of $\\mathrm { S F T + D P O }$ is a great way to first fine-tune your model to perform basic chatting and then align its answers with human preference. However, it does come at a cost since we need to perform two training loops and potentially tweak the parameters in two processes. ",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "Since the release of DPO, new methods of aligning preferences have been developed. Of note is Odds Ratio Preference Optimization (ORPO), a process that combines SFT and DPO into a single training process.23 It removes the need to perform two separate training loops, further simplifying the training process while allowing for the use of QLoRA. ",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "Summary ",
        "text_level": 1,
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "In this chapter, we explored different steps of fine-tuning pretrained LLMs. We per‐ formed fine-tuning by making use of parameter-efficient fine-tuning (PEFT) through the low-rank adaptation (LoRA) technique. We explained how LoRA can be extended through quantization, a technique for reducing memory constraints when represent‐ ing the parameters of the model and adapters. ",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "The fine-tuning process we explored has two steps. In the first step, we performed supervised fine-tuning using instruction data on a pretrained LLM, often called instruction tuning. This resulted in a model that has chat-like behavior and could closely follow instructions. ",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "In the second step, we further improved the model by fine-tuning it on alignment data, data that represents what type of answers are preferred over others. This pro‐ cess, referred to as preference tuning, distills human preference to the previously instruction-tuned model. ",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "Overall, this chapter has shown the two major steps of fine-tuning a pretrained LLM and how that could lead to more accurate and informative outputs. ",
        "page_idx": 410
    },
    {
        "type": "text",
        "text": "Afterword ",
        "text_level": 1,
        "page_idx": 412
    },
    {
        "type": "text",
        "text": "Thank you to all who joined us on this fascinating journey through the world of large language models. We are grateful for your dedication to learning about these powerful models that have revolutionized language processing. ",
        "page_idx": 412
    },
    {
        "type": "text",
        "text": "Throughout this book, we have seen how LLMs work and how they can be used to create a wide range of applications, from simple chatbots to more complex sys‐ tems like search engines. We have also explored various methods for fine-tuning pretrained LLMs on specific tasks, including classification, generation, and language representation. By mastering these techniques, readers will be able to unlock the potential of LLMs and create innovative solutions that can benefit from their capabili‐ ties. This knowledge will enable readers to stay ahead of the curve and adapt to new developments in the field. ",
        "page_idx": 412
    },
    {
        "type": "text",
        "text": "As we come to the end of this book, we want to emphasize that our exploration of LLMs is only just the beginning. There are many more exciting developments on the horizon, and we encourage you to continue following the advancements in the field. To help with this process, keep an eye out on the repository of this book as we continue to add resources. ",
        "page_idx": 412
    },
    {
        "type": "text",
        "text": "We hope that by reading this book, you gained a deeper understanding of how LLMs can be used in various applications and how they have the potential to transform industries. ",
        "page_idx": 412
    },
    {
        "type": "text",
        "text": "With this book as your guide, we believe that you will be well-equipped to navigate the exciting landscape of LLMs and make meaningful contributions to this rapidly advancing field. ",
        "page_idx": 412
    },
    {
        "type": "text",
        "text": "Index ",
        "text_level": 1,
        "page_idx": 414
    },
    {
        "type": "text",
        "text": "A ",
        "text_level": 1,
        "page_idx": 414
    },
    {
        "type": "text",
        "text": "accuracy confusion matrices, 120 output verification, 192   \nadaptive pretraining, 320   \nagents, 218-223 agentic RAG, 256 ReAct in LangChain, 221-223 step-by-step reasoning, 219-221   \nAI (artificial intelligence) accelerated development of, 3 defined, 4   \nALBERT, 115   \nalign_labels function, 350   \nall-MiniLM-L6-v2 model, 309   \nall-mpnet-base-v2 model, 337   \nAnnoy, 239   \nAnthropic Claude, 29   \nAPIs (application programming interfaces), 30 Cohere, xv, 230 external, 134 generating embeddings, 123 OpenAI, xv, 133   \nartificial intelligence (see AI)   \nArXiv, 138   \nattention, 11-18 overview of, 11-14 Transformer architecture, 15-18 attention calculation, 91-93 attention layer, 79, 86, 88, 106 Flash Attention, 100 grouped-query attention, 98-100 local attention, 96 multi-query attention, 98-100 optimizing attention, 98 self-attention and relevance scoring, 93-94 sparse attention, 96   \nattention heads, 91-93, 107   \naudience, in text-generation prompts, 178   \nAugmented SBERT, 311-315   \nautoregressive architecture, 12, 15, 22, 76, 97 ",
        "page_idx": 414
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 414
    },
    {
        "type": "text",
        "text": "B ",
        "text_level": 1,
        "page_idx": 414
    },
    {
        "type": "text",
        "text": "bag-of-words model, 6-7 embeddings, 10 topic modeling, 148-150, 156   \nbenchmarks, in generative model evaluation, 374   \nBERT (Bidirectional Encoder Representations from Transformers), 18-20 adoption by search engines, 225 BERT-like models, 115 comparing to other trained tokenizers, 47 fine-tuning pretrained BERT models, 325-328 masked language modeling, 311 Transformer blocks versus, 97   \nBERTopic, 148-155 algorithmic variants, 152 modularity of, 151 representation blocks, 156-163 KeyBERTInspired, 158 maximal marginal relevance, 159 text generation, 160-163   \nBERTScore, 374   \nbias and fairness, 28   \nBidirectional Encoder Representations from Transformers (see BERT)   \nbitsandbytes package, 369   \nBLEU, 374   \nBLIP-2 (Bootstrapping Language-Image Pretraining for Unified Vision-Language Understanding and Generation 2) chat-based prompting, 283-286 image captioning, 280 preprocessing text, 280 Q-Former, 273-277   \nBM25 algorithm, 233   \nBPE (byte pair encoding), 43, 55   \nbyte tokens, 45 ",
        "page_idx": 414
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 415
    },
    {
        "type": "text",
        "text": "C ",
        "text_level": 1,
        "page_idx": 415
    },
    {
        "type": "text",
        "text": "c-TF-IDF, 149, 158, 161   \ncapitalization, 56   \ncaptioning, 280-283   \ncentroid-based algorithms, 143   \nchains, 182-184, 202-209 chain-of-thought, 185-188 chaining single prompt, 203-205 sequential chaining of multiple prompts, 206-209   \ncharacter tokens, 45   \nchat tokens, 54   \nchat-based prompting, 283-286   \nChatbot Arena, 376   \nChatGPT, 202, 273, 383, 386 release of, 3 text classification, 132-135   \nchatgpt_generation function, 133   \nchat_history input variable, 211   \nclassification reports, 119-120   \nclassification step, embedding model, 121   \nCLIP, 265-268 connecting text and images, 265 generating multimodal embeddings, 265-268 OpenCLIP, 268-272   \nclosed-source LLMs, 29   \n[CLS] token, 18, 48, 60, 270, 294, 318, 351   \ncluster model, 142-144   \nclustering (see text clustering)   \nCNNs (convolutional neural networks), 260   \nCohere, 252 Command R+, 30, 256 creating accounts, xv, 230 ",
        "page_idx": 415
    },
    {
        "type": "text",
        "text": "generating embeddings, 123 query rewriting, 255 Rerank endpoint, 241 completion models, 22 compute_metrics function, 351 confusion matrices, 119 CoNLL-2003 dataset, 347 constrained sampling, 194-197 context attention and, 88 prompt engineering, 178 training datasets, 64 context length completion models, 22 token processing limits, 81 context window, in completion models, 22 contrastive learning text embedding models, 291-293, 296 word2vec algorithm and, 64-67 Contrastive Tension (CT), 316 conversation buffer memory, 217 overview of, 210-212 windowed, 212-214 conversation summary memory, 214-217 convert_ids_to_tokens function, 269 convolutional neural networks (CNNs), 260 cosine similarity, 125, 302-304 CountVectorizer, 150 cross-encoders, 244 cross-entropy loss, 305 CT (Contrastive Tension), 316 ",
        "page_idx": 415
    },
    {
        "type": "text",
        "text": "D ",
        "text_level": 1,
        "page_idx": 415
    },
    {
        "type": "text",
        "text": "data outliers, 143   \nDataCollator class, 326, 342   \ndatamapplot package, 163   \nDBSCAN (Density-Based Spatial Clustering), 143   \nDeBERTa, 59, 115   \ndecoder-only models (see generative models)   \ndecoding strategy, 79-81, 106   \ndense retrieval, 226, 228-240 caveats of, 234 example of, 230-234 fine-tuning embedding models for, 239 nearest neighbor search versus vector data‐ bases, 238 text chunking, 235-237   \ndensity-based algorithms, 143   \nDensity-Based Spatial Clustering (DBSCAN), 143   \ndimensionality reduction model, 140-142   \nDistilBERT, 115, 120   \ndomain adaptation, 320   \ndo_sample parameter, 34   \nDPO (Direct Preference Optimization), 384-389 fine-tuning, 388 model quantization, 386 templating alignment data, 386 training configuration, 387   \nDSPy, 200 ",
        "page_idx": 415
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "asy negatives, 308   \nlo rating system, 377   \nmbeddings, 8-10, 37, 57-70 dense retrieval, 226, 228-240 embedding models, defined, 114, 290 multimodality, 263-272 CLIP, 265-268 OpenCLIP, 268-272 overview of, 8-10, 289-291 positional embeddings, 102-104 recommendation systems, 67-70 text classification tasks that leverage, 120-126 supervised classification, 121-123 zero-shot classification, 123-126 text clustering pipeline, 139-146 cluster model, 142-144 dimensionality reduction model, 140-142 embedding model, 139 inspecting clusters, 144-146 text embedding models, 61-63, 289-321 contrastive learning, 291-293 creating, 296-308 fine-tuning, 309-315 SBERT, 293-296 unsupervised learning, 316 token embeddings, 57-61 creating contextualized word embed‐ dings, 58-61 tokenizer’s vocabulary and, 57 types of, 10 word embeddings, 63-67 pretrained, 63 ",
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "word2vec algorithm and contrastive training, 64-67 encoder-decoder models, 128 encoder-only models (see representation mod‐ els) ethics, validating output, 192 exponential backoff, 134 ",
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "F   \nF1 score, confusion matrices, 120   \nFAISS, 239   \nFalcon, 168   \nfeature extraction step, embedding model, 121   \nfeedforward layer, 86   \nfeedforward neural networks, 15, 87, 106, 326   \nfew-shot classification, 333-339 fine-tuning for classification, 337-339 SetFit, 333-336   \nfew-shot prompting, 181-182, 192-194   \nfind_topics() function, BERTopic, 154   \nfine-tuning embedding models for dense retrieval, 239 generative models, 192, 355-389 evaluating, 373-377 preference tuning, 356, 378-389 supervised fine-tuning, 356-373 training steps, 355-357 overview of, 26 representation models, 323-354 few-shot classification, 333-339 masked language modeling, 340-345 named-entity recognition, 345-353 supervised classification, 323-332 T5 model, 129 text embedding models, 309-315 Augmented SBERT, 311-315 supervised, 309-311   \nFlan-T5 model, 50, 130-131   \nFlash Attention, 100, 107   \nforward pass, 106 components of, 76-79 defined, 74   \nfoundation models, 23   \nfp16 parameter, 299   \nfreezing layers, 298, 328-332   \nfrozen (nontrainable) models, 114, 121, 324 ",
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "G Ġ symbol, 280 ",
        "text_level": 1,
        "page_idx": 416
    },
    {
        "type": "text",
        "text": "Galactica, 52   \nGeneral Language Understanding Evaluation (GLUE) benchmark, 297, 300, 359, 374   \ngenerated_text variable, 281, 283   \ngeneration_output variable, 42   \ngenerative models, 20-22 evaluating, 373-377 automated evaluation, 376 benchmarks, 374 human evaluation, 376 leaderboards, 376 word-level metrics, 374 fine-tuning, 355-389 evaluation, 373-377 preference tuning, 356, 378-389 supervised fine-tuning, 356-373 training steps, 355-357 reasoning, 184-191 self-consistency, 188 tree-of-thought, 189-191 representation models versus, 20 text classification, 127-135 ChatGPT, 132-135 T5, 128-131   \ngenerative pre-trained transformers (see GPTs)   \nGenerative Pseudo-Labeling (GPL), 316   \nGensim library, 63   \nget_topic function, 153   \nget_topic_info() method, 152   \nGGUF model, 195, 200   \nGitHub, xvi   \nGloVe, 294   \nGLUE (General Language Understanding Eval‐ uation) benchmark, 297, 300, 359, 374   \ngold datasets, 312   \nGoodhart’s Law, 377   \nGoogle Colab, xii, xiv, 29, 33, 85, 372   \nGoogle Gemini, 250   \nGoogle Search, 225   \nGPL (Generative Pseudo-Labeling), 316   \nGPT2Tokenizer, 280   \nGPTs (generative pre-trained transformers), 20, 167 (see also text generation) GPT-1, 20 GPT-2, 3, 20, 46, 49 GPT-3, 20, 87, 96, 364 GPT-3.5, 23, 132, 205, 221 GPT-4, 23, 29, 38, 50, 87   \nGPUs Flash Attention, 100 requirements, xiv, 28 SRAM and HBM, 100   \ngrammar, 192, 194-197   \ngreedy decoding, 80   \ngrounded generation, 250-252   \ngrouped-query attention, 98-100, 107   \nGSM8k, 374   \nGuardrails, 194   \nGuidance, 194 ",
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "H ",
        "text_level": 1,
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "hallucination avoiding in in instruction-based prompting, 177 text generation models, 225   \nhard negatives, 308   \nharmful content, generating, 28   \nHaystack, 200   \nHBM (high bandwidth memory), 100   \nHDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise), 143, 153   \nHellaSwag, 374   \nhigh bandwidth memory (HBM), 100   \nHugging Face, 32, 112 creating accounts, xv evaluate package, 351 tokenizers, 56   \nhuman evaluation, 145, 376   \nHumanEval, 375   \nhybrid search, 235, 242 ",
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "I ",
        "text_level": 1,
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "Idefics 2, 277   \nimages (see multimodality)   \nin-context learning, 180-182   \nindexes, 232   \nInfoNCE, 304   \ninput_ids variable, 40   \ninstruction-based prompting, 175-177   \nintellectual property, 28   \nintuition-first philosophy, xi   \ninvoke function, 204 ",
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "J ",
        "text_level": 1,
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "Jupyter, 85 ",
        "page_idx": 417
    },
    {
        "type": "text",
        "text": "",
        "text_level": 1,
        "page_idx": 418
    },
    {
        "type": "text",
        "text": "k-means algorithm, 143, 151, 153   \nKeyBERTInspired, 158, 160   \nkeyword search reranking, 242-243 verifying semantic search with, 233   \nkv (keys and values) cache, 83-85   \nL   \nLangChain, 200 (see also chains) loading quantized models with, 200-202 ReAct in, 221-223   \nLanguage AI (Language Artificial Intelligence), 3-24 defining, 4 recent history of, 5-24 attention, 11-18 bag-of-words model, 6-7 embeddings, 8-10 generative models, 20-22 representation models, 18-20 Year of Generative AI, 23-24   \nlanguage modeling, 355   \nlanguage modeling head (LM head), 76-79   \nlarge language models (see LLMs)   \nlatent Dirichlet allocation, 147   \nLayerNorm, 101   \nleaderboards, in generative model evaluation, 376   \nlearning_rate parameter, 371   \nLlama, 168   \nLlama 2, xv, 29, 53, 98, 168, 273   \nllama-cpp-python library, 195   \nLLaVA, 277   \nLLM-as-a-judge, 257   \nLLMs (large language models), 3-35 code examples and exercises, xvi embeddings, 37, 57-70 recommendation systems, 67-70 text embeddings, 61-63 token embeddings, 57-61 word embeddings, 63-67 fine-tuning generative models, 355-389 evaluation, 373-377 preference tuning, 356, 378-389 supervised fine-tuning, 356-373 training steps, 355-357 fine-tuning representation models, 323-354 few-shot classification, 333-339 masked language modeling, 340-345 named-entity recognition, 345-353 supervised classification, 323-332   \ngenerating text, 32-34   \ngenerative models, 20-22   \nhardware and software requirements, xiv, 2   \nhigh-level view, 38   \nhistory of Language AI, 5-24   \ninterfacing with, 29-32 closed-source models, 29 open models, 30-32   \nintuition-first philosophy, xi   \nmoving definition of, 25   \nmultimodality, 259-286 embedding models, 263-272 text generation models, 273-286 Vision Transformer, 260-262   \nprompt engineering, 167-198 chain prompting, 182-184 in-context learning, 180-182 instruction-based prompting, 175-177 output verification, 191-197 potential complexity of prompts, 177-179 prompt components, 173-175 reasoning with generative models, 184-191 text generation models, 167-172   \nrepresentation models, 18-20   \nresponsible development and usage of, 28   \nretrieval-augmented generation, 227, 249-257 agentic RAG, 256 converting search system to, 250 evaluating results, 257 grounded generation, 252 multi-hop RAG, 256 multi-query RAG, 255 query rewriting, 255 query routing, 256 with local models, 252, 254   \nsemantic search, 225-249 dense retrieval, 226, 228-240 reranking, 226, 240-244 retrieval evaluation metrics, 244-249   \ntext classification, 111-135 with generative models, 127-135 movie reviews, 112, 113 ",
        "page_idx": 418
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 418
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 418
    },
    {
        "type": "text",
        "text": "with representation models, 113-126 text clustering, 137-146 text embedding models, 289-321 contrastive learning, 291-293 creating, 296-308 fine-tuning, 309-315 SBERT, 293-296 unsupervised learning, 316 text generation, 199-224 agents, 218-223 chains, 202-209 memory of conversations, 209-217 model I/O, 200-202 tokens and tokenizers, 37-61 comparing trained tokenizers, 46-54 downloading and running LLMs, 39-42 input preparation, 38 text breakdown, 43 token embeddings, 57-61 tokenization schemes, 44-46 tokenizer properties, 55-56 topic modeling, 138, 146-163 training paradigm of, 25-26 Transformer architecture, 73-107 decoding strategy, 79-81 forward pass components, 76-79 inputs and outputs of, 74-76 keys and values cache, 83-85 parallel token processing and context size, 81-83 recent improvements to, 95-105 Transformer blocks, 85-94 utility of, 27 M head (language modeling head), 76-79 MQL, 194 ocal attention, 96 oRA (low-rank adaptation), 361-364, 387 (see also QLoRA) ora_alpha parameter, 370 oss functions, 301-308 cosine similarity loss, 302-304 multiple negatives ranking loss, 304-308 _scheduler_type parameter, 371 ",
        "page_idx": 419
    },
    {
        "type": "text",
        "text": "M ",
        "text_level": 1,
        "page_idx": 419
    },
    {
        "type": "text",
        "text": "Mamba, 24   \nMAP (mean average precision), 244-249   \nMarginMSE loss, 302   \nmasked language modeling (MLM), 340-345 ",
        "page_idx": 419
    },
    {
        "type": "text",
        "text": "mask_token [MASK], 48   \nMassive Text Embedding Benchmark (MTEB), 116, 140, 253, 300   \nmatplotlib library, 145   \nmaximal marginal relevance (MMR), 159   \nmax_new_tokens parameter, 34   \nMcCarthy, John, 4   \nmean average precision (MAP)   \nmemory of conversations, 209-217 conversation buffer, 210-212 conversation summary, 214-217 windowed conversation buffer, 212-214   \nMeta Llama model, 30   \nMicrosoft Bing, 225, 241   \nMicrosoft Bing AI, 250   \nMicrosoft Phi model, 30   \nmicrosoft/mpnet-base model, 297   \nmin_cluster_size parameter, 144   \nmin_dist parameter, 142   \nMIRACL, 243   \nMistral, 30, 168, 277   \nMLM (masked language modeling), 340-345   \nMMLU, 374, 376   \nMMR (maximal marginal relevance), 159   \nMNLI (Multi-Genre Natural Language Infer‐ ence) corpus, 297, 305   \nMNR (multiple negatives ranking) loss, 304-308   \nmodel I/O, 200-202   \nmonoBERT, 244   \nMTEB (Massive Text Embedding Benchmark), 116, 140, 253, 300   \nMulti-Genre Natural Language Inference (MNLI) corpus, 297, 305   \nmulti-hop RAG, 256   \nmulti-query attention, 98-100   \nmulti-query RAG, 255   \nmultilevel perceptrons, 79 (see also feedforward neural networks)   \nmultimodality, 259-286 defined, 259 embedding models, 263-272 CLIP, 265-268 OpenCLIP, 268-272 text generation models, 273-286 BLIP-2, 273-277 chat-based prompting, 283-286 image captioning, 280-283 preprocessing images, 278 ",
        "page_idx": 419
    },
    {
        "type": "text",
        "text": "preprocessing text, 279 Vision Transformer, 260-262 multiple negatives ranking (MNR) loss, 304-308 ",
        "page_idx": 420
    },
    {
        "type": "text",
        "text": "N ",
        "text_level": 1,
        "page_idx": 420
    },
    {
        "type": "text",
        "text": "named-entity recognition (see NER)   \nnatural language inference (NLI), 296   \nnatural language processing (NLP), 4, 293   \nnDCG (normalized discounted cumulative gain), 243, 249   \nnearest neighbor search pretrained word embeddings, 63 recommendation system embeddings, 68 vector databases versus, 238   \nnegative sampling, 65   \nNER (named-entity recognition), 345-353, 360 fine-tuning for, 352 preparing data for, 347-351   \nneural networks, 8   \nNLI (natural language inference), 296   \nNLP (natural language processing), 4, 293   \nnoise-contrastive estimation, 65   \nnonplayable characters (NPCs), 4   \nnontrainable (frozen) models, 114, 121, 324   \nnormalization, Transformer block, 101   \nnormalized discounted cumulative gain (nDCG), 243, 249   \nNPCs (nonplayable characters), 4   \nNTXentLoss, 304   \nnucleus sampling, 171   \nNumPy, 238   \nnum_train_epochs parameter, 299, 371   \nNVIDIA GPUs, xiv, 33   \nn_components parameter, 142   \nO   \nOdds Ratio Preference Optimization (ORPO), 389   \none-shot prompting, 182 chain-of-thought versus, 186 in-context learning, 181   \nOpen LLM Leaderboard, 202, 376   \nopen-source LLMs, 30-32   \nOpenAI, 132 (see also ChatGPT; GPTs) creating accounts, xv, 133 generating embeddings, 123   \nOpenCLIP, 268-272   \noptim parameter, 372   \nORPO (Odds Ratio Preference Optimization), 389   \noutput verification, 191-197 constrained sampling, 194-197 providing examples, 192   \nP   \npad_token [PAD], 47   \nparallel processing, 91, 106   \nparallel prompts, 184   \nPCA (Principal Component Analysis), 141   \nPEFT (parameter-efficient fine-tuning), 359-367 adapters, 359-361 compression, 364-367 LoRA, 361-364   \npeft library, 370   \npeft_config parameter, 371   \nPerplexity, 250   \npersona, in text-generation prompts, 178   \nper_device_eval_batch_size argument, 299   \nper_device_train_batch_size argument, 299   \nPhi-3 comparing to other trained tokenizers, 53 forward pass, 79 loading quantized models, 202 prompt template, 204 quantization, 201   \nPhi-3-mini, 32, 168   \nPinecone, 239   \npositional embeddings, 102-104   \nPPO (Proximal Policy Optimization), 383   \nprecision predictions, confusion matrices, 119   \npredictions, task-specific model, 118   \npreference tuning, 132, 356, 378-389 Direct Preference Optimization, 384-389 fine-tuning, 388 model quantization, 386 templating alignment data, 386 training configuration, 387 reward models, 379-383 inputs and outputs of, 380 training, 380-383   \npretraining, defined, 26   \nprimacy effect, 177   \nPrincipal Component Analysis (PCA), 141   \nprojection matrices, 92   \nprompt engineering, 127, 167-198 ",
        "page_idx": 420
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 420
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 420
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 420
    },
    {
        "type": "text",
        "text": "chain prompting, 182-184 in-context learning, 180-182 instruction-based prompting, 175-177 output verification, 191-197 constrained sampling, 194-197 providing examples, 192 potential complexity of prompts, 177-179 prompt components, 173-175 reasoning with generative models, 184-191 text generation models, 167-172 choosing, 167 controlling output, 170-172 loading, 168-169 roximal Policy Optimization (PPO), 383 ython, learning about, xii ",
        "page_idx": 421
    },
    {
        "type": "text",
        "text": "Q   \nQ-Former (Querying Transformer), 274-277   \nQ8 model, 201   \nQLoRA (quantized low-rank adaptation), 367-373 fine-tuning, 372 LoRA configuration, 370 merging weights, 373 model quantization, 369 templating instruction data, 367 training configuration, 371   \nquantization, 201, 364-367   \nquantization_config parameter, 371   \nquantized low-rank adaptation (see QLoRA)   \nQuerying Transformer (Q-Former), 274-277 ",
        "page_idx": 421
    },
    {
        "type": "text",
        "text": "R ",
        "text_level": 1,
        "page_idx": 421
    },
    {
        "type": "text",
        "text": "r parameter, 370   \nRAG (retrieval-augmented generation), 57, 63, 227, 249-257 agentic RAG, 256 basic pipeline, 249 converting search system to, 250 evaluating results, 257 grounded generation, 252 with local models, 252-254 multi-hop RAG, 256 multi-query RAG, 255 query rewriting, 255 query routing, 256   \nRagas, 257   \nrandom_state parameter, 142   \nrate limit errors, 134   \nReAct in LangChain, 221-223 step-by-step reasoning, 219   \nreasoning with generative models, 184-191 chain-of-thought, 185-188 self-consistency, 188 tree-of-thought, 189-191 step-by-step, 219, 221   \nrecall predictions, confusion matrices, 119   \nrecency effect, 177   \nrecommendation systems, 67-70   \nrecurrent neural networks (RNNs), 11   \nreduce_outliers() function, 153   \nregulation, 28   \nrelevance scoring, 90, 92-94, 104   \nrepository, xiv   \nrepresentation models, 18-20 defined, 7 fine-tuning for classification, 323-354 few-shot classification, 333-339 masked language modeling, 340-345 named-entity recognition, 345-353 supervised classification, 323-332 generative models versus, 20 text classification, 113-126 classification tasks that leverage embed‐ dings, 120-126 model selection, 115-116 task-specific models, 116   \nrepresentation_model parameter, 161   \nreranking, 226, 240-244 BERTopic, 156 example of, 241-243 function of reranking models, 244 sentence transformers, 243   \nresponse validation, in chain prompting, 184   \nretrieval evaluation metrics, 244-249 scoring multiple queries with mean average precision, 248 scoring single queries with average preci‐ sion, 247   \nretrieval-augmented generation (see RAG)   \nreturn_full_text parameter, 33   \nreward models, 379-383 inputs and outputs of, 380 training, 380-383   \nRMSNorm, 101   \nRNNs (recurrent neural networks), 11 ",
        "page_idx": 421
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 421
    },
    {
        "type": "text",
        "text": "RoBERTa, 46, 115   \nRoPE (rotary positional embeddings), 102-104   \nRorschach test, 282   \nRotten Tomatoes dataset, 112, 325   \nROUGE, 374   \nRWKV, 24 ",
        "page_idx": 422
    },
    {
        "type": "text",
        "text": "S   \nSBERT, 293-296, 311-315   \nself-attention, 15, 93-94   \nself-consistency, 188   \nsemantic search, 225-249 defined, 225 dense retrieval, 226, 228-240 caveats of, 234 example of, 230-234 fine-tuning embedding models for, 239 nearest neighbor search versus vector databases, 238 text chunking, 235-237 reranking, 226, 240-244 example of, 241-243 function of reranking models, 244 sentence transformers, 243 retrieval evaluation metrics, 244-249 scoring multiple queries with mean aver‐ age precision, 248 scoring single queries with average pre‐ cision, 247   \nSemantic Textual Similarity Benchmark (STSB), 298   \nsemi-hard negatives, 308   \nsentence-transformers, 62, 122, 151, 243, 272, 293-295, 309, 333   \nSentencePiece, 50   \n[SEP] token, 47, 60, 351   \nsequence-to-sequence models, 127, 128   \nSetFit, 323, 333-336   \nSFT (supervised fine-tuning), 356-373 full fine-tuning, 357 parameter-efficient fine-tuning, 359-367 adapters, 359-361 compression, 364-367 LoRA, 361-364 QLoRA, 367-373 fine-tuning, 372 LoRA configuration, 370 merging weights, 373 model quantization, 369 ",
        "page_idx": 422
    },
    {
        "type": "text",
        "text": "templating instruction data, 367 training configuration, 371 shared memory (SRAM), 100 shortlisting, 242 silver datasets, 312 SimCSE (Simple Contrastive Learning of Sen‐ tence Embeddings), 316 skip-gram, 65 softmax loss function, 301 song recommendation systems, 67-70 sparse attention, 96 special tokens, 47, 55 specificity, in instruction-based prompting, 176 SRAM (shared memory), 100 StableLM, 168 StarCoder2, 51 step-by-step reasoning, 219-221 structured output, validating, 191 STSB (Semantic Textual Similarity Bench‐ mark), 298 subword tokens, 44 supervised classification, 121-123 fine-tuning representation models for, 323-332 freezing layers, 328-332 pretrained BERT models, 325-328 supervised fine-tuning (see SFT) system 1 and 2 thinking processes, 185 ",
        "page_idx": 422
    },
    {
        "type": "text",
        "text": "T   \nT5 (Text-to-Text Transfer Transformer), 128-131   \ntarget_modules parameter, 370   \ntask-specific models, 113   \ntemperature parameter, 171-172, 188   \nTesla T4, 372   \ntest splits, 113, 118   \ntext chunking, 230, 235-237 approaches for, 237 multiple vectors per document, 236 one vector per document, 236   \ntext classification, 111-135 with generative models, 127-135 ChatGPT, 132-135 T5, 128-131 movie reviews, 112-113 with representation models, 113-126 classification tasks that leverage embed‐ dings, 120-126 model selection, 115-116 task-specific models, 116-120   \ntext clustering, 137-146 CLIP embedding model and, 265 common pipeline for, 139-146 cluster model, 142-144 dimensionality reduction model, 140-142 embedding model, 139 inspecting clusters, 144-146   \ntext embedding models, 61-63, 289-321 contrastive learning, 291-293 creating, 296-308 evaluating, 300 generating contrastive examples, 296 loss functions, 301-308 training, 297-300 fine-tuning, 309 Augmented SBERT, 311-315 supervised, 309-311 SBERT, 293-296 unsupervised learning, 316   \ntext generation, 32-34, 199-224 agents, 218-223 ReAct in LangChain, 221-223 step-by-step reasoning, 219-221 chains, 202-209 chaining single prompt, 203-205 sequential chaining of multiple prompts, 206-209 memory of conversations, 209-217 conversation buffer, 210-212 conversation summary, 214-217 windowed conversation buffer, 212-214 model I/O, 200-202 multimodality, 273-286 BLIP-2, 273-277 chat-based prompting, 283-286 image captioning, 280-283 preprocessing images, 278 preprocessing text, 279 prompt engineering, 167-172 choosing models, 167 controlling output, 170-172 loading models, 168-169 topic modeling, 160-163   \ntext-in-text-out model, 74   \nText-to-Text Transfer Transformer (T5), 128-131   \nenlper/gte-small model, 140   \n%timeit magic command, 85   \ninyLlama, 367, 385   \nokenization-free encoding, 45   \nokens and tokenizers, 33, 37-61 bag-of-words model, 6 comparing trained tokenizers, 46-54 BERT base model (cased), 48 BERT base model (uncased), 47 Flan-T5, 50 Galactica, 52 GPT-2, 49 GPT-4, 50 Phi-3 and Llama 2, 53 StarCoder2, 51 decoding strategy, 79-81 downloading and running LLMs, 39-42 forward pass, 76-77 input preparation, 38 masked language modeling, 129 parallel token processing and context size, 81-83 special tokens, 47 task-specific representation model, 117, 127 text breakdown, 43 text-focused versus code-focused models, 56 token embeddings, 57-61, 77, 106 creating contextualized word embed‐ dings, 58-61 tokenizer’s vocabulary and, 57 token spans, 129 tokenization schemes, 44-46 byte tokens, 45 character tokens, 45 subword tokens, 44 word tokens, 44 tokenizer properties, 55-56 datasets, 56 methods, 55 parameters, 55 white space characters, 50   \none of voice, in text-generation prompts, 178   \nopic modeling, 138, 146-163 BERTopic, 148-155 representation blocks, 156-163   \nop_k parameter, 172   \nop_p parameter, 171, 188   \nrain splits, 113   \nrainingArguments class, 327   \ntransfer learning, 19   \nTransformer architecture, 15-18, 73-107 attention layer, 79, 86 decoding strategy, 79-81 feedforward layer, 86 forward pass components, 76-79 inputs and outputs of, 74-76 keys and values cache, 83-85 optimizing attention, 98 parallel token processing and context size, 81-83 recent improvements to, 95-105 more efficient attention, 96-100 positional embeddings, 102-104 Transformer blocks, 101 Transformer blocks, 85-94 attention calculation, 91-93 attention layer, 88 attention mechanism, 89-91 feedforward neural networks, 87 self-attention and relevance scoring, 93-94 Vision Transformer, 260-262   \ntransparency and accountability, 28   \ntree-of-thought, 189-191   \nTruthfulQA, 374, 376   \nTSDAE (Transformer-Based Sequential Denois‐ ing Auto-Encoder) for domain adaptation, 320 overview of, 316-320 ",
        "page_idx": 422
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 423
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 423
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "U ",
        "text_level": 1,
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "UltraChat dataset, 368   \nUMAP (Uniform Manifold Approximation and   \nProjection), 141   \nunigram language model, 50   \nunk_token [UNK], 47   \nuse_cache parameter, 84 ",
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "V ",
        "text_level": 1,
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "valid output, verifying, 192 validation splits, 113 vector databases dense retrieval, 229 ",
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "nearest neighbor search versus, 238 retrieval-augmented generation, 253   \nvideo random-access memory (VRAM), xiv, 29   \nvisualization BERTopic, 155 cluster analysis, 144 dimensionality reduction and, 145   \nViT (Vision Transformer), 260-262, 274   \nvocabulary, of tokenizers, 55, 57, 77, 106   \nVRAM (video random-access memory), xiv, 29 ",
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "W ",
        "text_level": 1,
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "warmup_ratio parameter, 387   \nwarmup_steps argument, 299   \nWeaviate, 239   \nwhitespace characters, 50   \nwindowed conversation buffer memory, 212-214, 217   \nword embeddings, 63-67 pretrained, 63 word2vec algorithm and contrastive train‐ ing, 64-67   \nword tokens, 44   \nword-level metrics, in generative model evalua‐ tion, 374   \nword2vec algorithm, 8, 10-12 contrastive training and, 64-67, 293 embedding songs, 67   \nWordPiece, 43 cased BERT base model, 48 uncased BERT base model, 47   \n<work> token, 53 ",
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "Y Year of Generative AI, 23-24 ",
        "text_level": 1,
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "Z   \nzero-shot classification, 123-126 CLIP, 265 SetFit, 339   \nzero-shot prompting chain-of-thought, 187 in-context learning, 181 ",
        "page_idx": 424
    },
    {
        "type": "text",
        "text": "About the Authors ",
        "text_level": 1,
        "page_idx": 425
    },
    {
        "type": "text",
        "text": "Jay Alammar is Director and Engineering Fellow at Cohere (pioneering provider of large language models as an API). In this role, he advises and educates enterprises and the developer community on using language models for practical use cases. Through his popular AI/ML blog, Jay has helped millions of researchers and engi‐ neers visually understand machine learning tools and concepts from the basic (end‐ ing up in the documentation of packages like NumPy and pandas) to the cutting-edge (Transformers, BERT, GPT-3, Stable Diffusion). Jay is also a co-creator of popular machine learning and natural language processing courses on Deeplearning.ai and Udacity. ",
        "page_idx": 425
    },
    {
        "type": "text",
        "text": "Maarten Grootendorst is a Senior Clinical Data Scientist at IKNL (Netherlands Comprehensive Cancer Organization). He holds master’s degrees in organizational psychology, clinical psychology, and data science, which he leverages to communicate complex machine learning concepts to a wide audience. With his popular blogs, he has reached millions of readers by explaining the fundamentals of artificial intelli‐ gence—often from a psychological point of view. He is the author and maintainer of several open source packages that rely on the strength of large language models, such as BERTopic, PolyFuzz, and KeyBERT. His packages are downloaded millions of times and used by data professionals and organizations worldwide. ",
        "page_idx": 425
    },
    {
        "type": "text",
        "text": "Colophon ",
        "text_level": 1,
        "page_idx": 426
    },
    {
        "type": "text",
        "text": "The animal on the cover of Hands-On Large Language Models is a red kangaroo (Osphranter rufus). They are the largest of all kangaroos, with a body length that can get up to a little over 5 feet and a tail as long as 3 feet. They are very fast and can hop to speeds over 35 miles per hour. They can jump 6 feet high and leap a distance of 25 feet in a single bound. The position of their eyes allows them see up to 300 degrees. ",
        "page_idx": 426
    },
    {
        "type": "text",
        "text": "Red kangaroos are named after the color of their fur. While the name makes sense for the males—they have short, red-brown fur—females are typically more of a blue-grey color with a tinge of brown throughout. The red color in their fur comes from a red oil excreted from the glands in their skin. Because of their color, Australians refer to male red kangaroos as “big reds.” However, because females are faster than males, they are often called “blue fliers.” ",
        "page_idx": 426
    },
    {
        "type": "text",
        "text": "Preferring open, dry areas with some trees for shade, red kangaroos can be found across Australia’s mainland except in the upper north, lower southwest, and east coast regions of the country. Surrounding environmental conditions can affect repro‐ duction. Because of this, females can pause or postpone pregnancy or birth until conditions are better. They often use this ability to delay birth of a new baby (joey) until the previous one has left their pouch. ",
        "page_idx": 426
    },
    {
        "type": "text",
        "text": "The cover illustration is by Karen Montgomery, based on an antique line engraving from Cassell’s Popular Natural History. The series design is by Edie Freedman, Ellie Volckhausen, and Karen Montgomery. The cover fonts are Gilroy Semibold and Guardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono. ",
        "page_idx": 426
    },
    {
        "type": "text",
        "text": "O'REILLY? ",
        "page_idx": 427
    },
    {
        "type": "text",
        "text": "Learn from experts. Become one yourself. ",
        "text_level": 1,
        "page_idx": 427
    },
    {
        "type": "text",
        "text": "Books | Live online courses Instant answers | Virtual events Videos Interactive learning ",
        "page_idx": 427
    },
    {
        "type": "text",
        "text": "Get started at oreilly.com. ",
        "page_idx": 427
    }
]