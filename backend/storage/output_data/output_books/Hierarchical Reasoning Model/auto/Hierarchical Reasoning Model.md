# Hierarchical Reasoning Model

Guan Wang1,†, Jin $\mathrm { L i ^ { 1 } }$ , Yuhao $\operatorname { S u n } ^ { 1 }$ , Xing Chen1, Changling Liu1, Yue ${ \mathbf { W } } { \mathbf { u } } ^ { 1 }$ , Meng ${ \mathrm { L u } } ^ { 1 , \dagger }$ , Sen Song2,†, Yasin Abbasi Yadkori1,†

1Sapient Intelligence, Singapore

# Abstract

Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM’s potential as a transformative advancement toward universal computation and general-purpose reasoning systems.

![## Image Analysis: a8640cb65dc58e81d19ad3d6c0a032830820e405750cc0af79298f406f520e70.jpg

**Conceptual Understanding:**
This image primarily represents the conceptual architecture and empirical validation of a novel AI model called the Hierarchical Recurrent Machine (HRM). Conceptually, it illustrates that the HRM's design is deeply inspired by hierarchical processing and temporal separation observed in the brain, specifically 'Cross Frequency Coupling' between different neural oscillation frequencies (theta and gamma waves) associated with distinct levels of representation. The main purpose of the image is to demonstrate that by mimicking these biological principles, the HRM achieves significantly superior performance on complex inductive reasoning tasks (ARC-AGI benchmarks) and challenging symbolic tree-search puzzles (Sudoku-Extreme, Maze-Hard) compared to existing state-of-the-art models, including those utilizing chain-of-thought or direct prediction approaches. The key idea conveyed is that a multi-timescale, hierarchically organized recurrent network can offer a powerful and robust solution for problems where other advanced models fail or perform poorly, especially when learning from limited data.

**Content Interpretation:**
The image details the architecture and performance of a Hierarchical Recurrent Machine (HRM). The architecture is inspired by biological 'Cross Frequency Coupling' in the brain, which involves distinct 'Meta-representation' (theta wave 4-8Hz, slower processing) and 'Lower-level representation' (gamma wave 40Hz, faster processing). The HRM mirrors this with 'High-level' (Slower) and 'Low-level' (Faster) recurrent networks that interact bidirectionally, processing input and generating output via an 'Update' mechanism. The multi-timescale processing is central to its design.

The bar charts illustrate HRM's superior performance across various tasks: ARC-AGI-1 and ARC-AGI-2 (inductive benchmarks), and Sudoku-Extreme (9x9) and Maze-Hard (30x30) (challenging symbolic tree-search puzzles). The HRM consistently achieves higher accuracy than 'Chain-of-thought, pretrained' models (Deepseek R1, Claude 3.7 8K, o3-mini-high) and 'Direct prediction, small-sample learning' models ('Direct pred'). Notably, on Sudoku-Extreme and Maze-Hard, where other models completely fail (0.0% accuracy), the HRM achieves significant success (55.0% and 74.5% respectively). The number of training examples (960-1120) is provided for each benchmark. This demonstrates HRM's robustness and effectiveness in complex, inductive, and symbolic reasoning tasks, particularly in scenarios where other advanced models struggle or fail entirely, and its efficiency with a relatively small number of training examples.

**Key Insights:**
1.  **Bio-Inspired Design:** The HRM's architecture is explicitly inspired by biological brain mechanisms, specifically 'Cross Frequency Coupling' between 'Meta-representation' (theta waves, 4-8Hz) and 'Lower-level representation' (gamma waves, 40Hz), indicating the potential of biologically plausible models for AI advancement.
2.  **Multi-Timescale Processing Advantage:** The HRM effectively utilizes 'High-level' (Slower) and 'Low-level' (Faster) recurrent networks. This hierarchical, multi-timescale processing enables the model to handle complex tasks more effectively than single-timescale or non-hierarchical approaches.
3.  **Superior Performance on Complex Tasks:** HRM significantly outperforms state-of-the-art 'Chain-of-thought, pretrained' and 'Direct prediction, small-sample learning' models across diverse benchmarks. Its accuracy of 40.3% on ARC-AGI-1 and 5.0% on ARC-AGI-2 demonstrates strong inductive reasoning capabilities.
4.  **Robustness in Extreme Cases:** A key insight is HRM's ability to solve tasks like 'Sudoku-Extreme (9x9)' (55.0% accuracy) and 'Maze-Hard (30x30)' (74.5% accuracy) where all other tested models achieve 0.0% accuracy. This highlights HRM's unique capacity for challenging symbolic tree-search puzzles and suggests a fundamental difference in its problem-solving approach.
5.  **Efficiency with Limited Data:** The consistently high performance of HRM with a relatively small number of training examples (960-1120 examples per task) suggests that it is data-efficient and capable of learning complex representations and reasoning from limited exposure.

**Document Context:**
The image directly supports the document's abstract by visually explaining the Hierarchical Recurrent Machine's (HRM) conceptual basis and empirically validating its superior performance. The left side of the image illustrates the 'Cross Frequency Coupling' in the brain, showing 'Meta-representation' (theta wave 4-8Hz) and 'Lower-level representation' (gamma wave 40Hz), which is explicitly stated in the abstract as the inspiration for HRM's 'hierarchical processing and temporal separation'. The central diagram of the HRM, with its 'High-level' (Slower) and 'Low-level' (Faster) recurrent networks, directly demonstrates the abstract's claim of 'two recurrent networks operating at different timescales to collaboratively solve tasks'.

The bar charts on the right provide the crucial evidence for the abstract's statement that HRM 'surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (Sudoku-Extreme, Maze-Hard) where CoT models failed completely'. The consistent and often dramatic outperformance of HRM (e.g., 40.3% vs 34.5% on ARC-AGI-1, 5.0% vs 3.0% on ARC-AGI-2, and 55.0% vs 0.0% on Sudoku-Extreme, 74.5% vs 0.0% on Maze-Hard) directly substantiates these claims. Furthermore, the mention of '1000 training examples' reinforces the idea of effective learning with limited data, aligning with the abstract's point about solving tasks directly from inputs without chain of thoughts and with only about 1000 training examples.

**Summary:**
The image illustrates the concept and performance of a Hierarchical Recurrent Machine (HRM), drawing inspiration from brain function. On the left, 'Cross Frequency Coupling' in the brain is depicted, showing a 'Meta-representation' associated with 'theta wave 4-8Hz' influencing a 'Lower-level representation' associated with 'gamma wave 40Hz'. A large bidirectional arrow connects this biological inspiration to the 'HRM' architecture in the center. The HRM consists of two interconnected recurrent networks: a 'Low-level' network labeled 'Faster' that receives 'Input', and a 'High-level' network labeled 'Slower' that produces 'Output'. Both levels have internal recurrent loops, and interact bidirectionally. A legend 'O = Update' specifies a mechanism within the HRM. The right side of the image presents four bar charts comparing the HRM's accuracy against several 'Chain-of-thought, pretrained' models (Deepseek R1, Claude 3.7 8K, o3-mini-high) and a 'Direct prediction, small-sample learning' model ('Direct pred').

For 'ARC-AGI-1' (960 training examples), the HRM achieves 40.3% accuracy, outperforming o3-mini-high (34.5%), Claude 3.7 8K (21.2%), Direct pred (21.0%), and Deepseek R1 (15.8%).

For 'ARC-AGI-2' (1120 training examples), the HRM scores 5.0%, which is significantly higher than o3-mini-high (3.0%), Deepseek R1 (1.3%), Claude 3.7 8K (0.9%), and Direct pred (0.0%).

On 'Sudoku-Extreme (9x9)' (1000 training examples), the HRM achieves 55.0% accuracy, while all other models (Direct pred, o3-mini-high, Claude 3.7 8K, Deepseek R1) score 0.0%.

Similarly, for 'Maze-Hard (30x30)' (1000 training examples), the HRM achieves 74.5% accuracy, with all other compared models again scoring 0.0%. The y-axis for the ARC-AGI charts is 'Accuracy %'. The distinct bar colors represent: gray for 'Chain-of-thought, pretrained', yellow for 'Direct prediction, small-sample learning', and blue for 'HRM'.](images/a8640cb65dc58e81d19ad3d6c0a032830820e405750cc0af79298f406f520e70.jpg)
Figure 1: Left: HRM is inspired by hierarchical processing and temporal separation in the brain. It has two recurrent networks operating at different timescales to collaboratively solve tasks. Right: With only about 1000 training examples, the HRM ( $\mathrm { \sim } 2 7 \mathrm { M }$ parameters) surpasses state-of-the-art CoT models on inductive benchmarks (ARC-AGI) and challenging symbolic tree-search puzzles (Sudoku-Extreme, Maze-Hard) where CoT models failed completely. The HRM was randomly initialized, and it solved the tasks directly from inputs without chain of thoughts.

# 1 Introduction

Deep learning, as its name suggests, emerged from the idea of stacking more layers to achieve increased representation power and improved performance1,2. However, despite the remarkable success of large language models, their core architecture is paradoxically shallow3. This imposes a fundamental constraint on their most sought-after capability: reasoning. The fixed depth of standard Transformers places them in computational complexity classes such as $A C ^ { 0 }$ or $T C ^ { 0 4 }$ , preventing them from solving problems that require polynomial time5,6. LLMs are not Turing-complete and thus they cannot, at least in a purely end-to-end manner, execute complex algorithmic reasoning that is necessary for deliberate planning or symbolic manipulation tasks 7,8. For example, our results on the Sudoku task show that increasing Transformer model depth can improve performance,1 but performance remains far from optimal even with very deep models (see Figure 2), which supports the conjectured limitations of the LLM scaling paradigm9.

The LLMs literature has relied largely on Chain-of-Thought (CoT) prompting for reasoning10 CoT externalizes reasoning into token-level language by breaking down complex tasks into simpler intermediate steps, sequentially generating text using a shallow model11. However, CoT for reasoning is a crutch, not a satisfactory solution. It relies on brittle, human-defined decompositions where a single misstep or a misorder of the steps can derail the reasoning process entirely12,13. This dependency on explicit linguistic steps tethers reasoning to patterns at the token level. As a result, CoT reasoning often requires significant amount of training data and generates large number of tokens for complex reasoning tasks, resulting in slow response times. A more efficient approach is needed to minimize these data requirements14

Towards this goal, we explore “latent reasoning”, where the model conducts computations within its internal hidden state space15,16. This aligns with the understanding that language is a tool for human communication, not the substrate of thought itself17; the brain sustains lengthy, coherent chains of reasoning with remarkable efficiency in a latent space, without constant translation back to language. However, the power of latent reasoning is still fundamentally constrained by a model’s effective computational depth. Naively stacking layers is notoriously difficult due to vanishing gradients, which plague training stability and effectiveness 1,18. Recurrent architectures, a natural alternative for sequential tasks, often suffer from early convergence, rendering subsequent computational steps inert, and rely on the biologically implausible, computationally expensive and memory intensive Backpropagation Through Time (BPTT) for training19.

The human brain provides a compelling blueprint for achieving the effective computational depth that contemporary artificial models lack. It organizes computation hierarchically across cortical regions operating at different timescales, enabling deep, multi-stage reasoning20,21,22. Recurrent feedback loops iteratively refine internal representations, allowing slow, higher-level areas to guide—and fast, lower-level circuits to execute—subordinate processing while preserving global coherence23,24,25. Notably, the brain achieves such depth without incurring the prohibitive creditassignment costs that typically hamper recurrent networks from backpropagation through time19,26.

Inspired by this hierarchical and multi-timescale biological architecture, we propose the Hierarchical Reasoning Model (HRM). HRM is designed to significantly increase the effective computational depth. It features two coupled recurrent modules: a high-level (H) module for abstract, deliberate reasoning, and a low-level (L) module for fast, detailed computations. This structure avoids the rapid convergence of standard recurrent models through a process we term “hierarchical convergence.” The slow-updating H-module advances only after the fast-updating L-module has completed multiple computational steps and reached a local equilibrium, at which point the L-module is reset to begin a new computational phase.

![## Image Analysis: 27fa33a6a9339b5d97c0c56ab160a2729ff1e77e19a1e589d57d6ebb64fde0d0.jpg

**Conceptual Understanding:**
This image conceptually illustrates the relationship between neural network architectural design choices (specifically, scaling width vs. depth) and model performance (accuracy) on a task requiring complex reasoning, such as Sudoku-Extreme Full. It also compares the efficacy of different Transformer-based models, including a novel 'HRM' architecture, in leveraging computational depth.

The main purpose of the image is to empirically demonstrate two key points:
1.  For complex reasoning tasks, increasing the depth of a model is a far more effective strategy for improving accuracy than simply increasing its width.
2.  Standard Transformer architectures suffer from performance saturation as depth increases, meaning they cannot fully utilize additional computational depth to achieve higher accuracy. In contrast, the Hybrid Recurrent Model (HRM) is designed to overcome this limitation, effectively using increased depth to achieve near-perfect accuracy.

**Content Interpretation:**
The image conceptually represents an empirical study on the architectural design principles for neural networks, specifically Transformers and a novel architecture called HRM, when applied to tasks requiring complex reasoning, as indicated by the document context referencing 'Sudoku-Extreme Full'.

The left graph (`Scaling Width - 8 layers fixed` vs. `Scaling Depth - 512 hidden size fixed`) illustrates how different scaling strategies (increasing model width versus increasing model depth) impact accuracy as the total number of parameters increases. It directly compares the effectiveness of adding more neurons per layer (width) against adding more layers (depth) for a given computational budget (parameters).

The right graph (`Transformer` vs. `Recurrent Transformer` vs. `HRM`) demonstrates the performance of three distinct model architectures as their computational depth (number of Transformer layers computed) is increased. It highlights how different model designs leverage increased depth, particularly contrasting standard Transformer limitations with the capabilities of the Hybrid Recurrent Model (HRM).

Together, the graphs convey the central message that for tasks demanding complex reasoning, architectural depth is a far more critical factor for achieving high accuracy than width, and that specialized architectures like HRM are better equipped to exploit this depth effectively than traditional Transformers, which tend to saturate.

**Key Insights:**
The image provides several key takeaways and insights, supported by the specific text and data presented in the graphs:

1.  **Depth is critical for complex reasoning, while width is not.**
    *   **Evidence:** The left graph clearly shows that the 'Scaling Depth - 512 hidden size fixed' strategy leads to significant accuracy gains (reaching ~67% accuracy), whereas 'Scaling Width - 8 layers fixed' results in negligible accuracy improvement (staying below 20%) even with a substantial increase in parameters. This highlights that simply making a model wider with a shallow depth (8 layers) is ineffective for the type of complex reasoning required by Sudoku-Extreme Full, while increasing depth (with fixed hidden size) is essential.

2.  **Standard Transformer architectures exhibit performance saturation with increasing depth.**
    *   **Evidence:** In the right graph, both the 'Transformer' and 'Recurrent Transformer' lines show that after an initial increase, their accuracy plateaus or even slightly decreases at higher depths (beyond 128 layers). The 'Transformer' peaks around 67% and the 'Recurrent Transformer' around 81%, but neither continues to improve substantially with further increases in 'Depth / Transformer layers computed'. This indicates that these architectures hit a performance ceiling, failing to fully utilize additional computational depth for better results.

3.  **The HRM architecture effectively overcomes depth-related saturation, achieving superior and near-perfect accuracy.**
    *   **Evidence:** The 'HRM' line in the right graph stands out. Unlike the other two models, HRM's accuracy continues to rise sharply with increasing depth, starting above 90% at 128 layers and reaching nearly 100% (approaching the dashed 100% accuracy line) at 512 layers. This demonstrates HRM's ability to leverage greater computational depth to consistently improve performance, ultimately achieving significantly higher accuracy than both the 'Transformer' and 'Recurrent Transformer' models, which saturate at lower accuracy levels.

**Document Context:**
This image serves as crucial visual evidence for the claims made in the document's introduction, specifically in the '1 Introduction' section and the accompanying text. The document text explicitly states, 'The necessity of depth for complex reasoning. Left: On Sudoku-Extreme Full, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. Right: Standard architectures saturates, failing to benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.'

The left graph directly supports the assertion that for Sudoku-Extreme Full, increasing a Transformer's width yields no performance gain, while increasing depth is critical. The 'Scaling Width - 8 layers fixed' line shows near-flat, low accuracy regardless of parameter count, whereas the 'Scaling Depth - 512 hidden size fixed' line demonstrates significant accuracy improvements with more parameters (i.e., deeper models).

The right graph directly validates the claim that 'Standard architectures saturates, failing to benefit from increased depth.' Both the 'Transformer' and 'Recurrent Transformer' lines show a plateau or even a slight decline in accuracy after a certain depth, indicating saturation. In contrast, the 'HRM' line's continuous upward trend, reaching near-perfect accuracy, provides strong evidence for the statement that 'HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.'

Thus, the image is indispensable for understanding the core argument that depth is essential for complex reasoning tasks and that HRM is a superior architecture in this context.

**Summary:**
The image presents two line graphs, side-by-side, illustrating the impact of model architecture parameters on accuracy, specifically for complex reasoning tasks like Sudoku-Extreme Full. The graphs collectively demonstrate that increasing model depth is crucial for performance, while increasing width has negligible effect, and introduce the Hybrid Recurrent Model (HRM) as an architecture that effectively leverages depth to achieve superior accuracy, unlike standard Transformers.

**Left Graph: Scaling Width vs. Scaling Depth**
This graph, titled implicitly by its legend, plots "Accuracy %" on the y-axis (ranging from 0 to 100) against "Parameters" on the x-axis (with values: 27M, 54M, 109M, 218M, 436M, 872M). It compares two scaling strategies:
1.  **"Scaling Width - 8 layers fixed" (gray line with circles):** This line shows that when the model's width is scaled while keeping the depth fixed at 8 layers, the accuracy remains consistently low, around 15-18%, across the entire range of parameters from 27M to 872M. There is no significant performance gain.
2.  **"Scaling Depth - 512 hidden size fixed" (dark blue line with circles):** In contrast, when the model's depth is scaled while the hidden size is fixed at 512, the accuracy significantly increases as parameters grow. It starts around 17% at 27M parameters, rises sharply to over 55% at 109M, reaches approximately 67% at 218M, and then plateaus, showing minor fluctuations but generally maintaining accuracy around 67% up to 872M parameters. This indicates that increasing depth is critical for accuracy improvements.

**Right Graph: Model Performance with Increasing Depth**
This graph also plots "Accuracy %" on the y-axis (ranging from 0 to 100) against "Depth / Transformer layers computed" on the x-axis (with values: 8, 16, 32, 64, 128, 256, 512). It compares the performance of three different model architectures:
1.  **"Transformer" (dark blue line with circles):** The accuracy of the Transformer model increases with depth, starting at around 17% at 8 layers, rising to about 57% at 32 layers, and reaching a peak of approximately 67% at 128 layers. Beyond this point, the accuracy plateaus, remaining around 67% even with increased depth to 256 and 512 layers.
2.  **"Recurrent Transformer" (yellow line with circles):** This model also shows increasing accuracy with depth, starting around 22% at 16 layers, quickly surpassing the Transformer to reach about 75% at 64 layers, and peaking at approximately 81% at 128 layers. However, its accuracy slightly declines to around 79% at 256 layers and 77% at 512 layers, indicating saturation or a slight performance drop with excessive depth.
3.  **"HRM" (light blue line with circles):** The Hybrid Recurrent Model (HRM) demonstrates a distinct trend. Its accuracy is already high at 128 layers (around 92%), and it continues to increase with depth, reaching approximately 98% at 256 layers and nearly 100% at 512 layers, approaching the dashed grey line that signifies 100% accuracy. This shows that HRM effectively utilizes increased computational depth without saturation.

Both graphs include a dashed grey horizontal line at 100% on the y-axis, representing the maximum possible accuracy.](images/27fa33a6a9339b5d97c0c56ab160a2729ff1e77e19a1e589d57d6ebb64fde0d0.jpg)
Figure 2: The necessity of depth for complex reasoning. Left: On Sudoku-Extreme Full, which require extensive tree-search and backtracking, increasing a Transformer’s width yields no performance gain, while increasing depth is critical. Right: Standard architectures saturates, failing to benefit from increased depth. HRM overcomes this fundamental limitation, effectively using its computational depth to achieve near-perfect accuracy.

Furthermore, we proposed a one-step gradient approximation for training HRM, which offers improved efficiency and eliminates the requirement for BPTT. This design maintains a constant memory footprint ${ \cal O } ( 1 )$ compared to BPTT’s $O ( T )$ for $T$ timesteps) throughout the backpropagation process, making it scalable and more biologically plausible.

Leveraging its enhanced effective depth, HRM excels at tasks that demand extensive search and backtracking. Using only 1,000 input-output examples, without pre-training or CoT supervision, HRM learns to solve problems that are intractable for even the most advanced LLMs. For example, it achieves near-perfect accuracy in complex Sudoku puzzles (Sudoku-Extreme Full) and optimal pathfinding in $3 0 \mathrm { x } 3 0$ mazes, where state-of-the-art CoT methods completely fail $0 \%$ accuracy). In the Abstraction and Reasoning Corpus (ARC) AGI Challenge 27,28,29 - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset $( \sim 1 0 0 0$ examples), with only 27M parameters and a $3 0 \mathrm { x } 3 0$ grid context (900 tokens), achieves a performance of $40 . 3 \%$ , which substantially surpasses leading CoT-based models like o3-mini-high $( 3 4 . 5 \% )$ and Claude $3 . 7 ~ 8 \mathrm { K }$ context $( 2 1 . 2 \% )$ , despite their considerably larger parameter sizes and context lengths, as shown in Figure 1. This represents a promising direction toward the development of next-generation AI reasoning systems with universal computational capabilities.

# 2 Hierarchical Reasoning Model

We present the HRM, inspired by three fundamental principles of neural computation observed in the brain:

• Hierarchical processing: The brain processes information across a hierarchy of cortical areas. Higher-level areas integrate information over longer timescales and form abstract representations, while lower-level areas handle more immediate, detailed sensory and motor processing 20,22,21.

• Temporal Separation: These hierarchical levels in the brain operate at distinct intrinsic timescales, reflected in neural rhythms (e.g., slow theta waves, $4 { - } 8 \ \mathrm { H z }$ and fast gamma waves, 30–100 $\mathrm { H z } ) ^ { 3 0 , 3 1 }$ . This separation allows for stable, high-level guidance of rapid, low-level computations 32,33

• Recurrent Connectivity: The brain features extensive recurrent connections. These feedback loops enable iterative refinement, yielding more accurate and context-sensitive representations at the cost of additional processing time. Additionally, the brain largely avoids the problematic deep credit assignment problem associated with BPTT19.

The HRM model consists of four learnable components: an input network $f _ { I } ( \cdot ; \theta _ { I } )$ , a low-level recurrent module $f _ { L } ( \cdot ; \theta _ { L } )$ , a high-level recurrent module $f _ { H } ( \cdot ; \theta _ { H } )$ , and an output network $f _ { O } ( \cdot ; \theta _ { O } )$ . The model’s dynamics unfold over $N$ high-level cycles of $T$ low-level timesteps each2. We index the total timesteps of one forward pass by $i = 1 , \ldots , N \times T$ . The modules $f _ { L }$ and $f _ { H }$ each keep a hidden state— $\cdot z _ { L } ^ { i }$ for $f _ { L }$ and $z _ { H } ^ { i }$ for $f _ { H }$ —which are initialized with the vectors $z _ { L } ^ { 0 }$ and $z _ { H } ^ { 0 }$ , respectively.

The HRM maps an input vector $x$ to an output prediction vector $\hat { y }$ as follows. First, the input $x$ is projected into a working representation $\tilde { x }$ by the input network:

$$
\tilde { x } = f _ { I } ( x ; \theta _ { I } ) ~ .
$$

At each timestep $i$ , the $\mathrm { L }$ -module updates its state conditioned on its own previous state, the Hmodule’s current state (which remains fixed throughout the cycle), and the input representation. The H-module only updates once per cycle (i.e., every $T$ timesteps) using the L-module’s final state at the end of that cycle:

$$
\begin{array} { r l } & { z _ { L } ^ { i } = f _ { L } \left( z _ { L } ^ { i - 1 } , z _ { H } ^ { i - 1 } , \tilde { x } ; \theta _ { L } \right) , } \\ & { z _ { H } ^ { i } = \left\{ \begin{array} { l l } { f _ { H } \left( z _ { H } ^ { i - 1 } , z _ { L } ^ { i - 1 } ; \theta _ { H } \right) } & { \mathrm { i f } i \equiv 0 \left( \mathrm { m o d } T \right) , } \\ { z _ { H } ^ { i - 1 } } & { \mathrm { o t h e r w i s e } . } \end{array} \right. } \end{array}
$$

Finally, after $N$ full cycles, a prediction $\hat { y }$ is extracted from the hidden state of the H-module:

$$
\hat { y } = f _ { O } ( z _ { H } ^ { N T } ; \theta _ { O } ) \mathrm { ~ . ~ }
$$

This entire $N T$ -timestep process represents a single forward pass of the HRM. A halting mechanism (detailed later in this section) determines whether the model should terminate, in which case $\hat { y }$ will be used as the final prediction, or continue with an additional forward pass.

Hierarchical convergence Although convergence is crucial for recurrent networks, standard RNNs are fundamentally limited by their tendency to converge too early. As the hidden state settles toward a fixed point, update magnitudes shrink, effectively stalling subsequent computation and capping the network’s effective depth. To preserve computational power, we actually want convergence to proceed very slowly–but engineering that gradual approach is difficult, since pushing convergence too far edges the system toward instability.

![## Image Analysis: df6fbab0d13e494c74d724e8cee7bf600ce2fcb41c2381c0036d50c688c00be7.jpg

**Conceptual Understanding:**
This image represents a comparative analysis of the training or processing dynamics of three distinct neural network architectures: a Hierarchical Reasoning Model (HRM), a Recurrent Neural Network, and a Deep Neural Network. Conceptually, it illustrates how these different models handle errors (forward residuals) over time or network depth, and how their internal states evolve in a reduced-dimensional space (Principal Components).

The main purpose of the image is to highlight the differences in convergence behavior and internal representations among these models. It aims to demonstrate the hierarchical convergence characteristic of the HRM (with its high-level and low-level modules), the efficient and rapid convergence of Recurrent Neural Networks, and the challenges (like vanishing gradients) encountered in Deep Neural Networks, particularly at their extreme layers. The image serves to provide visual evidence supporting a discussion on the strengths and weaknesses of each architecture in terms of learning and information propagation.

**Content Interpretation:**
The image displays a comparative analysis of the training dynamics (forward residuals) and internal state evolution (Principal Component Analysis trajectories) for three neural network architectures: a Hierarchical Reasoning Model (HRM), a Recurrent Neural Network, and a Deep Neural Network.

**Forward Residuals (Top Row):**
*   The **HRM** (left plot) shows two components: "HRM H" with a steadily decreasing forward residual, and "HRM L" with a characteristic "sawtooth" pattern, indicating repeated convergence cycles followed by resets and residual spikes. The Y-axis represents "Forward residual" (values 0-250) and the X-axis is "Step Index #" (values 0-60).
*   The **Recurrent Neural Net** (middle plot) demonstrates a rapid reduction of forward residual, quickly approaching zero. The Y-axis represents "Forward residual" (values 0-250) and the X-axis is "Step Index #" (values 0-60).
*   The **Deep Neural Net** (right plot) exhibits significant residuals at the initial (near Layer 0, around 120) and final layers (sharp increase after Layer Index # 200, reaching 250), with relatively lower and fluctuating residuals in between. The Y-axis represents "Forward residual" (values 0-250) and the X-axis is "Layer Index #" (values 0-200).

**Principal Components Trajectories (Bottom Row):**
*   The **HRM** (left plot) shows two distinct trajectories in the "Principal Components" space, with the "HRM L" path exhibiting more oscillatory or cyclic behavior, colored by "Step Index #" (30-60).
*   The **Recurrent Neural Net** (middle plot) displays a smooth, converging trajectory in the "Principal Components" space, colored by "Step Index #" (30-60).
*   The **Deep Neural Net** (right plot) shows a single, curved trajectory in the "Principal Components" space, colored by "Layer Index #" (100-200), reflecting the transformation of representations across layers and possibly correlating with the observed residual issues.

**Key Insights:**
**1. Hierarchical Convergence in HRM:** The Hierarchical Reasoning Model (HRM) exhibits a unique two-tiered convergence. The "HRM H" module shows steady convergence (evidenced by the gradually decreasing orange line in the top-left 'Forward residual' plot), while the "HRM L" module repeatedly converges and then resets, causing characteristic 'sawtooth' residual spikes (evidenced by the blue line's pattern in the top-left plot). This is also reflected in the distinct, more cyclic PCA trajectory for HRM L (bottom-left plot).

**2. Rapid and Efficient Convergence of Recurrent Neural Networks:** Recurrent Neural Networks demonstrate quick and effective learning, with their 'Forward residual' rapidly approaching zero (evidenced by the sharp drop and flat line of the yellow curve in the top-middle plot) and exhibiting a smooth, converging trajectory in the Principal Components space (bottom-middle plot).

**3. Gradient Challenges in Deep Neural Networks:** Deep Neural Networks are susceptible to issues like vanishing or exploding gradients, particularly in their deeper layers. This is indicated by the significant 'Forward residual' spikes at the initial layers and the sharp increase in residuals in the final layers (evidenced by the dark blue line in the top-right plot, especially after 'Layer Index #' 200), as well as a distinct, perhaps more spread-out, trajectory in PCA space (bottom-right plot).

**4. Distinct Model Dynamics through PCA:** The Principal Component Analysis (PCA) trajectories provide a visual means to differentiate the internal processing and representational changes within each model type. Each architecture, HRM, Recurrent Neural Net, and Deep Neural Net, displays a unique path in the principal component space, reflecting their characteristic operational dynamics over steps or layers.

**Document Context:**
This image directly supports Section "2 Hierarchical Reasoning Model" by providing visual and empirical evidence for the discussion on the training dynamics of different neural network architectures. It specifically illustrates the unique hierarchical convergence pattern of the HRM, contrasting it with the rapid convergence of Recurrent Neural Networks and the vanishing/exploding gradient issues observed in Deep Neural Networks. The comparison helps to highlight the distinct characteristics and potential advantages of the HRM architecture within the broader context of neural network training and performance.

**Summary:**
This image comprises six distinct plots, arranged in two rows of three, offering a detailed comparison of three neural network architectures: a Hierarchical Reasoning Model (HRM), a Recurrent Neural Net, and a Deep Neural Net. The top row illustrates the "Forward residual" (error) over computation steps or layers, while the bottom row visualizes the models' internal state changes in a "Principal Components" space.

**Top Row: Forward Residuals**
*   **Left Plot (HRM H and HRM L):** This plot shows the "Forward residual" against "Step Index #" for the Hierarchical Reasoning Model. Two lines are depicted: "HRM H" (orange) and "HRM L" (blue). The "HRM H" residual demonstrates a steady, gradual decrease from around 200 to below 50 over 60 steps, indicating consistent convergence of the high-level module. In stark contrast, the "HRM L" residual exhibits a distinctive "sawtooth" pattern. It repeatedly drops sharply (converging) from peaks (e.g., over 200) to very low values (e.g., near 20), only to spike back up (resetting) before resuming its convergence. This visually confirms the hierarchical convergence behavior, where the low-level module rapidly converges in cycles and is then reset by the high-level module, leading to these characteristic spikes.
*   **Middle Plot (Recurrent Neural Net):** This plot displays the "Forward residual" against "Step Index #" for a "Recurrent Neural Net." A single yellow line shows a rapid and smooth convergence. The residual quickly drops from an initial value near 170 to almost zero within approximately 20 "Step Index #" units and remains stable at a very low level for the remaining steps. This signifies efficient and fast convergence for this network type.
*   **Right Plot (Deep Neural Net):** This plot illustrates the "Forward residual" against "Layer Index #" for a "Deep Neural Net." The dark blue line shows an initial high residual spike (over 100) near Layer 0, followed by fluctuating, relatively low residuals (mostly between 0 and 50) up to around "Layer Index #" 200. However, after Layer 200, the residual sharply increases, rising steeply to nearly 250 by "Layer Index #" 250. This pattern suggests issues such as vanishing or exploding gradients, primarily impacting the initial and final layers of the deep network.

**Bottom Row: Principal Components Trajectories**
*   These three plots visualize how the internal states of each model evolve over time or layers in a two-dimensional "Principal Components" space, providing insight into their learning or processing dynamics. The lines are colored to represent the progression of "Step Index #" or "Layer Index #."
*   **Left Plot (HRM H and HRM L):** This plot shows two distinct trajectories. The orange-colored path (corresponding to "HRM H") depicts a relatively direct movement, while the blue-colored path (corresponding to "HRM L"), which is colored by "Step Index #" from 30 to 60, shows a more complex, somewhat oscillatory or looped trajectory. This reinforces the idea of distinct and perhaps cyclic internal dynamics for the HRM's low-level module.
*   **Middle Plot (Recurrent Neural Net):** This plot shows a single, smooth yellow trajectory, colored by "Step Index #" from 30 to 60. The path starts from one region and converges smoothly towards another, indicating a stable and continuous evolution of the network's internal state.
*   **Right Plot (Deep Neural Net):** This plot shows a single, dark blue trajectory, colored by "Layer Index #" from 100 to 200. The trajectory forms a distinct curve that extends and potentially diverges, especially at later "Layer Index #" values. This visualizes the transformation of representations across the layers of the deep neural network, and its complex shape may correlate with the challenges observed in the residual plot for the Deep Neural Net.

In summary, the image provides a comprehensive visual comparison, using both quantitative residual data and qualitative state-space trajectories, to demonstrate the unique hierarchical convergence of HRM, the efficient learning of RNNs, and the common gradient challenges faced by deep DNNs.](images/df6fbab0d13e494c74d724e8cee7bf600ce2fcb41c2381c0036d50c688c00be7.jpg)
Figure 3: Comparison of forward residuals and PCA trajectories. HRM shows hierarchical convergence: the H-module steadily converges, while the L-module repeatedly converges within cycles before being reset by H, resulting in residual spikes. The recurrent neural network exhibits rapid convergence with residuals quickly approaching zero. In contrast, the deep neural network experiences vanishing gradients, with significant residuals primarily in the initial (input) and final layers.

HRM is explicitly designed to counteract this premature convergence through a process we term hierarchical convergence. During each cycle, the L-module (an RNN) exhibits stable convergence to a local equilibrium. This equilibrium, however, depends on the high-level state $z _ { H }$ supplied during that cycle. After completing the $T$ steps, the H-module incorporates the sub-computation’s outcome (the final state $z _ { L }$ ) and performs its own update. This $z _ { H }$ update establishes a fresh context for the L-module, essentially “restarting” its computational path and initiating a new convergence phase toward a different local equilibrium.

This process allows the HRM to perform a sequence of distinct, stable, nested computations, where the H-module directs the overall problem-solving strategy and the L-module executes the intensive search or refinement required for each step. Although a standard RNN may approach convergence within $T$ iterations, the hierarchical convergence benefits from an enhanced effective depth of $N T$ steps. As empirically shown in Figure 3, this mechanism allows HRM both to maintain high computational activity (forward residual) over many steps (in contrast to a standard RNN, whose activity rapidly decays) and to enjoy stable convergence. This translates into better performance at any computation depth, as illustrated in Figure 2.

Approximate gradient Recurrent models typically use BPTT to compute gradients. However, BPTT requires storing the hidden states from the forward pass and then combining them with gradients during the backward pass, which demands $O ( T )$ memory for $\mathrm { T }$ timesteps. This heavy memory burden forces smaller batch sizes and leads to poor GPU utilization, especially for largescale networks. Additionally, because retaining the full history trace through time is biologically implausible, it is unlikely that the brain implements BPTT19.

Fortunately, if a recurrent neural network converges to a fixed point, we can avoid unrolling its state sequence by applying backpropagation in a single step at that equilibrium point. Moreover, such a mechanism could plausibly be implemented in the brain using only local learning rules34,35. Based on this finding, we propose a one-step approximation of the HRM gradient–using the gradient of the last state of each module and treating other states as constant. The gradient path is, therefore,

Output head final state of the H-module final state of the L-module input embedding

The above method needs $O ( 1 )$ memory, does not require unrolling through time, and can be easily implemented with an autograd framework such as PyTorch, as shown in Figure 4. Given that each module only needs to back-propagate errors through its most recent local synaptic activity, this approach aligns well with the perspective that cortical credit assignment relies on short-range, temporally local mechanisms rather than on a global replay of activity patterns.

The one-step gradient approximation is theoretically grounded in the mathematics of Deep Equilibrium Models $( \mathrm { D E Q } ) ^ { 3 6 }$ which employs the Implicit Function Theorem (IFT) to bypass BPTT, as detailed next. Consider an idealized HRM behavior where, during high-level cycle $k$ , the L-module repeatedly updates until its state $z _ { L }$ converges to a local fixed point $z _ { L } ^ { \star }$ . This fixed point, given the current high-level state $z _ { H } ^ { k - 1 }$ , can be expressed as

$$
z _ { L } ^ { \star } = f _ { L } ( z _ { L } ^ { \star } , z _ { H } ^ { k - 1 } , \tilde { x } ; \theta _ { L } ) .
$$

The $_ \mathrm { H }$ -module then performs a single update using this converged L-state:

$$
z _ { H } ^ { k } = f _ { H } ( z _ { H } ^ { k - 1 } , z _ { L } ^ { \star } ; \theta _ { H } ) \ .
$$

With a proper mapping $\mathcal { F }$ , the updates to the high-level state can be written in a more compact form as $z _ { H } ^ { k } ~ =$ $\mathcal { F } ( z _ { H } ^ { k - 1 } ; \tilde { x } , \theta )$ , where $\theta \ : = \ : \left( \theta _ { I } , \theta _ { L } \right)$ , and the fixed-point can be written as $z _ { H } ^ { \star } = \mathcal { F } ( z _ { H } ^ { \star } ; \tilde { x } , \theta )$ . Let $\begin{array} { r } { J _ { \mathcal { F } } = \frac { \partial \bar { \mathcal { F } } } { \partial z _ { H } } } \end{array}$ be the Jacobian of $\mathcal { F }$ , and assume that the matrix $I - \dot { J } _ { \mathcal { F } }$ is invertible at $z _ { H } ^ { \star }$ and that the mapping $\mathcal { F }$ is continuously differentiable. The Implicit Function Theorem then allows us to calculate the exact gradient of fixed point $z _ { H } ^ { \star }$ with respect to the parameters $\theta$ without explicit backpropagation:

![## Image Analysis: 5508819029fb7775970238f308baebffec7d24cc5e537b4ccd458410adbf00b7.jpg

**Conceptual Understanding:**
The image conceptually represents a hierarchical recurrent neural network architecture designed for sequential data processing and reasoning. Its main purpose is to illustrate how information flows through different levels of abstraction (low-level and high-level modules) over time, and crucially, to highlight a selective gradient computation strategy during its training. The core idea communicated is that not all parts of the recurrent structure are equally involved in gradient-based learning, suggesting an optimized or specialized learning mechanism within this hierarchical framework.

**Content Interpretation:**
The image illustrates a hierarchical recurrent neural network, characterized by its "L Module" (Low-level) and "H Module" (High-level) components, which interact across time steps as indicated by "Unrolling over time." This hierarchical structure suggests the model processes information at different granularities, with "L Modules" likely capturing fine-grained temporal dependencies and "H Modules" extracting higher-level or longer-range features. The feedback loop from "H Module"s to "L Module"s signifies that high-level context influences low-level processing. A crucial aspect is the selective application of gradient computation, indicated by the "Requires Grad" (blue outline) and "No Grad" (black outline) labels. The majority of the intermediate recurrent "L Module"s and "H Module"s are marked "No Grad," implying their parameters are not updated through standard gradient descent during training. This design choice could be motivated by computational efficiency, memory conservation, gradient stability (e.g., preventing vanishing/exploding gradients in long sequences), or to implement a specific learning strategy where most recurrent steps are fixed or learned in a different stage. Conversely, the "Embed" module, the final "L Module," the final "H Module," and the "Head" module are all marked "Requires Grad." This suggests that the model's learning is primarily focused on the initial feature extraction (embedding) and the concluding stages of the hierarchical processing and prediction, adapting these specific components to the task while relying on potentially fixed or pre-trained knowledge in the intermediate recurrent layers. The inputs `[z0_H]` and `[z0_L]` likely represent initial states or context vectors for their respective module types.

**Key Insights:**
1.  **Hierarchical Recurrent Architecture:** The model employs a hierarchical design with both low-level ("L Module") and high-level ("H Module") components that interact across time. This allows for multi-scale temporal processing and the capture of complex dependencies, with higher-level abstracting features and feeding context back to lower levels. 
2.  **Targeted Learning via Selective Gradients:** A key insight is the differential application of gradient computation. Most recurrent "L" and "H" modules are "No Grad," suggesting that the bulk of the sequential processing might be fixed or learned separately, while the "Embed" module, the final "L" and "H" modules, and the "Head" module are "Requires Grad." This indicates a targeted learning approach focusing optimization on the initial input representation and the final stages of reasoning and output generation, which can offer benefits in terms of efficiency, stability, or specific learning objectives. 
3.  **Modular Design for Sequential Data:** The explicit "Unrolling over time" annotation and the repetitive modular structure (e.g., "L Module"s, "H Module"s) underscore the model's design for processing sequential data, typical in recurrent neural networks. The distinct roles of "Embed," "L Module," "H Module," and "Head" highlight a well-defined pipeline for converting raw input into a reasoned output.

**Document Context:**
This image is presented in Section 2, titled "Hierarchical Reasoning Model," and directly illustrates the architectural design of such a model. It visually explains how the proposed hierarchical model processes sequential data over time, detailing the interaction between low-level and high-level modules. The inclusion of "Requires Grad" and "No Grad" modules is critical for understanding the model's training dynamics, possibly highlighting a novel optimization strategy or a specific learning paradigm that is central to the document's argument about hierarchical reasoning. The comprehensive explanation details every textual and structural element, allowing readers to fully grasp the model's operational flow and the specific choices made regarding its training, which directly supports the document's narrative by laying out the technical foundation for its reasoning capabilities.

**Summary:**
This image depicts a hierarchical recurrent neural network architecture designed for processing information over time. The overall process is described as "Unrolling over time." The model takes three initial inputs: `[Input]`, `[z0_H]`, and `[z0_L]`. The `[Input]` is first processed by an "Embed" module, which is indicated by a blue outline and labeled "Requires Grad." The output of the "Embed" module, along with the `[z0_L]` input, feeds into a series of "L Module"s (Low-level Modules). These "L Module"s are arranged sequentially, with the output of one "L Module" typically feeding into the next "L Module" in the sequence. Most "L Module"s in the main sequence are shown with a black outline and are designated "No Grad." Periodically within this sequence, the output of an "L Module" also feeds upwards into an "H Module" (High-level Module). These "H Module"s receive input from the `[z0_H]` input and the output from the "L Module" directly below them. The output of an "H Module" then feeds back into the subsequent "L Module" in the sequence, illustrating a hierarchical interaction between the high and low-level processing units. Similar to the "L Module"s, the "H Module"s in the main sequence are also shown with a black outline and are designated "No Grad." The sequence of "L Module"s and "H Module"s is implied to be extensive by the use of ellipses (...). The final stages of the network include a blue-outlined "L Module" (Requires Grad), which feeds into a blue-outlined "H Module" (Requires Grad). The output of this final "H Module" then flows into a blue-outlined "Head" module (Requires Grad), which ultimately produces the `[Output]`. A legend in the bottom right clarifies the meaning of the module outlines: a hollow blue square indicates "Requires Grad," and a hollow black square indicates "No Grad."](images/5508819029fb7775970238f308baebffec7d24cc5e537b4ccd458410adbf00b7.jpg)

![## Image Analysis: 48640d377ff05ab68f39c8194cfac21600d20654d6dceb79de3bb1b5e6a07a68.jpg

**Conceptual Understanding:**
This image conceptually represents the algorithmic implementation of a Hierarchical Reasoning Model (HRM) and its training methodology. The main purpose is to demonstrate how a model can process information by maintaining and updating both low-level and high-level representations in a hierarchical manner, and how such a model can be effectively trained using a deep supervision approach within a PyTorch environment. Key ideas being communicated include the iterative update mechanism for different levels of representation, the conditional activation of higher-level processing, and a specific training paradigm that allows for gradients to be computed and applied at multiple stages or iterations of the model's reasoning process.

**Content Interpretation:**
This image presents the implementation details of a Hierarchical Reasoning Model (HRM) and its training strategy using deep supervision within a PyTorch framework. The `hrm` function defines the forward pass of the model, which includes an input embedding, and iterative updates for low-level (`L_net`) and high-level (`H_net`) representations. The hierarchical nature is evident as `H_net` updates are less frequent (every `T` steps) compared to `L_net`. The `torch.no_grad()` block suggests a specific phase where gradients are not computed for efficiency or to prevent unintended gradient flow. The subsequent '1-step grad' section implies a crucial step for gradient computation during a forward pass. The 'Deep Supervision' section illustrates a standard training loop, where the `hrm` function is called iteratively, and the loss is computed using `softmax_cross_entropy`. The `z.detach()` operation is key for how deep supervision is applied, allowing for gradient calculation within each supervision step without propagating gradients through `z` across different supervision steps.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Hierarchical Processing:** The HRM explicitly separates processing into low-level (`L_net`) and high-level (`H_net`) components. The high-level component updates less frequently (controlled by `T`), demonstrating a form of temporal or representational hierarchy.
2.  **Specific Update Frequencies:** `L_net` updates `zL` at every step within the main loop, while `H_net` updates `zH` only when `(_i + 1) % T == 0`, emphasizing the slower evolution of higher-level representations.
3.  **Deep Supervision Training:** The model is trained using a deep supervision approach where the `hrm` function is called iteratively within a training loop (`for step in range(N_supervision)`). This suggests that supervision might be applied at multiple points or over multiple 'steps' of reasoning, rather than just at the final output.
4.  **Gradient Management in PyTorch:** The use of `torch.no_grad()` for an initial sequence of updates within the `hrm` function, followed by a '1-step grad' section, suggests a nuanced approach to gradient computation, potentially to control memory usage or computational flow. The `z.detach()` in the training loop is critical for ensuring that gradients for the loss only flow through the current `hrm` call, preventing backpropagation through `z` across supervision steps.
5.  **Standard PyTorch Training Flow:** The training loop follows conventional PyTorch practices with data loading (`train_dataloader`), loss calculation (`softmax_cross_entropy`), backpropagation (`loss.backward()`), optimizer step (`opt.step()`), and gradient zeroing (`opt.zero_grad()`).

**Supporting Textual Evidence:**
*   `def hrm(z, x, N=2, T=2):` - Defines the model function with default hierarchical parameters.
*   `zL = L_net(zL, zH, x)` - Evidence for low-level network updates.
*   `if (_i + 1) % T == 0: zH = H_net(zH, zL)` - Evidence for conditional, less frequent high-level network updates based on the parameter `T`.
*   `# Deep Supervision` - Explicitly labels the training section as using deep supervision.
*   `for step in range(N_supervision): z, y_hat = hrm(z, x)` - Shows iterative calls to `hrm` within a training loop, indicating supervision at multiple 'steps'.
*   `loss = softmax_cross_entropy(y_hat, y)` - Specifies the loss function used.
*   `z = z.detach()` - Provides evidence for how gradients are managed between supervision steps to avoid propagating through `z` across iterations.
*   `loss.backward()`, `opt.step()`, `opt.zero_grad()` - Standard PyTorch optimization steps confirming the framework and training methodology.

**Document Context:**
This image is a crucial component of Section 2, 'Hierarchical Reasoning Model', directly illustrating the model's architectural and training mechanics. It provides the concrete implementation details, in pseudocode, for the conceptual HRM diagram likely presented in the 'Top' part of Figure 4 (as indicated by the 'Text after image'). The 'Bottom' part of Figure 4, this pseudocode, directly supports the understanding of how the HRM processes information hierarchically and how it is trained using a deep supervision approach in PyTorch. It bridges the theoretical description of the model with its practical application, allowing readers to grasp the internal workings and the specific algorithm used for training, including hyperparameter settings (N=2, T=2).

**Summary:**
The image displays two distinct sections of Python pseudocode related to a Hierarchical Reasoning Model (HRM). The top section defines the `hrm` function, outlining its forward pass logic, while the bottom section illustrates a deep supervision training loop for this model using PyTorch.

The `hrm` function, `def hrm(z, x, N=2, T=2):`, takes an initial hidden state `z`, input `x`, and optional parameters `N` and `T` (defaulting to 2). It first processes the input `x` through an `input_embedding` layer. The initial state `z` is then decomposed into `zH` (high-level) and `zL` (low-level) states. A `torch.no_grad()` block performs an iterative update for `N * T - 1` steps. In each step, `zL` is updated using `L_net` with `zL`, `zH`, and `x`. Periodically, specifically when `(_i + 1) % T == 0`, `zH` is updated using `H_net` with `zH` and `zL`. This loop operates without gradient tracking. Following this, a `1-step grad` section performs a final update of `zL` and `zH` using `L_net` and `H_net` respectively, which is where gradients would typically be computed. Finally, the function returns `zH`, `zL`, and an `output_head(zH)` result.

The `Deep Supervision` training section iterates through batches from a `train_dataloader`, providing `x` and `y_true`. For each batch, an initial state `z` is set (`z_init`). A loop runs for `N_supervision` steps. In each step, the `hrm` function is called with the current `z` and `x` to get an updated `z` and a prediction `y_hat`. A `softmax_cross_entropy` loss is calculated between `y_hat` and `y`. The state `z` is then detached from the computation graph (`z.detach()`), preventing gradients from flowing back into previous supervision steps. Standard PyTorch optimization steps follow: `loss.backward()` computes gradients, `opt.step()` updates model parameters, and `opt.zero_grad()` clears gradients for the next iteration.

This comprehensive description covers the sequential execution of the `hrm` function and its training process, highlighting the role of each component and the specific operations performed.](images/48640d377ff05ab68f39c8194cfac21600d20654d6dceb79de3bb1b5e6a07a68.jpg)
Figure 4: Top: Diagram of HRM with approximate gradient. Bottom: Pseudocode of HRM with deep supervision training in PyTorch.

$$
\frac { \partial z _ { H } ^ { \star } } { \partial \theta } = \left( I - J _ { \mathcal { F } } \big | _ { z _ { H } ^ { \star } } \right) ^ { - 1 } \frac { \partial \mathcal { F } } { \partial \theta } \bigg | _ { z _ { H } ^ { \star } } .
$$

Calculating the above gradient requires evaluating and inverting matrix $( I - J _ { \mathcal { F } } )$ that can be computationally expensive. Given the Neumann series expansion,

$$
( I - J _ { \mathcal { F } } ) ^ { - 1 } = I + J _ { \mathcal { F } } + J _ { \mathcal { F } } ^ { 2 } + J _ { \mathcal { F } } ^ { 3 } + \ldots ,
$$

the so-called $\boldsymbol { l }$ -step gradient $^ { 3 7 }$ approximates the series by considering only its first term, i.e. $( I -$ $J _ { \mathcal { F } } ) ^ { - 1 } \approx I$ , and leads to the following approximation of Equation (1):

$$
\frac { \partial z _ { H } ^ { * } } { \partial \theta _ { H } } \approx \frac { \partial f _ { H } } { \partial \theta _ { H } } , \quad \frac { \partial z _ { H } ^ { * } } { \partial \theta _ { L } } \approx \frac { \partial f _ { H } } { \partial z _ { L } ^ { * } } \cdot \frac { \partial z _ { L } ^ { * } } { \partial \theta _ { L } } , \quad \frac { \partial z _ { H } ^ { * } } { \partial \theta _ { I } } \approx \frac { \partial f _ { H } } { \partial z _ { L } ^ { * } } \cdot \frac { \partial z _ { L } ^ { * } } { \partial \theta _ { I } } \ .
$$

The gradients of the low-level fixed point, $\frac { \partial z _ { L } ^ { * } } { \partial \theta _ { L } }$ and $\frac { \partial z _ { L } ^ { * } } { \partial \theta _ { I } }$ , can also be approximated using another application of the 1-step gradient:

$$
\frac { \partial z _ { L } ^ { * } } { \partial \theta _ { L } } \approx \frac { \partial f _ { L } } { \partial \theta _ { L } } , \quad \frac { \partial z _ { L } ^ { * } } { \partial \theta _ { I } } \approx \frac { \partial f _ { L } } { \partial \theta _ { I } } ~ .
$$

By substituting Equation (3) back into Equation (2), we arrive at the final simplified gradients.

Before defining our loss function, we must first introduce two key elements of our proposed method: deep supervision and adaptive computational time.

Deep supervision Inspired by the principle that periodic neural oscillations regulate when learning occurs in the brain38, we incorporate a deep supervision mechanism into HRM, as detailed next.

Given a data sample $( x , y )$ , we run multiple forward passes of the HRM model, each of which we refer to as a segment. Let $M$ denote the total number of segments executed before termination. For each segment $m \in \{ 1 , \ldots , M \}$ , let $z ^ { m } = ( z _ { H } ^ { m N T } , z _ { L } ^ { m N T } )$ represent the hidden state at the conclusion of segment $m$ , encompassing both high-level and low-level state components.

At each segment $m$ , we apply a deep supervision step as follows:

1. Given the state $z ^ { m - 1 }$ from the previous segment, compute the next state $z ^ { m }$ and its associated output $\hat { y } ^ { m }$ through a forward pass in the HRM model:

$$
( z ^ { m } , \hat { y } ^ { m } ) \gets \mathrm { H R M } ( z ^ { m - 1 } , x ; \theta )
$$

2. Compute the loss for the current segment:

$$
L ^ { m } \gets \mathrm { L o s s } ( \hat { y } ^ { m } , y )
$$

3. Update parameters:

$$
\theta \gets \mathrm { O P T I M I Z E R S T E P } ( \theta , \nabla _ { \theta } L ^ { m } )
$$

The crucial aspect of this procedure is that the hidden state $z ^ { m }$ is “detached” from the computation graph before being used as the input state for the next segment. Consequently, gradients from segment $m + 1$ do not propagate back through segment $m$ , effectively creating a 1-step approximation of the gradient of the recursive deep supervision process 39,40. This approach provides more frequent feedback to the H-module and serves as a regularization mechanism, demonstrating superior empirical performance and enhanced stability in deep equilibrium models when compared to more complex, Jacobian-based regularization techniques 39,41. Figure 4 shows pseudocode of deep supervision training.

Adaptive computational time (ACT) The brain dynamically alternates between automatic thinking (“System 1”) and deliberate reasoning (“System 2”) 42. Neuroscientific evidence shows that these cognitive modes share overlapping neural circuits, particularly within regions such as the prefrontal cortex and the default mode network43,44. This indicates that the brain dynamically modulates the “runtime” of these circuits according to task complexity and potential rewards45,46.

Inspired by the above mechanism, we incorporate an adaptive halting strategy into HRM that enables “thinking, fast and slow”. This integration leverages deep supervision and uses the Q-learning algorithm47 to adaptively determine the number of segments. A Q-head uses the final state of the H-module to predict the $\mathrm { Q }$ -values $\hat { Q } ^ { m } = ( \hat { Q } _ { \mathrm { h a l t } } ^ { m } , \hat { Q } _ { \mathrm { c o n t i n u e } } ^ { m } )$ of the “halt” and “continue” actions:

$$
\hat { Q } ^ { m } = \sigma ( \theta _ { Q } ^ { \top } z _ { H } ^ { m N T } ) ,
$$

where $\sigma$ denotes the sigmoid function applied element-wise. The halt or continue action is chosen using a randomized strategy as detailed next. Let $M _ { \mathrm { m a x } }$ denote the maximum number of segments (a fixed hyperparameter) and $M _ { \mathrm { m i n } }$ denote the minimum number of segments (a random variable). The value of $M _ { \mathrm { m i n } }$ is determined stochastically: with probability $\varepsilon$ , it is sampled uniformly from the set $\{ 2 , \cdots , M _ { \mathrm { m a x } } \}$ (to encourage longer thinking), and with probability $1 - \varepsilon$ , it is set to 1. The halt action is selected under two conditions: when the segment count surpasses the maximum threshold $M _ { \mathrm { m a x } }$ , or when the estimated halt value $\hat { Q } _ { \mathrm { h a l t } }$ exceeds the estimated continue value $\hat { Q } _ { \mathrm { c o n t i n u e } }$ and the segment count has reached at least the minimum threshold $M _ { \mathrm { m i n } }$ .

The Q-head is updated through a Q-learning algorithm, which is defined on the following episodic Markov Decision Process (MDP). The state of the MDP at segment $m$ is $z ^ { m }$ , and the action space is {halt, continue}. Choosing the action “halt” terminates the episode and returns a binary reward indicating prediction the state transitions to $z ^ { m + 1 }$ tness, i.e., . Thus, the $\mathbf { 1 } \{ \hat { y } ^ { m } = y \}$ . Choosing “continue” yiargets for the two actions $\hat { G } ^ { m } = ( \hat { G } _ { \mathrm { h a l t } } ^ { m } , \hat { G } _ { \mathrm { c o n t i n u e } } ^ { m } )$ are given by

$$
\begin{array} { r l } & { \hat { G } _ { \mathrm { h a l t } } ^ { m } = \mathbf { 1 } \{ \hat { y } ^ { m } = y \} , } \\ & { \hat { G } _ { \mathrm { c o n t i n u e } } ^ { m } = \left\{ \begin{array} { l l } { \hat { Q } _ { \mathrm { h a l t } } ^ { m + 1 } , } & { \mathrm { i f ~ } m \geq N _ { \mathrm { m a x } } , } \\ { \operatorname* { m a x } ( \hat { Q } _ { \mathrm { h a l t } } ^ { m + 1 } , \hat { Q } _ { \mathrm { c o n t i n u e } } ^ { m + 1 } ) , } & { \mathrm { o t h e r w i s e ~ } . } \end{array} \right. } \end{array}
$$

We can now define the loss function of our learning procedure. The overall loss for each supervision segment combines both the Q-head loss and the sequence-to-sequence loss:

$$
L _ { \mathrm { A C T } } ^ { m } = \mathrm { L O s s } ( \hat { y } ^ { m } , y ) + \mathrm { B I N A R Y C R O s s E N T R O P Y } ( \hat { Q } ^ { m } , \hat { G } ^ { m } ) .
$$

Minimizing the above loss enables both accurate predictions and nearly optimal stopping decisions.

Selecting the “halt” action ends the supervision loop. In practice, sequences are processed in batches, which can be easily handled by substituting any halted sample in the batch with a fresh sample from the dataloader.

Figure 5 presents a performance comparison between two HRM variants: one incorporating ACT and another employing a fixed computational step count equivalent to ACT’s $M _ { \mathrm { m a x } }$ parameter. It shows that ACT effectively adapts its computational resources based on task complexity, achieving significant computational savings with minimal impact on performance.

Inference-time scaling An effective neural model should exploit additional computational resources during inference to enhance performance. As illustrated in Figure 5-(c), HRM seamlessly achieves inference-time scaling by simply increasing the computational limit parameter, $M _ { \mathrm { m a x } }$ without requiring further training or architectural modifications.

Additional compute is especially effective for tasks that demand deeper reasoning. On Sudoku— a problem that often requires long-term planning—HRM exhibits strong inference-time scaling. On the other hand, we find that extra computational resources yield minimal gains in ARC-AGI challenge, as solutions generally require only a few transformations.

![## Image Analysis: 8c542fd650c1e68bc1a2863c3d907e6ae885470284048f446e7bd4ab7c60a1e5.jpg

**Conceptual Understanding:**
The image conceptually represents the trade-offs and benefits of using an Adaptive Computation Time (ACT) mechanism in deep learning models, particularly within a hierarchical reasoning framework. The main purpose is to demonstrate that ACT allows models to dynamically allocate computational resources, leading to significantly improved efficiency without sacrificing performance, and also showcasing its ability to scale effectively during inference.

Key ideas being communicated are:
1.  **Adaptive Resource Allocation:** ACT enables models to use only the necessary number of compute steps for a given task, contrasting with fixed-compute models that use a predetermined (and often excessive) number of steps.
2.  **Efficiency vs. Accuracy Balance:** ACT strikes a favorable balance, achieving high accuracy comparable to more computationally expensive fixed-step methods while drastically reducing the average computational burden.
3.  **Inference-Time Generalization and Scalability:** Models trained with ACT can adapt to varying computational budgets during inference, potentially achieving higher accuracy if more compute steps are allowed, even if trained with a lower maximum limit. This implies flexibility and robustness in deployment.

**Content Interpretation:**
The image presents an empirical analysis of the Adaptive Computation Time (ACT) mechanism in a hierarchical reasoning model, specifically applied to the Sudoku-Extreme-Full dataset. It compares ACT's efficiency and performance against a fixed-compute model and investigates its scalability during inference. 

Sub-plot (a) 'ACT Compute Spent' illustrates the computational efficiency of ACT. It shows that while a model with a fixed number of compute steps (Fixed M) consumes increasingly more compute steps as 'M' (number of steps) increases, the ACT model (ACT (M_max limit)) maintains a consistently low and stable number of mean compute steps, regardless of the maximum allowed compute limit (M_max). The 'Fixed M' model's compute steps increase linearly from 2 to 8 as M goes from 2 to 8, whereas the 'ACT (M_max limit)' model's mean compute steps stay around 1.5 for M_max values of 2, 4, and 8. This indicates ACT's ability to adaptively use only necessary computation.

Sub-plot (b) 'ACT Performance' compares the accuracy of the ACT model against the fixed-compute model. While the 'Fixed M' model shows a steady increase in 'Accuracy %' from 92.5% to just under 100.0% as M increases from 2 to 8, the 'ACT (M_max limit)' model, despite starting at a lower accuracy of 82.5% at M_max=2, rapidly improves its accuracy, catching up to and nearly matching the 'Fixed M' model's accuracy at higher M_max values (e.g., approximately 96.5% at M_max=4 and just under 100.0% at M_max=8). This suggests that ACT can achieve comparable performance to fixed-compute models while being more computationally efficient.

Sub-plot (c) 'Inference-time scaling' examines how models trained with specific M_max limits perform when the inference-time M_max is varied. It demonstrates that models trained with a lower M_max (e.g., 'Train M_max = 2') can still benefit from higher 'Inference M_max' values, leading to improved accuracy (e.g., from 82.5% at Inference M_max=2 to 96.5% at Inference M_max=16). Furthermore, it shows a clear hierarchy: models trained with higher M_max limits (e.g., 'Train M_max = 8') consistently achieve higher accuracies across all 'Inference M_max' values compared to those trained with lower M_max limits (e.g., 92.5% vs 82.5% at Inference M_max=2). All models show continued accuracy gains as 'Inference M_max' increases, highlighting the scalability of ACT during inference.

**Key Insights:**
**Main Takeaways and Insights:**

1.  **Computational Efficiency of ACT:** Adaptive Computation Time (ACT) significantly reduces the computational overhead compared to models with a fixed number of compute steps. As shown in sub-plot (a) 'ACT Compute Spent', the mean compute steps for 'ACT (M_max limit)' remain stable and low (around 1.5 steps) even when the maximum allowed steps (M_max) increases, while 'Fixed M' compute steps increase linearly with M.

2.  **Comparable Performance with Reduced Compute:** ACT models can achieve performance (Accuracy %) on par with fixed-compute models, as demonstrated in sub-plot (b) 'ACT Performance'. The 'ACT (M_max limit)' curve closely approaches the 'Fixed M' curve at higher M_max values, reaching near 100% accuracy, despite using substantially fewer average compute steps as evidenced in sub-plot (a).

3.  **Inference-Time Scalability and Generalization:** Models trained with ACT exhibit strong generalization capabilities during inference. Sub-plot (c) 'Inference-time scaling' shows that increasing the 'Inference M_max' for a given trained model consistently leads to higher 'Accuracy %'. This indicates that a model doesn't need to be retrained for higher computational limits but can adapt and improve its accuracy simply by being allowed more inference time steps, as evidenced by all three 'Train M_max' lines showing upward trends.

4.  **Impact of Training M_max on Base Performance:** Training a model with a higher M_max limit (e.g., 'Train M_max = 8') results in a better baseline accuracy across all inference M_max values compared to training with lower M_max limits (e.g., 'Train M_max = 2' or 'Train M_max = 4'). This is seen in sub-plot (c) where the 'Train M_max = 8' curve (light blue) is consistently above the other two curves.

**Evidence for Insights from Transcription:**
*   **Efficiency:** (a) 'ACT Compute Spent' shows 'ACT (M_max limit)' at ~1.5 Mean Compute Steps for M_max = 2, 4, 8, while 'Fixed M' goes from 2 to 8. This directly supports the reduced computational overhead.
*   **Performance:** (b) 'ACT Performance' shows 'ACT (M_max limit)' reaches near 100.0% Accuracy at M_max = 8, matching 'Fixed M' at M = 8. This supports comparable performance.
*   **Scalability:** (c) 'Inference-time scaling' shows 'Accuracy %' increasing for all three 'Train M_max' lines (2, 4, 8) as 'Inference M_max' increases from 2 to 16. For example, 'Train M_max = 8' goes from ~92.5% to ~100.0%. This confirms generalization and accuracy gains with increased inference time.
*   **Training M_max Impact:** (c) 'Train M_max = 8' line is consistently above 'Train M_max = 4' and 'Train M_max = 2' lines, indicating superior performance from higher training limits.

**Document Context:**
This image directly supports the discussion in 'Section: 2 Hierarchical Reasoning Model' by providing empirical evidence for the effectiveness and efficiency of Adaptive Computation Time (ACT). The document's accompanying text, 'Figure 5: Effectiveness of Adaptive Computation Time (ACT) on the Sudoku-Extreme-Full,' explicitly refers to these plots. 

Specifically, sub-plot (a) 'ACT Compute Spent' visually confirms the claim that 'ACT maintains a low and stable number of average compute steps even as the maximum limit (M_max) increases.' Sub-plot (b) 'ACT Performance' validates that 'The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average.' Lastly, sub-plot (c) 'Inference-time scaling' illustrates and expands on the concept that 'Models trained with a specific M_max can generalize to higher computational limits during inference, leading to improved accuracy,' using the example 'a model trained with M_max = 8 continues to see accuracy gains when run with M_max = 16 during inference.' Together, these graphs provide critical quantitative support for the benefits of using ACT within a hierarchical reasoning model for tasks like Sudoku.

**Summary:**
The image contains three line graphs, (a) ACT Compute Spent, (b) ACT Performance, and (c) Inference-time scaling, all demonstrating the effectiveness of Adaptive Computation Time (ACT) on a Sudoku-Extreme-Full task. 

**Sub-plot (a) ACT Compute Spent:** This graph compares the 'Mean Compute Steps' (Y-axis, ranging from 1 to 8) against 'M (Fixed) or M_max (ACT)' (X-axis, with values 2, 4, 8). Two lines are plotted: 'Fixed M' (dark line) and 'ACT (M_max limit)' (light blue line). The 'Fixed M' line shows mean compute steps increasing significantly from 2 (at M=2) to approximately 4 (at M=4) and then to 8 (at M=8). In contrast, the 'ACT (M_max limit)' line remains relatively flat and low, at approximately 1.5 compute steps, across all M_max values (2, 4, 8).

**Sub-plot (b) ACT Performance:** This graph compares 'Accuracy %' (Y-axis, ranging from 82.5 to 100.0) against 'M (Fixed) or M_max (ACT)' (X-axis, with values 2, 4, 8). A dashed grey line at 100.0 indicates maximum accuracy. Two lines are plotted: 'Fixed M' (dark line) and 'ACT (M_max limit)' (light blue line). The 'Fixed M' line shows accuracy increasing from approximately 92.5% (at M=2) to about 97.5% (at M=4) and reaching just below 100.0% (at M=8). The 'ACT (M_max limit)' line starts at a lower accuracy of approximately 82.5% (at M_max=2), then increases sharply to around 96.5% (at M_max=4), and then to just below 100.0% (at M_max=8), converging with the 'Fixed M' line at M_max=8.

**Sub-plot (c) Inference-time scaling:** This graph displays 'Accuracy %' (Y-axis, ranging from 82.5 to 100.0) against 'Inference M_max' (X-axis, with values 2, 4, 8, 16). A dashed grey line at 100.0 indicates maximum accuracy. Three lines represent models trained with different M_max values: 'Train M_max = 2' (dark line), 'Train M_max = 4' (yellow line), and 'Train M_max = 8' (light blue line). For 'Train M_max = 2', accuracy increases from approximately 82.5% (at Inference M_max=2) to around 91.5% (at Inference M_max=4), 95.0% (at Inference M_max=8), and reaching approximately 96.5% (at Inference M_max=16). For 'Train M_max = 4', accuracy starts at roughly 90.0% (at Inference M_max=2), increases to about 95.5% (at Inference M_max=4), 97.0% (at Inference M_max=8), and reaches approximately 98.5% (at Inference M_max=16). For 'Train M_max = 8', accuracy begins at approximately 92.5% (at Inference M_max=2), increases to about 97.0% (at Inference M_max=4), 98.0% (at Inference M_max=8), and nears 100.0% (at Inference M_max=16). All three lines show a consistent trend of increasing accuracy as 'Inference M_max' increases, with models trained at higher M_max limits consistently outperforming those trained at lower M_max limits.](images/8c542fd650c1e68bc1a2863c3d907e6ae885470284048f446e7bd4ab7c60a1e5.jpg)
Figure 5: Effectiveness of Adaptive Computation Time (ACT) on the Sudoku-Extreme-Full. (a) Mean compute steps used by models with ACT versus models with a fixed number of compute steps $( M )$ . ACT maintains a low and stable number of average compute steps even as the maximum limit $( M _ { \mathrm { m a x } } )$ increases. (b) Accuracy comparison. The ACT model achieves performance comparable to the fixed-compute model while utilizing substantially fewer computational steps on average. (c) Inference-time scalability. Models trained with a specific $M _ { \mathrm { m a x } }$ can generalize to higher computational limits during inference, leading to improved accuracy. For example, a model trained with $M _ { \mathrm { m a x } } = 8$ continues to see accuracy gains when run with $M _ { \mathrm { m a x } } = 1 6$ during inference.

Stability of Q-learning in ACT The deep Q-learning that underpins our ACT mechanism is known to be prone to instability, often requiring stabilization techniques such as replay buffers and target networks 48, which are absent in our design. Our approach, however, achieves stability through the intrinsic properties of our model and training procedure. Recent theoretical work by Gallici et al. 49 shows that Q-learning can achieve convergence if network parameters are bounded, weight decay is incorporated during training, and post-normalization layers are implemented. Our model satisfies these conditions through its Post-Norm architecture that employs RMSNorm (a layer normalization variant) and the AdamW optimizer. AdamW has been shown to solve an $L _ { \infty }$ - constrained optimization problem, ensuring that model parameters remain bounded by $1 / \lambda ^ { 5 0 }$ .

Architectural details We employ a sequence-to-sequence architecture for HRM. Both input and output are represented as token sequences: $\boldsymbol { x } = ( x _ { 1 } , \dots , x _ { l } )$ and $y = ( y _ { 1 } , \dotsc , y _ { l ^ { \prime } } )$ respectively. The model includes an embedding layer $f _ { I }$ that converts discrete tokens into vector representations, and an output head $f _ { O } ( z ; \theta _ { O } ) = \operatorname { s o f t m a x } ( \theta _ { O } z )$ that transforms hidden states into token probability distributions $\hat { y }$ . For small-sample experiments, we replace softmax with stablemax51 to improve generalization performance. The sequence-to-sequence loss is averaged over all tokens, $\begin{array} { r } { \mathrm { L o s s } ( \hat { y } , \bar { y } ) = \frac { 1 } { l ^ { \prime } } \sum _ { i = 1 } ^ { l ^ { \prime } } \bar { \log { p ( y _ { i } ) } } } \end{array}$ , where $p ( y _ { i } )$ is the probability that distribution $\hat { y } _ { i }$ assigns to token $y _ { i }$ . The initial hidden states $z ^ { 0 }$ are initialized by sampling from a truncated normal distribution with standard deviation of 1, truncation of 2, and kept fixed throughout training.

Both the low-level and high-level recurrent modules $f _ { L }$ and $f _ { H }$ are implemented using encoderonly Transformer52 blocks with identical architectures and dimensions. These modules take multiple inputs, and we use straightforward element-wise addition to combine them, though more sophisticated merging techniques such as gating mechanisms could potentially improve performance and is left for future work. For all Transformer blocks in this work—including those in the baseline models—we incorporate the enhancements found in modern LLMs (based on Llama 53 architectures). These improvements include Rotary Positional Encoding54, Gated Linear Units 55, RMSNorm56, and the removal of bias terms from linear layers.

Furthermore, both HRM and recurrent Transformer models implement a Post-Norm architecture with weights initialized via truncated LeCun Normal initialization 57,58,59, while the scale and bias parameters are excluded from RMSNorm. All parameters are optimized using the Adam-atan2 optimizer60, a scale-invariant variant of Adam61, combined with a constant learning rate that includes linear warm-up.

![## Image Analysis: 93707a0a6eabd473b94cba459ec07f081512807638d969f0d50c2dd8bf83714c.jpg

**Conceptual Understanding:**
This image conceptually represents a collection of benchmark reasoning tasks used to evaluate artificial intelligence or computational models. Its main purpose is to visually demonstrate the types of problems that a 'Hierarchical Reasoning Model' (as suggested by the document context) might be designed to solve, and to quantify the varying difficulty levels associated with specific subsets of these tasks, particularly Sudoku. The image communicates the key idea that effective reasoning models must be capable of handling diverse and increasingly complex cognitive challenges, from abstract pattern recognition and logical deduction to pathfinding, and that the difficulty of these tasks can be empirically measured.

**Content Interpretation:**
The image visualizes four distinct types of benchmark reasoning tasks: Abstract Reasoning Corpus (ARC-AGI), Sudoku-Hard, and Maze Navigation, along with a bar chart illustrating the difficulty of various subsets of Sudoku-Extreme problems. 

**(a) ARC-AGI:** This illustrates input-output mapping problems where an agent must infer the underlying rule transforming a given input grid into an output grid. The tasks involve recognizing patterns, transformations, and generating a solution for a new input based on learned examples. The presence of the '?' in the third example indicates the challenge of predicting the output for a novel input.

**(b) Sudoku-Hard:** This demonstrates a classic constraint satisfaction problem. The top grid shows a partial Sudoku puzzle, requiring logical deduction to fill in the remaining numbers while adhering to Sudoku rules (each row, column, and 3x3 block must contain all digits from 1 to 9). The bottom grid provides the fully solved state, indicating the target for such a task.

**(c) Maze navigation:** This represents a pathfinding problem. An agent needs to navigate from a start point (green square) to an end point (red square) through a complex, grid-based maze, avoiding obstacles (black squares) and finding a valid path (highlighted in yellow in the bottom maze).

**(d) Sudoku-Extreme subset difficulty:** This bar chart quantifies the computational effort (mean backtracks) required to solve Sudoku puzzles from various sources or with specific characteristics. It shows a progression of difficulty, with 'Kaggle', '17-clue', and 'Unbiased' being relatively easy, 'Magictour 1465' moderately difficult, and 'Forum-Hard' and 'Forum-Extreme' being significantly more challenging, requiring a much higher number of backtracks.


**Key Insights:**
The main takeaways from this image are: 

1.  **Diversity of Reasoning Tasks:** The image illustrates that a comprehensive reasoning model needs to handle a variety of tasks, including abstract pattern generalization (ARC-AGI), logical puzzle solving (Sudoku), and pathfinding (Maze navigation). Each task presents a unique set of challenges that require different reasoning capabilities. 
2.  **Quantifiable Task Difficulty:** The 'Sudoku-Extreme subset difficulty' bar chart demonstrates that even within the same type of problem (Sudoku), there can be a wide spectrum of difficulty, as measured by 'Mean Backtracks'. This highlights the importance of evaluating models across diverse problem instances, not just a single difficulty level. 
3.  **Specific Difficulty Levels:** The numerical values on the bar chart provide clear evidence of increasing difficulty: 'Kaggle' (0.00 backtracks) and '17-clue' (0.49 backtracks) are trivial, 'Unbiased' (1.01 backtracks) and 'Magictour 1465' (6.31 backtracks) are relatively easy, while 'Forum-Hard' (29.95 backtracks) and especially 'Forum-Extreme' (62.00 backtracks) are significantly harder, far exceeding the 'Dataset mean' (approximately 22 backtracks). This suggests that 'Forum-Hard' and 'Forum-Extreme' are crucial benchmarks for truly testing robust reasoning capabilities in Sudoku.
4.  **Problem-Solving Visualization:** The 'ARC-AGI' and 'Maze navigation' examples visually convey the input-output transformation and pathfinding process, respectively, making the nature of these problems immediately understandable. The solved Sudoku grid provides a clear target for the Sudoku-Hard task. 

These insights are directly supported by the verbatim text extractions of the task labels, the numbers in the Sudoku grids, the path in the maze, and all the labels and values in the 'Sudoku-Extreme subset difficulty' bar chart. For instance, the high 'Mean Backtracks' values for 'Forum-Hard' (29.95) and 'Forum-Extreme' (62.00) directly evidence their significantly higher difficulty compared to other subsets.

**Document Context:**
The image serves as a visual exposition of different benchmark tasks used to evaluate the capabilities of a hierarchical reasoning model, as indicated by the '2 Hierarchical Reasoning Model' section title in the document context. It provides concrete examples of the types of problems the model is designed to tackle: abstract pattern recognition (ARC-AGI), logical deduction and constraint satisfaction (Sudoku-Hard, Sudoku-Extreme), and pathfinding (Maze navigation). The inclusion of the 'Sudoku-Extreme subset difficulty' bar chart directly quantifies the challenges posed by different instances of a particular task, providing empirical evidence of varying difficulty levels that a robust reasoning model would need to handle. The 'Dataset mean' line in the bar chart provides a baseline for comparing the difficulty of specific Sudoku subsets, implicitly suggesting the performance or target difficulty for the reasoning model discussed in the document.

**Summary:**
The image presents a visualization of four distinct benchmark tasks designed to test reasoning capabilities, followed by a detailed bar chart illustrating the difficulty of various subsets within Sudoku-Extreme puzzles. Each task is represented by visual examples and a descriptive label, aiding in understanding the challenges involved. 

**(a) ARC-AGI:** This section displays three pairs of Abstract Reasoning Corpus (ARC) tasks. Each pair consists of an input grid on the left and a desired output grid on the right, connected by a rightward arrow. The grids are composed of 10x10 squares, with various colored blocks (blue, red, yellow, green, orange, purple, grey) forming different shapes. The top two pairs show an input and its corresponding output, while the bottom pair shows an input on the left and a black grid with a large white question mark "?" in the center, indicating an unsolved or target state. The label for this section is "(a) ARC-AGI".

**(b) Sudoku-Hard:** This section illustrates two 9x9 Sudoku grids. The top grid is an incomplete Sudoku puzzle, with pre-filled numbers and many blank cells. The numbers present are: Row 1: 8, 4, blank, blank, 5, blank, blank, blank, 6. Row 2: blank, blank, blank, blank, 8, blank, 7, blank, blank. Row 3: 3, blank, blank, 4, blank, blank, blank, blank, blank. Row 4: blank, 3, 8, 4, blank, blank, blank, 2, blank. Row 5: blank, blank, 6, blank, blank, 3, blank, blank, 8. Row 6: 9, blank, blank, blank, blank, blank, blank, blank, 6. Row 7: blank, blank, blank, 5, blank, blank, 2, blank, blank. Row 8: blank, 2, 5, blank, blank, 3, blank, blank, 1. Row 9: blank, blank, blank, blank, blank, blank, blank, 8, blank. The bottom grid is a complete Sudoku puzzle, with all cells filled with numbers from 1 to 9. The numbers are: Row 1: 7, 8, 4, 1, 2, 5, 9, 6, 3. Row 2: 2, 6, 1, 3, 8, 9, 7, 4, 5. Row 3: 3, 5, 9, 6, 4, 7, 8, 1, 2. Row 4: 5, 3, 8, 4, 9, 6, 1, 2, 7. Row 5: 4, 1, 6, 2, 7, 3, 5, 9, 8. Row 6: 9, 7, 2, 8, 5, 1, 4, 3, 6. Row 7: 6, 9, 3, 5, 1, 8, 2, 7, 4. Row 8: 8, 4, 7, 9, 6, 2, 3, 5, 1. Row 9: 1, 2, 5, 7, 3, 4, 6, 8, 9. The label for this section is "(b) Sudoku-Hard".

**(c) Maze navigation:** This section shows two 20x20 grid-based mazes. Both grids are primarily composed of black and gray squares, with a small green square marking the start and a red square marking the end. The top maze shows the raw maze structure without a path. The bottom maze shows the same maze with a yellow path highlighted, leading from the green starting square to the red ending square. The label for this section is "(c) Maze navigation".

**(d) Sudoku-Extreme subset difficulty:** This section is a bar chart visualizing the mean number of backtracks required to solve different subsets of Sudoku-Extreme puzzles. The Y-axis is labeled "Mean Backtracks" and ranges from 0 to 60 in increments of 10. The X-axis has categories: "Kaggle", "17-clue", "Unbiased", "Magictour 1465", "Forum-Hard", and "Forum-Extreme". There is a horizontal dashed grey line labeled "Dataset mean" at approximately 22 backtracks. The bar values are: "Kaggle": 0.00, "17-clue": 0.49, "Unbiased": 1.01, "Magictour 1465": 6.31, "Forum-Hard": 29.95, and "Forum-Extreme": 62.00. The label for this section is "(d) Sudoku-Extreme subset difficulty".](images/93707a0a6eabd473b94cba459ec07f081512807638d969f0d50c2dd8bf83714c.jpg)
Figure 6: Left: Visualization of benchmark tasks. Right: Difficulty of Sudoku-Extreme examples.

# 3 Results

This section begins by describing the ARC-AGI, Sudoku, and Maze benchmarks, followed by an overview of the baseline models and their results. Figure 6-(a,b,c) presents a visual representation of the three benchmark tasks, which are selected to evaluate various reasoning abilities in AI models.

# 3.1 Benchmarks

ARC-AGI Challenge The ARC-AGI benchmark evaluates general fluid intelligence through IQtest-like puzzles that require inductive reasoning27. The initial version, ARC-AGI-1, presents challenges as input-output grid pairs that force AI systems to extract and generalize abstract rules from just a few examples. Each task provides a few input–output example pairs (usually 2–3) and a test input. An AI model has two attempts to produce the correct output grid. Although some believe that mastering ARC-AGI would signal true artificial general intelligence, its primary purpose is to expose the current roadblocks in AGI progress. In fact, both conventional deep learning methods and chain-of-thought techniques have faced significant challenges with ARC-AGI-1, primarily because it requires the ability to generalize to entirely new tasks28.

Addressing the limitations identified in ARC-AGI-1, ARC-AGI-2 significantly expands the benchmark by providing a more comprehensive and carefully refined collection of tasks. These new tasks emphasize deeper compositional reasoning, multi-step logic, contextual rule application, and symbolic abstraction. Human calibration studies show these tasks are challenging but doable for people, while being much harder for current AI systems, offering a clearer measure of general reasoning abilities2 9 .

Sudoku-Extreme Sudoku is a $9 \times 9$ logic puzzle, requiring each row, column, and $3 \times 3$ block to contain the digits 1–9 exactly once. A prediction is determined correct if it exactly matches the puzzle’s unique solution. Sudoku’s complex logical structure makes it a popular benchmark for evaluating logical reasoning in machine learning62,63,64.

The most frequently used Sudoku datasets in research, namely the Kaggle dataset 65, can be fully solved using elementary single-digit techniques 66. The minimal 17-clue puzzles 62, another widelyused collection, might seem more challenging due to its small number of clues. However, this perception is misleading—since 17 represents the minimum number of clues required to guarantee a unique Sudoku solution, these hints need to be highly orthogonal of each other. This orthogonal arrangement leads to many direct, easily-resolved solution paths67.

We introduce Sudoku-Extreme, a more challenging dataset that is compiled from the aforementioned easy datasets as well as puzzles recognized by the Sudoku community as exceptionally difficult for human players:

• Easy puzzles compiled from Kaggle, 17-clue, plus unbiased samples from the Sudoku puzzle distribution67: totaling 1 149 158 puzzles.   
• Challenging puzzles compiled from Magictour 1465, Forum-Hard and Forum-Extreme subsets: totaling 3 104 157 puzzles.

The compiled data then undergo a strict 90/10 train-test split, ensuring that the test set puzzles cannot be derived through equivalent transformations of any training samples. Sudoku-Extreme is a down-sampled subset of this data containing 1000 training examples. We use Sudoku-Extreme in our main experiments (Figure 1), which focuses on small-sample learning scenarios. To guarantee convergence and control overfitting effects in our analysis experiments (Figures 2, 3 and 5), we use the complete training data, Sudoku-Extreme-Full, containing 3 831 994 examples.

We measure puzzle difficulty by counting the number of search backtracks ("guesses") required by a smart Sudoku solver program tdoku, which uses propositional logic to reduce the number of guesses67. Our Sudoku-Extreme dataset exhibits a mean difficulty of 22 backtracks per puzzle, significantly higher than existing datasets, including recent handmade puzzles Sudoku-Bench68 which average just 0.45 backtracks per puzzle. These subset complexity levels are shown in Figure 6-(d).

Maze-Hard This task involves finding the optimal path in a $3 0 \times 3 0$ maze, making it interpretable and frequently used for training LLMs in search tasks 69,70,71. We adopt the instance generation procedure of Lehnert et al. 71, but introduce an additional filter to retain only those instances whose difficulty exceeds 110. Here, “difficulty” is defined as the length of the shortest path, which aligns with the linear time complexity of the wavefront breadth-first search algorithm on GPUs72. A path is considered correct if it is valid and optimal—that is, the shortest route from the start to the goal. The training and test set both include 1000 examples.

# 3.2 Evaluation Details

For all benchmarks, HRM models were initialized with random weights and trained in the sequenceto-sequence setup using the input–output pairs. The two-dimensional input and output grids were flattened and then padded to the maximum sequence length. The resulting performance is shown in Figure 1. Remarkably, HRM attains these results with just $\mathord { \sim } 1 0 0 0$ training examples per task—and without pretraining or CoT labels.

For ARC-AGI challenge, we start with all input-output example pairs in the training and the evaluation sets. The dataset is augmented by applying translations, rotations, flips, and color permutations to the puzzles. Each task examples is prepended with a learnable special token that represents the puzzle it belongs to. At test time, we proceed as follows for each test input in the evaluation set: (1) Generate and solve 1000 augmented variants and, for each, apply the inverse-augmentation transform to obtain a prediction. (2) Choose the two most popular predictions as the final outputs.3 All results are reported on the evaluation set.

We augment Sudoku puzzles by applying band and digit permutations, while data augmentation is disabled for Maze tasks. Both tasks undergo only a single inference pass.

For ARC-AGI, the scores of the CoT models are taken from the official leaderboard29, while for Sudoku and Maze, the scores are obtained by evaluating through the corresponding API.

In Figure 1, the baselines are grouped based on whether they are pre-trained and use CoT, or neither. The "Direct pred" baseline means using "direct prediction without CoT and pre-training", which retains the exact training setup of HRM but swaps in a Transformer architecture. Interestingly, on ARC-AGI-1, "Direct pred" matches the performance of Liao and $\mathrm { G u } ^ { 7 3 }$ , who built a carefully designed, domain-specific equivariant network for learning the ARC-AGI task from scratch, without pre-training. By substituting the Transformer architecture with HRM’s hierarchical framework and implementing ACT, we achieve more than a twofold performance improvement.

On the Sudoku-Extreme and Maze-Hard benchmarks, the performance gap between HRM and the baseline methods is significant, as the baselines almost never manage to solve the tasks. These benchmarks that demand lengthy reasoning traces are particularly difficult for CoT-based methods. With only 1000 training examples, the “Direct pred” baseline—which employs an 8-layer Transformer identical in size to HRM—fails entirely on these challenging reasoning problems. When trained on the larger Sudoku-Extreme-Full dataset, however, “Direct pred” can solve some easy Sudoku puzzles and reaches $1 6 . 9 \%$ accuracy (see Figure 2). Lehnert et al. 71 showed that a large vanilla Transformer model with 175M parameters, trained on 1 million examples across multiple trials, achieved only marginal success on $3 0 \mathrm { x } 3 0$ Maze tasks, with accuracy below $2 0 \%$ using the pass $@ 6 4$ evaluation metric.

# 3.3 Visualization of intermediate timesteps

Although HRM demonstrates strong performance on complex reasoning tasks, it raises an intriguing question: what underlying reasoning algorithms does the HRM neural network actually implement? Addressing this question is important for enhancing model interpretability and developing a deeper understanding of the HRM solution space.

While a definitive answer lies beyond our current scope, we begin our investigation by analyzing state trajectories and their corresponding solution evolution. More specifically, at each timestep $i$ and given the low-level and high-level state pair $\cdot z _ { L } ^ { i }$ and $z _ { H } ^ { i } .$ ) we perform a preliminary forward pass through the H-module to obtain $\hat { z } ^ { i } = f _ { H } \big ( z _ { H } ^ { i } , z _ { L } ^ { i } ; \theta _ { H } \big )$ and its corresponding decoded prediction $\bar { y } ^ { i } = f _ { O } ( \bar { z } ^ { i } ; \theta _ { O } )$ . The prediction $\hat { y } ^ { i }$ is then visualized in Figure 7.

In the Maze task, HRM appears to initially explore several potential paths simultaneously, subsequently eliminating blocked or inefficient routes, then constructing a preliminary solution outline followed by multiple refinement iterations. In Sudoku, the strategy resembles a depth-first search approach, where the model appears to explore potential solutions and backtracks when it hits dead ends. HRM uses a different approach for ARC tasks, making incremental adjustments to the board and iteratively improving it until reaching a solution. Unlike Sudoku, which involves frequent backtracking, the ARC solution path follows a more consistent progression similar to hill-climbing optimization.

![## Image Analysis: 30c3cc5e6e2ad4e11a7cc7e72ebf63bc19a43478d18d4478f74a6f325239a754.jpg

**Conceptual Understanding:**
This image conceptually represents the **visualization of intermediate predictions by a Hypothesis-based Reasoning Model (HRM)** as it attempts to solve three different types of benchmark tasks. It's designed to showcase the internal, step-by-step evolution of the model's reasoning process.

**Main Purpose or Message:**
The main purpose of this image is to clearly demonstrate *how* the HRM arrives at its solutions by breaking down complex problem-solving into a series of observable, discrete 

**Content Interpretation:**
The image showcases the HRM's ability to tackle diverse cognitive tasks by progressively refining its predictions across various timesteps.

**Processes, Concepts, Relationships, or Systems Being Shown:**
*   **Iterative Pathfinding (MazeHard):** This section demonstrates the HRM's spatial reasoning and search capabilities, where it iteratively finds a path from a start point (red square) to an end point (green square) within a complex maze. The blue cells visually indicate the predicted path, showing a gradual expansion and refinement over timesteps.
*   **Constraint Satisfaction and Logical Deduction (Sudoku-Extreme):** This section illustrates the model's ability to solve a logic puzzle by filling a 10x9 Sudoku grid. It involves identifying constraint violations (red highlights) and iteratively adjusting cell values (grey shading indicating changes) to satisfy all Sudoku rules. The bold cells consistently represent the initial givens.
*   **Abstract Pattern Transformation/Rule Inference (ARC-AGI-2 Tasks):** These tasks demonstrate the HRM's capacity for abstract reasoning. The model infers transformation rules from an example input-output pair and then applies these rules to a new test input to generate the correct output through a series of intermediate steps.

**Significance of Data, Trends, or Information Presented:**
*   **MazeHard:** The progression from "Timestep i = 0" (no path) to "Timestep i = 6" (complete blue path) signifies the model's iterative search, exploration, and successful path discovery. Each new blue segment at subsequent timesteps highlights the model's hypotheses about the correct route.
*   **Sudoku-Extreme:** The "Initial" grid shows the problem setup. "Timestep i = 0" reveals the model's first full solution attempt, complete with numerous "red highlights" indicating constraint violations. The subsequent timesteps ("Timestep i = 1" through "Timestep i = 6") show "grey shading" in cells where values are modified, demonstrating the model's active process of identifying and correcting errors. The crucial trend is the disappearance of "red highlights" by "Timestep i = 7," signifying successful constraint satisfaction and a valid solution. The 10x9 grid layout, though non-standard, shows the specific problem instance the model is solving.
*   **ARC-AGI-2 Tasks:** The provided "Example Input-Output pair" (e.g., [7666fa5d]) is vital as it provides the learning signal for the transformation rule. The "Test Input" (e.g., dark red diagonals for [7666fa5d]) is the challenge. The visual progression across "Timestep i = 0" to "Timestep i = 4" (for [7666fa5d]) and "Timestep i = 0" to "Timestep i = 6" (for [7b80bb43]) shows the model dynamically applying the inferred rule to reshape and connect pixels, demonstrating its ability to generalize and execute abstract visual transformations.

**Relationships:** The consistent labeling of "Timestep i = X" across all tasks establishes a clear sequential relationship, indicating that each displayed state is a direct evolution from the previous one, showing the model's reasoning as a series of incremental refinements towards a final goal.

**Key Insights:**
The image provides several key takeaways and supports significant conclusions about the HRM's capabilities, using the extracted textual and visual evidence:

*   **Takeaway 1: HRM Solves Diverse Tasks Iteratively.** The most prominent insight is the consistent display of "Timestep i = X" across all three distinct problem types: spatial pathfinding (MazeHard), logical deduction (Sudoku-Extreme), and abstract visual transformation (ARC-AGI-2). This directly supports the conclusion that the HRM employs a versatile and iterative problem-solving methodology, adaptable to varied cognitive challenges.

*   **Takeaway 2: Model's Reasoning Process is Observable and Interpretable.** By providing "intermediate predictions," the image allows for a degree of transparency into the AI's 

**Document Context:**
This image, located in "Section: 3.3 Visualization of intermediate timesteps," is a critical piece of evidence within the document, showcasing the practical application and operational mechanics of the Hypothesis-based Reasoning Model (HRM). It directly supports the document's broader narrative by providing visual proof of how the HRM processes information and arrives at solutions for diverse cognitive tasks.

The image's relevance lies in its ability to answer specific questions about the HRM's behavior:
*   **How does the model solve problems?** The sequential timesteps in each task (e.g., "Timestep i = 0" to "Timestep i = 6" in MazeHard) illustrate the iterative nature of the model's problem-solving. It doesn't instantly jump to a solution but progressively refines its state.
*   **How does the model handle constraints and errors?** In the Sudoku-Extreme task, the "red highlights cells violating Sudoku constraints" and "grey shading indicates changes from the previous timestep" provide concrete evidence of the model's ability to detect and correct errors, a crucial aspect of robust reasoning.
*   **How does the model perform abstract reasoning?** The ARC-AGI-2 tasks, with their "Example Input-Output pair" and subsequent transformations of "Test Input" over timesteps, demonstrate the HRM's capability to infer abstract rules and apply them creatively.

By providing a "clear, comprehensive explanation" and organizing the visual information from general concepts (iterative problem-solving) down to micro-details (specific cell values, color meanings), the image makes the HRM's complex internal processes more accessible and understandable to the reader. It reinforces the document's claims about the HRM's interpretability and reasoning prowess, moving beyond theoretical descriptions to concrete visual demonstrations.

**Summary:**
This figure, titled "Visualization of intermediate predictions by HRM on benchmark tasks," provides a detailed, step-by-step view of how a Hypothesis-based Reasoning Model (HRM) solves three different types of problems across multiple timesteps. The visualization is divided into three main sections, each demonstrating the model's progression on a specific task: MazeHard, Sudoku-Extreme, and ARC-AGI-2. The sequential "Timestep i = X" labels clearly indicate the iterative nature of the model's reasoning.

**Top Section: MazeHard (Pathfinding Task)**
This section shows the HRM solving a maze puzzle.
*   **Timestep i = 0:** The initial state of the maze is presented. It's a grid of black and white cells with a red square at the top, marking the starting point, and a green square near the bottom, indicating the destination. No path has been found yet.
*   **Timestep i = 1:** The model begins its search. A blue line, representing the predicted path, starts to emerge from the red starting square, moving downwards and slightly to the right.
*   **Timestep i = 2 to Timestep i = 5:** The blue path progressively extends, navigating through the intricate corridors of the maze. Each timestep shows the path growing longer and closer to the green destination, indicating the model's continuous exploration and hypothesis generation for the route.
*   **Timestep i = 6:** The model successfully finds a complete path. The blue line now fully connects the red starting point to the green destination, demonstrating the HRM's ability to solve spatial navigation problems.

**Middle Section: Sudoku-Extreme (Logic Puzzle Task)**
This section illustrates the HRM solving a challenging Sudoku puzzle, represented by a 10x9 grid. The process shows the model filling the grid, identifying errors, and correcting them.
*   **Initial:** The puzzle begins with an "Initial" grid where some numbers are already provided (givens), and many cells are empty. For example, Row 2 has '4' and '8 9', Row 3 has '7' and '3 1', and so on. These givens establish the starting constraints.
*   **Timestep i = 0:** The model makes an initial attempt to fill all empty cells in the grid with numbers. At this stage, many cells are highlighted in red (e.g., R2C4:6, R3C1:6, R4C1:8), signifying that the assigned numbers violate Sudoku rules (such as duplicates in a row, column, or 3x3 block). Cells with bold numbers (e.g., R2C1:4, R3C4:3) represent the initial givens, which remain constant.
*   **Timestep i = 1 to Timestep i = 6:** In these intermediate steps, the HRM iteratively adjusts the numbers to resolve the violations. Cells that have been changed from the previous timestep are shaded in grey (e.g., at Timestep i = 1, R4C2-R4C9 are grey-shaded; at Timestep i = 4, parts of R1, R2, R3, R7 are grey-shaded). As the timesteps progress, the model systematically works through the grid, reducing the number of red-highlighted constraint violations.
*   **Timestep i = 7:** The final state of the grid is shown. After multiple iterations of adjustments, there are no visible red-highlighted cells, indicating that all Sudoku constraints have been successfully satisfied. All grey-shaded cells (e.g., R10C4:9, R10C5:9) represent the final changes that led to the solution. This demonstrates the HRM's capability for logical deduction, constraint satisfaction, and self-correction.

**Bottom Sections: ARC-AGI-2 Task (Abstract Reasoning Tasks)**
These two sections demonstrate the HRM's ability to infer and apply abstract transformation rules based on visual examples. Each task presents an an example input-output pair, then shows the model transforming a test input over several timesteps.

*   **First ARC Task ([7666fa5d]):**
    *   **[7666fa5d] Example Input & Example Output:** A light blue grid with yellow diagonal pixels transforms into a grid with red and yellow blob-like shapes. This pair sets the rule for the task.
    *   **[7666fa5d] Test Input:** The input for the model is a light blue grid with dark red diagonal pixels.
    *   **Timestep i = 0:** The test input is displayed in its initial state.
    *   **Timestep i = 1 to Timestep i = 4:** The model progressively transforms the test input. The dark red pixels begin to change, expanding and merging to form a larger, more defined red shape. By **Timestep i = 4**, a complete red arrowhead/triangular shape is formed, which represents the model's derived output after applying the inferred transformation rule.

*   **Second ARC Task ([7b80bb43]):**
    *   **[7b80bb43] Example Input & Example Output:** A blue grid with pink L-shaped and straight line patterns guides the transformation.
    *   **[7b80bb43] Test Input:** The input consists of a blue grid with dark red vertical and horizontal line segments.
    *   **Timestep i = 0:** The test input is shown.
    *   **Timestep i = 1 to Timestep i = 6:** The model applies a sequence of transformations. The dark red line segments gradually change, extending, connecting, and reorienting. At **Timestep i = 2**, a small green pixel briefly appears, possibly an internal marker. By **Timestep i = 6**, the dark red patterns have been completely rearranged into a final, structured arrangement of connected L-shapes and lines, demonstrating the model's capacity to infer and execute complex spatial manipulation rules.

In summary, the entire figure provides a compelling visualization of the HRM's dynamic and iterative reasoning capabilities across a spectrum of cognitive challenges, highlighting its ability to adapt, learn, and self-correct through a series of measurable intermediate steps.](images/30c3cc5e6e2ad4e11a7cc7e72ebf63bc19a43478d18d4478f74a6f325239a754.jpg)
Figure 7: Visualization of intermediate predictions by HRM on benchmark tasks. Top: MazeHard—blue cells indicate the predicted path. Middle: Sudoku-Extreme—bold cells represent initial givens; red highlights cells violating Sudoku constraints; grey shading indicates changes from the previous timestep. Bottom: ARC-AGI-2 Task—left: provided example input-output pair; right: intermediate steps solving the test input.

Importantly, the model shows it can adapt different reasoning approaches, likely choosing an effective strategy for each particular task. Further research is needed to gain more comprehensive insights into these solution strategies.

# 4 Brain Correspondence

A key principle from systems neuroscience is that a brain region’s functional repertoire—its ability to handle diverse and complex tasks—is closely linked to the dimensionality of its neural representations75,76. Higher-order cortical areas, responsible for complex reasoning and decision-making, must handle a wide variety of tasks, demanding more flexible and context-dependent processing77 In dynamical systems, this flexibility is often realized through higher-dimensional state-space trajectories, which allow for a richer repertoire of potential computations 78. This principle gives rise to an observable dimensionality hierarchy, where a region’s position in the processing hierarchy correlates with its effective dimensionality. To quantify this phenomena, we can examine the

![## Image Analysis: 7cf92ea569bf25319ce5d52b1dfcdb06075ed6748bf8f3588e4d0a6ada158b52.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates the hierarchical organization of brain regions and their functional involvement, or "Participation Ratio (PR)," in neural activity or "trajectories" under different conditions. The main purpose is to investigate how the hierarchical position of brain regions correlates with their participation in neural processes and to compare this participation between "Low-Level" and "High-Level" neural states across a varying number of trajectories. Key ideas being communicated include the existence of a hierarchical structure within brain modules, the relationship between this hierarchy and a region's "Participation Ratio," and the differential engagement of "Low-Level" versus "High-Level" states in neural trajectories.

**Content Interpretation:**
The image presents several related concepts and relationships within the context of brain correspondence:

*   **Brain Regions and Modules (a):** This subplot displays a schematic view of different brain regions (e.g., VCo, VISp, AUDp, MOs, SSp-bfd, PL, ACAd) mapped onto a cortical surface. These regions are further classified into broader "Modules" which are either "Cognitive" (Medial, Lateral, Prefrontal) or "Sensory" (Visual, Auditory, Somatomotor). This provides an anatomical and categorical context for the regions analyzed in other plots. The detailed labels like "VISp" (primary visual cortex), "AUDp" (primary auditory cortex), "MOs" (secondary motor cortex), "PL" (prelimbic cortex), "ACAd" (anterior cingulate area, dorsal part) provide specific anatomical references for the data presented.

*   **Hierarchy and Participation Ratio Correlation (b):** This scatter plot shows a strong positive correlation (ρ = 0.79, P = 0.0003) between a brain region's "Position in the hierarchy" (x-axis) and its "Participation Ratio (PR)" (y-axis). The regions with higher hierarchical positions, such as "MOs" and "ACAd," exhibit higher Participation Ratios, whereas regions like "SSP-n" (primary somatosensory area, nose) show lower PRs and hierarchical positions. The color bar indicates that higher PR values (red) correspond to higher positions in the hierarchy, reinforcing the visual trend. This suggests that regions higher in the neural hierarchy tend to be more involved or 'participate' more in the overall neural dynamics being measured.

*   **Participation Ratio Across Trajectories for Different States (c & d):**
    *   Plot (c) illustrates the "Participation Ratio (PR)" as a function of the "Number of Trajectories" for "High-Level (z_H)" and "Low-Level (z_L)" states. The "High-Level (z_H)" state (orange line with circles) shows a significant and relatively steady increase in PR as the number of trajectories grows, reaching "89.95" at 100 trajectories. In contrast, the "Low-Level (z_L)" state (blue line with squares) shows a much lower PR that plateaus around "30.22" at 100 trajectories.
    *   Plot (d) is a bar chart that directly summarizes these end-point values: "Low-Level (z_L)" has a PR of "30.22," and "High-Level (z_H)" has a PR of "89.95." The significant difference between these values indicates that under the conditions represented by (c) and (d), high-level states exhibit substantially greater participation in neural trajectories compared to low-level states.

*   **Participation Ratio Under an Alternative Condition (e & f):**
    *   Plot (e) presents another scenario, similar to (c), showing "Participation Ratio (PR)" versus "Number of Trajectories" for "High-Level (z_H)" and "Low-Level (z_L)" states. However, in this case, both "Low-Level (z_L)" (blue line) and "High-Level (z_H)" (orange line) PRs increase with the number of trajectories and converge to similar values. At 100 trajectories, "z_L" is "42.09" and "z_H" is "40.75." The shaded areas around the lines likely indicate variability or confidence intervals.
    *   Plot (f) confirms these findings with a bar chart, showing "Low-Level (z_L)" at "42.09" and "High-Level (z_H)" at "40.75." This indicates that under the specific conditions underlying plots (e) and (f), there is a negligible difference, or even a slightly higher PR for low-level states, compared to high-level states. This contrasts sharply with the findings in (c) and (d), suggesting different types of "trajectories" or distinct experimental/computational contexts.

**Key Insights:**
The image provides several key takeaways and insights into brain organization and function:

*   **Hierarchical Organization is Linked to Functional Engagement:** A fundamental insight from plot (b) is that brain regions are organized hierarchically, and their "Position in the hierarchy" is strongly and positively correlated with their "Participation Ratio (PR)" (ρ = 0.79, P = 0.0003). This suggests that higher-level brain regions, such as "MOs" and "ACAd," are more globally involved or influential in neural processes compared to lower-level regions like "SSP-n."
*   **Context-Dependent Differential Participation of High vs. Low-Level States:** The comparison between plots (c)/(d) and (e)/(f) reveals a critical insight: the differential participation of "High-Level (z_H)" and "Low-Level (z_L)" states is highly context-dependent.
    *   In one context (c, d), "High-Level (z_H)" states show dramatically higher "Participation Ratio" (89.95) than "Low-Level (z_L)" states (30.22), indicating that high-level representations are significantly more involved in certain neural trajectories. This is evident from the values "z_H: 89.95" and "z_L: 30.22" in plot (c) and the corresponding bar heights in (d).
    *   However, in another context (e, f), the "Participation Ratio" for "Low-Level (z_L)" (42.09) and "High-Level (z_H)" (40.75) states are nearly indistinguishable, or even slightly inverted, at 100 trajectories. This implies that under different conditions or when considering different types of neural trajectories, the distinction in participation between high and low-level states can diminish or disappear. The annotations "z_L: 42.09" and "z_H: 40.75" in plot (e) and the bar chart in (f) provide direct evidence for this.
*   **Convergence of Participation Ratios:** Plots (c) and (e) both show that "Participation Ratio" tends to stabilize or reach a plateau as the "Number of Trajectories" increases. This suggests that a sufficient number of trajectories are needed to accurately measure the Participation Ratio, and beyond a certain point (e.g., around 80-100 trajectories), the observed PR values become stable, as seen by the flattening of the curves and the final values given at "X=100."

**Document Context:**
Given the section title "4 Brain Correspondence," this image is central to understanding how different brain regions interact and relate to each other in terms of their functional roles and hierarchical organization. The image specifically delves into the concept of "Participation Ratio" as a measure of a region's involvement in neural activity and explores how this measure is influenced by a region's hierarchical standing and the level of neural representation (low-level vs. high-level states). The distinct scenarios presented (plots c/d vs. e/f) likely correspond to different types of brain activity, tasks, or computational models being investigated within the broader theme of "Brain Correspondence."

**Summary:**
This figure comprehensively illustrates various aspects of brain organization and the involvement of different brain regions in neural activity, using "Participation Ratio (PR)" as a key metric. It is divided into six subplots, each providing a specific piece of information.

**Plot (a)**, titled "(a)", presents a schematic anatomical map of the brain, highlighting various cortical regions and classifying them into "Cognitive" (Medial, Lateral, Prefrontal) and "Sensory" (Visual, Auditory, Somatomotor) modules. Specific regions like "VISp," "AUDp," "MOs," and "ACAd" are labeled, providing an anatomical context for the subsequent analyses.

**Plot (b)**, titled "(b)", explores the relationship between a brain region's "Position in the hierarchy" and its "Participation Ratio (PR)." It shows a scatter plot where each point represents a specific brain region (e.g., "SSP-n," "GU," "MOp," "ACAd"). A strong positive correlation is evident, with statistical metrics ρ = 0.79 and P = 0.0003, indicating that regions higher in the functional hierarchy tend to exhibit a greater Participation Ratio. The color bar further reinforces this, associating higher PR values (red) with higher hierarchical positions.

**Plots (c) and (e)**, titled "(c)" and "(e)" respectively, are line graphs depicting how "Participation Ratio (PR)" changes as the "Number of Trajectories" increases for two distinct neural states: "High-Level (z_H)" (orange circles) and "Low-Level (z_L)" (blue squares).
*   In **Plot (c)**, the "High-Level (z_H)" state shows a significantly higher and increasing Participation Ratio, reaching "89.95" at 100 trajectories. In contrast, the "Low-Level (z_L)" state's PR remains much lower, plateauing around "30.22" at 100 trajectories.
*   **Plot (d)** is a bar chart that directly summarizes these findings from plot (c), showing a "Low-Level (z_L)" PR of "30.22" and a "High-Level (z_H)" PR of "89.95." This highlights a substantial difference in participation, with high-level states being much more engaged under these conditions.

*   In **Plot (e)**, however, a different scenario is presented. Here, both "High-Level (z_H)" and "Low-Level (z_L)" states show similar Participation Ratios as the number of trajectories increases, eventually converging. At 100 trajectories, "Low-Level (z_L)" has a PR of "42.09," and "High-Level (z_H)" has a PR of "40.75."
*   **Plot (f)**, a bar chart, summarizes the values from plot (e), confirming the near equivalence, or even a slight inversion, with "Low-Level (z_L)" at "42.09" and "High-Level (z_H)" at "40.75." This striking difference between the results in (c)/(d) and (e)/(f) suggests that the differential involvement of high-level versus low-level states is highly context-dependent, possibly reflecting different types of neural processes or experimental conditions.

In summary, the figure elucidates that brain regions are hierarchically organized, with higher-level regions generally showing greater participation in neural activity. Crucially, the extent to which "High-Level" and "Low-Level" neural states differ in their participation is not constant but varies significantly depending on the specific context or type of neural trajectories being considered. This provides detailed insights into the complex dynamics of brain correspondence.](images/7cf92ea569bf25319ce5d52b1dfcdb06075ed6748bf8f3588e4d0a6ada158b52.jpg)

Figure 8: Hierarchical Dimensionality Organization in the HRM and Mouse Cortex. (a,b) are adapted from Posani et al. 74. (a) Anatomical illustration of mouse cortical areas, color-coded by functional modules. (b) Correlation between Participation Ratio (PR), a measure of effective neural dimensionality, and hierarchical position across different mouse cortical areas. Higher positions in the hierarchy (e.g., MOs, ACAd) exhibit significantly higher PR values compared to lower sensory areas (e.g., SSp-n), with a Spearman correlation coefficient of $\rho { = } 0 . 7 9$ $\scriptstyle \mathrm { { P = 0 . 0 0 0 3 } }$ ). (c,d) Trained HRM. (c) PR scaling of the trained HRM with task diversity. The dimensionality of the highlevel module $( z _ { H } )$ scales with the number of unique tasks (trajectories) included in the analysis, indicating an adaptive expansion of its representational capacity. In contrast, the low-level module’s $( z _ { L } )$ dimensionality remains stable. (d) PR values for the low-level ( $z _ { L } , \mathrm { P R } = 3 0 . 2 2 )$ and highlevel ( $z _ { H }$ , $\mathrm { P R } = 8 9 . 9 5 ) $ ) modules of the trained HRM, computed from neural activity during 100 unique Sudoku-solving trajectories. A clear dimensionality hierarchy is observed, with the highlevel module operating in a substantially higher-dimensional space. (e,f) Analysis of Untrained Network. To verify that the dimensionality hierarchy is an emergent property of training, the same analyses were performed on an untrained HRM with random weights. (e) In contrast to the trained model’s scaling in (c), the dimensionality of both modules in the untrained model remains low and stable, failing to scale with the number of tasks. (f) Similarly, contrasting with the clear separation in (d), the PR values for the untrained model’s modules ( $z _ { L }$ , $\mathrm { P R } = 4 2 . 0 9 $ ; $z _ { H }$ , $ { \mathrm { P R } } = 4 0 . 7 5 ) ,$ ) are low and nearly identical, showing no evidence of hierarchical separation. This confirms that the observed hierarchical organization of dimensionality is a learned property that emerges through training, not an artifact of the model’s architecture.

Participation Ratio (PR) which serves as a standard measure of the effective dimensionality of a high-dimensional representation79. The PR is calculated using the formula

$$
\mathrm { P R } = \frac { ( \sum _ { i } \lambda _ { i } ) ^ { 2 } } { \sum _ { i } \lambda _ { i } ^ { 2 } } ,
$$

where $\{ \lambda _ { i } \}$ are the eigenvalues of the covariance matrix of neural trajectories. Intuitively, a higher PR value signifies that variance is distributed more evenly across many dimensions, corresponding to a higher-dimensional representation. Conversely, a lower PR value indicates that variance is concentrated in only a few principal components, reflecting a more compact, lower-dimensional structure.

The dimensionality hierarchy can be observed, for example, in the mouse cortex, where the PR of population activity increases monotonically from low-level sensory areas to high-level associative areas, supporting this link between dimensionality and functional complexity74 (Figure 8 a,b).

We evaluated whether HRM reproduces this neuroscientific principle by calculating the PR for both recurrent modules after training on the Sudoku-Extreme Full dataset. The PR computation used the covariance matrix derived from neural states gathered across multiple Sudoku-solving trajectories. The results show a striking parallel to the biological findings. The low-level module’s state $( z _ { L } )$ occupies a relatively small subspace with a participation ratio of 30.22, whereas the highlevel module’s state $( z _ { H } )$ operates in a substantially larger subspace with a participation ratio of 89.95, as shown in Figure 8-(c). Furthermore, Figure 8-(d) shows that increasing the number of unique tasks (trajectories) from 10 to 100 causes $z _ { H }$ dimensionality to scale up accordingly, while $z _ { L }$ dimensionality remains stable. These results suggest an emergent separation of representational capacity between the modules that parallels their functional roles.

To confirm that this hierarchical organization is an emergent property of training, and not an artifact of the network’s architecture, we performed a control analysis using an identical but untrained network with random weights.

We initialized an identical HRM architecture with random weights and, without any training, measured the PR of its modules as the network processed the same task-specific inputs given to the trained model.

The results, shown in Figure 8-(e,f), reveal a stark contrast: the high-level and low-level modules of the untrained network exhibit no hierarchical separation, with their PR values remaining low and nearly indistinguishable from each other. This control analysis validates that the dimensionality hierarchy is an emergent property that arises as the model learns to perform complex reasoning.

The high-to-low PR ratio in HRM $( z _ { H } / z _ { L } \approx 2 . 9 8 )$ closely matches that measured in the mouse cortex $( \approx ~ 2 . 2 5 )$ . In contrast, conventional deep networks often exhibit neural collapse, where last-layer features converge to a low-dimensional subspace 80,81,82. HRM therefore departs from the collapse pattern and instead fosters a high-dimensional representation in its higher module. This is significant because such representations are considered crucial for cognitive flexibility and are a hallmark of higher-order brain regions like the prefrontal cortex (PFC), which is central to complex reasoning.

This structural parallel suggests the model has discovered a fundamental organizational principle. By learning to partition its representations into a high-capacity, high-dimensional subspace $( z _ { H } )$ and a more specialized, low-dimensional one $( z _ { L } )$ , HRM autonomously discovers an organizational principle that is thought to be fundamental for achieving robust and flexible reasoning in biological systems. This provides a potential mechanistic explanation for the model’s success on complex, long-horizon tasks that are intractable for models lacking such a differentiated internal structure. We emphasize, however, that this evidence is correlational. While a causal link could be tested via intervention (e.g., by constraining the H-module’s dimensionality), such methods are difficult to interpret in deep learning due to potential confounding effects on the training process itself. Thus, the causal necessity of this emergent hierarchy remains an important question for future investigation.

# 5 Related Work

Reasoning and algorithm learning Given the central role of reasoning problems and their close relation to algorithms, researchers have long explored neural architectures that enable algorithm learning from training instances. This line of work includes Neural Turing Machines (NTM)83, the Differentiable Neural Computer $\mathrm { ( D N C ) } ^ { 8 4 }$ , and Neural GPUs85–all of which construct iterative neural architectures that mimic computational hardware for algorithm execution, and are trained to learn algorithms from data. Another notable work in this area is Recurrent Relational Networks (RRN)62, which executes algorithms on graph representations through graph neural networks.

Recent studies have integrated algorithm learning approaches with Transformer-based architectures. Universal Transformers extend the standard Transformer model by introducing a recurrent loop over the layers and implementing an adaptive halting mechanism. Geiping et al. 86 demonstrate that looped Transformers can generalize to a larger number of recurrent steps during inference than what they were trained on. Shen et al. 16 propose adding continuous recurrent reasoning tokens to the Transformer. Finally, TransNAR8 combine recurrent graph neural networks with language models.

Building on the success of CoT-based reasoning, a line of work have introduced fine-tuning methods that use reasoning paths from search algorithms (like $\mathbf { A } ^ { * }$ ) as SFT targets87,71,70.

We also mention adaptive halting mechanisms designed to allocate additional computational resources to more challenging problems. This includes the Adaptive Computation Time (ACT) for $\mathrm { R N N s } ^ { 8 8 }$ and follow-up research like PonderNet 89, which aims to improve the stability of this allocation process.

HRM further pushes the boundary of algorithm learning through a brain-inspired computational architecture that achieves exceptional data efficiency and model expressiveness, successfully discovering complex and diverse algorithms from just 1000 training examples.

Brain-inspired reasoning architectures Developing a model with the reasoning power of the brain has long been a goal in brain-inspired computing. Spaun90 is one notable example, which uses spiking neural networks to create distinct modules corresponding to brain regions like the visual cortex and prefrontal cortex. This design enables an architecture to perform a range of cognitive tasks, from memory recall to simple reasoning puzzles. However, its reasoning relies on handdesigned algorithms, which may limit its ability to learn new tasks. Another significant model is the Tolman-Eichenbaum Machine (TEM) 91, which is inspired by the hippocampal-entorhinal system’s role in spatial and relational memory tasks. TEM proposes that medial entorhinal cells create a basis for structural knowledge, while hippocampal cells link this basis to sensory information. This allows TEM to generalize and explains the emergence of various cell types like grid, border, and place cells. Another approach involves neural sampling models 92, which view the neural signaling process as inference over a distribution, functioning similarly to a Boltzmann machine. These models often require hand-made rules to be set up for solving a specific reasoning task. In essence, while prior models are restricted to simple reasoning problems, HRM is designed to solve complex tasks that are hard for even advanced LLMs, without pre-training or task-specific manual design.

Hierarchical memory The hierarchical multi-timescale structure plays also an important role in how the brain processes memory. Models such as Hierarchical Sequential Models93 and Clockwork $\mathrm { R N N } ^ { 9 4 }$ use multiple recurrent modules that operate at varying time scales to more effectively capture long-range dependencies within sequences, thereby mitigating the forgetting issue in RNNs.

Similar mechanisms have also been adopted in linear attention methods for memorizing long contexts (see the Discussions section). Since HRM focuses on reasoning, full attention is applied for simplicity. Incorporating hierarchical memory into HRM could be a promising future direction.

# 6 Discussions

Turing-completeness of HRM Like earlier neural reasoning algorithms including the Universal Transformer95, HRM is computationally universal when given sufficient memory and time constraints. In other words, it falls into the category of models that can simulate any Turing machine, overcoming the computational limitations of standard Transformers discussed previously in the introduction. Given that earlier neural algorithm reasoners were trained as recurrent neural networks, they suffer from premature convergence and memory intensive BPTT. Therefore, in practice, their effective computational depth remains limited, though still deeper than that of a standard Transformer. By resolving these two challenges and being equipped with adaptive computation, HRM could be trained on long reasoning processes, solve complex puzzles requiring intensive depth-first search and backtracking, and move closer to practical Turing-completeness.

Reinforcement learning with chain-of-thought Beyond fine-tuning using human-annotated CoT, reinforcement learning (RL) represents another widely adopted training methodology. However, recent evidence suggests that RL primarily unlock existing CoT-like capabilities rather than discovering fundamentally new reasoning mechanisms 96,97,98,99. Additionally, CoT-training with RL is known for its instability and data inefficiency, often requiring extensive exploration and careful reward design. In contrast, HRM takes feedback from dense gradient-based supervision rather than relying on a sparse reward signal. Moreover, HRM operates naturally in a continuous space, which is biologically plausible and avoids allocating same computational resources to each token, even though tokens vary in their reasoning and planning complexity16.

Linear attention Recurrence has been explored not only for its capability in universal computation, but also as a means to replace the attention mechanism in Transformers, which suffers from quadratic time and memory complexity100. Recurrent alternatives offer a more efficient design by processing input tokens sequentially and predicting the next token at each time step, similar to early RNN-based language models.

Some linear-attention variants, such as Log-linear Attention101, share an RNN-like state-update that can be interpreted as propagating multi-timescale summary statistics, thereby retaining long-range context without the quadratic memory growth of standard self-attention. However, substituting the attention mechanism alone does not change the fact that Transformers are still fixed-depth, and require CoT as a compensatory mechanism. Notably, linear attention can operate with a reduced key-value cache over extended contexts, making them more suitable for deployment on resourceconstrained edge devices.

# 7 Conclusion

This work introduces the Hierarchical Reasoning Model, a brain-inspired architecture that leverages hierarchical structure and multi-timescale processing to achieve substantial computational depth without sacrificing training stability or efficiency. With only 27M parameters and training on just 1000 examples, HRM effectively solves challenging reasoning problems such as ARC, Sudoku, and complex maze navigation–tasks that typically pose significant difficulties for contemporary LLM and chain-of-thought models.

Although the brain relies heavily on hierarchical structures to enable most cognitive processes, these concepts have largely remained confined to academic literature rather than being translated into practical applications. The prevailing AI approach continues to favor non-hierarchical models. Our results challenge this established paradigm and suggest that the Hierarchical Reasoning Model represents a viable alternative to the currently dominant chain-of-thought reasoning methods, advancing toward a foundational framework capable of Turing-complete universal computation.

Acknowledgements We thank Mingli Yuan, Ahmed Murtadha Hasan Mahyoub and Hengshuai Yao for their insightful discussions and valuable feedback throughout the course of this work.

References   
1. Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep Learning. MIT Press, 2016. http://www.deeplearningbook.org.   
2. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770–778, 2015. URL https://api.semanticscholar.org/CorpusID:206594692.   
3. Lena Strobl. Average-hard attention transformers are constant-depth uniform threshold circuits, 2023.   
4. Tom Bylander. Complexity results for planning. In Proceedings of the 12th International Joint Conference on Artificial Intelligence - Volume 1, IJCAI’91, page 274–279, San Francisco, CA, USA, 1991. Morgan Kaufmann Publishers Inc. ISBN 1558601600.   
5. William Merrill and Ashish Sabharwal. A logic for expressing log-precision transformers. In Neural Information Processing Systems, 2023.   
6. David Chiang. Transformers in DLOGTIME-uniform TC0. Transactions on Machine Learning Research, 2025.   
7. Lucas Lehnert, Sainbayar Sukhbaatar, Paul Mcvay, Michael Rabbat, and Yuandong Tian. Beyond a\*: Better planning with transformers via search dynamics bootstrapping. ArXiv, abs/2402.14083, 2024. URL https://api.semanticscholar.org/CorpusID: 267782588.   
8. Wilfried Bounsi, Borja Ibarz, Andrew Dudzik, Jessica B. Hamrick, Larisa Markeeva, Alex Vitvitskyi, Razvan Pascanu, and Petar Velivckovi’c. Transformers meet neural algorithmic reasoners. ArXiv, abs/2406.09308, 2024. URL https://api.semanticscholar.org/ CorpusID:270440310.   
9. William Merrill and Ashish Sabharwal. The parallelism tradeoff: Limitations of log-precision transformers. Transactions of the Association for Computational Linguistics, 11:531–545, 2023. doi: 10.1162/tacl_a_00562. URL https://aclanthology.org/2023.tacl-1.31/.   
10. Jason Wei, Yi Tay, et al. Chain-of-thought prompting elicits reasoning in large language models, 2022. arXiv preprint arXiv:2201.11903.   
11. William Merrill and Ashish Sabharwal. The expressive power of transformers with chain of thought. In ICLR, 2024.   
12. Xinyun Chen, Ryan A. Chi, Xuezhi Wang, and Denny Zhou. Premise order matters in reasoning with large language models. ArXiv, abs/2402.08939, 2024. URL https: //api.semanticscholar.org/CorpusID:267657940.   
13. Rongwu Xu, Zehan Qi, and Wei Xu. Preemptive answer "attacks" on chain-of-thought reasoning. In Annual Meeting of the Association for Computational Linguistics, 2024. URL https://api.semanticscholar.org/CorpusID:270199922.   
14. Pablo Villalobos, Jaime Sevilla, Lennart Heim, Tamay Besiroglu, Marius Hobbhahn, and An Chang Ho. Will we run out of data? limits of llm scaling based on human-generated data. 2022. URL https://api.semanticscholar.org/CorpusID:253397775.   
15. Xinghao Chen, Anhao Zhao, Heming Xia, Xuan Lu, Hanlin Wang, Yanjun Chen, Wei Zhang, Jian Wang, Wenjie Li, and Xiaoyu Shen. Reasoning beyond language: A comprehensive survey on latent chain-of-thought reasoning. 2025. URL https://api.semanticscholar. org/CorpusID:278789461.   
16. Xuan Shen, Yizhou Wang, Xiangxi Shi, Yanzhi Wang, Pu Zhao, and Jiuxiang Gu. Training large language models to reason in a continuous latent space. arXiv preprint arXiv:2412.07423, 2024.   
17. Evelina Fedorenko, Steven T Piantadosi, and Edward AF Gibson. Language is primarily a tool for communication rather than thought. Nature, 630(8017):575–586, 2024.   
18. Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. Deepnet: Scaling transformers to 1,000 layers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.   
19. Timothy P Lillicrap and Adam Santoro. Backpropagation through time and the brain. Current Opinion in Neurobiology, 55:82–89, 2019. ISSN 0959-4388. doi: https://doi.org/10.1016/ j.conb.2019.01.011. URL https://www.sciencedirect.com/science/article/pii/ S0959438818302009. Machine Learning, Big Data, and Neuroscience.   
20. John D Murray, Alberto Bernacchia, David J Freedman, Ranulfo Romo, Jonathan D Wallis, Xinying Cai, Camillo Padoa-Schioppa, Tatiana Pasternak, Hyojung Seo, Daeyeol Lee, et al. A hierarchy of intrinsic timescales across primate cortex. Nature neuroscience, 17(12):1661– 1663, 2014.   
21. Roxana Zeraati, Yan-Liang Shi, Nicholas A Steinmetz, Marc A Gieselmann, Alexander Thiele, Tirin Moore, Anna Levina, and Tatiana A Engel. Intrinsic timescales in the visual cortex change with selective attention and reflect spatial connectivity. Nature communications, 14(1):1858, 2023.   
22. Julia M Huntenburg, Pierre-Louis Bazin, and Daniel S Margulies. Large-scale gradients in human cortical organization. Trends in cognitive sciences, 22(1):21–31, 2018.   
23. Victor AF Lamme and Pieter R Roelfsema. The distinct modes of vision offered by feedforward and recurrent processing. Trends in neurosciences, 23(11):571–579, 2000.   
24. Andre M Bastos, W Martin Usrey, Rick A Adams, George R Mangun, Pascal Fries, and Karl J Friston. Canonical microcircuits for predictive coding. Neuron, 76(4):695–711, 2012.   
25. Klara Kaleb, Barbara Feulner, Juan Gallego, and Claudia Clopath. Feedback control guides credit assignment in recurrent neural networks. Advances in Neural Information Processing Systems, 37:5122–5144, 2024.   
26. Timothy P Lillicrap, Adam Santoro, Luke Marris, Colin J Akerman, and Geoffrey Hinton. Backpropagation and the brain. Nature Reviews Neuroscience, 21(6):335–346, 2020.   
27. François Chollet. On the measure of intelligence (abstraction and reasoning corpus), 2019. arXiv preprint arXiv:1911.01547.   
28. Francois Chollet, Mike Knoop, Gregory Kamradt, and Bryan Landers. Arc prize 2024: Technical report. ArXiv, abs/2412.04604, 2024. URL https://api.semanticscholar. org/CorpusID:274581906.   
29. Francois Chollet, Mike Knoop, Gregory Kamradt, Bryan Landers, and Henry Pinkard. Arcagi-2: A new challenge for frontier ai reasoning systems. 2025. URL https://api. semanticscholar.org/CorpusID:278740984.   
30. György Buzsáki. Gamma, alpha, delta, and theta oscillations govern cognitive processes. International Journal of Psychophysiology, 39:241–248, 2000.   
31. György Buzsáki. Rhythms of the Brain. Oxford university press, 2006.   
32. Anja Pahor and Norbert Jaušovec. Theta–gamma cross-frequency coupling relates to the level of human intelligence. Intelligence, 46:283–290, 2014.   
33. Adriano BL Tort, Robert W Komorowski, Joseph R Manns, Nancy J Kopell, and Howard Eichenbaum. Theta–gamma coupling increases during the learning of item–context associations. Proceedings of the National Academy of Sciences, 106(49):20942–20947, 2009.   
34. Benjamin Scellier and Yoshua Bengio. Equilibrium propagation: Bridging the gap between energy-based models and backpropagation. Frontiers in Computational Neuroscience, 11,   
2016. URL https://api.semanticscholar.org/CorpusID:139945.   
35. Guillaume Bellec, Franz Scherr, Anand Subramoney, Elias Hajek, Darjan Salaj, Robert Legenstein, and Wolfgang Maass. A solution to the learning dilemma for recurrent networks of spiking neurons. Nature Communications, 11, 07 2020. doi: 10.1038/ s41467-020-17236-y.   
36. Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Deep equilibrium models. In Advances in Neural Information Processing Systems, pages 690–701, 2019.   
37. Zhengyang Geng, Xinyu Zhang, Shaojie Bai, Yisen Wang, and Zhouchen Lin. On training implicit models. ArXiv, abs/2111.05177, 2021. URL https://api.semanticscholar. org/CorpusID:243861133.   
38. Katarina Begus and Elizabeth Bonawitz. The rhythm of learning: Theta oscillations as an index of active learning in infancy. Developmental Cognitive Neuroscience, 45:100810,   
2020. ISSN 1878-9293. doi: https://doi.org/10.1016/j.dcn.2020.100810. URL https: //www.sciencedirect.com/science/article/pii/S187892932030058X.   
39. Shaojie Bai, Zhengyang Geng, Yash Savani, and J. Zico Kolter. Deep Equilibrium Optical Flow Estimation . In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 610–620, 2022.   
40. Zaccharie Ramzi, Florian Mannel, Shaojie Bai, Jean-Luc Starck, Philippe Ciuciu, and Thomas Moreau. Shine: Sharing the inverse estimate from the forward pass for bi-level optimization and implicit models. ArXiv, abs/2106.00553, 2021. URL https://api. semanticscholar.org/CorpusID:235266229.   
41. Shaojie Bai, Vladlen Koltun, and J. Zico Kolter. Stabilizing equilibrium models by jacobian regularization. In International Conference on Machine Learning, 2021. URL https:// api.semanticscholar.org/CorpusID:235632013.   
42. Daniel Kahneman and P Egan. Thinking, fast and slow (farrar, straus and giroux, new york),   
2011.   
43. Matthew D Lieberman. Social cognitive neuroscience: a review of core processes. Annu. Rev. Psychol., 58(1):259–289, 2007.   
44. Randy L Buckner, Jessica R Andrews-Hanna, and Daniel L Schacter. The brain’s default network: anatomy, function, and relevance to disease. Annals of the new York Academy of Sciences, 1124(1):1–38, 2008.   
45. Marcus E Raichle. The brain’s default mode network. Annual review of neuroscience, 38(1):   
433–447, 2015.   
46. Andrew Westbrook and Todd S Braver. Cognitive effort: A neuroeconomic approach. Cognitive, Affective, & Behavioral Neuroscience, 15:395–415, 2015.   
47. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, MA, 2018.   
48. Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin A. Riedmiller. Playing atari with deep reinforcement learning. ArXiv, abs/1312.5602, 2013. URL https://api.semanticscholar.org/CorpusID:15238391.   
49. Matteo Gallici, Mattie Fellows, Benjamin Ellis, Bartomeu Pou, Ivan Masmitja, Jakob Nicolaus Foerster, and Mario Martin. Simplifying deep temporal difference learning, 2025. URL https://arxiv.org/abs/2407.04811.   
50. Shuo Xie and Zhiyuan Li. Implicit bias of adamw: Linf norm constrained optimization. ArXiv, abs/2404.04454, 2024. URL https://api.semanticscholar.org/CorpusID: 269004841.   
51. Lucas Prieto, Melih Barsbey, Pedro A. M. Mediano, and Tolga Birdal. Grokking at the edge of numerical stability. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id $\cdot ^ { = }$ TvfkSyHZRA.   
52. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998–6008, 2017.   
53. Meta AI. Llama 3: State-of-the-art open weight language models. Technical report, Meta, 2024. URL https://ai.meta.com/llama/.   
54. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024.   
55. Noam M. Shazeer. Glu variants improve transformer. ArXiv, abs/2002.05202, 2020. URL https://api.semanticscholar.org/CorpusID:211096588.   
56. Biao Zhang and Rico Sennrich. Root mean square layer normalization. ArXiv, abs/1910.07467, 2019. URL https://api.semanticscholar.org/CorpusID: 113405151.   
57. Günter Klambauer, Thomas Unterthiner, Andreas Mayr, and Sepp Hochreiter. Selfnormalizing neural networks. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar.org/CorpusID:13713980.   
58. JAX Developers. jax.nn.initializers.lecun_normal. Google Research, 2025. URL https://docs.jax.dev/en/latest/_autosummary/jax.nn.initializers.lecun_ normal.html. Accessed June 22, 2025.   
59. Yann LeCun, Léon Bottou, Genevieve B Orr, and Klaus-Robert Müller. Efficient backprop. In Neural networks: Tricks of the trade, pages 9–50. Springer, 2002.   
60. Katie E Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu, Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, and Jeffrey Pennington. Scaling exponents across parameterizations and optimizers. In Forty-first International Conference on Machine Learning, 2024.   
61. Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017.   
62. Rasmus Berg Palm, Ulrich Paquet, and Ole Winther. Recurrent relational networks. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar. org/CorpusID:46208513.   
63. Jieyi Long. Large language model guided tree-of-thought. ArXiv, abs/2305.08291, 2023. URL https://api.semanticscholar.org/CorpusID:258686311.   
64. Yilun Du, Jiayuan Mao, and Josh Tenenbaum. Learning iterative reasoning through energy diffusion. ArXiv, abs/2406.11179, 2024. URL https://api.semanticscholar.org/ CorpusID:270560003.   
65. Kyubyong Park. Can convolutional neural networks crack sudoku puzzles? https: //github.com/Kyubyong/sudoku, 2018.   
66. Single-digit techniques. https://hodoku.sourceforge.net/en/tech_singles.php. Accessed: 2025-06-16.   
67. Tom Dillion. Tdoku: A fast sudoku solver and generator. https://t-dillon.github.io/ tdoku/, 2025.   
68. Jeffrey Seely, Yuki Imajuku, Tianyu Zhao, Edoardo Cetin, and Llion Jones. Sudokubench: Evaluating creative reasoning with sudoku variants. 2025. URL https://api. semanticscholar.org/CorpusID:278788644.   
69. Luke Darlow, Ciaran Regan, Sebastian Risi, Jeffrey Seely, and Llion Jones. Continuous thought machines. 2025. URL https://api.semanticscholar.org/CorpusID: 278481338.   
70. DiJia Su, Sainbayar Sukhbaatar, Michael Rabbat, Yuandong Tian, and Qinqing Zheng. Dualformer: Controllable fast and slow thinking by learning with randomized reasoning traces, 2025. URL https://arxiv.org/abs/2410.09918.   
71. Lucas Lehnert, Sainbayar Sukhbaatar, DiJia Su, Qinqing Zheng, Paul McVay, Michael Rabbat, and Yuandong Tian. Beyond a\*: Better planning with transformers via search dynamics bootstrapping. In First Conference on Language Modeling, 2024. URL https: //openreview.net/forum?id $\cdot ^ { = }$ SGoVIC0u0f.   
72. Mubbasir Kapadia, Francisco Garcia, Cory D. Boatright, and Norman I. Badler. Dynamic search on the gpu. In 2013 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 3332–3337, 2013. doi: 10.1109/IROS.2013.6696830.   
73. Isaac Liao and Albert Gu. Arc-agi without pretraining, 2025. URL https: //iliao2345.github.io/blog_posts/arc_agi_without_pretraining/arc_agi_ without_pretraining.html.   
74. Lorenzo Posani, Shuqi Wang, Samuel P Muscinelli, Liam Paninski, and Stefano Fusi. Rarely categorical, always high-dimensional: how the neural code changes along the cortical hierarchy. bioRxiv, pages 2024–11, 2025.   
75. Mattia Rigotti, Omri Barak, Melissa R. Warden, Xiao-Jing Wang, Nathaniel D. Daw, Earl K. Miller, and Stefano Fusi. The importance of mixed selectivity in complex cognitive tasks. Nature, 497:585–590, 2013. doi: 10.1038/nature12160.   
76. Valerio Mante, David Sussillo, Krishna V. Shenoy, and William T. Newsome. Contextdependent computation by recurrent dynamics in prefrontal cortex. Nature, 503(7474):78–84, 2013. doi: 10.1038/nature12742.   
77. Earl K. Miller and Jonathan D. Cohen. An integrative theory of prefrontal cortex function. Annual Review of Neuroscience, 24(1):167–202, 2001. doi: 10.1146/annurev.neuro.24.1.167.   
78. Wolfgang Maass. Real-time computing without stable states: a new framework for neural computation based on perturbations. Neural Computation, 14(11):2531–2560, 2002. doi: 10.1162/089976602760407955.   
79. Ege Altan, Sara A. Solla, Lee E. Miller, and Eric J. Perreault. Estimating the dimensionality of the manifold underlying multi-electrode neural recordings. PLoS Computational Biology, 17(11):e1008591, 2021. doi: 10.1371/journal.pcbi.1008591.   
80. Vardan Papyan, X. Y. Han, and David L. Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020. doi: 10.1073/pnas.2015509117.   
81. Cong Fang, Hangfeng He, Qi Long, and Weijie J. Su. Exploring deep neural networks via layer–peeled model: Minority collapse in imbalanced training. Proceedings of the National Academy of Sciences, 118(43):e2103091118, 2021. doi: 10.1073/pnas.2103091118.   
82. Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. In Advances in Neural Information Processing Systems, volume 34 of NeurIPS, pages 29820–29834, 2021.   
83. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines, 2014.   
84. Wayne G. Reynolds M. et al. Graves, A. Hybrid computing using a neural network with dynamic external memory. Nature, 538:471–476, 2016.   
85. Lukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In ICLR, 2016.   
86. Jonas Geiping, Sean McLeish, Neel Jain, John Kirchenbauer, Siddharth Singh, Brian R. Bartoldson, Bhavya Kailkhura, Abhinav Bhatele, and Tom Goldstein. Scaling up test-time compute with latent reasoning: A recurrent depth approach, 2025.   
87. Tiedong Liu and Kian Hsiang Low. Goat: Fine-tuned llama outperforms gpt-4 on arithmetic tasks. ArXiv, abs/2305.14201, 2023. URL https://api.semanticscholar. org/CorpusID:258840942.   
88. Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016. URL https://api.semanticscholar.org/CorpusID:8224916.   
89. Andrea Banino, Jan Balaguer, and Charles Blundell. Pondernet: Learning to ponder. ArXiv, abs/2107.05407, 2021. URL https://api.semanticscholar.org/CorpusID: 235795251.   
90. Chris Eliasmith, Terrence C Stewart, Xuan Choo, Trevor Bekolay, Travis DeWolf, Yichuan Tang, and Daniel Rasmussen. A large-scale model of the functioning brain. science, 338 (6111):1202–1205, 2012.   
91. James CR Whittington, Timothy H Muller, Shirley Mark, Guifen Chen, Caswell Barry, Neil Burgess, and Timothy EJ Behrens. The tolman-eichenbaum machine: unifying space and relational memory through generalization in the hippocampal formation. Cell, 183(5):1249– 1263, 2020.   
92. Lars Buesing, Johannes Bill, Bernhard Nessler, and Wolfgang Maass. Neural dynamics as sampling: a model for stochastic computation in recurrent networks of spiking neurons. PLoS computational biology, 7(11):e1002211, 2011.   
93. Salah Hihi and Yoshua Bengio. Hierarchical recurrent neural networks for longterm dependencies. In D. Touretzky, M.C. Mozer, and M. Hasselmo, editors, Advances in Neural Information Processing Systems, volume 8. MIT Press, 1995. URL https://proceedings.neurips.cc/paper_files/paper/1995/file/ c667d53acd899a97a85de0c201ba99be-Paper.pdf.   
94. Jan Koutník, Klaus Greff, Faustino J. Gomez, and Jürgen Schmidhuber. A clockwork rnn. In International Conference on Machine Learning, 2014. URL https://api. semanticscholar.org/CorpusID:14936429.   
95. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers, 2018. arXiv preprint arXiv:1807.03819.   
96. Yiping Wang, Qing Yang, Zhiyuan Zeng, Liliang Ren, Lucas Liu, Baolin Peng, Hao Cheng, Xuehai He, Kuan Wang, Jianfeng Gao, Weizhu Chen, Shuohang Wang, Simon Shaolei Du, and Yelong Shen. Reinforcement learning for reasoning in large language models with one training example, 2025. URL https://arxiv.org/abs/2504.20571.   
97. Niklas Muennighoff. s1: Simple test-time scaling. arXiv preprint arXiv:2502.23456, 2025. URL https://arxiv.org/abs/2502.23456.   
98. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. Light-r1: Curriculum sft, dpo and rl for long cot from scratch and beyond, 2025. URL https://arxiv.org/abs/2503.10460.   
99. Xuefeng Li, Haoyang Zou, and Pengfei Liu. Limr: Less is more for rl scaling, 2025. URL https://arxiv.org/abs/2502.11886.   
100. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. ArXiv, abs/2405.21060, 2024. URL https://api. semanticscholar.org/CorpusID:270199762.   
101. Han Guo, Songlin Yang, Tarushii Goel, Eric P Xing, Tri Dao, and Yoon Kim. Log-linear attention. arXiv preprint arXiv:2506.04761, 2025.