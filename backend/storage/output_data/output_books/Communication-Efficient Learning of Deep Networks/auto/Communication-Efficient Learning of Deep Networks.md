# Communication-Efficient Learning of Deep Networks from Decentralized Data

# H. Brendan McMahan

Eider Moore Daniel Ramage Seth Hampson Google, Inc., 651 N 34th St., Seattle, WA 98103 USA

Blaise Aguera y Arcas ¨

# Abstract

Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning.

We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by $1 0 { - } 1 0 0 \times$ as compared to synchronized stochastic gradient descent.

# 1 Introduction

Increasingly, phones and tablets are the primary computing devices for many people [30, 2]. The powerful sensors on these devices (including cameras, microphones, and GPS), combined with the fact they are frequently carried, means they have access to an unprecedented amount of data, much of it private in nature. Models learned on such data hold the promise of greatly improving usability by powering more intelligent applications, but the sensitive nature of the data means there are risks and responsibilities to storing it in a centralized location.

We investigate a learning technique that allows users to collectively reap the benefits of shared models trained from this rich data, without the need to centrally store it. We term our approach Federated Learning, since the learning task is solved by a loose federation of participating devices (which we refer to as clients) which are coordinated by a central server. Each client has a local training dataset which is never uploaded to the server. Instead, each client computes an update to the current global model maintained by the server, and only this update is communicated. This is a direct application of the principle of focused collection or data minimization proposed by the 2012 White House report on privacy of consumer data [39]. Since these updates are specific to improving the current model, there is no reason to store them once they have been applied.

A principal advantage of this approach is the decoupling of model training from the need for direct access to the raw training data. Clearly, some trust of the server coordinating the training is still required. However, for applications where the training objective can be specified on the basis of data available on each client, federated learning can significantly reduce privacy and security risks by limiting the attack surface to only the device, rather than the device and the cloud.

Our primary contributions are 1) the identification of the problem of training on decentralized data from mobile devices as an important research direction; 2) the selection of a straightforward and practical algorithm that can be applied to this setting; and 3) an extensive empirical evaluation of the proposed approach. More concretely, we introduce the FederatedAveraging algorithm, which combines local stochastic gradient descent (SGD) on each client with a server that performs model averaging. We perform extensive experiments on this algorithm, demonstrating it is robust to unbalanced and non-IID data distributions, and can reduce the rounds of communication needed to train a deep network on decentralized data by orders of magnitude.

Federated Learning Ideal problems for federated learning have the following properties: 1) Training on real-world data from mobile devices provides a distinct advantage over training on proxy data that is generally available in the data center. 2) This data is privacy sensitive or large in size (compared to the size of the model), so it is preferable not to log it to the data center purely for the purpose of model training (in service of the focused collection principle). 3) For supervised tasks, labels on the data can be inferred naturally from user interaction.

mation than the raw training data (by the data processing inequality), and will generally contain much less. Further, the source of the updates is not needed by the aggregation algorithm, so updates can be transmitted without identifying meta-data over a mix network such as Tor [7] or via a trusted third party. We briefly discuss the possibility of combining federated learning with secure multiparty computation and differential privacy at the end of the paper.

Many models that power intelligent behavior on mobile devices fit the above criteria. As two examples, we consider image classification, for example predicting which photos are most likely to be viewed multiple times in the future, or shared; and language models, which can be used to improve voice recognition and text entry on touch-screen keyboards by improving decoding, next-word-prediction, and even predicting whole replies [10]. The potential training data for both these tasks (all the photos a user takes and everything they type on their mobile keyboard, including passwords, URLs, messages, etc.) can be privacy sensitive. The distributions from which these examples are drawn are also likely to differ substantially from easily available proxy datasets: the use of language in chat and text messages is generally much different than standard language corpora, e.g., Wikipedia and other web documents; the photos people take on their phone are likely quite different than typical Flickr photos. And finally, the labels for these problems are directly available: entered text is self-labeled for learning a language model, and photo labels can be defined by natural user interaction with their photo app (which photos are deleted, shared, or viewed).

Both of these tasks are well-suited to learning a neural network. For image classification feed-forward deep networks, and in particular convolutional networks, are well-known to provide state-of-the-art results [26, 25]. For language modeling tasks recurrent neural networks, and in particular LSTMs, have achieved state-of-the-art results [20, 5, 22].

Federated Optimization We refer to the optimization problem implicit in federated learning as federated optimization, drawing a connection (and contrast) to distributed optimization. Federated optimization has several key properties that differentiate it from a typical distributed optimization problem:

• Non-IID The training data on a given client is typically based on the usage of the mobile device by a particular user, and hence any particular user’s local dataset will not be representative of the population distribution. • Unbalanced Similarly, some users will make much heavier use of the service or app than others, leading to varying amounts of local training data. • Massively distributed We expect the number of clients participating in an optimization to be much larger than the average number of examples per client. • Limited communication Mobile devices are frequently offline or on slow or expensive connections.

In this work, our emphasis is on the non-IID and unbalanced properties of the optimization, as well as the critical nature of the communication constraints. A deployed federated optimization system must also address a myriad of practical issues: client datasets that change as data is added and deleted; client availability that correlates with the local data distribution in complex ways (e.g., phones from speakers of American English will likely be plugged in at different times than speakers of British English); and clients that never respond or send corrupted updates.

Privacy Federated learning has distinct privacy advantages compared to data center training on persisted data. Holding even an “anonymized” dataset can still put user privacy at risk via joins with other data [37]. In contrast, the information transmitted for federated learning is the minimal update necessary to improve a particular model (naturally, the strength of the privacy benefit depends on the content of the updates).1 The updates themselves can (and should) be ephemeral. They will never contain more infor

These issues are beyond the scope of the current work; instead, we use a controlled environment that is suitable for experiments, but still addresses the key issues of client availability and unbalanced and non-IID data. We assume a synchronous update scheme that proceeds in rounds of communication. There is a fixed set of $K$ clients, each with a fixed local dataset. At the beginning of each round, a random fraction $C$ of clients is selected, and the server sends the current global algorithm state to each of these clients (e.g., the current model parameters). We only select a fraction of clients for efficiency, as our experiments show diminishing returns for adding more clients beyond a certain point. Each selected client then performs local computation based on the global state and its local dataset, and sends an update to the server. The server then applies these updates to its global state, and the process repeats.

While we focus on non-convex neural network objectives, the algorithm we consider is applicable to any finite-sum objective of the form

$$
\operatorname* { m i n } _ { w \in \mathbb { R } ^ { d } } f ( w ) \qquad \mathrm { w h e r e } \qquad f ( w ) \stackrel { \mathrm { d e f } } { = } \frac 1 n \sum _ { i = 1 } ^ { n } f _ { i } ( w ) .
$$

For a machine learning problem, we typically take $f _ { i } ( w ) =$ $\ell ( x _ { i } , y _ { i } ; w )$ , that is, the loss of the prediction on example $( x _ { i } , y _ { i } )$ made with model parameters $w$ . We assume there are $K$ clients over which the data is partitioned, with $\mathcal { P } _ { k }$ the set of indexes of data points on client $k$ , with $n _ { k } = | \mathcal { P } _ { k } |$ Thus, we can re-write the objective (1) as

$$
f ( w ) = \sum _ { k = 1 } ^ { K } { \frac { n _ { k } } { n } } F _ { k } ( w ) \quad { \mathrm { w h e r e } } \quad F _ { k } ( w ) = { \frac { 1 } { n _ { k } } } \sum _ { i \in \mathcal { P } _ { k } } f _ { i } ( w ) .
$$

If the partition $\mathcal { P } _ { k }$ was formed by distributing the training examples over the clients uniformly at random, then we would have $\mathbb { E } p _ { k } [ F _ { k } ( w ) ] = f ( w )$ , where the expectation is over the set of examples assigned to a fixed client $k$ . This is the IID assumption typically made by distributed optimization algorithms; we refer to the case where this does not hold (that is, $F _ { k }$ could be an arbitrarily bad approximation to $f$ ) as the non-IID setting.

In data center optimization, communication costs are relatively small, and computational costs dominate, with much of the recent emphasis being on using GPUs to lower these costs. In contrast, in federated optimization communication costs dominate — we will typically be limited by an upload bandwidth of $1 \ : \mathrm { M B / s }$ or less. Further, clients will typically only volunteer to participate in the optimization when they are charged, plugged-in, and on an unmetered wi-fi connection. Further, we expect each client will only participate in a small number of update rounds per day. On the other hand, since any single on-device dataset is small compared to the total dataset size, and modern smartphones have relatively fast processors (including GPUs), computation becomes essentially free compared to communication costs for many model types. Thus, our goal is to use additional computation in order to decrease the number of rounds of communication needed to train a model. There are two primary ways we can add computation: 1) increased parallelism, where we use more clients working independently between each communication round; and, 2) increased computation on each client, where rather than performing a simple computation like a gradient calculation, each client performs a more complex calculation between each communication round. We investigate both of these approaches, but the speedups we achieve are due primarily to adding more computation on each client, once a minimum level of parallelism over clients is used.

Related Work Distributed training by iteratively averaging locally trained models has been studied by McDonald et al. [28] for the perceptron and Povey et al. [31] for speech recognition DNNs. Zhang et al. [42] studies an asynchronous approach with “soft” averaging. These works only consider the cluster / data center setting (at most 16 workers, wall-clock time based on fast networks), and do not consider datasets that are unbalanced and non-IID, properties that are essential to the federated learning setting. We adapt this style of algorithm to the federated setting and perform the appropriate empirical evaluation, which asks different questions than those relevant in the data center setting, and requires different methodology.

Using similar motivation to ours, Neverova et al. [29] also discusses the advantages of keeping sensitive user data on device. The work of Shokri and Shmatikov [35] is related in several ways: they focus on training deep networks, emphasize the importance of privacy, and address communication costs by only sharing a subset of the parameters during each round of communication; however, they also do not consider unbalanced and non-IID data, and the empirical evaluation is limited.

In the convex setting, the problem of distributed optimization and estimation has received significant attention [4, 15, 33], and some algorithms do focus specifically on communication efficiency [45, 34, 40, 27, 43]. In addition to assuming convexity, this existing work generally requires that the number of clients is much smaller than the number of examples per client, that the data is distributed across the clients in IID fashion, and that each node has an identical number of data points — all of these assumptions are violated in the federated optimization setting. Asynchronous distributed forms of SGD have also been applied to training neural networks, e.g., Dean et al. [12], but these approaches require a prohibitive number of updates in the federated setting. Distributed consensus algorithms (e.g., [41]) relax the IID assumption, but are still not a good fit for communication-constrained optimization over very many clients.

One endpoint of the (parameterized) algorithm family we consider is simple one-shot averaging, where each client solves for the model that minimizes (possibly regularized) loss on their local data, and these models are averaged to produce the final global model. This approach has been studied extensively in the convex case with IID data, and it is known that in the worst-case, the global model produced is no better than training a model on a single client [44, 3, 46].

# 2 The FederatedAveraging Algorithm

The recent multitude of successful applications of deep learning have almost exclusively relied on variants of stochastic gradient descent (SGD) for optimization; in fact, many advances can be understood as adapting the structure of the model (and hence the loss function) to be more amenable to optimization by simple gradient-based methods [16]. Thus, it is natural that we build algorithms for federated optimization by starting from SGD.

SGD can be applied naively to the federated optimization problem, where a single batch gradient calculation (say on a randomly selected client) is done per round of communication. This approach is computationally efficient, but requires very large numbers of rounds of training to produce good models (e.g., even using an advanced approach like batch normalization, Ioffe and Szegedy [21] trained MNIST for 50000 steps on minibatches of size 60). We consider this baseline in our CIFAR-10 experiments.

In the federated setting, there is little cost in wall-clock time to involving more clients, and so for our baseline we use large-batch synchronous SGD; experiments by Chen et al. [8] show this approach is state-of-the-art in the data center setting, where it outperforms asynchronous approaches. To apply this approach in the federated setting, we select a $C$ fraction of clients on each round, and compute the gradient of the loss over all the data held by these clients. Thus, $C$ controls the global batch size, with $C = 1$ corresponding to full-batch (non-stochastic) gradient descent.2 We refer to this baseline algorithm as FederatedSGD (or FedSGD).

A typical implementation of FedSGD with $C = 1$ and a fixed learning rate $\eta$ has each client $k$ compute $g _ { k } =$ $\nabla F _ { k } ( w _ { t } )$ , the average gradient on its local data at the current model $w _ { t }$ , and the central server aggregates these gradients and applies the update $\begin{array} { r } { w _ { t + 1 }  w _ { t } - \eta \sum _ { k = 1 } ^ { K } \frac { n _ { k } } { n } g _ { k } } \end{array}$ , since $\begin{array} { r } { \sum _ { k = 1 } ^ { K } \frac { n _ { k } } { n } g _ { k } = \nabla f ( w _ { t } ) } \end{array}$ . An equivalent update is given by $\forall k , \ w _ { t + 1 } ^ { k } \gets w _ { t } - \eta g _ { k }$ and then $\begin{array} { r } { w _ { t + 1 } \gets \sum _ { k = 1 } ^ { K } \frac { n _ { k } } { n } w _ { t + 1 } ^ { k } } \end{array}$ That is, each client locally takes one step of gradient descent on the current model using its local data, and the server then takes a weighted average of the resulting models. Once the algorithm is written this way, we can add more computation to each client by iterating the local update $w ^ { k }  w ^ { k } - \eta \nabla F _ { k } ( w ^ { k } )$ multiple times before the averaging step. We term this approach FederatedAveraging (or FedAvg). The amount of computation is controlled by three key parameters: $C$ , the fraction of clients that perform computation on each round; $E$ , then number of training passes each client makes over its local dataset on each round; and $B$ , the local minibatch size used for the client updates. We write $B = \infty$ to indicate that the full local dataset is treated as a single minibatch. Thus, at one endpoint of this algorithm family, we can take $B = \infty$ and $E = 1$ which corresponds exactly to FedSGD. For a client with $n _ { k }$ local examples, the number of local updates per round is given by $\begin{array} { r } { u _ { k } = E \frac { n _ { k } } { B } } \end{array}$ ; Complete pseudo-code is given in Algorithm 1.

For general non-convex objectives, averaging models in parameter space could produce an arbitrarily bad model.

![## Image Analysis: 2fcf6f8878571a5545c130a9e26e0639116b85928148722e86cf3ae938f6b064.jpg

**Conceptual Understanding:**
This image conceptually illustrates the 'loss landscape' between two trained neural network models when their parameters are linearly interpolated or averaged. The main purpose is to demonstrate how the initial conditions (specifically, independent vs. common parameter initialization) profoundly affect the convexity and effectiveness of averaging model parameters. It conveys the key idea that simply averaging model parameters, as done in techniques like Federated Averaging, does not always guarantee improved performance; the initialization strategy plays a crucial role in determining whether averaging leads to a better model (lower loss) or a worse one (higher loss).

**Content Interpretation:**
The image presents two plots that illustrate the 'loss' achieved by averaging the parameters of two machine learning models, \(w\) and \(w'\), using a mixing weight \(\theta\). The specific averaging formula used is \(\theta w + (1 - \theta) w'\). The plots vary based on how the initial parameters for \(w\) and \(w'\) were set:

1.  **Independent initialization (Left Plot):** In this scenario, the two models \(w\) and \(w'\) were initialized using different random seeds. The plot shows that when \(\theta = 0\), the loss corresponds to model \(w'\), and when \(\theta = 1\), it corresponds to model \(w\). Both points exhibit relatively low loss values (around 0.45). However, for intermediate values of \(\theta\) (i.e., when averaging the models), the loss significantly increases, peaking at around 1.2. This indicates that averaging independently initialized models can lead to a suboptimal or even worse performance than the individual models, suggesting that their 'loss basins' might be misaligned.

2.  **Common initialization (Right Plot):** Here, the models \(w\) and \(w'\) were initialized using a shared random seed before training. Similar to the left plot, \(\theta = 0\) and \(\theta = 1\) represent the individual models. The critical difference is that for intermediate values of \(\theta\), the loss curve dips significantly below the loss of the individual models (around 0.46). The minimum loss, approximately 0.39, is achieved when \(\theta\) is around 0.5, indicating an even average of the two models. This demonstrates that when models share a common initialization, averaging their parameters can lead to a substantial reduction in the overall loss, outperforming the individual models.

Both plots have horizontal gray lines (around 0.45 for independent, 0.46 for common) representing the best loss achieved by either \(w\) or \(w'\), and vertical gray lines at \(\theta = 0\) and \(\theta = 1\) mark the losses of the individual models. The difference in y-axis scales emphasizes the much higher loss values encountered with independent initialization compared to common initialization. The plots visualize the 'loss landscape' when traversing a linear path between two trained model parameters.

**Key Insights:**
The main takeaways from these plots are:

1.  **Impact of Initialization on Averaging Success:** The way models are initialized significantly influences the effectiveness of parameter averaging. Independent initialization can lead to a 'loss barrier' between models, where averaging parameters results in higher loss (as seen in the 'Independent initialization' plot, where the curve peaks significantly between \(\theta=0\) and \(\theta=1\), reaching 1.2 loss).
2.  **Benefits of Common Initialization:** When models share a common initialization, averaging their parameters can lead to a significantly lower loss than either individual model. The 'Common initialization' plot clearly shows a 'U'-shaped curve with a minimum loss (around 0.39) achieved at an intermediate mixing weight (around \(\theta=0.5\)), which is better than the individual models' losses (around 0.46).
3.  **Loss Landscape Geometry:** The plots implicitly reveal aspects of the loss landscape. With independent initialization, the loss landscape between the two models is highly non-convex along the linear path, indicating different 'basins of attraction'. With common initialization, the loss landscape between the two models is much smoother and more convex-like along the linear path, suggesting that the models converge to 'compatible' solutions or basins that allow for effective averaging.

These insights are directly supported by the distinct shapes of the blue curves in the two plots and the different y-axis scales. The 'Independent initialization' plot's title and the high peak of the curve, contrasting with the 'Common initialization' plot's title and its deep minimum, serve as direct textual and visual evidence for these conclusions.

**Document Context:**
This image is highly relevant to the document's section '2 The FederatedAveraging Algorithm'. It provides critical empirical evidence supporting the conditions under which model averaging, a core component of federated learning, can be effective or detrimental. The two plots directly illustrate the impact of initialization strategy (independent vs. common) on the success of averaging model parameters. This directly addresses the practical considerations and theoretical underpinnings of why federated averaging might work well in certain scenarios (common initialization leads to better averaging) and face challenges in others (independent initialization leads to poor averaging). The text after the image further clarifies that the models \(w\) and \(w'\) were trained using SGD on different small datasets, which is typical in a federated learning setup where client models are trained on local data subsets. The finding that 'With shared initialization, averaging the models produces a significant reduction in the loss on the total training set (much better than the loss of either parent model)' directly correlates with the visual evidence in the 'Common initialization' plot, reinforcing a key advantage of appropriate model averaging in federated learning contexts.

**Summary:**
The image consists of two line plots displayed side-by-side, comparing the effect of model averaging on loss under two different initialization conditions: 'Independent initialization' and 'Common initialization'. Both plots illustrate the 'loss' on the y-axis as a function of the 'mixing weight \theta' on the x-axis, ranging from -0.2 to 1.2. Each plot shows a blue curve representing the loss, along with horizontal and vertical grid lines. The y-axis scales differ between the two plots.

The left plot, titled "Independent initialization", shows a loss curve with two minima around \theta = 0 and \theta = 1, and a sharp peak in between, indicating increased loss for intermediate mixing weights. The y-axis ranges from 0.4 to 1.2 in increments of 0.1. A subtle horizontal gray line is present at approximately y = 0.45. Vertical gray lines are present at \theta = 0.0 and \theta = 1.0.

The right plot, titled "Common initialization", displays a 'U'-shaped loss curve, with a clear minimum around \theta = 0.5, signifying reduced loss for intermediate mixing weights. The y-axis for this plot has a tighter range, from 0.40 to 0.54 in increments of 0.02. A subtle horizontal gray line is present at approximately y = 0.46. Vertical gray lines are present at \theta = 0.0 and \theta = 1.0.

Overall, the visualization highlights the significant difference in the loss landscape when averaging models initialized independently versus those sharing a common initialization, with the latter leading to more favorable averaging outcomes in terms of loss reduction.](images/2fcf6f8878571a5545c130a9e26e0639116b85928148722e86cf3ae938f6b064.jpg)
Figure 1: The loss on the full MNIST training set for models generated by averaging the parameters of two models $w$ and $w ^ { \prime }$ using $\theta w + ( 1 - \theta ) w ^ { \prime }$ for 50 evenly spaced values $\theta \in [ - 0 . 2 , 1 . 2 ]$ .The models $w$ and $w ^ { \prime }$ were trained using SGD on different small datasets. For the left plot, $w$ and $w ^ { \prime }$ were initialized using different random seeds; for the right plot, a shared seed was used. Note the different $y$ -axis scales. The horizontal line gives the best loss achieved by $w$ or $w ^ { \prime }$ (which were quite close, corresponding to the vertical lines at $\theta = 0$ and $\theta = 1$ ). With shared initialization, averaging the models produces a significant reduction in the loss on the total training set (much better than the loss of either parent model).

Following the approach of Goodfellow et al. [17], we see exactly this bad behavior when we average two MNIST digit-recognition models3 trained from different initial conditions (Figure 1, left). For this figure, the parent models $w$ and $w ^ { \prime }$ were each trained on non-overlapping IID samples of 600 examples from the MNIST training set. Training was via SGD with a fixed learning rate of 0.1 for 240 updates on minibatches of size 50 (or $E = 2 0$ passes over the mini-datasets of size 600). This is approximately the amount of training where the models begin to overfit their local datasets.

Recent work indicates that in practice, the loss surfaces of sufficiently over-parameterized NNs are surprisingly wellbehaved and in particular less prone to bad local minima than previously thought [11, 17, 9]. And indeed, when we start two models from the same random initialization and then again train each independently on a different subset of the data (as described above), we find that naive parameter averaging works surprisingly well (Figure 1, right): the average of these two models, $\scriptstyle { \frac { 1 } { 2 } } w + { \frac { 1 } { 2 } } w ^ { \prime }$ , achieves significantly lower loss on the full MNIST training set than the best model achieved by training on either of the small datasets independently. While Figure 1 starts from a random initialization, note a shared starting model $w _ { t }$ is used for each round of FedAvg, and so the same intuition applies.

<table><tr><td>Algorithm1 FederatedAveraging.The K clients are indexed by k;B is the local minibatch size,E is the number of local epochs, and η is the learning rate.</td></tr><tr><td>Server executes: initialize wo for each round t = 1,2,... do m ←max(C:K,1) St ← (random set of m clients) for each client k ∈ St in parallel do</td></tr><tr><td>w+1←ClientUpdate(k, wut) nk ,k IlErratum4 mt mt←∑k∈St mw+1 Wt+1←∑k∈Stn</td></tr><tr><td>ClientUpdate(k,w): // Run on client k</td></tr><tr><td>B ← (split Pk into batches of size B)</td></tr><tr><td>for each local epoch i from 1 to E do forbatch b ∈Bdo w ←w-nvl(w;b) return w to server</td></tr></table>

# 3 Experimental Results

We are motivated by both image classification and language modeling tasks where good models can greatly enhance the usability of mobile devices. For each of these tasks we first picked a proxy dataset of modest enough size that we could thoroughly investigate the hyperparameters of the FedAvg algorithm. While each individual training run is relatively small, we trained over 2000 individual models for these experiments. We then present results on the benchmark CIFAR-10 image classification task. Finally, to demonstrate the effectiveness of FedAvg on a real-world problem with a natural partitioning of the data over clients, we evaluate on a large language modeling task.

Our initial study includes three model families on two datasets. The first two are for the MNIST digit recognition task [26]: 1) A simple multilayer-perceptron with 2-hidden layers with 200 units each using ReLu activations (199,210 total parameters), which we refer to as the MNIST 2NN. 2) A CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with $2 \mathbf { x } 2$ max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer (1,663,370 total parameters). To study federated optimization, we also need to specify how the data is distributed over the clients. We study two ways of partitioning the MNIST data over clients: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, and Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. This is a pathological non-IID partition of the data, as most clients will only have examples of two digits, letting us explore the degree to which our algorithms will break on highly non-IID data. Both of these partitions are balanced, however.5

For language modeling, we built a dataset from The Complete Works of William Shakespeare [32]. We construct a client dataset for each speaking role in each play with at least two lines. This produced a dataset with 1146 clients. For each client, we split the data into a set of training lines (the first $80 \%$ of lines for the role), and test lines (the last $20 \%$ , rounded up to at least one line). The resulting dataset has 3,564,579 characters in the training set, and 870,014 characters6 in the test set. This data is substantially unbalanced, with many roles having only a few lines, and a few with a large number of lines. Further, observe the test set is not a random sample of lines, but is temporally separated by the chronology of each play. Using an identical train/test split, we also form a balanced and IID version of the dataset, also with 1146 clients.

On this data we train a stacked character-level LSTM language model, which after reading each character in a line, predicts the next character [22]. The model takes a series of characters as input and embeds each of these into a learned 8 dimensional space. The embedded characters are then processed through 2 LSTM layers, each with 256 nodes. Finally the output of the second LSTM layer is sent to a softmax output layer with one node per character. The full model has 866,578 parameters, and we trained using an unroll length of 80 characters.

SGD is sensitive to the tuning of the learning-rate parameter $\eta$ . The results reported here are based on training over a sufficiently wide grid of learning rates (typically 11-13 values for $\eta$ on a multiplicative grid of resolution $1 0 ^ { \frac { 1 } { 3 } }$ or $1 0 ^ { \frac { 1 } { 6 } }$ ). We checked to ensure the best learning rates were in the middle of our grids, and that there was not a significant difference between the best learning rates. Unless otherwise noted, we plot metrics for the best performing rate selected individually for each $x$ -axis value. We find that the optimal learning rates do not vary too much as a function of the other parameters.

Increasing parallelism We first experiment with the client fraction $C$ , which controls the amount of multi-client parallelism. Table 1 shows the impact of varying $C$ for both MNIST models. We report the number of communication rounds necessary to achieve a target test-set accuracy. To compute this, we construct a learning curve for each combination of parameter settings, optimizing $\eta$ as described above and then making each curve monotonically improving by taking the best value of test-set accuracy achieved over

Table 1: Effect of the client fraction $C$ on the MNIST 2NN with $E = 1$ and CNN with $E = 5$ . Note $C = 0 . 0$ corresponds to one client per round; since we use 100 clients for the MNIST data, the rows correspond to 1, 10 20, 50, and 100 clients. Each table entry gives the number of rounds of communication necessary to achieve a test-set accuracy of $9 7 \%$ for the 2NN and $9 9 \%$ for the CNN, along with the speedup relative to the $C = 0$ baseline. Five runs with the large batch size did not reach the target accuracy in the allowed time.

<table><tr><td rowspan="3">2NN C</td><td colspan="2">IID</td><td colspan="2">-NON-IID</td></tr><tr><td colspan="2">B=8</td><td colspan="2">B=8</td></tr><tr><td></td><td>B=10</td><td></td><td>B=10</td></tr><tr><td>0.01455</td><td></td><td>316</td><td>4278</td><td>3275</td></tr><tr><td></td><td>0.11474 (1.0×)</td><td>87(3.6x)</td><td>1796 (2.4x)</td><td>664 (4.9x)</td></tr><tr><td></td><td>0.21658(0.9×)</td><td>77(4.1x)</td><td>1528(2.8×)</td><td>619(5.3x)</td></tr><tr><td>0.5</td><td>(- ）</td><td>75 (4.2x)</td><td>(-</td><td>443(7.4×)</td></tr><tr><td>1.0</td><td>(一)</td><td>70(4.5x)</td><td>(一)</td><td>380(8.6x)</td></tr><tr><td colspan="5">CNN,E=5</td></tr><tr><td>0.0</td><td>387</td><td>50</td><td>1181</td><td>956</td></tr><tr><td>0.1</td><td>339 (1.1x)</td><td>18(2.8x)</td><td>1100 (1.1x)</td><td>206(4.6x)</td></tr><tr><td>0.2</td><td>337(1.1x)</td><td>18(2.8x)</td><td>978 (1.2x)</td><td>200(4.8x）</td></tr><tr><td>0.5</td><td>164 (2.4x)</td><td>18(2.8x)</td><td>1067 (1.1x)</td><td>261(3.7x)</td></tr><tr><td>1.0</td><td>246(1.6×)</td><td>16(3.1x)</td><td>（一)</td><td>97(9.9x)</td></tr></table>

all prior rounds. We then calculate the number of rounds where the curve crosses the target accuracy, using linear interpolation between the discrete points forming the curve. This is perhaps best understood by reference to Figure 2, where the gray lines show the targets.

With $B = \infty$ (for MNIST processing all 600 client examples as a single batch per round), there is only a small advantage in increasing the client fraction. Using the smaller batch size $B = 1 0$ shows a significant improvement in using $C \geq 0 . 1$ , especially in the non-IID case. Based on these results, for most of the remainder of our experiments we fix $C = 0 . 1$ , which strikes a good balance between computational efficiency and convergence rate. Comparing the number of rounds for the $B = \infty$ and $B = 1 0$ columns in Table 1 shows a dramatic speedup, which we investigate next.

Increasing computation per client In this section, we fix $C = 0 . 1$ , and add more computation per client on each round, either decreasing $B$ , increasing $E$ , or both. Figure 2 demonstrates that adding more local SGD updates per round can produce a dramatic decrease in communication costs, and Table 2 quantifies these speedups. The expected number of updates per client per round is $u \ = \ ( \mathbb { E } [ n _ { k } ] / B ) E \ =$ $n E / ( K B )$ , where the expectation is over the draw of a random client $k$ . We order the rows in each section of Table 2 by this statistic. We see that increasing $u$ by varying both $E$ and $B$ is effective. As long as $B$ is large enough to take full advantage of available parallelism on the client hardware, there is essentially no cost in computation time for lowering it, and so in practice this should be the first parameter tuned.

Table 2: Number of communication rounds to reach a target accuracy for FedAvg, versus FedSGD (first row, $E = 1$ and $B = \infty$ ). The $u$ column gives $u = E n / ( K B )$ , the expected number of updates per round.

<table><tr><td colspan="7">MNIST CNN,99% ACCURACY</td></tr><tr><td>CNN</td><td>E</td><td>B</td><td>u</td><td>IID</td><td colspan="3">NON-IID</td></tr><tr><td>FEDSGD</td><td>1</td><td>8</td><td>1</td><td>626</td><td></td><td>483</td><td></td></tr><tr><td>FEDAVG</td><td>5</td><td>8</td><td>5</td><td>179</td><td>(3.5x)</td><td>1000</td><td>（0.5x）</td></tr><tr><td>FEDAVG</td><td>1</td><td>50</td><td>12</td><td>65</td><td>(9.6×）</td><td>600</td><td>（0.8×）</td></tr><tr><td>FEDAVG</td><td>20</td><td>8</td><td>20</td><td>234</td><td>(2.7x）</td><td>672</td><td>（0.7x）</td></tr><tr><td>FEDAVG</td><td>1</td><td>10</td><td>60</td><td></td><td>34 (18.4x)</td><td>350</td><td>(1.4x)</td></tr><tr><td>FEDAVG</td><td>5</td><td>50</td><td>60</td><td></td><td>29 (21.6x)</td><td>334</td><td>(1.4×)</td></tr><tr><td>FEDAVG</td><td>20</td><td>50</td><td>240</td><td></td><td>32(19.6×)</td><td>426</td><td>(1.1x）</td></tr><tr><td>FEDAVG</td><td>5</td><td>10</td><td>300</td><td></td><td>20 (31.3x)</td><td>229</td><td>(2.1x)</td></tr><tr><td>FEDAVG</td><td>20</td><td>10</td><td>1200</td><td></td><td>18(34.8x)</td><td>173</td><td>（2.8x）</td></tr><tr><td colspan="8">SHAKESPEARE LSTM, 54% ACCURACY</td></tr><tr><td>LSTM</td><td>E</td><td>B</td><td>u</td><td>IID</td><td></td><td>NON-IID</td><td></td></tr><tr><td>FEDSGD</td><td>1</td><td>8</td><td>1.0</td><td>2488</td><td></td><td>3906</td><td></td></tr><tr><td>FEDAVG</td><td>1</td><td>50</td><td>1.5</td><td>1635</td><td>(1.5x)</td><td>549</td><td>（7.1x）</td></tr><tr><td>FEDAVG</td><td>5</td><td>8</td><td>5.0</td><td>613</td><td>(4.1x)</td><td></td><td>597 (6.5x)</td></tr><tr><td>FEDAVG</td><td>1</td><td>10</td><td>7.4</td><td>460</td><td>(5.4x）</td><td></td><td>164 (23.8x)</td></tr><tr><td>FEDAVG</td><td>5</td><td>50</td><td>7.4</td><td>401</td><td>(6.2×）</td><td></td><td>152 (25.7×)</td></tr><tr><td>FEDAVG</td><td>5</td><td>10</td><td>37.1</td><td></td><td>192 (13.0x)</td><td></td><td>41 (95.3×)</td></tr></table>

For the IID partition of the MNIST data, using more computation per client decreases the number of rounds to reach the target accuracy by $3 5 \times$ for the CNN and $4 6 \times$ for the 2NN (see Table 4 in Appendix A for details for the 2NN). The speedups for the pathologically partitioned non-IID data are smaller, but still substantial $( 2 . 8 - 3 . 7 \times )$ . It is impressive that averaging provides any advantage (vs. actually diverging) when we naively average the parameters of models trained on entirely different pairs of digits. Thus, we view this as strong evidence for the robustness of this approach.

The unbalanced and non-IID distribution of the Shakespeare (by role in the play) is much more representative of the kind of data distribution we expect for real-world applications. Encouragingly, for this problem learning on the non-IID and unbalanced data is actually much easier (a $9 5 \times$ speedup vs $1 3 \times$ for the balanced IID data); we conjecture this is largely due to the fact some roles have relatively large local datasets, which makes increased local training particularly valuable.

For all three model classes, FedAvg converges to a higher level of test-set accuracy than the baseline FedSGD models. This trend continues even if the lines are extended beyond the plotted ranges. For example, for the CNN the $B = \infty , E = 1$ FedSGD model eventually reaches $9 9 . 2 2 \%$ accuracy after 1200 rounds (and had not improved further after 6000 rounds), while the $B = 1 0 , E = 2 0$ FedAvg model reaches an accuracy of $9 9 . 4 4 \%$ after 300 rounds. We conjecture that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout [36].

We are primarily concerned with generalization performance, but FedAvg is effective at optimizing the training loss as well, even beyond the point where test-set accuracy plateaus. We observed similar behavior for all three model classes, and present plots for the MNIST CNN in Figure 6

![## Image Analysis: 0fe12b21f1acdf92459ac217f8ea690410bb534233d2741c8225afa15d565726.jpg

**Conceptual Understanding:**
This image conceptually represents the performance evaluation of federated learning algorithms under varying local training parameters and data distribution characteristics. The main purpose is to demonstrate how factors like local batch size (`B`), local epochs (`E`), and the independent and identically distributed (IID) or non-IID nature of data affect the model's convergence speed and final test accuracy in a federated learning setup. It aims to provide insights into optimal hyperparameter selection for different datasets (MNIST, Shakespeare) and data distributions (IID, pathological non-IID). The key ideas being communicated are the trade-offs between local computation and global convergence, and the impact of data heterogeneity on model training effectiveness in a federated context. Play&Role)".
*   **Significance:** This plot highlights the extreme challenges posed by highly heterogeneous data. Convergence is generally slower than in the IID Shakespeare case, and achieving stable, higher accuracies requires more communication rounds, especially for configurations like `B=∞ E=1` which show very slow progress and only begin to reach higher accuracies after 2000 rounds. The specific data distribution "Non-IID by Play&Role" is a critical piece of textual evidence indicating the unique challenge.

**Content Interpretation:**
The image displays four line graphs showing the performance of federated learning algorithms. Each graph plots "Test Accuracy" on the y-axis against "Communication Rounds" on the x-axis. The lines within each plot represent different configurations of local client batch size (`B`) and local epochs (`E`).

**Plot 1 (Top-Left): "MNIST CNN IID"**
*   **Processes/Concepts:** Federated learning training for a Convolutional Neural Network (CNN) on the MNIST dataset with data distributed independently and identically (IID).
*   **Significance:** This plot shows how different `B` and `E` values influence the convergence of test accuracy for a relatively straightforward task under ideal data distribution. High `E` values (e.g., `E=5`, `E=20`) generally lead to rapid initial accuracy gains, with most configurations achieving high accuracies (above 0.990) within 1000 communication rounds. The `B=∞ E=1` configuration shows significantly slower convergence.

**Plot 2 (Top-Right): "MNIST CNN Non-IID"**
*   **Processes/Concepts:** Federated learning training for a CNN on MNIST with non-IID data distribution.
*   **Significance:** This plot illustrates the impact of data heterogeneity on model performance. Compared to the IID case, convergence is often slower, and peak accuracies can be slightly lower or more variable, indicating the increased difficulty of learning from non-IID data. The `B=∞ E=1` configuration continues to show the slowest convergence.

**Plot 3 (Bottom-Left): "IID"** (Shakespeare LSTM)
*   **Processes/Concepts:** Federated learning training for an LSTM model on the Shakespeare dataset with IID data distribution.
*   **Significance:** This plot demonstrates the performance for a potentially more complex natural language processing task. The overall test accuracies are considerably lower (0.2 to 0.6) and require more communication rounds (up to 4000) for convergence compared to MNIST. Configurations with higher `E` (e.g., `B=10 E=5`) still show faster initial improvements.

**Plot 4 (Bottom-Right): "Non-IID by Play&Role"** (Shakespeare LSTM)
*   **Processes/Concepts:** Federated learning training for an LSTM model on the Shakespeare dataset with a specific, highly heterogeneous non-IID distribution (

**Key Insights:**
**1. Impact of Local Epochs (E) on Convergence Speed:**
*   **Takeaway:** Increasing the number of local epochs (`E`) generally leads to faster initial convergence and quicker improvements in "Test Accuracy" across different datasets and data distributions.
*   **Textual Evidence:** In the "MNIST CNN IID" plot, comparing "B=10 E=1", "B=10 E=5", and "B=10 E=20", the dashed (E=5) and dotted (E=20) red lines rise more steeply and reach higher accuracies earlier than the solid red line (E=1).

**2. Impact of Local Batch Size (B) and Frequency of Updates:**
*   **Takeaway:** Very large local batch sizes (`B=∞`) combined with few local epochs (`E=1`) significantly slow down the convergence of "Test Accuracy" in federated learning.
*   **Textual Evidence:** The solid blue lines, labeled "B=∞ E=1" in all four plots, consistently show the slowest increase in "Test Accuracy" over "Communication Rounds" compared to all other configurations.

**3. Challenges Posed by Non-IID Data Distribution:**
*   **Takeaway:** Non-IID data distributions generally result in slower convergence, potentially lower peak accuracies, or more varied performance, making federated learning more challenging.
*   **Textual Evidence:** Comparing "MNIST CNN IID" with "MNIST CNN Non-IID" (top plots), and "IID" with "Non-IID by Play&Role" (bottom plots), the non-IID scenarios often show lines that rise less steeply or plateau at slightly lower "Test Accuracy" values. For example, in the "Non-IID by Play&Role" plot, the "B=∞ E=1" line makes significant gains only after thousands of communication rounds.

**4. Task-Specific Performance Characteristics:**
*   **Takeaway:** The absolute "Test Accuracy" levels and convergence behaviors are highly dependent on the specific task (e.g., MNIST vs. Shakespeare) and model architecture (CNN vs. LSTM).
*   **Textual Evidence:** The y-axis scales for MNIST plots (0.970-1.000) are much higher than for Shakespeare plots (0.2-0.6), indicating MNIST is either an easier task or the CNN model achieves higher performance. The x-axis for Shakespeare extends to 4000 rounds, compared to 1000 for MNIST, implying a longer training duration is needed.

**5. Identification of Effective Parameter Combinations:**
*   **Takeaway:** Intermediate values of local epochs (e.g., `E=5`) often provide a good balance between rapid initial accuracy gain and reaching high overall test accuracy.
*   **Textual Evidence:** In many plots, configurations like "B=10 E=5" (dashed red line) and "B=50 E=5" (dashed orange line) demonstrate strong performance, achieving fast convergence and high "Test Accuracy".

**6. Importance of Target Benchmarks:**
*   **Takeaway:** The experiments are designed to evaluate performance against predefined target accuracies.
*   **Textual Evidence:** The accompanying text explicitly states: "The gray lines show the target accuracies used in Table 2.", indicating that these lines serve as reference points for evaluating the success of different federated learning configurations.

**Document Context:**
This image, Figure 2, is centrally positioned within the "3 Experimental Results" section of the document. As stated in the accompanying text, it directly presents "Test set accuracy vs. communication rounds for the MNIST CNN (IID and then pathological non-IID) and Shakespeare LSTM (IID and then by Play&Role) with C = 0.1 and optimized η." This confirms that the figure is a crucial visual summary of the performance of the federated learning models under different data conditions and hyperparameter settings (specifically `B` and `E`, while `C` and `η` are fixed). The reference to "The gray lines show the target accuracies used in Table 2" links these experimental results to further tabulated data, while "Plots for the 2NN are given as Figure 7 in Appendix A" indicates this figure is part of a larger set of experimental evaluations. Therefore, this image serves to visually support and elaborate on the core experimental findings regarding model convergence and accuracy under various configurations within the document's broader discussion of federated learning performance.

**Summary:**
This figure comprises four distinct line graphs, each charting "Test Accuracy" on the vertical (y) axis against "Communication Rounds" on the horizontal (x) axis. These plots are designed to illustrate the performance of federated learning models for two different tasks—MNIST image classification using a CNN, and Shakespeare text generation using an LSTM—under varying data distribution scenarios and local client training parameters. Each line within the graphs represents a unique combination of the local client batch size (`B`) and the number of local epochs (`E`) performed by clients before sending updates to the central server. Gray horizontal lines are included in all plots, indicating specific target accuracy benchmarks.

The **top two plots** focus on the **MNIST CNN** task.
*   The **top-left plot, titled "MNIST CNN IID"**, shows the model's accuracy when the MNIST data is distributed in an Independent and Identically Distributed (IID) manner among clients. The x-axis spans from 0 to 1000 communication rounds, and the y-axis ranges from 0.970 to 1.000 test accuracy. Nine configurations are tested: `B=10` with `E=1`, `E=5`, `E=20`; `B=50` with `E=1`, `E=5`, `E=20`; and `B=∞` (representing full local data batching) with `E=1`, `E=5`, `E=20`. Generally, configurations with higher `E` values (e.g., `E=5`, `E=20`) tend to converge faster, reaching high accuracies above 0.990 within fewer communication rounds. The `B=∞ E=1` configuration shows a notably slower convergence.
*   The **top-right plot, titled "MNIST CNN Non-IID"**, shows performance under a non-IID distribution of MNIST data, which often poses a greater challenge for federated learning. The axes and legend configurations are identical to the IID plot. While the general trends of faster convergence with higher `E` persist, some configurations appear to reach slightly lower peak accuracies or exhibit more varied convergence paths compared to their IID counterparts, highlighting the impact of data heterogeneity.

The **bottom two plots** depict results for the **Shakespeare LSTM** task. The absolute "Test Accuracy" values for this task are significantly lower, ranging from 0.2 to 0.6, and the "Communication Rounds" extend up to 4000, indicating a longer training process or a more complex task.
*   The **bottom-left plot, titled "IID"**, shows the Shakespeare LSTM performance with IID data. It examines six configurations: `B=10` with `E=1`, `E=5`; `B=50` with `E=1`, `E=5`; and `B=∞` with `E=1`, `E=5`. Similar to MNIST, higher `E` values (e.g., `E=5`) generally lead to quicker accuracy improvements. For instance, the dashed red line (`B=10 E=5`) reaches its peak around 1000 communication rounds, whereas the solid blue line (`B=∞ E=1`) shows very gradual progress.
*   The **bottom-right plot, titled "Non-IID by Play&Role"**, displays the Shakespeare LSTM performance with a specific "Non-IID by Play&Role" data distribution, which implies a high degree of data heterogeneity across clients. The configurations are the same six as in the IID Shakespeare plot. This non-IID scenario often results in even slower convergence or slightly reduced peak accuracies compared to the IID Shakespeare task, further emphasizing the challenges of heterogeneous data distributions in federated learning.

Collectively, these graphs provide a detailed visual comparison, supported by specific parameter labels, demonstrating how the choice of local batch size (`B`) and local epochs (`E`) interacts with different datasets and data distribution types to influence the speed and effectiveness of federated learning model convergence. They emphasize that while increasing local computation (higher `E`) can accelerate early gains, the overall training dynamics are strongly modulated by data distribution characteristics and the specific task at hand.](images/0fe12b21f1acdf92459ac217f8ea690410bb534233d2741c8225afa15d565726.jpg)
Figure 2: Test set accuracy vs. communication rounds for the MNIST CNN (IID and then pathological non-IID) and Shakespeare LSTM (IID and then by Play&Role) with $C = 0 . 1$ and optimized $\eta$ . The gray lines show the target accuracies used in Table 2. Plots for the 2NN are given as Figure 7 in Appendix A.

in Appendix A.

Can we over-optimize on the client datasets? The current model parameters only influence the optimization performed in each ClientUpdate via initialization. Thus, as $E  \infty$ , at least for a convex problem eventually the initial conditions should be irrelevant, and the global minimum would be reached regardless of initialization. Even for a non-convex problem, one might conjecture the algorithm would converge to the same local minimum as long as the initialization was in the same basin. That is, we would expect that while one round of averaging might produce a reasonable model, additional rounds of communication (and averaging) would not produce further improvements.

Figure 3 shows the impact of large $E$ during initial training on the Shakespeare LSTM problem. Indeed, for very large numbers of local epochs, FedAvg can plateau or diverge.7 This result suggests that for some models, especially in the later stages of convergence, it may be useful to decay the amount of local computation per round (moving to smaller $E$ or larger $B$ ) in the same way decaying learning rates can be useful. Figure 8 in Appendix A gives the analogous experiment for the MNIST CNN. Interestingly, for this model we see no significant degradation in the convergence rate for large values of $E$ . However, we see slightly better performance for $E = 1$ versus $E = 5$ for the large-scale language modeling task described below (see Figure 10 in Appendix A).

![## Image Analysis: e97d3bf184734de4b1746516181a7b226fee1faa9b9522ea41f39402ec0648d8.jpg

**Conceptual Understanding:**
This image conceptually illustrates the performance dynamics of a federated learning model under varying local training intensities (number of local epochs, E) and different data distribution characteristics (IID vs. Non-IID). The main purpose is to evaluate and compare how the choice of 'E' impacts the 'Train Loss' (a measure of model error during training) over successive 'Communication Rounds' (iterations of global model updates) for a fixed learning rate. It aims to identify optimal or suboptimal settings for 'E' and understand the influence of data heterogeneity on model convergence in a federated learning context.

**Content Interpretation:**
The image presents a comparative analysis of federated learning model performance, specifically focusing on how the number of local epochs (E) affects the 'Train Loss' across 'Communication Rounds.' It depicts two experimental conditions: one with Independent and Identically Distributed (IID) data and another with Non-IID data, categorized 'by Play&Role.' The core concept illustrated is the trade-off and impact of increasing local computation (E) before global model aggregation in a federated learning setup. The graphs show that while a moderate increase in local epochs can lead to faster convergence and lower training loss, excessively large numbers of local epochs do not always yield better results and can sometimes lead to plateaus or less optimal performance, particularly for the highest 'E' values compared to intermediate ones, especially in the long run. The significance of the data lies in demonstrating that the optimal number of local epochs is crucial for efficient federated learning and can vary depending on the data distribution (IID vs. Non-IID).

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Optimal Local Epochs (E) for Convergence:** The graphs clearly demonstrate that there isn't a direct linear relationship between increasing 'E' and improved performance. While small 'E' (E=1) leads to slow convergence and high loss, very large 'E' values (E=100, E=200) do not necessarily outperform moderate 'E' values (E=5, E=25) in terms of achieving the lowest loss. For instance, 'E=5' (green line) consistently achieves the lowest 'Train Loss' in both IID and Non-IID settings over the observed 'Communication Rounds'.
2.  **Impact of Data Distribution:** The two subplots highlight the difference in training dynamics between IID and Non-IID data. The 'Non-IID by Play&Role' scenario generally starts with higher 'Train Loss' and exhibits more fluctuations, especially for intermediate 'E' values, indicating that non-IID data presents a more challenging learning environment compared to IID data.
3.  **Potential for Overfitting/Stagnation with Large E:** For very large 'E' values (e.g., E=100, E=200), particularly in the IID case, the 'Train Loss' shows an initial sharp drop but then either plateaus at a higher loss or even slightly increases before slowly declining. This suggests that extensive local training without frequent global model aggregation can lead to local overfitting or slow generalization to the global task, negating the benefits of more local computation.

**Specific Textual Evidence for Insights:**
*   **Legend (E=1, E=5, E=25, E=50, E=100, E=200):** These values allow for direct comparison of the effect of different local epoch counts, showing the 'E=5' (green line) consistently reaching the lowest 'Train Loss'.
*   **Y-axis label ("Train Loss") and X-axis label ("Communication Rounds"):** These labels define the performance metric and the progression of the federated learning process, enabling the analysis of convergence behavior.
*   **Titles ("η = 1.47 IID" and "η = 1.47 Non-IID by Play&Role"):** These titles clearly distinguish the two experimental conditions, providing evidence for the comparison of data distribution impacts on training effectiveness.
*   **Trend of E=5 (green line):** Visually, in both graphs, the green line consistently descends to the lowest 'Train Loss' values by the end of the 'Communication Rounds' (around 60), demonstrating its superior performance compared to other 'E' values.

**Document Context:**
This image is placed within the "3 Experimental Results" section and directly follows the text: "Figure 3: The effect of training for many local epochs (large $E$ ) between averaging steps, fixing $B = 1 0$ and $C = 0 . 1$ for the Shakespeare LSTM with a fixed learning rate $\eta = 1 . 4 7$ ." This explicitly connects the image to the document's narrative by providing the context of a specific federated learning experiment using a Shakespeare LSTM model. The graphs visually demonstrate the 'effect of training for many local epochs (large E)' as described, by showing how 'Train Loss' changes with varying 'E' values over 'Communication Rounds' under both IID and Non-IID data conditions. This supports the experimental findings by providing visual evidence for the observed performance characteristics of the model under different configurations of local epoch settings, which is a critical hyperparameter in federated learning.

**Summary:**
The image displays two line graphs, side-by-side, illustrating the effect of local epochs (E) on training loss over communication rounds for a fixed learning rate η = 1.47. The left graph represents an "IID" (Independent and Identically Distributed) data distribution, while the right graph represents a "Non-IID by Play&Role" data distribution. Both graphs share the same Y-axis, labeled "Train Loss," ranging from 1.0 to 2.0, and the same X-axis, labeled "Communication Rounds," ranging from 0 to 60. Each graph contains six distinct colored lines, each representing a different value of 'E' (local epochs): E=1 (blue), E=5 (green), E=25 (yellow), E=50 (purple), E=100 (red), and E=200 (orange). In both IID and Non-IID scenarios, initially, all models experience a rapid decrease in Train Loss, followed by varying convergence patterns. For E=1, the Train Loss remains relatively high and decreases slowly in both cases. As 'E' increases (E=5, E=25, E=50), the initial drop in loss is steeper, and the overall loss values tend to be lower after several communication rounds, indicating faster convergence to lower loss values. Specifically, the green line (E=5) shows the most significant and consistent decrease in Train Loss over communication rounds in both IID and Non-IID settings, achieving the lowest loss by 60 communication rounds. For larger 'E' values (E=100, E=200), particularly in the IID case, the loss initially drops rapidly but then plateaus or even slightly increases before slowly decreasing again, suggesting that very large 'E' values can sometimes lead to less stable or slower long-term convergence, or possibly lead to issues like overfitting to local data without sufficient global model updates. The Non-IID graph shows similar trends, but with generally higher initial losses and more pronounced fluctuations, especially for intermediate 'E' values like E=25 and E=50, before settling. The effect of "many local epochs" (large E) mentioned in the document context is clearly depicted, showing that while moderate increases in E (e.g., E=5, E=25) can accelerate loss reduction, excessively large E (e.g., E=100, E=200) can lead to diminishing returns or even hinder smooth convergence, especially when comparing the long-term performance of E=5 to E=100 or E=200 after numerous communication rounds.](images/e97d3bf184734de4b1746516181a7b226fee1faa9b9522ea41f39402ec0648d8.jpg)
Figure 3: The effect of training for many local epochs (large $E$ ) between averaging steps, fixing $B = 1 0$ and $C = 0 . 1$ for the Shakespeare LSTM with a fixed learning rate $\eta = 1 . 4 7$ .

![## Image Analysis: 6cdbdd037b1da8aa5aabbb1407d59f2c0ff8667b7dc0a0c7071d46d71f9529c6.jpg

**Conceptual Understanding:**
This image conceptually illustrates the convergence behavior and performance efficiency of two different federated learning algorithms, FedAvg and FedSGD, on the CIFAR-10 image classification dataset. The main purpose is to compare how quickly and effectively each algorithm achieves high test accuracy as the number of communication rounds (an indicator of communication cost and training time) increases, and how varying learning rates (η) affect this performance. Key ideas being communicated include the trade-offs between communication efficiency and model accuracy, and the impact of algorithmic choice and hyperparameter tuning on federated learning outcomes.

**Content Interpretation:**
The image displays a performance comparison of two federated learning algorithms, FedAvg and FedSGD, on the CIFAR-10 dataset. The x-axis, labeled 'Communication Rounds', quantifies the amount of data exchange or model updates between clients and a central server. The y-axis, labeled 'Test Accuracy', indicates the model's performance on unseen data. The three solid lines correspond to 'FedAvg' with learning rates (η) of '0.05', '0.15', and '0.25'. These lines show that FedAvg achieves high test accuracy quickly, with minimal differences between these learning rates, although η=0.25 reaches a slightly higher peak. The three dashed lines represent 'FedSGD' with learning rates of '0.45', '0.6', and '0.7'. These lines indicate that FedSGD's test accuracy improves more gradually and with more variability across communication rounds, eventually reaching a similar, but slightly lower, plateau than FedAvg for these parameters, and requiring more communication rounds to stabilize.

**Key Insights:**
The main takeaways are that FedAvg generally achieves higher and more stable test accuracy much faster (with fewer communication rounds) compared to FedSGD on the CIFAR-10 dataset, given the specified experimental conditions. The extracted legend text 'FedAvg, η=0.05', 'FedAvg, η=0.15', 'FedAvg, η=0.25' all show a rapid ascent and early saturation around 0.8 to 0.85 test accuracy. In contrast, 'FedSGD, η=0.45', 'FedSGD, η=0.6', and 'FedSGD, η=0.7' show slower, more fluctuating convergence reaching slightly lower or comparable accuracy levels later. Specifically, FedAvg appears less sensitive to the specific learning rates within the tested range for fast convergence, while FedSGD's performance still lags behind FedAvg even with higher learning rates. This suggests FedAvg is a more efficient algorithm in terms of communication rounds for achieving high accuracy in this CIFAR-10 scenario.

**Document Context:**
This image is located in the '3 Experimental Results' section of the document and is explicitly referred to as 'Figure 4: Test accuracy versus communication for the CIFAR10 experiments.' The accompanying text provides specific hyperparameters used for the experiments: 'FedSGD uses a learning-rate decay of 0.9934 per round; FedAvg uses B = 50, learning-rate decay of 0.99 per round, and E = 5.' This contextual information is crucial for understanding the experimental setup under which the results presented in the graph were obtained. The figure visually supports the discussion of the experimental performance of FedAvg and FedSGD, allowing readers to observe the impact of communication rounds and learning rates on model accuracy for the CIFAR-10 dataset.

**Summary:**
This graph illustrates the relationship between test accuracy and the number of communication rounds for two federated learning algorithms, FedAvg and FedSGD, applied to the CIFAR-10 dataset. The x-axis represents 'Communication Rounds', ranging from 0 to 3000, with major ticks at intervals of 500. The y-axis represents 'Test Accuracy', ranging from 0 to 1.0, with major ticks at intervals of 0.2. The graph contains six distinct lines, each representing a different configuration of the federated learning algorithms, detailed in the legend. The FedAvg configurations (light orange, orange, and dark orange solid lines) show a rapid increase in test accuracy in the initial communication rounds, quickly converging to a high accuracy level (around 0.8 to 0.85). Among these, 'FedAvg, η=0.25' (dark orange) appears to reach slightly higher accuracy and stability. The FedSGD configurations (light blue, medium blue, and dark blue dashed lines) demonstrate a slower and more erratic increase in test accuracy, eventually reaching an accuracy level around 0.7 to 0.8, but with more fluctuations and a longer convergence time compared to FedAvg. The legend clearly distinguishes between 'FedAvg' and 'FedSGD' curves and their respective learning rates (η values). The graph provides a comprehensive visual comparison of the performance of these two algorithms under varying learning rate conditions over increasing communication rounds.](images/6cdbdd037b1da8aa5aabbb1407d59f2c0ff8667b7dc0a0c7071d46d71f9529c6.jpg)
Figure 4: Test accuracy versus communication for the CIFAR10 experiments. FedSGD uses a learning-rate decay of 0.9934 per round; FedAvg uses $B = 5 0$ , learning-rate decay of 0.99 per round, and $E = 5$ .

CIFAR experiments We also ran experiments on the CIFAR-10 dataset [24] to further validate FedAvg. The dataset consists of 10 classes of $3 2 \mathbf { x } 3 2$ images with three RGB channels. There are 50,000 training examples and 10,000 testing examples, which we partitioned into 100 clients each containing 500 training and 100 testing examples; since there isn’t a natural user partitioning of this data, we considered the balanced and IID setting. The model architecture was taken from the TensorFlow tutorial [38], which consists of two convolutional layers followed by two fully connected layers and then a linear transformation layer to produce logits, for a total of about $1 0 ^ { 6 }$ parameters. Note that state-of-the-art approaches have achieved a test accuracy of $9 6 . 5 \%$ [19] for CIFAR; nevertheless, the standard model we use is sufficient for our needs, as our goal is to evaluate our optimization method, not achieve the best possible accuracy on this task. The images are preprocessed as part of the training input pipeline, which consists of cropping the images to 24x24, randomly flipping left-right and adjusting the contrast, brightness and whitening.

Table 3: Number of rounds and speedup relative to baseline SGD to reach a target test-set accuracy on CIFAR10. SGD used a minibatch size of 100. FedSGD and FedAvg used $C = 0 . 1$ , with FedAvg using $E = 5$ and $B = 5 0$ .   

<table><tr><td>Acc.</td><td colspan="2">80%</td><td colspan="2">82%</td><td colspan="2">85%</td></tr><tr><td>SGD</td><td>18000</td><td>（一）</td><td>31000</td><td>（一）</td><td>99000</td><td>（1）</td></tr><tr><td>FEDSGD</td><td></td><td>3750(4.8x)</td><td>6600(4.7x)</td><td></td><td>N/A</td><td>（ -)</td></tr><tr><td>FEDAVG</td><td></td><td>280(64.3x)</td><td></td><td>630(49.2x)</td><td>2000 (49.5x)</td><td></td></tr></table>

For these experiments, we considered an additional baseline, standard SGD training on the full training set (no user partitioning), using minibatches of size 100. We achieved an $86 \%$ test accuracy after 197,500 minibatch updates (each minibatch update requires a communication round in the federated setting). FedAvg achieves a similar test accuracy of $85 \%$ after only 2,000 communication rounds. For all algorithms, we tuned a learning-rate decay parameter in addition to the initial learning rate. Table 3 gives the number of communication rounds for baseline SGD, FedSGD, and FedAvg to reach three different accuracy targets, and Figure 4 gives learning-rate curves for FedAvg versus FedSGD.

By running experiments with minibatches of size $B = 5 0$ for both SGD and FedAvg, we can also look at accuracy as a function of the number of such minibatch gradient calculations. We expect SGD to do better here, because a sequential step is taken after each minibatch computation. However, as Figure 9 in the appendix shows, for modest values of $C$ and $E$ , FedAvg makes a similar amount of progress per minibatch computation. Further, we see that both standard SGD and FedAvg with only one client per round $\left( C = 0 \right)$ ), demonstrate significant oscillations in accuracy, whereas averaging over more clients smooths this out.

Large-scale LSTM experiments We ran experiments on a large-scale next-word prediction task to demonstrate the effectiveness of our approach on a real-world problem. Our training dataset consists 10 million public posts from a large social network. We grouped the posts by author, for a total of over 500,000 clients. This dataset is a realistic proxy for the type of text entry data that would be present on a user’s mobile device. We limited each client dataset to at most 5000 words, and report accuracy (the fraction of the data where the highest predicted probability was on the correct next word, out of 10000 possibilities) on a test set of 1e5 posts from different (non-training) authors. Our model is a 256 node LSTM on a vocabulary of 10,000 words. The input and output embeddings for each word were of dimension 192, and co-trained with the model; there are 4,950,544 parameters in all. We used an unroll of 10 words.

![## Image Analysis: 17619a97fe419e12a92f876847ff0533987e47c9246d66127edb08d6064ed47e.jpg

**Conceptual Understanding:**
This image conceptually illustrates and compares the training performance of two federated learning optimization algorithms, FedSGD and FedAvg, on a Next Word Prediction LSTM model when data is non-IID. The main purpose is to demonstrate how each algorithm, with varying learning rates, impacts the model's test accuracy as the number of communication rounds (global updates) increases. It aims to highlight the relative effectiveness and convergence characteristics of FedAvg versus FedSGD in a challenging federated learning scenario, particularly showing that FedAvg (E=1) achieves superior performance in terms of accuracy and training efficiency.

**Content Interpretation:**
The image displays the learning curves for a Next Word Prediction Long Short-Term Memory (LSTM) model trained on Non-IID (non-independent and identically distributed) data using two different federated optimization algorithms: FedSGD and FedAvg (E=1). Each algorithm's performance is evaluated across various learning rates (η). The graph illustrates how test accuracy evolves over communication rounds, providing insights into the convergence and final performance of these algorithms under different hyperparameter settings. The 'E=1' in FedAvg (E=1) likely signifies that each client performs one local epoch of training before model aggregation.

**Key Insights:**
The main takeaway from this image is that the FedAvg (E=1) algorithm significantly outperforms FedSGD in terms of both convergence speed and peak test accuracy for the Next Word Prediction LSTM on Non-IID Data. This is evident from the higher and faster rising orange curves (FedAvg (E=1)) compared to the blue curves (FedSGD). For instance, FedAvg (E=1) with `η`=18.0 (darkest orange line) achieves the highest accuracy, reaching over 0.13, while the best FedSGD curve (`η`=22.0, darkest blue line) barely reaches 0.09. Additionally, the choice of learning rate (`η`) is critical, as different `η` values lead to varying performance within each algorithm group, with some values clearly leading to better or faster results. The monotonic nature of the curves, as referenced in the document text, is visually confirmed, indicating a consistent improvement in test accuracy over communication rounds.

**Document Context:**
This image is presented in the "3 Experimental Results" section of a document, immediately followed by the caption "Figure 5: Monotonic learning curves for the large-scale language model word LSTM." This context indicates that the graph is a crucial piece of evidence demonstrating the empirical performance of different training strategies for a large-scale language model. It specifically focuses on the learning behavior (monotonicity) and comparative effectiveness of FedSGD versus FedAvg on challenging non-IID data, which is a common scenario in federated learning. The figure directly supports the experimental findings related to the efficiency and accuracy achieved by these federated optimization methods.

**Summary:**
The image displays a line graph titled "Next Word Prediction LSTM, Non-IID Data" which plots the Test Accuracy @ 1 on the Y-axis against Communication Rounds on the X-axis. The X-axis ranges from 0 to 1000 Communication Rounds, with major ticks at 0, 200, 400, 600, 800, and 1000. The Y-axis, labeled "Test Accuracy @ 1", ranges from 0.00 to 0.14, with major ticks at 0.00, 0.02, 0.04, 0.06, 0.08, 0.10, 0.12, and 0.14. There is also a light gray horizontal line around 0.105 on the Y-axis, possibly indicating a baseline or target. The graph presents eight distinct learning curves, divided into two groups based on the optimization algorithm used: FedSGD and FedAvg (E=1). Each group shows the performance for different learning rates, denoted by 'η'. For FedSGD, four curves are plotted with `η` values of 6.0 (lightest blue), 9.0 (medium light blue), 18.0 (medium dark blue), and 22.0 (darkest blue). For FedAvg (E=1), four curves are plotted with `η` values of 3.0 (lightest orange), 6.0 (medium light orange), 9.0 (medium dark orange), and 18.0 (darkest orange). The FedAvg (E=1) curves generally show higher Test Accuracy @ 1 and faster convergence compared to the FedSGD curves across the Communication Rounds. Specifically, the FedAvg (E=1) curves quickly rise to reach accuracies between 0.12 and 0.14 within approximately 200-300 communication rounds, then show minor fluctuations and a slight increase towards 1000 rounds. In contrast, the FedSGD curves exhibit slower initial growth, generally staying below 0.10 Test Accuracy @ 1 for most `η` values, with the `η`=22.0 FedSGD curve eventually reaching close to 0.09 at 1000 communication rounds. The FedAvg (E=1) curves consistently outperform the FedSGD curves in terms of both the speed of achieving higher accuracy and the peak accuracy reached.](images/17619a97fe419e12a92f876847ff0533987e47c9246d66127edb08d6064ed47e.jpg)
Figure 5: Monotonic learning curves for the large-scale language model word LSTM.

These experiments required significant computational resources and so we did not explore hyper-parameters as thoroughly: all runs trained on 200 clients per round; FedAvg used $B = 8$ and $E = 1$ . We explored a variety of learning rates for FedAvg and the baseline FedSGD. Figure 5 shows monotonic learning curves for the best learning rates. FedSGD with $\eta = 1 8 . 0$ required 820 rounds to reach $10 . 5 \%$ accuracy, while FedAvg with $\eta = 9 . 0$ reached an accuracy of $10 . 5 \%$ in only 35 communication rounds $2 3 \times$ fewer then FedSGD). We observed lower variance in test accuracy for FedAvg, see Figure 10 in Appendix A. This figure also include results for $E = 5$ , which performed slightly worse than $E = 1$ .

# 4 Conclusions and Future Work

Our experiments show that federated learning can be made practical, as FedAvg trains high-quality models using relatively few rounds of communication, as demonstrated by results on a variety of model architectures: a multi-layer perceptron, two different convolutional NNs, a two-layer character LSTM, and a large-scale word-level LSTM.

While federated learning offers many practical privacy benefits, providing stronger guarantees via differential privacy [14, 13, 1], secure multi-party computation [18], or their combination is an interesting direction for future work. Note that both classes of techniques apply most naturally to synchronous algorithms like FedAvg.8

References   
[1] Martin Abadi, Andy Chu, Ian Goodfellow, Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In 23rd ACM Conference on Computer and Communications Security (ACM CCS), 2016.   
[2] Monica Anderson. Technology device ownership: 2015. http://www. pewinternet.org/2015/10/29/ technology-device-ownership-2015/, 2015.   
[3] Yossi Arjevani and Ohad Shamir. Communication complexity of distributed convex learning and optimization. In Advances in Neural Information Processing Systems 28. 2015.   
[4] Maria-Florina Balcan, Avrim Blum, Shai Fine, and Yishay Mansour. Distributed learning, communication complexity and privacy. arXiv preprint arXiv:1204.3514, 2012.   
[5] Yoshua Bengio, Rejean Ducharme, Pascal Vincent, ´ and Christian Janvin. A neural probabilistic language model. J. Mach. Learn. Res., 2003.   
[6] Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. Brendan McMahan, Sarvar Patel, Daniel Ramage, Aaron Segal, and Karn Seth. Practical secure aggregation for federated learning on user-held data. In NIPS Workshop on Private Multi-Party Machine Learning, 2016.   
[7] David L. Chaum. Untraceable electronic mail, return addresses, and digital pseudonyms. Commun. ACM, 24(2), 1981.   
[8] Jianmin Chen, Rajat Monga, Samy Bengio, and Rafal Jozefowicz. Revisiting distributed synchronous sgd. In ICLR Workshop Track, 2016.   
[9] Anna Choromanska, Mikael Henaff, Michael Mathieu, ¨ Gerard Ben Arous, and Yann LeCun. The loss surfaces ´ of multilayer networks. In AISTATS, 2015.   
[10] Greg Corrado. Computer, respond to this email. http:// googleresearch.blogspot.com/2015/ 11/computer-respond-to-this-email. html, November 2015.   
[11] Yann N. Dauphin, Razvan Pascanu, C¸ aglar Gul¨ c¸ehre, KyungHyun Cho, Surya Ganguli, and Yoshua Bengio. Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS, 2014.   
[12] Jeffrey Dean, Greg S. Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew Senior, Paul Tucker, Ke Yang, and Andrew Y. Ng. Large scale distributed deep networks. In NIPS, 2012.   
[13] John Duchi, Michael I. Jordan, and Martin J. Wainwright. Privacy aware learning. Journal of the Association for Computing Machinery, 2014.   
[14] Cynthia Dwork and Aaron Roth. The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science. Now Publishers, 2014.   
[15] Olivier Fercoq, Zheng Qu, Peter Richtarik, and Martin ´ Takac. Fast distributed coordinate descent for non- ´ strongly convex losses. In Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on, 2014.   
[16] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. Book in preparation for MIT Press, 2016.   
[17] Ian J. Goodfellow, Oriol Vinyals, and Andrew M. Saxe. Qualitatively characterizing neural network optimization problems. In ICLR, 2015.   
[18] Slawomir Goryczka, Li Xiong, and Vaidy Sunderam. Secure multiparty aggregation with differential privacy: A comparative study. In Proceedings of the Joint EDBT/ICDT 2013 Workshops, 2013.   
[19] Benjamin Graham. Fractional max-pooling. CoRR, abs/1412.6071, 2014. URL http://arxiv.org/ abs/1412.6071.   
[20] Sepp Hochreiter and Jurgen Schmidhuber. Long short- ¨ term memory. Neural Computation, 9(8), November 1997.   
[21] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In ICML, 2015.   
[22] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M. Rush. Character-aware neural language models. CoRR, abs/1508.06615, 2015.   
[23] Jakub Konecnˇ y, H. Brendan McMahan, Felix X. Yu, ´ Peter Richtarik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. In NIPS Workshop on Private Multi-Party Machine Learning, 2016.   
[24] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.   
[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classification with deep convolutional neural networks. In NIPS. 2012.   
[26] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 1998.   
[27] Chenxin Ma, Virginia Smith, Martin Jaggi, Michael I Jordan, Peter Richtarik, and Martin Tak ´ a´c. Adding vs. ˇ averaging in distributed primal-dual optimization. In ICML, 2015.   
[43] Yuchen Zhang and Lin Xiao. Communication-efficient distributed optimization of self-concordant empirical loss. arXiv preprint arXiv:1501.00263, 2015.   
[44] Yuchen Zhang, Martin J Wainwright, and John C Duchi. Communication-efficient algorithms for statistical optimization. In NIPS, 2012.   
[45] Yuchen Zhang, John Duchi, Michael I Jordan, and Martin J Wainwright. Information-theoretic lower bounds for distributed statistical estimation with communication constraints. In Advances in Neural Information Processing Systems, 2013.   
[46] Martin Zinkevich, Markus Weimer, Lihong Li, and Alex J. Smola. Parallelized stochastic gradient descent. In NIPS. 2010.   
[28] Ryan McDonald, Keith Hall, and Gideon Mann. Distributed training strategies for the structured perceptron. In NAACL HLT, 2010.   
[29] Natalia Neverova, Christian Wolf, Griffin Lacey, Lex Fridman, Deepak Chandra, Brandon Barbello, and Graham W. Taylor. Learning human identity from motion patterns. IEEE Access, 4:1810–1820, 2016.   
[30] Jacob Poushter. Smartphone ownership and internet usage continues to climb in emerging economies. Pew Research Center Report, 2016.   
[31] Daniel Povey, Xiaohui Zhang, and Sanjeev Khudanpur. Parallel training of deep neural networks with natural gradient and parameter averaging. In ICLR Workshop Track, 2015.   
[32] William Shakespeare. The Complete Works of William Shakespeare. Publically available at https: //www.gutenberg.org/ebooks/100.   
[33] Ohad Shamir and Nathan Srebro. Distributed stochastic optimization and learning. In Communication, Control, and Computing (Allerton), 2014.   
[34] Ohad Shamir, Nathan Srebro, and Tong Zhang. Communication efficient distributed optimization using an approximate newton-type method. arXiv preprint arXiv:1312.7853, 2013.   
[35] Reza Shokri and Vitaly Shmatikov. Privacy-preserving deep learning. In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security, CCS ’15, 2015.   
[36] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Dropout: A simple way to prevent neural networks from overfitting. 15, 2014.   
[37] Latanya Sweeney. Simple demographics often identify people uniquely. 2000.   
[38] TensorFlow team. Tensorflow convolutional neural networks tutorial, 2016. http://www.tensorflow. org/tutorials/deep_cnn.   
[39] White House Report. Consumer data privacy in a networked world: A framework for protecting privacy and promoting innovation in the global digital economy. Journal of Privacy and Confidentiality, 2013.   
[40] Tianbao Yang. Trading computation for communication: Distributed stochastic dual coordinate ascent. In Advances in Neural Information Processing Systems, 2013.   
[41] Ruiliang Zhang and James Kwok. Asynchronous distributed admm for consensus optimization. In ICML. JMLR Workshop and Conference Proceedings, 2014.   
[42] Sixin Zhang, Anna E Choromanska, and Yann LeCun. Deep learning with elastic averaging sgd. In NIPS. 2015.

# A Supplemental Figures and Tables

![## Image Analysis: dfd7828f52875216d60bd0ff7b8e385aaa32ce349b59ad76c49a2b602e65d8cb.jpg

**Conceptual Understanding:**
This image conceptually illustrates the convergence behavior of a Convolutional Neural Network (CNN) trained on the MNIST dataset within a federated learning framework. The core idea is to show how effectively the model learns and reduces its training error (Train Loss) over a series of communication rounds between clients and a central server.

The main purpose of these plots is to compare the impact of different hyperparameter choices—specifically the client batch size ('B') and the number of local training epochs ('E')—under two fundamental data distribution scenarios: Independent and Identically Distributed (IID) and Non-Independent and Identically Distributed (Non-IID) data. This comparison aims to reveal which parameter configurations lead to faster, more stable, and ultimately lower training loss, and how these choices interact with data heterogeneity.

Key ideas communicated are:
1.  **Convergence Dynamics:** How 'Train Loss' decreases as 'Communication Rounds' increase, indicating the learning progress.
2.  **Hyperparameter Sensitivity:** The varying performance (speed and final loss) across different 'B' and 'E' combinations.
3.  **Data Heterogeneity Impact:** The stark differences in convergence patterns and relative performance between the 'IID' and 'Non-IID' conditions, highlighting the challenges posed by non-uniform data distributions in federated learning.
4.  **Trade-offs in Federated Learning:** The plots implicitly suggest trade-offs between communication efficiency (fewer rounds to converge) and computational load (higher 'E'), and the robustness of different strategies under data heterogeneity.

**Content Interpretation:**
The image displays two line plots that illustrate the training loss convergence of an MNIST Convolutional Neural Network (CNN) in a federated learning setting. The x-axis represents 'Communication Rounds', indicating the number of rounds of communication between clients and the server. The y-axis, 'Train Loss', is on a log scale, showing the reduction in the model's error during training.

Each plot compares nine different training scenarios, defined by combinations of two parameters: 'B' (likely representing batch size or client batch size) and 'E' (likely representing local epochs or the number of local updates performed by each client per communication round). These scenarios are plotted for two distinct data distribution conditions: 'MNIST CNN IID' (Independent and Identically Distributed data, meaning each client's data is representative of the global distribution) and 'MNIST CNN Non-IID' (Non-Independent and Identically Distributed data, meaning client data distributions may differ significantly from each other and the global distribution).

The plots show how quickly and effectively the training loss decreases under these different parameter settings and data conditions. A steeper downward slope indicates faster convergence, and lower values on the 'Train Loss' axis indicate better model performance.

**Extracted text supporting this interpretation:**
- **Titles:** 'MNIST CNN IID', 'MNIST CNN Non-IID' clearly define the two experimental conditions.
- **Axis Labels:** 'Train Loss' and 'Communication Rounds' identify the metrics being measured and the progression of training.
- **Y-axis scale:** '5e-01', '5e-02', '5e-03', '5e-04' (logarithmic scale) indicates the range and type of loss measurement.
- **Legend entries:** 'B=10 E=1', 'B=10 E=5', 'B=10 E=20', 'B=50 E=1', 'B=50 E=5', 'B=50 E=20', 'B=∞ E=1', 'B=∞ E=5', 'B=∞ E=20' explicitly define the nine experimental configurations being compared on each plot. These parameters (B and E) are the key variables being investigated for their impact on convergence.

**Key Insights:**
The image provides several key takeaways and insights into the training convergence of an MNIST CNN in a federated learning context, especially concerning the impact of client batch size (B), local epochs (E), and data distribution (IID vs. Non-IID):

1.  **Impact of Local Epochs (E) on Convergence Speed (IID Data):** For 'MNIST CNN IID' data, increasing the number of local epochs ('E') generally leads to significantly faster initial convergence. For a fixed 'B' value, higher 'E' values (e.g., 'B=10 E=20' vs. 'B=10 E=1' or 'B=50 E=20' vs. 'B=50 E=1') result in a much quicker drop in 'Train Loss' within fewer 'Communication Rounds'. This suggests that allowing clients to perform more local computations can accelerate the learning process when data is well-distributed.
    *   **Textual Evidence:** Comparison of 'B=10 E=1' (solid red), 'B=10 E=5' (dashed red), 'B=10 E=20' (dotted red) and 'B=50 E=1' (solid orange), 'B=50 E=5' (dashed orange), 'B=50 E=20' (dotted orange) lines in the 'MNIST CNN IID' plot, showing dotted lines converging fastest.

2.  **Trade-offs with Large Batch Sizes (B) or 'B=∞' (IID Data):** When 'B=∞' (representing a very large batch size or possibly a centralized approach in some contexts), the convergence is generally slower, especially for lower 'E' values (e.g., 'B=∞ E=1'). Even with increased local epochs ('B=∞ E=5', 'B=∞ E=20'), the 'Train Loss' reduction is not as rapid as with smaller 'B' values combined with higher 'E' values in the initial rounds.
    *   **Textual Evidence:** The 'B=∞ E=1' (solid blue) line in the 'MNIST CNN IID' plot shows the slowest initial reduction in 'Train Loss' and takes the longest to reach lower loss values, even compared to 'B=10 E=1' or 'B=50 E=1'.

3.  **Challenges with Non-IID Data:** The 'MNIST CNN Non-IID' plot generally shows a more complex convergence behavior. While higher 'E' values still contribute to faster initial drops, the overall loss values achieved and the stability of convergence can differ compared to the IID case. The performance differences between various parameter settings might be less pronounced or change in relative order under Non-IID conditions.
    *   **Textual Evidence:** Comparing the overall spread and slopes of the lines in 'MNIST CNN Non-IID' vs. 'MNIST CNN IID'. For example, 'B=∞ E=1' (solid blue) in 'Non-IID' shows a more consistent decrease compared to its IID counterpart's plateau.

4.  **Resilience of 'B=∞' to Non-IID Data:** Interestingly, in the 'MNIST CNN Non-IID' plot, the 'B=∞' configurations (blue lines), especially with higher 'E' values like 'B=∞ E=5' and 'B=∞ E=20', appear to perform relatively better than some of the smaller 'B' values in terms of achieving lower final loss and more stable convergence, contrasting with their slower initial performance in the IID case. This could suggest that for highly Non-IID data, larger effective batch sizes or a more global aggregation strategy might be more robust.
    *   **Textual Evidence:** Observing that the blue lines ('B=∞ E=5', 'B=∞ E=20') in 'MNIST CNN Non-IID' eventually achieve comparable or lower loss values than some of the red ('B=10') or orange ('B=50') lines, and do not exhibit the significant performance gap seen in the IID plot with 'B=∞ E=1'.

5.  **Importance of Parameter Tuning for Data Distribution:** The plots collectively highlight that there is no single 'best' set of parameters (B, E) for all scenarios. The optimal configuration for achieving fast and effective convergence critically depends on whether the data is IID or Non-IID. For example, aggressive local training (high E, low B) is highly effective for IID data, but its benefits might be mitigated, or different strategies (like higher B or B=∞) might become more viable, under Non-IID conditions.
    *   **Textual Evidence:** The distinct patterns and relative performance of all nine lines across the 'MNIST CNN IID' and 'MNIST CNN Non-IID' plots underscore the need for context-specific parameter tuning.

**Document Context:**
This image, described as 'Figure 6: Training set convergence for the MNIST CNN', directly supports the document's broader narrative by providing empirical evidence regarding the performance of a Convolutional Neural Network (CNN) on the MNIST dataset under federated learning conditions. Specifically, it investigates how varying communication parameters (batch size 'B' and local epochs 'E') affect training convergence when data is distributed in either an Independent and Identically Distributed (IID) or Non-IID manner among clients.

The text after the image, 'Note the y-axis is on a log scale, and the x-axis covers more training than Figure 2. These plots fix C = 0.1', provides crucial context. It highlights specific technical details of the plots (log scale y-axis, extended x-axis range compared to Figure 2) and specifies a fixed parameter 'C=0.1', which is not explicitly shown on the graphs but is essential for understanding the experimental setup and reproducibility.

The figure's main purpose is to demonstrate the impact of client-side computation (E) and client participation/batch size (B) on the efficiency and effectiveness of federated learning, particularly when data heterogeneity (Non-IID) is a factor. By presenting results for both IID and Non-IID scenarios, the image helps to illustrate the challenges and potential solutions for training robust models in decentralized environments where data distribution is often not uniform. It allows readers to compare convergence rates and final loss values across different parameter settings to infer optimal configurations for different data characteristics.

**Summary:**
This image displays two line plots side-by-side, visualizing the training loss convergence for an MNIST Convolutional Neural Network (CNN) across different communication rounds under two distinct data distribution scenarios: 'IID' (Independent and Identically Distributed) and 'Non-IID'. Both plots share the same y-axis, labeled 'Train Loss', which is on a logarithmic scale, with tick marks at '5e-01', '5e-02', '5e-03', and '5e-04'. The x-axis for both plots is labeled 'Communication Rounds' and has tick marks at '0', '1000', '3000', and '5000'.

Each plot contains nine distinct lines, differentiated by color and line style, representing various combinations of parameters 'B' and 'E'. The legend, identical for both plots, specifies these parameter settings:
- Red lines (B=10): 'B=10 E=1' (solid), 'B=10 E=5' (dashed), 'B=10 E=20' (dotted).
- Orange lines (B=50): 'B=50 E=1' (solid), 'B=50 E=5' (dashed), 'B=50 E=20' (dotted).
- Blue lines (B=∞): 'B=∞ E=1' (solid), 'B=∞ E=5' (dashed), 'B=∞ E=20' (dotted).

The plots illustrate how different batch sizes (B) and local epoch counts (E) impact the training convergence behavior of the CNN under varying data distribution assumptions. In the 'MNIST CNN IID' plot, parameters with lower 'B' values (B=10, B=50) and higher 'E' values (E=5, E=20) generally achieve faster initial loss reduction. For instance, the 'B=10 E=20' (dotted red) and 'B=50 E=20' (dotted orange) lines drop rapidly to very low train loss values within fewer than 1000 communication rounds. Conversely, 'B=∞' (blue lines) configurations show a slower and more gradual decrease in training loss, with 'B=∞ E=1' (solid blue) having the slowest convergence.

In the 'MNIST CNN Non-IID' plot, the overall pattern is similar, but there are notable differences in convergence speeds and stability. While 'B=10 E=20' and 'B=50 E=20' still show fast initial drops, the 'B=∞' configurations (blue lines) appear to have relatively better performance compared to their IID counterparts in terms of reaching lower loss values, especially 'B=∞ E=5' and 'B=∞ E=20' which converge faster than 'B=∞ E=1' and also faster than some of the B=10 and B=50 configurations in the Non-IID setting. The curves for 'B=∞' in the Non-IID case do not exhibit the same plateau observed in the 'B=∞ E=1' curve in the IID case, indicating a more consistent, albeit still slower, reduction in loss.

The image allows for a detailed comparison of the effectiveness of different federated learning parameters (batch size and local epochs) on training loss convergence for an MNIST CNN when data is either IID or Non-IID. It highlights that optimal parameter choices for federated learning can vary significantly depending on the data distribution characteristics.](images/dfd7828f52875216d60bd0ff7b8e385aaa32ce349b59ad76c49a2b602e65d8cb.jpg)
Figure 6: Training set convergence for the MNIST CNN. Note the $y$ -axis is on a log scale, and the $x$ -axis covers more training than Figure 2. These plots fix $C = 0 . 1$ .

![## Image Analysis: ad06b660399440df891793d67f6b6cc17b20643d5e2a889d46dc5494cbf2165e.jpg

**Conceptual Understanding:**
This image conceptually illustrates the performance evaluation of a federated learning model, specifically 'MNIST 2NN', under varying data distribution scenarios and hyper-parameter settings. The main purpose is to compare how 'Test Accuracy' evolves over 'Communication Rounds', highlighting the influence of whether the data across clients is 'IID' (Independent and Identically Distributed) or 'Non-IID', and the impact of local training parameters 'B' (likely client batch size or fraction) and 'E' (local epochs). It aims to demonstrate the robustness and efficiency of the federated learning approach in different data environments, particularly assessing convergence behavior and the eventual accuracy achieved by the model under various configurations of local computation and communication strategies. The figure provides insights into how to tune these parameters for optimal performance given different data characteristics.

**Content Interpretation:**
The image displays two performance curves for a federated learning setup, specifically for an 'MNIST 2NN' model. The Y-axis, 'Test Accuracy', indicates the model's predictive performance, ranging from 0.94 to 0.98. The X-axis, 'Communication Rounds', represents the number of global aggregation steps performed. The two main systems being shown are the model's performance on 'IID' (Independent and Identically Distributed) data and 'Non-IID' data, which are fundamental data distribution scenarios in federated learning. Each line in the graphs represents a distinct training configuration defined by 'B' and 'E' parameters. 'B' likely refers to the batch size for local training or the fraction of clients participating in each round (as suggested by the problem context C=0.1 and optimized eta), while 'E' denotes the number of local epochs clients perform before sending updates. The data shows how different combinations of these parameters affect the test accuracy over time for both IID and Non-IID data. For example, in the IID case, most configurations (e.g., B=10 E=10, B=10 E=20, B=50 E=10, B=50 E=20, B=∞ E=10, B=∞ E=20) rapidly achieve high accuracy (above 0.97) within fewer than 500 communication rounds, whereas 'B=∞ E=1' takes significantly longer to reach similar accuracy levels. For the Non-IID case, the convergence generally appears slower and the peak accuracy might be slightly lower compared to the IID scenario for some configurations, especially for 'B=∞ E=1' which struggles to reach 0.97 even after 2000 rounds. The 'B=∞' configurations generally represent a larger batch size or more comprehensive client participation, and their performance varies depending on the 'E' value and data distribution.

**Key Insights:**
The main takeaways from this image are: 1.  **Impact of Data Distribution:** Federated learning models generally achieve higher test accuracy faster when data is 'IID' compared to 'Non-IID'. This is evident by the steeper initial accuracy curves and higher final accuracy plateaus in the 'MNIST 2NN IID' graph for most configurations, whereas the 'MNIST 2NN Non-IID' graph shows slower convergence and sometimes lower peak accuracies, especially for configurations like 'B=∞ E=1'. 2.  **Role of Local Epochs (E) and Batch Size (B):** Different combinations of 'B' and 'E' significantly affect convergence speed and final accuracy. For instance, in the IID case, larger local epochs (E=10, E=20) generally lead to faster initial convergence across various 'B' values. Configurations with 'B=10' or 'B=50' and higher 'E' values (E=10, E=20) tend to converge quickly to high accuracy for both IID and Non-IID data, often outperforming 'B=∞ E=1'. 3.  **Specific Configuration Performance:** The configuration 'B=∞ E=1' consistently exhibits the slowest convergence and often the lowest accuracy among the tested parameters for both IID and Non-IID data, indicating that a larger 'batch size'/'client participation' without sufficient local computation can be detrimental or inefficient. Conversely, higher local computation (larger 'E' values) generally improves convergence speed and accuracy, even with smaller 'B' values. For example, 'B=10 E=20' achieves high accuracy rapidly in both scenarios. 4.  **Trade-offs in Federated Learning:** The results highlight the trade-offs between communication efficiency (fewer communication rounds needed for convergence) and computational intensity (higher 'E' values) or client participation ('B' values). Optimized configurations can achieve high accuracy within fewer communication rounds, which is crucial for practical federated learning systems. The detailed legend entries like 'B=10 E=10' and 'B=50 E=20' provide the specific parameter settings that yield these observed performances.

**Document Context:**
This image directly supports the document's narrative by visually demonstrating the impact of data distribution (IID vs. Non-IID) and federated learning hyper-parameters (client fraction/batch size 'B' and local epochs 'E') on the test accuracy and convergence speed of an 'MNIST 2NN' model. The figure complements the text's discussion of 'C = 0.1' and 'optimized eta' by providing empirical results for accuracy against communication rounds under these conditions. It serves as a key piece of evidence for understanding the performance characteristics of the proposed or analyzed federated learning algorithm, particularly highlighting challenges and effective configurations when dealing with non-IID data, which is a common and difficult scenario in real-world federated learning deployments. The comparison between IID and Non-IID datasets directly addresses a critical area of research in federated learning, providing visual data to support conclusions about algorithm robustness and efficiency under varying data conditions. The figure, along with its caption (which mentions 'Test set accuracy vs. communication rounds for MNIST 2NN with C = 0.1 and optimized η. The left column is the IID dataset, and right is the pathological 2- digits-per-client non-IID data.'), collectively explains the experimental results and validates the model's behavior under different settings.

**Summary:**
The image presents two line graphs, side-by-side, illustrating the 'Test Accuracy' of an 'MNIST 2NN' model against 'Communication Rounds'. The left graph is for 'MNIST 2NN IID' data, and the right graph is for 'MNIST 2NN Non-IID' data. Both graphs share the same Y-axis, labeled 'Test Accuracy', with numerical ticks at 0.94, 0.95, 0.96, 0.97, and 0.98. The X-axis for both graphs is labeled 'Communication Rounds', with numerical ticks at 0, 500, 1000, 1500, and 2000. Each graph contains nine distinct lines, each representing a different configuration of 'B' (batch size or client fraction) and 'E' (local epochs), as detailed in the legend. The legend entries are identical for both plots and include: 'B=10 E=1', 'B=10 E=10', 'B=10 E=20', 'B=50 E=1', 'B=50 E=10', 'B=50 E=20', 'B=∞ E=1', 'B=∞ E=10', and 'B=∞ E=20'. The lines are distinguished by color and style (solid, dashed, dotted, etc.) to represent these different parameter combinations. The overall presentation allows for a direct comparison of how test accuracy evolves over communication rounds under various training configurations for both IID and Non-IID data distributions.](images/ad06b660399440df891793d67f6b6cc17b20643d5e2a889d46dc5494cbf2165e.jpg)
Figure 7: Test set accuracy vs. communication rounds for MNIST 2NN with $C = 0 . 1$ and optimized $\eta$ . The left column is the IID dataset, and right is the pathological 2- digits-per-client non-IID data.

![## Image Analysis: b0e3a4a3004f05c7a5016557833153a4ae2b550fcab54c3258cb03bc234923ed.jpg

**Conceptual Understanding:**
This image conceptually represents an experimental evaluation of a federated learning algorithm, specifically how the local training intensity (parameter 'E', number of local epochs) influences the convergence of a Convolutional Neural Network (CNN) trained on the MNIST dataset. The main purpose is to demonstrate and compare the training performance, measured by "Train Loss", over successive "Communication Rounds" under two crucial data distribution scenarios: Independent and Identically Distributed (IID) and Non-IID. The core message conveyed is that while increased local computation (higher 'E') is generally beneficial for IID data, its efficacy becomes complex and less predictable, and potentially detrimental, in the presence of challenging Non-IID data distributions, necessitating careful parameter tuning and highlighting the inherent difficulties of federated learning with non-uniformly distributed data. Key ideas communicated include the trade-offs between local computation and global communication, the impact of data heterogeneity on model convergence, and the need for distinct strategies for different data distribution characteristics.

**Content Interpretation:**
The image displays the training loss of a Convolutional Neural Network (CNN) on the MNIST dataset as a function of communication rounds in a federated learning setup. The main concept being explored is the impact of varying the number of local epochs ('E') between global averaging steps under two different data distribution conditions: Independent and Identically Distributed (IID) and Non-IID.

The left plot (titled "MNIST CNN η=0.215 IID") shows the training performance when data is IID, which is a simpler scenario. The y-axis, "Train Loss", decreases smoothly and rapidly for all 'E' values, with larger 'E' generally leading to faster convergence and lower final loss. This indicates that more local computation (higher 'E') is beneficial when data is well-distributed.

The right plot (titled "MNIST CNN η=0.1 Non-IID") illustrates the training performance when data is Non-IID, which is a more challenging and realistic scenario for federated learning. The y-axis, "Train Loss", is on a different, higher scale ("1.00" down to "0.02") compared to the IID plot ("0.500" down to "0.005"), signifying increased training difficulty. The convergence patterns are more unstable and less consistent across different 'E' values. High 'E' values do not uniformly yield the best results, and some even show less stable or poorer performance towards the end of training. This highlights the complexities introduced by Non-IID data, where simply increasing local epochs might not be optimal or might even be detrimental to convergence.

**Key Insights:**
The main takeaways from this image are:

1.  **Effectiveness of Local Epochs (E) on IID Data:** For Independently and Identically Distributed (IID) data, increasing the number of local epochs ('E') between communication rounds generally leads to faster convergence and lower ultimate training loss. This is evident in the left plot, where lines for "E=100", "E=200", and "E=400" descend more steeply and reach lower "Train Loss" values compared to smaller 'E' values.

2.  **Challenges with Non-IID Data:** Training on Non-IID data presents significantly greater challenges, resulting in generally higher training loss and less stable convergence patterns. This is supported by the right plot's higher "Train Loss" y-axis scale ("1.00" down to "0.02") compared to the IID plot, and the more erratic, less consistently improving loss curves.

3.  **Complex Relationship of E with Non-IID Data:** For Non-IID data, simply increasing 'E' does not guarantee improved performance. The right plot shows that very high 'E' values (e.g., "E=200", "E=400") do not always achieve the lowest loss or the most stable convergence. This contrasts with the IID case and suggests that an optimal 'E' might be much smaller or that other regularization techniques are needed for Non-IID settings.

4.  **Importance of Data Distribution Context:** The figure underscores that the choice and impact of hyper-parameters like 'E' are highly dependent on the data distribution (IID vs. Non-IID), necessitating different strategies and expectations for each scenario. The use of "different learning rates" (η=0.215 vs η=0.1) and "y-axis scales" for the two plots directly provides textual evidence for the distinct challenges and tuning required for the "pathological non-IID MNIST dataset".

**Document Context:**
The image, labeled as Figure 8, directly supports the document's discussion on the effects of training for many local epochs (large 'E') between averaging steps in a federated learning context, with fixed parameters B=10 and C=0.1. The accompanying text explicitly states that the figure shows the "Training loss for the MNIST CNN" and notes that "different learning rates and y-axis scales are used due to the difficulty of our pathological non-IID MNIST dataset." This aligns perfectly with the visual representation, which clearly demonstrates how the 'E' parameter influences training loss convergence under both IID and Non-IID data distributions, using the specified learning rates (η=0.215 for IID and η=0.1 for Non-IID) and the distinct y-axis scales reflecting the varying levels of training difficulty. The figure serves as empirical evidence illustrating the challenges posed by non-IID data and the varied impact of local computation settings.

**Summary:**
The image displays two side-by-side line graphs, both illustrating the relationship between "Train Loss" on the y-axis and "Communication Rounds" on the x-axis for an "MNIST CNN". Each graph presents multiple colored lines, with each line representing a different value for the parameter 'E', which signifies the number of local epochs between averaging steps. The legend for both graphs consistently lists the 'E' values and their corresponding line colors: "E=1" (blue), "E=5" (purple), "E=25" (yellow), "E=50" (black), "E=100" (orange), "E=200" (red), and "E=400" (green).

The left graph is titled "MNIST CNN η=0.215 IID". Its y-axis, labeled "Train Loss", has tick marks at "0.500", "0.100", "0.020", and "0.005". The x-axis, labeled "Communication Rounds", has tick marks at "0", "20", "40", "60", "80", and "100". In this IID (Independent and Identically Distributed) data scenario, a learning rate (η) of "0.215" is used. The lines show a consistent decrease in "Train Loss" as "Communication Rounds" increase, with higher 'E' values generally leading to lower loss and faster convergence.

The right graph is titled "MNIST CNN η=0.1 Non-IID". Its y-axis, also labeled "Train Loss", has tick marks at "1.00", "0.50", "0.20", "0.10", "0.05", and "0.02". The x-axis, labeled "Communication Rounds", is identical to the left graph, with tick marks at "0", "20", "40", "60", "80", and "100". This graph represents training with Non-IID (Non-Independent and Identically Distributed) data using a learning rate (η) of "0.1". The "Train Loss" values are generally higher and the convergence patterns are more erratic compared to the IID case. Higher 'E' values do not consistently result in superior performance, with some showing oscillations or less stable convergence, reflecting the increased difficulty of the Non-IID dataset.](images/b0e3a4a3004f05c7a5016557833153a4ae2b550fcab54c3258cb03bc234923ed.jpg)
Figure 8: The effect of training for many local epochs (large $E$ ) between averaging steps, fixing $B = 1 0$ and $C = 0 . 1$ . Training loss for the MNIST CNN. Note different learning rates and $y$ -axis scales are used due to the difficulty of our pathological non-IID MNIST dataset.

Table 4: Speedups in the number of communication rounds to reach a target accuracy of $9 7 \%$ for FedAvg, versus FedSGD (first row) on the MNIST 2NN model.   

<table><tr><td>MNIST2NN</td><td>E</td><td>B</td><td>u</td><td colspan="2">IID</td><td>NON-IID</td></tr><tr><td>FEDSGD</td><td>1</td><td>8</td><td>1</td><td>1468</td><td></td><td>1817</td></tr><tr><td>FEDAVG</td><td>10</td><td>8</td><td>10</td><td></td><td>156(9.4×)</td><td>1100 (1.7x)</td></tr><tr><td>FEDAVG</td><td>1</td><td>50</td><td>12</td><td></td><td>144(10.2×)</td><td>1183(1.5×)</td></tr><tr><td>FEDAVG</td><td>20</td><td>8</td><td>20</td><td></td><td>92 (16.0x)</td><td>957(1.9×)</td></tr><tr><td>FEDAVG</td><td>1</td><td>10</td><td>60</td><td></td><td>92 (16.0×)</td><td>831(2.2×)</td></tr><tr><td>FEDAVG</td><td>10</td><td>50</td><td>120</td><td></td><td>45(32.6x)</td><td>881(2.1x)</td></tr><tr><td>FEDAVG</td><td>20</td><td>50</td><td>240</td><td></td><td>39 (37.6x)</td><td>835(2.2×）</td></tr><tr><td>FEDAVG</td><td>10</td><td>10</td><td>600</td><td></td><td>34(43.2x)</td><td>497 (3.7×)</td></tr><tr><td>FEDAVG</td><td>20</td><td>10</td><td>1200</td><td></td><td>32(45.9x)</td><td>738(2.5x)</td></tr></table>

![## Image Analysis: c9f87d2699d4f72aa470e670afcc5aae22c5395e18e465f8bb91e8d26aa3362b.jpg

**Conceptual Understanding:**
This image conceptually represents a performance comparison of different machine learning optimization algorithms, specifically Federated Averaging (FedAvg) and Stochastic Gradient Descent (SGD), in terms of their training efficiency and achieved accuracy. The main purpose is to illustrate how varying key parameters within the FedAvg framework (client fraction C, local epochs E, and learning rate η) affects the model's ability to learn and generalize on the CIFAR-10 dataset, measured by test accuracy against the computational cost (number of minibatch gradient computations). It aims to show the trade-offs and optimal settings for distributed learning compared to a centralized approach.

**Content Interpretation:**
The image is a line graph displaying the relationship between 'Test Accuracy' (Y-axis) and 'Number of Minibatch Gradient Computations' (X-axis) for different machine learning optimization algorithms on the CIFAR-10 dataset. It shows the performance curves for various configurations of Federated Averaging (FedAvg) and a baseline of Stochastic Gradient Descent (SGD). The graph illustrates how modifications to parameters like client fraction (C), local epochs (E), and learning rate (η) in FedAvg impact its convergence speed and final accuracy, in comparison to a standard centralized SGD approach. The processes being shown are the training progress and performance evaluation of these algorithms over computational budget.

**Key Insights:**
The main takeaways from this image are: 1. Standard SGD (η=0.15) provides a strong baseline, achieving high accuracy (above 0.8) relatively quickly. 2. FedAvg can achieve performance comparable to SGD, especially with C=0.0, E=5, η=0.05, which closely matches SGD's curve. This suggests that with careful parameter tuning, FedAvg can be as efficient and accurate as centralized SGD in terms of gradient computations. 3. Increasing the number of local epochs (E) while keeping the client fraction (C=0.1) constant (e.g., E=10, E=20 vs. E=1, E=5) generally leads to higher final accuracy but may require more gradient computations to reach that level. 4. A very large client fraction (C=1.0) combined with a high learning rate (η=0.50), as shown by the dashed red line, results in significantly slower convergence and lower overall accuracy compared to other FedAvg configurations and SGD. This indicates that a high client fraction and learning rate might not be an optimal combination. 5. The choice of C, E, and η significantly influences the performance of FedAvg. For instance, C=0.0 (equivalent to one client per round) with E=5 local epochs and a small learning rate (η=0.05) performs very well, almost identical to SGD. This supports the idea that even with a low client participation, efficient federated learning is possible with appropriate parameter tuning. The extracted text elements from the legend (e.g., 'SGD, η = 0.15', 'FedAvg, C=0.0, E=5, η = 0.05', 'FedAvg, C=0.1, E=1, η = 0.30', etc.) directly provide the specific configurations and parameters that lead to these observed performance differences, serving as the evidence for these insights.

**Document Context:**
This image, Figure 9, directly supports the document's analysis of federated learning algorithms by visualizing the trade-offs and performance characteristics of FedAvg under different parameter settings (client fraction C, local epochs E, and learning rate η) compared to standard sequential SGD. The accompanying text states the baseline is standard sequential SGD, compared to FedAvg with different client fractions C (with C=0 meaning one client per round) and different numbers of local epochs E. This graph quantifies how these choices impact test accuracy relative to the number of minibatch gradient computations (given a batch size B=50), providing empirical evidence for the discussion on the efficiency and effectiveness of federated learning strategies.

**Summary:**
This graph illustrates the test accuracy of different machine learning optimization algorithms on the CIFAR-10 dataset as a function of the number of minibatch gradient computations. The baseline is standard sequential Stochastic Gradient Descent (SGD) with a learning rate (η) of 0.15. This is compared against various configurations of Federated Averaging (FedAvg), differing in client fraction (C), number of local epochs (E), and learning rate (η). The FedAvg configurations show a clear trade-off between the number of computations required to reach a certain accuracy and the peak accuracy achieved. Specifically, FedAvg with a client fraction C=0.0 and E=5 local epochs, using a small learning rate (η=0.05), performs very similarly to SGD in terms of both convergence speed and final accuracy, reaching over 0.8 test accuracy relatively quickly. Increasing the number of local epochs (E) for FedAvg, while keeping C=0.1, generally leads to slower initial convergence but can achieve competitive final accuracy, as seen with FedAvg, C=0.1, E=10, η=0.17 and FedAvg, C=0.1, E=20, η=0.15, which eventually surpass FedAvg with E=1 or E=5 for the same C. The FedAvg configuration with a large client fraction (C=1.0) and E=5 local epochs, using a significantly higher learning rate (η=0.50), shows the slowest convergence and lowest overall accuracy, indicating that a high client fraction and learning rate might not be optimal for this setup. The graph effectively demonstrates how varying these parameters impacts the performance of federated learning in terms of computational efficiency and model quality. The x-axis spans from 0 to 300,000 minibatch gradient computations, and the y-axis shows test accuracy ranging from 0.4 to 0.9, allowing for a detailed comparison of the algorithms' convergence behavior and ultimate performance.](images/c9f87d2699d4f72aa470e670afcc5aae22c5395e18e465f8bb91e8d26aa3362b.jpg)
Figure 9: Test accuracy versus number of minibatch gradient computations $[ B = 5 0 ]$ ). The baseline is standard sequential SGD, as compared to FedAvg with different client fractions $C$ (recall $C = 0$ means one client per round), and different numbers of local epochs $E$ .

![## Image Analysis: 9db91ca301c70dabb5f06edb59c86a557b80567050e8a5c56e9d549c032a57df.jpg

**Conceptual Understanding:**
This image conceptually represents the learning curves of a Next Word Prediction LSTM model. The main purpose is to compare the effectiveness and stability of two federated learning optimization algorithms, FedSGD and FedAvg, under different hyperparameter settings (learning rate η and local epochs E) when dealing with non-IID data. It communicates the key idea that FedAvg is a more robust and higher-performing approach for this specific task and data distribution compared to FedSGD, particularly highlighting the benefit of using fewer local epochs in FedAvg.

**Content Interpretation:**
The image displays a comparative analysis of the performance of a Next Word Prediction LSTM model under different federated learning algorithms, namely FedSGD and FedAvg, when trained on non-IID (Independent and Identically Distributed) data. The performance is measured by "Test Accuracy @ 1" over "Communication Rounds". The graph investigates the impact of the learning rate (η) across all algorithms and the number of local epochs (E) for FedAvg (E=1 vs E=5). FedAvg (both E=1 and E=5) shows a clear advantage over FedSGD in terms of achieving higher accuracy and exhibiting more stable learning curves. Specifically, FedAvg with fewer local epochs (E=1) generally performs better and with lower variance than FedAvg with more local epochs (E=5). FedSGD, while showing an initial increase, exhibits more volatile accuracy throughout the communication rounds and reaches a lower overall accuracy plateau.

**Key Insights:**
The main takeaways from this image are:
1.  **FedAvg outperforms FedSGD:** The FedAvg algorithms (both E=1 and E=5) consistently achieve significantly higher "Test Accuracy @ 1" compared to FedSGD. This is evident as the orange lines for FedAvg are positioned much higher on the graph than the blue lines for FedSGD.
2.  **Fewer local epochs (E=1) are better for FedAvg:** For the FedAvg algorithm, using fewer local epochs (E=1, solid orange lines) generally results in slightly higher or comparable accuracy and lower variance than using more local epochs (E=5, dashed orange lines). This is explicitly supported by the document's text, "FedAvg actually performs better with fewer local epochs E (1 vs 5)".
3.  **FedAvg exhibits lower variance in accuracy:** The learning curves for FedAvg are much smoother and less volatile than those for FedSGD, indicating more stable training and less fluctuation in accuracy across "Communication Rounds". This observation is also reinforced by the document's text: "and also has lower variance in accuracy across evaluation rounds compared to FedSGD."
4.  **Learning Rate (η) impact:** While the optimal η varies, FedAvg appears less sensitive to the specific η values within the tested range compared to FedSGD, where higher η values tend to introduce more significant oscillations.

**Document Context:**
This figure is presented as Supplemental Figure 10 and directly supports the document's broader discussion on federated learning algorithms for language models. It provides empirical evidence to illustrate the differences in performance, convergence, and stability between FedSGD and FedAvg when applied to a Next Word Prediction LSTM on non-IID data. The accompanying text, "Figure 10: Learning curves for the large-scale language model word LSTM, with evaluation computed every 20 rounds. FedAvg actually performs better with fewer local epochs E (1 vs 5), and also has lower variance in accuracy across evaluation rounds compared to FedSGD,", explicitly links the visual data to key findings regarding the superior performance of FedAvg with fewer local epochs and its lower variance compared to FedSGD. It strengthens the arguments made in the main body of the document regarding the efficacy and robustness of FedAvg.

**Summary:**
This image displays a line graph titled "Next Word Prediction LSTM, Non-IID Data" which illustrates the performance of a language model under different federated learning conditions. The x-axis is labeled "Communication Rounds" with tick marks at "0", "200", "400", "600", "800", and "100". The y-axis is labeled "Test Accuracy @ 1" with tick marks at "0.00", "0.02", "0.04", "0.06", "0.08", "0.10", "0.12", and "0.14". The graph contains three main legend categories: "FedSGD", "FedAvg (E=1)", and "FedAvg (E=5)", each with different learning rates (η). 

The "FedSGD" category is represented by solid blue lines of varying shades, corresponding to learning rates: "η=6.0" (lightest blue), "η=9.0" (medium light blue), "η=18.0" (medium dark blue), and "η=22.0" (darkest blue). The accuracy for FedSGD generally starts low and shows a rapid increase before fluctuating significantly, especially at higher η values, reaching peak accuracies around 0.08-0.10.

The "FedAvg (E=1)" category is represented by solid orange lines of varying shades, corresponding to learning rates: "η=3.0" (lightest orange), "η=6.0" (medium light orange), "η=9.0" (medium dark orange), and "η=18.0" (darkest orange). These curves show a rapid increase in accuracy initially, followed by a relatively smooth and stable ascent, reaching much higher accuracies, typically between 0.12 and 0.14.

The "FedAvg (E=5)" category is represented by dashed orange lines of varying shades, corresponding to learning rates: "η=3.0" (lightest dashed orange), "η=6.0" (medium light dashed orange), "η=9.0" (medium dark dashed orange), and "η=18.0" (darkest dashed orange). These curves also show a rapid initial increase, then stabilize at high accuracies, generally slightly below or similar to the "FedAvg (E=1)" curves, between 0.10 and 0.13, with slightly more variance compared to E=1 but still smoother than FedSGD.

Overall, the graph visually demonstrates that FedAvg (both E=1 and E=5) consistently outperforms FedSGD in terms of achieving higher test accuracy and exhibits greater stability with less fluctuation across communication rounds. Furthermore, FedAvg with E=1 tends to achieve slightly better or comparable performance with lower variance compared to FedAvg with E=5.](images/9db91ca301c70dabb5f06edb59c86a557b80567050e8a5c56e9d549c032a57df.jpg)
Figure 10: Learning curves for the large-scale language model word LSTM, with evaluation computed every 20 rounds. FedAvg actually performs better with fewer local epochs $E$ (1 vs 5), and also has lower variance in accuracy across evaluation rounds compared to FedSGD.