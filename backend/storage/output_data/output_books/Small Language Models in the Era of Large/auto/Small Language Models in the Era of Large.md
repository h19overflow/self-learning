# A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness

FALI WANG, ZHIWEI ZHANG, XIANREN ZHANG, and ZONGYU WU, The Pennsylvania State

University, USA   
TZUHAO MO, University of Pennsylvania, USA   
QIUHAO LU, WANJING WANG, and RUI LI, UTHealth Houston, USA   
JUNJIE XU, The Pennsylvania State University, USA   
XIANFENG TANG and QI HE, Amazon, USA   
YAO MA, Rensselaer Polytechnic Institute, USA   
MING HUANG, UTHealth Houston, USA   
SUHANG WANG∗, The Pennsylvania State University, USA

Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: https://github.com/FairyFali/SLMs-Survey.

# CCS Concepts: $\cdot$ Computing methodologies Natural language generation.

# ACM Reference Format:

Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, TzuHao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, and Suhang Wang. 2018. A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness. J. ACM 37, 4, Article 111 (August 2018), 78 pages. https://doi.org/XXXXXXX.XXXXXXX

# 1 INTRODUCTION

The evolution of neural language models (LMs) from BERT’s [83] pre-training and fine-tuning paradigm to T5’s [284] pre-training plus prompting approach, and finally to GPT-3’s [37] pre-training plus in-context learning, has greatly enhanced natural language processing (NLP). These advancements have broadened NLP’s application across various fields, including language understanding [350], programming [255, 331], recommendation systems [369], information retrieval [44, 152, 228, 317], mobile-device control [87], scientific discovery [311, 436], medical question answering [34, 366], and legal question answering [11]. In particular, the recent emergence of proprietary commercial models including ChatGPT, Bard, and Claude, and open-sourced models such as Llama [94, 338, 339] has led to rapid growth in the development of large language models (LLMs). Even though neural networks consistently improve on various tasks with longer training times, larger datasets, and increased model sizes—a phenomenon known as a neural scaling law [166], these models unpredictably exhibit a sudden acquisition of versatile abilities, termed "emergent ability," once they reach a critical scale threshold, thereby supporting the "larger is better" trend. This ability is not present in small-scale models. For instance, the latest Llama-3.1 model with 405 billion parameters performs better in dialogue, logical reasoning, and programming compared to the smaller 7B counterpart [94].

Despite their prowess in complex tasks, LLMs’ huge parameters and computational needs impose significant limitations, hindering their adoption in many real-world applications. For example, the LLaMa 3.1 model with 405 billion parameters [94], trained on 16K H100 GPUs for 54 days, requires about 202.5 GB of GPU memory using int4 precision and has large inference latency. These issues present several challenges in specific contexts: (1) LLMs are generally hosted in the cloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users need to upload their data to query LLMs, raising data leakage and privacy concerns, especially in high-stake scenarios such as healthcare, finance, and e-commerce; (2) Driven by personal agents, on-device deployment is a critical requirement. Several factors, including cloud costs, latency, and privacy concerns, hinder the on-device processing of cloud-based LLMs, and direct deployment is impractical due to their high parameter and cache requirements, which often exceed the capabilities of devices such as mobile phones; (3) Their large parameter count can cause inference delays from seconds to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes approximately 84 seconds to process 100 tokens on benchmarks including HellaSwag, TruthfulQA, MMLU, and Arc_C when run on a smartphone equipped with a Snapdragon 685 processor [336]; (4) To boost performance in specialized domains such as healthcare and law, where generic LLMs underperform, LLMs are often fine-tuned. However, this process is computationally expensive due to their large size. (5) Though general-purpose LLMs are powerful, many real-world applications require only specific abilities and domain knowledge, deploying general-purpose LLMs would be a waste of resources and such LLMs often cannot match the performance of models tailored for specific tasks [52, 125, 156, 275, 369].

Recently, small language models (SLMs) have shown great potential in alleviating these issues while achieving performance comparable to LLMs for domain-specific problems [1, 26, 116, 143, 223, 274, 333, 336, 402, 439]. Owing to fewer parameters, SLMs excel in efficiency, cost, flexibility, and customization. They provide significant computational Manuscript submitted to ACM

![## Image Analysis: 77c4c32c1adca3a145486fc4768303feb5c12b34b9a1d2be0efc16a27cd435fa.jpg

**Conceptual Understanding:**
This image conceptually represents a comprehensive overview and taxonomy of Small Language Models (SLMs). It maps out the key areas, challenges, and advancements within the SLM domain.

The main purpose of this diagram is to categorize and illustrate the multifaceted landscape of SLMs, from their foundational building blocks and enhancement strategies to their diverse applications, specific model types, their symbiotic relationship with Large Language Models (LLMs), and crucial aspects of their trustworthiness. It provides a structured framework for understanding the current state and various facets of SLM research and development.

Key ideas being communicated include:
*   **Foundation:** How SLMs are built and derived (e.g., from LLMs).
*   **Improvement:** Methods to make SLMs better (e.g., fine-tuning, quantization).
*   **Utility:** Where SLMs can be applied (e.g., QA, coding, mobile devices).
*   **Variety:** The different types of SLMs available (generic vs. specific domain).
*   **Collaboration:** How SLMs can assist or work with LLMs.
*   **Reliability:** The importance of trustworthiness in SLM development.

**Content Interpretation:**
The image depicts a detailed hierarchical classification of topics related to Small Language Models (SLMs), presenting various processes, concepts, relationships, and systems.

*   **Processes/Concepts Shown:**
    *   **Building LMs:** The section "Concepts in Building LMs (§2)" outlines core techniques like "Architecture (§2.1)", "Training techniques (§2.2)", and specifically how to "Obtain SLMs from LLMs (§2.3)" through methods like "Pruning (§2.3.1)", "Knowledge Distillation (§2.3.2)", and "Quantization (§2.3.3)". The presence of specific examples like "Unstructured Pruning" and "White-Box KD" supports the interpretation of these as fundamental building processes.
    *   **Enhancement:** "Enhancement of SLMs (§3)" focuses on improving SLM capabilities. This includes "Training Methods for SLMs from Scratch (§3.1)" (e.g., MobiLlama), "Supervised Fine-Tuning (§3.2)" (e.g., Alpaca, WizardLM), "Data quality in Knowledge Distillation (§3.3)" (e.g., TinyStory), "Distillation techniques for enhancing SLM (§3.4)" (e.g., GKD), "Performance improvement through quantization (§3.5)" (e.g., Bit-level Inference Scaling Laws), and "Techniques in LLMs contributing SLMs (§3.6)" (e.g., RAG for SLMs). These sub-sections clearly show various strategies for making SLMs more effective.
    *   **Applications:** "Applications of SLMs (§4)" highlights practical uses. "Task-specific SLM applications (§4.1)" lists domains like "QA (§4.1.1)" (e.g., Phi-series, Orca 2), "Coding (§4.1.2)" (e.g., DeepSeek-Coder, CodeGemma), "Recommender Systems (§4.1.3)" (e.g., PromptRec), "Web Search (§4.1.4)" (e.g., Content encoder), and "Mobile-device (§4.1.5)" (e.g., Octopus, MobileAgent). "Techniques during SLM Deployment (§4.2)" addresses efficiency concerns through "Memory efficiency (§4.2.1)" (e.g., MoE-I^2) and "Computing efficiency (§4.2.2)" (e.g., COST-EFF). This demonstrates the broad applicability and deployment considerations for SLMs.
    *   **Model Taxonomy:** "Models (§5)" categorizes SLMs themselves into "Generic-domain SLMs (§5.1)" (e.g., Llama 3.2, Gemma) and "Specific-domain SLMs (§5.2)" (e.g., Hippocrates for biomedicine). This indicates a classification based on their training data and intended use.
    *   **SLM-LLM Interaction:** "SLMs for LLMs (§6)" and "Synergy between SLMs and LLMs (§7)" describe the relationship between small and large models. SLMs can be used for "reliable LLM generation (§6.1)", "extracting LLM prompts (§6.2)", "fine-tuning LLMs (§6.3)", "LLM applications (§6.4)", "LLM safety (§6.5)", and "LLM evaluation (§6.6)". The synergy section further details "Cloud-edge synergy (§7.1)" and "Task-centric synergy (§7.2)". This illustrates a collaborative ecosystem where SLMs can augment or specialize LLMs.
    *   **Trustworthiness:** "Trustworthiness in SLMs (§8)" lists various benchmarks and frameworks like "HELM", "Do-Not-Answer", "TrustLLM", and "JailbreakBench". This section emphasizes the critical importance of evaluating and ensuring the reliability, fairness, and safety of SLMs.

*   **Significance of Information:**
    *   The extensive list of bracketed numbers (e.g., "[76, 102, 201]") following each technique or model name signifies the academic and research-driven nature of the document. These are references to specific research papers, underscoring that the diagram is a synthesis of existing literature. This provides strong textual evidence for the depth and breadth of research in each area.
    *   The use of section numbers like "§2.1" and "§4.1.1" within the labels indicates that this diagram likely corresponds to a detailed accompanying text in the document, allowing readers to cross-reference for more in-depth information.
    *   The specific names of models (e.g., "Alpaca", "Gemma", "CodeGemma") and techniques (e.g., "RAG for SLMs", "Prompt Stealing Attacks") provide concrete examples that illustrate the concepts, making the abstract ideas tangible.

**Key Insights:**
**Main Takeaways/Lessons:**

1.  **SLMs are a rapidly evolving field with diverse research directions:** The sheer number of categories (8 main sections) and hundreds of cited references across all sub-topics (e.g., "Unstructured Pruning [76, 102, ...]" or "Alpaca [329]; UltraChat [86]; ...") strongly indicates a vibrant and multifaceted research area.
2.  **Building SLMs involves various established and emerging techniques:** Concepts like "Pruning", "Knowledge Distillation", and "Quantization" are fundamental. The detailed breakdown with specific methodologies (e.g., "White-Box KD", "SqueezeLLM", "OneBit") highlights the technical depth required.
3.  **SLM enhancement focuses on efficiency and performance:** Sections like "Enhancement of SLMs (§3)" with sub-topics like "Data quality in Knowledge Distillation", "Performance improvement through quantization", and "Techniques during SLM Deployment (§4.2)" with "Memory efficiency" and "Computing efficiency" show a strong emphasis on making SLMs more effective and resource-friendly.
4.  **SLMs have a broad spectrum of practical applications:** The "Applications of SLMs (§4)" section, with categories ranging from "QA" and "Coding" to "Web Search" and "Mobile-device" applications, demonstrates their utility in various real-world scenarios, often with specific model examples (e.g., "Phi-series" for QA, "DeepSeek-Coder" for Coding).
5.  **SLMs can be specialized or general-purpose:** The "Models (§5)" section explicitly distinguishes between "Generic-domain SLMs" (e.g., Llama, Gemma) and "Specific-domain SLMs" (e.g., Hippocrates, BioMedLM), illustrating the flexibility in their design and application.
6.  **SLMs play a complementary role to LLMs:** The sections "SLMs for LLMs (§6)" and "Synergy between SLMs and LLMs (§7)" highlight that SLMs are not just alternatives but can augment LLMs in tasks like "reliable LLM generation", "LLM safety", "LLM evaluation", and through "Cloud-edge synergy". This indicates a collaborative rather than purely competitive relationship.
7.  **Trustworthiness is a critical, ongoing concern for SLMs:** The dedicated "Trustworthiness in SLMs (§8)" section, listing numerous benchmarks (e.g., "HELM", "TrustLLM", "JailbreakBench"), underscores the importance of evaluating and ensuring the ethical and reliable behavior of these models.

**Document Context:**
Given the document context "Section: 1 INTRODUCTION" and the caption "Fig. 1. Overview of Small Language Models," this image serves as a foundational visual aid to introduce the scope and structure of the document's discussion on Small Language Models. It likely precedes a more detailed textual explanation of each section and sub-section, providing a high-level roadmap for the reader. The diagram visually organizes the complex topic into manageable, interconnected categories, helping the reader grasp the breadth of the subject matter before delving into specifics. It acts as an advanced table of contents or a conceptual map for the paper.

**Summary:**
This diagram provides a comprehensive, hierarchical overview of the domain of Small Language Models (SLMs). It is structured into eight main sections, each exploring a different facet of SLMs, and supported by numerous academic references to specific research works.

The journey begins with **Introduction (§1)**, setting the stage for the topic.

Following this, **Concepts in Building LMs (§2)** delves into the fundamental aspects of creating these models. This section first covers general "Architecture (§2.1)" and "Training techniques (§2.2)". A significant portion is dedicated to "Obtain SLMs from LLMs (§2.3)", detailing three primary methods:
*   **Pruning (§2.3.1)**, which includes both "Unstructured Pruning" (e.g., [76, 102]) and "Structured Pruning" (e.g., [13, 19]) to reduce model size.
*   **Knowledge Distillation (§2.3.2)**, categorized into "White-Box KD" (e.g., [6, 119]) and "Black-Box KD" (e.g., [271, 360]), where knowledge from larger models is transferred to smaller ones.
*   **Quantization (§2.3.3)**, which reduces the precision of model weights, with examples like "SqueezeLLM [170]", "JSQ [122]", and "OneBit [400]".

The next section, **Enhancement of SLMs (§3)**, focuses on improving the performance and capabilities of SLMs. This involves:
*   "Training Methods for SLMs from Scratch (§3.1)" (e.g., MobiLlama [336]).
*   "Supervised Fine-Tuning (§3.2)" using techniques like "Alpaca [329]", "WizardLM [392]", and "DPO [283]".
*   Addressing "Data quality in Knowledge Distillation (§3.3)" (e.g., TinyStory [96]).
*   Specific "Distillation techniques for enhancing SLM (§3.4)" like "GKD [6]" and "Adapt-and-Distill [414]".
*   "Performance improvement through quantization (§3.5)" (e.g., Bit-level Inference Scaling Laws [82]).
*   And leveraging "Techniques in LLMs contributing SLMs (§3.6)" such as "RAG for SLMs [215, 415]" and "MoE for SLMs [172, 194]".

**Applications of SLMs (§4)** outlines the practical uses and deployment considerations. It's divided into:
*   **Task-specific SLM applications (§4.1)** across various domains:
    *   "QA (§4.1.1)" (Question Answering) with models like "Phi-series [1, 120]" and "Orca 2 [247]".
    *   "Coding (§4.1.2)" using models like "DeepSeek-Coder [121]" and "CodeGemma [331]".
    *   "Recommender Systems (§4.1.3)" with examples such as "PromptRec [382]" and "SLIM [369]".
    *   "Web Search (§4.1.4)" involving "Content encoder [44, 152]" and "H-ERNIE [63]".
    *   "Mobile-device (§4.1.5)" applications with models like "Octopus [52]" and "MobileAgent [87]".
*   **Techniques during SLM Deployment (§4.2)**, focusing on efficiency:
    *   "Memory efficiency (§4.2.1)" (e.g., MoE-I^2 [404], MobileLLM [223]).
    *   "Computing efficiency (§4.2.2)" (e.g., COST-EFF [305], MobiLlama [336]).

**Models (§5)** categorizes the SLMs themselves:
*   "Generic-domain SLMs (§5.1)" are broad-purpose models like "PhoneLM [417]", "Llama 3.2", "Gemma [332]", and "Phi [1, 1, 120]".
*   "Specific-domain SLMs (§5.2)" are tailored for particular areas, such as "Hippocrates [3]" (medical), "BioMedLM [34]", and "ChemLLM [436]".

**SLMs for LLMs (§6)** explores how SLMs can assist Large Language Models:
*   "SLM for reliable LLM generation (§6.1)" (e.g., POLAR [450], Self-RAG [18]).
*   "SLM for extracting LLM prompts (§6.2)" (e.g., Prompt Stealing Attacks [297]).
*   "SLM for fine-tuning LLMs (§6.3)" (e.g., Weak-to-Strong Search [453]).
*   "SLM for LLM applications (§6.4)" (e.g., SLoCoLM [327]).
*   "SLM for LLM safety (§6.5)" (e.g., Llama Guard [153]).
*   "SLM for LLM evaluation (§6.6)" (e.g., SLIDE [449], Selfcheckgpt [240]).

**Synergy between SLMs and LLMs (§7)** details collaborative frameworks:
*   "Cloud-edge synergy (§7.1)" (e.g., CoGenesis [437], LLM-to-SLM [28]).
*   "Task-centric synergy (§7.2)" (e.g., α-UMi [307], SynCID [204]).

Finally, **Trustworthiness in SLMs (§8)** lists crucial benchmarks and frameworks for assessing the reliability and ethical aspects of SLMs, including "HELM [205]", "Do-Not-Answer [364]", "TrustLLM [320]", and "JailbreakBench [45]".

This comprehensive diagram, supported by a wealth of research references, effectively maps the current landscape of Small Language Models, from their fundamental construction and enhancement to their varied applications, model types, interactions with LLMs, and essential trustworthiness considerations.](images/77c4c32c1adca3a145486fc4768303feb5c12b34b9a1d2be0efc16a27cd435fa.jpg)
Fig. 1. Overview of Small Language Models.

![## Image Analysis: df663fc0f0f03e94a63f985ffce187344c3d15b4e8e08b798ac33582aed8fd97.jpg

**Conceptual Understanding:**
This image conceptually represents an empirical study of Large Language Model (LLM) adoption trends. Specifically, it illustrates the download popularity of various LLM models from different families, categorized by their parameter size, over a specified period (last month, as per the document context). The main purpose of these charts is to demonstrate the varying levels of demand and preference for different LLM architectures and their scaled versions within the developer and research community. It conveys the key idea that while larger models might theoretically offer more capabilities, there is often a 'sweet spot' in terms of model size that strikes a balance between performance, accessibility, and utility, leading to higher download rates. Furthermore, it highlights significant differences in the overall popularity and community engagement across different LLM projects, with some models attracting orders of magnitude more downloads than others. The charts serve as a data-driven snapshot of the LLM landscape on Huggingface as of October 2024.

**Content Interpretation:**
The image presents download statistics for four distinct families of Large Language Models (LLMs): GALACTICA, StableLM-2, Qwen, and Phi-3. For each family, the number of downloads is shown across various model sizes. This allows for an analysis of how model size influences the popularity or adoption of specific LLMs within each family. The X-axis consistently represents 'Model Size' (e.g., '125M', '1.3B', '72B'), while the Y-axis uniformly represents 'Downloads'. The charts highlight that for each model family, there tends to be an optimal or most popular model size that garners the highest number of downloads, with popularity generally decreasing for significantly smaller or larger models, though this pattern has variations. For instance, 'GALACTICA Models' show '6355' downloads for the '1.3B' model, which is the peak, while 'StableLM-2 Models' see '5270' downloads for the '3B' model as its highest. The 'Qwen Models' and 'Phi-3 Models' demonstrate a much higher scale of downloads, in the millions, with the 'Qwen 1.5B' model reaching '3M' downloads and the 'Phi-3 3.8B' model achieving '2M' downloads. The data suggests that for these newer and potentially more widely adopted LLMs (Qwen, Phi-3), the download counts are orders of magnitude higher than for GALACTICA and StableLM-2 models, indicating different levels of overall community engagement or utility.

**Key Insights:**
The main takeaways from this image are: 1.  **Varied Popularity Across LLM Families:** Different LLM families exhibit significantly different download volumes. GALACTICA and StableLM-2 models generally have downloads in the thousands, whereas Qwen and Phi-3 models attract millions of downloads. This is evident from 'GALACTICA Models' peaking at '6355' downloads and 'StableLM-2 Models' at '5270', contrasting sharply with 'Qwen Models' reaching '3M' downloads and 'Phi-3 Models' reaching '2M'. 2.  **Optimal Model Size for Downloads:** For each LLM family, there appears to be a specific model size that garners the most downloads. This is not always the smallest or largest model. For 'GALACTICA Models', '1.3B' is most popular with '6355' downloads. For 'StableLM-2 Models', '3B' is most popular with '5270' downloads. For 'Qwen Models', '1.5B' leads with '3M' downloads. For 'Phi-3 Models', '3.8B' is dominant with '2M' downloads. 3.  **Decreasing Popularity for Extreme Sizes:** Generally, very small or very large models within a family tend to have fewer downloads compared to moderately sized, popular versions. For example, in 'Qwen Models', '0.5B' has '157K' downloads and '72B' has '127K', significantly less than '1.5B' with '3M'. Similarly, in 'Phi-3 Models', '7B' ('44K') and '14B' ('75K') are less popular than '3.8B' ('2M'). 4.  **Significant Market Presence of Newer Models:** Qwen and Phi-3, potentially newer or more impactful models, show massive download numbers compared to GALACTICA and StableLM-2, indicating their broader adoption and impact in the LLM ecosystem, as evidenced by '3M' and '2M' download figures. These insights are directly supported by the verbatim download counts and model sizes extracted from each chart.

**Document Context:**
This image, described as 'Fig. 2. Download Statistics Last Month in Huggingface for LLMs with Various Model Sizes, obtained on October 7, 2024.', is highly relevant to the '1 INTRODUCTION' section of a document. It serves to empirically illustrate the landscape of LLM adoption and popularity based on model size. By presenting real-world download data from Huggingface, a prominent platform for machine learning models, the figure provides concrete evidence supporting discussions around the practical usage and demand for different LLMs. It likely introduces or supports the rationale for studying or developing LLMs, emphasizing that not all model sizes or families achieve the same level of traction. The specific date of data acquisition ('October 7, 2024') provides a temporal context, indicating the recency and relevance of the statistics for current LLM trends. This data can inform research directions, model development strategies, and market analysis within the LLM domain.

**Summary:**
The image displays four bar charts arranged horizontally, each detailing the download statistics for a specific family of Large Language Models (LLMs) on Huggingface, categorized by their model size. From left to right, the charts present data for 'GALACTICA Models', 'StableLM-2 Models', 'Qwen Models', and 'Phi-3 Models'. Each chart shares common Y-axis and X-axis labels. The Y-axis, labeled 'Downloads' on the leftmost chart, represents the number of downloads, though its scale varies across the different model families (ranging from thousands for GALACTICA and StableLM-2, to millions for Qwen and Phi-3). The X-axis for all charts is labeled 'Model Size', indicating the size of the LLM in billions (B), millions (M), or thousands (K) of parameters. Each bar within a chart represents a specific model size, with the exact download count annotated above it. The first chart, 'GALACTICA Models', shows model sizes '125M', '1.3B', '6.7B', '30B', and '120B', with downloads of '2306', '6355', '1119', '1245', and '771' respectively. The second chart, 'StableLM-2 Models', features '1.6B', '3B', and '12B' model sizes, corresponding to '1860', '5270', and '341' downloads. The third chart, 'Qwen Models', has model sizes '0.5B', '1.5B', '3B', '7B', '14B', '32B', and '72B', with download counts of '157K' (157,000), '3M' (3,000,000), '187K' (187,000), '186K' (186,000), '90K' (90,000), '66K' (66,000), and '127K' (127,000). Finally, the 'Phi-3 Models' chart shows sizes '3.8B', '7B', and '14B', with downloads of '2M' (2,000,000), '44K' (44,000), and '75K' (75,000). The specific units (K, M) are clearly indicated for the Qwen and Phi-3 model download figures, distinguishing them from the raw counts in the GALACTICA and StableLM-2 charts. This comprehensive view allows for a detailed comparison of download popularity across different LLM families and their respective sizes.](images/df663fc0f0f03e94a63f985ffce187344c3d15b4e8e08b798ac33582aed8fd97.jpg)
Fig. 2. Download Statistics Last Month in Huggingface for LLMs with Various Model Sizes, obtained on October 7, 2024.

savings in pre-training and inference with reduced memory and storage needs, which is vital for applications requiring efficient resource use. These small models are especially effective in resource-limited settings, performing well on low-power devices such as edge devices. Besides, SLMs improve on-device processing by enhancing privacy, security, response times, and personalization. This supports advanced personal assistants and cloud-independent applications, boosting energy efficiency and reducing carbon emissions. For example, the Llama 3.2 models (1B & 3B) demonstrate that local processing enables immediate execution of prompts and responses [7]. This approach protects privacy by keeping sensitive data such as patient health information (PHI), business data, personal messages, and calendar details local, enhancing confidentiality. It also allows for precise control over which queries are processed on-device versus those requiring cloud-based models. Therefore, small language models are gaining increasing attention as alternatives to LLMs, as indicated in Figure 2, which shows that SLMs are downloaded more frequently than larger models in the Hugging Face community, and Figure 3, which illustrates the growing popularity of SLM releases over time.

Typically, LMs that exhibit emergent abilities are classified as LLMs. However, the categorization of SLMs remains unclear. Studies vary in their contexts: some define SLMs as models with fewer than one billion parameters [223], while others consider the term “small language model” relative to the larger counterparts [183, 327, 369], with no consensus on a unified definition in the current landscape of LLMs. Research suggests SLMs for mobile devices, typically possessing around 6GB of memory, consist of sub-billion parameter models [223], whereas others classify models with up to 10 billion parameters as small, noting their lack of emergent abilities [105]. Given their use in resource-constrained environments and for specific tasks, we propose a generalized definition: Given specific tasks and resource constraints, we define SLMs as falling within a range where the lower bound is the minimum size at which the model exhibits emergent abilities for a specialized task, and the upper bound is the largest size manageable within limited resource conditions. This definition integrates various perspectives and addresses factors related to mobile computing and capability thresholds.

Due to the growing demand for SLMs, extensive literature has emerged on various aspects of SLMs. For example, several resource-efficient techniques [398] and training methods optimized for SLMs, such as quantization-aware training [221, 357, 400] and selective architectural component choices [223, 280, 336], aim to enhance performance in specific applications [36, 52, 275, 292, 382]. These methods have led to the development of numerous open-source, generalpurpose, and domain-specific SLMs [3, 26, 34, 333, 402, 435]. Beyond their inherent capabilities, SLMs can enhance LLMs by serving as modules or effective proxies [246, 297, 383, 399, 413, 450]. Furthermore, the complementary advantages of SLMs and LLMs can be leveraged collectively to better complete tasks [79, 204, 234, 307, 437]. Despite the commendable performance of SLMs, it is crucial not to overlook their credibility issues, such as the risks of adversarial attacks, producing hallucinations, and privacy breaches [88, 95, 138, 176, 176, 222, 248, 254, 273, 351, 354, 370, 371, 425, 445]. However, currently, there is no comprehensive survey thoroughly exploring these works on SLMs in the era of LLMs. Therefore, this paper offers the first comprehensive survey that analyzes various aspects of SLMs in the LLM era and their future directions. The overview structure of our paper is shown in Figure 1. To summarize, our major contributions are:

![## Image Analysis: fe7a5deacc121dd09cb056dc09b2fd7d8e528d9525f679eca295d180628c994a.jpg

**Conceptual Understanding:**
This image represents a chronological timeline of Small Language Models (SLMs). Conceptually, it illustrates the rapid evolution, growth, and diversification of the SLM landscape over several years. The main purpose of the image is to visually present when different SLMs were released, which entities developed them, and to highlight the emergence of specialized, domain-specific models. It communicates the idea that the field of small language models is dynamic, with continuous innovation and increasing participation from various research groups and companies.

**Content Interpretation:**
The image shows the evolution and proliferation of Small Language Models (SLMs) over time, organized chronologically by year and within 2023 and 2024, by monthly intervals. It illustrates the landscape of SLMs, identifying specific models and their associated developers/organizations through distinct icons. The legend indicates that some models are 'Domain-specific SLMs' (represented by a document icon) or 'Developed by Diverse Groups' (also represented by a document icon, indicating that these two categories share the same icon. This suggests that models marked with a document icon are either domain-specific or developed by diverse groups. The timeline highlights a significant acceleration in the development and release of SLMs, especially from 2023, showcasing a diverse ecosystem of contributors and specialized applications.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Accelerated Development:** There has been a significant and continuous acceleration in the development and release of Small Language Models, with a marked increase in the number of models from 2023 onwards.
2.  **Diverse Developers:** A wide array of organizations and entities, including major tech companies (Google, Meta, Microsoft, NVIDIA, Apple, Alibaba Cloud), research institutions (EleutherAI, Tsinghua, Peking University), and AI-focused companies (Stability AI, Hugging Face, Cerebras, Databricks, AI2, Kuaishou, H2O.ai, Cartesia, BioMistral), are actively contributing to the SLM space.
3.  **Emergence of Domain-Specific Models:** The presence of the 'Domain-specific SLMs' legend and models like AdaLM, MentalLLaMA, AstroLLaMA, MindLLM, Me-LLaMA, BioMistral, Hippocrates, BioMedLM, PhoneLM, CT-LLM, ChemLLM, and SciGLM (all marked with a document icon, indicating they are either domain-specific or developed by diverse groups) signifies a trend towards specialized models tailored for particular fields or applications, moving beyond general-purpose models.
4.  **Continuous Innovation:** The timeline shows continuous iterations and new versions of models (e.g., Phi-1 to Phi-3.5, Qwen 1 to Qwen 2.5, StableLM to StableLM 2, Gemma to Gemma 2, Llama 3.2), indicating ongoing research and improvements in SLM capabilities.
5.  **Key Players:** Microsoft (Phi series, Orca series), Google (T5, Flan-T5, Gemma series, MobileBERT), Meta (XGLM, OPT, Galactica, Llama series), and Alibaba Cloud (Qwen series) are prominent and consistent developers of SLMs.

**Textual Evidence for Insights:**
*   The dense listing of models across 2023 and 2024, compared to <2022 and 2022, provides clear evidence of accelerated development.
*   The variety of company logos and icons associated with model names (e.g., Google 'G', Meta 'infinity loop', Microsoft 'grid', NVIDIA, EleutherAI, Stability AI 'S.', Hugging Face 'flower'/'fox', Alibaba Cloud 'spider web', Apple 'apple logo', Tsinghua 'brick wall', H2O.ai, Cartesia) directly supports the insight about diverse developers.
*   The 'Domain-specific SLMs' legend and the document icons next to models like AdaLM, MentalLLaMA, AstroLLaMA, MindLLM, Me-LLaMA, BioMistral, Hippocrates, BioMedLM, PhoneLM, CT-LLM, ChemLLM, SciGLM provide direct evidence for the emergence of domain-specific models.
*   The sequential numbering in model names (e.g., 'Phi-1', 'Phi-1.5', 'Phi-2', 'Phi-3', 'Phi-3.5'; 'Qwen 1', 'Qwen 1.5', 'Qwen 2', 'Qwen 2.5'; 'StableLM', 'StableLM 2'; 'Gemma', 'Gemma 2') demonstrates continuous innovation and iteration.

**Document Context:**
This image, titled 'Fig. 3. A timeline of existing small language models.', directly supports the '1 INTRODUCTION' section of the document by providing a historical and current overview of the small language model landscape. It establishes the context for understanding the state-of-the-art and the rapid development in this field, setting the stage for discussions about SLMs, their applications, and future research directions. By showing the numerous models and their origins, it emphasizes the importance and dynamism of SLM research.

**Summary:**
This image is a timeline illustrating the release of various small language models (SLMs) from before 2022 to potentially beyond 2025. It organizes models chronologically, highlights their developers or associated entities through icons, and categorizes some as 'Domain-specific SLMs' or 'Developed by Diverse Groups'. The timeline shows a rapid increase in SLM development, particularly from 2023 onwards. The initial years feature foundational models, while later years introduce more specialized and continuously evolving models from various organizations. The visual layout progresses from left to right for years and from top to bottom for monthly intervals within a year, making it easy to track the evolution and expansion of the SLM landscape.](images/fe7a5deacc121dd09cb056dc09b2fd7d8e528d9525f679eca295d180628c994a.jpg)
Fig. 3. A timeline of existing small language models.

• In Section 3, we examine various techniques for improving the performance of SLMs, including training from scratch, fine-tuning, knowledge distillation, quantization, and leveraging LLM-enhancing technologies to optimize SLMs.   
• In Section 4, we discuss the tasks that SLMs can enhance and the deployment strategies that enable models to fit within the resource constraints of edge devices while maintaining acceptable inference speed.   
• In Section 5, we collect SLMs with fewer than 7 billion parameters across both general-purpose and domain-specific applications, reviewing common architectural choices, training techniques, and datasets, and providing a comparative summary of performance across different model sizes. Recent SLMs are listed.   
• In Section 6, we explore how SLMs can address key challenges faced by LLMs, such as high inference latency, labor-intensive fine-tuning, susceptibility to knowledge noise, and risks of copyright infringement.   
• In Section 7, we survey two kinds of synergies between LLMs and SLMs: one involves cloud-based LLMs and local SLMs, while the other leverages the unique advantages of both to more effectively solve tasks.   
• In Section 8, we investigate the trustworthiness issues of SLMs, including hallucination and privacy concerns, by providing a taxonomic summary of current evaluation methods.

Concurrently with our survey, Lu et al. [229] evaluate open-source SLMs, focusing on their architectures, datasets, algorithms, and on-device performance metrics such as inference latency and memory usage. Van Nguyen et al. [345] delve into optimization strategies for SLMs, including model compression, pruning, and quantization. Chen and Varoquaux [49] investigate how SLMs enhance LLMs and vice versa. In contrast, our survey offers a more comprehensive review with the following differences: (1) we present a detailed taxonomy of recent advancements in SLMs in the era of LLMs; (2) we define SLMs based on emergent capabilities and device specifications, which refines previous unclear definitions related to LLMs; (3) we discuss SLM applications, especially in on-device tasks and deployment, topics previously unexplored; (4) we examine domain-specific SLMs previously overlooked; and (5) we additionally consider the synergy between SLMs and LLMs.

# 2 FOUNDATIONAL CONCEPTS IN BUILDING LANGUAGE MODELS

This section will introduce foundational concepts and background knowledge for language models, including the concepts of architecture and the training process, as well as methods for obtaining SLMs from LLMs. The advanced training strategy to improve SLM performance will be introduced in Section 3.

# 2.1 Architecture of SLMs

SLMs commonly employ the Transformer architecture [346] (see Figure 4), which utilizes self-attention mechanisms to manage long-range text dependencies, essential for maintaining performance with constrained resources. However, due to the attention mechanism, Transformers have large inference cost. Hence, to alleviate the issue, several subquadratictime architectures such as Mamba [117], Hymba [90], and xLSTM [25] are proposed. Next, we will give details of Transformer due to its popularity and briefly introduce newly emerged models.

2.1.1 Transformer. The Transformer’s self-attention mechanism [346] allows language models to efficiently capture contextual information across longer sequences, even with limited resources. The Transformer generally adopts an encoder-decoder structure featuring self-attention mechanisms, feedforward networks, positional embeddings, and layer normalization. The Transformer architecture design tailored for SLMs is detailed in Section 5; this section will provide only foundational concepts.

Self-Attention Mechanism enables the model to evaluate the importance of tokens relative to each other. The self-attention mechanism is written as

$$
{ \mathrm { A t t e n t i o n } } ( \mathbf { Q } , \mathbf { K } , \mathbf { V } ) = { \mathrm { s o f t m a x } } \left( { \frac { \mathbf { Q } \mathbf { K } ^ { \top } } { \sqrt { d _ { k } } } } \right) \mathbf { V }
$$

where ${ \bf { Q } } , { \bf { K } } ,$ and $\mathbf { V }$ are query, key, and value matrices, scaled by $\sqrt { d _ { k } }$ for stability where $d _ { k }$ is the dimension of key matrices. The dot product $\mathbf { Q } \mathbf { K } ^ { \top }$ reflects the similarity between the query and key vectors.

![## Image Analysis: 0ec07b9236260ae50875f629ad20462a0550afc9867c569b52597bc40aa089be.jpg

**Conceptual Understanding:**
This image conceptually represents the **Transformer neural network architecture**, a pivotal model in sequence transduction tasks, particularly in Natural Language Processing. The main purpose conveyed is to illustrate how input sequences are processed, encoded, and then used to generate output sequences, emphasizing the role of attention mechanisms over traditional recurrent or convolutional networks. The key ideas communicated are the **encoder-decoder structure**, the use of **self-attention** and **encoder-decoder attention**, the importance of **positional encoding** for maintaining sequence order, and the integration of **residual connections with layer normalization** (Add & Norm) for stable training of deep networks.

**Content Interpretation:**
The image clearly depicts the internal workings of a Transformer model, showcasing the interplay between its various components:

*   **Input and Output Embeddings:** "Input Embedding" and "Output Embedding" layers convert discrete input/output tokens into continuous vector representations, a fundamental step in neural network processing of symbolic data.
*   **Positional Encoding:** The explicit "Positional Encoding" added (⊕) to both embeddings highlights a critical design choice. Since attention mechanisms intrinsically lack knowledge of token order, positional encodings inject this sequential information, allowing the model to distinguish between words at different positions in a sequence.
*   **Encoder Block (Nx repetitions):**
    *   **Multi-Head Attention:** This is the core innovation. In the encoder, "Multi-Head Attention" allows the model to weigh the importance of all other input tokens when processing a specific token, capturing long-range dependencies. The "Nx" label indicates multiple identical layers, allowing for hierarchical feature extraction.
    *   **Add & Norm:** These blocks signify "residual connections" followed by "layer normalization". The residual connection (arrow bypassing "Multi-Head Attention" or "Feed Forward" and adding to its output before normalization) helps mitigate the vanishing gradient problem in deep networks, allowing information to flow more easily. Layer normalization stabilizes training by normalizing inputs across features for each example.
    *   **Feed Forward:** A simple, position-wise fully connected "Feed Forward" network applies a non-linear transformation independently to each position, adding more representational capacity to the model.
*   **Decoder Block (Nx repetitions):**
    *   **Masked Multi-Head Attention:** The first attention layer in the decoder is "Masked Multi-Head Attention". The "Masked" aspect is crucial for auto-regressive decoding, ensuring that predictions for the current output token can only attend to previous output tokens and not future ones, preventing information leakage.
    *   **Second Multi-Head Attention (Encoder-Decoder Attention):** This layer takes its query from the *masked attention output* of the decoder and its keys and values from the *final output of the encoder*. This is where the decoder "attends" to the encoded representation of the input sequence, aligning source and target information.
    *   **Add & Norm and Feed Forward:** Similar to the encoder, these components provide stability and further processing within each decoder layer.
*   **Final Output Layers:**
    *   **Linear:** The "Linear" layer projects the high-dimensional output of the decoder to the size of the vocabulary.
    *   **Softmax:** The "Softmax" function converts these raw scores into a probability distribution over all possible output tokens, indicating the likelihood of each token being the next in the sequence, ultimately leading to "Output Probabilities".

All the extracted text elements ("Input Embedding", "Positional Encoding", "Multi-Head Attention", "Add & Norm", "Feed Forward", "Masked Multi-Head Attention", "Linear", "Softmax", "Nx") are crucial to understanding these processes, serving as labels for the distinct computational steps and architectural patterns that define the Transformer.

**Key Insights:**
The image provides several key insights into the Transformer architecture:

*   **Attention is the Core Mechanism:** The prominence of "Multi-Head Attention" and "Masked Multi-Head Attention" in both the Encoder and Decoder highlights that attention is the primary mechanism for capturing dependencies within sequences and between encoder and decoder outputs. This is a fundamental shift from previous sequential models.
*   **Parallelization through Self-Attention:** Unlike recurrent networks, the "Multi-Head Attention" layers allow for parallel computation over input tokens, as dependencies are calculated directly between any two positions, making Transformers highly efficient for training.
*   **Importance of Positional Information:** The explicit addition of "Positional Encoding" demonstrates that while attention mechanisms are powerful, they are permutation-invariant and require an external source to embed sequence order. This text confirms its necessity for tasks like language modeling where order is critical.
*   **Deep and Stable Architectures:** The "Nx" stacking of layers and the consistent use of "Add & Norm" blocks point to a design aimed at building very deep neural networks that can still be trained effectively. The residual connections prevent gradient vanishing, and layer normalization stabilizes activations.
*   **Encoder-Decoder Paradigm for Sequence-to-Sequence:** The clear separation into an Encoder and Decoder, with the Encoder's output feeding into the Decoder's attention mechanism, exemplifies the standard sequence-to-sequence paradigm, widely used in machine translation, summarization, and other generative tasks. The "Inputs" leading to "Output Probabilities" confirms this end-to-end functionality.
*   **Auto-Regressive Decoding:** The "Masked Multi-Head Attention" in the decoder is specific evidence for its auto-regressive nature, meaning it generates output tokens one at a time, conditioned on previously generated tokens.

The exhaustive transcription of every label and connection directly supports these conclusions by outlining the precise computational graph and the specific components responsible for these functionalities.

**Document Context:**
This image, "Fig. 4. Transformer architecture [346]," is highly relevant to the "2.1 Architecture of SLMs" section. It serves as the foundational diagram for understanding the Transformer model, which is a key building block or inspiration for many modern Self-supervised Language Models (SLMs). By presenting this architecture, the document sets the stage for discussing how SLMs leverage or modify this powerful design for various language understanding and generation tasks. It illustrates the core components and data flow that enable these models to process and generate sequential data, particularly text.

**Summary:**
The image displays the full architecture of a Transformer neural network, a state-of-the-art model primarily used for sequence-to-sequence tasks like language translation. It consists of two main parts: an Encoder (on the left) and a Decoder (on the right), which work in tandem. Both the Encoder and Decoder are composed of Nx identical stacked layers, indicating that these blocks are repeated multiple times to form a deep network.

The process begins with Inputs entering the Encoder. First, the "Inputs" are converted into numerical representations by an "Input Embedding" layer. To retain information about the order of words in the sequence, "Positional Encoding" is added to these embeddings (indicated by the "⊕" symbol). This combined representation then flows into the stack of Encoder layers.

Each Encoder layer contains two main sub-layers: a "Multi-Head Attention" mechanism and a "Feed Forward" network. The "Multi-Head Attention" allows the model to weigh the importance of different parts of the input sequence for each word, capturing complex relationships. After this, a crucial step involves "Add & Norm", which means the output of the attention layer is added back to its input (a "residual connection") and then normalized. This helps in training very deep networks. The output then passes through a "Feed Forward" network, followed by another "Add & Norm" layer. The output of the final "Add & Norm" layer of the last Encoder block is passed to the Decoder.

Simultaneously, the Decoder processes Outputs, which are the previously generated target sequence tokens (shifted to the right). Similar to the Encoder, these "Outputs" first go through an "Output Embedding" layer, and "Positional Encoding" is added to provide sequential context.

Each Decoder layer is more complex, featuring three main sub-layers:
1.  A "Masked Multi-Head Attention" layer: This is similar to the Encoder's attention but "masked." The masking ensures that when generating a word, the model can only look at words already generated (or the start-of-sequence token), preventing it from "cheating" by seeing future words. This also includes an "Add & Norm" step.
2.  A second "Multi-Head Attention" layer: This layer is critical for connecting the Encoder and Decoder. It takes its "query" from the output of the *masked attention* in the Decoder and its "keys" and "values" from the *final output of the Encoder*. This allows the Decoder to focus on relevant parts of the input sequence while generating the output. This is also followed by an "Add & Norm" step.
3.  A "Feed Forward" network: Similar to the Encoder, this processes the output of the attention layers, followed by a final "Add & Norm" for that decoder layer.

After the Nx Decoder layers, the final output is fed into a "Linear" layer. This layer projects the processed sequence into a high-dimensional space corresponding to the vocabulary size. Finally, a "Softmax" function is applied to convert these values into "Output Probabilities," representing the likelihood of each possible word being the next token in the output sequence.

In summary, the Transformer takes input sequences, processes them through an encoder to create rich contextual representations, and then uses a decoder to generate output sequences token by token, leveraging multiple attention mechanisms and positional information to handle sequence order and dependencies efficiently.](images/0ec07b9236260ae50875f629ad20462a0550afc9867c569b52597bc40aa089be.jpg)
Fig. 4. Transformer architecture [346].

Multi-Head Attention (MHA) [346] is the first method that uses multiple heads to capture diverse information. MHA allows the model to attend to different parts of the input sequence using multiple attention heads as

$\mathrm { M u l t i H e a d ( Q , K , V ) } = \mathrm { C o n c a t } ( \mathrm { h e a d } _ { 1 } , \mathrm { h e a d } _ { 2 } , \ldots , \mathrm { h e a d } _ { h } ) { \mathbf { W } } ^ { O } , \mathrm { w i t h ~ h e a d } _ { i } = \mathrm { A t t e n t i o n } ( \mathbf { Q } { \mathbf { W } } _ { i } ^ { Q } , \mathbf { K } { \mathbf { W } } _ { i } ^ { K } , \mathbf { V } { \mathbf { W } } _ { i } ^ { V } )$ (1) Each head in the Multi-Head Attention mechanism operates independently, allowing the model to capture diverse aspects of the data. The outputs are combined using learned projection matrices $\mathbf { W } _ { i } ^ { Q } , \mathbf { W } _ { i } ^ { K }$ , and $\mathbf { W } _ { i } ^ { V }$ , concatenated, and passed through the output projection matrix $\mathbf { W } ^ { O }$ .

Building on this foundation, several modifications have been introduced to further optimize self-attention mechanisms for specific challenges such as memory efficiency and computational speed. To address the KV-cache bottleneck in MHA, Multi-Query Attention (MQA) [303] proposes that all attention heads share the same set of keys and values, which reduces the memory and computational overhead associated with storing and managing multiple key-value pairs. Grouped Query Attention (GQA) [8] serves as a middle ground between MHA and MQA. It introduces subgroups of query heads (fewer than the total number of attention heads), where each subgroup shares a single key and value head.

Manuscript submitted to ACM

Unlike MQA and GQA, which reduce the number of key and value heads, Multi-Head Latent Attention (MLA) [211] compresses the keys and values into a joint latent vector. This compression allows for efficient handling of key-value pairs while maintaining high performance, significantly reducing the KV-cache and improving inference efficiency. Flash Attention [73, 74] accelerates the self-attention mechanism by minimizing the memory overhead typical of standard attention calculations. This optimization allows SLMs to process longer sequences more efficiently, enhancing their functionality under strict hardware constraints.

Feedforward Network (FFN) comprises two linear transformations separated by a non-linearity, typically modeled as $\mathrm { F F N } ( \mathbf { x } ) \ = \ \sigma ( \mathbf { x } \mathbf { W } _ { 1 } + b _ { 1 } ) \mathbf { W } _ { 2 } + b _ { 2 }$ . where $\mathbf { W } _ { 1 }$ and $\mathbf { W } _ { 2 }$ are the weight matrices, and $b _ { 1 }$ and $b _ { 2 }$ are bias terms. $\sigma$ is the activation function, which introduces non-linearity, allowing models to learn complex patterns. Generally, ReLU is used as the activation function. In addition to ReLU, activation functions such as GeLU and SiLU are also used in SLMs to improve performance. We give the details here: (i) ReLU (Rectified Linear Unit) [5] is defined as $\sigma ( x ) = \operatorname* { m a x } ( 0 , x )$ , which is commonly used for its simplicity and effectiveness. (ii) GELU (Gaussian Error Linear Unit) [135] is defined as ${ \mathrm { G E L U } } ( x ) = x \cdot \Phi ( x ) = x \cdot { \textstyle { \frac { 1 } { 2 } } } \left[ 1 + \operatorname { e r f } \left( { \frac { x } { \sqrt { 2 } } } \right) \right] ,$ , where $\Phi ( x )$ is the standard Gaussian CDF and erf is the error function. It is smoother than ReLU and widely used in models such as BERT [83] and GPT [282] for better gradient flow control. Since calculating the Gaussian error function for each neuron is computationally expensive and time-consuming, there are approximations using tanh and sigmoid functions, corresponding to $\mathrm { G E L U } _ { \mathrm { t a n h } }$ and SiLU: (iii) GELU with tanh is defined as $\mathrm { G E L U } _ { \mathrm { t a n h } } ( x ) = 0 . 5 \cdot x \cdot \left( 1 + \operatorname { t a n h } \left( \sqrt { \frac { 2 } { \pi } } \cdot ( x + 0 . 0 4 4 7 1 5 \cdot x ^ { 3 } ) \right) \right)$ This approximation uses the Tanh function to simplify computations. (iv) SiLU (Sigmoid Linear Unit) [97] is calculated as $\mathrm { S i L U } ( x ) = x \cdot \mathrm { s i g m o i d } ( x ) = x \cdot { \frac { 1 } { 1 + e ^ { - x } } } .$ . It effectively combines the sigmoid function with its input, enhancing modeling capabilities. (v) SwiGLU (Swish-Gated Linear Units) [304] integrates the Swish activation function with Gated Linear Units, defined as ${ \mathrm { S w i G L U } } ( x ) = { \mathrm { S w i s h } } ( x \cdot W + b ) \odot ( x \cdot V + c )$ where $W , V$ are the weight matrix and $b , c$ are the bias terms. The Swish function is expressed as $\operatorname { S w i s h } ( x ) = x \cdot \operatorname { s i g m o i d } ( x )$ . This combination enhances expressiveness and computational efficiency, making it a preferred choice in advanced models such as the Qwen series [402].

Positional Embeddings in Transformer models [346] are essential for capturing token order, providing context about relative positions within a sequence. Traditional positional embeddings in the Transformer architecture utilize a sinusoidal function, defined as: $\begin{array} { r } { P E ( p o s , 2 i ) = \sin \left( \frac { p o s } { 1 0 0 0 0 ^ { 2 i / d _ { \mathrm { m o d e l } } } } \right) } \end{array}$ $\begin{array} { r } { P E ( p o s , 2 i + 1 ) = \cos \left( \frac { p o s } { 1 0 0 0 0 ^ { 2 i / d _ { \mathrm { m o d e l } } } } \right) } \end{array}$ where ?????? represents the position within the sequence, $i$ is the dimension index, and $d _ { \mathrm { m o d e l } }$ is the dimensionality of the model. To improve the model’s capacity for understanding the relative positions of tokens within a sequence, Rotary Positional Embedding (RoPE) [318] introduces a rotational matrix to the embeddings. RoPE significantly enhances the positional encoding by maintaining the relative distances through rotational transformations, thus optimizing the model’s interpretative ability regarding sequence dynamics.

Layer Normalization [185] stabilizes the training process by normalizing layer outputs, accelerating convergence. Two types of layer normalization are commonly used [185]: (i) Non-Parametric Layer Norm normalizes inputs using the mean and variance calculated across the layer’s dimensions without learnable parameters as $\textstyle \operatorname { L N } ( x ) = { \frac { x - \mu } { \sigma } }$ where $\mu$ is the mean and $\sigma$ is the standard deviation of the inputs. Its simplicity makes it ideal for SLMs. (ii) Parametric Layer Norm includes learnable parameters $\gamma$ and $\beta$ for adaptive scaling and bias, enhancing model flexibility: $\begin{array} { r } { \operatorname { P L N } ( x ) = \gamma \left( \frac { x - \mu } { \sigma } \right) + \beta } \end{array}$ Additionally, RMS Norm (Root Mean Square Layer Normalization) [431] simplifies the calculation by using the root mean square of inputs, reducing computational demands: $\begin{array} { r } { \mathrm { R M S N o r m } ( x ) = \gamma \frac { x } { \sqrt { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } + \epsilon } } + \beta } \end{array}$ where $N$ is the number of inputs, $x _ { i }$ is the $i$ -th input, and $\epsilon$ is a small constant to prevent division by zero.

2.1.2 Mamba. The attention mechanism in Transformer suffers from a drawback: it requires recalculating attention scores with every previous token for each new token generated during inference, leading to quadratic time complexity. This increases the inference cost as sequence lengths grow. In contrast, Mamba [75, 117], based on state space models (SSMs) [164] which are a superclass of recurrent neural networks (RNNs), rely only on the last hidden state for generating the next token, enabling faster inference speeds, as shown in Figure 5. To address the Linear Time Invariant nature of traditional SSMs, which hinders their ability to focus on or ignore specific inputs, Mamba improves SSMs with a dynamic selection mechanism. This mechanism selectively filters out irrelevant information while retaining essential data, tailored to the content of the input. Leveraging this selective SSM foundation, Mamba adeptly captures complex global relationships within sequence data. Due to its focus on the immediate previous hidden state, as opposed to Transformer which requires access to all previous hidden states, Mamba achieves a higher utilization rate of model parameters. This makes it more suitable for SLMs. However, we identify two drawbacks of Mamba: (i) its focus on selectively capturing global information may compromise performance on tasks that require nuanced understanding, such as detailed sentiment analysis or complex entity recognition; and (ii) to balance inference speed, Mamba’s recurrent structure primarily encodes static global information, which limits its effectiveness in handling multi-round tasks within a single query, such as interactive dialogue systems or iterative problem-solving scenarios.

![## Image Analysis: c910d53cf633fd17f0283ebbd319032712214d9a42da2f28a15fe0577ca151de.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural design of a Mamba 1 block, which is a fundamental building block in the Mamba series of models, typically used for sequence modeling. Its main purpose is to visually explain the data flow and the key computational components within a single Mamba 1 layer, particularly emphasizing the 'Selective SSM' and its internal state representation. The diagram communicates how inputs are transformed through various linear, convolutional, and selective state-space operations, with a gating mechanism, to yield outputs, thereby elucidating a specific approach to structured sequence modeling.

**Content Interpretation:**
The image illustrates the computational graph of a Mamba 1 block, a core component within the Mamba architecture. It details the sequence of operations applied to an input to produce an output, highlighting the integration of linear transformations, convolutional layers, non-linear activation functions (σ), and a specialized 'Selective SSM' block. The diagram also provides an explicit conceptual model for the internal recurrence of a state-space model via the 'H_k' component. The architecture suggests a mechanism where the outputs of the 'Selective SSM' are dynamically modulated or gated by a parallel input processing path through a multiplication operation.

**Key Insights:**
The main takeaway from this image is the specific design of a Mamba 1 block, which integrates conventional neural network layers with a 'Selective SSM' for sequence processing. The presence of two parallel processing paths that converge via a multiplication operation suggests a gating or selective mechanism where one path (the right 'Linear' and 'σ' branch) potentially controls or modulates the output of the 'Selective SSM' component. The magnified view of 'H_k' clearly shows the recurrent nature of the underlying state-space model, with parameters 'A', 'B', and 'C' explicitly indicated, highlighting the core components of its state transition and output generation. This provides insight into how the 'Selective SSM' processes sequential data by maintaining and updating an internal state, dynamically influenced by inputs.

**Document Context:**
This image directly supports 'Section: 2.1 Architecture of SLMs' by providing a concrete visual example of a 'Mamba 1 architecture'. It serves to illustrate the structural details of a specific Structured Language Model (SLM) discussed in the document, showing how different computational layers and a 'Selective SSM' are combined. The figure provides the foundational visual information for understanding the operational flow and key components of Mamba 1, which is crucial for comprehending the broader discussion on SLM architectures.

**Summary:**
The image presents the architecture of a Mamba 1 block, a type of Selective State Space Model (SSM), detailing how an input sequence is processed to produce an output. The overall flow begins with 'Inputs' at the bottom and culminates in 'Outputs' at the top.

The input path splits into two main branches. The left branch processes the input sequentially through a 'Linear' layer, followed by a 'Conv' (convolutional) layer, then a 'σ' (sigma) activation function, and finally feeds into a 'Selective SSM' block. The right branch takes the same 'Inputs' through a separate 'Linear' layer and then a 'σ' (sigma) activation function. The outputs of the 'Selective SSM' block and the right branch's 'σ' activation are then combined using a 'x' (multiplication) operation. The result of this multiplication then passes through a final 'Linear' layer to generate the 'Outputs'.

A magnified view on the left side of the diagram provides a detailed representation of the internal mechanism of the 'H_k' state within the Selective SSM. It shows 'x_k' as an input entering the 'H_k' block, and 'y_k' as the output exiting the 'H_k' block. A feedback loop, labeled 'A', indicates recurrence within the 'H_k' state. Additionally, parameters 'B' and 'C' are associated with the input 'x_k' and output 'y_k' paths, respectively, representing the input and output transformations within the state-space model.](images/c910d53cf633fd17f0283ebbd319032712214d9a42da2f28a15fe0577ca151de.jpg)
Fig. 5. Mamba 1 architecture [117].

In language modeling, Mamba 1 [117] is pre-trained on the Pile dataset [108] using the training recipe from [37] and ranges from 125M to 1.3B parameters. It outperforms comparable models such as Pythia [31] and RWKV [270] in various tasks; for instance, Mamba-1.4B achieves a $3 2 . 8 \%$ accuracy on the Arc-Challenge [65] dataset, surpassing Pythia-1.4B’s $2 8 . 5 \%$ and RWKV-1.5B’s $2 9 . 4 \%$ . Mamba 2 [75] develops a theoretical framework linking SSMs with attention mechanisms through structured semi-separable matrices, enhancing the selective SSM to achieve $2 { - } 8 \mathrm { x }$ faster speeds while competing with Transformer models. Training and configuration for Mamba 2 align with Mamba 1. Additionally, Mamba-series models are applied widely across different fields [160, 280, 281, 463]. Other follow-up Mamba-based language models such as Falcon Mamba 7B [463] and Mistral 7B [160] also demonstrate the efficiency of the architecture for NLP tasks. Falcon Mamba 7B scales Mamba’s long-sequence processing capabilities to extensive language data, reducing memory overhead and making it ideal for long-form NLP tasks. Similarly, Mistral 7B incorporates Mamba’s efficient sequence handling ability to improve processing speed and computational efficiency in long-context NLP tasks, showcasing Mamba’s scalability and practicality for large-scale language modeling.

2.1.3 Hymba. Attention heads in the Transformer facilitate high-resolution recall, while SSM heads in Mamba efficiently summarize context. To balance performance and efficiency for SLMs, Hymba [90] integrates both attention and SSM heads within the same layer, allowing for parallel and complementary processing of inputs, as depicted in Fig

![## Image Analysis: 6cd678b847173b848c1429c04ccfa78a775c56e46fec00eed0142c8567e93bcd.jpg

**Conceptual Understanding:**
Conceptually, this image represents the architecture of a deep learning model, specifically a hybrid model that processes input tokens through parallel pathways. Its main purpose is to illustrate the internal structure and data flow of the "Hymba" model, showcasing how it integrates two distinct mechanisms: State Space Models (SSM) and Attention (Attn). The key ideas being communicated are the use of a shared latent feature that is then split for specialized, parallel processing by different types of neural network heads, followed by a fusion of their outputs to generate a final result.

**Content Interpretation:**
The image illustrates a hybrid neural network architecture, likely for sequence or language modeling, that combines State Space Models (SSM) and Attention (Attn) mechanisms. The processes shown include:

*   **Input Projection ("Input Proj"):** This initial layer transforms "Meta Tokens" and "Input Tokens" into a shared feature space.
*   **Latent Feature Representation ("Latent Feature"):** A central, unified representation derived from the input, serving as the basis for specialized processing.
*   **Feature Splitting ("Split"):** The "Latent Feature" is divided to feed two distinct processing branches, enabling specialized feature extraction.
*   **State Space Model (SSM) Feature Extraction ("SSM Feat.", "SSM Head"):** One branch processes features using multiple "SSM Head" modules, characteristic of State Space Models for efficient sequence modeling.
*   **Attention (Attn) Feature Extraction ("Attn Feat.", "Attn Head"):** The parallel branch processes features using multiple "Attn Head" modules, employing Attention mechanisms to capture dependencies.
*   **Gate Normalization ("Gate Norm."):** A normalization or gating step applied to the outputs of both the SSM and Attention heads, likely for controlling information flow or stabilizing training.
*   **Feature Merging ("Mean"):** The outputs from the parallel branches are combined using a "Mean" operation, indicating an averaging or fusion of the learned representations.
*   **Output Projection ("Output Proj"):** A final layer that maps the combined features to the desired output space.

The significance lies in this architecture's attempt to leverage the advantages of both SSMs (e.g., computational efficiency, long-sequence handling) and Attention (e.g., strong contextual understanding) within a single model. The parallel structure with "Split" and "Mean" operations highlights a deliberate design for integrated, multi-faceted feature processing. The stacked "SSM Head" and "Attn Head" blocks suggest a layered or multi-headed approach within each specialized branch, typical in deep learning architectures.

**Key Insights:**
The image provides several key insights into the Hymba architecture:

1.  **Hybrid Architecture:** The model clearly integrates two distinct neural network paradigms: State Space Models and Attention mechanisms. This is evident from the explicit labels "SSM Head" and "Attn Head" in parallel branches, suggesting an attempt to combine their respective strengths.

2.  **Parallel Processing and Feature Specialization:** A shared "Latent Feature" is explicitly "Split" into two specialized pathways, "SSM Feat." and "Attn Feat.", which are then processed independently. This indicates a design choice to capture different aspects or scales of information simultaneously through different mechanisms.

3.  **Gated/Normalized Outputs:** Both parallel branches include a "Gate Norm." module before merging. This suggests a crucial step for controlling, refining, or stabilizing the information flow from the specialized heads, potentially improving model performance or interpretability.

4.  **Feature Fusion through Averaging:** The final combination of the processed features from the parallel branches is performed by a "Mean" operation. This simple aggregation method implies an equal weighting or straightforward averaging of the representations learned by the SSM and Attention components.

5.  **Token-based Input/Output Flow:** The architecture is designed to handle "Meta Tokens" and "Input Tokens" as its initial input, and the final "Output Proj" indicates that it produces a processed output, likely also in a token-based format, consistent with sequence-to-sequence or language modeling tasks.

**Document Context:**
The image, identified as "Fig. 6. Hymba [90] architecture," is presented in Section 2.1 titled "Architecture of SLMs." This placement indicates its critical role in illustrating the specific structural design of the Hymba model, which is an example of a Selective/State-Space Language Model (SLM). The diagram visually explains how the Hymba model processes information, emphasizing its dual-path approach that integrates both State Space Model (SSM) and Attention mechanisms. This visual representation directly supports and elaborates on the textual discussion within the document by providing a clear, detailed overview of the model's internal components and data flow.

**Summary:**
The image, titled "Fig. 6. Hymba [90] architecture," presents a high-level block diagram of a neural network model designed for processing tokens. The architecture is characterized by a dual-path processing approach, combining State Space Model (SSM) and Attention (Attn) mechanisms.

The process begins on the left with two types of input: "Meta Tokens" (represented by green rectangles) and "Input Tokens" (represented by grey rectangles). These inputs are first fed into an "Input Proj" (Input Projection) module, which transforms them into a unified "Latent Feature" representation.

From the "Latent Feature," the processing path "Split" into two parallel branches.

The **upper branch** processes "SSM Feat." (State Space Model Features). This involves passing the features through multiple "SSM Head" modules (stacked blue rectangles), which are responsible for extracting information using State Space Model principles. The output of these "SSM Head" modules then goes through a "Gate Norm." (Gate Normalization) module.

The **lower branch** processes "Attn Feat." (Attention Features). Similarly, these features are passed through multiple "Attn Head" modules (stacked red rectangles), which employ Attention mechanisms to capture relevant information. The output from these "Attn Head" modules also proceeds to a "Gate Norm." (Gate Normalization) module.

Finally, the outputs from both "Gate Norm." modules (one from the SSM path and one from the Attention path) are combined using a "Mean" operation, indicating an averaging of their respective representations. The resulting merged feature is then passed to an "Output Proj" (Output Projection) module, which produces the final output of the Hymba architecture.

In essence, the Hymba architecture converts input tokens into a shared latent space, then processes this information simultaneously through dedicated State Space Model and Attention pathways, normalizes their outputs, averages them, and finally projects the combined representation to generate the model's output. This design suggests an effort to harness the complementary strengths of both SSMs and Attention for robust and efficient token processing.](images/6cd678b847173b848c1429c04ccfa78a775c56e46fec00eed0142c8567e93bcd.jpg)
Fig. 6. Hymba [90] architecture.

ure 6. This hybrid-head approach enables each layer to simultaneously leverage the high-resolution recall of attention heads and the contextual summarization of SSMs, increasing the model’s flexibility and expressiveness in managing diverse information flows and memory access patterns.

Hymba has been developed in models of varying sizes—125M, 350M, and 1.5B, trained on a combination of the DCLM-Baseline-1.0 [192], SmolLM-Corpus [27], and a proprietary high-quality dataset, with token counts of 1 trillion, 250 billion, and 50 billion, respectively. The models incorporate the Warmup-Stable-Decay (WSD) learning rate scheduler [143] and the data annealing technique [94] to ensure stable pretraining, conducted on 128 NVIDIA A100 GPUs. The 1.5B base model was post-trained using full finetuning (FFT), followed by direct preference optimization (DPO) [283] to develop the Hymba-1.5B-Instruct model. In commonsense reasoning tasks, the Hymba 1.5B model surpasses Llama3.2-3B [7] by achieving $1 . 3 2 \%$ higher average accuracy, requiring an $1 1 . 6 7 \times$ smaller cache size, and delivering a $3 . 4 9 \times$ increase in processing speed.

2.1.4 xLSTM. Long Short-Term Memory (LSTM) [137] shares a conceptual similarity with Mamba that achieves success in language modeling through the introduction of time-dependent weights. This similarity raises an intriguing question: how effective would LSTMs be at language modeling if scaled to billions of parameters, incorporating advanced techniques from modern LLMs while addressing known LSTM limitations? Inspired by this question, Beck et al. [25] propose xLSTM architecture, which performs favorably compared to state-of-the-art Transformers and SSMs in empirical evaluations. To address the limitations of LSTM, xLSTM designs exponential gates to enhance effectiveness with long sequences, expand memory cells from scalars to matrices to increase storage capacity, and remove memory mixing to enable parallel processing.

To test the language modeling capabilities of xLSTM scaled to billions of parameters, it is trained on a large dataset comprising 300 billion tokens from SlimPajama [314] across various model sizes (125M, 350M, 760M, 1.3B). The performance of pre-trained xLSTM is compared against RWKV-4 [270], Llama [153], and Mamba [117] across 571 text domains of the PALOMA benchmark [237] and various downstream tasks. Across all model sizes and the majority of tasks, xLSTM consistently outperforms the others, suggesting that larger xLSTM models could become formidable competitors to existing Large Language Models that utilize Transformer technology.

# 2.2 Training SLMs from Scratch

Training SLMs from scratch entails several critical steps: (i) Pre-training, focused on acquiring general features and knowledge from the corpus; (ii) Fine-tuning, targeted at boosting the model’s abilities and performance for specific tasks; (iii) Decoding strategies, which involve the methods used for iteratively selecting the next token during generation.

2.2.1 Pre-training. Typically, the pre-training paradigm for language models is divided into encoder-based and decoderbased approaches. Encoder-based models, such as BERT [83], utilize Masked Language Modeling (MLM) tasks where the goal is to predict masked tokens within a sentence. This is achieved by maximizing:

$$
\begin{array} { r } { P ( \mathrm { m a s k e d t o k e n } \mid \mathrm { c o n t e x t } ) = \mathrm { s o f t m a x } ( \mathbf { W } \cdot \mathbf { h } _ { \mathrm { m a s k } } + b ) , } \end{array}
$$

where masked token is the original token that has been masked, context represents the other unmasked tokens in the sentence, $\mathbf { W }$ and $^ { b }$ are trainable parameters of a linear output layer, $\mathbf { h } _ { \mathrm { m a s k } }$ is the output from the transformer encoder for the masked position, and softmax is the activation function that converts logits to probabilities over the vocabulary. This process enhances the model’s language encoding capabilities. Decoder-based models, such as GPT [282], employ

Next Token Prediction (NTP) tasks, aiming to model the distribution of the next token by maximizing:

$$
\begin{array} { r } { P ( \mathrm { n e x t ~ t o k e n } \mid \mathrm { c o n t e x t } ) = \mathrm { s o f t m a x } ( \mathbf { W } \cdot \mathbf { h } _ { \mathrm { l a s t } } + b ) , } \end{array}
$$

where next token is the token that the model aims to predict, context represents the sequence of tokens preceding the token to be predicted, and $\mathbf { h } _ { \mathrm { l a s t } }$ is the output from the transformer encoder for the last token in the context. Effective data preprocessing, crucial for optimizing the performance of SLMs trained from scratch, involves meticulous data cleaning and strategic tokenization.

Data Cleaning involves techniques such as filtering, deduplication, and noise reduction, which improve data quality and help the model generalize better. Filtering noisy or irrelevant data, addressing outliers, and handling imbalances in the dataset ensure that the training data is both representative and efficient. Deduplication, in particular, helps prevent overfitting by removing repeated instances, making the model more robust with efficient parameter usage.

Tokenization plays a vital role in handling diverse vocabularies without increasing model size. Advanced methods such as Byte-Pair Encoding (BPE) [106] and WordPiece [316] break text into subwords [83], allowing the model to manage rare and compound words efficiently. These strategies ensure that SLMs maintain a balance between vocabulary coverage and model compactness, crucial for improving generalization while minimizing computational demands.

2.2.2 Fine-Tuning. After the initial training, SLMs are fine-tuned on specific tasks using task-specific data and loss functions. Parameter-efficient fine-tuning methods [118, 140, 142, 199], such as Low-Rank Adaptation (LoRA), prefixtuning, and adapter modules, are particularly effective for SLMs. Low-Rank Adaptation (LoRA) [142] modifies Transformer weights by introducing trainable low-rank matrices A and B for efficient fine-tuning, avoiding significant alterations to pre-trained weights. The update is represented as: $\Delta \mathbf { W } = \mathbf { A } \mathbf { B } ^ { \top }$ The fine-tuned weight matrix used in Transformer operations then becomes: $\mathbf { W } _ { \mathrm { f t } } = \mathbf { W } + \alpha \Delta \mathbf { W }$ where $\alpha$ is a scaling factor adjusting the adaptation’s impact, allowing fine-tuning on a smaller set of parameters while retaining the model’s foundational capabilities. Prefix-Tuning [199] prepends learnable prefixes to the input sequence, guiding the model’s attention without altering core model parameters. It is especially useful for generative tasks. Adapter Modules [140] are small, trainable layers inserted into the pre-trained model. These layers are fine-tuned on task-specific data, allowing the base model to remain fixed while the adapters learn the necessary adjustments. The typical structure of an adapter module includes a down-projection, a non-linearity, and an up-projection: Adapter $\mathbf { \Pi } ( \mathbf { h } ) = \mathbf { h } + \mathbf { W } _ { \mathrm { u p } } \cdot \sigma ( \mathbf { W } _ { \mathrm { d o w n } } \cdot \mathbf { h } + \mathbf { b } _ { \mathrm { d o w n } } ) + \mathbf { b } _ { \mathrm { u p } }$ where h is the input hidden state, $\mathbf { W } _ { \mathrm { d o w n } }$ and $\mathbf { W } _ { \mathrm { u p } }$ are the projection matrices, $\mathbf { b } _ { \mathrm { d o w n } }$ and ${ \bf b } _ { \mathrm { u p } }$ are the bias terms, and $\sigma$ is a non-linear activation function.

2.2.3 Decoding Strategies. After pre-training or fine-tuning, employing an effective decoding strategy is crucial for generating output from language models. Decoding, the process of text generation from SLMs, involves iteratively selecting the next word. A fundamental method is the greedy search, which predicts the most likely token at each step. This is formally modeled as: $x _ { i } = \arg \operatorname* { m a x } _ { x } P ( x \mid x _ { < i } )$ , where $x _ { i }$ is the token with the highest probability at the $i$ -th step, conditioned on the preceding context $x _ { < i }$ . Other decoding strategies, such as beam search or top- $\mathbf { \nabla } \cdot \mathbf { k }$ sampling, are crucial for generating high-quality outputs. Beam search balances exploration and exploitation by considering multiple possible sequences simultaneously, while top- $\mathbf { \nabla } _ { \cdot \mathrm { k } }$ sampling introduces diversity and creativity in text generation. These strategies collectively ensure that SLMs are efficient and capable of delivering high performance across various natural language processing tasks.

Manuscript submitted to ACM

# 2.3 Obtain SLMs from LLMs

Obtaining an SLM from an LLM is crucial for deploying in resource-constrained environments. Instead of training from scratch, leveraging an LLM allows for knowledge transfer, enabling SLMs to retain much of the LLM’s linguistic and domain knowledge with reduced training time and data. To obtain SLMs from LLMs, three primary techniques are used: pruning, knowledge distillation, and quantization. Pruning removes less critical parameters, reducing model size while aiming to maintain performance. Knowledge distillation transfers knowledge from a large teacher model to a smaller student model, preserving much of the original model’s understanding. Quantization decreases parameter precision, significantly lowering memory and computation needs with minimal impact on accuracy. These methods balance size reduction, efficiency, and performance retention.

2.3.1 Pruning. Pruning is a technique used to reduce a model’s size and computational requirements (e.g., LLMs) without significantly sacrificing its performance [126]. This process involves identifying and removing less important or redundant parameters and components from the model. The primary goal of LLM pruning is to make the model more efficient, faster, and suitable for deployment in resource-constrained environments. Typically, pruning can be categorized into two main types: unstructured pruning and structured pruning. An illustration of unstructured pruning and structured pruning is shown in Figure 7.

![## Image Analysis: bc5d3036a35f80350c1a852c4442462e99358340f2f1f023976984b679b5e8f2.jpg

**Conceptual Understanding:**
This image conceptually illustrates two different approaches to neural network pruning. Its main purpose is to visually explain and differentiate between "Unstructured Pruning" and "Structured Pruning" in the context of neural network architectures. The image communicates the idea of removing redundant or less important neurons and their connections to make a neural network smaller and more efficient, highlighting how the structural integrity of the network is affected by each method.

**Content Interpretation:**
The image depicts two common techniques for reducing the size and computational complexity of neural networks: unstructured and structured pruning. Unstructured pruning targets individual weights or neurons, potentially creating a highly sparse but irregular network structure. Structured pruning, on the other hand, removes entire neurons or even layers, leading to a more regular and often more hardware-friendly pruned architecture. The diagrams visually differentiate between active neurons (light blue) and pruned neurons (light pink), and also illustrate the nature of connections (solid for active, dotted for pruned or dropped) in each method.

**Key Insights:**
The main takeaway from this image is the visual distinction between unstructured and structured pruning. Unstructured pruning (a) is characterized by the removal of individual neurons or connections, leading to a network where connections between active and pruned neurons are represented by dotted lines, and active neurons are interspersed with pruned ones within layers. In contrast, structured pruning (b) involves the complete removal of entire neurons, including all their connections, resulting in a cleaner, sparser network of only active neurons and their full connections. This implies that structured pruning often leads to more regular, hardware-efficient models, while unstructured pruning might offer higher sparsity but more complex implementation. The explicit labels "(a) Unstructured Pruning" and "(b) Structured Pruning," along with the visual differentiation of "Active Neurons" and "Pruned Neurons," clearly support this distinction.

**Document Context:**
This image is highly relevant to the document's section "2.3 Obtain SLMs from LLMs" as pruning is a critical technique for compressing large language models (LLMs) into smaller, more efficient small language models (SLMs). By illustrating unstructured and structured pruning, the image provides a foundational understanding of how these compression methods work at a neural network level, directly supporting the goal of obtaining SLMs that are lighter, faster, and require less computational resources while maintaining performance.

**Summary:**
The image illustrates two distinct methods of neural network pruning: (a) Unstructured Pruning and (b) Structured Pruning. The legend at the top defines two types of neurons: light blue circles represent "Active Neurons," and light pink circles represent "Pruned Neurons." 

In (a) "Unstructured Pruning," the diagram shows a three-layer neural network. The input layer on the left has four neurons, with the second from the bottom being a "Pruned Neuron." The hidden layer in the middle has four neurons, with the second from the top and the bottom-most neuron being "Pruned Neurons." The output layer on the right has two neurons, both of which are "Active Neurons." Connections between neurons are represented by lines: solid lines denote connections between "Active Neurons," while dotted lines indicate connections involving at least one "Pruned Neuron." This method removes individual connections or neurons without regard to the overall network structure, leading to a sparse, irregular connection pattern.

In (b) "Structured Pruning," the diagram also shows a three-layer neural network. The input layer on the left has four neurons, all depicted as "Active Neurons." The hidden layer in the middle has four neurons, with the second from the top and the bottom-most neuron being "Pruned Neurons." The output layer on the right has two neurons, both of which are "Active Neurons." In this method, entire "Pruned Neurons" (and all their associated connections) are removed, resulting in a network with fewer, but fully connected, active neurons. All connections shown are solid lines, indicating that only connections between "Active Neurons" are maintained, and the overall structure remains more regular.](images/bc5d3036a35f80350c1a852c4442462e99358340f2f1f023976984b679b5e8f2.jpg)
Fig. 7. Unstructured and structured pruning.

Unstructured Pruning [76, 102, 201, 300, 321, 438, 442] prunes an LLM by removing weights individually without considering its internal structure. The least significant parameters are pruned according to specific criteria (e.g. magnitude or impact on the output). This method can achieve significant compression while maintaining performance. However, it can also lead to irregular memory access patterns and reduced hardware efficiency because the pruned model lacks a regular structure. SparseGPT [102] is a representative unstructured pruning method that can reduce large-scale GPT models like OPT-175B [440] and BLOOM-176B [181] to up to $6 0 \%$ sparsity using a novel sparse regression solver. Wanda [321] combines weight magnitudes with input activations to efficiently identify and discard less impactful parameters. It operates in a single forward pass, rapidly achieving high sparsity without retraining. It is also worth noting that recent studies specifically address the compatibility issues between pruning and Low-rank Adaptation (LoRA) [142], such as LoRAPrune [438].

Structured Pruning [13, 19, 51, 110, 124, 188, 196, 232, 243, 305, 306, 385, 410, 446], which prunes an LLM by targeting entire structural components—such as neurons, channels, or layers—rather. This approach allows for a direct reduction in dimensionality, thus efficiently reducing model complexity and memory usage. Although structured pruning may lead to higher accuracy degradation than unstructured pruning, it simplifies implementation without requiring specialized hardware. ShortGPT [243] proposes the Block Influence (BI) metric, which measures the significance of each layer based on its transformation of hidden states. Essentially, a transformer block’s influence is measured by how much it alters the hidden states. By calculating BI scores, ShortGPT determines which layers contribute minimally to the overall performance and removes these low-importance layers. This simple yet effective layer removal strategy significantly reduces the model’s parameters and computational requirements. LLM Pruner [232] offers a method to efficiently prune LLMs without access to the original training dataset. It employs a three-step compression pipeline: Discovery (identifying interdependent structures), Estimation (evaluating the performance impact of each group), and Recovery (post-training to address performance loss). NutePrune [196] enhances structured pruning with a

![## Image Analysis: dac7bbe74f6604b3a7c2f738489f1e9f8bd44ea608360eca77d1fe9e35bf73c0.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates two fundamental approaches to "Knowledge Distillation (KD)" within machine learning: "White-box KD" and "Balck-box KD" (transcribed with the typo from the image). The main purpose is to visually differentiate these two methods, highlighting how the accessibility of the 'Teacher Model' dictates the distillation strategy when aiming to train a 'Student Model' from 'Training Data'. It conveys the key idea that knowledge can be transferred from a larger, more complex model to a smaller, more efficient one, either by directly accessing the teacher's internal states (white-box) or by interacting with it as an opaque system via an API (black-box).

**Content Interpretation:**
The image illustrates two distinct methodologies for knowledge distillation (KD): 'White-box KD' and 'Balck-box KD'. These processes are shown to leverage 'Training Data' to inform the creation of a 'Student Model' from a more complex or larger 'Teacher Model'.

In the 'White-box KD' section, the 'Teacher Model' is depicted with its internal architecture (five interconnected rectangular blocks), implying that its internal states, parameters, or intermediate representations are accessible during the distillation process. The flow shows 'Training Data' being used to train or interact with this 'Teacher Model', and then knowledge is directly distilled (labeled 'White-box KD') from this transparent teacher to the 'Student Model' (three interconnected rectangular blocks), which is a simplified version.

The 'Balck-box KD' (transcribed verbatim, including the typo for 'Black-box KD') section, on the other hand, shows 'Training Data' interacting with an 'LLM API' (represented by a logo). This signifies that the teacher model is treated as a black box, meaning its internal workings are not accessible; only its outputs or predictions via an API are used for distillation. The knowledge is then distilled (labeled 'Balck-box KD') from this opaque 'LLM API' to the 'Student Model'.

Therefore, the image visually contrasts two approaches based on the level of access to the teacher model's internals: full access (white-box) versus API-only access (black-box). The 'Student Model' consistently appears to be a smaller, perhaps more efficient, version of the teacher.

**Key Insights:**
The main takeaways from this image are the clear distinction between white-box and black-box knowledge distillation methods, and their implications for model accessibility and use. Specifically:

1.  **Teacher Model Transparency:** 'White-box KD' (as evidenced by the explicit representation of the 'Teacher Model' with its internal structure of five blue rectangles) requires direct access to the internal architecture and possibly intermediate layers of the teacher model for distillation. This allows for more granular knowledge transfer.
2.  **API-Based Teacher Model:** 'Balck-box KD' (as labeled in the diagram and connected to the 'LLM API' logo) is used when the teacher model, particularly a Large Language Model (LLM), is only accessible via an API. This means the distillation process relies solely on the teacher's outputs, without insight into its internal workings.
3.  **Consistent Student Model Goal:** In both scenarios, the objective is to create a 'Student Model' (represented by three blue rectangles), which is typically a smaller or simpler model that retains the knowledge of the larger teacher model.
4.  **Application to LLMs:** The inclusion of 'LLM API' specifically highlights the relevance of black-box knowledge distillation for leveraging powerful, proprietary, or very large LLMs where internal access is not feasible.

The image provides insights into practical considerations for knowledge distillation, particularly in the context of leveraging powerful LLMs. It underscores that the choice of distillation method is largely determined by the accessibility of the teacher model.

**Document Context:**
This image directly supports the document's Section 2.3, titled "Obtain SLMs from LLMs" (Small Language Models from Large Language Models). The diagrams visually explain two primary methods—white-box and black-box knowledge distillation—that are crucial for training smaller, more efficient 'Student Models' from larger 'Teacher Models', particularly when the teacher is a complex 'LLM API'. It clarifies how the nature of access to the teacher model (full transparency vs. API-only) dictates the choice of distillation strategy, which is fundamental to the process of obtaining SLMs from LLMs as discussed in the section. The illustration provides a concrete visual representation of the abstract concepts of knowledge distillation.

**Summary:**
The image illustrates two distinct approaches to knowledge distillation: White-box KD and Black-box KD, for obtaining Student Learning Models (SLMs) from Large Language Models (LLMs). Both processes begin with 'Training Data' and aim to produce a 'Student Model'.

In the 'White-box KD' process, 'Training Data' is fed into a 'Teacher Model', which is depicted as a series of five interconnected blue rectangular blocks, representing an accessible and inspectable model architecture. Knowledge is then transferred directly from this 'Teacher Model' to a 'Student Model' via a process explicitly labeled 'White-box KD'. The 'Student Model' is represented by a simpler architecture of three interconnected blue rectangular blocks. This implies that the internal workings and representations of the teacher model are accessible during the distillation process.

Conversely, the 'Balck-box KD' (transcribed exactly as shown, noting a potential typo for 'Black-box KD') process also starts with 'Training Data'. However, instead of a visible 'Teacher Model' architecture, the training data is directed to an 'LLM API', represented by a stylized logo (resembling the OpenAI logo). This signifies that the teacher model's internal structure is not directly accessible; only its outputs are available via an API. Knowledge is then distilled from this 'LLM API' to a 'Student Model', which, similar to the white-box approach, is represented by three interconnected blue rectangular blocks. The distillation process here is labeled 'Balck-box KD', indicating that it treats the teacher model as an opaque system.

The core difference highlighted is the accessibility of the teacher model: direct access to internal layers in White-box KD versus interaction solely through an API in Black-box KD. Both methods aim to create a more compact 'Student Model' from a larger or more complex teacher model.](images/dac7bbe74f6604b3a7c2f738489f1e9f8bd44ea608360eca77d1fe9e35bf73c0.jpg)
Fig. 8. Illustration of white-box and black-box knowledge distillation [245].

Numerous-teacher method, employing variable sparsity masks and LoRA modules to guide the pruning process. This approach effectively reduces model size and complexity. COST-EFF [305] introduces a slenderized backbone—a form of structured pruning—and a multi-exit model that employs task-specific calibration through knowledge distillation. This slenderization reduces the model’s spatial footprint, while the multi-exit strategy effectively balances utility and runtime costs. To enhance the flexibility of structural pruning, DISP-LLM [110] breaks the structural dependencies in regular methods by allowing different layers to have different subsets of features along the embedding dimension.

2.3.2 Knowledge Distillation. Knowledge distillation (KD) compresses a larger teacher model into a smaller student model by training the student to mimic the teacher’s outputs [136]. This enables the student to retain much of the teacher’s capabilities with fewer parameters, making it ideal for scaling down LLMs for resource-limited environments while maintaining performance. KD can be categorized into white-box and black-box approaches [361, 403, 457] as shown in Figure 8. In White-Box KD, the student has access to the teacher’s internal states or output distributions [6, 119, 157, 169, 173, 265, 381, 434]. Generalized Knowledge Distillation (GKD) [173] introduces skew KL divergence to stabilize gradients and enhance performance, using an adaptive off-policy approach to minimize noisy feedback and improve efficiency. Black-Box KD relies only on teacher outputs without having access to model internals [48, 182, 271, 360]. Methods like Distilling Step-by-Step [141] use teacher-generated rationales to train smaller models, improving performance with fewer examples. LaMini-LM [380] creates a diverse instruction dataset with GPT-3.5 Turbo responses, enabling robust performance in smaller models.

![## Image Analysis: f384686f4977d2530cb223bab852d8696ab57ce6dde9887671940026792ea987.jpg

**Conceptual Understanding:**
This image conceptually illustrates two primary model quantization techniques, Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). The main purpose is to clearly differentiate these methods by showing their distinct workflows, the states (trainable or frozen) of model parameters at various stages, and the type of data used (training or calibration). It communicates the key ideas of reducing model precision from 'Full-Precision' to 'Low-Precision' for efficiency, and how this process can be integrated either during or after the model training phase. The diagram visually explains the procedural differences and trade-offs between a more integrated, data-driven approach (QAT) and a more direct, post-hoc approach (PTQ).

**Content Interpretation:**
The image details two methods for model quantization: Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ). Both processes illustrate the transformation of model parameters from a Full-Precision representation to a Low-Precision representation, using specific numerical examples (e.g., 1.2, 0.2 for full-precision; 1, 0 for low-precision). The diagrams highlight the 'trainable' or 'frozen' state of different model components at various stages using flame and snowflake icons, respectively. QAT involves an initial trainable Full-Precision model, a decomposition step that results in a frozen Low-Precision model alongside intermediate trainable parameters, and then a finetuning phase using Training Data. PTQ, on the other hand, directly converts a Full-Precision model (implicitly frozen) to a Low-Precision model using Calibration Data, with the resulting Low-Precision model being optionally trainable. The different data inputs (Training Data vs. Calibration Data) and the explicit indication of trainable/frozen components at each step are significant in differentiating the two approaches.

**Key Insights:**
1. QAT integrates quantization into the training or finetuning process: It begins with a 'Trainable' 'Full-Precision' model, undergoes 'Decomposition', and then 'Finetune for downstream task' where specific auxiliary parameters are 'Trainable' while the 'Low-Precision' model is 'Frozen', utilizing 'Training Data'. This suggests a more thorough, accuracy-focused quantization approach. 2. PTQ applies quantization post-training: It directly converts a 'Full-Precision' model to a 'Low-Precision' model using 'Calibration Data', with the resulting 'Low-Precision' model being '(Optional)' 'Trainable'. This indicates a simpler, faster method for quantization. 3. Distinct data usage: QAT leverages 'Training Data' for finetuning, implying active learning for quantized parameters. PTQ uses 'Calibration Data', typically a smaller, representative dataset, for parameter determination, suggesting a lower computational cost. 4. Different 'Trainable' components: In QAT, specific parameters (e.g., scale factors represented by the columns of numbers) are trainable during finetuning, while the quantized weights are frozen. In PTQ, the quantized weights themselves can be optionally finetuned. 5. Both methods achieve precision reduction: Both diagrams clearly show the transformation of numerical values from fractional 'Full-Precision' (e.g., 1.2, 0.2) to integer 'Low-Precision' (e.g., 1, 0), illustrating the core goal of quantization.

**Document Context:**
The image is presented in Section 2.3, titled 'Obtain SLMs from LLMs', indicating its direct relevance to the process of creating Small Language Models (SLMs) from Large Language Models (LLMs). Quantization, as depicted, is a critical technique for reducing the computational and memory footprint of models, which is essential for deploying large models as smaller, more efficient SLMs. The illustration provides a foundational understanding of the methods used to achieve this model compression, supporting the broader narrative of obtaining compact and efficient language models.

**Summary:**
This diagram illustrates two primary methods for model quantization, Quantization-Aware Training (QAT) and Post-Training Quantization (PTQ), both designed to convert neural network models from a Full-Precision representation to a Low-Precision representation. This conversion is crucial for reducing model size and improving inference speed, particularly when deriving Small Language Models (SLMs) from Large Language Models (LLMs).

General Legend:
- A flame icon signifies that parameters are Trainable, meaning they can be updated during an optimization process.
- A snowflake icon signifies that parameters are Frozen, meaning they are fixed and will not be updated.

1. Quantization-Aware Training (QAT):
The QAT process begins with a model represented in Full-Precision. This initial Full-Precision model has parameters, exemplified by a grid showing values such as 1.2, 0.2, ..., 3.1, 2.3 in the top row and 0.9, 1.0, ..., 0.3, 2.5 in the bottom row. This Full-Precision component is Trainable.

The model then undergoes a Decomposition step. This process transforms the Full-Precision model into an intermediate representation. This representation includes specific numerical values, shown in a column as 2.1, 5.8, ..., 3.5, 5.3, alongside a Low-Precision model. The Low-Precision model is depicted as a grid with integer values like 1, 0, ..., 3, 2 in the top row and 1, 1, ..., 0, 3 in the bottom row. At this stage, the Low-Precision component is Frozen.

Following decomposition, the model proceeds to a Finetune for downstream task step. During this finetuning, Training Data is fed into the process. In this phase, the specific numerical values (2.5, 4.8, ..., 3.8, 3.4) are Trainable, while the Low-Precision model parameters (1, 0, ..., 3, 2 and 1, 1, ..., 0, 3) remain Frozen. This suggests that QAT involves training a model where quantization is considered from the outset or introduced during finetuning, allowing for the optimization of quantization parameters alongside model weights.

2. Post-Training Quantization (PTQ):
The PTQ process starts with a pre-trained model in Full-Precision, again with example parameters like 1.2, 0.2, ..., 3.1, 2.3 and 0.9, 1.0, ..., 0.3, 2.5. In this initial state, the Full-Precision model is implicitly Frozen (no flame icon).

This Full-Precision model is directly converted into a Low-Precision model, represented by the grid of integer values like 1, 0, ..., 3, 2 and 1, 1, ..., 0, 3. This conversion process utilizes Calibration Data. Calibration Data is typically a smaller dataset used to determine the optimal quantization parameters without requiring full retraining.

Crucially, the resulting Low-Precision model in PTQ is indicated as (Optional) Trainable. This means that after quantization, there is an option to further finetune the Low-Precision model, though it's not a mandatory step like in QAT's finetuning phase.

In summary, QAT integrates quantization during training or finetuning with Training Data to achieve higher accuracy by optimizing for the low-precision format, while PTQ applies quantization after training using Calibration Data, offering a faster, post-hoc approach with an optional finetuning step for the low-precision model. Both methods are critical for efficient model deployment.](images/f384686f4977d2530cb223bab852d8696ab57ce6dde9887671940026792ea987.jpg)
Fig. 9. Illustration of quantization-aware training (QAT) and post-training quantization (PTQ).

Table 1. Representative quantization methods.   

<table><tr><td>Methods</td><td>Bit</td><td>Type</td><td>Technical Contribution</td><td>Problems</td></tr><tr><td>SqueezeLLM[170]</td><td>3-bit</td><td>PTQ</td><td>Sensitivity-based non-uniform quantization, dense and sparse decomposition</td><td>ultra-low bit quantization</td></tr><tr><td>JSQ [122] FrameQuant [4]</td><td>Flexible Fractional bit</td><td>PTQ</td><td>Joint Sparsification and Quantization Fractional bit widths</td><td>better compression-accuracy trade-offs. better compression-accuracy trade-offs.</td></tr><tr><td>OneBit [400]</td><td>1-bit</td><td>PTQ PTQ</td><td>Quantization-aware knowledge distillation</td><td>1-bit quantization</td></tr><tr><td>BiLLM [147]</td><td>1-bit</td><td>PTQ</td><td>Crucial Weights Selection,Block-based error compensation</td><td>1-bit quantization</td></tr><tr><td>LQER [432] I-LLM[144]</td><td>Flexible Flexible</td><td>PTQ PTQ</td><td>Quantization Error Minimization Fully-Smooth Block-Reconstruction， Dy-</td><td>better compression-accuracy trade-offs Integer-only Quantization</td></tr><tr><td>PV-Tuning [239]</td><td></td><td></td><td>namic Integer-only MatMul and Integer-only Non-linear Operators</td><td></td></tr><tr><td>BitNet [357]</td><td>1-bit/2-bit 1-bit</td><td>PTQ</td><td>PV algorithm</td><td>better compression-accuracy trade-offs.</td></tr><tr><td>BitNet b1.58 [231]</td><td>{-1,0,1}</td><td>QAT</td><td>1-bit Transformer Architecture</td><td>1-bit quantization</td></tr><tr><td>PEQA[168]</td><td>Flexible</td><td>QAT</td><td>Ternary Parameters</td><td>1-bit quantization</td></tr><tr><td></td><td></td><td>QAT</td><td>Quantization Scales Optimization</td><td>Parameter-Effcient Finetuning</td></tr><tr><td>QLoRA[81]</td><td>NF4</td><td>QAT</td><td>4-bit NormalFloat and Double Quantization</td><td>Parameter-Efficient Finetuning</td></tr></table>

2.3.3 Quantization. Quantization reduces the storage and computational demands of LLMs by converting floatingpoint representations into lower-precision formats, significantly cutting both storage requirements and computational complexity. Existing methods fall into two categories: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). Figure 9 illustrates the two quantization methods. Post-Training Quantization, applied after training, simplifies model compression without altering the architecture or requiring retraining, though it may result in precision loss. Consider a group or block of weights w; the linear operation can be expressed as $y = \mathbf { w x }$ , while the quantized version is given by ?? = ?? (w)x. Generally, the quantization function ?? is defined as [207]: ?? (w) = Δ · Round   wΔ  , Δ = max( |w| )2?? −1 , where $N$ is the number of quantization bits, and $\Delta$ is the quantization scale factor determined by the absolute maximum value of w. Quantization-Aware Training (QAT) enhances LLM efficiency by directly incorporating quantization into the training process, often resulting in higher accuracy than PTQ. During QAT, the forward pass utilizes quantized weights $Q ( \mathbf { W } )$ and activations $Q ( \mathbf { X } )$ , while retaining full-precision values during the backward pass and for updating gradients to ensure stable learning dynamics. The comparisons of the post-training quantization methods are summarized in Table 1, detailing precision, addressed problems, and technical contributions of each method.

2.3.4 Low-Rank Techniques. Low-rank techniques compress LLMs by approximating a high-dimensional weight matrix with two lower-dimensional matrices, reducing computational and memory requirements. A matrix $\mathbf { W } \in \mathbb { R } ^ { m \times n }$ is approximated as $\mathbf { W } \approx \mathbf { A } \times \mathbf { B }$ , where $\mathbf { A } \in \mathbb { R } ^ { m \times r }$ and $\mathbf { B } \in \mathbb { R } ^ { r \times n }$ , with $r$ much smaller than $m$ or $n$ , reducing the number of parameters. Building on this concept, Ji et al. [158] propose a low-rank method tailored for LLMs, leveraging the observation that while LLMs have high-rank weights, their feature interactions tend to exhibit low-rank properties. The method estimates feature distributions using pooled covariance matrices and allocates distinct compression ratios to layers based on their sensitivity to low-rank compression. A Bayesian optimization strategy, using a Gaussian process as the surrogate model, optimizes the allocation of low-rank dimensions, ensuring the model maintains performance while achieving significant compression. Transitioning from model compression to fine-tuning, Cho et al. [62] tackles system and data heterogeneity with the HETLORA method, which uses heterogeneous low-rank approximations to accommodate the diverse capabilities of clients and data complexities. By combining local rank self-pruning with sparsityweighted aggregation, it balances high and low-rank LoRA modules, improving convergence speed and performance compared to uniform approaches. LLM-Neo [409] combines knowledge distillation with low-rank adaptation (LoRA) to improve the efficiency of transferring knowledge from a teacher LLM to a compact student model.

Table 2. Advanced enhancement methods for SLM.   

<table><tr><td>Topic</td><td>Method</td><td>Main Contribution</td></tr><tr><td>Training from Scratch</td><td>MindLLM[412] MobiLlama [336] MobileLLM[223]</td><td>Bilingual models with advanced features. On-device SLM with dual objectives for efficiency and capability. Optimizes LLM deployment on mobile with advanced architecture.</td></tr><tr><td>Supervised Fine-tuning</td><td>MobileBERT322] Alpaca 7B [329] RLHF [264] DPO [283]</td><td>Compact BERT for efficient fine-tuning. Uses ChatGPT-generated tasks to tune Llama 7B. Trains using human-preferred data and reinforcement learning.</td></tr><tr><td>Data Quality in KD</td><td>TinyStory [96] AS-ES [384] Self-Amplify [29]</td><td>Dynamically adjusts log probabilities to prevent model degradation. Enhances narrative coherence in child-friendly datasets. Improves CoT by categorizing reasoning steps. Automates CoT data annotation for small models.</td></tr><tr><td>Distillation for SLM</td><td>GKD [6] DistiLLM [173] Adapt-and-Distill [414] SmoothQuant [386]</td><td>Aligns training and inference distributions using on-policy sequences. Uses skew KL divergence and adaptive off-policy for output utilization. Domain adapts both teacher and student models before distillation.</td></tr><tr><td>Quantization</td><td>BiLLM[147] LLM-QAT [221] PB-LLM [299] OneBit [400] BitNet [357] BitNetb1.58[231]</td><td>Balances quantization difficulty using per-channel scaling. Applies Hessian-based metrics for binary residual approximation. Uses data-free knowledge distilation and logit distilation forfine-tuning. Binarizes non-salient weights while preserving others in higher precision. Achieves near 1-bit quantization with minimal performance loss. Introduces 1-bit Transformer architecture with BitLinear layers.</td></tr><tr><td>LLM techniques</td><td>Moqt[234] SLM-RAG [215]</td><td>Implements a ternary weight system in enhanced BitNet. Copmpiesfiterianderanlighis oue Informatin ratin tas Shows that SLMs with RAG can match LLM performance.</td></tr></table>

# 3 ADVANCED ENHANCEMENT STRATEGIES FOR SMALL LANGUAGE MODELS

With the foundational concepts introduced in Section 2, this section explores various advanced techniques that enhance the performance of SLMs, including innovative training methods for training SLMs from scratch, supervised fine-tuning (SFT) to align SLMs to adhere to instructions, advanced knowledge distillation and quantization techniques, and techniques frequently used in LLMs such as mixture-of-experts to enhance SLM for specific applications. A summary of enhancement techniques is also summarized in Table 2.

# 3.1 Innovative Training Methods for Small Language Models from Scratch

In scenarios with limited resources, we aim to train small language models to provide efficient, cost-effective solutions tailored for specific domains, while still maintaining competitive performance with larger models. Training small language models (SLMs) from scratch involves unique strategies that diverge significantly from those used for large language models (LLMs). This section synthesizes cutting-edge techniques tailored to optimize the inherent capabilities of SLMs, underscoring their potential to match or surpass larger counterparts in efficiency and effectiveness. As shown in Figure 10, the methods for training SLMs from scratch can be categorized into three primary categories: Architecture Design, Data Construction, and Optimization Strategy. Next, we introduce each category in detail.

Architecture Design for SLMs When designing SLM architectures, parameter-sharing techniques are employed to minimize space usage and reduce the model’s size. As shown in the first part of Figure 10, parameter sharing is achieved by two approaches: (i) a single Feed-Forward Network (FFN) module is shared by every transformer layer. As shown in Figure 10 (1) middle, FFN layer sharing/reusing can maintain a smaller size while still benefiting from the depth and complexity gained through repeated processing of input data. This technique is firstly applied in MobiLlama [336] which surpasses the performance of existing SLMs of comparable size. (ii) Entire transformer blocks are shared. As shown in Manuscript submitted to ACM

![## Image Analysis: 1d496ff147a34dad2f537bee9636ed019e3dc371882a648ff85001144a298f46.jpg

**Conceptual Understanding:**
This image conceptually illustrates three innovative strategies for training Small Language Models (SLMs) from scratch. Its main purpose is to present a multifaceted approach that addresses architectural efficiency, data quality, and iterative training optimization. It communicates key ideas about how to make SLM training more effective and resource-efficient by focusing on parameter sharing, data preprocessing, and targeting challenging samples during the learning process.

**Content Interpretation:**
The image illustrates three key innovative strategies for training Small Language Models (SLMs) from scratch: architectural parameter sharing, data filtering for quality, and a multi-round optimization strategy. Each section details a specific aspect of the training pipeline, emphasizing methods to potentially improve efficiency, data quality, and model robustness.

**Architectural Design (Parameter Sharing):** This part shows different ways to structure the computational blocks within a language model. The 'Reuglar LLM' baseline has independent FFN and MHSA layers per block. The 'Shared FFN' approach suggests reusing the FFN weights across different blocks, likely to reduce the total number of parameters and potentially improve generalization for smaller models. 'Immediate Block Sharing' implies reusing entire blocks, which could be a form of weight tying or block-level recursion to build deeper networks with fewer unique parameters. The significance is exploring parameter efficiency for SLMs.

**Data Construction (Data Filtering):** This section highlights the critical importance of data quality. It shows that raw 'Datasets' are transformed into 'High quality Datasets' by applying various 'Filtering methods'. The specified methods—'Format Cleaning, Deduplication, Sensitive information Exclusion, Self-Repeating Content Filter....'—are crucial for removing noise, redundancy, privacy-sensitive content, and repetitive patterns that could hinder effective learning and lead to undesirable model behaviors. This emphasizes that for SLMs, quality often trumps raw quantity.

**Optimization Strategy (Multiple-round training):** This part describes an iterative, feedback-driven training loop. An 'SLM' is trained on 'Datasets'. The loop indicates that 'Hard samples' are identified from the datasets, fed into the SLM, and the 'Training loss' from this process informs further iterations. This suggests a curriculum learning or active learning approach where the model focuses on the most challenging examples to improve its performance and robustness incrementally, rather than just passively training on a fixed dataset.

**Key Insights:**
The main takeaways from this image are: 

1.  **Parameter Efficiency in Architecture:** For Small Language Models, architectural designs like 'Shared FFN' and 'Immediate Block Sharing' are explored to reduce the number of unique parameters and potentially improve learning efficiency compared to 'Reuglar LLM' structures. This implies that for SLMs, creative parameter sharing can be a viable strategy (evidence: 'Shared FFN', 'Immediate Block Sharing' labels and block diagrams).
2.  **Crucial Role of Data Quality:** Data preprocessing through rigorous filtering is paramount for effective SLM training. Simply using raw 'Datasets' is insufficient; instead, methods like 'Format Cleaning, Deduplication, Sensitive information Exclusion, Self-Repeating Content Filter....' are essential to produce 'High quality Datasets' that prevent the model from learning from noisy, redundant, or problematic data (evidence: 'Filtering methods' box and its detailed list).
3.  **Iterative and Targeted Optimization:** Training SLMs can benefit significantly from a multiple-round, iterative strategy that specifically addresses 'Hard samples' identified during the training process. This feedback loop, driven by 'Training loss', allows the model to continuously improve its performance on challenging examples, leading to a more robust and capable SLM (evidence: the cyclical flow between 'Datasets' and 'SLM' labeled with 'Hard samples' and 'Training loss').

These insights collectively suggest that innovative SLM training involves a holistic approach covering efficient model design, meticulous data preparation, and smart iterative optimization.

**Document Context:**
This image directly supports and elaborates on the document section titled "3.1 Innovative Training Methods for Small Language Models from Scratch." It serves as a visual summary and breakdown of the proposed or discussed innovative techniques. By presenting these three core strategies—architecture design, data construction, and optimization—the image provides concrete examples and a conceptual framework for understanding how to approach SLM training. It visually reinforces the text's discussion by illustrating the mechanisms of parameter sharing, the necessity of data filtering, and the iterative nature of targeted training, thereby enhancing the reader's comprehension of the methodologies involved in building SLMs effectively.

**Summary:**
This image, titled "Innovative Training Methods for Small Language Models from Scratch," illustrates three distinct strategies for enhancing the training of Small Language Models (SLMs). It covers architectural design with parameter sharing, data construction through filtering, and an optimization strategy based on multiple-round training.

**1. Architecture Design: Parameter Sharing:** This section presents three different architectural configurations of blocks, each containing a Feed-Forward Network (FFN) and a Multi-Head Self-Attention (MHSA) module. The first column depicts a "Reuglar LLM" structure, showing distinct "FFN 1" and "MHSA 1" within "Block 1", followed by "FFN 2" and "MHSA 2" in "Block 2", and similarly for "Block i". The second column introduces "Shared FFN" where the FFN component is shared across blocks, meaning "Shared FFN" and "MHSA 1" are in "Block 1", "Shared FFN" and "MHSA 2" are in "Block 2", and "Shared FFN" and "MHSA i" are in "Block i". The third column illustrates "Immediate Block Sharing" where individual blocks are reused, showing "Block1" repeated, then "Block2" repeated, and similarly "Block i", suggesting immediate reuse of the entire block structure.

**2. Data Construction: Data Filtering:** This section outlines a process for preparing high-quality datasets. It starts with generic "Datasets" which then undergo various "Filtering methods". The specific filtering methods listed are "Format Cleaning, Deduplication, Sensitive information Exclusion, Self-Repeating Content Filter...." These methods are applied to transform the initial datasets into "High quality Datasets."

**3. Optimization Strategy: Multiple-round training:** This section describes an iterative training approach. It shows a cyclical process between "Datasets" and the "SLM". "Hard samples" are identified from the "Datasets" and fed into the "SLM" for training. The training process generates "Training loss", which is then used to refine the selection of "Datasets" or to guide further training rounds, creating a feedback loop to improve the SLM's performance, particularly on challenging examples.](images/1d496ff147a34dad2f537bee9636ed019e3dc371882a648ff85001144a298f46.jpg)
Fig. 10. Innovative Training Methods for Small Language Models from Scratch

Figure 10 (1) right, Transformer Block-wise Sharing is another parameter-sharing approach that maintains depth and complexity. There are different transformer block-wise sharing strategies such as repeating the transformer blocks all over again or repeating the immediate transformer block. This technique is applied in MobileLLMs [223] which has 125M and 350M parameters. MobileLLMs demonstrate performance improvements of $2 . 7 \%$ and $4 . 3 \%$ , respectively, compared to previous models with equivalent parameters. Moreover, they exhibit accuracy comparable to LLaMa-2-7B on API call tasks, highlighting the capabilities of smaller models in mobile environments.

Data Construction For SLMs, the emphasis on data quality surpasses that of quantity and diversity [412]. Experiments demonstrate that using a quality filtering approach to remove low-quality data can lead to improved performance in SLMs [412]. Unlike large models, which can handle diverse and large datasets, SLMs benefit more from cleaner, high-quality data probably due to their limited capacity against noise. Generally, data processing has several steps: (i) Remove HTML, CSS, JS, and non-text elements for clean text; (ii) Filter low text-to-content ratio web pages; (iii) Deduplicate using SimHash [77, 293]; (iv) Exclude sensitive/offensive content with heuristics and token replacements; (v) Remove self-repeating phrases of advertisements to enhance dataset informativeness [47, 412]. These steps collectively ensure that training data has high-quality, informative texts. SLMs also significantly benefit from these techniques. For example, MindLLMs [412], which are bilingual lightweight language models (available in 1.3B and 3B versions), adopt these data processing techniques and achieve improved capability acquisition.

Training Strategy for SLMs For LLMs, due to the large model size and data volume, LLMs are usually trained with one round. For SLMs, multiple-round training can be applied [328]. Considering some examples are hard to fit, hard examples can be trained with a high probability [328]. For each round of training, the data sampling probability is updated according to the overall loss of that sample. Experiments results show that two rounds of training and a $5 0 \%$ sampling rate are a good trade-off between performance and training efficiency. Tang et al. [328] show that a deep and thin neural architecture and multiple-round training can enhance the performance of the trained Pangu 1.5B pro model. This model outperforms the conventionally trained Pangu 1.5B and a series of other comparable large language models with similar model sizes on multiple benchmark datasets, achieving an average performance increase of $8 . 8 7 \%$ .

![## Image Analysis: bbd77bd34f4d7955af6799a68109e49f177df7268d73ddaf5fc5f74d572be046.jpg

**Conceptual Understanding:**
This image conceptually represents three distinct, innovative methodologies for the training and enhancement of Small Language Models (SLMs) from an initial, untrained state. It visually outlines different strategies to imbue SLMs with capabilities for specific tasks, general instruction following, and alignment with human preferences.

The main purpose of the image is to illustrate the various fine-tuning paradigms available for SLMs, highlighting how different types of data and training mechanisms can be employed to achieve different outcomes. It aims to clarify the processes of supervised fine-tuning on downstream tasks, instruction tuning (which often involves leveraging larger models for data generation), and preference optimization (a form of reinforcement learning from feedback).

Key ideas and concepts being communicated include:
1.  **Supervised Fine-Tuning (SFT):** The foundational method of training a model on labeled data for a specific objective.
2.  **Data Generation/Augmentation:** The concept of using powerful external resources (like Large Language Models) to create or enhance training datasets for SLMs, especially for instruction-following.
3.  **Instruction Tuning:** A method focused on training models to understand and execute general instructions, rather than just performing specific, narrow tasks.
4.  **Preference-Based Learning/Reinforcement Learning from Human Feedback (RLHF):** An advanced technique for aligning model behavior with human values, preferences, or complex objectives by learning from comparative feedback rather than direct labels.
5.  **Model Iteration and Refinement:** The idea that models can be continuously improved through feedback loops, particularly evident in preference optimization.

**Content Interpretation:**
The image illustrates three distinct methodologies for fine-tuning Small Language Models (SLMs) from scratch, as outlined in the document context "3.1 Innovative Training Methods for Small Language Models from Scratch."

**Processes Shown:**
*   **(a) Fine-tuning on downstream tasks:** This depicts the direct application of Supervised Fine-Tuning (SFT) to an Untrained/Unfine-tuned Small Language Model (UFT SLM) using "Task-specific data" to produce a Fine-Tuned Small Language Model (FT SLM). This is a standard supervised learning paradigm where a model is trained on a specific dataset for a particular task.
*   **(b) Fine-tuning on instructions (instruction tuning):** This process involves using a larger model, implied by the OpenAI logo and the "Annotate/Self-instruct" label, to generate high-quality training data. "Unlabeled instructions" are transformed into "LLM-annotated instructions," which then serve as the input for Supervised Fine-Tuning (SFT) of an UFT SLM to create an FT SLM. This highlights the use of Large Language Models (LLMs) for data generation and augmentation to improve SLMs.
*   **(c) Preference Optimization:** This illustrates a reinforcement learning from human feedback (RLHF) or similar alignment process. It involves an "LM Policy" (the language model being optimized) and a "Reward model." The LM Policy generates "Sample completions." The Reward model evaluates these completions, as well as external "Preference data" (where one output `yw` is preferred over another `yt`, denoted as `yw > yt`), to provide a "Label reward" back to the LM Policy. This creates a feedback loop for the LM Policy to learn and align with desired preferences.

**Significance of Information Presented:**
*   The distinction between supervised fine-tuning (SFT) and preference optimization (RLHF-like methods) is crucial for understanding the landscape of SLM training.
*   The use of "Task-specific data" in (a) emphasizes traditional supervised learning where labeled datasets are directly available.
*   The introduction of "Unlabeled instructions" and "LLM-annotated instructions" in (b) with the "Annotate/Self-instruct" step highlights a growing trend of using larger, more capable models (LLMs) to automatically generate or enhance training data for smaller models, reducing the need for extensive human annotation.
*   The "Preference data" `yw > yt` in (c) is significant because it represents human feedback or predefined criteria for what constitutes a better output. The "Reward model" learns from this preference data to assign values (rewards) to different model outputs, guiding the "LM Policy" towards generating more desirable responses. This is a key mechanism for aligning language models with human values and complex objectives.
*   The cyclical arrows in (c) depicting "Sample completions" and "Label reward" signify an iterative learning process where the LM Policy continuously refines its behavior based on the rewards received from the Reward model.

**Support from Extracted Text Elements:**
*   "FT SLM" and "UFT SLM" clearly denote the states of the Small Language Model before and after fine-tuning.
*   "SFT" (Supervised Fine-Tuning) explicitly labels the direct training step in (a) and (b).
*   "Task-specific data" in (a) directly specifies the input for traditional fine-tuning.
*   "Unlabeled instructions," "Annotate/Self-instruct," and "LLM-annotated instructions" in (b) precisely detail the data generation pipeline using an LLM.
*   "LM Policy," "Reward model," "Sample completions," "Label reward," and "Preference data" (with `yw > yt`) in (c) explicitly describe the components and flow of the preference optimization process, including the specific notation `yw` and `yt` for preferred and less preferred outputs.
*   The sub-titles "(a) Fine-tuning on downstream tasks.", "(b) Fine-tuning on instructions (instruction tuning).", and "(c) Preference Optimization" categorize and explain the purpose of each diagram.

**Key Insights:**
The image provides several key takeaways and insights into innovative training methods for Small Language Models (SLMs):

*   **Diversity in Fine-tuning Approaches:** The primary insight is that there isn't a single method for fine-tuning SLMs; rather, there are diverse strategies tailored to different objectives and data availability. This is evidenced by the distinct processes labeled "(a) Fine-tuning on downstream tasks.", "(b) Fine-tuning on instructions (instruction tuning).", and "(c) Preference Optimization."
*   **Traditional Supervised Fine-Tuning (SFT) for Specific Tasks:** "Fine-tuning on downstream tasks" (a) highlights the foundational approach where an "UFT SLM" is directly trained using "Task-specific data" via "SFT" to become an "FT SLM." This illustrates that for well-defined tasks with available labeled data, direct supervised fine-tuning remains a viable and effective strategy.
*   **Leveraging Large Language Models (LLMs) for Data Generation:** "Fine-tuning on instructions (instruction tuning)" (b) demonstrates an innovative method where "Unlabeled instructions" are transformed into high-quality "LLM-annotated instructions" using an "Annotate/Self-instruct" process (depicted with the OpenAI logo). This shows that LLMs can act as powerful data annotators or generators, reducing manual effort and enabling "SFT" for SLMs even when human-labeled instruction data is scarce. This points to the concept of knowledge distillation or transfer from larger models to smaller ones.
*   **Importance of Preference-Based Learning for Alignment:** "Preference Optimization" (c) introduces a more advanced alignment technique. It reveals that beyond simply learning to perform tasks, SLMs can be optimized to align with specific "Preference data" (`yw > yt`) through a "Reward model" that provides "Label reward" to an "LM Policy." This highlights the move towards training models that not only perform well but also generate outputs that are preferred by humans or satisfy complex, often subjective, criteria. This approach is crucial for developing SLMs that are helpful, harmless, and honest.
*   **Iterative Refinement in Preference Optimization:** The cyclical nature of "Sample completions" from "LM Policy" to "Reward model" and "Label reward" back to "LM Policy" in (c) indicates an iterative reinforcement learning process. This signifies that models learn and improve over time through continuous feedback, constantly refining their behavior based on the reward signals.

These insights are directly supported by the verbatim text extraction, which clearly delineates the inputs, processes, and outputs for each method, as well as the specific roles of components like "Reward model," "LLM-annotated instructions," and "Preference data."

**Document Context:**
This image is highly relevant to the document's broader narrative, which is about "Innovative Training Methods for Small Language Models from Scratch." It serves as a visual explanation and categorization of three key strategies for enhancing SLMs. By detailing these methods—supervised fine-tuning on downstream tasks, instruction tuning using LLM-annotated data, and preference optimization—the image directly supports the section's aim to discuss and illustrate advanced techniques for developing and refining smaller language models.

The visual breakdown helps readers understand the distinct mechanisms and data requirements for each approach, which are critical for appreciating the innovation in SLM training. For instance, (a) provides the baseline of traditional fine-tuning, while (b) introduces the concept of leveraging larger models for data generation, and (c) dives into reinforcement learning for alignment. This progression from simpler to more complex and data-intensive methods showcases the diverse toolkit available for SLM development, providing concrete examples for the theoretical discussions within the text.

The image's comprehensive explanation, organized from main concepts to micro-details, allows readers to grasp both the overall strategy and the specific operational flow of each training method. This detailed visual representation complements and reinforces the textual descriptions of these innovative training methodologies for SLMs.

**Summary:**
This image, titled "Fig. 11. Fine-tuning for Enhancing SLMs," presents three distinct innovative training methods for Small Language Models (SLMs) from scratch. It systematically illustrates different approaches to fine-tuning SLMs, covering fine-tuning on downstream tasks, fine-tuning on instructions (instruction tuning), and preference optimization. Each method is depicted as a separate process flow, with clear labels for components, data types, and actions.

**(a) Fine-tuning on downstream tasks:** This process begins with "Task-specific data" which is used to perform "SFT" (Supervised Fine-Tuning) on an "UFT SLM" (Untrained/Unfine-tuned Small Language Model). The result of this process is an "FT SLM" (Fine-Tuned Small Language Model). This section visually explains how SLMs can be directly trained for specific applications using relevant datasets.

**(b) Fine-tuning on instructions (instruction tuning):** This method starts with "Unlabeled instructions." These instructions are then processed by an entity represented by the OpenAI logo, with the action labeled "Annotate/Self-instruct," which transforms them into "LLM-annotated instructions." These annotated instructions are then used to perform "SFT" on an "UFT SLM," leading to an "FT SLM." This illustrates a method where a Large Language Model (LLM) assists in generating a supervised dataset for an SLM.

**(c) Preference Optimization:** This section describes a reinforcement learning approach. It involves an "LM Policy" (Language Model Policy) and a "Reward model." The "LM Policy" generates "Sample completions" which are fed to the "Reward model." Concurrently, "Preference data" consisting of pairs of completions, where one is preferred over the other (symbolized by a trophy icon representing the preferred completion `yw` and a plain chat bubble representing the less preferred completion `yt`, connected by a `>` symbol, thus `yw > yt`), is also fed into the "Reward model." The "Reward model" then provides a "Label reward" back to the "LM Policy," creating a feedback loop for optimization. This process details how SLMs can be aligned with human preferences or desired behaviors through a reward-based learning mechanism.

Overall, the image provides a comprehensive visual guide to modern fine-tuning strategies for SLMs, detailing the inputs, processes, and outputs for each method, enhancing understanding of how SLMs are developed and refined for various applications.](images/bbd77bd34f4d7955af6799a68109e49f177df7268d73ddaf5fc5f74d572be046.jpg)
Fig. 11. Fine-tuning for Enhancing SLMs

# 3.2 Supervised Fine-Tuning (SFT) for Enhancing SLM performance

Supervised Fine-Tuning (SFT) employs a training methodology similar to pre-training but is specifically tailored to align models to adhere to the instructions encapsulated within various instructional datasets. This approach is designed to refine the model’s responsiveness and appropriateness to given contexts as dictated by the training data. For example, various models, such as Alpaca [329], UltraChat [86], WizardLM [392], SlimOrca [203], ShareGPT [356], Capybara [72], Deita [216], and MetaMathQA [420], incorporates a suite of conversational datasets to enhance their capabilities in context-aware dialogue and instruction adherence. Usually, as shown in Figure 11, existing SFT methods can be categorized into three categories:

• (i) Classical fine-tuning with downstream data [83, 282] trains SLMs on task-specific annotated data, transferring general language representations to specific tasks such as sentiment analysis. In the LLM era, this approach remains effective, such as enhancing LLMs by calibrating responses or assigning risk scores with smaller models such as BERT [450], or optimizing for mobile devices with MobileBERT [322].   
• (ii) Instruction tuning with LLM-generated data [86, 203, 329] or human-generated questions with LLM annotations [356] aims to align generative models with specific instructions, enhancing their instruction-following and reasoning capabilities. For example, Alpaca 7B [329] uses 52k ChatGPT-generated instruction-following examples from 175 self-instructed seed tasks to tune Llama 7B [338]. Meanwhile, StableLM [26, 340] is trained on the Restruct-v1 dataset, which includes summarization, question-answering, and sentiment analysis tasks, using instruction data from [226].   
• (iii) Preference optimization with human feedback [264, 283, 356] aims to better align language models with human preferences. Reinforcement Learning from Human Feedback (RLHF) [264] gathers human-preferred data, trains a reward model, and fine-tunes the LM using reinforcement learning. Direct Preference Optimization (DPO) [283] provides a simpler alternative to RLHF. Unlike RLHF, DPO avoids explicit reward modeling and reinforcement learning techniques. Instead, it adjusts the log probabilities of preferred versus non-preferred responses using a dynamic weighting mechanism, preventing model degradation issues typical of methods relying on probability ratios. For instance, Llama 3.2 1B & 3B apply SFT and DPO in post-training to enhance alignment with instructions and human preferences.

![## Image Analysis: 40b74914fda927e82ca795290fe714f8e91ef13c65ad3e542a958b9d257aa21e.jpg

**Conceptual Understanding:**
This image conceptually illustrates strategies for improving the performance and capabilities of Small Language Models (SLMs) through advanced data generation and augmentation techniques. The main purpose is to demonstrate how high-quality training data can be acquired or created for Supervised Fine-Tuning (SFT), particularly by utilizing the reasoning power of larger models (LLMs) and enabling SLMs to self-generate explanatory rationales. The core idea communicated is enhancing 'Data Quality in Knowledge Distillation (KD)' by providing diverse and structured examples, including simplified content and detailed step-by-step reasoning processes.

**Content Interpretation:**
The image illustrates two primary methods for enhancing the performance of Small Language Models (SLMs) through data generation and augmentation, particularly in the context of Supervised Fine-Tuning (SFT) and Data Quality in Knowledge Distillation (KD). It showcases how external, larger models (LLMs like GPT-4) can be used to augment training data in two ways: generating simple, accessible content (like 'Simple Story Datasets') and decomposing complex 'Chain of Thoughts' into structured 'Question-Step' pairs. Simultaneously, it demonstrates an SLM's capability to generate its own answers and then subsequently create rationales for those answers, including keyword extraction and Chain of Thought explanations. This dual approach emphasizes both leveraging external knowledge and fostering internal reasoning capabilities to improve data quality and quantity for SLM training.

**Key Insights:**
The image conveys several key takeaways regarding SLM enhancement:

1.  **Leveraging LLMs for Data Augmentation:** Powerful LLMs (e.g., GPT4) can be effectively used to create diverse and tailored datasets for SLMs. This is evidenced by '1. Augment data from other models (e.g., GPT4)' showing both 'Simple Story Datasets' generation and 'Complex Chain of Thoughts' decomposition.
2.  **Two Forms of Augmentation:** Data augmentation can take different forms based on the desired complexity. Simple content can be generated (e.g., 'Instruction: Create words that 3-year-old child would likely understand.' leading to 'Simple Story Datasets'), or complex reasoning can be broken down into structured, granular steps ('Complex Chain of Thoughts' being transformed into 'Q -> S1', 'Q S1 -> S2', etc.).
3.  **Self-Generation of Rationales by SLMs:** SLMs are capable of not just answering questions ('Answer: C ✓'), but also generating the underlying rationales ('Rationale Generation Methods') including extracting 'Keywords: which, type, weather, map' and constructing 'CoT: Answer is C because of keywords: ......'.
4.  **Importance of Structured Data for SFT/KD:** The emphasis on 'Complex Chain of Thoughts' breakdown into 'Q-S' pairs and the SLM's self-generated 'CoT' rationales highlight the value of structured, explainable data for Supervised Fine-Tuning and Knowledge Distillation, enabling SLMs to learn not just answers but also reasoning processes.
5.  **Data Quality:** The overall aim, as suggested by the figure title 'Data Quality in Knowledge Distillation (KD)', is to produce high-quality training data, either through external augmentation or internal generation, to improve the SLM's performance and reasoning capabilities.

**Document Context:**
This image directly supports the document's section '3.2 Supervised Fine-Tuning (SFT) for Enhancing SLM performance' by visually detailing the mechanisms of data generation and augmentation crucial for SFT. It also aligns with the caption 'Fig. 12. Data Quality in Knowledge Distillation (KD)' by showing processes that produce high-quality, structured data (e.g., CoT rationales, Q-S pairs, simple stories) which are essential for effective knowledge distillation from larger models to smaller ones. The two distinct approaches presented (augmenting data from other models vs. generating data by itself) provide concrete examples of how data for SFT can be sourced and processed to improve SLM capabilities, particularly in reasoning and explanation generation.

**Summary:**
This image, titled 'Fig. 12. Data Quality in Knowledge Distillation (KD)' and related to '3.2 Supervised Fine-Tuning (SFT) for Enhancing SLM performance', illustrates two distinct methods for generating and augmenting data to improve the performance of Small Language Models (SLMs). The diagram is divided into two main sections: '1. Augment data from other models (e.g., GPT4)' on the left, and '2. Generate data by itself' on the right, separated by a vertical dashed line.

**Section 1: Augment data from other models (e.g., GPT4)**
This section demonstrates how to leverage powerful Large Language Models (LLMs) like GPT-4 to generate diverse datasets for SLM training. It presents two parallel data augmentation paths:

*   **Path 1: Simple Story Datasets Generation**
    *   The process begins with an 'Instruction: Create words that 3-year-old child would likely understand.'
    *   This instruction is fed into an 'LLM (e.g., GPT4)'.
    *   The LLM then generates 'Simple Story Datasets', indicating the creation of easily understandable content for specific training needs.

*   **Path 2: Complex Chain of Thoughts (CoT) Augmentation**
    *   This path starts with an 'LLM (e.g., GPT4)'.
    *   The LLM processes a 'Q' (Question) input, leading to a 'Complex Chain of Thoughts'. This 'Complex Chain of Thoughts' is a detailed, multi-step reasoning process.
    *   From this complex chain, a sequence of steps is derived: 'Q' (Question) -> 'S1' (Step 1) -> 'S2' (Step 2) -> 'S3' (Step 3) -> 'S4' (Step 4) followed by an ellipsis '...'. This signifies a multi-step reasoning process.
    *   The output of this sequence is then used to create augmented data, shown in a larger box below. This augmented data represents the breakdown of complex chains of thought into simpler Q-S (Question-Step) pairs. The examples show:
        *   'Q' -> 'S1'
        *   'Q' 'S1' -> 'S2'
        *   'Q' 'S1' 'S2' -> 'S3'
        *   Followed by an ellipsis '...', implying further decomposition into more steps. This indicates that complex reasoning chains are broken down into granular question-step sequences for data generation.

**Section 2: Generate data by itself**
This section illustrates how an SLM can independently generate answers and associated rationales, effectively creating its own training or validation data.

*   The process starts with a 'Question: Which type of activity would most likely be included on a weather map A: satellite, B: seismic, C: hurricane'.
*   This question is fed into an 'SLM' (Small Language Model).
*   The 'SLM' provides an 'Answer: C ✓', indicating 'hurricane' as the correct choice.
*   Following the answer, a large blue downward arrow, symbolizing further processing, leads to 'Rationale Generation Methods'.
*   These methods then extract 'Keywords: which, type, weather, map' from the question and/or the reasoning process.
*   Finally, a 'CoT: Answer is C because of keywords: ......' is generated. This represents a Chain of Thought explanation, providing a rationale for the given answer based on the extracted keywords and implied reasoning. The ellipsis '......' suggests a more complete textual explanation would follow.

In summary, the image details two primary strategies for enhancing SLM data: leveraging powerful external LLMs for diverse data augmentation (simple stories and structured CoT breakdowns) and enabling SLMs to self-generate answers with detailed rationales, contributing to an improved and self-sufficient data generation pipeline for better SLM performance via Supervised Fine-Tuning and Knowledge Distillation.](images/40b74914fda927e82ca795290fe714f8e91ef13c65ad3e542a958b9d257aa21e.jpg)
Fig. 12. Data Quality in Knowledge Distillation (KD)

# 3.3 Data Quality in Knowledge Distillation (KD)

Transitioning from the discussion on training SLMs from scratch, this section delves into the critical role of data quality in Knowledge Distillation (KD). The motivation here is to highlight how high-quality data generated from LLMs can significantly enhance the learning efficiency and performance of SLMs. The central idea is that meticulously crafted datasets when used in KD, enable SLMs to more effectively mimic the advanced capabilities of their larger counterparts. As shown in Figure 12, the data can come either from (1) other strong LLMs (e.g., GPT-4 [2]) which are much larger and more powerful than the target SLM, or (2) the target SLM itself.

Augment Data from LLMs. LLM-generated data could be categorized as pre-training data and fine-tuning data. Firstly, due to the limitations of model size, studies have shown that training SLMs requires simple and comprehensible data [96, 183, 187, 384]. As shown in Figure 12 (1) left, TinyStory [96] shows that small models (tens of millions of parameters) can generate coherent stories for 3-4-year-olds. GPT-3.5 or GPT-4 [2] prompts create simple stories from three keywords chosen from a 1,500-word vocabulary, which are then used to train SLMs for similar outputs. This approach shows that simple and comprehensible data can help smaller models exhibit behaviors similar to those of larger language models, such as obeying scaling laws and achieving enhanced performance. On the other hand, many efforts to enhance the Chain-of-Thought (CoT) capabilities of small models involve using LLMs to generate high-quality fine-tuning CoT data. As shown in Figure 12 (1) right, these data train small models end-to-end to mimic CoT reasoning [235, 384]. AS-ES Learning [384] highlights that small models struggle with complex reasoning, even when provided detailed steps, as these require nuanced extraction and abstraction. Therefore, the study introduces a paradigm splitting reasoning into extractive segments (context reminders) and abstractive segments (inferred insights).

Augment Data from Itself. Besides distilling data from other LLMs, language models can also train on their own outputs [29, 145, 337]. Since voting strategies can improve the performance of LLMs, reasoning paths that lead to the majority answer can be further utilized to fine-tune LLMs [145]. Similarly, SLMs can generate their training data with the aid of existing rationale generation methods. Self-Amplify [29] notes that human annotation of Chain-of-Thought (CoT) data is very time-consuming; thus, automated rationale generation methods have been proposed. These methods Manuscript submitted to ACM involve three main steps: (1) Selection of samples $( x , y )$ that the model predicts correctly as few-shot examples; (2) Rationale generation, where rationales are produced using post hoc explanation methods; (3) Prompt design for SLMs, where the final prompt is crafted based on the previously generated rationales.

![## Image Analysis: 5ef0bc349ca2896ceaffe910aeb9516f9094df1212bdea9facea688a55284eba.jpg

**Conceptual Understanding:**
This image conceptually represents various methodologies for improving machine learning model performance, particularly Small Language Models (SLMs), by addressing two critical challenges: '1. Distribution Mismatch' in data and '2. Domain Gap' between training and target environments. The main purpose is to illustrate the architectural and procedural differences between standard Knowledge Distillation (KD) techniques, on-policy vs. off-policy learning paradigms, and the integration of domain adaptation with knowledge distillation. It communicates key ideas about how large, performant models (teachers) can transfer their knowledge to smaller, more efficient models (students), and how models can be effectively retrained or fine-tuned for new data domains.

**Content Interpretation:**
The image systematically illustrates different methodologies within knowledge distillation (KD) and domain adaptation, particularly focusing on how data quality and domain discrepancies are handled. 

1.  **Distribution Mismatch** section details various KD training policies:
    *   **(a) Original KD** shows the standard setup where both student and teacher models learn from the same dataset, and their outputs are directly compared for distillation. The text `Dataset`, `Student q`, `Teacher p`, `D(q,p)`, and the label `(x, y)` on the arrow from `Dataset` to `Teacher p` and `Student q` clearly indicate this direct learning and comparison.
    *   **(b) On-policy Approach** emphasizes the student's role in guiding the teacher. The text `Student q` feeding `y'` to `Teacher p` highlights that the teacher's learning or distillation input is influenced by the student's current policy or outputs. The label `x` from `Dataset` to `Teacher p` and `Student q` signifies the input, while `y'` is the student's output used by the teacher.
    *   **(c) Off-policy Approach** introduces a `Replay Buffer` to store and reuse past experiences. The dashed green arrows from `Dataset` to `Student q` and `Teacher p`, and `Teacher p` generating `(x, y')` which goes to `Replay Buffer` and then from `Replay Buffer` to `Student q`, explicitly show the mechanism for leveraging past data. This aligns with the definition of off-policy learning provided in the document context.

2.  **Domain Gap** section explores how models trained on one domain can be adapted or distilled for another domain:
    *   **(a) Domain Adaptation** depicts a straightforward transfer: a `Model in wiki domain` is adapted to a `Model in Pubmed domain` using `Pubmed` data, as indicated by the arrow label `Pubmed`.
    *   **(b) Knowledge Distillation** illustrates the standard size reduction from a `Large model` to a `Small model` through the `Distill` process.
    *   **(c) Adapt-and-Distill** combines both concepts. It shows a `Large, wiki model` being adapted to a `Large, pubmed model` (via `Pubmed` data) and also being distilled into a `Small, wiki model`. Furthermore, the `Large, pubmed model` is distilled into a `Small, pubmed model`, and the `Small, wiki model` is adapted to a `Small, pubmed model` (via `Pubmed` data). This demonstrates complex scenarios where models might undergo both domain adaptation and size reduction, potentially in sequence or parallel, to achieve a `Small, pubmed model` from different starting points.

**Key Insights:**
The image provides several key takeaways regarding knowledge distillation and domain adaptation:

1.  **Three Primary KD Policy Approaches Exist for Distribution Mismatch:** The '1. Distribution Mismatch' section clearly demonstrates `(a) Original KD`, `(b) On-policy Approach`, and `(c) Off-policy Approach`. Each approach varies in how the 'Dataset', 'Student q', and 'Teacher p' interact and exchange information (`(x, y)`, `x`, `y'`, `(x, y')`) and how the distillation loss `D(q, p)` is computed. The `Replay Buffer` in the off-policy approach is a critical component for leveraging previously gathered data, distinguishing it from on-policy learning where only current student data is used.

2.  **Domain Adaptation Involves Transferring Knowledge Across Domains:** The '2. Domain Gap' section's `(a) Domain Adaptation` explicitly shows a `Model in wiki domain` being transformed into a `Model in Pubmed domain` using `Pubmed` data, highlighting the process of adapting models to new data distributions or contexts.

3.  **Knowledge Distillation Reduces Model Size while Retaining Knowledge:** The `(b) Knowledge Distillation` sub-section concisely illustrates the core concept of `Distill`ing a `Large model` into a `Small model`, emphasizing model compression while attempting to preserve performance.

4.  **Adapt-and-Distill Combines Both Strategies for Complex Scenarios:** The `(c) Adapt-and-Distill` flow is the most comprehensive, showing how domain adaptation (`Pubmed` labeled arrows) and knowledge distillation (`Large, wiki model` to `Small, wiki model`, and `Large, pubmed model` to `Small, pubmed model`) can be combined and applied to different model sizes (`Large` vs `Small`) and domains (`wiki` vs `pubmed`). This indicates that achieving high-performing small models in specific domains often requires a multi-step approach involving both domain transfer and knowledge transfer/compression. The various paths leading to a `Small, pubmed model` (e.g., from `Large, wiki model` via `Large, pubmed model`, or directly from `Small, wiki model`) underscore the flexibility and complexity of these combined techniques.

**Document Context:**
This image directly supports Section 3.3, 'Data Quality in Knowledge Distillation (KD),' by visually detailing different techniques for addressing challenges related to data distribution and domain differences in the context of KD. The first major section, '1. Distribution Mismatch,' explains how different policy approaches (Original KD, On-policy, Off-policy) handle data flow and interaction between student and teacher models, directly linking to data utilization strategies in KD. The second major section, '2. Domain Gap,' illustrates methods for adapting models across different domains (e.g., 'wiki' to 'Pubmed') and how knowledge distillation ('Distill') can be integrated with these adaptation processes. The image's sub-figure (c) 'Off-policy Approach' directly aligns with the text provided after the image, which clarifies 'on-policy' versus 'off-policy' learning, enhancing the reader's understanding of these specific KD strategies. The 'Adapt-and-Distill' approach shown in the second part (c) provides a concrete example of how the concepts of domain adaptation and knowledge distillation can be combined, which is crucial for enhancing SLM (Small Language Model) performance as mentioned in the document context.

**Summary:**
The image illustrates various techniques in knowledge distillation (KD) and domain adaptation, structured into two main categories: '1. Distribution Mismatch' and '2. Domain Gap.' Each category presents different approaches to address these challenges. 

Under '1. Distribution Mismatch,' three sub-approaches are detailed:
- **(a) Original KD:** This foundational method involves a 'Dataset' feeding data (x, y) to both a 'Student q' model and a 'Teacher p' model. The outputs from 'Student q' and 'Teacher p' are then compared in 'D(q, p)', which represents a distillation process.
- **(b) On-policy Approach:** In this method, the 'Dataset' provides input 'x' to 'Student q' and 'Teacher p'. Additionally, 'Student q' provides an output 'y'' to 'Teacher p', which then uses this information. The outputs from 'Student q' and 'Teacher p' are finally compared in 'D(q, p)'. This approach focuses on learning from the current student's data.
- **(c) Off-policy Approach:** Here, the 'Dataset' (indicated by dashed green arrows) feeds data to both 'Student q' and 'Teacher p'. The 'Teacher p' model produces outputs (x, y'), which are then stored in a 'Replay Buffer'. The 'Replay Buffer' then feeds this data (x, y') back to 'Student q'. The outputs of 'Student q' and 'Teacher p' are compared in 'D(q, p)'. This method allows the use of previously gathered data.

Under '2. Domain Gap,' three distinct scenarios or steps are presented:
- **(a) Domain Adaptation:** This shows a 'Model in wiki domain' being adapted to a 'Model in Pubmed domain' using 'Pubmed' data.
- **(b) Knowledge Distillation:** A 'Large model' is distilled into a 'Small model' with the process labeled 'Distill'.
- **(c) Adapt-and-Distill (as identified in the document context):** This more complex flow begins with a 'Large, wiki model'. This model can either be adapted to a 'Large, pubmed model' using 'Pubmed' data, or it can be distilled directly into a 'Small, wiki model'. Separately, the 'Large, pubmed model' is distilled into a 'Small, pubmed model'. Furthermore, the 'Small, wiki model' can also be adapted to a 'Small, pubmed model' using 'Pubmed' data. This section demonstrates combining domain adaptation and knowledge distillation for different model sizes and domains.](images/5ef0bc349ca2896ceaffe910aeb9516f9094df1212bdea9facea688a55284eba.jpg)
(c) Adapt-and-Distill Fig. 13. Distillation Techniques for Enhancing SLM Performance. On-policy means learning only use data from the current student (policy), while off-policy permits the use of previously gathered data.

# 3.4 Distillation Techniques for Enhancing SLM Performance

Following the discussion on data quality in KD, this section reviews specialized KD training strategies designed to enhance the performance of SLMs. The motivation is to address the unique challenges and constraints involved in distilling knowledge from LLMs to SLMs, ensuring that the smaller models can maximize their performance gains. As shown in Figure 13, two main gaps between LLMs and SLMs lead to challenges in distillation: distribution mismatch and domain gap. Distribution mismatch [6, 173] occurs when the distribution of output sequences during training does not align with the distribution of sequences that SLMs produce during inference, leading to suboptimal performance of the student model. The domain gap [414] arises when there is a discrepancy between the domains or tasks on which the LLMs and SLMs are trained and applied. This gap can cause significant degradation in the performance of the student model if not properly addressed during the distillation process. To address these issues, specialized strategies involve first aligning the teacher and student models with the target domain before proceeding with knowledge distillation. To explore these challenges further, we now delve into the details of these two branches of methods.

Distribution Mismatch In original knowledge distillation, illustrated in Figure 13 Distribution Mismatch (a), the teacher and student are provided with the same input sequences $x$ and output labels $y$ , producing probability distributions for the next token $\cdot q$ and $p$ ). The loss is calculated as the difference between these two distributions, $D ( q , p )$ . However, a key challenge arises due to distribution mismatch: the output sequences during training $( y )$ differ in distribution from those the SLMs produce during inference $( y ^ { \prime } )$ . To address this challenge, various techniques have been proposed. As shown in Figure 13 Distribution Mismatch (b), one approach trains the student model using on-policy sequences—sequences generated by the student itself—guided by the teacher model’s feedback. Specifically, both the student and teacher take the same input $( x )$ and the student-generated output $( y ^ { \prime } )$ , producing probability distributions Manuscript submitted to ACM

for the next token ( $\dot { \boldsymbol { g } }$ and $\mathcal { P }$ , respectively). The loss is calculated as the difference between these two distributions, $D ( q , p )$ . This approach helps the student model reduce the distribution gap between training and inference by learning from the teacher’s feedback on its own generated sequences. Generalized Knowledge Distillation (GKD) [6] is the first work using this technique and improves distillation outcomes. However, a drawback of this technique is that it requires the student to constantly produce new training sequences, which can be computationally expensive. To improve efficiency, as shown in Figure 13 Distribution Mismatch (c), an adaptive off-policy approach can be used to efficiently manage student-generated outputs by storing them in a replay buffer, thereby reducing computational costs. DistiLLM [173] employs this off-policy approach and improves the efficiency of KD.

Domain Gap When training an SLM in a specific domain that differs from the domain of the LLMs, the gap between the two domains becomes problematic. As illustrated in Figure 13 Domain Gap (a), domain adaptation fine-tunes a language model, initially trained on a general corpus, using a specialized dataset such as PubMed to enhance performance in that specific domain. As illustrated in Figure 13 Domain Gap (b), Knowledge distillation transfers knowledge from the larger model to the smaller one. However, because the teacher model may not produce high-quality outputs on specialized datasets, domain adaptation is needed prior to knowledge distillation. As illustrated in Figure 13 Domain Gap (c), Adapt-and-Distill [414] tackles the domain gap by distilling general large models into smaller ones. This paper introduces AdaLM and demonstrates that the “Adapt-and-Distill” strategy—first involving domain adaptation of both the large teacher model and the small student model, followed by distillation—is the most effective compared to three other strategies: training directly from scratch, distillation followed by adaptation, and adapting the teacher model before distillation into a general small student model. These innovative techniques are crucial for enhancing the capabilities of SLMs, making them more efficient and effective for various applications. However, adapting both the teacher (LLMs) and the student (SLMs) models to the target domain can be time-consuming. Future research could focus on efficiently solving the domain gap problem.

Insights: Here are some insights from distillation techniques:

• Sampling SLM outputs during the training process is the main approach to resolving distribution mismatch. • Techniques like Adapt-and-Distill address the domain gap by first adapting both the teacher (LLMs) and the student (SLMs) models to the target domain before proceeding with distillation.

# 3.5 Performance Improvement through Quantization

As mentioned in Section 2, quantization is one of the most effective methods for adapting LLMs to SLMs. However, compression to smaller sizes often compromises performance. To address the performance drop associated with quantization, various methods have been proposed. This section examines how these quantization methods specifically enhance the performance of SLMs. While the general introduction to compression methods is discussed in the compression section, the focus here is on detailing those approaches that boost the efficiency and effectiveness of SLMs. As shown in Figure 9, we categorize these quantization methods into two main approaches: Post-Training Quantization (PTQ), where quantization is conducted on a well-trained fixed model, and Quantization-Aware Training (QAT), where quantization is integrated into the training process. This section introduces advanced techniques in PTQ and QAT respectively.

Post-Training Quantization (PTQ) primarily includes weight quantization and activation quantization. Weight quantization aims to quantize model parameters while preserving performance. GPTQ [103] compresses LLMs to 4-bit or 2-bit by quantizing weights layer-by-layer to minimize layer-wise quantization errors. PB-LLM [299], applicable to both Manuscript submitted to ACM

PTQ and QAT, retains the most salient weights while binarizing the rest based on magnitudes. BiLLM [147], another PTQ method, uses a Hessian-based metric to identify salient and non-salient weights. Salient weights undergo binary residual approximation to minimize loss, while non-salient weights are divided into sparse and concentrated groups for separate binarization, reducing quantization errors. Activation quantization faces challenges with outliers that can stretch the quantization range, causing most values to cluster at few bits and introducing significant errors. To address this, LLM.int8() [80] isolates outlier features for 16-bit processing and handles the rest in 8-bit. SmoothQuant [386] circumvents per-channel quantization issues by employing a "smoothing" technique that shifts the quantization challenge from activations to weights through a per-channel scaling transformation. This balance between activating and weight quantization allows effective 8-bit quantization (W8A8), preserving accuracy while significantly reducing memory and computational costs. SmoothQuant thus enhances the efficiency of SLMs in resource-constrained environments.

Quantization-Aware Training (QAT) differs from PTQ in that it includes a training phase after the model has been quantized. When models are quantized to extremes, such as 2-bit or 1-bit, performance typically drops significantly, but further training can help the model retain its capabilities. For instance, to mitigate performance degradation from binarization, PB-LLM [299] selectively binarizes only non-salient weights, preserving the most salient ones at higher precision. This method effectively reduces the model size without significantly impacting performance. Salient weights are chosen based on their magnitude, ensuring that the most influential weights maintain higher precision to preserve the model’s reasoning capabilities. The paper explores both post-training quantization (PTQ) and quantization-aware training (QAT) to fine-tune and recover the performance of partially binarized models, achieving a balance between compression and accuracy. OneBit [400] and BitNet [357] address the severe performance degradation associated with 1-bit quantization by decomposing floating-point matrices and employing mixed-precision strategies. Specifically, OneBit introduces Sign-Value-Independent Decomposition (SVID), which decomposes a floating-point matrix into a 1-bit matrix and two floating-point vectors. This method allows LLMs to be quantized to a 1-bit level while minimizing performance loss. By retaining critical information with the floating-point vectors, OneBit effectively balances extreme compression with maintaining model accuracy. BitNet b1.58 [231] improves on the original BitNet by introducing a ternary matrix weight system -1, 0, 1, resulting in a 1.58-bit model. BitNet b1.58 matches the performance of full-precision models starting from a 3 billion parameter size while further reducing memory and latency costs. LLM-QAT [221] employs data-free knowledge distillation, where the pre-trained model itself generates data for fine-tuning the quantized model (student) using logit distillation from the full-precision model (teacher). This method incorporates quantization of weights, activations, and key-value cache, achieving accurate 4-bit quantization for weights and key-value caches, and 6-bit for activations, demonstrating substantial improvements over existing post-training quantization methods.

Insights: Insights drawn from quantization strategies include:

• Post-Training Quantization techniques primarily focus on quantizing model weights, where selecting salient weights is crucial. Beyond weight quantization, handling outliers in activation signals is a significant challenge in quantizing activations.   
• Quantization-Aware Training methods show that low-bit quantization (e.g., 1-bit models) requires additional tuning to maintain performance. Knowledge can be distilled from the model before quantization to the quantized model.

# 3.6 Techniques in LLMs Contributing to SLMs

This subsection explores the potential of advanced techniques such as RAG and MoE, which enhance LLM performance, to also maintain or boost SLM performance within constrained computational budgets. However, effectively integrating these techniques into SLMs, which inherently possess limited capabilities, remains an unresolved challenge.

Retrieval Augmented Generation (RAG) enhances the capabilities of language models in knowledge-intensive tasks by incorporating a retrieval mechanism. This approach allows models to access relevant contextual information from a data repository in response to user queries. By integrating this retrieved data, RAG-equipped models better understand specific topics, enabling more informed and accurate outputs. For SLMs, a significant concern is whether they possess the capacity for long-context reasoning. A recent study [215] compares SLMs at the 7B level with RAG to larger models such as GPT-3.5 and GPT-4, suggesting that SLMs equipped with RAG can sometimes perform comparably or even better than LLMs. These findings indicate that RAG for SLMs is effective and represents a promising direction for future research.

Mixture-of-Experts (MoE) [39] has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead in LLMs. The MoE framework is founded on a straightforward yet potent concept: distinct components of a model, referred to as “experts”, specialize in different tasks or data facets. In this paradigm, only the relevant experts are activated for a specific input, which manages computational costs while leveraging a vast pool of specialized knowledge. This scalable and adaptable approach enables increased model capacity without proportionally escalating computational demands. We argue that MoE is particularly suitable for SLM architectures [161] as it minimizes both computational load and memory overhead. However, research on MoE for SLMs remains sparse. Future studies could investigate how large LLM MoE architectures can be effectively compressed into small ones or how to develop an SLM with MoE tailored for specific devices from scratch.

# 4 APPLICATIONS OF SMALL LANGUAGE MODELS

In this section, we delve into the applications of small language models (SLMs) across various NLP tasks and their deployment strategies. Due to benefits such as enhanced privacy, faster inference, and lower memory requirements, many NLP applications are now leveraging SLMs over LLMs. Additionally, deploying SLMs often involves considerations of memory and runtime efficiency, which are crucial for optimizing resource use on budget-constrained edge devices, particularly mobile phones. Then, we will discuss task-specific applications of SLMs and their deployment methods on mobile and edge devices.

# 4.1 Task-specific SLM Applications

This subsection explores the diverse NLP tasks to which SLMs can contribute. Question-answering and coding represent generative tasks, while recommender systems and web search (though not strictly within the NLP domain) typically leverage the encoding capabilities of SLMs. Additionally, the application of SLMs on mobile devices is particularly well-suited due to constraints in memory and computing resources. The representative works are systematically organized in Table 3.

4.1.1 SLM Applications in Question-Answering. Question-answering (QA) is a fundamental task in the NLP field, demanding language models to exhibit abilities in understanding language, reasoning, common sense, and recalling specialized knowledge. Typically, larger language models yield better QA performance. However, the substantial size of these models introduces challenges such as immense computational requirements, privacy concerns when using proprietary LLMs, and difficulties in customization. These issues lead researchers and developers to favor SLMs in scenarios that demand efficiency, privacy, and customization. Therefore, we explore methods to enhance the capabilities of SLMs in QA across three key areas: (i) Instruction Tuning of Generic SLMs for QA, (ii) Instruction Tuning of Domain-Specific SLMs for QA, and (iii) Enhancing SLMs for Out-of-Domain Questions.

Table 3. Task-specific SLM Applications   

<table><tr><td>Aspect</td><td>Representativework</td><td>Key point</td></tr><tr><td rowspan="2"> SLM in QA</td><td>Alpaca [329] Stable Beluga 7B [238]</td><td>TuneLlama 7B[338] using 52k ChatGPT-generated examples.</td></tr><tr><td>Fine-tuned BioGPT[125] Financial SLMs [275] ColBERT[112] Rationale Ranking [128] T-SAS [156]</td><td>Employ explanation tuning to Llama-2 7B [339] on an Orca-style dataset. Fine-tuning BioGPT(1.6B)[230] on PubMedQA. Transfer financial knowledge from GPT-4 [2] to multiple SLMs. Fetch retrieval documents for SLMs to answer domain-specific questions. For unseen questions,combine retrieval with LLM-generated rationales.</td></tr><tr><td>SLM in Coding</td><td>Phi-3.5-mini [1] TinyLlama [439] CodeLlama [292] CodeGemma [331]</td><td>Enhance SLMs adaptability with self-generated pseudo labels. New addition to the Phi-3 series and focus on high-quality data. 1.1B Transformer model is trained on 3T corpus. A derivative of Llama 2 fine-tuned on domain-specific datasets. Fine-tuning Gemma to enhancing coding capabilities.</td></tr><tr><td>SLM in Recommen- dation</td><td>PromptRec [382] SLIM [369] BiLLP [309] ONCE [214] RecLoRA [455] Content encoder [44,152, 228]</td><td>Training on prompt templates Step-by-step Knowledge Distillation LLaMa-2-7B as planner and reflector LLaMa-2-7B as Content Encoder Personalized low-rank adaptation</td></tr><tr><td>SLM in Web Search</td><td>Ranker [63,260] Rewriter [233] Octopus [52]</td><td>Encode concatenated queries and documents. Re-rank retrieved documents using a specially SLM. Bridge the gap between queries and needed knowledge by rewriting inputs. Calling software APIs via learning in documents</td></tr><tr><td>SLM in Mobile- device</td><td>MobileAgent [87] α-UMI[307] Mobile Interaction [42] AutoDroid [376] M4 [423] Agent for Text Rewriting [458]</td><td>Standard Operating Procedure (SOP) SLMs serve as Multi-agents in tool uses. Text-to-action control and tests on 6GB and 4GB Android devices Interaction based on GUI and APP knowledge injection a foundation model handling all mobile AI tasks. Data Knowledge Distillation from LLMs</td></tr></table>

Instruction Tuning Generic SLMs for QA. Despite the Phi series’ high question-answering capability, its training cost with over 3.4T tokens on 512 H100 GPUs for 10 days [1] is prohibitive for many researchers and developers. Instruction tuning [372] offers a cost-effective alternative, enhancing small models by fine-tuning on large model outputs. Alpaca 7B [329] tunes Llama 7B [338] with 52k ChatGPT-generated examples from 175 seed tasks. This behavior cloning mimics teacher models effectively but struggles in reasoning-intensive QA tasks where accuracy is key, not style [60]. To counter it, explanation tuning [238] enhances Llama-2 7B [339] using explanatory LLM answers to improve reasoning. However, its effectiveness varies with system instructions, and those effective for larger models like GPT-4 may not suit smaller ones. SLMs also struggle to identify optimal system instructions for different tasks. Therefore, Orca 2 [247] addresses this by promoting cautious reasoning, deciding which solution strategy to choose for a given task among direct answer generation, or “Slow Thinking” strategies (step-by-step, guess and check or explain-then-answer, etc.) and erasing specific system instructions during training. This involves (1) solution strategy is guided by the performance of Orca 1 [251], (2) writing task-specific system instructions corresponding to the chosen strategy to obtain teacher responses for each task, and (3) at training time, employing Prompt Erasing to replace student’s system instructions with generic ones vacated of details of how to approach the task, encouraging students learn not just task solutions but also deeper reasoning abilities.

Table 4. Comparison of instruction-tuned domain SLMs for QA and LLMs on FinQA [56] and PubMedQA [163].   

<table><tr><td>Model</td><td>Size</td><td>Instruction tuned?</td><td>Task Name</td><td>Shot Type</td><td>Accuracy (%)</td></tr><tr><td>GPT-4 [2]</td><td>1</td><td>×</td><td>FinQA</td><td>Zero-shot</td><td>77.5</td></tr><tr><td>Phi-3-Mini [1]</td><td>2.7B</td><td>√</td><td>FinQA</td><td>Zero-shot</td><td>77.6</td></tr><tr><td>Meditron-70B[55]</td><td>70B</td><td>×</td><td>PubMedQA</td><td>Zero-shot</td><td>81.6</td></tr><tr><td>RankRAG-llama3-70B [421]</td><td>70B</td><td>×</td><td>PubMedQA</td><td>Zero-shot</td><td>79.8</td></tr><tr><td>Flan-PaLM [313]</td><td>540B</td><td>×</td><td>PubMedQA</td><td>Few-shot</td><td>79.0</td></tr><tr><td>GAL 120B [330]</td><td>120B</td><td>×</td><td>PubMedQA</td><td>Zero-shot</td><td>77.6</td></tr><tr><td>Flan-PaLM[313]</td><td>62B</td><td>×</td><td>PubMedQA</td><td>Few-shot</td><td>77.2</td></tr><tr><td>BioGPT[230]</td><td>345M</td><td>√</td><td>PubMedQA</td><td>Zero-shot</td><td>78.2</td></tr><tr><td>BioGPT-Large [230]</td><td>1.5B</td><td>√</td><td>PubMedQA</td><td>Zero-shot</td><td>81.0</td></tr></table>

Instruction Tuning Domain SLMs for QA. Beyond instruction tuning for generic SLMs, tuning domain-specific SLMs is also crucial, as they provide specialized assistance where generic SLMs may underperform. Instruction-tuning generic SLMs can derive domain SLMs. We summarize some representatives in several domains. (1) In finance, Phogat et al. [275] transfer financial QA abilities from teacher LLMs such as GPT-4 [2] to specialized SLMs such as Phi-3-Mini [1], using datasets such as FinQA [56], ConvFinQA [57], and TATQA [454]. They train SLMs with Python programs created by the teacher model, which detail steps for financial reasoning, including concept comprehension, formula identification, entity extraction, and calculations. During inference, SLMs generate Python code that an external interpreter executes. (2) In the medical field, Guo et al. [125] enhance student SLMs, including domain-specific BioGPT (1.6B) [230] and general Llama 7B [338], by fine-tuning on enriched PubMedQA [163] data. This enhancement is achieved by generating new samples or rewriting existing ones using teacher LLMs, which include the highly knowledgeable GPT-4 and the relatively weaker ChatGPT. The best SLM, with under 1.6 billion parameters, achieves $7 5 . 4 \%$ accuracy, surpassing GPT-4’s $7 4 . 4 \%$ in few-shot settings on the PubmedQA test sets. It demonstrates that LLMs effectively refine and diversify question-answer pairs, leading to enhanced performance in a significantly smaller model after fine-tuning. We report the detailed results of comparisons of instruction-tuned domain-specific language models for QA and larger language models on FinQA [56] and PubMedQA [163], as shown in Table 4.

Enhancing SLMs for Out-of-Domain Questions. One of the major advantages of LLMs is their strong comprehension and logical reasoning abilities, which SLMs often struggle to match due to their limited parameters, especially when handling unseen or out-of-domain questions. Various methods have been developed to address this limitation, including Retrieval-Augmented Generation (RAG) and self-adaptive techniques.

(1) Retrieval-Augmented Generation (RAG): Incorporating External Knowledge for Domain-Specific QA. RAG addresses OOD questions by integrating external knowledge during inference, allowing models to access information beyond their pre-trained parameters. By retrieving relevant documents in real time, RAG enables small language models to provide accurate answers on specialized topics. In the telecommunications domain, Gichamba et al. [112] use ColBERT as a dense retrieval system to fetch documents from technical datasets. By encoding queries and documents separately, ColBERT computes relevance scores, helping small models like Phi-2 and Falcon-7B retrieve precise technical information to answer complex telecom-related queries. Rationale Ranking [128] addresses answering unseen questions using smaller language models by integrating external explanatory contexts from retrieval systems with reasoning rationales from LLMs. This method involves ranking both the retrieved explanatory contexts and LLM-generated rationales using a scoring module, which then combines them to form a cohesive context. Consequently, this integrated approach enhances the SLMs’ performance on unseen questions.

(2) Self-Adaptive Techniques: Enhancing Model Adaptability with Self-Generated Pseudo Labels. Fine-tuning, while effective in adapting domain knowledge, can be impractical in realistic scenarios where labeled datasets are scarce. To overcome this, self-adaptive techniques employ self-generated pseudo labels to activate specific aspects of the target tasks, thereby enhancing model adaptability [312, 347]. Test-time Self-Adaptive Small LMs (T-SAS) [156] first stochastically generates multiple answers for an unlabeled question. The most plausible answer is then selected via majority voting to enhance pseudo-label accuracy, serving as a pseudo-label for training during test-time.

Comparison between LLMs and SLMs for QA. When comparing LLMs such as GPT-4 [2] or BLOOM-175B [181] with fine-tuned SLMs in QA tasks, the benefits of SLMs are clear. LLMs, while versatile across multiple domains due to extensive pre-training, are computationally demanding, making them less ideal for resource-limited settings. SLMs, however, when fine-tuned for specific domains, often match or exceed the performance of larger models within those specialties. The trade-off is between large-scale models’ generalization and small-scale model’s specialization: LLMs handle diverse domains but may need additional techniques such as knowledge injection for domain-specific queries. In contrast, domain-specific SLMs, though less flexible, provide higher accuracy and more relevant responses, making them ideal for edge deployments where computational resources are scarce but domain precision is crucial.

4.1.2 SLM Applications in Coding. The adoption of SLMs for coding offers an alternative to LLMs due to their lower computational needs and potential for domain-specific tuning. Despite LLMs’ proficiency in code generation and programming support, SLMs are advantageous for their faster inference, reduced operational costs, and suitability for real-time environments where rapid responses are crucial. Representative works are discussed next. The Phi series [1, 155, 200] showcase SLMs’ evolution in coding tasks. For instance, Phi-1 [120], a Transformer with 1.3B parameters, specializes in basic Python coding and achieves notable scores in benchmarks such as HumanEval [120], which includes 164 programming problems. Subsequent models, Phi-1.5 and Phi-2, have enhanced these capabilities, while Phi-3 demonstrated SLMs’ potential to rival larger models [1]. The latest model, Phi-3.5-mini, with 3.8B parameters, excels in long context tasks using advanced fine-tuning and optimization techniques, performing comparably to larger models such as Llama-3.1-8B-instruct [94] and surpassing smaller ones like Gemma-2 [333].

Another avenue of development is the fine-tuning of general-purpose SLMs for coding tasks [23, 121, 227, 292, 331]. For instance, CodeLlama models [292], derivatives of Llama 2 [339], undergo a rigorous fine-tuning process on domain-specific datasets, enhancing their proficiency in specific programming languages such as Python. They are trained to handle tasks such as syntax error detection, code suggestion, and infilling, where they learn to predict and complete missing parts of the code. This specialized fine-tuning improves their ability to interpret and execute detailed programming instructions, making them highly effective in real-time code editing environments [292]. CodeGemma models [331], stemming from Google DeepMind’s Gemma framework, also exhibit a focused approach to enhancing coding capabilities through fine-tuning. These models are specifically engineered for high-performance code generation and infilling, underpinned by extensive training on a vast corpus of over 500 billion to 1 trillion tokens, predominantly consisting of code. This comprehensive dataset enables CodeGemma models to excel in mathematical reasoning and complex problem-solving within code contexts, setting new benchmarks in latency-sensitive applications such as real-time IDE support and automated code reviews [331].

Comparison between SLMs and LLMs on Coding. Table 5 provides a comparative analysis of SLMs and LLMs on coding benchmarks HumanEval [50] and MBPP [20]. Insights include: (i) Small SLMs (1.3B - 3.8B Parameters) like Phi-3.5-mini [342] achieve high scores, demonstrating the efficacy of small models. Mid-sized SLMs (6.7B - 9B Parameters), such as DeepSeekCoder 6.7B [121] and Llama 3.1 8B [94], show improved performance, indicating that larger model sizes and enhanced training contribute to better accuracy. Large models (33B and above) like Llama 3.1 405B [94], GPT-4o [263], and Claude 3.5 Sonnet [15] excel, supporting the idea that bigger models generalize better across diverse coding tasks; (ii) There’s a notable trade-off between computational efficiency and performance, with larger models requiring more resources, impacting their practical deployment in constrained environments; (iii) Specialized training and fine-tuning, as used in models like DeepSeek-Coder [121], are crucial for excelling in coding tasks, though such models may not handle complex requests as effectively, highlighting the versatility of general SLMs for broader applications.

Table 5. Performance comparison between SLMs and LLMs in coding benchmarks. All models listed are chat or instruct versions, and performance are sourced from respective research papers or technical reports [94, 121, 292, 331, 342].   

<table><tr><td>Model</td><td>Size</td><td>HumanEval</td><td>MBPP</td></tr><tr><td>DeepSeek-Coder [121]</td><td>1.3B</td><td>65.2</td><td>49.4</td></tr><tr><td>CodeGemma [331]</td><td>2B</td><td>37.8</td><td>49.2</td></tr><tr><td>Gemma 2 [333]</td><td>2B</td><td>17.7</td><td>40.2</td></tr><tr><td>Phi-3.5-mini [342]</td><td>3.8B</td><td>62.8</td><td>69.6</td></tr><tr><td>DeepSeek-Coder [121]</td><td>6.7B</td><td>78.6</td><td>65.4</td></tr><tr><td>CodeGemma [331]</td><td>7B</td><td>60.4</td><td>55.2</td></tr><tr><td>Llama 3.1 [94]</td><td>8B</td><td>66.5</td><td>69.4</td></tr><tr><td>Gemma 2 [333]</td><td>9B</td><td>61.0</td><td>69.3</td></tr><tr><td>GPT-3.5 Turbo</td><td>-</td><td>68.0</td><td>71.2</td></tr><tr><td>DeepSeek-Coder [121]</td><td>33B</td><td>79.3</td><td>70.0</td></tr><tr><td>Llama 3.1 [94]</td><td>70B</td><td>80.5</td><td>75.4</td></tr><tr><td>Llama 3.1 [94]</td><td>405B</td><td>89.0</td><td>78.8</td></tr><tr><td>GPT-4o OpenAI [263]</td><td></td><td>90.2</td><td>81.4</td></tr><tr><td>Claude 3.5 Sonnet [15]</td><td>-</td><td>92.0</td><td>76.6</td></tr></table>

4.1.3 SLM Applications in Recommender Systems. Recommender systems are essential in various online services, helping to manage information overload and meet users’ personal needs. SLMs enhance recommendation systems by (1) addressing the cold start problem; (2) reducing popularity bias; (3) improving long-term planning; (4) serving as personalized recommenders; and (5) acting as content encoders. These applications show the versatility and effectiveness of SLMs in boosting performance and personalization in recommendation. Next, we introduce the details.

SLM for System Cold Start Problem. Traditional recommendation systems, which utilize historical user-item interactions such as clicks, purchases, and ratings to learn representations and match items to users, fail in scenarios lacking any user-item interactions, known as the cold-start recommendation problem, often occurring in start-up businesses [289]. Although LLMs address this with in-context learning, their slow and costly inference restricts real-time use. Thus, PromptRec [382] explores using SLMs as in-context recommenders for recommendation system cold-start problems. However, SLMs often struggle without emergent context-learning abilities. To overcome this, SLMs are enhanced by pre-training on relevant corpora, using a improved C4 corpus subset [284], and by developing training prompts for different domains, enhancing cold-start performance. Results show that enhanced SLMs like BERT-mini [83], with 11.3M parameters, achieve BERT-large’s performance in cold-start scenarios, with only $1 7 \%$ of BERT-large’s inference time. Similarly, many studies have addressed the cold-start problem by leveraging BERT [133, 261, 441, 459]. For example, ADLRS [133] employs BERT to convert web-crawled item profiles into vectors that highlight key aspects, aiding recommender systems in acquiring essential initial information.

SLM for Mitigating Popularity Bias. Popularity bias in recommender systems, marked by discrepancies between item popularity in training datasets and the real world, often stems from using closed-loop datasets with limited information. Recent LLMs leverage their broad open-world knowledge to better reason about user-item interactions [206, 214], reducing this bias by providing recommenders with more extensive item details. Using the chain-of-thought (CoT) prompting, LLMs decompose complex tasks into intermediate reasoning steps, enhancing understanding of user behavior and interests. However, LLMs’ high resource demands limit their practical use. To overcome this, the

Step-by-step Knowledge Distillation Framework for Recommendation (SLIM) [369] distills LLM reasoning capabilities into SLMs, keeping just $4 \%$ of the original parameters, transitioning from ChatGPT to Llama 7B [338]. SLIM uses detailed LLM templates to extract reasoning steps and streamlined templates for fine-tuning, enabling SLMs to improve recommender systems by better reasoning on richer item information.

SLM for Long-term Planning. Traditional recommender systems focus on optimizing immediate user responses, often maximizing short-term gains but overlooking long-term engagement. This can trap users in echo chambers and filter bubbles [107, 368]. To tackle this, integrating planning capabilities into recommendations to balance immediate and long-term outcomes is vital. LMs, with their extensive knowledge and reasoning abilities, are expected to enhance planning capabilities. BiLLP [309] adopts a hierarchical learning approach with macro and micro-learning phases. In macro-learning, a Planner and a Reflector, both as SLM instances like Llama-2-7B [339], operate; the Planner forms long-term plans using high-level experiences, while the Reflector updates plans based on past actions. Micro-learning uses an SLM-based Actor-Critic mechanism for personalized planning, with the Actor implementing plans and the Critic assessing actions for long-term benefits. The use of SLMs for long-term planning, similar to their use in cold-start scenarios, remains underexplored and merits further research.

SLMs as a Personalized Recommender. Generative language model-based recommender systems require integrating user knowledge, typically achieved through fine-tuning. Fine-tuning techniques like LoRA [142] can incorporate extensive knowledge across all users by training an external module with a small number of parameters A and B, but this approach often overlooks individual user preferences. To address this, RecLoRA [455] utilizes Vicuna-7B [61] to integrate personalized knowledge into SLMs/LLMs tailored for recommendation tasks, as illustrated in Figure 14. Specifically, RecLoRA maintains a set of parallel, independent LoRA weights $( \mathbf { A } _ { i } , \mathbf { B } _ { i } )$ , allowing for the customization of language model parameters to match individual user preferences more effectively.

![## Image Analysis: 3b43c2adad6b79f991c200f73540556e2caeebf4d3fa4388bf6043d9b05842b2.jpg

**Conceptual Understanding:**
The image conceptually represents a framework for leveraging Large Language Models (LLMs) to build recommendation systems. Its main purpose is to demonstrate two distinct approaches to finetuning these LLMs for recommendation: a non-personalized method and a personalized method using Low-Rank Adaptation (LoRA). The core idea conveyed is that by analyzing a user's "Lifelong Behavior Sequence" (purchase history, for example), LLMs can be adapted to provide recommendations, with personalization via LoRA offering a more refined and specific user experience.

**Content Interpretation:**
The image illustrates how Large Language Models (LLMs) can be adapted for recommendation tasks, specifically highlighting the distinction between non-personalized and personalized Low-Rank Adaptation (LoRA) finetuning. The lifelong behavior sequence represents the historical data or preferences of users, which serves as input for the LLMs. In the non-personalized approach, a single LoRA module (A and B) is applied universally to all users, implying a generalized finetuning. Conversely, the personalized approach utilizes multiple distinct LoRA modules (A₁, B₁ through Aₖ, Bₖ), suggesting that finetuning is tailored to individual users or specific user groups. The inclusion of specific LLM names (ChatGPT, Claude 3, Gemini, Meta AI) indicates the practical application of prominent pre-trained models. The '...' in both user groups and personalized LoRA modules signifies scalability and application to a larger number of users or personalized modules.

**Key Insights:**
The main takeaway is that Large Language Models (LLMs) can be effectively adapted for recommendation tasks using LoRA finetuning. A critical insight is the advantage of personalization: instead of a single, generic LoRA module, creating multiple personalized LoRA modules (A₁, B₁ through Aₖ, Bₖ) allows for more tailored and potentially more accurate recommendations by considering individual user behaviors. The image also highlights the use of well-known LLMs (ChatGPT, Claude 3, Gemini, Meta AI) in such applications. The concept of a 'Lifelong Behavior Sequence' emphasizes the importance of historical user data as input for these advanced recommendation systems.

**Document Context:**
This image directly supports the document's section '4.1 Task-specific SLM Applications' by illustrating a concrete example of how language models, particularly Large Language Models (LLMs) and their finetuning techniques like LoRA, can be applied to recommendation systems. It provides a visual explanation of two different strategies (non-personalized vs. personalized) for adapting LLMs to user behavior, which is crucial for understanding how these models can generate relevant recommendations.

**Summary:**
This illustration depicts two approaches to fine-tuning Large Language Models (LLMs) for recommendation systems based on a user's lifelong behavior sequence. The top section, "(a) Lifelong Behavior Sequence," shows a single user who "has bought..." a series of items: a smartphone, a camera, a desktop monitor, a green sweater, a birthday cake, a pink dress, and a wrapped gift box. This sequence represents the historical purchase data of a user.

The image then diverges into two distinct LoRA (Low-Rank Adaptation) finetuning methods. The left side, "(b) Non-personalized LoRA Finetune," illustrates multiple users (represented by diverse avatars) whose data feeds into a single set of "Large Language Models." The LLMs mentioned are ChatGPT, Claude 3, Gemini, and Meta AI. From these LLMs, the process proceeds to a non-personalized LoRA finetuning module, represented by two stacked trapezoids: the top one labeled "B" and the bottom one labeled "A". This signifies a single, shared LoRA module applied to all users.

The right side, "(c) Personalized LoRA Finetune," also shows multiple users (represented by different diverse avatars, including one with a '57' shirt and one in a hijab) whose data similarly feeds into the same set of "Large Language Models" (ChatGPT, Claude 3, Gemini, Meta AI). However, unlike the non-personalized approach, the output from these LLMs branches out to multiple distinct LoRA finetuning modules. These are represented by a series of stacked trapezoids: (B₁, A₁), (B₂, A₂), ..., up to (Bₖ, Aₖ). This indicates that each user, or a segment of users, receives a personalized LoRA module for finetuning. The connections are clearly drawn with arrows indicating the flow from users to LLMs, and then from LLMs to the LoRA modules.](images/3b43c2adad6b79f991c200f73540556e2caeebf4d3fa4388bf6043d9b05842b2.jpg)
Fig. 14. The illustration of lifelong behavior sequence and personalized low-rank adaption (LoRA) for recommendation [455].

SLM as a Content Encoder. Language models, particularly when deep, provide an effective starting point for finetuning on downstream tasks. In news recommendation systems, the representational capability of a model significantly impacts performance. Consequently, many news recommender systems now employ language models fine-tuned on specific datasets as text encoders. For example, Wu et al. [379] conducts pioneering work using a pre-trained language model to enhance large-scale news recommender systems by substituting traditional news encoders with a BERT model [83]. However, BERT may struggle to capture content as it is pre-trained on limited data. Therefore, ONCE [214] propose using Llama-2-7B [339] as an encoder to overcome the limitations of BERT in content-based recommendations. Additionally, the study explores the synergistic use of LLMs in recommendation systems, finding that SLMs optimized with LoRA [142] outperform the recommendation results of systems assisted by generic LLMs such as ChatGPT.

4.1.4 SLM Applications in Web Search. Web search systems, involving retrieval and ranking, face challenges due to the diverse web documents and search queries. Traditional keyword-matching methods often fall short because of phrasing variations and the long-tail distribution of queries and content, complicating accurate semantic inference. Effective integration of retrieval and ranking models is also crucial. Language models, serving as content encoders, help overcome semantic challenges through their language understanding from pre-training [63, 89, 359]. Joint training of Manuscript submitted to ACM

![## Image Analysis: 55caed2c4f7f78a5d857a38a853bc5c203ddb2ed2f0ce8cc74b86cc6c50f991c.jpg

**Conceptual Understanding:**
This image represents a conceptual overview of three distinct roles that Small Language Models (SLMs) play within web search applications. The main purpose of the image is to illustrate the specific functionalities and integration points of SLMs as: (a) content encoders for relevance scoring, (b) rankers for refining search results, and (c) rewriters for optimizing queries for large language models. The key idea being communicated is the versatility and targeted application of SLMs to enhance different stages of the web search process, from initial content analysis to final result generation.

**Content Interpretation:**
The image, titled 'Fig. 15. Roles of SLM in Web Search,' illustrates three distinct applications of Small Language Models (SLM) within web search systems. Each sub-diagram (a, b, c) represents a different functional role and processing flow for SLMs.

**(a) SLM as a Content Encoder:** This sub-diagram shows a system where an SLM is used to encode the semantic relationship between a search 'Query' and a 'Passage.' The 'Passage' and 'Query' are combined (indicated by the '+' symbol), processed by the 'SLM as a Content Encoder' to extract salient 'Features,' which are then fed into a 'Scoring head' to produce a 'Relevance score.' This system is designed to determine how well a passage matches a query by converting their content into a numerical representation.

**(b) SLM as a Ranker:** This sub-diagram illustrates the SLM's role in refining search results. An initial 'Query' is used by a 'Retriever' to search an 'index' of 'Passages' and retrieve 'Top-k passages.' The 'SLM as a Ranker' then takes these 'Top-k passages' as input and re-orders them to produce 'Ranked Top-k passages.' This process enhances the quality of search results by providing a more precise ordering of retrieved documents based on relevance.

**(c) SLM as a Rewriter:** This sub-diagram demonstrates how an SLM can optimize a query before it is used for information retrieval and subsequent generation by a Large Language Model (LLM). An initial 'Query to LLM' is first processed by the 'SLM as a Rewriter' to produce a 'Modified Query.' This 'Modified Query' is then used by a 'Retriever' to obtain 'Top-k passages,' which are finally passed to an 'LLM' for 'Generation.' This setup aims to improve the quality or specificity of the generated output by ensuring the LLM receives an optimized query and relevant passages.

**Key Insights:**
The image effectively teaches several key lessons about the application of SLMs in web search:

1.  **Multi-faceted Roles of SLMs:** SLMs are not limited to a single function but can perform diverse roles in a web search system, specifically 'Content Encoder,' 'Ranker,' and 'Rewriter.'
2.  **Content Encoding for Relevance Scoring:** SLMs can be used to combine 'Passage' and 'Query' inputs, extract 'Features,' and ultimately determine a 'Relevance score,' providing a foundational step for understanding the relatedness of text.
3.  **Enhanced Ranking of Search Results:** SLMs can refine the output of a 'Retriever' by re-ranking 'Top-k passages' into 'Ranked Top-k passages,' thereby improving the precision and user experience of search results.
4.  **Query Optimization for LLM Generation:** SLMs can act as 'Rewriters' to transform an initial 'Query to LLM' into a 'Modified Query,' which then leads to more relevant 'Top-k passages' retrieved by a 'Retriever' for 'Generation' by an 'LLM.' This highlights SLMs' utility in improving the inputs for larger generative models.
5.  **Modular System Design:** The diagrams show a modular approach where SLMs integrate with other components like 'Scoring head,' 'Retriever,' and 'LLM,' indicating that SLMs can be specialized tools within a larger, interconnected system.

These insights are directly supported by the verbatim text extracted from each block and arrow label, which explicitly defines the inputs, outputs, processes, and the specific role of the SLM at each stage.

**Document Context:**
The image 'Fig. 15. Roles of SLM in Web Search' directly supports Section 4.1 'Task-specific SLM Applications' by providing concrete, visual examples of how Small Language Models (SLMs) can be integrated into different stages of a web search pipeline. It visually breaks down complex interactions into three understandable functional roles: content encoding, ranking, and query rewriting. This allows the document to elaborate on the versatility and specific contributions of SLMs beyond just their language understanding capabilities, detailing their practical implementation in improving search relevance and generation quality.

**Summary:**
This image, titled 'Fig. 15. Roles of SLM in Web Search,' comprises three distinct process flow diagrams illustrating different applications of Small Language Models (SLM) in web search contexts. Each diagram details a specific role: (a) SLM as a Content Encoder, (b) SLM as a Ranker, and (c) SLM as a Rewriter. The explanation below breaks down each role, its purpose, and its step-by-step process, incorporating all textual elements from the diagrams.

**(a) SLM as a Content Encoder:**
This diagram shows how an SLM processes content to generate a relevance score. The process begins with two inputs: a 'Passage' and a 'Query.' These two inputs are combined (indicated by the circle with a cross, typically representing concatenation or element-wise addition) and fed into the 'SLM as a Content Encoder.' The SLM then processes this combined input to extract 'Features.' These features are subsequently passed to a 'Scoring head,' which utilizes them to produce a 'Relevance score.' The ultimate goal is to quantify how relevant a given passage is to a query by encoding their combined content into features.

**(b) SLM as a Ranker:**
This diagram illustrates the SLM's role in ranking search results. The process starts with a collection of 'Passages,' which are fed into a 'Retriever' along with a 'Query.' The arrow between 'Passages' and 'Retriever' is labeled 'index,' suggesting the Retriever uses an index of passages. The arrow between 'Query' and 'Retriever' is labeled 'query.' The 'Retriever' then identifies and outputs 'Top-k passages,' which are the most relevant initial passages. These 'Top-k passages' are then passed to the 'SLM as a Ranker.' The SLM's function here is to further refine the ordering of these passages, producing 'Ranked Top-k passages.' This process enhances the quality of search results by re-ranking initial retrievals.

**(c) SLM as a Rewriter:**
This diagram depicts the SLM's application in rewriting queries, specifically for interaction with a Large Language Model (LLM) in a generative task. The process begins with a 'Query to LLM.' This initial query is fed into the 'SLM as a Rewriter.' The SLM's task is to transform this input into a 'Modified Query.' This modified query is then passed to a 'Retriever.' The 'Retriever' uses this modified query to fetch 'Top-k passages.' These retrieved passages are finally fed into an 'LLM,' which uses them to perform 'Generation.' This workflow enables more effective generation by an LLM by first optimizing the query through an SLM.](images/55caed2c4f7f78a5d857a38a853bc5c203ddb2ed2f0ce8cc74b86cc6c50f991c.jpg)
Fig. 15. Roles of SLM in Web Search.

retrieval and ranking models addresses integration, with SLMs ranking retrieved documents and acting as re-rankers. Additionally, SLMs serve as rewriters in scenarios requiring enhanced query understanding. Thus, in web search, SLMs fulfill three roles: (1) content encoder, (2) ranker, and (3) rewriter, as depicted in Figure 15. Next, we give details.

SLM as a Content Encoder. Text embeddings are vector representations that encode semantic information, widely used in retrieval; SLM-based dense retrieval utilizes pre-trained deep language understanding to effectively tackle semantic challenges. H-ERNIE [63] employs a hierarchical model that encodes queries and documents at multiple granularity—character, word, and phrase—to improve specificity and relevance in web search results by aggregating finer details into coarser layers, addressing issues like ambiguous queries. Implicit Interaction $( I ^ { 3 } )$ [89] uses BERT [83] as a content encoder, generating implicit pseudo-queries from passages to enable high online efficiency with offline caching of passage vectors. However, ERNIE and BERT-style models overlook advancements in SLMs such as context length extension [292]. Thus, Peng et al. [272] employs LLaMa-7B [338] and Vicuna-7B [61] as semantic encoders for embedding retrieval, demonstrating improved performance through soft prompt tuning. CoCondenser [109] addresses sensitivity to noisy data and large batch requirements during dense retriever training. Using the Condenser architecture with Transformer blocks, the model condenses information into dense vectors effectively.

SLM as a Ranker. The reranking task improves the order of multiple candidates to enhance retrieval quality because rerankers are more accurate than embedding retrievers. InPars (Inquisitive Parrots for Search) [36] employs the T5 base 220M [284] as a re-ranker to enhance the BM25 retriever [291]. Initially, BM25 selects 1K candidates, re-ranked by a fine-tuned T5 model (monoT5) adapted as a binary classifier to assess document-query relevance. Training data, generated by GPT-3 [37], formulates queries and selects random negative examples. Experiments show the monoT5- enhanced retriever significantly outperforms GPT-3; for example, it achieves a 0.3599 MAP score on the TREC-DL 2020 dataset [69], surpassing GPT-3’s 0.3163.

SLM as a Rewriter. Queries to the retriever, typically just a few keywords, may reveal a knowledge gap between the query and the knowledge needed for effective retrieval, thus limiting performance. To address this, the “rewriteretrieve-read” framework [233] uses T5-large [284] to bridge the knowledge gap in queries by rewriting them for more effective retrieval. This rewriter, trained via reinforcement learning with downstream LLM performance as a reward, outperforms general LLM rewrites. For example, it achieves a 45.97 F1 score on HotpotQA, surpassing the generic LLM’s 43.85 F1 score.

4.1.5 SLM Applications in Mobile-device. The use of cloud-based LLMs on devices raises privacy concerns and their large size limits real-time responses in urgent scenarios such as medical emergencies. To overcome these issues, researchers are creating smaller, domain-specific models (SLMs) that offer accurate results and suit mobile use. This subsection discusses SLM applications on mobile devices, focusing on three aspects: (1) software API calls, (2) mobile control, and (3) basic NLP applications.

SLM for Tool Learning. Integrating LLMs with APIs enhances capabilities but incurs high training costs, prompting a shift to smaller, task-specific models that cut costs but risk errors. In response, Octopus [52] uses a diverse dataset from over 30K APIs and curriculum learning [217] to improve API function accuracy. This method boosts API performance in models like Codellama-7b [292] and Google’s Gemma series [332]. PhoneLM-1.5B-Call [417] is fine-tuned on DroidCall [389] datasets and achieves comparable performance compared to GPT-4o-mini [262]. $\alpha$ -UMI [307] employs SLMs as planners, callers, and summarizers within multi-agent systems, outperforming a single LLM in tool uses.

SLM for Mobile Control. LM agents facilitate user-device interactions through taps, gestures, and text, automating tasks and enhancing user handsfree convenience. Unlike traditional developerbased approaches that require extensive developer effort to design interfaces and translate commands into API calls, LMs offer scalable automation via GUI-based text contents. MobileAgent [87] integrates instructions and Standard Operating Procedures (SOP) to improve SLMs for mobile control. As shown in Figure 16, it processes goals (e.g., booking a dental appointment) by analyzing screens, queries, prior actions, and UI elements, forming

![## Image Analysis: 844ac2068793ef36b83eb6f0bf3c6390326b42f040099f022d3cd83076a77653.jpg

**Conceptual Understanding:**
This image conceptually represents an automated workflow driven by an Artificial Intelligence (AI) agent designed to interact with a mobile application interface. It illustrates how an AI system can understand a high-level goal, analyze the current state of a mobile screen (environment), consider past interactions, and then determine and execute a specific action within the app to achieve that goal. The main purpose is to demonstrate the decision-making process of an AI agent in navigating and operating a user interface, effectively automating tasks within a mobile application based on context and objectives.

**Content Interpretation:**
The image clearly shows the interaction between an AI Agent and a mobile application UI for task automation.

*   **System Being Shown:** An "AI Agent" interacting with a mobile application (specifically the "上海瑞金医院" app). This represents an automated execution tool.
*   **Processes Being Shown:**
    *   **Information Gathering/Contextualization:** The AI gathers "Goal," "Role," "Previous Actions," and "Environment" data from the mobile app's current state and historical interactions.
        *   *Evidence:* The "Generate Prompt" box explicitly lists these categories: "Goal: Book an appointment for the dentistry department tomorrow.", "Role: Given a mobile screen and a question, provide the action based on the screen information.", "Previous Actions: step_id:0 action_type:TYPE typed_textHospital...", "Environment: id:0 ui_text:<ui_type...".
    *   **Prompt Generation:** This information is synthesized into a "Prompt" that serves as input for the AI agent.
        *   *Evidence:* The arrow labeled "Generate Prompt" originating from the mobile screen and pointing to the detailed context box.
    *   **AI Decision Making:** The "AI Agent" receives the prompt as "Input" and processes it to decide the next best action.
        *   *Evidence:* The "AI Agent" box with "Input" arrow coming from the prompt box and "Output" arrow going to the action details.
    *   **Action Execution:** The AI Agent's decision (Output) is then translated into a concrete action to be performed on the mobile application, labeled "Execute Action."
        *   *Evidence:* The "Execute Action" box detailing "Action_type: DUAL_POINT_ui_text: Outpatient appointment. ui_typeid:16" and "Location: Position: [100, 252, 300, 500] Xpath: view[0]/view[2]/text[0]", with an arrow pointing back to the mobile screen.
*   **Relationships:** The diagram shows a clear cyclical relationship where the AI observes the UI, interprets the task context, decides on an action, and then performs that action on the UI, which would then lead to a new UI state for subsequent processing.
*   **Significance of Data:**
    *   The **"Goal"** ("Book an appointment for the dentistry department tomorrow.") is crucial as it defines the overall objective the AI agent is trying to achieve.
    *   The **"Role"** ("Given a mobile screen and a question, provide the action based on the screen information.") clarifies the AI's function and constraints.
    *   **"Previous Actions"** provide historical context, allowing the AI to understand the state of the interaction and avoid redundant or incorrect steps. For example, "action_type:TYPE typed_textHospital" suggests the AI previously typed in the hospital name.
    *   **"Environment"** provides details of the current mobile screen's UI elements, enabling the AI to identify interactive components.
    *   The **"Action_type: DUAL_POINT_ui_text: Outpatient appointment. ui_typeid:16"** specifically indicates the AI has identified "Outpatient appointment" as the correct next step, based on the current screen and goal. "DUAL_POINT" likely refers to a type of interaction, possibly tapping on a UI element.
    *   The **"Location: Position: [100, 252, 300, 500] Xpath: view[0]/view[2]/text[0]"** provides the exact coordinates and hierarchical path (Xpath) of the UI element to interact with, ensuring precise execution of the determined action. In this case, it corresponds to the "门诊预约" (Outpatient appointment) box on the mobile screen.

**Key Insights:**
The main takeaways and insights from this image are:

*   **AI-driven Automation of UI Tasks:** The image demonstrates a practical approach to automating tasks within mobile applications using an AI agent. This is evident from the explicit "AI Agent" component and the flow of "Generate Prompt," "Input," "Output," and "Execute Action" on the mobile screen.
*   **Contextual Understanding is Key for AI Action:** The AI agent does not act in isolation but relies heavily on a comprehensive understanding of the task's "Goal," its "Role," past "Previous Actions," and the current "Environment" (mobile screen state).
    *   *Evidence:* The detailed content of the "Generate Prompt" box explicitly lists all these contextual elements, which feed into the "AI Agent."
*   **Detailed Action Specification:** AI outputs are not vague but highly specific, providing both the type of action and its precise location within the UI for execution.
    *   *Evidence:* The "Action Execution" box details "Action_type: DUAL_POINT_ui_text: Outpatient appointment. ui_typeid:16" and "Location: Position: [100, 252, 300, 500] Xpath: view[0]/view[2]/text[0]". This level of detail ensures accurate interaction with the UI element "门诊预约" (Outpatient appointment).
*   **Mimicking Human Interaction:** The "DUAL_POINT" action type and "Position" / "Xpath" for location suggest that the AI is programmed to interact with the UI in a manner similar to a human user tapping on a specific area of the screen, targeting text-based UI elements ("ui_text: Outpatient appointment").
*   **Workflow for Task-Specific SLM Applications:** As per the document context, this workflow exemplifies how Small Language Models (SLMs) could be applied to specific tasks, using the provided context (Goal, Role, Previous Actions, Environment) to generate an actionable output. The textual inputs and outputs heavily imply a language model's role in interpreting the goal and environment to produce a structured action.

**Document Context:**
This image fits directly into the document's section "4.1 Task-specific SLM Applications" by providing a concrete example of how an automated execution tool, likely powered by an SLM, operates. It visually explains the mechanics behind an AI agent's interaction with a mobile application to achieve a user-defined goal. The workflow demonstrates the capabilities of AI in understanding context, planning actions, and executing them precisely within complex user interfaces, which is a core application area for SLMs in automating specific tasks. It highlights the structured input and output an SLM might handle to bridge the gap between human language goals and programmatic UI interactions.

**Summary:**
This diagram illustrates a detailed workflow for an automated system, an "AI Agent," designed to complete tasks within a mobile application, specifically the "上海瑞金医院" (Shanghai Ruijin Hospital) app. The process begins with the display of the mobile app's interface, which offers various services like "门诊预约" (Outpatient appointment) and "当日挂号" (Same-day registration).

To guide the AI Agent, a "Prompt" is generated. This prompt is comprehensive, including:
1.  **Goal:** The specific objective the AI needs to achieve, such as "Book an appointment for the dentistry department tomorrow."
2.  **Role:** A description of the AI's function, stating it should "Given a mobile screen and a question, provide the action based on the screen information."
3.  **Previous Actions:** A log of past steps taken by the AI, for example, "step_id:0 action_type:TYPE typed_textHospital" (typing a hospital name) or "step_id:1 action_type:DUAL_POINT ui_textappointment" (tapping an appointment UI element).
4.  **Environment:** Details about the current mobile screen's interactive elements, such as "id:0 ui_text:<ui_type" and "id:1 ui_textPayment ui_type".

This generated prompt then serves as the "Input" for the "AI Agent." The AI Agent processes this information, interprets the goal in the context of the screen and previous actions, and determines the most appropriate next step.

The AI Agent's decision is its "Output," which specifies the exact "Action_type" and "Location" for the interaction. In this example, the output is to perform a "DUAL_POINT_ui_text" action on "Outpatient appointment" (ui_typeid:16). The precise location for this action is given by "Position: [100, 252, 300, 500]" and "Xpath: view[0]/view[2]/text[0]," which correspond to the "门诊预约" (Outpatient appointment) box on the mobile screen.

Finally, this detailed action is "Execute[d] Action" on the mobile application interface. This completes one cycle of the automated process, where the AI observes, decides, and acts on the mobile screen to progress towards its overall goal. This systematic approach allows AI to reliably navigate and operate complex mobile applications without direct human intervention for specific tasks.](images/844ac2068793ef36b83eb6f0bf3c6390326b42f040099f022d3cd83076a77653.jpg)
Fig. 16. An example workflow for an automated execution tool [87]. The screenshot in the left is taken from [87].

prompts to generate outputs and execute actions (e.g., selecting text, XPath). Fine-tuning Qwen-7B [23] on AIA medical data, it outperforms GPT-4 [2] on AitW [290], a key mobile benchmark, without extra inference costs. Carreira et al. [42] run a small offline model on mobile devices, fine-tuned with ChatGPT-3.5 data, enabling tasks like calls and web searches. RedPajama-INCITE-Chat-3B-v1 Computer [67], selected for its size and chat features, uses native code and quantization, performing well on 6GB and 4GB Android devices.

AutoDroid [376] improves Android app interactions via GUI automation. Figure 17 shows LLMpowered task automation (e.g., setting a laundry reminder for Aug 17) in four steps: (1) click ’New Event’, (2) enter ’laundry’ in ’Title’, (3) click ’Save’, (4) finish. Using Vicuna-7B and app-specific knowledge, AutoDroid generates privacy-filtered prompts for tasks. On its DroidTask benchmark, it outperforms GPT-3.5 $( 3 4 . 7 \% )$ and GPT-4 $( 5 4 . 5 \% )$ with $5 7 . 7 \%$ accuracy. M4 (composable mobile foundation model) [423] introduces a 9.2B parameter model

![## Image Analysis: f0676f031f8be6ddbcdd421342e02c7aa2dd3e785aaeb77f576fd2a9803bf571.jpg

**Conceptual Understanding:**
This image conceptually represents the automation of mobile tasks using AI. Its main purpose is to illustrate how a user's natural language instruction for a reminder can be seamlessly executed on a mobile calendar application through an intelligent system. The image communicates the idea of an AI agent, powered by an LLM and domain knowledge, acting as an intermediary to perform specific UI actions, thereby automating a common mobile task.

**Content Interpretation:**
The image demonstrates a mobile task automation workflow where a natural language request from a user is translated into a sequence of user interface (UI) interactions on a mobile calendar application. The system processes the request by leveraging a Large Language Model (LLM) and domain-specific knowledge to generate and execute the necessary actions. The core processes shown are: user intent understanding (from the "Task" input), LLM processing, action generation based on domain knowledge, and step-by-step execution of UI commands on a mobile device.

**Key Insights:**
The main takeaway from this image is that advanced AI models, specifically LLMs combined with domain knowledge, are capable of understanding complex natural language user requests and translating them into precise, sequential actions to automate tasks on mobile interfaces. The image highlights the feasibility of creating intelligent agents that can interact with applications in a human-like manner, effectively completing multi-step procedures without direct user intervention beyond the initial command. This showcases a practical application of AI in enhancing mobile user experience and productivity through automation.

**Document Context:**
This image directly supports the document's section "4.1 Task-specific SLM Applications" by providing a concrete example of how an LLM (or SLM) can be applied to automate real-world, task-specific operations on a mobile platform. It illustrates the practical utility and the step-by-step mechanism of such an automation, demonstrating the conversion of a high-level natural language instruction into low-level, actionable UI commands. The figure is taken from [42].

**Summary:**
This image illustrates a four-step mobile task automation process driven by a user's natural language request, utilizing an LLM (Large Language Model) and domain knowledge. The user initiates a task: "Remind me to do laundry on August 17". 

The process begins with the system interacting with the calendar application. In Step 1, the system "Click[s] 'New event'" on the calendar screen. This action is indicated by a red circle with a white plus icon at the bottom right of the calendar app being highlighted with a pointer. 

Next, in Step 2, the system "Input[s] 'laundry' into the 'Title' field" of the 'New Event' screen, demonstrated by a pointer hovering over the 'Title' field. 

Following this, Step 3 involves the system "Click[ing] 'Save'" on the 'New Event' screen, indicated by a pointer clicking the checkmark icon at the top right, signifying the creation of the event. 

Finally, Step 4 indicates that the "Task is completed", returning to the calendar screen where the event is presumably added, although not explicitly visible on the calendar grid itself. 

Throughout this process, there are double-headed arrows between "LLM", a robot icon (representing the automation agent), and "Domain Knowledge", indicating continuous interaction and utilization of these components for task execution.](images/f0676f031f8be6ddbcdd421342e02c7aa2dd3e785aaeb77f576fd2a9803bf571.jpg)
Fig. 17. An illustration of Vicuna-7B-powered mobile task automation [42] shows a user asking to be reminded about doing laundry on Aug 17. The figure is taken from [42].

(7.5GB memory) for mobile AI tasks, managed by the OS and hardware. Currently limited to high-end devices, its applicability will expand with increasing mobile memory/storage. These works highlight customizing smaller, domain-specific SLMs to address memory limits while preserving functionality in mobile environments.

SLM for Basic NLP Applications on Devices Performing basic NLP tasks such as text rewriting directly on the device can enable personalization while ensuring privacy. The sparse annotation on devices is a challenge. Qin et al. [278] utilizes self-supervised data selection and synthesis for on-device fine-tuning, leveraging sparse annotations and limited storage effectively. This approach, demonstrated in Figure 18, employs the Llama-3B model [338] and the LoRA fine-tuning method [142], enhancing personalization by efficiently managing data through metrics including embedding entropy and domain-specific scores. In mobile text rewriting, Zhu et al. [458] train the compact Palm 2-XXS model [14] using data generated by the larger Palm 2-L to ensure user privacy and accommodate device constraints. On its new benchmark, MESSAGEREWRITEEVAL, Palm 2-XXS achieves a BLEU score of 34.59, outperforming LLaMa-7B (16.65) [338]. tokens/s), proving its mobile efficiency for text rewriting.

![## Image Analysis: d4ef979fb35a43bc43f105144a9a822511b5247f2074c5314c41005aa974897d.jpg

**Conceptual Understanding:**
This image conceptually represents an iterative framework for the continuous improvement and fine-tuning of Small Language Models (SLMs). Its main purpose is to illustrate a robust and systematic approach to enhance SLM performance for task-specific applications by leveraging both synthetically generated data and real user interactions. The key idea communicated is the integration of data synthesis, user feedback, and data selection with specific quality metrics to create a self-improving system for language models, ensuring they remain relevant and accurate over time.

**Content Interpretation:**
The image details a comprehensive, iterative system for enhancing Small Language Models (SLMs) through a dual data strategy: synthesizing new data and processing real user interactions. It shows how initial dialogue sets are generated or collected, annotated by users, and then used to 'Fine-tune' various 'Large Language Models' (e.g., 'Claude 3', 'Gemini', 'Meta AI'). The system also incorporates 'User-Generated Data' where users provide 'Prompt's and receive 'Response's, with 'Feedback' loops. This user data is then systematically processed by a 'Data Selector' which 'Get Text Domain Tag' and 'Calculate EOE, DSS, IDD' (likely metrics for 'Ease of Evaluation', 'Dialogue Similarity Score', and 'Intent Detection Difficulty' or similar task-specific metrics as suggested by the surrounding text), before 'Forward'ing it to update the 'Data. Embedding' and inform the 'Original dialogue set' and 'User Annotation' processes. This creates a closed-loop system where data synthesis and user interaction continuously refine the SLMs, indicating a robust strategy for task-specific SLM application.

**Key Insights:**
1.  **Iterative Fine-Tuning Process:** SLMs are continuously refined through an iterative loop that involves data generation, selection, and fine-tuning, highlighted by the feedback loops and the 'Forward' path back to data sources. 
2.  **Dual Data Strategy:** Both 'Generated dialogue sets' (synthesized data) and 'User-Generated Data' are critical inputs for 'Fine-tune'ing 'Large Language Models'. This suggests a hybrid approach to data collection and model training.
3.  **Data Selection Metrics:** The 'Data Selector' employs specific metrics ('EOE, DSS, IDD') to process and prioritize 'User-Generated Data', indicating a quality-controlled approach to training data, ensuring only relevant and high-quality data contributes to model improvement.
4.  **Semantic Similarity for Synthesis:** The 'Data Synthesis' process explicitly aims to 'generate a semantically similar text based on input', emphasizing the importance of creating relevant and contextually appropriate synthetic data. 
5.  **User Annotation's Role:** 'User Annotation' is a key step, specifically applied to the 'Original dialogue set', indicating the importance of human oversight and quality control in preparing data for fine-tuning.
6.  **Model Agnostic Framework:** The framework is applicable to various 'Large Language Models', as evidenced by the mention of 'Claude 3', 'Gemini', and 'Meta AI' within the 'Large Language Models' box, suggesting a generalizable approach.
7.  **Feedback Loops for Improvement:** The diagram clearly shows feedback from the fine-tuned 'Large Language Models' back to 'Prompt' generation for 'User-Generated Data' and the 'Forward' path from 'Data Selector' to 'Original dialogue set' and 'User Annotation', underscoring a continuous improvement paradigm.

**Document Context:**
This image serves as a detailed visual explanation for the '4.1 Task-specific SLM Applications' section in the document. It directly illustrates the mechanism described in the accompanying text, which states, 'Data Synthesis generates semantically similar text via prompts, creating dialogue sets. Data Selection processes user data, tags domains, and calculates metrics (EOE, DSS, IDD). Selected data fine-tunes SLMs with user annotations. The iterative framework refines SLMs through continuous data generation and selection.' The diagram provides a granular, step-by-step flowchart of this iterative fine-tuning process, clarifying how synthetic and user-generated data are integrated to enhance SLMs, specifically mentioning popular models like 'Claude 3', 'Gemini', and 'Meta AI'. It graphically outlines the 'data synthesis' (top section) and 'data selection' (bottom section) components and their interaction with the 'Large Language Models', which is central to understanding task-specific applications.

**Summary:**
This comprehensive diagram illustrates an iterative framework for fine-tuning Small Language Models (SLMs) using a combination of synthesized data and user-generated data. The process is divided into three main conceptual sections: Data Synthesis, User-Generated Data processing, and Data Selection. 

Starting with 'Data Synthesis' at the top, an LLM (Large Language Model) is prompted to generate semantically similar text based on an initial input. This input, such as 'Doctor, I have been experiencing some discomfort and curvature of my spine. Could you recommend some medications that could help alleviate my symptoms? Response: 'Based on your symptoms, you may...' leads to the collection of 'Generated dialogue sets'. Simultaneously, an 'Original dialogue set' is created from another input, for example, 'Input: 'Doctor, I have been diagnosed with scoliosis. What medication do I need to take? Response: 'Based on your symptoms, you may...'. The generated dialogue sets are then used to 'Fine-tune' the 'Large Language Models' (which include 'Claude 3', 'Gemini', 'Meta AI'). The original dialogue set, along with 'User Annotation', also feeds into the fine-tuning process for these SLMs.

The middle section focuses on 'User-Generated Data'. Users provide a 'Prompt' to the SLMs and receive a 'Response', with 'Feedback' arrows indicating an interactive loop. This user interaction data is stored in a 'Data Buffer', which can contain various 'Domain' specific information and 'Data. Embedding'.

The bottom section, 'Data Selector', processes this 'User-Generated Data'. It first 'Get Text Domain Tag' to categorize the data. Following this, it 'Calculate EOE, DSS, IDD' (metrics for data quality or relevance). This processed data, including the 'Domain' tag and the calculated 'Data (EOE, DSS, IDD)', is then forwarded via the 'Forward' path. This 'Forward' path updates the 'Data. Embedding' in the 'Data Buffer' and feeds back into the 'Original dialogue set' and 'User Annotation' processes, creating a continuous loop for improvement. The entire system has a 'Stop update' mechanism, indicating that the fine-tuning can be halted. This iterative loop ensures continuous refinement of the SLMs based on both automatically generated and carefully selected user interactions, enhancing their task-specific performance.](images/d4ef979fb35a43bc43f105144a9a822511b5247f2074c5314c41005aa974897d.jpg)
Fig. 18. Overview of fine-tuning SLMs with synthesized and user data [278]. Data Synthesis generates semantically similar text via prompts, creating dialogue sets. Data Selection processes user data, tags domains, and calculates metrics (EOE, DSS, IDD). Selected data fine-tunes SLMs with user annotations. The iterative framework refines SLMs through continuous data generation and selection.

Tests on the Samsung S23 show lower latency (29 tokens/s) than a 4-bit LLaMa-7B on a MacBook M1 Pro (18-22

Insights: We draw several key insights from the development of task-specific SLMs:

• There is considerable potential in enhancing the efficiency and effectiveness of small models by integrating self-adaptive techniques with further fine-tuning and optimization of RAG-based methods.   
• The growing relevance of SLMs in coding highlights their cost-effectiveness and efficiency as alternatives to LLMs, providing quick processing and easy fine-tuning for specialized tasks; while LLMs handle complex tasks well, SLMs, optimized and fine-tuned on specific data, are increasingly essential in resource-limited settings.   
• SLMs significantly enhance recommendation systems due to their robust generalization, reasoning abilities, and in-context learning, addressing key challenges such as cold-start problems and distribution biases. They support long-term planning, replace traditional encoders, and use parallel low-rank parameters to inject personalized user knowledge effectively.   
• SLMs play a crucial role in web search such as document encoding, text reordering, and query rewriting, often outperforming LLMs through techniques such as supervised fine-tuning, soft prompt tuning, unsupervised contrastive loss, and reinforcement learning, thereby enhancing adaptability and efficiency.   
• SLMs are utilized on mobile devices primarily for privacy and memory constraints, with applications in API calls and mobile control; they are typically developed by generating data with LLMs and fine-tuning with SLMs, or by using local SLMs to handle privacy with LLMs boosting performance, and their training involves innovative techniques like learning from data streams and managing non-IID time series data.

Table 6. On-device Deployment Optimization Techniques   

<table><tr><td>Aspect</td><td>Representative Work</td><td>KeyPoint</td></tr><tr><td rowspan="10"></td><td>EDGE-LLM[422]</td><td>Edge LLMs use LUC and adaptive tuning for efficiency</td></tr><tr><td>LLM-PQ [448]</td><td>Optimize quantization and layer partitioning for complex setups.</td></tr><tr><td>AWQ[207]</td><td>Preservekey weights based onactivation distribution.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>DMC[256]</td><td>Adaptively compress KV cache, optimizing storage efficiency.</td></tr><tr><td>Transformer-Lite [193]</td><td>Optimize KV cache to reduce redundancy and memory use.</td></tr><tr><td>LLMaaS [419]</td><td>LLMaaS manages apps via chunk-wise KV cache optimization on mobiles.</td></tr><tr><td rowspan="5">Runtime Efficiency Optimization</td><td>mllm-NPU [395]</td><td>On-device NPU (neural processing units) to reduce prefill latency.</td></tr><tr><td>COST-EFF [305]</td><td>Distill amulti-exit model from the original PLM.</td></tr><tr><td>LLMCad [394]</td><td>Use SLM for fast token generation and cloud verification.</td></tr><tr><td>EdgeMoE 416]</td><td>Predict expert needs, boosting inference speed and reducing latency.</td></tr><tr><td>LinguaLinked [447]</td><td>Optimize data flow and load,enhancing multi-threading efficiency.</td></tr></table>

# 4.2 SLM Deployment on Mobile and Edge Devices

On-device applications benefit uniquely from the memory-saving efficiency and rapid runtime performance of SLMs, which offer advantages over LLMs. However, devices with extremely limited resources may still struggle with the parameter sizes of SLMs. To ensure both memory and runtime remain within acceptable range while still maintaining performance, it is crucial to integrate technologies that facilitate the deployment of SLMs on resource-constrained devices. The primary challenge for memory-efficient technologies arises from the inherent size of the SLMs and their associated caches. To address this, we survey existing works focused on compressing SLMs and their caches. Additionally, the large size of models significantly impacts runtime efficiency due to the extensive computing workload and potential weight transfers between the memory buffer and RAM/GPU memory. Other challenges include switching the Mixture of Experts between CPU and GPU memory and managing resource scheduling when deploying SLMs across multiple local devices. Therefore, in this subsection, we review representative works that address these challenges under two aspects: memory efficiency optimization and runtime efficiency optimization, as systematically compiled in Table 6.

4.2.1 Memory Efficiency Optimization. Memory efficiency involves minimizing the memory usage of both the model and the KV cache during deployment. This is typically achieved through model compression techniques such as quantization [207, 285, 422, 448], the cache of MoE experts [416], and KV cache compression [165].

Compression on model parameters. Quantization, a common method for deploying SLMs, lowers numerical precision to significantly save memory while preserving accuracy. We detail quantization strategies in Sections 2.3.3 and 3.5, focusing here on representative works for edge devices. EDGE-LLM [422] adapts LLMs for edge devices using a Layer-wise Unified Compression (LUC) method that combines layer-specific pruning and quantization to reduce computational demands and an Adaptive Layer Tuning and Voting scheme to optimize memory use while ensuring performance. Meanwhile, LLM-PQ [448] addresses quantization and model layer partitioning for heterogeneous clusters, incorporating a Cost Model and an Indicator Generator to optimize bit-width assignment and layer partitioning through integer linear programming, enhancing quantization for complex computational setups. Activation-aware Weight Quantization (AWQ) [207] is a hardware-friendly, low-bit, weight-only quantization method for on-device LLMs, preserving essential weights based on activation distribution to minimize quantization loss. MoE- $\cdot \mathbf { I } ^ { 2 }$ [404] prune less important experts and applies low-rank decomposition to the remaining experts to further optimize efficiency. Manuscript submitted to ACM

Cache of MoE Experts. Beyond standard quantization, which reduces storage for all model parameters, another strategy involves caching a mixture of experts (MoE). Driven by the fact that memory storage is more cost-effective and scalable than computing capacity, the MoE architecture [154] boosts performance while minimizing computational costs by activating only portions of the LLM as needed. However, this approach incurs significant memory overhead, making it impractical for edge device memory constraints. For example, Switch Transformers [100], with 32 experts per layer, require 54GBs of memory for inference, exceeding the capacity of most edge devices. Yi et al. [416] notes that in the Switch Transformers model, although most of the model weights $( 8 6 . 5 \% )$ are attributed to experts, these weights account for only a small fraction $( 2 6 . 4 \% )$ of the computations. To address this, EdgeMoE [416] introduces a method where experts are loaded into an expert memory buffer only when activated, achieving approximately $3 \times$ memory savings compared to the baseline where all weights are held in memory.

KV Cache Compression. When serving LMs for inference, using a KV cache is common practice to avoid intensive recalculations and speed up generation [276]. However, cache memory consumption escalates with model size and sequence length, posing a challenge for edge devices. To manage this, offloading techniques transfer KV caches to CPU memory [12, 308], although this can introduce significant overhead due to the switching costs between GPUs and CPUs. Token dropping compresses cache size by keeping only key tokens, often identified by low attention scores [111, 220, 444]. However, this method struggles with complex tasks, especially at high compression levels, due to increased estimation errors in compressed KV values. GEAR [165] addresses these issues by enhancing KV cache quantization with error-reduction techniques, including: (i) quantizing caches of similar magnitudes to ultra-low precision, (ii) using a low-rank matrix for efficient quantization residual approximation, and (iii) employing a sparse matrix for correcting outliers. This approach separates coherent from incoherent approximation errors, enabling near-lossless KV cache compression and achieving up to $2 . 2 9 \times$ peak memory reduction.

Besides, Dynamic Memory Compression (DMC) [256] adaptively compresses the KV cache by either adding new key and value representations directly or blending them with the top cache item using a weighted average. Transformer-Lite [193] tackles the redundancy of storing the KV cache twice in model inputs and outputs, which increases memory use. It optimizes storage by allocating a large tensor based on the maximum sequence length needed for inference. Sub-tensors are then created from this main tensor at different address offsets to serve as input and output KV caches, allowing direct writing to the correct locations during inference and removing extra copying steps. LLMaaS [419] introduces LLM as a Service for mobile devices, managing all apps through LLMS. This system uses chunk-wise KV cache compression and swapping, enabling efficient context switching within memory constraints. By segmenting the KV cache into independently compressed and swapped chunks, LLMS balances memory use and I/O bandwidth better than token-level or context-level management.

4.2.2 Runtime Efficiency Optimization. The goal of decreasing computing workload aligns with enhancing memory efficiency through methods such as quantization, as mentioned in the previous section. Decreasing model weight precision or reducing the number of weights naturally lowers latency. Other runtime efficiency techniques of minimizing inference latency involve, reducing prefill latency, early exits, large and small model collaboration, decreasing switching time in MoE, and reducing latency in distributed SLMs.

Reducing prefill latency. mllm-NPU [395] is the first LLM inference system that leverages on-device NPU (neural processing units) to reduce prefill latency and energy consumption. It incorporates a chunk-sharing graph, shadow outlier execution, and out-of-order subgraph execution to enhance NPU offloading efficiency. Experiments have shown mllm-NPU’s superior performance benefits, including up to $4 3 . 6 \times$ speedup and $5 9 . 5 \times$ energy savings.

Dynamic Early Exits A decoupled runtime saving technique is dynamic early exits. Originating from BranchyNet [335], which introduces exit branches after specific convolution layers in the CNN model, this concept has been adapted for PLMs as Transformer layer-wise early exiting [391]. Early exiting enables dynamic acceleration during inference and reduces temporal overhead by allowing exit without passing through all model layers. To address the inconsistency issue arising from exiting at different layers, COST-EFF [305] distills a multi-exit model from the original PLM.

Large and Small Model Collaboration Model collaboration, deploying SLMs on devices with cloud-based LLM support, enhances runtime efficiency. LLMs will increase latency when directly deployed via mobile engines like llama.cpp due to a large number of computing operations. LLMCad [394] addresses this by using a real-time, memoryresident SLM for simple tokens such as determiners and punctuation. The SLM generates tokens, while a cloud-based LLM verifies and corrects them, speeding up the process. LLMCad enhances token generation up to $9 . 3 \times$ by pairing the memory-resident SLM, Llama 2 7B, with the cloud-based LLM, Llama 2 13B, cutting latency from 16.2 to 3.9 seconds on Xiaomi Pro for TruthfulQA tasks [208].

Reducing MoE Switching Time. To reduce latency in MoE architectures caused by frequently switching experts in limited device memory, EdgeMoE [416] enhances runtime efficiency by preemptively predicting which expert will be needed, based on the observed long-tail distribution of unbalanced expert activations. It utilizes a statistical model, built offline, to estimate expert activation probabilities in transformer layers from previous activations. During inference, EdgeMoE preemptively loads the most likely needed expert, accelerating inference by $1 . 1 1 \times$ to $2 . 7 8 \times$ and significantly reducing latency. For instance, in a switch transformer model with 8 experts, latency drops from approximately 0.7s to 0.3s, outperforming the baseline method that preloads experts based on hit ratios.

Reducing Latency in Distributed SLMs. Distributing an SLM across smaller devices reduces the need for extensive model compression while preserving accuracy. However, this approach faces challenges that incur latency such as managing diverse device capabilities, handling data dependencies between model segments, and adapting to dynamic resource availability. To address these issues, LinguaLinked [447] addresses these issues by optimizing model assignment to match device capabilities and minimize data transmission, implementing runtime load balancing to redistribute tasks and prevent bottlenecks, and enhancing communication for efficient data exchange between segments. With multi-threading, the system improves, achieving acceleration rates between $1 . 7 3 \times$ and $2 . 6 5 \times$ for both quantized and full-precision models.

Insights: We draw several key insights from the deployment of SLMs:

• Model size remains a bottleneck for both memory and runtime efficiency. A common solution is model quantization, which reduces model precision to save memory and lessen computing workload, thereby boosting inference speed [207, 223, 253, 285, 422, 448]. Similarly, KV cache compression also helps achieve these efficiency gains [165, 193, 256, 419].   
• Mixture of Experts (MoE) is commonly used in SLMs to enhance performance using the same computing resources, but it results in increased memory usage. To address this, only activated experts are loaded into the memory buffer while the majority are stored cold on disk. However, the cost of switching can slow down inference. Designing a preemptive expert pre-load strategy could therefore accelerate the inference [416].   
• Model collaboration between local SLMs and cloud-based LLMs enhances both memory and runtime efficiency by using smaller models on local devices, which are then verified by cloud LLMs to ensure performance is maintained. Using SLMs locally reduces memory usage and shortens the inference time from the local model. However, internet latency and delays in cloud LLM inference can still introduce latency. Verifying SLM outputs every $N$ tokens using LLMs can effectively mitigate this latency [394]. One deployment approach involves deploying SLMs/LLMs across multiple trusted local devices to maintain original performance while only loading a fraction of the model weights. However, this method can incur latency due to varying device capabilities and resource scheduling challenges. To address these issues, optimizing model assignment to align with device capabilities and minimizing data transmission are effective strategies [447].

# 5 GENERIC AND DOMAIN-SPECIFIC SMALL LANGUAGE MODELS

This section investigates small language models (with fewer than 7 billion parameters) in both general and specific domains. It details the methods of obtaining these small language models, the datasets, and the evaluation tasks, exploring the techniques for acquiring SLMs through compression, fine-tuning, or training from scratch. Additionally, we summarize the representative small language models, as detailed in Table 7 and 10.

# 5.1 Generic-domain SLMs

Overview. SLMs, with fewer parameters than LLMs, enhance computational efficiency in pre-training, fine-tuning, and inference, reducing memory and energy demands—crucial for resource-limited environments. Their compact, localized nature boosts privacy, personalization, and response times, making them ideal for low-power edge devices. Therefore, SLMs are attracting increasing attention, and various models are being developed. Table 7 summarizes current representative generic-domain 42 SLMs/SLM families. Although all chosen SLMs have similar architectures, they vary in specific training datasets and techniques, with some datasets not being openly available. Taking the latest Llama 3.2 1B models [7] in Figure 19 as an example, its parameter size and use of filtered high-quality training data, pruning-based initialization, knowledge distillation pre-training tasks, and training techniques such as Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO) distinguish it from others.

![## Image Analysis: 5c4d53a6a4c5f09f4ddfaecb6f9efc56e2048c001e55fb6f1444260ba8c674b2.jpg

**Conceptual Understanding:**
This image represents a 'model card' for the Llama 3.2 1B language model. Conceptually, it functions as a standardized technical data sheet or a transparent disclosure document, providing a comprehensive, structured overview of the model's key characteristics. The main purpose is to inform potential users, researchers, and stakeholders about the model's design, capabilities, limitations, and ethical considerations. It aims to enhance understanding and promote responsible deployment by detailing everything from its architectural specifics to its training data, performance metrics, and safety evaluations.

**Content Interpretation:**
The image details the technical and operational profile of the Llama 3.2 1B language model. It covers fundamental model architecture, data used for training, the training methodologies employed, evaluation results, and crucial ethical and safety considerations. Each section provides specific values and descriptions that collectively define the model's characteristics and intended application. The 'Model Specs' explain the core design and accessible nature (Open Source), 'Dataset Specs' clarify the data scale and recency, 'Training Specs' outline the specific techniques and resources used, 'Evaluation Specs' quantify its performance on a specific benchmark, and 'Responsibility' addresses critical safety and ethical concerns.

**Key Insights:**
**Key Takeaways/Insights:**

1.  **Model Origin and Accessibility:** The Llama 3.2 1B model is developed by Meta AI, is relatively new with a release date of 9/25/2024, and is fully Open Source, indicating transparency and community involvement.
    *   *Evidence:* 'Developer: Meta AI', 'Release Date: 9/25/2024', 'Open Source?: Yes', 'Code: https://huggingface.co/meta-llama/Llama-3.2-3B'

2.  **Scalability and Multilingual Capability:** It is a relatively compact model at 1.23 billion parameters but supports a wide range of languages, making it suitable for diverse applications.
    *   *Evidence:* 'Params: 1.23B', 'Language: EN, DE, FR, IT, PT, HI, ES, TH'

3.  **Advanced Transformer Architecture:** The model leverages modern Transformer architecture with specific features like GQA attention and Shared Embeddings, and has a substantial context window.
    *   *Evidence:* 'Architecture: Transformer', 'Layer Number: 16', 'Hidden Size: 2048', 'Attention: GQA', 'Attention Heads: 32', 'Architectural Tech: Shared Embeddings', 'Context length: 128K', 'Vocab Size: 128256'

4.  **Training Methodologies for Efficiency and Performance:** The model utilizes advanced training techniques like Knowledge Distillation (KD) and pruning-based initialization, and further post-training through Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO).
    *   *Evidence:* 'Pre-text tasks: Knowledge Distillation', 'Pre-training methods: Pruning-based initialization, KD from Llama 3.1 8B & 70B', 'Post-training methods: SFT, RS, DPO'

5.  **Rigorous Evaluation and Safety Focus:** The model's performance is measured against the MMLU benchmark, and significant effort is put into safety through Red Teaming and evaluations against serious risks like CBRNE and Child Safety.
    *   *Evidence:* 'Benchmark: MMLU (English)', 'Performance: 32.2', 'Safety: Red Teaming, Llama safeguards', 'Evaluations: Red Teaming, CBRNE, Child Safety, Cyber Attacks'

These insights demonstrate the model's technical foundation, its development and operational characteristics, and the responsible practices incorporated into its lifecycle.

**Document Context:**
As indicated by the document context 'Section: 5.1 Generic-domain SLMs' and 'Fig. 19. Llama 3.2 1B model card', this image serves as a detailed reference for the Llama 3.2 1B model. It provides foundational and technical information crucial for understanding the discussion around generic-domain Small Language Models (SLMs) in the broader document. The comprehensive details about its architecture, training, and safety aspects directly support any analysis, comparison, or discussion of this specific model within the academic, technical, or research context of the document. It acts as a primary data source for the model's characteristics.

**Summary:**
The image displays a comprehensive 'model card' for the 'Llama 3.2 1B Model', providing a detailed overview of its specifications, training, evaluation, and responsible use. It is organized into five main sections: Model Specs, Dataset Specs, Training Specs, Evaluation Specs, and Responsibility. The Model Specs detail the developer as Meta AI, the model size at 1.23 billion parameters, and a release date of 9/25/2024. It operates under a Llama 3.2 Community License and is intended for commercial and research use across multiple languages (EN, DE, FR, IT, PT, HI, ES, TH). The architecture is a Transformer with 16 layers, a hidden size of 2048, GQA attention, 32 attention heads, and SiLU activation, utilizing Shared Embeddings and a TikToken-based tokenizer. It supports a context length of 128K and a vocab size of 128256, is Open Source, and its code is accessible via a Hugging Face link. The Dataset Specs indicate that it does not use an Open Training Dataset, the training data source is unknown, but comprises 9 trillion tokens, with a data freshness of December 2023. Training Specs involve Knowledge Distillation for pre-text tasks, pruning-based initialization and KD from Llama 3.1 8B & 70B for pre-training methods. It uses H100-80GB hardware, consumed 370K GPU hours, and post-training methods include SFT, RS, DPO. Evaluation Specs show MMLU (English) as the benchmark, with a motivation to Test Multi-subject Knowledge, using Macro Avg as the metric, achieving a performance of 32.2. Finally, the Responsibility section highlights safety measures like Red Teaming and Llama safeguards, and evaluations against CBRNE, Child Safety, and Cyber Attacks. This card provides a transparent and structured summary for understanding the model's capabilities and limitations.](images/5c4d53a6a4c5f09f4ddfaecb6f9efc56e2048c001e55fb6f1444260ba8c674b2.jpg)
Fig. 19. Llama 3.2 1B model card

5.1.1 Architecture Design. From Table 7, we observe several trends in component choices for SLMs:

(1) Recent SLMs frequently employ Grouped Query Attention (GQA) in self-attention mechanisms because it can reduce computational complexity. GQA achieves this by sharing query representations across multiple heads while keeping key and value representations separate. This approach aligns with the goals of SLM to enhance efficiency without compromising functionality.

Table 7. High-level Overview and Training Details of Generic-domain Small Language Models. #Params means Parameter amounts. $" > "$ indicates parameters larger than 7B.   

<table><tr><td>Model</td><td>#Params</td><td>Date</td><td>Paradigm</td><td>Domain</td><td>Training Datasets</td><td>Training Techniques</td></tr><tr><td>PhoneLM [417]</td><td>0.5B; 1.5B</td><td></td><td>2024.11Pre-train</td><td>Generic</td><td>DCLM-baseline[192],StarCoderData[195]; OpenWebMath[267],Dolma-algebraic and Dolma-arXiv [315]</td><td>RoPE,MHA,GtedFFNorm,ReLUDP,Faso 2,ZeRO</td></tr><tr><td>Llama 3.2 [7]</td><td>1B;3B</td><td>2024.9</td><td>Pre-train</td><td>Generic</td><td>no release (9T tokens)</td><td>GQA,SiLU, Multilingual Textand code,Shared embedding run- ing,Disilatio</td></tr><tr><td>Qwen 1[23]</td><td>1.8B; 7B; &gt;</td><td></td><td>2023.12Pre-train</td><td>Generic</td><td>no release</td><td>MHA; RoPE; SwiGLU; RMSNorm</td></tr><tr><td>Qwen 1.5 [23]</td><td>0.5B 1.8B;4B; 7B; </td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>no release</td><td>MHA; RoPE; SwiGLU; RMSNorm; Multilingual support</td></tr><tr><td>Qwen 2 402]</td><td>0.5B:1.5B; 7B; &gt;</td><td>2024.6</td><td>Pre-train</td><td>Generic</td><td>no release</td><td>GQA; RoPE;SwiGLU; RMSNorm; Multilingual support</td></tr><tr><td>Qwen 2.5 [402]</td><td>0.5B; 1.5B; 3B; 7B; &gt;</td><td>2024.9</td><td>Pre-train</td><td>Generic</td><td>no release</td><td>GQA; RoPE; SwiGLU; RMSNorm; Mulilingual support; Larger corpus</td></tr><tr><td>Gemma [332] Gemma 2[333]</td><td>2B;7B</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>Unknown</td><td>MHA, RoPE, GELUtanh</td></tr><tr><td></td><td>2B;</td><td>2024.7</td><td>Pre-train</td><td>Generic</td><td>Unknown</td><td>GQA;RoPE;GELUtalteatiaandGobion LogitSoft-Capping;RMSNormforPreandPost-Normalization</td></tr><tr><td>SmolLM[9]</td><td>135M; 360M; 1.7B</td><td>2024.7</td><td>Pre-train</td><td>Generic</td><td>smollm-corpus [27]</td><td>GQA,trapezoidalRscheduler</td></tr><tr><td>H2O-Danube3 [274]</td><td>500M; 4B</td><td>2024.7</td><td>Pre-train</td><td>Generic</td><td>Unknown</td><td>Thredifferent tainingstageswitdifferetdamixes</td></tr><tr><td>Fox-1[334]</td><td>1.6B</td><td>2024.6</td><td>Pre-train</td><td>Generic</td><td>Unknown(3T tokens)</td><td>GQA; Deeparchitecture</td></tr><tr><td>Rene[113] MiniCPM [143]</td><td>1.3B 1.2B; 2.4B</td><td>2024.5</td><td>Pre-train Pre-train</td><td>Generic Generic</td><td>Dolma-1.7 [315] Dolma[315]; C4[284]; Pile[84];stack[174];</td><td>Mamba-2layers,sliding-window attention(SWA)</td></tr><tr><td></td><td></td><td>2024.4</td><td></td><td></td><td>StarCoder[195]; UltraChat 86];Ossnstrut [373]; Evollnstrc[92]</td><td>Warmup-Stable-Decay (WSD)learning ate scheduler</td></tr><tr><td>CT-LLM [92]</td><td>2B</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>MAP-CC</td><td>Chinese, MHA,RoPE,SwiGLU,RMSNorm</td></tr><tr><td>OLMo[116]</td><td>1B;7B</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>Dolma[315]1</td><td>SwiGLU;RoPEameeri</td></tr><tr><td>TinyLlama [439]</td><td>1B</td><td>2024.1</td><td>Pre-train</td><td>Generic</td><td>SlimPajama [314]and StarCoder[195]</td><td>GQA, SiLU,FSDP,Flash Attention [73],xFormers [184]</td></tr><tr><td>Phi-1[120]</td><td>1.3B</td><td>2023.6</td><td>Pre-train</td><td>Coding</td><td>CodeTextBook[120] 2</td><td>MHA,GELUtanh,RoPE,FlashAttention</td></tr><tr><td>Pi-1.5[200]</td><td>1.3B</td><td>2023.9</td><td>Pre-train</td><td>Generic</td><td>CodeTextBook [120]; Synthetic Datasets (20B)</td><td>MHA,GELUtanh,RoPE,FlashAtentonDeep Otage</td></tr><tr><td>Phi-2[155]</td><td>2.7B</td><td></td><td>2023.12Pre-train</td><td>Generic</td><td>CodeTextBook[20];SyntheticDatasets ()</td><td>MHA,GELUtanh,RoPE,FlashAtentonDeep Otage</td></tr><tr><td>Phi-3[1]</td><td>3.8B; 7B; &gt;</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>Scaled-up datase from phi-2</td><td>MHA, SiLU, RoPE,FlashAttention, Deep ZeRO Stage 2</td></tr><tr><td>Phi-3.5[1]</td><td>3.8B; 4.2B; 6.6B</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>more multilingual and long-text data RefinedWeb269],duplicated]</td><td>Mltilingual; Vsion; HA, SiLU,RoPE,FlashAttetio,Ze</td></tr><tr><td>OpenELM [241]</td><td>270M: 450M 1.1B; 3B</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>partial RedPajama [67],partialDolmav1.6 [315]</td><td>Nobiases inFClayers;Pre-norm:RMSNorm;Posencoding: RoPE; Attention: GQA; FFN:SwiGLU; Tokenizer:LLaMA-style</td></tr><tr><td>MobiLlama [336]</td><td>0.5B;0.8B</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>LLM360 Amber3</td><td>GQA; SwiGLUParameter-sharing</td></tr><tr><td>MobileLLM[22]</td><td>125M; 350M</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>Unknown (1T tokens)</td><td>SwiGLUFN,endtinrchitectures,mbeddingsing and grouped queryattention</td></tr><tr><td></td><td></td><td>2</td><td></td><td></td><td></td><td>M</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Cerebras-GPT [84]</td><td>111M;256M; 590M; 1.3B; 2.7B; 6.7B; &gt;</td><td>2023.4</td><td>Pre-train</td><td>Generic</td><td>Pile [108]</td><td>MHA; GELU; Maximal Update Parameterization</td></tr><tr><td>Pythia [31]</td><td>14M;70M;160M:410M 1B:1.4B;2.8B;6.9B;&gt;</td><td>2023.4</td><td>Pre-train</td><td>Generic</td><td>Pile[108]</td><td>MHA; GELU; Flash Atention[74];RoPE[318]; ZeRO[286]</td></tr><tr><td>BLOOMZ [181]</td><td></td><td></td><td>2022.1Pre-train</td><td>Generic</td><td>ROOTS[180]and13programminglanguages</td><td></td></tr><tr><td>Galactica [330]</td><td>125M; 1.3B; 6.7B;&gt;</td><td></td><td>2022.1Pre-tran</td><td>Scientific</td><td>Open-access scientifc materials (106B tokens) but not released</td><td>MHA; GeLU;LearedPosinalEmbeddings</td></tr><tr><td>OPT[440]</td><td>125M350M;1.3B; 2.7B;5.7B</td><td>2022.5</td><td>Pre-train</td><td>Generic</td><td>Pile[108]andPushShift.io Redit[24]</td><td>MHA; ReLU</td></tr><tr><td>XGLM [209]</td><td>1.7B; 2.9B; &gt;</td><td></td><td>2021.12Pre-train</td><td>Generic</td><td>CC100-XL</td><td>-</td></tr><tr><td>GPT-Neo [33]</td><td>125M；350M；1.3B; 2.7B</td><td>2021.5</td><td>Pre-train</td><td>Generic</td><td>Pile[08]</td><td></td></tr><tr><td>Megatron-gpt2 [310]</td><td>355M;2.5B; &gt;</td><td>2019.9</td><td>Pre-train</td><td>Generic</td><td>Wikipedia[83],CC-Stories [341],RealNews [430],OpenWebtext</td><td>-</td></tr><tr><td>MINITRON [252]</td><td>4B；&gt;</td><td>2024.7</td><td>Distilation</td><td>Generic</td><td>8Ttokens inNemotron-4[26]</td><td>LR WSD Scheduer</td></tr><tr><td>Orca 2 [247]</td><td>7B</td><td>2023.11</td><td>Pruning Distillation</td><td>Generic</td><td>Orca 2 dataset</td><td>LLaMA-2-7B based; prompt erasing</td></tr><tr><td>Orca[251]</td><td>13B</td><td>2023.6</td><td>Distillation</td><td></td><td>FLAN-v2 [225]</td><td>From ChatGPT and GPT4, Explanation tuning; Progressive Learming</td></tr><tr><td>MINIMA [434]</td><td>3B</td><td></td><td>2023.11Distilation</td><td>Generic</td><td>ile[108],Wudao[424],Giub[67]</td><td>FromLlama-2-7B,Zero2,FlashAtention,Optimal teachrze</td></tr><tr><td>Dolly-v2 [68]</td><td>3B;7B; &gt;</td><td>2023.4</td><td>Instruction tuning</td><td>Generic</td><td>Databrcks-dolly-15[68]</td><td>from pythia</td></tr><tr><td>LaMini-LM [171]</td><td>61M-7B</td><td>2023.4</td><td>Distilation</td><td>Generic</td><td>LaMini instruction dataset</td><td>a colection ofSLMs distilld fromChatGPT-generated 2.58M instructions.</td></tr><tr><td>Specialized FlanT5[105]</td><td>250M; 760M; 3B</td><td>2023.1</td><td>Instruction Tuning</td><td>Generic (math）</td><td>GSM8K</td><td>Base modelis FlanT5</td></tr></table>

(2) The choice of activation function should balance model capability and efficiency. ReLU, known for its efficiency, introduces greater sparsity to the model, which facilitates faster coefficient calculations for inference acceleration. In contrast, SwiGLU’s parameters are learned during training, allowing the model to dynamically adapt to diverse tasks and datasets, thereby enhancing model capabilities and establishing it as a state-of-the-art option. SiLU, situated between these two, is favored for its balance of computational efficiency and model performance.

(3) RMS normalization is commonly used than layer normalization due to its reduced computational demands.

A basic introduction to these options is provided in Section 2. Apart from component choices, there are notable innovations in architecture for SLMs:

• Mobilellm [223] highlights that deeper models are more effective than wider ones for improving performance. • Embedding sharing [440] is crucial as embedding layers often constitute over $2 0 \%$ of a model’s parameters—for example, with 512 dimensions and a $3 2 \mathrm { k }$ vocabulary, each layer holds 16M parameters in a 125M-parameter model. Smaller models often reuse these weights for both input and output layers, enhancing efficiency and compactness. • Layer sharing [223] increases hidden layers in small Transformer models without additional storage costs. • Shared FFNs [336] make up about $6 5 \%$ of all trainable parameters, with attention mechanisms and heads accounting for the rest. Sharing FFN parameters across all transformer layers of an SLM is proposed to increase efficiency. • Architecture search ahead of pre-training. PhoneLM [417] proposes a principle for constructing on-device small language models: searching for a resource-efficient architecture on a given hardware to optimize the speed-capacity trade-off before pretraining. This approach inspires the tailored selection of architectural components for on-device SLMs, based on specific compositional requirements such as computing efficiency, model capability, and safety.

A detailed description of these architectural designs can be found in Section 3.1.

5.1.2 Training Datasets. From Table 7, we can observe a set of widely used training datasets in SLM development. We provide the details below:

• Pile [108]: It comprises 22 smaller, high-quality diverse corpora from various domains, such as Pile-CC, PubMed Central, ArXiv, GitHub, and FreeLaw, designed to offer a comprehensive foundation for language model training. The dataset contains 207 billion tokens and totals 825 GB.   
• C4 (Colossal Clean Crawled Corpus) [284]: This dataset includes 350 billion tokens, representing a cleaned version of the Common Crawl web corpus, intended to capture a wide snapshot of the internet 4.   
• The Stack [174]: It contains 6 trillion tokens of source code from over 300 programming languages, useful for developing code-centric AI applications. Python-edu in smollm-corpus [27] consists of Python files that are scored 4 or more by the educational code model and are extracted from the stack-v2-train dataset.   
• StarCoder [195]: It features 35 billion tokens, predominantly Python code, aimed at programming language understanding and generation.   
• RedPajama [67]: This dataset encompasses 1.2 trillion tokens derived from over 100 billion text documents, processed using the CCNet pipeline to ensure a rich collection of web texts.   
• RefinedWeb [269]: This dataset includes 5 trillion tokens of high-quality, extensively filtered web data, offering a valuable resource for training web-aware models.   
• PushShift.io Reddit [24]: A around 5 billion tokens resource for social media data collection, analysis, and archiving, specifically of Reddit data, aiding research into social media dynamics.   
• CulturaX [257]: It comprises 6.3 trillion tokens across 167 languages, supporting the development of models with extensive linguistic and cultural understanding.   
• FineWeb [268], a large-scale (15-trillion tokens, 44 TB disk space) dataset for LLM pretraining. FineWeb is derived from 96 CommonCrawl snapshots. FineWeb-Edu is a subset of FineWeb constructed using scalable automated high-quality annotations for educational value.

From the analysis of these datasets, we can derive several critical insights regarding the development of SLMs: (i) Data quality is crucial for training effective SLMs, involving sophisticated filtering like removing duplicates or irrelevant content, often with another LLM’s help. For example, the TinyStories corpus [96] is tailored for simplicity, ideal for training models to handle straightforward narratives. RedPajama-V2 [67] uses the CCNet pipeline to process 30B documents, providing quality signals and IDs for creating a 20B deduplicated dataset. (ii) Code Data: Source code constitutes a significant component of valuable data for training models, particularly because of its structured nature and logical content. Training on code data enhances a model’s reasoning capabilities and supports its ability to generalize across multiple natural languages, which is crucial for applications requiring robust problem-solving and interpretation skills in diverse coding environments [17, 104, 120, 236]

5.1.3 Training Algorithms. To enhance the alignment of SLMs with desirable properties such as safety and reasoning, training algorithms, particularly during the fine-tuning phase, are crucial in evolving pre-trained SLMs.

• Direct Preference Optimization (DPO) [283] presents a simpler alternative to RLHF for optimizing language models based on human preferences, preventing explicit reward modeling and reinforcement learning. Instead, DPO modifies log probabilities of responses with a dynamic weighting mechanism to prevent model degradation common in probability ratio-focused methods. The DPO loss function is:

$\begin{array} { r } { \mathcal { L } _ { D P O } ( \pi _ { \theta } ; \pi _ { \mathrm { r e f } } ) = - \mathbb { E } _ { ( x , y _ { w } , y _ { l } ) \sim D } \left[ \log \sigma \left( \beta \log \frac { \pi _ { \theta } ( y _ { w } | x ) } { \pi _ { \mathrm { r e f } } ( y _ { w } | x ) } - \beta \log \frac { \pi _ { \theta } ( y _ { l } | x ) } { \pi _ { \mathrm { r e f } } ( y _ { l } | x ) } \right) \right] } \end{array}$ where $\pi _ { \theta }$ is the policy being optimized, $\pi _ { \mathrm { r e f } }$ is the reference policy, $D$ includes tuples $( x , y _ { w } , y _ { l } )$ , $\sigma$ is the sigmoid function, and $\beta$ scales the log-ratios between $\pi _ { \theta }$ and $\pi _ { \mathrm { r e f } }$ , guiding the model towards human-preferred outputs. • Reinforcement Learning from Contrast Distillation (RLCD) [405] aims to calibrate generative SLMs/LLMs towards embodying harmless and beneficial characteristics. The process starts with an unaligned LM and initial prompts, which are modified into two variants ${ p + }$ and $\hbar -$ , intended to promote and suppress, respectively, attributes like helpfulness and harmlessness. Upon inputting these prompts, the LM generates outputs $^ { o + }$ and $o -$ , with $^ { o + }$ automatically designated as the preferred response. This automation speeds up training by avoiding additional evaluative scoring. The training continues under the RLHF framework. • Conditioned Reinforcement Learning Fine-Tuning (C-RLFT), by OpenChat [356], enhances model performance by incorporating low-quality data during SFT. C-RLFT leverages varied data qualities with simple rewards (e.g., expert data at 1 credit, sub-optimal at 0.1), using distinct prompt tokens to condition data sources, eliminating costly human feedback. Similarly, Data Mix [274] trains on English text in three stages, reducing noisy web data progressively in each stage in favor of higher-quality data. • Explanation Tuning, proposed by Orca [251], addresses the limitations of standard instruction-based fine-tuning, which often restricts SLMs to style imitation rather than reasoning. It uses system prompts with instructions to direct GPT-4 to produce detailed explanations or perform step-by-step reasoning. The resulting instructions and the responses are used as a dataset for fine-tuning SLMs to have better ability of reasoning.

• Progressive Learning, proposed by Orca [251], aims to bridge the capability gap between Orca and the more capable GPT-4. It starts with training on five million data points from ChatGPT, followed by one million from GPT-4. Research suggests that an intermediate-level teacher can improve distillation effects, enabling a stepwise learning approach where students start with simpler examples and gradually move to more complex ones, receiving improved reasoning and explanations from a more advanced teacher.   
• Prompt Erasing introduced by Orac 2 [247], is a distillation strategy designed to enhance the independent reasoning capabilities of student SLMs. In this approach, a more capable teacher LLM is given intricate prompts intended to elicit specific strategic behaviors and more precise outcomes. During the training phase, the SLM is exposed only to the task instruction and the resulting behavior, without access to the original intricate prompts that initiate such responses. This technique, known as Prompt Erasing, positions the student model as a cautious reasoner because it not only learns to perform specific reasoning steps but also develops strategies for approaching tasks at a higher level.   
• Maximal Update Parameterization $( \mu \mathbf { P } )$ optimizes control initialization, layer-wise learning rates, and activation magnitudes to ensure stable training regardless of model layer widths. This method enhances training stability and allows the same optimizer settings, especially learning rates, to be used across different model scales. For instance, Cerebras-GPT [314] employs $\mu \mathrm { P }$ to train its models efficiently.

5.1.4 Model Performance. To compare the performance of SLMs, we have extracted experimental results from two recent and concurrent studies published in June 2024, OLMo [116] and MobiLlama [336], and the recently proposed edge-device Llama 3.2 1B & 3B in September 2024 5. The extracted results are merged and shown in Table 8. From the table, we can find that the following evaluation benchmarks are commonly used:

(1) MMLU [134]: Evaluate broad knowledge across diverse fields such as humanities, science, technology, engineering, and management. It includes multiple-choice questions covering 57 tasks ranging from elementary mathematics to US history, computer science, law, and beyond, with a total of 14K test samples.   
(2) HellaSwag [429]: Assesses the model’s ability to select the correct ending to scenarios from multiple options, testing common sense reasoning, including 10K test samples.   
(3) ARC [65]: The AI2’s Reasoning Challenge (ARC) dataset features multiple-choice science exam questions for grades 3 to 9, divided into Easy and Challenge partitions, with the latter containing more complex questions necessitating reasoning. Most questions offer four answer choices. ARC includes a supporting knowledge base of 14.3M unstructured text passages, with 1.17K test samples in ARC_Challenge and 2.25K in ARC_Easy.   
(4) PIQA [32]: A commonsense reasoning dataset designed to evaluate the physical knowledge of NLP models. It presents questions (goals) that require physical commonsense for correct resolution, alongside two detailed response options (sol1 and sol2). The dataset comprises 3,000 test samples.   
(5) Winogrande [294]: a dataset structured as a fill-in-the-blank task with binary options, designed to assess commonsense reasoning. The dataset includes 1,767 test samples by default splits.

Accuracy is used as the evaluation metric in the table. Open Language Model (OLMo) [116] is publicly available with its training data and code 6. MobiLlama [336] is a general-purpose SLM designed from scratch, available in 0.5B and 0.8B versions. It adopts a unique approach by using a shared FFN across all transformer blocks, enhancing efficiency. MobiLlama also show high efficiency on diverse hardware (Table 9).

Table 8. Performance of Various SLMs on Common Benchmarks: data from MobiLlama [336], OLMo [116], and Llama 3.2.   

<table><tr><td>Model Size Range</td><td>Model</td><td>MMLU</td><td>HellaSwag</td><td>ARC</td><td>PIQA</td><td>Winogrande</td></tr><tr><td rowspan="10">&lt;1B</td><td>gpt-neo-125m</td><td>26.0</td><td>30.3</td><td>23.0</td><td>62.5</td><td>51.8</td></tr><tr><td>tiny-starcoder-170M</td><td>26.8</td><td>28.2</td><td>21.0</td><td>52.6</td><td>51.2</td></tr><tr><td>cerberas-gpt-256m</td><td>26.8</td><td>29.0</td><td>22.0</td><td>61.4</td><td>52.5</td></tr><tr><td>opt-350m</td><td>26.0</td><td>36.7</td><td>23.6</td><td>64.7</td><td>52.6</td></tr><tr><td>megatron-gpt2-345m</td><td>24.3</td><td>39.2</td><td>24.2</td><td>66.9</td><td>53.0</td></tr><tr><td>LiteLlama</td><td>26.2</td><td>38.5</td><td>24.9</td><td>67.7</td><td>49.9</td></tr><tr><td>gpt-sw3-356m</td><td>25.9</td><td>37.1</td><td>23.6</td><td>64.9</td><td>53.0</td></tr><tr><td>pythia-410m</td><td>27.3</td><td>40.9</td><td>26.2</td><td>67.2</td><td>53.1</td></tr><tr><td>xglm-564m</td><td>25.2</td><td>34.6</td><td>24.6</td><td>64.9</td><td>53.0</td></tr><tr><td>Lamini-GPT-LM0.59B</td><td>25.5</td><td>31.6</td><td>24.2</td><td>63.9</td><td>47.8</td></tr><tr><td></td><td>MobiLlama 0.5B</td><td>26.5</td><td>52.5</td><td>29.5</td><td>72.0</td><td>57.5</td></tr><tr><td></td><td>MobiLlama 0.8B</td><td>26.9</td><td>54.1</td><td>30.2</td><td>73.2</td><td>57.5</td></tr><tr><td rowspan="19"></td><td>StableLM1.6B</td><td></td><td>68.2</td><td>43.8</td><td>74.0</td><td></td></tr><tr><td>Pythia 1B</td><td>=</td><td>44.7</td><td>33.1</td><td>69.1</td><td>=</td></tr><tr><td>TinyLlama 1.1B</td><td>=</td><td>58.7</td><td>34.8</td><td>71.1</td><td>=</td></tr><tr><td>OLMo-1B</td><td></td><td>62.5</td><td>34.5</td><td>73.7</td><td></td></tr><tr><td>OLMo 1.2B</td><td>25.9</td><td>62.5</td><td>34.5</td><td>-</td><td>58.9</td></tr><tr><td>Boomer 1B</td><td>25.4</td><td>31.6</td><td>22.3</td><td></td><td>51.0</td></tr><tr><td>Pythia-Dedup 1B</td><td>24.3</td><td>49.6</td><td>29.1</td><td>=</td><td>54.0</td></tr><tr><td>Falcon-RW 1B</td><td>25.4</td><td>63.1</td><td>35.1</td><td>=</td><td>61.9</td></tr><tr><td>Cerebras-GPT1.3B</td><td>26.7</td><td>38.5</td><td>26.1</td><td>=</td><td>53.6</td></tr><tr><td>Lamini 1.3B</td><td>28.5</td><td>38.1</td><td>26.6</td><td>=</td><td>50.6</td></tr><tr><td>OPT1.3B</td><td>24.6</td><td>54.5</td><td>29.6</td><td>=</td><td>59.7</td></tr><tr><td>GPT-NEO 1.3B</td><td>24.8</td><td>48.5</td><td>31.3</td><td>=</td><td>57.1</td></tr><tr><td>Pythia-Deduped 1.4B</td><td>25.5</td><td>55.0</td><td>32.6</td><td></td><td>56.9</td></tr><tr><td>MobiLlama 1.2B</td><td>24.8</td><td>63.0</td><td>34.6</td><td></td><td>62.0</td></tr><tr><td>Gemma 2 2B</td><td>57.8</td><td>61.1</td><td>76.7</td><td></td><td></td></tr><tr><td>Llama 3.2 1B</td><td>49.3</td><td>41.2</td><td>59.4</td><td></td><td></td></tr><tr><td>Llama 3.2 3B</td><td>63.4</td><td>69.8</td><td>78.6</td><td></td><td></td></tr><tr><td rowspan="9"></td><td>Phi-3.5-mini 3.8B</td><td>69.0</td><td>81.4</td><td>87.4</td><td>=</td><td>=</td></tr><tr><td>Pythia 6.9B</td><td></td><td>63.8</td><td>44.1</td><td>75.1</td><td></td></tr><tr><td>Falcon-7B</td><td>=</td><td>75.9</td><td>47.5</td><td>78.5</td><td></td></tr><tr><td>LLaMA 7B</td><td>=</td><td>76.2</td><td>44.5</td><td>77.2</td><td></td></tr><tr><td>Llama 2 7B</td><td>=</td><td>76.8</td><td>48.5</td><td>76.7</td><td></td></tr><tr><td>MPT-7B</td><td>=</td><td>77.6</td><td>46.5</td><td>77.3</td><td></td></tr><tr><td>RPJ-INCITE-7B</td><td></td><td>70.3</td><td>42.8</td><td>76.0</td><td></td></tr><tr><td>OLMo-7B</td><td></td><td>76.4</td><td>48.5</td><td>78.4</td><td>=</td></tr><tr><td></td><td>=</td><td></td><td></td><td></td><td></td></tr></table>

Table 9. Comparison of MobiLlama 0.5B with large-base 1.2B, Llama2 7B, and Phi2-2.7B in terms of efficiency and resource consumption on low-end hardware devices [336].   

<table><tr><td>Platform</td><td>Model</td><td>#Params</td><td>Precision</td><td>Avg Tokens/Sec</td><td>Avg Memory Consumption</td><td>Avg Battery Consumption /1k Tokens</td><td>CPU Utilization</td></tr><tr><td rowspan="4">RTX2080Ti</td><td>Llama2</td><td>7B</td><td>bf16</td><td>14.85</td><td>27793MB</td><td>135.51 mAH</td><td>31.62%</td></tr><tr><td>Phi2</td><td>2.7B</td><td>bf16</td><td>32.19</td><td>12071 MB</td><td>59.13 mAH</td><td>24.73%</td></tr><tr><td>large-base</td><td>1.2B</td><td>bf16</td><td>50.61</td><td>6254MB</td><td>18.91 mAH</td><td>18.25%</td></tr><tr><td>MobiLlama</td><td>0.5B</td><td>bf16</td><td>63.38</td><td>3046MB</td><td>8.19 mAH</td><td>14.79%</td></tr><tr><td rowspan="4">CPU-i7</td><td>Llama2</td><td>7B</td><td>4bit</td><td>5.96</td><td>4188MB</td><td>73.5 mAH</td><td>49.16%</td></tr><tr><td>Phi2</td><td>2.7B</td><td>4bit</td><td>22.14</td><td>1972 MB</td><td>27.36 mAH</td><td>34.92%</td></tr><tr><td>large-base</td><td>1.2B</td><td>4bit</td><td>29.23</td><td>1163MB</td><td>10.81 mAH</td><td>30.84%</td></tr><tr><td>MobiLlama</td><td>0.5B</td><td>4bit</td><td>36.32</td><td>799 MB</td><td>4.86 mAH</td><td>24.64%</td></tr><tr><td rowspan="4">Snapdragon-685</td><td>Llama2</td><td>7B</td><td>4bit</td><td>1.193</td><td>4287MB</td><td>10.07 mAH</td><td>77.41%</td></tr><tr><td>Phi2</td><td>2.7B</td><td>4bit</td><td>2.882</td><td>1893MB</td><td>14.61 mAH</td><td>56.82%</td></tr><tr><td>large-base</td><td>1.2B</td><td>4bit</td><td>6.687</td><td>780MB</td><td>6.00 mAH</td><td>17.15%</td></tr><tr><td>MobiLlama</td><td>0.5B</td><td>4bit</td><td>7.021</td><td>770MB</td><td>5.32 mAH</td><td>13.02%</td></tr></table>

From Table 8 and 9, we can conclude that: (1) MobiLlama 0.5B and 0.8B demonstrate that a shared FFN design can facilitate excellent performance in SLMs with fewer than 1B parameters, even rivaling some models in the 1B-3B range. (2) The performance of MobiLlama 1.2B and OLMo 1.2B illustrates that advanced SLM architectures incorporating high-quality data, SwiGLU, non-parametric layer normalization, RoPE, BPE tokenization, and a shared FFN design can achieve competitive results among models with 1B-3B parameters. (3) MobiLlama demonstrates that SLMs can significantly reduce resource consumption on low-end hardware devices, achieving comparable performance while using a smaller proportion of battery power, memory, and GPU utilization. (4) Popular techniques such as pruning, quantization, distillation, SFT, and DPO, utilized in Llama 3.2, have substantially enhanced SLM performance.

Insights: We draw several key insights from the development of generic-domain SLMs:

• Typical SLM architectures generally incorporate features such as GQA, gated FFN with SiLU activations, RMS normalization, deep and thin architectures, embedding sharing, layer sharing, and shared FFNs.   
• Although these components are widely used, current research has not yet thoroughly explored their specific contributions within SLMs.   
• The importance of data quality in SLM research is increasingly emphasized, often considered more critical than the quantity of data and model architectural configurations.   
• Post-pretraining, meticulous fine-tuning is often required to enhance the safety of SLMs, involving strategies to distill capabilities from LLMs better. Common strategies include explanatory tuning, progressive learning, and prompt erasing.

# 5.2 Domain-Specific SLMs

Overview. The capability of LLMs to generate human-like text has significantly captured public interest, highlighting their potential in the field of general artificial intelligence. However, inefficiencies persist when integrating LLMs into specialized applications due to resource constraints. Unlike the need for extensive general knowledge and capabilities, domain-specific SLMs should focus on well-defined tasks and expertise pertinent to specific fields. For instance, specialized models can significantly impact biomedical research and healthcare by fine-tuning for interpretable mental health analysis, or assisting humans in legal dialogues and financial tasks through instruction tuning, showcasing their potential transformative influence. Given the limited number of SLMs specialized in specific domains, we demonstrate some existing SLMs individually across healthcare, science, finance, and law domains.

5.2.1 SLMs for Healthcare. Hippocrates [3] is an open-source medical language model framework with free access to its data, codebase, checkpoints, and protocols 7. It utilizes a medical pre-training corpus from Medical Guidelines, PMCPatients [452], and PubMedQA-contexts [163], totaling about 300M tokens. The Hippo series, a 7B model, undergoes continuous pre-training, instruction tuning, and RLHF. Fine-tuned on Mistral and Llama-2, it rivals 70B models in some evaluation. For example, Hippo-Mistral 7B scores $5 9 . 9 \%$ on MedQA, outperforming Meditron 70B [55] at $5 8 . 5 \%$ . BioMedLM [34], a 2.7B GPT-style model trained on PubMed content [108], excels in biomedical QA after fine-tuning, achieving $5 7 . 3 \%$ on MedMCQA (dev) and $6 9 . 0 \%$ on MMLU medical genetics exams. Available on Hugging Face Hub 8. AdaLM [414] enhances domain-specific SLMs by continuing training on a medical-focused SLM atop a general pretrained model. It emperically validates adaptation then distillation is the most effective distillation way. AdaLM modified a BERT_base model (12 layers, 768 hidden size) [83] with a 16GB PubMed 9 abstracts corpus. MentalLLaMA [406] introduces the first IMHI dataset for mental health analysis and the first open-source LM for explainable analysis on social media. The IMHI is compiled from ten sources, totaling 105K samples. Expert-designed mental health analysis prompts are employed via ChatGPT for explanations. Based on Llama-2-7B, MentalLLaMA is instruction-tuned on this data and matches top methods in accuracy on the IMHI test set. Project code is available at 10.

Table 10. High-level Overview and Training Details of Specific-domain Small Language Mode   

<table><tr><td>Model</td><td>#Params</td><td>Date</td><td>Base Models</td><td>Domain</td><td>Training Datasets</td><td>Train Techniques</td></tr><tr><td>Hippocrates [3]</td><td>7B</td><td>2024.4</td><td>Instruction Tuning (LLaMA2[339],Mistral [160])</td><td>Healthcare</td><td>Medical Guidelines, PMC-Patients [452], andPubMedQA- contexts [163]</td><td>Continual pre-training, in- struction tuning, RLHF</td></tr><tr><td>BioMedLM[34]</td><td>2.7B</td><td>2024.3</td><td>From scratch and Fine- tuning</td><td>Healthcare</td><td>PubMed[108]</td><td>FSDP</td></tr><tr><td>BioMistral[178]</td><td>7B</td><td>2024.2</td><td>Mistral[160]</td><td>Biomedicine</td><td>PubMed [108]</td><td>Continual pretraining</td></tr><tr><td> MentaLLaMA [406]</td><td>7B; 13B</td><td>2023.9</td><td>Instruction (LLaMA2 [339]) Tuning</td><td>Healthcare</td><td>IMHI dataset</td><td>RLHF; PEFT</td></tr><tr><td>AdaLM [414]</td><td>34M</td><td>2021.6</td><td>Distillation (BERT[83] or MiniLM[362])</td><td>Healthcare</td><td>PubMed [108]</td><td>Continual pretraining, Adapt-and-Distill</td></tr><tr><td>Rho-1 [210]</td><td>1B; 7B</td><td>2024.4</td><td>TinyLlama-1.1B [439], Mistral-7B[160]</td><td>Science (Math- ematics)</td><td>OpenWebMath [267]</td><td>Continual pretraining</td></tr><tr><td>ChemLLM [436]</td><td>7B</td><td>2024.4</td><td>Instruction Tuning (In- ternLM2)</td><td>Science (Chem- istry)</td><td>ChemData</td><td>Continual training and fine-tuning</td></tr><tr><td>SciGLM [435]</td><td>6B</td><td>2024.3</td><td>Instruction Tuning (ChatGLM-6B)</td><td>Science</td><td>SciInstruct</td><td>Self-reflective instruction annotation</td></tr><tr><td>Llemma [22]</td><td>7B</td><td>2023.10</td><td>Code Llama 7B</td><td>Science (Math- ematics)</td><td>Proof-Pile-2 [22]</td><td>Continual pre-training</td></tr><tr><td>OceanGPT [30]</td><td>2B； 7B； 14B</td><td>2023.10</td><td>LLaMA2 [339]</td><td>Science (Ocean)</td><td>Open-access litera- ture,DoINSTRUCT</td><td>Continual pre-training, In- struction tuning</td></tr><tr><td>AstroLLaMA [258]</td><td>7B</td><td>2023.9</td><td>Tuning (LLaMA-2-7B)</td><td>Science (As- tronomy)</td><td>arXiv abstracts from Kaggle</td><td>Continual traning</td></tr><tr><td>DARWIN [388]</td><td>7B</td><td>2023.8</td><td>LLaMa 7B</td><td>Science (physics, chem- istry, and material)</td><td>SciQ [375], Scien- tific paper[388], FAIR [388]</td><td>Fine-tuning</td></tr><tr><td>MindLLM [412]</td><td>1.3B; 3B</td><td>2023.10</td><td>From-scratch and Super- vised Fine-tuning</td><td>Law, Finance</td><td>Pile [108]，Wudao [424], CBooks</td><td>Train on Bilingual Mixture Data,SFT</td></tr></table>

5.2.2 SLMs for Science. SciGLM [435] is a collegiate-level scientific language model overcoming data scarcity with a self-reflective instruction annotation framework. Utilizing GPT-4 [2], it generates detailed reasoning for unlabeled scientific problems through three steps with designed prompts in Table ??: (i) CoT prompt for step-by-step answers (Prompt 1), (ii) reflective prompt for correcting errors (Prompt 2), and (iii) integrating the correct answer for clarity (Prompt 3). The SciInstruct dataset spans physics, chemistry, math, and proofs, tuning ChatGLM-6B’s [93] reasoning abilities. SciGLM boosts the base model’s (ChatGLM3-6B-Base) scientific QA accuracy by $3 . 0 6 \%$ on benchmarks such as CEval-Hard [149], CEval-Sci [149], MMLU-Sci [134], SciEval [319], and SciBench [363]. Llemma [22], an SLM derived from CodeLlama [292], specializes in mathematical reasoning. By continual pre-training, its 7B model is evolved on 55B tokens from the newly created Proof-Pile-2 dataset, which includes scientific papers, math web content, and mathematical code up until April 2023, to enhance few-shot capabilities. It excels in mathematical benchmarks like MATH [134], GSM8k [66], OCWCourses [186], MMLU-STEM [134], and SAT, surpassing all comparable open-weight models. ChemLLM [436] is a chemistry-focused language model that utilizes its proposed ChemData, a dataset designed for instruction tuning that transforms chemical data into dialogue format for training. ChemLLM is based on InternLM2-Base-7B [40], initially enhancing its language skills with a multi-corpus of 1.7 million Q&A pairs from Hugging Face, then fine-tunes using ChemData and the multi-corpus to maintain its general capabilities. ChemLLM excels in interdisciplinary chemical tasks within the proposed ChemBench, achieving results comparable to GPT-4 [2] and outperforming GPT-3.5 with a score of 92.6 in Mol2caption, slightly below that of GPT-4. AstroLLaMA [258] introduces an astronomy-focused language model. Based on Llama-2-7B [339] and enhanced via continual pre-training, it has been developed using over 300K astronomy abstracts from arXiv 11. AstroLLaMA achieves $3 0 \%$ lower perplexity than Llama-2-7B, indicating substantial improvements in domain adaptability. AstroLLaMA is available 12 for tasks such as automated paper summarization and conversational agent development in astronomy.

Table 11. Prompts for self-reflective instruction annotation framework   

<table><tr><td>Chain-of-Thought</td><td>[Prompt 1] The folowing input consists of a science problem, please generate an elaborate step- by-step solution to the problem.</td></tr><tr><td>ReflectiveGeneration</td><td>[Prompt 2] The following input comprises a science problem and a corresponding solution. However, this solution is incorrect, please reflect on its errors and then generate a correct step-by- step solution to the problem.</td></tr><tr><td>Prompt Answer</td><td>[Prompt 3] The following input consists of a science problem, a corresponding solution, and the real answer. The given solution is incorrect, please reflect on its errors and then generate a correct step-by-step solution to the problem based on the real answer.</td></tr></table>

5.2.3 SLMs for Finance and Law. MindLLM [412] introduces a bilingual (Chinese and English) SLM, pretrained on the Pile dataset [108] for English and WuDao [424], CBook, and various Chinese web content for Chinese. Bilingual training enhances capacity and prevents catastrophic forgetting. It explores specific domains such as law and finance through supervised fine-tuning. In law, it utilizes publicly available legal data, scenario-based Q&A from LaW-GPT [139], and NLP-based legal tasks from DISC-LawLLM [427]. In finance, EastMoney 13 is selected as the data source.

Insights: We draw several key insights from the development of domain-specific SLMs:

• Adapting SLMs to domain-specific data is a common practice for acquiring domain-specific SLMs, prompting many to create their datasets [258, 406, 435, 436]. These datasets are often annotated using LLMs like GPT-4 and used to continual pre-train or fine-tune general models such as LLaMa-2-7B [3, 34]. To ensure the data quality, specialized annotation frameworks are developed, such as SciGLM [435].   
• In domains with abundant corpora, training a general model from scratch and fine-tuning it using SFT [412] is practical. Bilingual settings during training can prevent catastrophic forgetting.   
• Distilling general capabilities from LLMs while integrating domain-specific knowledge from corpora is another method for developing domain-specific SLMs [414].

Table 12. SLMs help LLMs in different aspects   

<table><tr><td>Aspect</td><td>Representative work</td><td>Key point</td></tr><tr><td rowspan="9"> LM for relablee</td><td>APRICOT [343]</td><td>Trains a smallauxiliary model to predict LLM&#x27;s confidence using only textual inputs and outputs.</td></tr><tr><td>POLAR [450] Hallucination Detector in NMT</td><td>Using a BERT model to calibrate LLM responses. Using lightweight classifiers to detect hallucinations in Neural Ma-</td></tr><tr><td>[399]</td><td>chine Translation.</td></tr><tr><td>SAPIA</td><td></td></tr><tr><td>SuperICL [393]</td><td></td></tr><tr><td></td><td>SLM Plug-ins provide confidence and prediction for contextual ex- emplars to aid in-context learning.</td></tr><tr><td>SuperContext [408]</td><td>Specific SLM enhances ICL by providing confidence and predictions to overcome out-of-domain challenges.</td></tr><tr><td>Self-RAG [18]</td><td>A proxy model labels special tokens during RAG data generation for fine-tuning.</td></tr><tr><td>SKR [365]</td><td>Training a small model to detect its self-knowledge for better use of external knowledge.</td></tr><tr><td>SlimPLM [325]</td><td>Detecting missing knowledge in LLMs with a slim proxy model, enhancing the LLM&#x27;s knowledge integration.</td></tr><tr><td>In-Context RALM [287]</td><td>Training a RoBERTa-based reranker for top-k BM25 documents using LM signals to enhance LM gains.</td></tr><tr><td>CRAG [401]</td><td>Training a lightweight evaluator to assess document quality and trigger actions based on confidence levels.</td></tr><tr><td>GSR [148]</td><td>Traininga Generative Sub-graph Retriever (GSR) for relation chain in RAG when retrieving from knowledge graphs.</td></tr><tr><td>SLM for Prompt Extraction [443] extracting LLM</td><td>Small model trained to predict confidence of extracted system prompt from adversarial prompts.</td></tr><tr><td rowspan="4"> prompts</td><td>Prompt Stealing Attacks [297]</td><td>galst</td></tr><tr><td>Output2prompt [433]</td><td>Using a sparse encoder-decoder-based T5 small model to reverse-</td></tr><tr><td>Model Purifying [197]</td><td>engineer LLM inputs from outputs. Using SLMs to ensemble with LLMs, mitigating negative effects</td></tr><tr><td>LP[242]</td><td>from uncurated data. Learning Percentage as a difficulty metric.</td></tr><tr><td rowspan="4">SLM for Fine-tuning</td><td>Emulated Fine-tuning [246]</td><td></td></tr><tr><td>CROSSLM[79]</td><td> SLMs enhance LLMs by generating task-specific high-quality data.</td></tr><tr><td>Weak-to-Strong Search [453]</td><td>FramingLLMalignment asa test-time greedy search to maximize</td></tr><tr><td>SLCoLM[327]</td><td>the log-probability difference between tuned and untuned SLMs. Using SLM predictions to guide the LLM generation process in</td></tr><tr><td rowspan="3">SLM for LLM applications SLM for LLM</td><td>HEF [413]</td><td>CingseEtyM&#x27;sacersai</td></tr><tr><td>Contrastive decoding [198]</td><td>Enhancing text quality by maximizing the difference between expert</td></tr><tr><td>Llama Guard [153]</td><td>and amateur log probabilities. An LLM-based input-output safeguard model geared towards</td></tr><tr><td rowspan="2"> safety</td><td>SLM as Guardian [177]</td><td>Human-AI conversation use cases. Asmaller LLM for both harmful query detection and safeguard</td></tr><tr><td></td><td>response generation.</td></tr><tr><td rowspan="4">SLM for LLM evaluation</td><td>SLIDE [449]</td><td>Utilizing SLMs trained via contrast learning to distinguish and score responses in dialogue scenarios effectively.</td></tr><tr><td>Kuhn et al. [175]</td><td> An SLM is used as the natural language inference classifier.</td></tr><tr><td>SelfCheckGPT[240]</td><td>An SLM is used to calculate BERTScore.</td></tr><tr><td>Factscore [244]</td><td>An SLM functions as the natural language inference classifier.</td></tr></table>

# 6 SLMS FOR LLMS

In this section, we provide a comprehensive review of how SLMs enhance LLMs. While LLMs are robust, they face challenges such as latency during inference, labor-intensive fine-tuning, noise filtration issues in retrieval, suboptimal zero-shot performance, copyright infringement risks, and evaluation difficulties. SLMs can help LLMs to alleviate these issues. Research in this field can be categorized into five primary areas: (i) using SLMs for reliable LLM generation; (ii) extracting prompts for LLMs using SLMs; (iii) fine-tuning LLMs with SLMs; (iv) applying SLMs in LLM applications; (v) utilizing SLMs as guardian; and (vi) evaluating LLMs using SLMs. A summary of representative work in each category along with their key point is given in Table 12. Next, we introduce each category in detail.

# 6.1 SLM for Reliable LLM Generation

Although LLMs generally produce fluent and convincing text, they can occasionally generate erroneous responses [159, 349]. Additionally, LLMs are susceptible to privacy breaches from untrusted data collection, which can erode user trust or cause harm. To address these issues, recent studies have focused on using SLMs to calibrate LLM confidence, detect hallucinations, and improve retrieval-augmented LLMs and their reasoning capabilities.

![## Image Analysis: fd9856ea9edd8abf68bb3993615db91a48a01c7506699c07a2722ea28ad8f21a.jpg

**Conceptual Understanding:**
This image conceptually illustrates two distinct architectural patterns for improving the reliability and trustworthiness of Large Language Models (LLMs) by integrating them with Smaller Language Models (SLMs). The main purpose of the image is to demonstrate how an SLM can act as an auxiliary module to either calibrate the confidence of LLM outputs or detect potential hallucinations (factual errors or fabrications) within LLM generations.

Key ideas being communicated include:
*   **Enhancing LLM Trustworthiness:** Addressing common LLM limitations like overconfidence and hallucination.
*   **Role of SLMs:** Showcasing SLMs as specialized tools for specific verification tasks related to LLM output quality.
*   **Two-pronged Approach:** Presenting two different strategies (calibration and hallucination detection) each with distinct input requirements for the SLM.
*   **Information Flow:** Depicting how 'Questions', 'LLM Answers', and 'LLM Internal States' are processed and utilized by both the LLM and the SLM to achieve the desired reliability improvements.

**Content Interpretation:**
The image shows two distinct architectures for enhancing the reliability of Large Language Models (LLMs) using Smaller Language Models (SLMs). Both architectures illustrate a workflow where an initial 'Question' is processed by an 'LLM', and then both the 'Question' and specific outputs from the 'LLM' (either 'LLM Answers' or 'LLM Internal States') are fed into an 'SLM' for further analysis.

In the first architecture, (a) 'SLM-based Calibrator', the 'SLM' is used to evaluate the confidence of the 'LLM Answers' by taking the original 'Question' and the 'LLM Answers' as inputs. The output, 'Calibrated Confidence', represents a refined measure of the LLM's certainty.

In the second architecture, (b) 'SLM-based Hallucination Detector', the 'SLM' analyzes the 'Question' and 'LLM Internal States' to determine the presence of hallucinations. The output, 'Hallucination score', quantifies the likelihood or degree of hallucination in the LLM's generated content.

The significance of these processes lies in their ability to address critical challenges in LLM deployment: the need for reliable confidence estimations and the detection of generated falsehoods. The use of an SLM suggests a potentially more efficient or specialized model for these specific verification tasks, rather than relying solely on the LLM itself.

**Key Insights:**
The main takeaways from this image are:

1.  **Dual Applications of SLMs:** SLMs can be effectively leveraged for at least two critical functions in LLM reliability: calibration and hallucination detection.
2.  **Calibration Process:** For calibration, an SLM takes both the original 'Question' and the 'LLM Answers' as input to produce 'Calibrated Confidence'. This implies that the SLM is assessing the reliability of the LLM's response based on both the query and the generated answer.
3.  **Hallucination Detection Process:** For hallucination detection, an SLM utilizes the original 'Question' and the 'LLM Internal States' to generate a 'Hallucination score'. This suggests that analyzing internal states of the LLM, rather than just the final answer, can be crucial for identifying factual errors or non-existent information.
4.  **Modular Design:** Both architectures demonstrate a modular approach where the SLM acts as an auxiliary component, working in conjunction with the LLM to provide supplementary insights into its performance. This allows for specialized models (SLMs) to address specific quality concerns without altering the core LLM.
5.  **Information Flow:** The diagrams clearly show the flow of information, distinguishing between primary data flow (solid arrows) and secondary/feedback information (dashed arrows) directed towards the SLM. This highlights the dependency of the SLM on both the initial input and the LLM's output/internal workings for its specific task. These insights are directly evidenced by the explicit labels and directional arrows in both diagrams (a) and (b), detailing the inputs and outputs for each component ('Question', 'LLM', 'LLM Answers', 'LLM Internal States', 'SLM', 'Calibrated Confidence', 'Hallucination score').

**Document Context:**
This image directly supports the section '6.1 SLM for Reliable LLM Generation' by visually presenting two architectural approaches where a Smaller Language Model (SLM) is employed to improve the reliability of a Large Language Model (LLM). It provides concrete examples of how SLMs can be integrated into the LLM workflow to address key issues such as calibration (estimating confidence) and hallucination detection. The figure serves as a foundational visual aid, illustrating the theoretical frameworks discussed in the text, and detailing the input-output relationships for each proposed method. The diagrams clearly define the roles of 'Question', 'LLM', 'LLM Answers'/'LLM Internal States', 'SLM', and the final outputs ('Calibrated Confidence'/'Hallucination score'), making the concepts of enhancing LLM calibration and hallucination detection tangible for the reader.

**Summary:**
The image displays two distinct architectures, (a) an 'SLM-based Calibrator' and (b) an 'SLM-based Hallucination Detector,' which illustrate methods for enhancing the reliability of Large Language Models (LLMs). Both diagrams show how a Smaller Language Model (SLM) can be integrated with an LLM to perform specific tasks related to output quality.

In the 'SLM-based Calibrator' (a), a 'Question' is fed into an 'LLM', which then produces 'LLM Answers'. Simultaneously, the original 'Question' and the 'LLM Answers' are both directed as inputs to an 'SLM'. The 'SLM' processes these inputs to generate 'Calibrated Confidence', indicating a measure of the LLM's answer reliability.

In the 'SLM-based Hallucination Detector' (b), a 'Question' is also fed into an 'LLM'. In this case, the 'LLM' outputs 'LLM Internal States' rather than direct answers. Similar to the calibrator, the initial 'Question' and the 'LLM Internal States' are both provided as inputs to an 'SLM'. The 'SLM' then uses these inputs to calculate a 'Hallucination score', which quantifies the likelihood of the LLM generating factual errors or non-existent information.

Both architectures highlight the role of an SLM in augmenting LLM performance by either assessing confidence or detecting hallucinations, showcasing two distinct strategies for improving LLM output quality.](images/fd9856ea9edd8abf68bb3993615db91a48a01c7506699c07a2722ea28ad8f21a.jpg)
Fig. 20. Architectures of Enhancing Calibration and Hallucination Detection of LLMs.

Enhancing Calibration and Hallucination Detection of LLMs As illustrated in Figure 20 (a), to calibrate LLMs, an SLM processes both questions and LLM-generated answers to predict calibrated confidence. This training involves minimizing the discrepancy between estimated calibration error and predicted confidence score. For instance, APRICOT [343] uses an auxiliary DeBERTaV3 model [131] to assess LLM confidence in open-question scenarios, aiming to improve uncertainty expression and response adjustment. Similarly, POLAR [450] has developed a self-supervised approach that generates risk scores for each response to calibrate LLM confidence, utilizing a small BERT model [83] to synchronize LLM outputs with other weak supervision sources. As shown in Figure 20 (b), for hallucination detection, an SLM analyzes LLM internal states to output the likelihood of hallucination. This process uses supervised data obtained by testing the knowledge boundaries of the LLM. In neural machine translation, Xu et al. [399] develop a lightweight detector that analyzes token contributions to hallucinations, outperforming both model-free baselines and quality estimation classifiers. Furthermore, SAPLMA [21] found that LLM internal states can signal the truthfulness of statements, with a small BERT classifier trained to differentiate correct from incorrect predictions achieving accuracies of $7 1 \%$ to $8 3 \%$ .

Enhancing Retrieval-Augmented Generation Generally, as shown in Figure 21, SLMs can also serve as proxy models to evaluate the familiarity of LLMs with user queries, determining whether LLMs need to retrieve additional information or can respond directly. For example, SlimPLM [325] is a small proxy model that assesses the necessity for LLM retrieval by generating heuristic answers. High-quality responses indicate that LLMs can handle queries independently, whereas lower-quality outputs require further retrieval. Additionally, Self-Knowledge Guided Retrieval (SKR) [365] enables SLMs to autonomously decide when LLMs should operate independently, based on their self-assessment of knowledge limitations. Further, SELF-RAG [18] improves the factual accuracy and quality of LLM outputs through on-demand retrieval and self-reflection. This method employs a small critic language model to issue reflective markers and make binary decisions regarding the need for further information retrieval. Moreover, some studies utilize SLMs to evaluate the relevance of retrieved documents. LongLLMLingua [162] employs SLMs to calculate the relevance of documents to a query $x ^ { q u e }$ using perplexity, as formalized by the equation:

![## Image Analysis: a799fdedf2acbfe0cf778e6b9e31f6394c944c22af26e4e01b46328d0a6c4a24.jpg

**Conceptual Understanding:**
This image conceptually illustrates an architecture for enhancing the reliability of Large Language Model (LLM) generations using a Small Language Model (SLM) as a "Heuristic RAG Prober" and a conditional retrieval mechanism. The main purpose is to show how an initial assessment by an SLM can determine whether an LLM can answer a question directly or if a Retrieval-Augmented Generation (RAG) approach, involving a search engine and retrieval documents, is required. It conveys the idea of an adaptive and efficient system that intelligently decides when to use external knowledge retrieval to improve answer quality and reliability for LLMs.

**Content Interpretation:**
The image depicts a conditional workflow for question answering using two language models (SLM and LLM) and a retrieval system.

*   **Initial Question Processing:** A "Question" is initially handled by an "SLM". This implies that the SLM performs a preliminary, potentially faster or less resource-intensive, analysis of the question.
*   **Heuristic Answers:** The "SLM" generates "SLM Heuristic Answers." The term "Heuristic" suggests these are preliminary, rule-of-thumb, or estimated answers, not necessarily definitive or fully comprehensive. This is explicitly stated by the arrow label "SLM Heuristic Answers".
*   **Retrieval Necessity Judgement:** These heuristic answers are then evaluated by the "Retrieval Necessity Judgement Model." This component acts as a gatekeeper, deciding if the existing SLM answer is sufficient or if more information is needed. This is a critical decision point.
*   **Conditional Pathways:**
    *   **"known" path:** If the judgement model determines the answer is "known" (sufficiently handled), the "LLM" receives the information directly. This suggests efficiency, bypassing costly retrieval if not needed, as indicated by the "known" arrow label.
    *   **"unknown" path (Retrieval Augmented Generation - RAG):** If the judgement model deems the answer "unknown" (requiring external context), it triggers the "Search Engine." This is explicitly shown by the dashed red arrow labeled "unknown."
*   **Retrieval Process:** The "Search Engine" then fetches external information. This information can be directly fed to the "LLM," or it can first go through "Retrieval Docs" (likely a database or corpus of documents) before being passed to the "LLM." The multiple dashed red arrows originating from the "Search Engine" and "Retrieval Docs" to the "LLM" highlight the flow of retrieved knowledge.
*   **Final LLM Generation:** Ultimately, the "LLM" processes all available information (either the initially "known" answer or the retrieved "unknown" context, or both) to produce the "LLM Answers." This demonstrates the LLM's role in synthesizing information to provide a final response.

The explicit text elements like "Question", "SLM", "SLM Heuristic Answers", "Retrieval Necessity Judgement Model", "known", "unknown", "Search Engine", "Retrieval Docs", "LLM", and "LLM Answers" clearly delineate the distinct stages and components of this sophisticated question-answering architecture. The use of "known" and "unknown" for the decision pathways highlights the adaptive nature of the system.

**Key Insights:**
**Main takeaways and insights:**

1.  **Adaptive Retrieval Strategy:** The system employs an adaptive strategy for retrieval. It doesn't always perform a search; instead, it uses a "Retrieval Necessity Judgement Model" to decide whether a "Search Engine" is needed based on "SLM Heuristic Answers." This suggests an optimization for resource usage and latency, as retrieval can be costly.
2.  **SLM as a Heuristic Prober:** The "SLM" plays a crucial role as an initial "Heuristic Prober." It provides "SLM Heuristic Answers" that act as preliminary indicators for the "Retrieval Necessity Judgement Model." This implies that a smaller, faster model can quickly assess the need for deeper processing or external knowledge.
3.  **Enhanced LLM Reliability:** The overall architecture aims for "Reliable LLM Generation." By conditionally augmenting the LLM with retrieved information only when necessary (the "unknown" path via "Search Engine" and "Retrieval Docs"), the system helps prevent the LLM from hallucinating or providing incorrect answers when its internal knowledge is insufficient.
4.  **Integration of RAG:** The "unknown" pathway clearly demonstrates the integration of Retrieval-Augmented Generation (RAG) principles. The "Search Engine" and "Retrieval Docs" are components of a RAG system, providing external, up-to-date, or specialized information to the "LLM."
5.  **Multi-Model Approach:** The system uses a multi-model approach involving both an "SLM" and an "LLM." The SLM is for initial heuristics and judgement, while the LLM is for final answer generation, indicating a division of labor that could improve overall performance.

**Evidence for these insights:**
*   "SLM Heuristic Answers" and "Retrieval Necessity Judgement Model" for the adaptive strategy and SLM's role.
*   "known" and "unknown" labels on the decision paths from the "Retrieval Necessity Judgement Model" explicitly show the conditional nature.
*   "Search Engine" and "Retrieval Docs" confirm the RAG integration when the answer is "unknown".
*   The overall context of the image, as part of a section "SLM for Reliable LLM Generation," implicitly supports the goal of enhancing LLM reliability through this architecture.

**Document Context:**
This image, titled "Architecture of SLM as a Heuristic RAG Prober," is situated within Section 6.1 "SLM for Reliable LLM Generation" of the document. It serves to visually explain a proposed system design that uses a Small Language Model (SLM) to improve the reliability of answers generated by a Large Language Model (LLM). Specifically, it details how the SLM acts as an initial "prober" to determine the necessity of external information retrieval (RAG) before the LLM generates its final answer, thereby enhancing the trustworthiness and accuracy of the LLM's output by mitigating issues like hallucination.

**Summary:**
The diagram illustrates a sophisticated system for generating reliable answers to user "Questions" by intelligently combining a Small Language Model (SLM), a Large Language Model (LLM), and a dynamic retrieval mechanism.

The process begins when a "Question" is posed. This question is first directed to an "SLM" (Small Language Model). The SLM quickly processes the question and generates "SLM Heuristic Answers," which are preliminary or best-guess responses.

These "SLM Heuristic Answers" are then fed into a central component called the "Retrieval Necessity Judgement Model." This model's crucial role is to evaluate whether the SLM's heuristic answers are sufficient to directly formulate a reliable response, or if additional, external information needs to be retrieved.

There are two distinct paths stemming from this judgement model:

1.  **"known" path:** If the "Retrieval Necessity Judgement Model" determines that the information is "known" (meaning the SLM's heuristic answer is sufficient or reliable enough), the process sends this information directly to the "LLM" (Large Language Model). The LLM then uses this to formulate its final answer.

2.  **"unknown" path (Retrieval Augmented Generation - RAG):** If the "Retrieval Necessity Judgement Model" determines the information is "unknown" (meaning the SLM's heuristic answer is insufficient or requires external validation), the process triggers a "Search Engine." This search engine is responsible for finding relevant external information.

Once the "Search Engine" is activated, it can provide information to the "LLM" in two ways:
    *   It can directly feed the retrieved information to the "LLM."
    *   Alternatively, the "Search Engine" can populate "Retrieval Docs" (a collection of relevant documents), and these "Retrieval Docs" then feed their content into the "LLM." This approach ensures that the LLM has access to comprehensive and verified external knowledge when needed.

Finally, the "LLM" consolidates all the information it receives—either directly from the "Retrieval Necessity Judgement Model" (if "known") or through the "Search Engine" and "Retrieval Docs" (if "unknown")—to generate the ultimate "LLM Answers." This architecture ensures that the LLM's answers are robust and reliable, especially when dealing with complex or less common questions, by conditionally leveraging external knowledge retrieval.](images/a799fdedf2acbfe0cf778e6b9e31f6394c944c22af26e4e01b46328d0a6c4a24.jpg)
Fig. 21. Architecture of SLM as a Heuristic RAG Prober. Manuscript submitted to ACM

$$
r _ { k } = - \frac { 1 } { N _ { c } } \sum _ { i } \log \phi _ { ^ { \mathrm { S L M } } } ( x _ { i } ^ { q u e } | x _ { k } ^ { d o c } ) , \quad k \in \{ 1 , 2 , . . . , K \}
$$

where $x _ { i } ^ { q u e }$ is the $i \cdot$ -th token in the query sequence, $x _ { k } ^ { d o c }$ is the retrieved document, and $N _ { c }$ is the total number of tokens in the query. $ { p _ { \mathrm { S L M } } }$ represents the probability generated by an SLM. CRAG [401] employs SLMs as evaluators of document relevance in the same way. RA-ISF [219] trains a small language model that checks the base LLM in self-knowledge, relevance judgment, and question decomposition. In addition, some research employs SLMs as re-rankers to refine the order of documents provided by initial retrieval efforts such as BM25 [291]. In-Context RALM [287] positions SLMs as rankers, optimizing the document sequence with a fine-tuning process on RoBERTa [218] as defined by the loss function:

$$
\operatorname* { m i n } _ { r a n k e r } \sum _ { i = 1 } ^ { k } - \log p _ { \mathrm { r a n k } } ( d _ { i } | \boldsymbol x _ { \le s _ { j } } ) \cdot p _ { \theta } ( \boldsymbol y | d _ { i } ; \boldsymbol x _ { \le s _ { j } } )
$$

where $x _ { \leq s _ { i } }$ is a prefix sampled from the training data, $y = x _ { s _ { i } + 1 } , . . . , x _ { s _ { i } + s }$ represents the text to be generated in the next stride, $p _ { \theta } ( y | d _ { i } ; x _ { \le s i } )$ denotes the probability of the LLM generating $y$ given $d _ { i }$ and $x { \le } s i$ , and $p _ { \mathrm { r a n k } } ( d _ { i } | \boldsymbol x _ { \le s _ { j } } )$ is the ranking score of $d _ { i }$ . Lastly, some studies leverage SLMs to retrieve sub-graphs when utilizing knowledge graphs as external sources. Huang et al. [148] introduces the Generative Sub-graph Retriever (GSR), which employs SLMs to predict relation chains for answering questions, offering a cost-effective alternative to training LLMs. Specifically, it uses customized T5 (220M, 770M, and 3B) [284] as retrievers to enhance LLM readers, including Llama2-chat-7B [339] and Llama3-instruct-8B [94], on the WebQSP [418] and CWQ [324] datasets.

Enhancing Reasoning Capabilities of LLMs As illustrated in Figure 22, SLMs enhance LLMs reasoning by transferring task knowledge to in-context examples, effectively reducing hallucinations. While In-context Learning (ICL) generally handles few-shot learning

![## Image Analysis: ae6045a43b98b5c6bb8da34d5ac3c583c0ff5d17ee9737e665173841d5bfaff3.jpg

**Conceptual Understanding:**
This image conceptually represents a system architecture or process flow for enhancing the reliability of Large Language Model (LLM) outputs using a Small Language Model (SLM). The main purpose is to illustrate how an SLM can process initial inputs (examples and a question) and provide refined, knowledge-rich signals to an LLM, thereby improving the LLM's ability to generate accurate answers. It conveys the idea that an SLM acts as an intelligent pre-processor, adding value and structure to the input before it reaches the more powerful, but potentially less steerable, LLM, particularly in the context of in-context learning (ICL).

**Content Interpretation:**
The image displays a conceptual data flow diagram illustrating how a Small Language Model (SLM) enhances the input for a Large Language Model (LLM) to achieve more reliable generations. The core concept is that the SLM acts as an intermediary, processing initial inputs ('Examples' and 'Question') to generate structured information that guides the LLM. The two distinct outputs from the SLM, one including 'Ground Truth' and another without, suggest different modes or stages of interaction or evaluation. The overall system shows a method for transferring 'knowledge' from the SLM to the LLM through a refined input format, aiming to improve the LLM's 'Answers'. This setup is particularly relevant for in-context learning (ICL) scenarios where examples and a question are used to prompt an LLM.

**Key Insights:**
The main takeaways from this image are: 1. An SLM can be leveraged to process initial inputs ('Examples' and 'Question') before feeding them to an LLM. This is evidenced by the 'Examples' and 'Question' arrows pointing to 'SLM', and 'SLM' then outputting to 'LLM'. 2. The SLM generates structured information for the LLM, including 'Predicted Label' and 'Confidence'. This is shown by the arrow labels 'Question | Predicted Label | Confidence | Ground Truth' and 'Question | Predicted Label | Confidence'. 3. The SLM can incorporate 'Ground Truth' information in its output, potentially for training, validation, or specific inference modes. This is specifically highlighted by one output path including 'Ground Truth'. 4. The ultimate goal of this setup is to produce 'LLM Answers' which are implicitly made more reliable due to the structured input from the SLM, supporting the section's title 'SLM for Reliable LLM Generation'. The image outlines a mechanism for 'SLM transfers knowledge into ICL' by demonstrating how an SLM provides richer, pre-analyzed input to the LLM.

**Document Context:**
This image is presented in Section 6.1, titled 'SLM for Reliable LLM Generation,' and is specifically referenced as 'Fig. 22. SLM transfers knowledge into ICL.' This context explicitly states the image's purpose: to depict how a Small Language Model (SLM) facilitates knowledge transfer for In-Context Learning (ICL) within an LLM generation process. The diagram visually supports the document's narrative by showing the practical implementation of using an SLM to pre-process or enrich inputs for an LLM, thereby making the LLM's outputs more reliable. It provides a visual explanation of the proposed architecture or mechanism described in the surrounding text, detailing the flow of information and the specific components involved.

**Summary:**
The image illustrates a process for reliable Large Language Model (LLM) generation using a Small Language Model (SLM) for knowledge transfer into In-Context Learning (ICL). The process begins with two inputs: 'Examples' and a 'Question'. Both of these inputs are fed into the 'SLM' (Small Language Model). The 'SLM' then generates two distinct outputs, both of which are directed to the 'LLM' (Large Language Model). The first output from the 'SLM' is labeled 'Question | Predicted Label | Confidence | Ground Truth'. The second output from the 'SLM' is labeled 'Question | Predicted Label | Confidence'. The 'LLM' receives both of these detailed outputs from the 'SLM' as its input. Finally, the 'LLM' processes this information and produces 'LLM Answers'. This diagram visually represents how an SLM enriches the input for an LLM by providing structured information, including predictions and confidence, and in one case, ground truth, to facilitate more reliable answers.](images/ae6045a43b98b5c6bb8da34d5ac3c583c0ff5d17ee9737e665173841d5bfaff3.jpg)
Fig. 22. SLM transfers knowledge into ICL.

with 16 to 32 examples, it struggles when faced with extensive supervised data. SLMs, specialized in task-specific training, complement the broader domain knowledge of extensively pre-trained LLMs. For example, SuperICL [393] incorporates SLMs as plugins for efficiently executing supervised tasks. It predicts labels for contextual examples and integrates these predictions with the input text and actual labels to enhance knowledge transfer, thereby boosting the understanding and responsiveness of LLMs. SuperContext [408] tackles challenges that LLMs encounter with new tasks and out-of-distribution data in natural language understanding by synergizing SLM outputs with LLM prompts during inference. This integration merges model predictions with their confidence levels, effectively leveraging SLM task-specific knowledge and LLM domain expertise. Furthermore, SLMs efficiently decompose complex reasoning by breaking tasks into simpler components, as demonstrated in [383]. This strategy increases efficiency and reduces deployment costs when SLMs and LLMs are used collaboratively, transforming complex tasks into manageable segments.

![## Image Analysis: 9c3657c5bb8abd7484009440e98895b181ebe7f9d445c79c6060797acd00bc03.jpg

**Conceptual Understanding:**
The image conceptually represents different architectural patterns or methodologies for 'prompt extraction' from Large Language Models (LLMs) using Small Language Models (SLMs). The core idea is to reverse-engineer the input prompt given an LLM's output. The main purpose is to demonstrate various approaches to this problem, highlighting the specific roles and interactions between LLMs and SLMs in each scenario.

Key ideas being communicated include:

*   **The utility of SLMs:** SLMs are presented as crucial components for analyzing LLM outputs and inferring prompts, performing roles such as estimation, parameter extraction, encoding, and decoding.
*   **Prompt extraction as a multi-faceted problem:** The existence of three distinct paradigms (Likelihood Estimation, Parameter Extraction, and Direct Model Inversion) indicates that prompt extraction is not a monolithic task and can be approached from different angles, leveraging different computational strategies.
*   **Interplay between different model sizes:** The diagrams show how LLMs and SLMs can work in concert, with LLMs sometimes acting as the 'service provider' or 'reconstructor', and SLMs typically performing analytical or transformation tasks to deduce the prompt.

**Content Interpretation:**
The image, titled "Fig. 24. SLM for LLM Prompt Extraction Paradigm," illustrates three distinct conceptual frameworks for extracting prompts from Large Language Models (LLMs) using Small Language Models (SLMs). Each sub-diagram (a), (b), and (c) represents a different methodology or 'paradigm' for achieving this goal.

**Paradigm (a): Prompt Likelihood Estimation**
This paradigm shows a process where "Language model outputs" are combined with "Attack queries ①②③ ..." and fed into a "Service Provider M_L" (an LLM). This LLM then produces "Extracted prompts ①②③ ...", suggesting multiple potential prompts. An "SLM-based Estimation M_S" (an SLM) then takes these multiple extracted prompts and selects the most likely one, resulting in a single "Extracted prompt ①". This process signifies an iterative or comparative approach where an SLM evaluates the likelihood of various potential prompts generated by an LLM in response to attack queries.

**Paradigm (b): Prompt Parameter Extractor**
This paradigm depicts a two-stage process. "Language model outputs" are first fed into a "Parameter Extractor M_S" (an SLM). This SLM's role is to identify or extract parameters from the LLM outputs. The output from the SLM is then passed to a "Prompt Reconstructor M_L" (an LLM), which uses these parameters to reconstruct the original "Extracted prompt". This suggests that the SLM is used to pre-process the LLM output to identify key characteristics or parameters, which are then used by an LLM to formulate the actual prompt.

**Paradigm (c): Direct Model Inversion**
This paradigm shows a direct, two-stage inversion process using only SLMs. "Language model outputs" are first fed into an "Encoder M_S1" (an SLM), which presumably transforms the output into an intermediate representation. This encoded representation is then fed into a "Decoder M_S2" (another SLM), which directly inverts the process to produce the "Extracted prompt". This highlights an end-to-end SLM-based approach to invert the LLM's generation process to recover the input prompt.

**Key Insights:**
The image provides several key takeaways regarding SLM-based prompt extraction strategies:

1.  **Multiple Prompt Extraction Paradigms Exist:** There isn't a single method for prompt extraction; at least three distinct strategies are presented: Likelihood Estimation, Parameter Extraction, and Direct Model Inversion.
    *   *Evidence:* The presence of three separate sub-diagrams, each with a unique title: "(a) Prompt Likelihood Estimation", "(b) Prompt Parameter Extractor", and "(c) Direct Model Inversion".

2.  **SLMs Play Diverse Roles in Prompt Extraction:** SLMs (M_S, M_S1, M_S2) are not merely used for a single task but perform different functions depending on the paradigm.
    *   *Evidence:* In (a), the SLM is an "SLM-based Estimation M_S" selecting the most likely prompt. In (b), the SLM is a "Parameter Extractor M_S". In (c), SLMs act as an "Encoder M_S1" and a "Decoder M_S2" for direct inversion.

3.  **LLMs Can Serve as Both Targets and Tools:** LLMs (M_L) are the source of the outputs from which prompts are extracted, but they can also be used as components in the extraction process itself.
    *   *Evidence:* In (a), the LLM is the "Service Provider M_L" whose "Language model outputs" are analyzed. In (b), an LLM acts as a "Prompt Reconstructor M_L".

4.  **Prompt Extraction Can Involve Multiple Steps:** Some paradigms show a multi-stage process for extracting prompts.
    *   *Evidence:* Paradigm (a) involves an LLM and then an SLM. Paradigm (b) involves an SLM followed by an LLM. Paradigm (c) involves two sequential SLMs (Encoder and Decoder).

5.  **Output of Extraction Can Be Refined:** In some cases, the extraction process aims to refine multiple potential prompts into a single, most probable one.
    *   *Evidence:* In (a), the LLM initially produces "Extracted prompts ①②③ ...", but the subsequent SLM refines this to a single "Extracted prompt ①". Other paradigms directly produce a single "Extracted prompt".

**Document Context:**
This image is crucial for understanding the methodologies proposed in Section 6.1, titled "SLM for Reliable LLM Generation." The text immediately following the image explicitly defines the components ($M_S$ for small language models and $M_L$ for large language models) and provides a concise summary of each paradigm:

*   **(a) SLM-based prompt estimation:** Explains how $M_S$ selects the most likely prompt from various attack prompts. This aligns perfectly with the diagram showing multiple "Extracted prompts ①②③ ..." going into "SLM-based Estimation M_S" to yield "Extracted prompt ①".
*   **(b) SLM-based Parameter Extractor:** States that this identifies the type of input prompt. The diagram shows "Parameter Extractor M_S" feeding into "Prompt Reconstructor M_L" to get the "Extracted prompt", supporting the idea of identifying prompt parameters.
*   **(c) SLM-based Model Inversion:** Describes $M_S$ as inverting the LLM output back into the input. The diagram illustrates an "Encoder M_S1" and "Decoder M_S2" (both SLMs) directly leading to an "Extracted prompt" from "Language model outputs", which is a clear representation of model inversion.

Thus, the image visually details the three core approaches discussed in the surrounding text, providing concrete architectural diagrams for how SLMs can be leveraged for prompt extraction, which is foundational to ensuring reliable LLM generation by understanding their inputs.

**Summary:**
The image illustrates three distinct paradigms for SLM-based prompt extraction from Large Language Models (LLMs), as presented in Section 6.1 "SLM for Reliable LLM Generation". Each paradigm demonstrates a different approach using Small Language Models (SLMs) to either estimate, extract parameters for, or directly invert LLM outputs to recover the original prompts. The diagram clearly separates these three methods with dashed vertical lines. Each paradigm starts with an input, processes it through one or more language models, and concludes with an extracted prompt.

**Paradigm (a): Prompt Likelihood Estimation**
This process begins with "Language model outputs + Attack queries ①②③ ...". These inputs are fed into an "LLM" labeled "Service Provider M_L". The output from this LLM consists of "Extracted prompts ①②③ ...". These multiple extracted prompts are then input into an "SLM" labeled "SLM-based Estimation M_S". The final output of this stage is a single "Extracted prompt ①", indicating the most likely prompt identified by the SLM.

**Paradigm (b): Prompt Parameter Extractor**
This paradigm starts with "Language model outputs". These outputs are first processed by an "SLM" labeled "Parameter Extractor M_S". The output of this SLM is then fed into an "LLM" labeled "Prompt Reconstructor M_L". The ultimate result of this sequence is a single "Extracted prompt".

**Paradigm (c): Direct Model Inversion**
This approach also begins with "Language model outputs". These outputs are sequentially passed through two "SLM" components. The first SLM is labeled "Encoder M_S1", and its output is then fed into the second SLM, which is labeled "Decoder M_S2". This two-stage SLM process directly yields an "Extracted prompt".

In summary, the image details three methodologies for leveraging SLMs to deduce or recover prompts that were likely used to generate certain LLM outputs, varying from a two-stage estimation process to a direct inversion technique, showcasing the versatility of SLMs in this context.](images/9c3657c5bb8abd7484009440e98895b181ebe7f9d445c79c6060797acd00bc03.jpg)
Fig. 24. SLM for LLM Prompt Extraction Paradigm. $M _ { S }$ denotes small language models and $M _ { L }$ denotes large language models. (a) SLM-based prompt estimation tries various attack prompts; $M _ { S }$ selects the most likely extracted one. (b) SLM-based Parameter Extractor identifies the type of input prompt. (c) SLM-based Model Inversion uses $M _ { S }$ to invert the LLM output back into the input.

Alleviate Copyright and Privacy Issues of LLMs LLMs pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. As depicted in Figure 23, SLMs can assist LLMs in addressing copyright and privacy concerns arising from online data collection. By training on

![## Image Analysis: 626c7e9d43b4aab04b75e0c85a0ebd44a849b1c90de29088e83b6acbc899b0f8.jpg

**Conceptual Understanding:**
The image conceptually represents an architectural design for enhancing the reliability and data protection capabilities of Large Language Models (LLMs) by integrating them with Small Language Models (SLMs) through an ensembling approach. Its main purpose is to demonstrate a methodology where the distinct strengths of LLMs (e.g., generation performance) and SLMs (e.g., data protection features like privacy and resistance to poisoning) can be combined to achieve a more robust and secure AI system. The key idea communicated is that a simple ensemble of a large, performant model and a smaller, specialized protective model can yield a superior outcome compared to using either model in isolation, particularly focusing on the crucial aspects of data security and compliance in LLM generation.

**Content Interpretation:**
The image shows a system architecture that combines the strengths of a Large Language Model (LLM) and a Small Language Model (SLM) through an ensembling technique to achieve robust data protection alongside performance. The radar charts illustrate the performance profiles of individual models and the combined 'Ensemble' across five critical dimensions:

1.  **Copyright Compliance:** Indicated by a copyright symbol (©).
2.  **Protection against Data Poisoning:** Indicated by a shield symbol.
3.  **Protection of Privacy:** Indicated by a person's head with a shield symbol.
4.  **Generation Performance:** Indicated by a lightning bolt symbol.
5.  **Other Standard Performance:** Indicated by a wrench symbol.

The LLM, being 'Large scale (e.g. 15B)', is shown to excel in 'Generation Performance' (larger orange area on that axis). The SLM, being 'Small scale (e.g. 3B)', appears to offer stronger 'Protection against Data Poisoning' and 'Protection of Privacy' (larger yellow area on those axes). The bar charts signify the outputs or distributions from each model. The 'α' and '1-α' labels represent weighting factors used to combine the outputs of the LLM and SLM, respectively, into the 'Ensemble' model. This ensembling process aims to create a combined model whose green radar chart indicates a balanced improvement across all metrics, suggesting that the ensemble mitigates the weaknesses of individual models by leveraging their complementary strengths, particularly enhancing data protection aspects while maintaining good generation performance. The final output is a more reliable and protected LLM generation system.

**Key Insights:**
**Main Takeaways:**
1.  **Complementary Strengths:** LLMs excel in 'Generation Performance' (as shown by the orange shading on the large scale radar chart), while SLMs can offer superior 'Protection against Data Poisoning' and 'Protection of Privacy' (as shown by the yellow shading on the small scale radar chart).
2.  **Ensembling for Balance:** Combining LLM and SLM outputs through weighted ensembling (using 'α' and '1-α') allows for the creation of a system ('Ensemble') that balances the strengths of both models.
3.  **Enhanced Data Protection:** The architecture highlights that SLM plays a crucial role in boosting data protection aspects, which is vital for 'Reliable LLM Generation'. The final green radar chart for the 'Ensemble' suggests a more comprehensive and balanced performance, implying that the combined system achieves better overall security and utility than individual models.

**Insights:**
*   The specific 'e.g. 15B' and 'e.g. 3B' indicate the parameter scales, suggesting that even smaller models can contribute significantly to specialized aspects like data protection.
*   The use of 'α' and '1-α' implies a configurable trade-off or optimization between the contributions of the LLM and SLM, allowing the system to be tuned for specific requirements.

**Textual Evidence:**
*   'Large scale (e.g. 15B)' LLM with strong 'Generation Performance' (orange shading).
*   'Small scale (e.g. 3B)' SLM with strong 'Protection against Data Poisoning' and 'Protection of Privacy' (yellow shading).
*   The labels 'α' and '1-α' explicitly show the weighted combination.
*   The central 'Ensemble' box represents the combined model.
*   The final green radar chart with a more balanced shaded area visually confirms the integrated performance improvement across 'Copyright Compliance', 'Protection against Data Poisoning', 'Protection of Privacy', 'Generation Performance', and 'Other Standard Performance'.

**Document Context:**
This image directly supports the document section '6.1 SLM for Reliable LLM Generation' by visually presenting the architectural approach. It illustrates how an SLM can be integrated with an LLM to enhance data protection, which is a critical aspect of 'reliable LLM generation'. The diagram explains the mechanism for combining these models and demonstrates the expected improved performance profile of the 'Ensemble' in terms of both general performance and specific protection criteria, thus providing a concrete architectural solution to the problem discussed in the text.

**Summary:**
This image illustrates the architecture of SLM-based data protection, showing how a Large Language Model (LLM) and a Small Language Model (SLM) can be combined into an 'Ensemble' to achieve a desired performance and protection profile. The process begins with two distinct models: an LLM and an SLM. Each model is associated with a radar chart indicating its performance across five key metrics: 'Copyright Compliance', 'Protection against Data Poisoning', 'Protection of Privacy', 'Generation Performance', and 'Other Standard Performance'.

The 'Large scale (e.g. 15B)' LLM is depicted with an orange shaded area on its radar chart, showing a strong 'Generation Performance' but potentially lesser performance in protection aspects. The 'Small scale (e.g. 3B)' SLM is depicted with a yellow shaded area, which appears to emphasize 'Protection against Data Poisoning' and 'Protection of Privacy' more prominently.

Both the LLM and SLM independently produce outputs, represented by a series of vertical bars (histograms or distributions). These outputs are then fed into an ensembling mechanism. The LLM's output is weighted by 'α', and the SLM's output is weighted by '1-α', indicating a weighted combination. These weighted outputs are combined into an 'Ensemble' model. The final output of the 'Ensemble' is represented by a third radar chart, colored green, which shows a balanced performance profile across all five metrics, aiming to leverage the strengths of both the LLM and SLM for a comprehensive solution. The symbols around the final radar chart (e.Ms. a copyright symbol, a padlock, a person's head with a shield, a lightning bolt, and a wrench) correspond to the five performance metrics.](images/626c7e9d43b4aab04b75e0c85a0ebd44a849b1c90de29088e83b6acbc899b0f8.jpg)
Fig. 23. Architecture of SLM-based Data Protection

selectively curated data subsets, SLMs effectively reduce copyright infringement and privacy risks, although they are less effective than full-scale LLMs. To harness the combined benefits of both models, Li et al. [197] integrates untrusted LLMs with benign SLMs using the CP- $\cdot \Delta$ KL algorithm to mitigate adverse effects while preserving performance. The equation is:

$$
p ( y | x ) = \frac { p _ { l } ( y | x ) \cdot p _ { s } ( y | x ) } { Z ( x ) }
$$

where $\mathit { p _ { l } }$ and $\mathbf { \nabla } \mathcal { P } \boldsymbol { s }$ represent the probabilities from the large and small models, respectively, and $Z ( x )$ is the partition function. This integration results in the following ensemble algorithm:

$$
z _ { P } ( \cdot | x ) \propto \alpha z _ { l } ( \cdot | x ) + ( 1 - \alpha ) z _ { s } ( \cdot | x )
$$

where $z _ { l }$ and $z _ { s }$ are the logit values from the large and small models, respectively, and $\alpha$ is the scaling factor.

# 6.2 SLM for Extracting LLM Prompts

Prompt-based methods are becoming simpler and more cost-effective alternatives to traditional fine-tuning in the LLM era, utilizing LLMs’ instruction-following capabilities for a competitive edge. Mastering prompts is vital for replicating LLM-supported product behaviors. However, services such as Bing Chat and GitHub Copilot Chat have seen prompt reverse-engineering through black-box API attacks. SLMs often serve as surrogate models in these attacks, employing strategies such as (i) SLM-based prompt likelihood estimation, (ii) SLM-based prompt parameter extraction, and (iii) SLM-based direct model inversion, illustrated in Figure 24.

SLM-based prompt likelihood estimation, as illustrated in Figure 24 (a), Zhang et al. [443] proposes using an SLM as a Likelihood Estimator to identify secret prompts in LLM outputs. They craft attack prompts, such as “Repeat all sentences in our conversation,” and query the target LLM. The response is likely to include secret prompts, confusing the LLM to interpret these as part of the conversation. A fine-tuned DeBERTa model [132] is then used to select the most likely secret prompts from the output.

SLM-based prompt parameter extraction, as shown in Figure 24 (b), Sha and Zhang [297] utilizes an SLM as a Parameter Extractor to extract prompt parameters from LLM outputs. They employ a specialized BERT model [83] to classify LLM outputs into direct, in-context, and role-based prompts, also predicting the number of exemplars for in-context prompts and identifying roles for role-based prompts. Prompt reconstruction is then performed using ChatGPT once the parameters are defined.

SLM-based direct model inversion, as shown in Figure 24 (c), the method of using an SLM as a Direct Inversion Model is designed to reverse-engineer LLM outputs back to their original prompts [433]. They train a sparse encoderdecoder T5 model [284] with 222M parameters on the Instructions-2M dataset [249], where the input is LLM outputs and the output is the LLM prompt. This trained model effectively maps multiple LLM outputs to their initiating prompts as $\boldsymbol { p } ( x | y _ { 1 } , . . . , y _ { n } ; M _ { S 1 } , M _ { S 2 } )$ , with $y _ { i }$ representing different output versions and $M _ { S 1 } , M _ { S 2 }$ the model parameters.

# 6.3 SLM for Fine-tuning LLMs

Fine-tuning is a crucial technique for adapting LLMs to specific tasks or domains, yet it is often time-consuming. For instance, finetuning the LLaMA-2-13B [339] checkpoint on 32 NVIDIA A100 GPUs with 80GB memory using bfloat16 format requires approximately 70 hours [247]. This process also demands high-quality data. Therefore, we examine how SLMs can enhance LLM finetuning through three approaches: (i) proxy fine-tuning, (ii) selecting high-quality data, and (iii) guiding LLM-generated task data, as illustrated in Figure 25.

![## Image Analysis: dbb90c927b541ccb1dbead5da7a8cacff664dcc463e7dcbbe77d6329cea8382d.jpg

**Conceptual Understanding:**
This image conceptually represents various strategies for leveraging Small Language Models (SLMs) to enhance the performance and efficiency of Large Language Models (LLMs), particularly within the domain of fine-tuning and data management. The main purpose of the image is to illustrate three distinct, yet interconnected, ways SLMs can be integrated into the LLM lifecycle. These include approximating fine-tuning behaviors on a large scale, assisting in intelligent data selection for more effective training, and facilitating the generation of high-quality, task-specific synthetic data through a feedback mechanism. The image effectively conveys the idea that SLMs are not just scaled-down versions of LLMs, but valuable tools that can perform specialized tasks to support and improve larger models.

**Content Interpretation:**
The image illustrates three distinct mechanisms through which Small Language Models (SLMs) can play a crucial role in enhancing Large Language Model (LLM) performance, particularly in the context of fine-tuning and data management. 

**Sub-figure (a) 'SLM approximates the fine-tuning on a large scale'** demonstrates how SLMs can be used to model or approximate the fine-tuning process of an LLM. It shows a 'Question' being processed by both fine-tuned (FT SLM) and un-fine-tuned (UFT SLM) versions of an SLM, producing outputs $M_{S}^{SFT}$ and $M_{S}^{UFT}$ respectively. Simultaneously, the 'Question' is processed by an 'LLM', yielding output $M_{L}$. The differences and sums of these outputs are then aggregated, as indicated by the subtraction and addition operations, leading to a comparative visualization (bar chart with 'Paris', 'NYC', '1955'). This suggests that SLMs can provide insights into how fine-tuning affects model outputs and potentially simulate large-scale fine-tuning effects without directly fine-tuning the full LLM.

**Sub-figure (b) 'SLM helps data selection'** depicts the SLM's utility in curating effective training data for an LLM. The SLM analyzes 'Training samples' to identify 'Training difficulties'. Concurrently, 'Hard Training Samples' are prepared for the 'LLM'. The crucial step is the 're-order and filter' action, which is informed by the 'Training difficulties' identified by the SLM. This indicates that SLMs can intelligently pre-process or select challenging training examples, optimizing the data presented to the LLM for more efficient and effective learning.

**Sub-figure (c) 'SLMs promote the LLM to generate task-specific high-quality data'** illustrates a generative feedback loop. 'Labels' and 'Label-descriptive prompts' are used by an 'LLM' to produce 'Synthetic data', which is stored in a 'Synthetic dataset'. This 'Synthetic dataset' is then used to 'train' an 'SLM', along with a 'Local dataset'. The SLM, having been trained on this combined data, provides 'Feedback' to the 'LLM'. This signifies that SLMs can act as quality evaluators or guides, helping the LLM iteratively refine its synthetic data generation process to produce 'task-specific high-quality data'. The text elements clearly outline a closed-loop system where SLMs validate and guide the LLM's creative process.

**Key Insights:**
The main takeaways from this image are: 

1.  **SLMs can approximate or simulate LLM fine-tuning behavior at scale.** As shown in (a), different states of an SLM (FT SLM and UFT SLM) can process a 'Question' to yield outputs ($M_{S}^{SFT}$, $M_{S}^{UFT}$) that, when compared with the LLM's output ($M_{L}$), help understand the impact of fine-tuning. The use of subtraction and addition operations on these outputs, leading to a bar chart with specific location labels ('Paris', 'NYC') and a numerical value ('1955'), suggests a quantitative analysis or approximation of large-scale fine-tuning effects.

2.  **SLMs are effective tools for intelligent data selection and curation for LLMs.** Sub-figure (b) illustrates that an 'SLM' can analyze 'Training samples' to identify 'Training difficulties'. This insight is then used to 're-order and filter' 'Hard Training Samples' before they are used to 'train' an 'LLM'. This highlights the capability of SLMs to optimize the training data, focusing the LLM on more challenging or relevant examples, thereby improving training efficiency and effectiveness.

3.  **SLMs can act as a feedback mechanism to promote LLMs in generating high-quality, task-specific synthetic data.** In sub-figure (c), an 'LLM' generates 'Synthetic data' based on 'Labels' and 'Label-descriptive prompts'. This synthetic data, along with a 'Local dataset', is used to 'train' an 'SLM'. The 'SLM' then provides 'Feedback' to the 'LLM'. This establishes a continuous improvement loop where the SLM, presumably acting as a discriminator or quality evaluator, guides the LLM to refine its synthetic data generation process, ensuring the output is 'task-specific high-quality data'.

These insights demonstrate the versatility of SLMs as supporting agents in the LLM ecosystem, going beyond just serving as smaller models to actively participate in the fine-tuning, data preparation, and synthetic data generation workflows for LLMs.

**Document Context:**
This image directly supports the document's section titled "6.3 SLM for Fine-tuning LLMs" by providing detailed visual explanations of how Small Language Models (SLMs) can be instrumental in various aspects of Large Language Model (LLM) fine-tuning and associated data processes. Each sub-figure (a), (b), and (c) describes a distinct mechanism or application of SLMs in this context. Sub-figure (a) relates to approximating fine-tuning outcomes, sub-figure (b) addresses data selection for effective training, and sub-figure (c) details how SLMs can drive the generation of high-quality synthetic data for LLMs. Together, these processes provide a comprehensive overview of how SLMs can be strategically integrated to optimize and improve LLM performance, directly aligning with the section's focus on leveraging SLMs for fine-tuning.

**Summary:**
The image, titled "SLM for LLM Fine-tuning," illustrates three distinct ways Small Language Models (SLMs) can be utilized to enhance Large Language Model (LLM) performance, specifically focusing on fine-tuning and data generation. 

**Sub-figure (a) "SLM approximates the fine-tuning on a large scale"** depicts a process initiated by a "Question". This question branches into three paths: 
1. To an "FT SLM" (Fine-Tuned Small Language Model) which produces an output denoted as "$M_{S}^{SFT}$". 
2. To a "UFT SLM" (Un-Fine-Tuned Small Language Model) which produces an output denoted as "$M_{S}^{UFT}$". 
3. Directly to an "LLM" (Large Language Model) which produces an output denoted as "$M_{L}$". 

The output from "$M_{S}^{SFT}$" and "$M_{S}^{UFT}$" are fed into a subtraction operation, indicated by a "-" symbol within a gray box. The output of this subtraction, along with "$M_{L}$", are then fed into an addition operation, indicated by a "+" symbol within a gray box. The final output of this addition is represented as a bar chart with labels "Paris", "NYC", and a specific value "1955" under "NYC", suggesting a comparison or aggregation of performance metrics.

**Sub-figure (b) "SLM helps data selection"** outlines how an SLM assists in curating training data for an LLM. "Training samples" (represented by a blue cylinder) serve as an "input" to an "SLM". This SLM identifies "Training difficulties". Separately, "Hard Training Samples" (represented by a red cylinder) are used to "train" an "LLM". The "Training difficulties" identified by the SLM inform a process to "re-order and filter" the data before it's used by the LLM, suggesting a mechanism to prioritize or refine the hard training samples based on SLM's analysis.

**Sub-figure (c) "SLMs promote the LLM to generate task-specific high-quality data"** details a feedback loop for synthetic data generation. The process starts with "Labels" (represented by a tag icon) which lead to "Label-descriptive prompts" (represented by a browser icon with a clock). These prompts are fed into an "LLM". The LLM generates "Synthetic data". This synthetic data is then used to populate a "Synthetic dataset" (represented by a red cylinder). The "Synthetic dataset" is used to "train" an "SLM". The SLM also receives input from a "Local dataset" (represented by a gray cylinder). The SLM then provides "Feedback" (indicated by a red arrow) back to the "LLM", completing the loop to iteratively improve the LLM's ability to generate high-quality, task-specific synthetic data.](images/dbb90c927b541ccb1dbead5da7a8cacff664dcc463e7dcbbe77d6329cea8382d.jpg)
Fig. 25. SLM for LLM Fine-tuning.

SLMs as proxy models: SLMs can approximate the gradient of fine-tuning large-scale LLMs on target datasets, avoiding the costly fine-tuning process in terms of time and computational resources. As shown in Figure 25 (a), Emulated Fine-Tuning (EFT) [246] simulates both unsupervised pre-training and supervised fine-tuning stages across different scales by manipulating log probabilities. EFT, for example, combines base log probabilities from a 70B model with behavioral deltas from a 7B model—these deltas represent differences between fine-tuned and unfine-tuned SLMs, effectively emulating outcomes for the Llama-2 series. This method allows fine-tuning on smaller models such as Falcon-7B [10] while capturing most benefits of fine-tuning larger models such as Falcon-180B, benefiting applications such as dialogue, question-answering, and code generation. Similarly, Proxy-tuning [212] adjusts LLM predictions by adding the differences between the outputs of a fine-tuned small model and its untuned version to the LLM’s output vocabulary during decoding, maintaining the advantages of large-scale pre-training while integrating small-scale fine-tuning benefits. Moreover, SLMs can act as proxies for approximate LLM fine-tuning during decoding. Weakto-Strong Search [453] strategy frames the alignment of LLMs as a test-time greedy search, aiming to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This approach serves as a dual-purpose method: (1) a compute-efficient model up-scaling strategy that circumvents direct tuning of the large model, and (2) an instance of weak-to-strong generalization that bolsters a strong model with weak test-time guidance.

SLMs play a role in selecting high-quality fine-tuning data for LLMs. Figure 25 (b) illustrates how SLMs within the same family as the LLM can identify training samples that are likely to be challenging, enhancing the training efficiency and generalization capability of the LLM. As demonstrated by Swayamdipta et al. [323] and further advanced by Mekala et al. [242], the learning percentage $L P ( i )$ is a metric used to curate high-quality datasets with hard samples: ???? (??) = ??−1 − ????0 −???? where $P _ { i }$ represents the perplexity at the end of epoch-??, and $P _ { 0 }$ is the initial perplexity. A higher $L P ( i )$ early in training indicates significant learning in the initial epochs, highlighting the potential of these samples to enhance LLMs. SmallToLarge (S2L) [411] utilizes training loss trajectories from smaller models to guide data selection for larger models fine-tuning. Experimental results demonstrate that S2L significantly enhances data efficiency in SFT for mathematical problem-solving, reducing the required training data to just $1 1 \%$ of the original MathInstruct dataset [428] to achieve performance comparable to that obtained using the full dataset.

SLMs enhance the quality of LLM-generated data for specific tasks. As depicted in Figure 25 (c), CROSSLM [79] promotes the local training of SLMs on client-specific private data to mitigate privacy risks associated with server-based LLMs. An SLM trained in this manner can guide the server-side LLM to produce high-quality synthetic datasets. Feedback from SLMs regarding the quality of this synthetic data serves as a supervisory signal, enhancing both the quality of LLM outputs and the utility of the data for further training.

# 6.4 SLM for LLM Applications

LLMs are utilized across various applications due to their open-ended generation capabilities, yet they often lack specialized knowledge and other generation issues. SLMs can supplement this by providing taskspecific knowledge or reflecting weaknesses. Therefore, we explore how SLMs enhance the performance of LLMs in specific applications, focusing on open-ended generation, knowledge integration, relation extraction, and empathetic response.

In open-ended text generation—such as writing assistance and story creation—LLMs often suffer from issues such as incoherence and thematic drift over extended sequences. Due to more frequent failure patterns observed in SLMs, such as short, repeated, and irrelevant strings, these patterns serve as negative examples for LLM decoding.

![## Image Analysis: 7bd4c45becca7b686c32b583209d1f7a73eb6551e34b45c46098c5c022c94672.jpg

**Conceptual Understanding:**
Conceptually, this image illustrates a sophisticated text generation or 'decoding' strategy known as Contrastive Decoding. Its main purpose is to demonstrate how this method can be used to improve the specificity and factual accuracy of text generated by Language Models (LMs) by contrasting the predictions of an 'Expert LM' with an 'Amateur LM'. The image conveys the idea that by identifying tokens that are highly likely for a more knowledgeable model but less so for a simpler one, we can encourage the generation of less common but more informative and precise text, thereby overcoming the tendency of LMs to produce generic or highly probable, yet vague, continuations.

**Content Interpretation:**
The image illustrates the 'Contrastive Decoding' method, a technique used in Language Model (LM) applications for 'next token prediction'. It depicts a process where a prompt is fed into two different language models: an 'Expert LM' (GPT-2 XL) and an 'Amateur LM' (GPT-2 small). Both LMs generate probability distributions for the next token. Contrastive Decoding then calculates the logarithmic difference between these probabilities (`log p_EXP - log p_AMA`) to identify tokens that the expert model is significantly more confident about than the amateur model. This process aims to guide text generation towards more specific, less generic, and potentially more factual continuations compared to standard decoding strategies like Greedy or Nucleus sampling.

**Key Insights:**
The main takeaway from this image is that Contrastive Decoding provides a method to generate more specific, factually rich, and less generic text continuations from language models. This is achieved by leveraging the difference in predictive capabilities between an 'Expert LM' and an 'Amateur LM'. The image clearly demonstrates that standard decoding methods (Greedy, Nucleus) can lead to either repetitive or plausible but less precise outputs. For instance, the 'Greedy' continuation, 'Hawaii. He was born in Hawaii. He was born in Hawaii...', is highly repetitive, while the 'Nucleus' continuation, 'Washington, D.C., to Barack Obama and Michelle Robinson...', is plausible but incorrect for Barack Obama's birthplace. In contrast, the 'CD' (Contrastive Decoding) continuation, '1961 to a Kenyan father, Barack Hussein Obama and a mother of American descent, Stanley Ann Dunham...', is highly specific and factually accurate regarding the context of the prompt, indicating a more informed generation. The textual evidence in the `log p_EXP - log p_AMA` table supports this by showing '1961' as having the highest contrastive score (4.13), which directly contributes to its selection and inclusion in the CD output, leading to the highly specific year and parental details.

**Document Context:**
This image is presented in Section 6.4, titled 'SLM for LLM Applications', and is explicitly referenced as 'Fig. 26. Contrastive Decoding [198]'. Its primary role is to visually explain the mechanism and benefits of Contrastive Decoding in the context of improving language model outputs, particularly for generating factual and specific text. It fits into the document's narrative by detailing a specific application or technique related to Small Language Models (SLMs) in conjunction with Large Language Models (LLMs), providing a concrete example of how advanced decoding strategies can enhance the quality of generated content beyond basic methods.

**Summary:**
This image illustrates the Contrastive Decoding process for next token prediction in Language Models (LMs), comparing an 'Expert LM' (GPT-2 XL) and an 'Amateur LM' (GPT-2 small) to generate more specific and factual continuations from a given prompt. The process begins with a 'Prompt: Barack Obama was born in Honolulu, Hawaii. He was born in'. This prompt is fed into two distinct language models. First, an 'Expert LM (GPT-2 XL)' processes the prompt and generates a probability distribution `p_EXP` for the next tokens. For this example, the probabilities are: '0.27 Hawaii', '0.18 the', '0.16 Honolulu', '0.10 1961', '0.02 Washington', followed by '...'. Simultaneously, an 'Amateur LM (GPT-2 small)' also processes the same prompt, generating its own probability distribution `p_AMA` for the next tokens. Its probabilities are: '0.08 Honolulu', '0.04 Washington', '0.04 the', '0.001 1961', followed by '...'. The next step, labeled 'next token prediction', involves 'Contrastive Decoding'. This method calculates the difference in the logarithmic probabilities of tokens between the expert and amateur models, specifically `log p_EXP - log p_AMA`. This calculation yields a score for potential next tokens, presented in a table: '1961 4.13', 'Hawaii 2.34', 'Honolulu 0.65', 'Washington -0.73', followed by '...'. These scores help identify tokens that are highly probable for the expert model but less so for the amateur model, suggesting they might be more informative or specific. Finally, the image shows three different 'Continuations' based on various decoding strategies. 'Greedy' decoding results in a repetitive and generic continuation: 'Hawaii. He was born in Hawaii. He was born in Hawaii...'. 'Nucleus' decoding provides a plausible but potentially incorrect or less specific continuation: 'Washington, D.C., to Barack Obama and Michelle Robinson...'. In contrast, 'CD' (Contrastive Decoding) yields a highly specific and fact-rich continuation: '1961 to a Kenyan father, Barack Hussein Obama and a mother of American descent, Stanley Ann Dunham...'. This demonstrates how Contrastive Decoding can guide an LM to generate less common yet more precise and factual text by focusing on tokens that significantly differentiate an expert model's knowledge from an amateur one.](images/7bd4c45becca7b686c32b583209d1f7a73eb6551e34b45c46098c5c022c94672.jpg)
Fig. 26. Contrastive Decoding [198].

Contrastive Decoding (CD) [198] improves coherence and lexical diversity by leveraging the differential capabilities between a large model, OPT-13B [440], and a smaller model, OPT-125M. As illustrated in Figure 26, CD improves content quality by sampling generation based on the difference in log probabilities, $\log p _ { E X P } - \log p _ { A M A }$ , between an expert LM and an amateur LM, rather than relying solely on the expert LM’s log probability. This approach effectively reduces generative failures, including repetition.

In knowledge injection, general LLMs may lack domain-specific expertise for specialized tasks like law or medicine [91, 353]. Domain-specific SLMs can supply crucial knowledge in a format suitable for LLMs. To this end, BLADE [189] integrates black-box LLMs with small domain-specific models. BLADE combines the comprehensive language capabilities of LLMs with the specialized knowledge of small LMs. As shown in

1. Domain-specific  2. Knowledge  3. Bayesian Prompted pre-training Instruction Tuning Optimization

![## Image Analysis: 0e42f91999f7419cb80623e72013f86267aee5f36093e88b937c5c8097ca9307.jpg

**Conceptual Understanding:**
This image conceptually represents the 'BLADE Framework', which outlines a method for developing and deploying Small Language Models (SLMs) by integrating them with Large Language Models (LLMs) and various types of domain-specific knowledge. Its main purpose is to illustrate a multi-stage, iterative process where SLMs are pre-trained, fine-tuned, and optimized, with LLMs providing crucial feedback for refinement, and domain knowledge serving as foundational information for both SLM training and LLM operations. The diagram communicates the key idea of leveraging the strengths of both small and large language models, alongside targeted knowledge, to create effective and specialized AI applications.

**Content Interpretation:**
This image illustrates the BLADE Framework, a structured approach for the development and application of Small Language Models (SLMs) in scenarios involving Large Language Models (LLMs) and domain-specific knowledge. It details a multi-stage process for training and refining SLMs: 'pre-training', 'instruction tuning', and 'Optimization'. The framework shows how 'Domain-specific Knowledge' is initially used to 'Train SLM' during the 'pre-training' phase. An 'LLM' plays a crucial role by providing 'generate feedback' to the 'instruction tuning' and 'Optimization' stages of the SLM development, indicating an iterative refinement process. The 'LLM' itself processes 'Question-oriented domain knowledge' and produces 'Prediction from LLM'. Finally, the optimized SLM performs 'Inference' using a 'Test Input'. The legend explicitly clarifies the nature of the arrows: solid black arrows signify 'Train SLM' operations, and solid orange arrows indicate 'Inference' operations. This depicts a comprehensive system where SLMs are trained, fine-tuned, and applied, with LLMs contributing to the learning process through feedback and various forms of domain knowledge informing both training and LLM operations.

**Key Insights:**
The main takeaways from this image are: 1. **Iterative SLM Development:** SLMs undergo a structured development pipeline consisting of 'pre-training', 'instruction tuning', and 'Optimization', as evidenced by the sequence of SLM boxes with these labels. 2. **LLM for Feedback and Enhancement:** An 'LLM' is critical for providing 'generate feedback' during the 'instruction tuning' and 'Optimization' phases, indicating that LLMs can guide and improve SLM performance. 3. **Integration of Domain Knowledge:** Both 'Domain-specific Knowledge' (for initial 'pre-training') and 'Question-oriented domain knowledge' (for the 'LLM') are essential inputs, showing the importance of specialized information in the framework. 4. **Clear Operational Phases:** The legend distinguishes between 'Train SLM' (solid black arrows) and 'Inference' (solid orange arrows), clarifying the operational modes within the framework. 5. **Application through Test Input:** The final optimized SLM is used for 'Inference' with a 'Test Input', demonstrating its practical application.

**Document Context:**
This image, described as 'Fig. 27. BLADE Framework [189]', directly supports the document's section '6.4 SLM for LLM Applications' by visually detailing a proposed framework for the integration and enhancement of Small Language Models (SLMs) within an ecosystem that includes Large Language Models (LLMs) and specific domain knowledge. It serves as a visual explanation of the methodological approach for building and utilizing SLMs, showing the sequential steps of training, refinement, and application, and highlighting the interactive role of LLMs in providing feedback. This framework provides a concrete model for understanding how SLMs can be effectively developed and deployed in practical applications discussed in the text, particularly when leveraging the capabilities of larger models and specialized knowledge.

**Summary:**
The diagram, titled 'BLADE Framework', illustrates a comprehensive workflow for developing and utilizing Small Language Models (SLMs) in conjunction with Large Language Models (LLMs) and various forms of domain knowledge. The process begins with the 'Domain-specific Knowledge' feeding into the initial 'SLM' stage, which is labeled 'pre-training'. Following this, the SLM undergoes an 'instruction tuning' phase, then an 'Optimization' phase, and finally a stage where it processes a 'Test Input'. The 'instruction tuning' and 'Optimization' phases receive iterative 'generate feedback' from an 'LLM'. This 'LLM' also processes 'Question-oriented domain knowledge' and outputs a 'Prediction from LLM'. A legend clarifies that a solid black arrow represents 'Train SLM' activities, and a solid orange arrow represents 'Inference' activities. This framework details how SLMs are iteratively refined and applied, leveraging both broad LLM capabilities for feedback and specific domain knowledge for training and inference, aiming to enhance the SLM's performance and applicability.](images/0e42f91999f7419cb80623e72013f86267aee5f36093e88b937c5c8097ca9307.jpg)
Fig. 27. BLADE Framework [189].

Figure 27, BLADE’s process includes: 1) pre-training SLMs on domain-specific data, 2) fine-tuning with knowledge instruction to meet task-specific needs, and 3) using joint Bayesian optimization to enhance synergy between the LLM and the small LM, boosting overall performance.

In relation extraction, a field limited by scarce labeled data and prevalent long-tail categories, the “Train-GuidePredict” framework [327] employs SLMs to learn task-specific knowledge for dominant categories. SLMs struggle with rare categories, whereas LLMs manage these effectively due to their extensive pre-trained text. Therefore, this framework leverages the strengths of both models: it utilizes SLMs to acquire task knowledge and guide the LLM’s generative process with initial SLM predictions, enhancing the LLM’s handling of underrepresented categories.

In generating empathetic responses, LLMs excel in expressiveness but struggle with nuanced emotions and cognition. HEF [413] addresses this by incorporating Small Empathy Models (SEMs) to enhance LLMs’ emotional and cognitive depth. This framework employs a two-tiered emotion prediction method: SEMs identify primary emotions, directing LLMs to concentrate on these emotions and their triggers, resulting in more accurate and empathetic responses.

# 6.5 SLM for LLM Safety

As demonstrated by various works [250, 302, 426, 460], LLMs are vulnerable to adversarial attacks and jailbreaking. For example, Wang et al. [358] shows that ChatGPT’s performance on adversarial datasets is still far from perfect, indicating that potential risks of adversarial vulnerability remain. Another example includes jailbreaking ChatGPT by asking it to ’pretend to be a sarcastic mean girl.’ Using such techniques, it has been shown that even the most advanced LLMs are far from being safe against generating potentially harmful content. Hence, the widely adopted LLM-based services to generate are at high risk of being misused for nefarious purposes. Consequently, resources such as the Llama 2 Responsible Use Guide 14 strongly advocate for implementing robust guardrails in products that utilize Generative AI. These guardrails are specifically designed to mitigate risks associated with both inputs to and outputs from the model, ensuring safeguards against the generation of high-risk or policy-violating content, as well as protecting against adversarial inputs and attempts to compromise the model. In addition to developing trustworthy LLMs, adopting SLMs for LLM safety [153, 177] has also attracted increasing attention. For example, Llama Guard [153], fine-tuned on Llama2-7B, has publicly released an input-output safeguard tool specifically for classifying safety risks in prompts and responses within conversational AI applications. However, this tool is limited to assessing the harmfulness of questions and answers and does not facilitate the generation of fluent, safe responses. In response to this limitation, Kwon et al. [177] fine-tune a specialized small language model with harmful query detection and safeguard answer generation tasks to accurately detect harmful user queries and generate appropriate safeguard explanations, thereby enhancing the safety measures in conversational AI.

# 6.6 SLM for LLM Evaluation

SLMs can also enhance the evaluation of LLMs. In dialog evaluation, generating dialog reference responses is computationally complex, making accurate assessment difficult due to the multiple plausible but semantically different responses possible for a single dialog context. Relying on LLM prompting for evaluation can lead to problems such as dependency on prompt wording and inconsistent results. One solution involves training specialized SLMs to evaluate LLMs, as these SLMs can be fine-tuned more quickly and generate outputs faster during inference, owing to their reduced number of parameters. For example, SLIDE [449] employs contrastive learning to fine-tune an SLM to effectively distinguish between positive and negative responses. Based on its observation that SLMs are more accurate in identifying positive responses and LLMs excel at classifying negative ones, the trained SLM is subsequently integrated with an LLM to assign a score to each response. The scoring method used is formalized as follows:

$$
s c o r e = \left\{ \begin{array} { l l } { s c o r e _ { S L M } , } & { \mathrm { i f } s c o r e _ { S L M } \ge 0 . 5 } \\ { s c o r e _ { L L M } , } & { \mathrm { e l i f } s c o r e _ { L L M } < 0 . 5 } \\ { \frac { s c o r e _ { S L M } + s c o r e _ { L L M } } { 2 } , } & { \mathrm { o t h e r w i s e } } \end{array} \right.
$$

This equation allows for adaptive response evaluation, leveraging the strengths of both models to ensure a more reliable and consistent assessment across varying dialogue contexts. In the natural language generation task, Kuhn et al. [175] designs a novel entropy to evaluate the uncertainty of LLMs. It aims to tackle the challenge of semantic equivalence [175]. For instance, $A$ ’s son is $B$ and $B$ is $A$ ’s son are semantically equivalent. It should not be considered uncertain if an LLM is unsure about which of the two previously mentioned sentences to generate due to semantic equivalence. A DeBERTa-Large [132] fine-tuned on the MNLI [378] dataset serves as the classifier guided by semantic equivalence in the clustering stage. SelfCheckGPT [240] proposes a black-box hallucination detection method for LLMs. The core idea is to leverage uncertainty derived from sampled outputs. To be specific, Manakul et al. [240] claim that an LLM trained on a concept generates responses that are similar and factually consistent. One of the five variants of SelfCheckGPT uses BertScore to achieve it. A DeBERTa-Large [218] is utilized to calculate the BERTScore. Factscore [244] is proposed to evaluate the factuality of LM-generated long-form content. It divides the generated long content into multiple short texts, enabling a more precise assessment of factual accuracy. In addition to manual evaluation, Min et al. [244] also proposes an automated evaluation framework to estimate Factscore which can reduce costs. LLaMa 7B [338] fine-tuned on Super-NaturalInstructions [367] is one of the LMs employed as an evaluation assistant and shows promising performance. They also employ Generalizable T5-based dense retrievers [259] to facilitate passage retrieval.

Insights: SLMs can improve LLMs in various aspects, including enhancing the reliability of LLM generation, extracting prompts, fine-tuning,   
application, and evaluation. This discussion seeks to answer when SLMs should be utilized to augment LLMs. We identify several suitable   
scenarios:   
• Adapting LLMs to specific tasks can require substantial computational resources and time. In such cases, a smaller model could be fine-tuned instead to serve functions such as hallucination detection.   
• SLMs can outperform LLMs in certain aspects, hence combining SLMs with LLMs can create a more powerful model, e.g., SLMs typically have fewer security issues than LLMs, and integrating both can generate a model that is both powerful and secure.   
• SLMs, despite their limitations, can alert LLMs to these issues, such as the tendency to produce repetitive vocabulary. Designing contrastive losses can help LLMs overcome these issues by learning from the nuanced feedback of SLMs.   
• The fast inference speed and certain characteristics of SLMs can emulate and thus enhance the behavior of LLMs, acting as effective proxies. For example, the training data selection for LLMs can be guided by the difficulty metrics assessed by SLMs, and the parameter adjustments during the fine-tuning of SLMs can also approximate the fine-tuning processes of LLMs.

Table 13. Synergy between SLMs and LLMs   

<table><tr><td>Synergy</td><td>Representative Work</td><td>KeyPoint</td></tr><tr><td rowspan="6"></td><td>CoGenesis [437]</td><td>Divide user instructions into general part by LLMs and private parts by SLMs.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>武</td></tr><tr><td>g</td><td></td></tr><tr><td>LLMCad [394]</td><td>Combine lightweight and high-precision LLMs for on-device inference.</td></tr><tr><td>Khattab et al. [167], Ma et al. [234]</td><td>Focuse on LLM&#x27;s reasoning and SLM&#x27;s efficient decoding.</td></tr><tr><td>Cloud-Edge Synergy (Training)</td><td>CROSSLM [79]</td><td>Preserve client data privacy by training SLMlocally and LLM remotely; mutual improvement using SLM-labeled data from LLM outputs.</td></tr><tr><td rowspan="4">Task-Centric Synergy</td><td>α-UMi [307]</td><td>Break downa single LLM into specialized agents.</td></tr><tr><td>SynCID[204]</td><td>Merge LLM&#x27;s semantic with SLM&#x27;s speed; refine labels via contrastive learning.</td></tr><tr><td>Filter-then-rerank [234]</td><td>SLMs process simple samples and flag complex ones for LLM reranking.</td></tr><tr><td>Data Shunt+ (DS+) [46]</td><td>Process easy samples with SLMs and delegates hard samples to LLMs.</td></tr></table>

# 7 SYNERGY BETWEEN SMALL AND LARGE LANGUAGE MODELS

The synergy between small and large language models leverages the unique strengths of each to enhance overall system performance and efficiency. SLMs, being lightweight and resource-efficient, are ideal for deployment on edge devices, enabling rapid responses and low latency for straightforward tasks. LLMs, on the other hand, possess greater computational power and a deeper understanding of complex language patterns, allowing them to handle more intricate and nuanced tasks. By integrating SLMs and LLMs, systems can dynamically allocate tasks based on complexity, ensuring that simple queries are processed quickly on the edge while more demanding requests are escalated to the cloud. This collaborative approach optimizes resource usage, reduces operational costs, and maintains high-quality outputs across a diverse range of applications. The synergy between small and large language models can be categorized into two parts: cloud-edge synergy and task-centric synergy. Cloud-edge synergy refers to a setup where SLMs operate on edge devices, while LLMs reside on the server. When the SLM is not powerful enough, the LLM compensates by handling more complex tasks and providing additional support. Task-centric synergy refers to the scenario where SLMs and LLMs leverage their respective strengths to improve task-oriented efficiency. Table 13 summarizes representative work in each category and their key points. Next, we introduce each category in detail.

![## Image Analysis: 48df989cfaadb8fac983caaf7d5a0543997244da2d91323c876c84ed2b7ce3c2.jpg

**Conceptual Understanding:**
This image conceptually illustrates two distinct but related frameworks for integrating Large Language Models (LLMs) and Small Language Models (SLMs) to achieve enhanced system capabilities. The overarching theme is 'Could-edge synergy between LLMs and SLMs' as stated in the figure caption.

Diagram (a) focuses on the *inference* phase, portraying a collaborative architecture where a 'Specialized SLM on User Device' works in conjunction with a 'Generic LLM on Cloud'. Its main purpose is to demonstrate how this collaboration can enhance user privacy by keeping detailed user data local to the SLM while leveraging the broader 'High-level knowledge' and 'General instructions' from the cloud LLM. It aims to showcase improved performance and privacy during real-time use by intelligently distributing computational and data responsibilities.

Diagram (b) shifts the focus to the *training* phase, presenting a 'Client-server collaborative training framework for language models'. The main purpose here is to show how LLMs can be utilized to generate 'Synthetic data' for training SLMs, and how multiple client SLMs can collaboratively train under a central SLM (server) model. This framework aims to highlight efficient and scalable training methodologies for SLMs, potentially addressing data limitations and fostering distributed learning paradigms.

**Content Interpretation:**
The image presents two distinct architectures for collaborative language model systems. Diagram (a) outlines an inference-time collaboration between a local, specialized SLM and a cloud-based generic LLM. The SLM leverages its local context and specialized knowledge, while the LLM provides broader, general instructions, indicating a split in processing to optimize for privacy (keeping sensitive data local) and efficiency. The 'High-level knowledge' sent to the cloud suggests an abstraction or generalization of local data, and 'General instructions' from the cloud imply generic guidance. The user's input ('thumbs-up icon') and local processing ('LOG', 'person', '</>') are managed by the Specialized SLM. The interaction with the 'LOG' and 'person' icons suggests that the specialized SLM handles user-specific data and logs locally, sending only generalized, 'High-level knowledge' to the cloud, thus enhancing privacy.

Diagram (b) details a training framework where an LLM is used to generate 'Synthetic data' and 'Labels' from 'Label-descriptive prompts'. This synthetic data, along with labels, is then used to train a central 'SLM' (server). This central SLM collaborates bidirectionally with multiple client SLMs ('SLM 1', 'SLM 2', 'SLM 3'), each potentially having their own local datasets (red cylinders). This represents a client-server or federated learning-like approach for training, where a larger model (SLM server) coordinates with smaller, potentially specialized, SLMs to build a robust system. The synthetic data generation by LLM allows for data augmentation and potentially addresses data scarcity or privacy concerns in the training process for the SLMs.

**Key Insights:**
**Diagram (a) (Inference Synergy for Privacy and Performance):**
*   **Key Takeaway 1: Localized Specialization and Privacy:** A 'Specialized SLM on User Device' handles specific user interactions and data (implied by 'thumbs-up icon', 'LOG' file, 'person icon', and '</>' icon within the dashed box). This suggests that sensitive or personalized data processing remains local, enhancing user privacy. The text 'Collaborating LLMs and SLMs enhances privacy and performance during inference' directly supports this. The specialized SLM provides 'High-level knowledge' to the 'Generic LLM on Cloud', indicating an abstraction of local data to preserve privacy while still leveraging cloud-based intelligence.
*   **Key Takeaway 2: Cloud for Generalization and Guidance:** The 'Generic LLM on Cloud' provides 'General instructions' back to the specialized SLM, implying that the cloud LLM handles broader tasks or provides generic guidance without direct access to raw user data, thus maintaining performance and privacy by offloading complex, general tasks.

**Diagram (b) (Client-Server Collaborative Training Framework):**
*   **Key Takeaway 3: LLM as a Synthetic Data Generator:** An 'LLM' plays a crucial role in generating 'Synthetic data' and 'Labels' from 'Label-descriptive prompts'. This indicates a strategy to overcome data scarcity, improve data diversity, or reduce reliance on human-labeled data for training SLMs. The arrows from 'LLM' to 'Synthetic data' and 'Labels' directly illustrate this.
*   **Key Takeaway 4: Hierarchical and Collaborative SLM Training:** A central 'SLM' (acting as a server) coordinates and collaborates with multiple client SLMs ('SLM 1', 'SLM 2', 'SLM 3'). Each client SLM also has its own local data. This structure, explicitly labeled 'Client-server collaborative training framework for language models', suggests an efficient and scalable method for distributed training, potentially incorporating federated learning principles or knowledge distillation across a network of SLMs. The bidirectional arrows between the central SLM and client SLMs imply iterative knowledge sharing and refinement.

**Document Context:**
This image, placed in the section '7 SYNERGY BETWEEN SMALL AND LARGE LANGUAGE MODELS' and captioned as 'Fig. 28. Could-edge synergy between LLMs and SLMs', directly supports the document's broader narrative on the collaborative relationship between Small Language Models (SLMs) and Large Language Models (LLMs). Diagram (a) specifically illustrates how this synergy can enhance privacy and performance during the *inference* phase by distributing tasks between a local SLM and a cloud-based LLM. Diagram (b) extends this concept to the *training* phase, showcasing a 'client-server collaborative training framework' that leverages LLMs for synthetic data generation to support SLM training. Together, the diagrams provide concrete architectural examples of how edge computing (SLMs on user devices) and cloud computing (LLMs) can be integrated to achieve benefits in both the deployment (inference) and development (training) lifecycle of language models, addressing issues like data privacy, computational efficiency, and effective model training.

**Summary:**
The image presents two distinct diagrams, (a) and (b), illustrating different aspects of synergy between Small Language Models (SLMs) and Large Language Models (LLMs). Both diagrams focus on how these models can collaborate to enhance performance, privacy, or training efficiency.

Diagram (a) depicts a scenario for enhancing privacy and performance during inference. A 'Specialized SLM on User Device' interacts with a 'Generic LLM on Cloud'. The SLM sends 'High-level knowledge' to the LLM and receives 'General instructions' back. The user's device, represented by a dashed rectangle containing a 'LOG' file icon, a person icon, and a program code icon ('</>'), processes these interactions. The SLM receives input (represented by a thumbs-up icon) and also interacts with the user device's local processing (represented by the program code icon).

Diagram (b) illustrates a 'Client-server collaborative training framework for language models'. In this framework, 'Labels' are fed into a process to generate 'Label-descriptive prompts', which are then sent to an 'LLM'. The LLM generates 'Synthetic data' which, along with the 'Labels', is stored in a data repository (red cylinder). This data repository feeds the 'SLM' (which acts as a server). The 'SLM' then interacts bidirectionally with multiple client SLMs, specifically 'SLM 1', 'SLM 2', and 'SLM 3'. Each of these client SLMs also receives data from their respective local data repositories (red cylinders). This forms a hierarchical and collaborative training structure.](images/48df989cfaadb8fac983caaf7d5a0543997244da2d91323c876c84ed2b7ce3c2.jpg)
Fig. 28. Could-edge synergy between LLMs and SLMs.

# 7.1 Cloud-Edge Synergy

The current utilization of LLMs typically involves uploading private data to the cloud for response. Fine-tuning LLMs usually also requires uploading data to clouds for computing. However, this raises privacy concerns as the collection and usage of private data are constrained by personal privacy awareness and legal regulations [348]. Consequently, the cloud-edge synergy between SLMs and LLMs is proposed to alleviate this issue, i.e., SLMs handle privacy-sensitive data locally, LLMs handle de-identified or non-privacy-sensitive data, and these two models collaborate. This section discusses such cloud-edge synergy, dividing them into two categories: cloud-edge synergy during inference and cloud-edge synergy during training, as shown in Figure 28.

Cloud-edge Synergy During Inference. CoGenesis [437] breaks down the user instruction into a general section and a personal section. The LLM generates replies solely based on general instruction, and the SLM considers both user instruction and additional personal context for its output generation. A fusion strategy blends the output of LLM and SLM synergistically. Xu et al. [397] introduces a split learning system for LLM agents in 6G networks, optimizing mobile device and cloud server collaboration. Mobile devices operate lightweight SLMs with 0–10B parameters for real-time tasks, while cloud servers handle larger LLMs with over 10B parameters for complex reasoning and planning. This setup allows efficient local task management on mobile devices and offloads heavy operations to cloud servers. The system’s architecture features three modules—perception, grounding, and alignment—facilitating effective communication to meet the sophisticated needs of 6G networks.

Besides these frameworks, more specific models are proposed to facilitate the cloud-edge synergy. A common strategy is to use SLM’s fast decoding ability. LLM-to-SLM [28] proposes a framework in which the pre-trained frozen encoder-decoder LLM resides on the server and computes a high-quality representation of the prompt for the planning of an appropriate response. The SLM residing on the edge device, conditioned on this representation, decodes the response efficiently. Some variants put more emphasis on the reasoning ability of LLMs [167, 234, 298]. In Synergy of Thoughts [298], the SLMs generate multiple low-cost reasoning paths. If these paths conflict, the larger LLMs are invoked to provide reflective reasoning and correct any intuitive errors. Hao et al. [127] proposes a framework in which an SLM residing on the edge devices generates tokens, calling LLMs to verify and correct threshold-gated "harder" tokens, to achieve a controllable trade-off between inference quality and cost. LLMCad [394] presents an on-device inference engine addressing memory and latency issues in deploying LLMs on mobile devices. It combines a lightweight LM for token generation with a high-precision LLM for verification, leveraging a token tree structure and speculative generation for efficiency. Tested on devices such as Jetson TX2, it achieves up to $9 . 3 \times$ speedup for LLMs with over 10 billion parameters while maintaining accuracy.

Cloud-edge Synergy During Training. CROSSLM [79] introduces a client-server collaborative training framework that preserves data privacy by having clients locally train SLMs instead of fine-tuning LLMs. The framework enables mutual enhancement through a feedback loop where SLMs evaluate LLM-generated synthetic data and provide feedback to improve the LLM’s generative capabilities, ensuring high-quality and task-specific data. Concurrently, the synthetic data trains the SLMs, boosting their performance. This cyclical exchange fosters cloud-edge synergy and mutual model improvement.

# 7.2 Task-Centric Synergy

The advent of LLMs has significantly propelled various natural language processing tasks and inspired research into their synergistic interactions with SLMs to enhance the performance of models tailored for specific tasks. This section introduces scenarios where small language models exhibit specialized capabilities after fine-tuning and discusses how combining their unique strengths with the versatile abilities of LLMs can yield superior performance on specific tasks. For example, LLMs excel at handling difficult examples or can rewrite content to eliminate task-irrelevant redundancy, thereby enhancing overall task performance, as illustrated in Figure 29, 30 and 31.

$\alpha$ -UMi [307] introduces a multi-agent framework to enhance tool learning by overcoming the limitations of singleLLM approaches for complex tasks. It utilizes three specialized LMs—planner, caller, and summarizer—as depicted in Figure 29—each handling specific subtasks such as planning, tool invocation, and summarization. This modular design allows the use of small and large open-source LLMs (e.g., LLaMa-7B/12B) and supports easy tool updates. Evaluated on benchmarks like ToolBench [279] and ToolAlpaca [326], $\alpha$ -UMi outperforms traditional single-LLM methods and even exceeds GPT-4 in tool learning performance.

![## Image Analysis: bc23a40ce635f273f1242537db1150953da333aa08351c4a52224d8b31af729a.jpg

**Conceptual Understanding:**
This image conceptually illustrates a task-centric architecture designed to process an "Instruction" by synergistically combining a central "Planner" with specialized "Caller" and "Summarizer" components, all of which interact with a variety of external "Tools".

The main purpose of this image is to convey a structured, iterative, and modular approach to 'tool learning' within an AI or language model context. It shows how an initial user instruction can be broken down, executed through external programming environments and applications, and then summarized, with the entire process managed by an intelligent planning component. The core idea is to demonstrate how language models (implied by the 'Planner', 'Caller', 'Summarizer' roles, especially in the context of 'SLMs and LLMs' from the figure caption) can be empowered to utilize a diverse set of real-world tools to solve complex problems more effectively than they could in isolation. It highlights a feedback-driven system where outcomes from tool usage inform the planning and summarization processes.

**Content Interpretation:**
The image depicts a systematic workflow for handling an "Instruction" by synergizing intelligent components (Planner, Caller, Summarizer) with external "Tools".

- **Instruction:** The initial input or query that initiates the entire process.
- **Planner:** This is the central control component. It receives the "Instruction" and interacts bidirectionally with it, suggesting it might analyze, break down, or refine the instruction. It then dispatches sub-tasks or commands.
- **Caller:** A component responsible for executing actions, specifically by invoking or utilizing external "Tools". It receives directives from the "Planner" (blue arrow from Planner to Caller) and reports back to the "Planner" (orange arrow from Caller to Planner).
- **Summarizer:** A component dedicated to processing information, likely from the "Tools" or other sources, to create concise summaries. It also receives tasks from the "Planner" (blue arrow from Planner to Summarizer) and feeds its summaries back to the "Planner" (orange arrow from Summarizer to Planner).
- **Tools:** This represents a suite of external computational resources or programming environments that the Caller and Summarizer can interact with. The explicitly shown tools are R, Python, Jupyter, and Java, indicating a broad capability for data analysis, scripting, and application execution. The circular arrow between the Caller/Summarizer and Tools signifies a continuous or iterative interaction, where these components use the tools, process their outputs, and potentially loop back for further operations.

The blue arrows indicate a flow of directives or tasks from the "Planner" to the "Caller" and "Summarizer", while the orange arrows represent a flow of results, reports, or summaries back to the "Planner". The bidirectional arrow between "Instruction" and "Planner" suggests an iterative understanding or refinement of the initial input. The overall significance is to demonstrate a robust, modular, and iterative system for task execution, where an intelligent planner orchestrates specialized agents to leverage diverse external capabilities.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Modular System Design:** The system is composed of distinct, specialized modules: "Instruction" input, a "Planner" for orchestration, a "Caller" for tool execution, and a "Summarizer" for information synthesis. This modularity (evidenced by the separate labeled boxes for "Planner", "Caller", and "Summarizer") allows for clear division of responsibilities and potentially easier development and maintenance.
2.  **Centralized Planning and Orchestration:** The "Planner" acts as the brain of the operation, receiving the "Instruction" and intelligently directing the flow of tasks to other components. The bidirectional arrow between "Instruction" and "Planner" highlights the Planner's role in understanding and potentially refining the initial request, indicating an advanced level of reasoning.
3.  **Dynamic Tool Integration:** The system demonstrates the ability to integrate and utilize a diverse set of external "Tools" (R, Python, Jupyter, Java). This signifies a powerful capability for executing real-world tasks, performing complex computations, and interacting with various data environments. The visual representation of these specific tool logos under the "Tools" label provides concrete evidence of this integration.
4.  **Iterative and Feedback-Driven Process:** The circular arrow between the "Caller"/"Summarizer" and "Tools", along with the orange feedback arrows from "Caller" and "Summarizer" back to the "Planner", indicates an iterative workflow. This suggests that the system doesn't just execute once but can continuously interact with tools and refine its approach based on intermediate results or summaries, leading to more robust task completion.
5.  **Task-Centric Synergy:** The overall diagram, as explicitly referenced in its figure title "Synergizing SLMs and LLMs in tool learning", illustrates how different AI components can work synergistically. The Planner, Caller, and Summarizer roles work in concert, each contributing a specialized function to achieve a common goal, which is to fulfill the initial "Instruction" effectively by leveraging external tools.

**Document Context:**
This image directly supports the document's Section 7.2, "Task-Centric Synergy," by visually representing a proposed architectural model for achieving synergy in tool learning. Given the document's context, the "Planner," "Caller," and "Summarizer" likely embody functionalities of Small Language Models (SLMs) and/or Large Language Models (LLMs). The diagram illustrates how these language models can be integrated to process an initial instruction, strategically utilize a variety of external tools (R, Python, Jupyter, Java), and then consolidate the results. It provides a concrete example of how the abstract concept of "synergy" in tool learning can be implemented in a system, showing a structured approach to problem-solving that combines the reasoning capabilities of language models with the execution power of diverse programming tools. This model addresses the challenge of enabling AI systems to effectively use tools, which is a key topic in advanced AI research.

**Summary:**
This image, titled "Fig. 29. Synergizing SLMs and LLMs in tool learning" and appearing in Section 7.2 "Task-Centric Synergy", illustrates a conceptual framework for how an initial "Instruction" is processed and executed using a combination of intelligent components and external tools. The process begins with an "Instruction" on the far left, which represents the initial input or query. This instruction is fed into a central "Planner" component, indicated by a double-headed arrow, signifying a bidirectional interaction where the Planner can interpret, refine, or seek clarification for the instruction. The "Planner" acts as an orchestrator, deciding how to fulfill the instruction. From the "Planner", tasks or directives are sent to two distinct components: a "Caller" and a "Summarizer". Blue arrows originating from the "Planner" and pointing towards both the "Caller" and the "Summarizer" indicate this directional flow of instruction or task assignment. The "Caller" component is responsible for interacting with and utilizing external "Tools", while the "Summarizer" component processes information, likely from the tools' outputs, to generate summaries. Both the "Caller" and the "Summarizer" engage in a continuous and iterative interaction with a collection of "Tools", shown as a rounded rectangle containing various application icons and explicitly labeled "Tools" at the bottom. This interaction is represented by a large circular arrow between the Caller/Summarizer components and the "Tools" block. The specific tools depicted are the R programming language logo, the Python programming language logo, the Jupyter logo (likely Jupyter Notebook or Lab), and the Java programming language logo (coffee cup icon). This indicates the system's ability to leverage diverse computational and programming environments. After interacting with the "Tools", both the "Caller" and the "Summarizer" send their respective outputs, results, or summarized information back to the "Planner". This feedback loop is represented by orange arrows originating from the "Caller" and "Summarizer" and pointing back to the "Planner". This comprehensive flow describes a modular and intelligent system designed to execute complex tasks by strategically planning, calling external functionalities, and summarizing results, thereby achieving synergy between different components, likely including Large Language Models (LLMs) and Small Language Models (SLMs) as per the document's context.](images/bc23a40ce635f273f1242537db1150953da333aa08351c4a52224d8b31af729a.jpg)
Fig. 29. Synergizing SLMs and LLMs in tool learning.

SynCID [204] focuses on Conversational Intent Discovery (CID), a task where both known and new intents must be identified from user utterances in an open-world setting. SynCID combines LLMs’ deep semantic insights with SLMs’ agility and specialized capabilities. As illustrated in Figure 30, the framework uses LLM prompting to refine discourse and intent labels, enhancing semantic accuracy and assigning new labels to unlabeled data. SLMs are trained via contrastive learning to align

![## Image Analysis: c81cb9b79eff7862cf4e886d507bc4aab412fb27e8b2d18cadc4977dc37d6307.jpg

**Conceptual Understanding:**
This image conceptually represents an architectural framework or a processing pipeline designed for conversational intent detection, specifically highlighting a synergistic approach involving Large Language Models (LLMs) and Small Language Models (SLMs). Its main purpose is to illustrate how these different models, in conjunction with dedicated representational spaces, can effectively process user utterances to determine their underlying intent, even for novel or 'zero-shot' intents. The key ideas being communicated are: the integration of LLMs for complex language understanding and generation, the role of SLMs in potentially more focused or efficient processing, the importance of distinct 'Descriptor Space' and 'Utterance Space' for managing different forms of linguistic representations, the process of 'Alignment' between these spaces, and a mechanism for 'clustering' utterances to identify intents. A central theme is the system's capability to handle both previously known and entirely new intents without explicit prior training for the latter.

**Content Interpretation:**
The image depicts a synergistic architecture for conversational intent detection, leveraging both Large Language Models (LLMs) and Small Language Models (SLMs). The core processes involve: 1. Initial processing of both 'Unlabeled' and 'Labeled' data by an LLM to generate initial representations: a 'Concise utterance x' and a 'Generated intent descriptor y (include zero-shot labels)'. 2. The 'Generated intent descriptor y' then populates a 'Descriptor Space' and feeds into an 'SLM'. 3. The 'SLM' acts as an intermediary, connecting to both the 'Descriptor Space' and an 'Utterance Space'. 4. A crucial step is the 'Alignment' between the 'Descriptor Space' and the 'Utterance Space', suggesting a mapping or harmonization of these different representational domains. 5. From the 'Utterance Space', 'Selected close-to-center utterances as clusters of existing intent and zero-shot intent' are identified. This indicates a clustering mechanism to find representative utterances for known and novel intents. 6. Finally, a second LLM takes these 'Selected close-to-center utterances' along with a 'Test utterance x' as input to produce a 'New label y'. This system highlights a method for classifying user intent, explicitly including the handling of zero-shot labels, indicating its ability to categorize intents not previously encountered during training. The significance of this setup is its capacity to combine the power of LLMs for generating diverse representations and final labeling, with the potential efficiency or specialized processing of SLMs and structured 'spaces' for intermediate alignment and clustering.

**Key Insights:**
The main takeaways and insights from this image are: 1. **Hybrid Model Effectiveness:** The diagram illustrates an effective architecture for intent detection by combining the strengths of both LLMs and SLMs, suggesting that a synergistic approach can outperform single-model solutions. 2. **Leveraging Diverse Data:** The system is designed to utilize both 'Unlabeled' and 'Labeled' data at its initial stage, indicating robustness in data handling. 3. **Zero-Shot Learning Capability:** A crucial insight is the explicit inclusion of 'zero-shot labels' in the 'Generated intent descriptor y' and 'zero-shot intent' in the selection of clusters. This demonstrates the system's capacity to classify intents for which it has not been directly trained. 4. **Representational Spaces and Alignment:** The use of 'Descriptor Space' and 'Utterance Space' along with an 'Alignment' process highlights the importance of mapping and harmonizing different semantic representations for effective intent detection. 5. **Clustering for Intent Identification:** The process of selecting 'close-to-center utterances as clusters' suggests that a form of clustering or grouping of similar utterances is central to identifying both existing and novel intents. 6. **End-to-End Workflow:** The diagram provides a complete, step-by-step workflow from initial data input to the generation of a 'New label y' for a 'Test utterance x', making the entire process transparent. These insights are directly supported by the verbatim textual elements: 'Unlabeled', 'Labeled', 'LLM', 'Concise utterance x', 'Generated intent descriptor y (include zero-shot labels)', 'SLM', 'Descriptor Space', 'Utterance Space', 'Alignment', 'Selected close-to-center utterances as clusters of existing intent and zero-shot intent', 'Test utterance x', and 'New label y'.

**Document Context:**
This image, titled 'Fig. 30. Synergizing SLMs and LLMs in Conversational Intent Detection' as per the document context, is situated within Section 7.2, 'Task-Centric Synergy'. It serves as a visual explanation of a proposed methodology or architecture that demonstrates how different types of language models (SLMs and LLMs) can collaborate effectively to achieve a specific task: conversational intent detection. The diagram visually supports the broader narrative of the document by illustrating a concrete, detailed process for achieving synergy between these models, especially emphasizing the capability for zero-shot intent recognition, which is a significant challenge in conversational AI. It elaborates on the practical application of the synergy concept by breaking down the workflow into distinct components and interactions.

**Summary:**
This diagram illustrates a process for synergizing Small Language Models (SLMs) and Large Language Models (LLMs) to perform conversational intent detection, particularly with the capability for zero-shot intent recognition. The process begins with an initial LLM that takes in both 'Unlabeled' and 'Labeled' data. From this data, the LLM generates a 'Concise utterance x' and a 'Generated intent descriptor y' which specifically '(include zero-shot labels)'. This generated intent descriptor feeds into a 'Descriptor Space' and also into an 'SLM'. The 'SLM' also connects to the 'Descriptor Space' and directly to an 'Utterance Space'. The 'Descriptor Space' and 'Utterance Space' are linked by an 'Alignment' process. Following this, the system selects 'close-to-center utterances as clusters of existing intent and zero-shot intent' from the 'Utterance Space'. These selected utterances, along with a 'Test utterance x', are then fed into a second LLM. This final LLM processes these inputs to output a 'New label y'. In essence, the flow demonstrates how an LLM can initially process diverse data to generate descriptors, how an SLM can refine or interact with these representations in dedicated spaces, and how these spaces are aligned to identify clusters of intent, ultimately leading to a new label for a test utterance, even for previously unseen (zero-shot) intents.](images/c81cb9b79eff7862cf4e886d507bc4aab412fb27e8b2d18cadc4977dc37d6307.jpg)
Fig. 30. Synergizing SLMs and LLMs in Conversational Intent Detection.

semantic spaces of discourse and intent descriptors, reducing clustering distortion and improving new intent detection.   
Tested on BANKING [43], CLINC [179], and StackOverflow [396], SynCID outperforms CID baselines significantly.

Filter-then-rerank [234] addresses LLMs’ poor performance on simpler IE tasks by integrating LLMs and SLMs. SLMs act as filters, predicting and identifying difficult samples, while LLMs

![## Image Analysis: 706a36e19b603c76d6aae7b544d112d98b1d3baf832b822766b357c5ba1f7446.jpg

**Conceptual Understanding:**
This image conceptually represents a hybrid system architecture for information extraction, integrating a Small Language Model (SLM) and a Large Language Model (LLM). The main purpose is to demonstrate a synergistic approach where an SLM acts as an initial, potentially lighter-weight processor, handling tasks it can confidently manage. For tasks deemed 'Hard?' by the SLM, the more powerful LLM is invoked, followed by a 'Top-k reranking' step to refine its predictions. The core idea communicated is an intelligent task routing and processing strategy to optimize the efficiency and effectiveness of information extraction by dynamically assigning tasks to the most appropriate language model based on perceived difficulty or confidence levels.

**Content Interpretation:**
This image illustrates a two-stage information extraction process involving a Small Language Model (SLM) and a Large Language Model (LLM). The core concept is an intelligent routing mechanism where an SLM first evaluates the 'Test Input'. If the SLM determines, 'by confidence', that the task is not 'Hard?', it generates a 'Prediction from SLM'. This suggests that the SLM is capable of handling relatively easier or more confident predictions. However, if the SLM classifies the task as 'Hard?', the responsibility is escalated to the 'LLM'. The LLM then processes this difficult input, and its output is further refined through 'Top-k reranking' before producing the final 'Prediction from LLM'. This system demonstrates a hierarchical processing approach to information extraction, optimizing resource utilization by using the more computationally intensive LLM only for tasks where the SLM expresses lower confidence or identifies as complex.

**Key Insights:**
The main takeaway from this image is that an efficient information extraction system can be designed by synergistically combining the strengths of SLMs and LLMs. The SLM serves as a first-pass processor, handling cases it can confidently predict, thus optimizing resource usage by not engaging the more computationally intensive LLM unnecessarily. For tasks identified as 'Hard?' by the SLM, the LLM is leveraged, implying its superior capability for complex problem-solving. The inclusion of 'Top-k reranking' for LLM predictions suggests that even after LLM processing, an additional refinement step is crucial for achieving high-quality results in challenging scenarios. This architecture highlights a strategy for balancing computational cost with performance in AI systems by matching task complexity with model capability.

**Document Context:**
This image directly supports the document's section '7.2 Task-Centric Synergy' by providing a concrete example of how Small Language Models (SLMs) and Large Language Models (LLMs) can synergize for information extraction tasks. The title 'Fig. 31. Synergizing SLMs and LLMs in Information Extraction' explicitly states its purpose. The diagram visually explains the mechanism of this synergy, where the SLM acts as an initial filter and processor, offloading difficult cases to the LLM. This approach illustrates a practical application of 'task-centric synergy' by assigning tasks based on their perceived 'hardness' and the capabilities of different models, thereby enhancing efficiency and potentially accuracy in information extraction.

**Summary:**
The image titled 'Fig. 31. Synergizing SLMs and LLMs in Information Extraction' illustrates a process flow for information extraction that leverages both Small Language Models (SLMs) and Large Language Models (LLMs) in a task-centric synergistic manner. The process begins with a 'Test Input'. This input is then fed into an 'SLM'. Following processing by the SLM, a decision point is reached, indicated by the diamond shape labeled 'Hard?'. This decision is made 'by confidence', suggesting the SLM assesses its confidence in handling the input. If the input is not 'Hard?' (indicated by 'N' for No), the process leads to a 'Prediction from SLM'. If the input is deemed 'Hard?' (indicated by 'Y' for Yes), it is then passed to an 'LLM'. After processing by the LLM, the output undergoes 'Top-k reranking' before resulting in a 'Prediction from LLM'. The diagram clearly shows an adaptive strategy where the simpler, potentially more efficient SLM handles straightforward cases, while the more powerful LLM is engaged only for challenging cases that require advanced processing and reranking.](images/706a36e19b603c76d6aae7b544d112d98b1d3baf832b822766b357c5ba1f7446.jpg)
Fig. 31. Synergizing SLMs and LLMs in Information Extraction.

rerank the top N candidate labels for these cases. As illustrated in Figure 31, SLM predictions are final for non-difficult samples, minimizing reliance on LLMs and reducing latency and costs; for those difficult samples, the top N predicted candidate labels from the SLM are passed to the LLM for reranking (predicting). Tested on small-sample IE tasks, this approach improves performance by an average of $2 . 4 \%$ compared to previous methods. Data Shunt $^ +$ $\mathbf { ( D S + ) }$ [46] introduces a framework to reduce costs by minimizing large model queries during inference and boosting LLM performance with SLMs for tasks like sentiment analysis and image processing. ${ \mathrm { D S } } +$ uses SLMs for “easy” samples within the main training distribution and LLMs for "hard" outliers or boundary cases, maintaining accuracy while reducing LLM use. It incorporates S4L and L4S modules with Prompt Pruning (PP) and 2-stage Confidence Distillation (2CD) for better input processing and knowledge transfer. Tests show ${ \mathrm { D S } } +$ outperforms fine-tuning in accuracy and cost efficiency, significantly cutting down on LLM queries.

![## Image Analysis: bd7e4807f7a9d6400352932e88e02d9e09460a88ebef1ff96ca0ce0b08089699.jpg

**Conceptual Understanding:**
This image represents a conceptual taxonomy or classification of key attributes and challenges associated with ensuring the trustworthiness of systems, likely in the context of artificial intelligence or machine learning. The main purpose of the image is to provide a structured overview of the different dimensions that contribute to or detract from a system's trustworthiness. It communicates the idea that trustworthiness is a complex, multi-dimensional concept requiring consideration of various factors beyond mere functionality, encompassing aspects like resilience, safety, accuracy, data protection, and equity. The image serves as a conceptual framework for understanding the scope of 'trustworthy scenarios.'

**Content Interpretation:**
The image illustrates a taxonomy of 'trustworthy scenarios' broken down into five key dimensions: Robustness, Safety, Reliability, Privacy, and Fairness. These represent critical areas to consider when aiming for trustworthy systems, likely in the context of AI or machine learning given the terminology (e.g., 'Adversarial Robustness,' 'Hallucination').

- **Robustness** (represented by a flexing arm icon) is concerned with how a system performs under various conditions. It is further divided into 'Adversarial Robustness,' which refers to the system's ability to resist malicious attacks designed to cause it to fail or behave incorrectly, and 'Out-of-Distribution Robustness,' which addresses the system's performance when encountering data that differs significantly from its training distribution.

- **Safety** (represented by a red warning triangle with an exclamation mark icon) focuses on preventing harm. It includes 'Toxicity,' referring to the generation or propagation of harmful, abusive, or offensive content, and 'Misinformation,' which denotes the spread of false or inaccurate information.

- **Reliability** (represented by a clapping hands icon) pertains to the consistent and accurate functioning of a system. This category includes 'Hallucination,' a phenomenon where models generate plausible but factually incorrect information, and 'Sycophancy,' which describes a model's tendency to conform to user prompts or opinions, even if they are logically flawed or incorrect, in an attempt to be helpful or agreeable.

- **Privacy** (represented by an orange warning triangle with a CCTV camera icon) highlights the protection of sensitive user data and information.

- **Fairness** (represented by a balance scale icon) emphasizes the importance of equitable treatment and outcomes, ensuring that a system does not exhibit biased or discriminatory behavior towards certain groups or individuals.

**Key Insights:**
The main takeaway from this image is that 'trustworthiness' in a system is a multifaceted concept, not a singular property. It comprises at least five distinct dimensions: Robustness, Safety, Reliability, Privacy, and Fairness. Each dimension addresses different potential vulnerabilities or undesirable behaviors of a system.

Key insights supported by the image's textual evidence include:
1.  **Robustness has two primary facets:** 'Adversarial Robustness' (resilience to attacks) and 'Out-of-Distribution Robustness' (performance on novel data). This highlights the need for systems to be resilient both to intentional manipulation and unexpected data variations.
2.  **Safety concerns extend beyond physical harm:** The inclusion of 'Toxicity' and 'Misinformation' under 'Safety' demonstrates that the safety of a system encompasses the prevention of harmful content generation and the spread of false information, particularly relevant for generative AI systems.
3.  **Reliability issues include subtle errors:** 'Hallucination' and 'Sycophancy' are identified as key reliability challenges. 'Hallucination' refers to the generation of incorrect yet convincing information, while 'Sycophancy' points to the problem of a system uncritically agreeing with users, both undermining the system's factual integrity and independent reasoning.
4.  **Privacy and Fairness are fundamental ethical considerations:** Their standalone inclusion with distinct icons underscores their critical importance as independent pillars of trustworthiness, essential for ethical and responsible system design and deployment.

These insights demonstrate a comprehensive understanding of the challenges and requirements for building and evaluating trustworthy systems, moving beyond basic functionality to encompass ethical, safety, and reliability considerations.

**Document Context:**
This image directly supports Section 7.2, titled 'Task-Centric Synergy,' by visually outlining the 'trustworthy scenarios' that are discussed within that section. It serves as a visual taxonomy, categorizing the various aspects of trustworthiness that are crucial for understanding the synergy in task-centric systems. The surrounding text explicitly states, 'Fig. 32. Scenarios we discuss in this section. The taxonomy is inspired by previous works [320, 351]. Please note that the trustworthy scenarios listed here are not exhaustive.' This confirms that the image provides the foundational categories and sub-categories for the subsequent discussion on trustworthiness within the document, serving as a structured overview for the concepts explored.

**Summary:**
The image presents a taxonomy of different aspects crucial for achieving 'trustworthy scenarios,' as indicated by the document context. It is structured into five main categories: Robustness, Safety, Reliability, Privacy, and Fairness. Each category, except for Privacy and Fairness, is further elaborated with specific sub-concepts or challenges related to that dimension of trustworthiness. The description systematically details each category and its associated elements, providing a comprehensive understanding of the different facets of trustworthiness highlighted in the document.](images/bd7e4807f7a9d6400352932e88e02d9e09460a88ebef1ff96ca0ce0b08089699.jpg)
Fig. 32. Scenarios we discuss in this section. The taxonomy is inspired by previous works [320, 351]. Please note that the trustworthy scenarios listed here are not exhaustive.

# 8 TRUSTWORTHINESS IN SMALL LANGUAGE MODELS

Language models have become ubiquitous in our daily lives, and we increasingly rely on them. However, they pose risks regarding their limitations in trustworthy dimensions like privacy and fairness. These concerns are especially critical in high-stakes domains such as healthcare [130] and finance [202]. Consequently, numerous studies have emerged to evaluate the trustworthiness of LMs [88, 95, 138, 176, 176, 248, 254, 273, 351, 370, 425]. In this section, we consider the works that benchmark various LMs’ trustworthiness and omit the specific attack methods [41, 53, 150, 461] or work [407] that only focuses on early pre-trained LMs like BERT [83] as they are already covered in previous survey papers [78, 115, 123, 288]. Inspired by previous works [320, 351], we discuss the following five key trustworthy scenarios: robustness, privacy, reliability, safety, and fairness, as shown in Figure 32. We consider two dimensions for robustness: Adversarial (Adv) Robustness [352] and Out-of-Distribution (OOD) Robustness [38, 213]. For safety, we explore two key concerns: Misinformation [344] and Toxicity [374]. For reliability, we focus on Hallucination [146] and Sycophancy [301]. Please note that these are just the aspects we are focusing on, and therefore this is not a comprehensive classification or taxonomy. For example, robustness also contains robustness to adversarial demonstration.

Though there are a lot of works benchmarking LMs’ trustworthiness, their main focus is on LLMs. Therefore, we survey some representative works evaluating the trustworthiness of LMs, focusing specifically on those that include SLMs of around 7B parameters or smaller. We also summarize these works in Table 14. Next, we briefly introduce them.

Holistic Evaluation of Language Models (HELM) [205] benchmarks a large number of LMs from various aspects, including a lot of metrics related to trustworthiness such as robustness and fairness. Do-Not-Answer [364] introduces a dataset to evaluate how LMs act when they face content that should not be answered. Wang et al. [364] also label the output of several LMs output on their dataset and then use the labeled data to train some classifiers. PromptRobust [456] constructs two kinds of adversarial prompts to evaluate LMs’ robustness: One kind is designed under non-adversarial settings with semantic integrity while another category is created under adversarial settings. Their results show that LMs perform poorly under such prompts. HaluEval [191] builds a dataset comprising both the samples generated by their proposed framework and human-labeled hallucinations. It facilitates analysis of when LMs produce hallucinated output and how well they detect hallucinated content. Then they use some strategies such as knowledge retrieval to help LMs better recognize hallucinations. Mo et al. [248] evaluates the trustworthiness of open-source LMs, presenting a variety of scenarios such as fairness and privacy. Results show that smaller LMs sometimes outperform larger ones in terms of trustworthiness. PrivLM-Bench [190] is designed to evaluate the privacy issues in LMs. It enables a fair comparison of privacy-preserving LMs by considering more than just differential privacy parameters. FFT [71] introduces around two thousand crafted examples to evaluate LMs’ performances on three trustworthy dimensions: factuality, fairness, and toxicity. Their results suggest that larger LMs do not always show better harmlessness. ROBBIE [98] first benchmarks various series of LMs using a lot of datasets, including two newly introduced datasets developed by ROBBIE. It also Manuscript submitted to ACM

Table 14. Comparison of Different Works that Evaluate the Trustworthiness Issues in LMs. Please note that for the "No. of LMs" attribute, compressed or pruned LMs are not included in the count.

<table><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Paper HELM[205]</td><td>√</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>Do-Not-Answer [364]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td></tr><tr><td>PromptRobust [456]</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>HaluEval[191]</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Mo et al. [248]</td><td>√</td><td>×</td><td>√</td><td>×</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td>PrivLM-Bench [190]</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>FFT[71]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>ROBBIE [98]</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>TrustLLM [320]</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td>RAmBLA[35]</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>JailbreakBench [45]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>Xie et al. [390]</td><td>×</td><td>×</td><td>√</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>OR-Bench [70]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>SORRY-Bench [387]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>BeHonest [59]</td><td>×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Hong et al. [138]</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>√</td></tr><tr><td>RUPBench [370]</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Nakka et al. [254]</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td></tr></table>

evaluates mitigation techniques designed to reduce bias and toxicity. TrustLLM [320] is a comprehensive benchmark that contains a large number of datasets and various well-designed metrics to systematically evaluate various LMs across multiple trustworthy dimensions, including truthfulness, safety, fairness, robustness, privacy, and machine ethics. They also carefully design specific subcategories for each dimension. RAmBLA [35] evaluates the trustworthiness of four LMs as biomedical assistants from three dimensions: Robustness, High Recall, and Hallucination. RAmBLA suggests LMs with more parameters are less likely to cause hallucinations and may choose to reject providing an answer in uncertain situations. JailbreakBench [45] constructs a jailbreaking dataset named JBB-Behaviors and jailbreak artifacts to evaluate current LMs’ performance regarding jailbreaking. It also proposes a unified evaluation pipeline that can incorporate new jailbreak defense techniques. Xie et al. [390] tests online safety analysis methods, filling the gap where no methods focus on the generation phase. OR-Bench [70] constructs three datasets: OR-Bench-80K, OR-Bench-Hard-1K, and OR-Bench-Toxic, to systematically evaluate over-refusal problems in LMs, emphasizing the challenge of balancing safety alignment with the models’ usefulness. SORRY-Bench [387] systematically tests 43 different LMs to see how they perform when facing requests that should be refused. They also collect more than annotations created by humans and find that fine-tuned 7B LMs can achieve performance comparable to GPT-4 scale LMs as evaluators. BeHonest [59] evaluates the honesty of LMs from three aspects: Self-Knowledge, Non-Deceptiveness, and Consistency. They use many different metrics for each aspect. For example, sycophancy rate and lying rate are adopted in Non-Deceptiveness. The results in both the Self-Knowledge and Consistency parts reveal that larger model sizes generally bring improved performance for the Llama-2 [339] and Llama-3 [94] series. Hong et al. [138] examines the effects of compression Manuscript submitted to ACM

methods, including quantization and pruning, on the trustworthiness of language models. They find that pruning and extreme quantization significantly affect the trustworthiness of LMs. RUPBench [370] comprises 15 reasoning datasets designed to assess the performance of LMs both in normal conditions and under various adversarial perturbations. Their results indicate that larger LMs generally demonstrate better resilience to perturbations. Nakka et al. [254] investigates the trust and ethical implications of SLMs deployed on personal devices. It reveals the vulnerabilities of on-device SLMs compared with their on-server counterparts.

Please note that the dimensions discussed in this section reflect only those relevant to our current focus; additional dimensions may be discussed in those works, but not listed in table 14. For example, TrustLLM [320] also explores Machine Ethics.

# 9 FUTURE DIRECTIONS

In this section, we offer insights into several promising future research directions that could inspire and motivate the community to address existing gaps in the development of small language models.

# 9.1 Developing Efficient SLM Model Architecture

Although Transformers [346] are foundational in most language models, they face significant computational and memory challenges that worsen with model size, impacting training and autoregressive decoding. Recently, Mamba [117] has emerged as a promising alternative, adapting state space models to dynamically select inputs based on demands, thereby enhancing efficiency. Thereafter, xLSTM [25] demonstrates that an improved LSTM could function as an LLM, revealing the potential of traditional SSMs. The integration of global static information captured by SSMs with the dynamic information processing of Transformers could complement each other, leading to new architectures that balance effectiveness and efficiency.

# 9.2 Addressing SLM Training Inefficiencies

One study [85] explores the disparate learning dynamics between small and large language models. Utilizing the Pythia model suite, the research demonstrates that layers’ activations in larger models converge more rapidly and monotonically to their final states. This phenomenon is associated with a higher proportional effective rank (PER) in the parameters and gradients of larger models. The analysis enhances our understanding of training inefficiencies in small models and provides insights for future efforts, such as developing methods to increase the PER of layers’ parameters.

# 9.3 Expanding Domain-Specific SLMs

Domain-specific SLMs, which are tailored for specific fields, can provide a stronger foundation for relevant downstream tasks than general-purpose models. Currently, these models primarily focus on scientific and healthcare domains. However, there is significant potential for expansion into other key areas such as law, finance, education, telecommunications, and transportation. The scarcity of SLMs that cater to these domains presents an urgent call for research into developing more specialized models.

# 9.4 Establishing Benchmarking and Leaderboard Platforms for SLMs

Several compelling reasons justify the establishment of benchmarking and leaderboard platforms for SLMs. Firstly, most state-of-the-art SLMs are trained on proprietary datasets, which may include test sets from existing evaluation tasks, presenting challenges for fair capability comparisons. Secondly, many SLMs are designed for specific device applications, significantly differing from general open-domain tasks. Thus, there is a lack of comprehensive benchmarks that accurately reflect SLM performance in specific device applications. For example, SLMs deployed on smartphones often handle tasks sensitive to user data, such as auto-replies based on historical chat texts or GUI context understanding—tasks not typically included in current benchmarks, potentially leading to an underestimation of their importance. Finally, current evaluation tasks focus primarily on metrics like accuracy. Evaluating on-device SLMs involves balancing multiple factors, including overall capabilities, response times, storage and memory usage, power consumption, CPU utilization, additional fine-tuning requirements, and context window constraints, making comprehensive and detailed assessments essential.

# 9.5 Enhancing SLM Performance and Efficiency

In terms of enhancing SLM performance and efficiency, the efficiency of using teacher LLMs via instruction tuning can be further developed, such as Efficient Instruction Tuning of SLMs from LLMs-generated data, Optimizing Teacher LLM Selection for SLM Learning, and Applying Emerging Techniques from LLMs to SLMs.

• Efficient Instruction Tuning of SLMs from LLMs-generated data. Enhancing the specialization of SLMs through instruction tuning from LLMs-generated data is crucial, yet finding the most cost-effective instructional strategies remains an underexplored status. Some key areas for exploration are: (1) Instruction Design Adaptability: The performance of LLMs and SLMs varies significantly with changes in instructions. Therefore, designing tailored instructions that effectively activate relevant sub-competencies and reasoning pathways in SLMs for specific tasks is crucial. This approach would optimize their ability to utilize instructional data, representing a significant future research direction. (2) SLM Capability Adaptability: Given that SLMs exhibit diverse capabilities across domains, simply supplying extensive data samples for instruction tuning is often inefficient, as SLMs may spend excessive time processing unnecessary data. To optimize efficiency when adapting to specific domains, we suggest first assessing the intrinsic capabilities of an SLM within those domains. Subsequently, one could select appropriate data and activate essential fine-grained capabilities to effectively adapt to domain shifts. This targeted approach ensures efficient and domainspecific instruction tuning. (3) Optimizing Data Efficiency: SLMs may possess missing or latent domain knowledge, and activating this latent knowledge may not require substantial data. Thus, identifying inherent knowledge within SLMs and determining the minimal data necessary for effective fine-tuning is a future direction. This research aims to optimize performance while minimizing training resources.   
• Optimizing Teacher LLM Selection for SLM Learning. Teacher LLMs with different abilities and knowledge facilitate diverse applications for SLM training, including data rewriting and generation. Selecting the appropriate teacher model based on specific use cases is crucial. This process requires evaluating the teacher’s capabilities and knowledge to ensure optimal application. For example, GPT-4 excels in generating domain-specific data, outperforming ChatGPT, which may produce inferior outcomes. Strategic selection of teacher LLMs is essential for future work to ensure their strengths are effectively utilized to enhance SLM performance.   
• Applying Emerging Techniques from LLMs to SLMs. To improve LLM performance, techniques such as RetrievalAugmented Generation (RAG) and Mixture of Experts (MoE) are employed. The adoption of RAG in SLMs shows significant promise [215], suggesting benefits from further tailoring retrieved information for SLMs. Future research should account for SLMs’ constraints, such as limited context windows, and customize RAG accordingly. MoE uses

multiple experts to enhance learning without increasing active neurons, but its storage demands pose challenges for SLM deployment, making this a promising area for exploration. Additionally, the application of LLM techniques such as in-context learning and prompting engineering to maximize SLM performance, while accounting for SLMs’ constraints, warrants further investigation.

# 9.6 Applications of SLMs

In real-world applications, SLMs often need to provide personalized services and need to be updated periodically to reflect new needs and new knowledge. Hence, there are several promising directions in terms of the real-world application of SLMs, which are listed as follows:

• LoRA for Personalized Services. Companies often provide personalized services, but user-specific complexities can render simple rules ineffective. Training a separate SLM for each user is impractical. LoRA suggests a method of separable training weights alongside fixed original weights, enabling scalable customization. For instance, RecLoRA [455] integrates personalized knowledge into SLMs/LLMs tailored for recommendation tasks by maintaining a set of parallel, independent LoRA weights. This approach effectively customizes language model parameters to align with individual user preferences. This approach is a promising direction that inspires further investigation. • Lifelong On-device Learning for Knowledge Injection. SLMs on devices can access local data without risking data privacy through two main methods. The first method uses retrieval-augmented generation to integrate local data into prompts, requiring SLMs with advanced processing and reasoning capabilities. The second method fine-tunes SLMs with local data, integrating customized knowledge into the model’s weights. However, this approach demands significant device resources, including memory and energy. A promising solution is lifelong learning, where SLMs continuously learn and adapt while in use. • Strategic Use of SLMs and LLMs in Multi-Agent Systems. LLMs can function as agents; however, their extensive capabilities are often underutilized in many scenarios, leading to resource wastage. Consequently, strategically routing to appropriately capable SLMs and LLMs within multi-agent systems can optimize cost and functionality.

# 9.7 Multimodal SLMs

Research on small language models also includes multimodal data. For example, SmolVLM [99] is a compact model that handles image and text inputs to produce text outputs, suitable for on-device use and various multimodal tasks. SOLO [54] integrates vision and language processing in a single 7B Transformer model. The limited scope of existing research on multimodal SLMs provides a compelling impetus for researchers to investigate the integration of various modalities, including audio and graphs.

# 9.8 SLMs Assisting LLMs

In Section 6, we introduced existing works on the use of SLMs to assist LLMs. For instance, EFT [246] emulates fine-tuning on LLMs by leveraging behavior deltas between SLMs’ pre-trained and fine-tuned weights to alleviate the time-cost issues associated with fine-tuning LLMs; SlimPLM [325] detects missing knowledge in LLMs using a slim proxy SLM to accelerate knowledge injection; Contrastive Decoding [198] enhances text quality by maximizing the difference between the log probabilities of an expert LLM and an amateur SLM to mitigate issues of low-quality generation. The research on adopting SLMs to assist LLMs is still in its early stages, with many promising directions yet to be explored. We list some as follows:

• Enhancing LLM Performance Across Broader Tasks Through SLM Integration. SLMs can outperform LLMs in certain scenarios. For example, SLMs often present fewer security vulnerabilities compared to their larger counterparts and demonstrate superior performance on easier samples in specific tasks [197, 234]. Therefore, integrating SLMs with LLMs can promote the development of models that are not only more robust but also inherently safer. Currently, research in this domain is relatively sparse, suggesting that this collaborative framework could potentially be applied to a wider array of tasks.

• Efficient Enhancement of LLMs through Proxy SLMs. Existing research [18, 212, 246, 325] indicates that SLMs, owing to their accelerated fine-tuning and inference speeds, can effectively mimic the behaviors of LLMs, thereby serving as efficient proxies for optimization. However, the application of SLMs as operational proxies for LLMs is currently underexplored. This mimicry could potentially be expanded to include various aspects of LLM functionality, such as the optimization of prompts, the filtration and integration of supplementary knowledge, and the management of additional knowledge repositories.

• SLMs Assist in Managing Data Quality. LLMs tend to produce hallucinations and toxic content due to low-quality real-world training data. One solution is to remove these low-quality data [355]. However, directly eliminating low-quality content can diminish certain functionalities of LLMs, such as versatility [356]. Therefore, it is crucial to define more refined data quality assessment criteria across dimensions such as factuality, safety, and diversity [377] for real-world data. Researching efficient data selection methods using small models represents a valuable research direction. Additionally, while synthetic data serves as a vital complement to scarce human-generated data [224], the potential for small models to effectively manage synthetic data remains largely unexplored.

• SLMs Assist in LLM Assessment. LLMs are producing vast amounts of increasingly complex texts, such as specialized code and scientific papers, presenting challenges not only for human evaluators but also for traditional assessment metrics. Consequently, developing effective evaluators to assess various aspects of generated content, including factuality, safety, and uncertainty, becomes crucial. Given their proficiency in handling specific tasks, exploring the potential of SLMs to evaluate LLM outputs is a promising research direction.

• SLMs Optimize Query and Reduce Noise for LLM RAG. For Retrieval-Augmented Generation (RAG) using LLMs, differing query requirements between LLMs and search engines pose a challenge. The query for LLMs is often abstract and difficult for search engines to handle, so they require more detailed query keywords. Moreover, LLMs may not need all the information related to a query because they only require partial additional knowledge. Thus, intermediate agents are crucial to adapting LLM queries for search engines by clarifying the required detailed keywords that can search for necessary extra knowledge. Additionally, search engine outputs contain noises, requiring refinement to boost LLM efficiency. SLMs, skilled in a single task, are ideal for optimizing query rewriting and noise reduction in RAG systems, making their application in LLM RAG a promising research area.

• SLMs safeguard LLM. Resources such as the Llama 2 Responsible Use Guide strongly advocate for the implementation of robust guardrails in products that utilize Generative AI. SLMs can be strategically designed to serve as such guardrails, mitigating risks associated with both inputs and outputs from the model. This approach ensures safeguards against the generation of high-risk or policy-violating content and provides protection against adversarial inputs and attempts to compromise the model. Future research can investigate the various safety roles that SLMs play in protecting LLMs.

# 9.9 Synergy between Small and Large Language Models

In Section 7, we discussed how small and large language models can complement each other. For example, CoGenesis [437] integrates SLMs for private data and LLMs for broader context, while Synergy of Thoughts [298] uses SLMs for initial reasoning and LLMs for conflict resolution. CROSSLM [79] shows how privacy can be preserved by training SLMs locally to support LLMs without data exposure. Research in this area is still evolving, and we outline several promising future directions below:

• Refined Cloud-Edge Division of Labor. Current research mainly focuses on splitting tasks between edge-based SLMs and cloud-based LLMs along privacy-sensitive and non-sensitive data boundaries. A potential future direction involves more granular task partitioning: determining which subtasks should be handled locally by SLMs (e.g., initial data filtering, quick semantic parsing) and which should be delegated to the cloud-based LLM (e.g., advanced reasoning, complex generation). This approach can further optimize latency, privacy, and resource utilization. • Adaptive On-Device Specialization for Dynamic Environments. Although SLMs have shown the ability to handle private or personalized data locally, continuous changes in user preferences, application requirements, and data distributions pose challenges. Future work can explore adaptive strategies where edge-based SLMs dynamically specialize or update their parameters, guided by the cloud-based LLM. For instance, the LLM can periodically distill new knowledge into the SLM or provide feedback signals to help the SLM adapt to evolving scenarios.

# 9.10 Trustworthy SLMs

As SLMs are playing crucial roles in various aspects, understanding and improving the trustworthiness of SLMs are essential. Hence, two promising directions are:

• A Comprehensive Evaluation of SLMs’ Trustworthiness. While numerous studies address trustworthiness issues in LLMs, research on SLMs remains sparse. Most existing literature focuses on models with at least 7 billion parameters, leaving a gap in the comprehensive analysis of SLMs’ trustworthiness. Current evaluations typically cover only a fraction of the necessary aspects. Therefore, a systematic assessment, such as TrustLLM [320], is essential to thoroughly evaluate the trustworthiness of SLMs and understand their reliability across various applications. • Developing Trustworthy SLMs. Developing trustworthy SLMs is crucial, with three key research directions: (i) Training SLMs to be trustworthy from scratch; (ii) Ensuring SLMs retain or gain trustworthiness when compressed from LLMs—maintaining trustworthiness if the LLM is trustworthy and instilling trustworthiness if it is not; (iii) Fine-tuning non-trustworthy SLMs to enhance their robustness.

# 10 CONCLUSION

This paper provides a comprehensive survey of Small Language Models (SLMs) with up to 7 billion parameters. Initially, we address the need to clearly define SLMs due to existing ambiguities in their characterization. We then present the foundational concepts essential for constructing SLMs. The survey progresses to explore enhancement techniques, including knowledge distillation and quantization, as well as strategies for adapting Large Language Models (LLMs) to SLM contexts. We survey representative SLMs, both general-domain and domain-specific, discussing their preferred datasets and architectural decisions. We also assess their applications across various tasks and deployment strategies on devices. Further, we investigate their role in augmenting the capabilities of LLMs, serving as proxies for fine-tuning and facilitating two types of synergies: cloud-local and task-centric. Additionally, we discuss the critical aspect of their trustworthiness. The paper concludes with key insights aimed at guiding future research on small language models.

# REFERENCES

[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 (2024). [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [3] Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, and Erkut Erdem. 2024. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. arXiv preprint arXiv:2404.16621 (2024).   
[4] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. 2024. FrameQuant: Flexible Low-Bit Quantization for Transformers. arXiv preprint arXiv:2403.06082 (2024). [5] Abien Fred Agarap. 2018. Deep Learning using Rectified Linear Units (ReLU). CoRR abs/1803.08375 (2018). arXiv:1803.08375 http://arxiv.org/abs/ 1803.08375 [6] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. 2024. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations. [7] Meta AI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. https://ai.meta.com/blog/llama-3-2-connect-2024- vision-edge-mobile-devices/ Accessed: 2024-9-25. [8] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv:2305.13245 [cs.CL] https://arxiv.org/abs/2305.13245 [9] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. 2024. SmolLM - blazingly fast and remarkably powerful.   
[10] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867 (2023).   
[11] Guilherme FCF Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, and Marcelo de Araújo. 2024. Exploring the psychology of LLMs’ moral and legal reasoning. Artificial Intelligence 333 (2024), 104145.   
[12] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.   
[13] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. 2024. Fluctuation-based adaptive structured pruning for large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 10865–10873.   
[14] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).   
[15] AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1 (2024).   
[16] David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, and En-Shiun Annie Lee. 2024. ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models. arXiv preprint arXiv:2406.09334 (2024).   
[17] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. To Code, or Not To Code? Exploring Impact of Code in Pre-training. arXiv preprint arXiv:2408.10914 (2024).   
[18] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=hSyW5go0v8   
[19] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. In The Twelfth International Conference on Learning Representations. https://openreview.net/ forum?id=vXxardq6db   
[20] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).   
[21] Amos Azaria and Tom Mitchell. 2023. The Internal State of an LLM Knows When It’s Lying. In Findings of the Association for Computational Linguistics: EMNLP 2023. 967–976.   
[22] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631 (2023).   
[23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. arXiv:2309.16609 [cs.CL] https://arxiv.org/abs/2309.16609   
[24] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media, Vol. 14. 830–839.   
[25] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended Long Short-Term Memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=ARAxPPIAhq   
[26] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. 2024. Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834 (2024).   
[27] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. SmolLM-Corpus. https://huggingface.co/ datasets/HuggingFaceTB/smollm-corpus   
[28] Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehteshami Bejnordi. 2024. Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding. arXiv preprint arXiv:2402.16844 (2024).   
[29] Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, and Marie-Jeanne Lesot. 2024. Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations. arXiv preprint arXiv:2402.12038 (2024).   
[30] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. Oceangpt: A large language model for ocean science tasks. arXiv preprint arXiv:2310.02031 (2023).   
[31] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. arXiv:2304.01373 [cs.CL] https://arxiv.org/abs/2304.01373   
[32] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 7432–7439.   
[33] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using these metadata..   
[34] Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et al. 2024. Biomedlm: A 2.7 b parameter language model trained on biomedical text. arXiv preprint arXiv:2403.18421 (2024).   
[35] William James Bolton, Rafael Poyiadzi, Edward R Morrell, Gabriela van Bergen Gonzalez Bueno, and Lea Goetz. 2024. RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain. arXiv preprint arXiv:2403.14578 (2024).   
[36] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144 (2022).   
[37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf   
[38] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. 2020. Anomalous example detection in deep learning: A survey. IEEE Access 8 (2020), 132330–132347.   
[39] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts. Authorea Preprints (2024).   
[40] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297 (2024).   
[41] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). 2633–2650.   
[42] Samuel Carreira, Tomás Marques, José Ribeiro, and Carlos Grilo. 2023. Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. arXiv:2310.01434 [cs.CL] https://arxiv.org/abs/2310.01434   
[43] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan Vulić. 2020. Efficient Intent Detection with Dual Sentence Encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI. Association for Computational Linguistics, Online, 38–45.   
[44] Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. [n. d.]. Pre-training Tasks for Embedding-based Large-scale Retrieval. In International Conference on Learning Representations.   
[45] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. 2024. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. arXiv preprint arXiv:2404.01318 (2024).   
[46] Dong Chen, Shuo Zhang, Yueting Zhuang, Siliang Tang, Qidong Liu, Hua Wang, and Mingliang Xu. 2024. Improving Large Models with Small models: Lower Costs and Better Performance. arXiv preprint arXiv:2406.15471 (2024).   
[47] Dong Chen, Yueting Zhuang, Shuo Zhang, Jinfeng Liu, Su Dong, and Siliang Tang. 2024. Data Shunt: Collaboration of Small and Large Models for Lower Costs and Better Performance. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 11249–11257.   
[48] Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT Consistent Knowledge Distillation. In Findings of the Association for Computational Linguistics: EMNLP 2023. 6805–6820.   
[49] Lihu Chen and Gaël Varoquaux. 2024. What is the role of small models in the llm era: A survey. arXiv preprint arXiv:2409.06857 (2024).   
[50] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).   
[51] Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. 2023. Lorashear: Efficient large language model structured pruning and knowledge recovery. arXiv preprint arXiv:2310.18356 (2023).   
[52] Wei Chen, Zhiyuan Li, and Mingyuan Ma. 2024. Octopus: On-device language model for function calling of software APIs. arXiv:2404.01549 [cs.CL] https://arxiv.org/abs/2404.01549   
[53] Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2022. Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP. 11215–11221.   
[54] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. 2024. A Single Transformer for Scalable Vision-Language Modeling. Transactions on Machine Learning Research (2024). https://openreview.net/forum?id=nuzFG0Rbhy   
[55] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. arXiv preprint arXiv:2311.16079 (2023).   
[56] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021. FinQA: A Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 3697–3711.   
[57] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 6279–6292.   
[58] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen. 2024. Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector. arXiv preprint arXiv:2406.11277 (2024).   
[59] Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, and Pengfei Liu. 2024. BeHonest: Benchmarking Honesty of Large Language Models. arXiv preprint arXiv:2406.13261 (2024).   
[60] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2024. InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. In Proceedings of the First edition of the Workshop on the Scaling Behavior of Large Language Models (SCALE-LLM 2024). 35–64.   
[61] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $9 0 \% ^ { \star }$ ChatGPT Quality. https://lmsys.org/blog/2023- 03-30-vicuna/   
[62] Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi. 2024. Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models. arXiv:2401.06432 [cs.LG] https://arxiv.org/abs/2401.06432   
[63] Xiaokai Chu, Jiashu Zhao, Lixin Zou, and Dawei Yin. 2022. H-ERNIE: A multi-granularity pre-trained language model for web search. In Proceedings of the 45th International ACM SIGIR conference on research and development in information retrieval. 1478–1489.   
[64] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research 25, 70 (2024), 1–53.   
[65] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018).   
[66] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021).   
[67] Together Computer. 2023. RedPajama: an Open Dataset for Training Large Language Models. https://github.com/togethercomputer/RedPajama-Data   
[68] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM. https://www.databricks.com/blog/2023/04/12/dolly-first-open-commerciallyviable-instruction-tuned-llm   
[69] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning track. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662   
[70] Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. 2024. OR-Bench: An Over-Refusal Benchmark for Large Language Models. arXiv preprint arXiv:2405.20947 (2024).   
[71] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. 2023. Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity. arXiv preprint arXiv:2311.18580 (2023).   
[72] Luigi Daniele and Suphavadeeprasit. 2023. Amplify-Instruct: Synthetically Generated Diverse Multi-turn Conversations for efficient LLM Training. arXiv preprint arXiv:(coming soon) (2023). https://huggingface.co/datasets/LDJnr/Capybara   
[73] Tri Dao. [n. d.]. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In The Twelfth International Conference on Learning Representations.   
[74] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344–16359.   
[75] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060 (2024).   
[76] Rocktim Jyoti Das, Liqun Ma, and Zhiqiang Shen. 2023. Beyond size: How gradients shape pruning decisions in large language models. arXiv preprint arXiv:2311.04902 (2023).   
[77] Anirban Dasgupta, Ravi Kumar, and Tamás Sarlós. 2011. Fast locality-sensitive hashing. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 1073–1081.   
[78] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics. 1693–1706.   
[79] Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, and Yaoxue Zhang. 2023. Mutual enhancement of large and small language models with cross-silo knowledge transfer. arXiv preprint arXiv:2312.05842 (2023).   
[80] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https: //openreview.net/forum?id=dXiGWqBoxaD   
[81] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024).   
[82] Tim Dettmers and Luke Zettlemoyer. 2023. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning. PMLR, 7750–7774.   
[83] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171–4186.   
[84] Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. CoRR abs/2304.03208 (2023).   
[85] Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. 2024. Tending Towards Stability: Convergence Challenges in Small Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3275–3286. https://doi.org/10.18653/v1/2024.findings-emnlp.187   
[86] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233 (2023).   
[87] Tinghe Ding. 2024. MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. arXiv:2401.04124 [cs.HC] https://arxiv.org/abs/2401.04124   
[88] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2023. Questioning the survey responses of large language models. arXiv preprint arXiv:2306.07951 (2023).   
[89] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. 2023. I3 retriever: incorporating implicit interaction in pre-trained language models for passage retrieval. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 441–451.   
[90] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. 2024. Hymba: A Hybrid-head Architecture for Small Language Models. arXiv preprint arXiv:2411.13676 (2024).   
[91] Jason Xiaotian Dou, Haiyi Mao, Runxue Bao, Paul Pu Liang, Xiaoqing Tan, Shiyi Zhang, Minxue Jia, Pengfei Zhou, and Zhi-Hong Mao. 2023. The Measurement of Knowledge in Knowledge Graphs. In Proceedings of the AAAI 2023 Workshop on Representation Learning for Responsible Human-Centric AI (R2HCAI). Association for the Advancement of Artificial Intelligence (AAAI) Washington . . .   
[92] Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, and Ge Zhang. 2024. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. arXiv:2404.04167 [cs.CL] https://arxiv.org/abs/2404.04167   
[93] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 320–335.   
[94] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).   
[95] Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. 2024. Exploiting LLM Quantization. arXiv preprint arXiv:2405.18137 (2024).   
[96] Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759 (2023).   
[97] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks 107 (2018), 3–11.   
[98] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Smith. 2023. ROBBIE: Robust bias evaluation of large generative language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 3764–3814. https://aclanthology.org/2023.emnlp-main.230   
[99] Hugging Face. 2024. SmolVLM - small yet mighty Vision Language Model. https://huggingface.co/blog/smolvlm Accessed: 2024-11-26.   
[100] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 1–39.   
[101] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Knowledge Card: Filling LLMs’ Knowledge Gaps with Plug-in Specialized Language Models. arXiv preprint arXiv:2305.09955 (2023).   
[102] Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning. PMLR, 10323–10337.   
[103] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. In The Eleventh International Conference on Learning Representations.   
[104] Hao Fu, Yao; Peng and Tushar Khot. 2022. How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao Fu’s Notion (Dec 2022). https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1   
[105] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning. PMLR, 10421–10430.   
[106] Philip Gage. 1994. A new algorithm for data compression. The C Users Journal 12, 2 (1994), 23–38.   
[107] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting filter bubbles by counterfactual interactive recommender system. ACM Transactions on Information Systems 42, 1 (2023), 1–27.   
[108] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).   
[109] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2843–2853.   
[110] Shangqian Gao, Chi-Heng Lin, Ting Hua, Zheng Tang, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. 2024. DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https: //openreview.net/forum?id=YxaY6tHgg0   
[111] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=uNrFpDPMyo   
[112] Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, and Teruko Mitamura. 2024. ColBERT Retrieval and Ensemble Response Scoring for Language Model Question Answering. arXiv:2408.10808 [cs.CL] https://arxiv.org/abs/2408.10808   
[113] Karan Goel. 2024. The On-Device Intelligence Update. https://www.cartesia.ai/blog/on-device   
[114] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus.   
[115] Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. 2023. A survey of adversarial defenses and robustness in nlp. Comput. Surveys 55, 14s (2023), 1–39.   
[116] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838 (2024).   
[117] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023).   
[118] Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. 2024. Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 7528–7541. https://doi.org/10.18653/v1/2024.findings-acl.447   
[119] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations.   
[120] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. arXiv:2306.11644 [cs.CL] https://arxiv.org/abs/2306.11644   
[121] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024).   
[122] Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong Liu. 2024. Compressing large language models by joint sparsification and quantization. In Forty-first International Conference on Machine Learning.   
[123] Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language models: Survey and taxonomy. arXiv preprint arXiv:2202.06862 (2022).   
[124] Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang. 2023. Compresso: Structured pruning with collaborative prompting learns compact large language models. arXiv preprint arXiv:2310.05015 (2023).   
[125] Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. 2023. Improving Small Language Models on PubMedQA via Generative Data Augmentation. arXiv:2305.07804 [cs.CL] https://arxiv.org/abs/2305.07804   
[126] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural network. Advances in neural information processing systems 28 (2015).   
[127] Zixu Hao, Huiqiang Jiang, Shiqi Jiang, Ju Ren, and Ting Cao. 2024. Hybrid SLM and LLM for Edge-Cloud Collaborative Inference. In Proceedings of the Workshop on Edge and Mobile Foundation Models. 36–41.   
[128] Tim Hartill, Diana Benavides-Prado, Michael Witbrock, and Patricia J. Riddle. 2023. Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval. arXiv:2308.04711 [cs.CL] https://arxiv.org/abs/2308.04711   
[129] Tim Hartill, Neset Tan, Michael Witbrock, and Patricia J. Riddle. 2023. Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. arXiv:2308.00946 [cs.CL] https://arxiv.org/abs/2308.00946   
[130] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694 (2023).   
[131] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with GradientDisentangled Embedding Sharing. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=sE7- XhLxHA   
[132] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 (2020).   
[133] Narges Heidari, Parham Moradi, and Abbas Koochari. 2022. An attention-based deep learning method for solving the cold-start and sparsity issues of recommender systems. Knowledge-Based Systems 256 (2022), 109835.   
[134] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations. https://openreview.net/forum?id=d7KBjmI3GmQ   
[135] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016).   
[136] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).   
[137] Sepp Hochreiter and Jürgen Schmidhuber. 1996. LSTM can solve hard long time lag problems. Advances in neural information processing systems 9 (1996).   
[138] Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian R. Bartoldson, Ajay Kumar Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, and Bo Li. 2024. Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression. In Proceedings of the Forty-first International Conference on Machine Learning, ICML. https://openreview.net/forum?id=e3Dpq3WdMv   
[139] Yutong Meng Yuhao Wang Hongcheng Liu, Yusheng Liao. 2023. XieZhi: Chinese Law Large Language Model. https://github.com/LiuHC0428/ LAW_GPT.   
[140] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning. PMLR, 2790–2799.   
[141] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. In Findings of the Association for Computational Linguistics: ACL 2023. 8003–8017.   
[142] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).   
[143] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 (2024).   
[144] Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, and Chen Xu. 2024. I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models. arXiv preprint arXiv:2405.17849 (2024).   
[145] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. [n. d.]. Large Language Models Can Self-Improve. In The 2023 Conference on Empirical Methods in Natural Language Processing.   
[146] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232 (2023).   
[147] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. 2024. Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291 (2024).   
[148] Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, and Jeff Z. Pan. 2024. Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 15787–15803. https://doi.org/10.18653/v1/2024.findings-emnlp.927   
[149] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems 36 (2024).   
[150] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. arXiv preprint arXiv:2310.06987 (2023).   
[151] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2023. Look before you leap: An exploratory study of uncertainty measurement for large language models. arXiv preprint arXiv:2307.10236 (2023).   
[152] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. [n. d.]. Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. In International Conference on Learning Representations.   
[153] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674 (2023).   
[154] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79–87.   
[155] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. 2023. Phi-2: The surprising power of small language models. Microsoft Research Blog (2023).   
[156] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Hwang, and Jong C Park. 2023. Test-Time Self-Adaptive Small Language Models for Question Answering. In Findings of the Association for Computational Linguistics: EMNLP 2023. 15459–15469.   
[157] Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, and Iz Beltagy. 2024. Just CHOP: Embarrassingly Simple LLM Compression. arXiv:2305.14864 [cs.CL] https://arxiv.org/abs/2305.14864   
[158] Yixin Ji, Yang Xiang, Juntao Li, Wei Chen, Zhongyi Liu, Kehai Chen, and Min Zhang. 2024. Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization. arXiv preprint arXiv:2405.10616 (2024).   
[159] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023. 1827–1843.   
[160] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023).   
[161] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).   
[162] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. https://openreview.net/forum?id=9YvfRrpmyw   
[163] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2567–2577.   
[164] Rudolph Emil Kalman. 1960. A new approach to linear filtering and prediction problems. (1960).   
[165] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527 (2024).   
[166] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).   
[167] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023).   
[168] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. 2024. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. Advances in Neural Information Processing Systems 36 (2024).   
[169] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook Choi. 2024. Token-scaled logit distillation for ternary weight generative language models. Advances in Neural Information Processing Systems 36 (2024).   
[170] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629 (2023).   
[171] Yoon Kim and Alexander M Rush. 2016. Sequence-Level Knowledge Distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 1317–1327.   
[172] Young Jin Kim, Raffy Fahim, and Hany Hassan Awadalla. 2023. Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. arXiv preprint arXiv:2310.02410 (2023).   
[173] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. 2024. DistiLLM: Towards Streamlined Distillation for Large Language Models. arXiv preprint arXiv:2402.03898 (2024).   
[174] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533 (2022).   
[175] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. In Proceedings of the Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=VD-AYtP0dve   
[176] Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. 2024. Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes. arXiv preprint arXiv:2404.04392 (2024).   
[177] Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Hwiyeol Jo, Changbong Kim, Hyunwoo Lee, Inho Kang, Sun Kim, and Taiwoo Park. 2024. SLM as Guardian: Pioneering AI Safety with Small Language Model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US, 1333–1350. https://doi.org/10.18653/v1/2024.emnlp-industry.99   
[178] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. Biomistral: A collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373 (2024).   
[179] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China,   
Manuscript submitted to ACM 1311–1316. https://doi.org/10.18653/v1/D19-1131   
[180] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 (2022), 31809–31826.   
[181] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: A 176b-parameter open-access multilingual language model. (2023).   
[182] Hojae Lee, Junho Kim, and SangKeun Lee. 2024. Mentor-KD: Making Small Language Models Better Multi-step Reasoners. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 17643–17658. https://doi.org/10.18653/v1/2024.emnlp-main.977   
[183] Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, and Kai-Wei Chang. 2024. Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 2835–2843.   
[184] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. 2022. xFormers: A modular and hackable Transformer modelling library. https://github.com/facebookresearch/xformers.   
[185] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. ArXiv e-prints (2016), arXiv–1607.   
[186] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems 35 (2022), 3843–3857.   
[187] Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. 2023. Mixed distillation helps smaller language model better reasoning. arXiv preprint arXiv:2312.10730 (2023).   
[188] Guangyan Li, Yongqiang Tang, and Wensheng Zhang. 2024. LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models. arXiv preprint arXiv:2404.09695 (2024).   
[189] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2024. BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models. arXiv:2403.18365 [cs.CL] https://arxiv.org/abs/2403.18365   
[190] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yuan Yao, and Yangqiu Song. 2024. PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. 54–73.   
[191] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Singapore, 6449–6464. https://doi.org/10.18653/v1/2023.emnlp-main.397   
[192] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. 2024. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794 (2024).   
[193] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arXiv:2403.20041 [cs.CL] https://arxiv.org/abs/2403.20041   
[194] Pingzhi Li, Xiaolong Jin, Yu Cheng, and Tianlong Chen. 2024. Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark. arXiv preprint arXiv:2406.08155 (2024).   
[195] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! arXiv:2305.06161 [cs.CL] https://arxiv.org/abs/2305.06161   
[196] Shengrui Li, Xueting Han, and Jing Bai. 2024. Nuteprune: Efficient progressive pruning with numerous teachers for large language models. arXiv preprint arXiv:2402.09773 (2024).   
[197] Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, and Min Lin. 2024. Purifying large language models by ensembling a small language model. arXiv preprint arXiv:2402.14845 (2024).   
[198] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive Decoding: Open-ended Text Generation as Optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 12286–12312.   
[199] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).   
[200] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. arXiv:2309.05463 [cs.CL] https://arxiv.org/abs/2309.05463   
[201] Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. 2023. E-sparse: Boosting the large language model inference through entropy-based n: M sparsity. arXiv preprint arXiv:2310.15929 (2023).   
[202] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: A survey. In Proceedings of the fourth ACM international conference on AI in finance. 374–382.   
[203] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium". 2023. SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification. https://https://huggingface.co/Open-Orca/SlimOrca   
[204] Jinggui Liang, Lizi Liao, Hao Fei, and Jing Jiang. 2024. Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational Intent Discovery. In Findings of the Association for Computational Linguistics ACL 2024. 14133–14147.   
[205] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic Evaluation of Language Models. Transactions on Machine Learning Research (2023). https://openreview.net/forum?id=iO4LZibEqW   
[206] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web Conference 2024. 3497–3508.   
[207] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of Machine Learning and Systems 6 (2024), 87–100.   
[208] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 3214–3252.   
[209] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with Multilingual Generative Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 9019–9052. https://doi.org/10.18653/v1/2022.emnlp-main.616   
[210] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. 2024. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965 (2024).   
[211] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 (2024).   
[212] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. 2024. Tuning language models by proxy. arXiv preprint arXiv:2401.08565 (2024).   
[213] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624 (2021).   
[214] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting content-based recommendation with both open-and closed-source large language models. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 452–461.   
[215] Suqing Liu, Zezhu Yu, Feiran Huang, Yousef Bulbulia, Andreas Bergen, and Michael Liut. 2024. Can Small Language Models With RetrievalAugmented Generation Replace Large Language Models When Learning Computer Science? In Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1. 388–393.   
[216] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685 (2023).   
[217] Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. 2024. Let’s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning. arXiv preprint arXiv:2402.10738 (2024).   
[218] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).   
[219] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024. RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback. arXiv preprint arXiv:2403.06840 (2024).   
[220] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2024. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems 36 (2024).   
[221] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 (2023).   
[222] Zhengxiao Liu, Bowen Shen, Zheng Lin, Fali Wang, and Weiping Wang. 2023. Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 3850–3868. https://doi.org/10.18653/v1/2023.findings-acl.237   
[223] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. 2024. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905 (2024).   
[224] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey. In Findings of the Association for Computational Linguistics ACL 2024. 11065–11082.   
[225] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning. PMLR, 22631–22648.   
[226] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. 2023. The data provenance initiative: A large scale audit of dataset licensing & attribution in ai. arXiv preprint arXiv:2310.16787 (2023).   
[227] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 (2024).   
[228] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2645–2652.   
[229] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D Lane, and Mengwei Xu. 2024. Small language models: Survey, measurements, and insights. arXiv preprint arXiv:2409.15790 (2024).   
[230] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics 23, 6 (2022), bbac409.   
[231] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764 (2024).   
[232] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems 36 (2023), 21702–21720.   
[233] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting in Retrieval-Augmented Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 5303–5315.   
[234] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559 (2023).   
[235] Yuhan Ma, Chenyou Fan, and Haiqi Jiang. 2023. Sci-cot: Leveraging large language models for enhanced knowledge distillation in small models for scientific qa. In 2023 9th International Conference on Computer and Communications (ICCC). IEEE, 2394–2398.   
[236] YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At Which Training Stage Does Code Data Help LLMs Reasoning?. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=KIPJKST4gw   
[237] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. 2023. Paloma: A benchmark for evaluating language model fit. arXiv preprint arXiv:2312.10523 (2023).   
[238] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. [n. d.]. Stable Beluga models. [https://huggingface.co/ stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)   
[239] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtarik. 2024. PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression. arXiv preprint arXiv:2405.14852 (2024).   
[240] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 9004–9017. https://doi.org/10.18653/v1/2023.emnlp-main.557   
[241] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. 2024. Openelm: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II@ ICML2024.   
[242] Dheeraj Mekala, Alex Nguyen, and Jingbo Shang. 2024. Smaller language models are capable of selecting instruction-tuning training data for larger language models. arXiv preprint arXiv:2402.10430 (2024).   
[243] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853 (2024).   
[244] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 12076–12100. https://doi.org/10.18653/v1/2023.emnlp-main.741   
[245] Go Min-su. 2024. Deep Learning Bible - 8. Large Language Models. WikiDocs. https://wikidocs.net/237419   
[246] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. 2024. An Emulator for Fine-tuning Large Language Models using Small Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=Eo7kv0sllr   
[247] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045 (2023).   
[248] Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun. 2024. How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2775–2792.   
[249] John Xavier Morris, Wenting Zhao, Justin T Chiu, Vitaly Shmatikov, and Alexander M Rush. 2024. Language Model Inversion. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=t9dWHpGkPj   
[250] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D Griffin. 2023. Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833 (2023).   
[251] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 (2023).   
[252] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679 (2024).   
[253] Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin Desai, Jason Wu, Ran Xu, Sarah Tan, Jianguo Zhang, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang, Caiming Xiong, and Silvio Savarese. 2024. MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases. arXiv:2406.10290 [cs.CL] https://arxiv.org/abs/2406.10290   
[254] Kalyan Nakka, Jimmy Dani, and Nitesh Saxena. 2024. Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models. arXiv preprint arXiv:2406.05364 (2024).   
[255] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1–13.   
[256] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. 2024. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=tDRYrAkOB7   
[257] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2024. CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 4226–4237.   
[258] Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charles O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk, Ernest Perkowski, Jack Miller, Jason Jason Jingsh Li, et al. 2023. AstroLLaMA: Towards Specialized Foundation Models in Astronomy. In Proceedings of the Second Workshop on Information Extraction from Scientific Publications. 49–55.   
[259] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 9844–9855. https://doi.org/10.18653/v1/2022.emnlp-main.669   
[260] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019).   
[261] A Noorian. 2024. A BERT-based sequential POI recommender system in social media. Computer Standards & Interfaces 87 (2024), 103766.   
[262] OpenAI. 2024. GPT-4o mini: advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ Accessed: 2024-7-18.   
[263] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ Accessed: 2024-5-13.   
[264] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730–27744.   
[265] Shankar Padmanabhan, Yasumasa Onoe, Michael Zhang, Greg Durrett, and Eunsol Choi. 2024. Propagating knowledge updates to lms through distillation. Advances in Neural Information Processing Systems 36 (2024).   
[266] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report. arXiv preprint arXiv:2402.16819 (2024).   
[267] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2024. OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=jKHmjlpViu   
[268] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. arXiv:2406.17557 [cs.CL]   
[269] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 (2023).   
[270] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936   
[271] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 (2023).   
[272] Zhiyuan Peng, Xuyang Wu, Qifan Wang, and Yi Fang. 2023. Soft prompt tuning for augmenting dense retrieval with large language models. arXiv preprint arXiv:2307.08303 (2023).   
[273] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023. Discovering Language Model Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023. 13387–13434.   
[274] Pascal Pfeiffer, Philipp Singer, Yauhen Babakhin, Gabor Fodor, Nischay Dhankhar, and Sri Satish Ambati. 2024. H2O-Danube3 Technical Report. arXiv preprint arXiv:2407.09276 (2024).   
[275] Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, and Shashishekar Ramakrishna. 2024. Fine-tuning Smaller Language Models for Question Answering over Financial Documents. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10528–10548. https://doi.org/10.18653/v1/2024.findings-emnlp.617   
[276] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 (2023), 606–624.   
[277] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations. https://openreview.net/forum?id=R8sQPpGCv0   
[278] Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. 2023. Enabling on-device large language model personalization with self-supervised data selection and synthesis. arXiv preprint arXiv:2311.12275 (2023).   
[279] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. [n. d.]. ToolLLM: Facilitating Large Language Models to Master $1 6 0 0 0 +$ Real-world APIs. In The Twelfth International Conference on Learning Representations.   
[280] Haohao $\mathrm { Q u } .$ , Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, and Qing Li. 2024. A survey of mamba. arXiv preprint arXiv:2408.01129 (2024).   
[281] Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, and Qing Li. 2024. Ssd4rec: a structured state space duality model for efficient sequential recommendation. arXiv preprint arXiv:2409.01192 (2024).   
[282] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.   
[283] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 36 (2024).   
[284] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1–67.   
[285] Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, and Soheil Salehi. 2023. Quantized Transformer Language Model Implementations on Edge Devices. arXiv:2310.03971 [cs.CL] https://arxiv.org/abs/2310.03971   
[286] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–16.   
[287] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context RetrievalAugmented Language Models. Transactions of the Association for Computational Linguistics 11 (2023), 1316–1331.   
[288] Krithika Ramesh, Arnav Chavan, Shrey Pandit, and Sunayana Sitaram. 2023. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 15762–15782. https://aclanthology.org/2023.acl-long.878   
[289] Al Mamunur Rashid, George Karypis, and John Riedl. 2008. Learning preferences of new users in recommender systems: an information theoretic approach. Acm Sigkdd Explorations Newsletter 10, 2 (2008), 90–100.   
[290] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: A large-scale dataset for android device control. Advances in Neural Information Processing Systems 36 (2024).   
[291] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333–389.   
[292] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).   
[293] Caitlin Sadowski and Greg Levin. 2007. Simhash: Hash-based similarity detection.   
[294] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM 64, 9 (2021), 99–106.   
[295] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2024).   
[296] Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023. Mitigating Hallucinations and Off-target Machine Translation with SourceContrastive and Language-Contrastive Decoding. arXiv preprint arXiv:2309.07098 (2023).   
[297] Zeyang Sha and Yang Zhang. 2024. Prompt stealing attacks against large language models. arXiv preprint arXiv:2402.12959 (2024).   
[298] Yu Shang, Yu Li, Fengli Xu, and Yong Li. 2024. Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models. arXiv preprint arXiv:2402.02563 (2024).   
[299] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. PB-LLM: Partially Binarized Large Language Models. arXiv:2310.00034 [cs.LG] https://arxiv.org/abs/2310.00034   
[300] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-shot sensitivity-aware mixed sparsity pruning for large language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 11296–11300.   
[301] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac HatfieldDodds, Scott R Johnston, et al. 2023. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548 (2023).   
[302] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844 (2023).   
[303] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 (2019).   
[304] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020).   
[305] Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, and Weiping Wang. 2022. COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1719–1730. https://doi.org/10.18653/v1/2022.emnlp-main.112   
[306] Bowen Shen, Zheng Lin, Daren Zha, Wei Liu, Jian Luan, Bin Wang, and Weiping Wang. 2024. Pruning Large Language Models to Intra-module Lowrank Architecture with Transitional Activations. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 9781–9793. https://doi.org/10.18653/v1/2024.findingsacl.582   
[307] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024. Small LLMs Are Weak Tool Learners: A Multi-LLM Agent. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Miami, Florida, USA, 16658–16680. https://doi.org/10.18653/v1/2024.emnlp-main.929   
[308] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning. PMLR, 31094–31116.   
[309] Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, and Fuli Feng. 2024. Large language models are learnable planners for long-term recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1893–1903.   
[310] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).   
[311] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. 2024. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400 (2024).   
[312] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. 2022. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems 35 (2022), 14274–14289.   
[313] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023), 172–180.   
[314] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama. https://huggingface.co/datasets/cerebras/SlimPajama-627B   
[315] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint (2024).   
[316] Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, and Denny Zhou. 2021. Fast WordPiece Tokenization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2089–2103.   
[317] Sofia Eleni Spatharioti, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. 2023. Comparing traditional and llm-based search for consumer choice: A randomized experiment. arXiv preprint arXiv:2307.03744 (2023).   
[318] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063.   
[319] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 19053–19061.   
[320] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561 (2024).   
[321] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2024. A Simple and Effective Pruning Approach for Large Language Models. In Proceedings of the Twelfth International Conference on Learning Representations, ICLR.   
[322] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984 (2020).   
[323] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. 2020. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 9275–9293.   
[324] Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for Answering Complex Questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 641–651. https://doi.org/10.18653/v1/N18-1059   
[325] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs. arXiv preprint arXiv:2402.12052 (2024).   
[326] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301 (2023).   
[327] Xuemei Tang, Jun Wang, and Qi Su. 2024. Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction. arXiv preprint arXiv:2402.14373 (2024).   
[328] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, and Yunhe Wang. 2024. Rethinking optimization and architecture for tiny language models. arXiv preprint arXiv:2402.02791 (2024).   
[329] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.   
[330] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 (2022).   
[331] CodeGemma Team. 2024. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409 (2024).   
[332] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024).   
[333] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 (2024).   
[334] TensorOpera Team. 2024. TensorOpera Unveils Fox Foundation Model: A Pioneering Small Language Model (SLM) for Cloud and Edge. https: //blog.tensoropera.ai/tensoropera-unveils-fox-foundation-model-a-pioneering-open-source-slm-leading-the-way-against-tech-giants/ Accessed: 2024-6-13.   
[335] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR). IEEE, 2464–2469.   
[336] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. 2024. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 (2024).   
[337] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing. arXiv preprint arXivko2024distillm:2404.12253 (2024).   
[338] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).   
[339] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).   
[340] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. [n. d.]. StableLM 3B 4E1T. [https://huggingface.co/stabilityai/stablelm-3b4e1t](https://huggingface.co/stabilityai/stablelm-3b-4e1t)   
[341] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847 (2018).   
[342] Adina Trufinescu. 2024. Discover the New Multi-Lingual High-Quality Phi-3.5 SLMs. https://techcommunity.microsoft.com/t5/ai-azure-ai-servicesblog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280.   
[343] Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. Calibrating Large Language Models Using Their Generations Only. arXiv preprint arXiv:2403.05973 (2024).   
[344] Sander Van Der Linden. 2022. Misinformation: susceptibility, spread, and interventions to immunize the public. Nature medicine 28, 3 (2022), 460–467.   
[345] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, et al. 2024. A Survey of Small Language Models. arXiv preprint arXiv:2410.20011 (2024).   
[346] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).   
[347] Olga Veksler. 2023. Test time adaptation with regularized loss for weakly supervised salient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7360–7369.   
[348] Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing 10, 3152676 (2017), 10–5555.   
[349] Yuxian Wan, Wenlin Zhang, and Zhen Li. 2023. Multi-Task Feature Self-Distillation for Semi-Supervised Machine Translation. In International Conference on Neural Information Processing. Springer, 238–254.   
[350] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 353–355.   
[351] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. In Proceedings of the Annual Conference on Neural Information Processing Systems.   
[352] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. 2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840 (2021).   
[353] Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, and Haifeng Chen. 2024. InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration. arXiv preprint arXiv:2402.11441 (2024).   
[354] Fali Wang, Zheng Lin, Zhengxiao Liu, Mingyu Zheng, Lei Wang, and Daren Zha. 2021. Macrobert: Maximizing certified region of bert to adversarial word substitutions. In Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II 26. Springer, 253–261.   
[355] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023. OpenLLMs: Less is More for Open-source Models. https://doi.org/10.5281/zenodo.8105775   
[356] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=AOJyfhWYHf   
[357] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453 (2023).   
[358] Jindong Wang, HU Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, et al. [n. d.]. On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models.   
[359] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving Text Embeddings with Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 11897–11916. https://doi.org/10.18653/v1/2024.acl-long.642   
[360] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-Consistent Chain-of-Thought Distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 5546–5558.   
[361] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei He. 2024. Model compression and efficient inference for large language models: A survey. arXiv preprint arXiv:2402.09748 (2024).   
[362] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv:2002.10957 [cs.CL] https://arxiv.org/abs/2002.10957   
[363] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=bq1JEgioLr   
[364] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024. Do-Not-Answer: Evaluating Safeguards in LLMs. In Findings of the Association for Computational Linguistics: EACL 2024. Association for Computational Linguistics, 896–911. https://aclanthology.org/2024.findingseacl.61   
[365] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided Retrieval Augmentation for Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023. 10303–10315.   
[366] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting black-box llms with medical textbooks for clinical question answering. arXiv preprint arXiv:2309.02233 (2023).   
[367] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on $1 6 0 0 +$ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 5085–5109. https://doi.org/10.18653/v1/2022.emnlp-main.340   
[368] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed H. Chi, and Minmin Chen. 2022. Surrogate for Long-Term User Experience in Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Association for Computing Machinery, 4100–4109. https://doi.org/10.1145/3534678.3539073   
[369] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao Wang. 2024. Can Small Language Models be Good Reasoners for Sequential Recommendation?. In Proceedings of the ACM on Web Conference 2024. 3876–3887.   
[370] Yuqing Wang and Yun Zhao. 2024. RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models. arXiv preprint arXiv:2406.11020 (2024).   
[371] Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, and Yanfu Zhang. 2024. Unlocking Memorization in Large Language Models with Dynamic Soft Prompting. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 9782–9796. https://doi.org/10.18653/v1/2024.emnlp-main.546   
[372] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. https://openreview.net/forum?id=gEZrGCozdqR   
[373] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120 (2023).   
[374] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in Detoxifying Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics, 2447–2469. https://doi.org/10.18653/v1/2021.findings-emnlp.210   
[375] Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text. 94–106.   
[376] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024. AutoDroid: LLM-powered Task Automation in Android. arXiv:2308.15272 [cs.AI] https://arxiv.org/abs/2308.15272   
[377] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for Training Language Models. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=GLGYYqPwjy   
[378] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, 1112–1122. https://doi.org/10.18653/v1/N18-1101   
[379] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 1652–1656.   
[380] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2024. LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, St. Julian’s, Malta, 944–964. https://aclanthology.org/2024.eacl-long.57   
[381] Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, and Yujiu Yang. 2024. Weight-Inherited Distillation for Task-Agnostic BERT Compression. In Findings of the Association for Computational Linguistics: NAACL 2024, Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 13–28. https://doi.org/10.18653/v1/2024.findings-naacl.2   
[382] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2024. Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation. In Proceedings of the ACM on Web Conference 2024. 3566–3575.   
[383] Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vydiswaran, Navdeep Jaitly, and Yizhe Zhang. 2024. Divide-or-Conquer? Which Part Should You Distill Your LLM? arXiv preprint arXiv:2402.15000 (2024).   
[384] Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin, and Ting Liu. 2024. AS-ES Learning: Towards Efficient CoT Learning in Small Models. arXiv preprint arXiv:2403.01969 (2024).   
[385] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=09iOdaeOzp   
[386] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, 38087–38099.   
[387] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al. 2024. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598 (2024).   
[388] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al. 2023. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565 (2023).   
[389] Weikai Xie, Li Zhang, Shihe Wang, Rongjie Yi, and Mengwei Xu. 2024. DroidCall: A Dataset for LLM-powered Android Intent Invocation. arXiv preprint arXiv:2412.00402 (2024).   
[390] Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, and Lei Ma. 2024. Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward. arXiv preprint arXiv:2404.08517 (2024).   
[391] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 91–104. https://doi.org/10.18653/v1/2021.eacl-main.8   
[392] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).   
[393] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. 2023. Small models are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848 (2023).   
[394] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. 2023. LLMCad: Fast and Scalable On-device Large Language Model Inference. arXiv:2309.04255 [cs.NI] https://arxiv.org/abs/2309.04255   
[395] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. 2024. Empowering 1000 tokens/second on-device llm prefilling with mllm-npu. arXiv preprint arXiv:2407.05858 (2024).   
[396] Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun Zhao, Fangyuan Wang, and Hongwei Hao. 2015. Short Text Clustering via Convolutional Neural Networks. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, Phil Blunsom, Shay Cohen, Paramveer Dhillon, and Percy Liang (Eds.). Association for Computational Linguistics, Denver, Colorado, 62–69. https://doi.org/10.3115/v1/W15-1509   
[397] Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, and Khaled B Letaief. 2024. When large language model agents meet 6G networks: Perception, grounding, and alignment. arXiv preprint arXiv:2401.07764 (2024).   
[398] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et al. 2024. A survey of resource-efficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024).   
[399] Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna Martindale, and Marine Carpuat. 2023. Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection. Transactions of the Association for Computational Linguistics 11 (2023), 546–564.   
[400] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che. 2024. OneBit: Towards Extremely Low-bit Large Language Models. arXiv preprint arXiv:2402.11295 (2024).   
[401] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884 (2024).   
[402] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024).   
[403] Chuanpeng Yang, Wang Lu, Yao Zhu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. 2024. Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application. arXiv preprint arXiv:2407.01885 (2024).   
[404] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and Bo Yuan. 2024. MoE- $\mathrm { I } ^ { 2 }$ : Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10456–10466. https://doi.org/10.18653/v1/2024.findings-emnlp.612   
[405] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2024. RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=v3XXtxWKi6   
[406] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM on Web Conference 2024. 4489–4500.   
[407] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. 2023. GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, 12731–12750. https://doi.org/10.18653/v1/2023.findings-acl.806   
[408] Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, and Yue Zhang. 2024. Supervised Knowledge Makes Large Language Models Better In-context Learners. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=bAMPOUF227   
[409] Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, and Yujiu Yang. 2024. LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models. arXiv preprint arXiv:2411.06839 (2024).   
[410] Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187 (2024).   
[411] Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, and Baharan Mirzasoleiman. 2024. SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=K9IGlMQpif   
[412] Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, and Yang Gao. 2023. Mindllm: Pre-training lightweight large language model from scratch, evaluations and domain applications. arXiv preprint arXiv:2310.15777 (2023).   
[413] Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei Zhu, and Xiangwen Liao. 2024. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. arXiv preprint arXiv:2402.11801 (2024).   
[414] Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. 2021. Adapt-and-distill: Developing small, fast and effective pretrained language models for domains. arXiv preprint arXiv:2106.13474 (2021), 460–470.   
[415] Mert Yazan, Suzan Verberne, and Frederik Situmeang. 2024. The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs. arXiv preprint arXiv:2406.10251 (2024).   
[416] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. arXiv:2308.14352 [cs.LG] https://arxiv.org/abs/2308.14352   
[417] Rongjie Yi, Xiang Li, Weikai Xie, Zhenyan Lu, Chenghua Wang, Ao Zhou, Shangguang Wang, Xiwen Zhang, and Mengwei Xu. 2024. PhoneLM: an Efficient and Capable Small Language Model Family through Principled Pre-training. arXiv preprint arXiv:2411.05046 (2024).   
[418] Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The Value of Semantic Parse Labeling for Knowledge Base Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Katrin Erk and Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin, Germany, 201–206. https://doi.org/10.18653/v1/P16-2033   
[419] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. LLM as a System Service on Mobile Devices. arXiv:2403.11805 [cs.OS] https://arxiv.org/abs/2403.11805   
[420] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=N8N0hgNDRt   
[421] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. arXiv preprint arXiv:2407.02485 (2024).   
[422] Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie Zhao, and Yingyan Celine Lin. 2024. EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting. arXiv preprint arXiv:2406.15758 (2024).   
[423] Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, et al. 2024. Mobile Foundation Model as Firmware. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking. 279–295.   
[424] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open 2 (2021), 65–68.   
[425] Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang, Kui Ren, and Jingyi Wang. 2024. S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models. arXiv preprint arXiv:2405.14191 (2024).   
[426] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2024. GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id= MbfAK4s61A   
[427] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, et al. 2023. Disc-lawllm: Fine-tuning large language models for intelligent legal services. arXiv preprint arXiv:2309.11325 (2023).   
[428] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. [n. d.]. Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL https://arxiv. org/abs/2309.05653 ([n. d.]).   
[429] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 4791–4800.   
[430] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. Advances in neural information processing systems 32 (2019).   
[431] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems 32 (2019).   
[432] Cheng Zhang, Jianyi Cheng, George A Constantinides, and Yiren Zhao. 2024. LQER: Low-Rank Quantization Error Reconstruction for LLMs. arXiv preprint arXiv:2402.02446 (2024).   
[433] Collin Zhang, John X Morris, and Vitaly Shmatikov. 2024. Extracting Prompts by Inverting LLM Outputs. arXiv preprint arXiv:2405.15012 (2024).   
[434] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. 2023. Towards the law of capacity gap in distilling language models. arXiv preprint arXiv:2311.07052 (2023).   
[435] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950 (2024).   
[436] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. 2024. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852 (2024).   
[437] Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, and Bowen Zhou. 2024. Cogenesis: A framework collaborating large and small language models for secure context-aware instruction following. arXiv preprint arXiv:2403.03129 (2024).   
[438] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan Zhuang. 2023. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403 (2023).   
[439] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An Open-Source Small Language Model. arXiv:2401.02385 [cs.CL] https://arxiv.org/abs/2401.02385   
[440] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).   
[441] Xinran Zhang, Xin Yuan, Yunwei Li, and Yanru Zhang. 2019. Cold-Start representation learning: A recommendation approach with bert4Movie and movie2Vec. In Proceedings of the 27th ACM International Conference on Multimedia. 2612–2616.   
[442] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. 2024. Plug-and-play: An efficient post-training pruning method for large language models. In The Twelfth International Conference on Learning Representations.   
[443] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024. Effective Prompt Extraction from Language Models. arXiv:2307.06865 [cs.CL] https://arxiv.org/abs/2307.06865   
[444] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. 2024. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems 36 (2024).   
[445] Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, and Suhang Wang. 2024. Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge. arXiv preprint arXiv:2410.16454 (2024).   
[446] Bowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. 2024. APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference. In Forty-first International Conference on Machine Learning.   
[447] Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, and Sangeetha Abdu Jyothi. 2023. LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices. arXiv:2312.00388 [cs.LG] https://arxiv.org/abs/2312.00388   
[448] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization. arXiv preprint arXiv:2403.01136 (2024).   
[449] Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, and Liang Zhan. 2024. SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation. arXiv preprint arXiv:2405.15924 (2024).   
[450] Theodore Zhao, Mu Wei, J Samuel Preston, and Hoifung Poon. 2023. Automatic calibration and error correction for large language models via pareto optimal self-supervision. arXiv preprint arXiv:2306.16564 (2023).   
[451] Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, and Jun Wang. 2024. Merino: Entropy-driven Design for Generative Language Models on IoT Devices. arXiv:2403.07921 [cs.LG] https://arxiv.org/abs/2403.07921   
[452] Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. 2022. Pmc-patients: A large-scale dataset of patient summaries and relations for benchmarking retrieval-based clinical decision support systems. arXiv preprint arXiv:2202.13876 (2022).   
[453] Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. 2024. Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/ forum?id=dOJ6CqWDf1   
[454] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 3277–3287.   
[455] Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation. arXiv preprint arXiv:2408.03533 (2024).   
[456] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528 (2023).   
[457] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language models. arXiv preprint arXiv:2308.07633 (2023).   
[458] Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, and Lei Meng. 2023. Towards an On-device Agent for Text Rewriting. arXiv:2308.11807 [cs.CL] https://arxiv.org/abs/2308.11807   
[459] Yuanyuan Zhuang and Jaekyeong Kim. 2021. A bert-based multi-criteria recommender system for hotel promotion management. Sustainability 13, 14 (2021), 8039.   
[460] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. arXiv preprint arXiv:2301.12867 (2023).   
[461] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 (2023).   
[462] Lixin Zou, Weixue Lu, Yiding Liu, Hengyi Cai, Xiaokai Chu, Dehong Ma, Daiting Shi, Yu Sun, Zhicong Cheng, Simiu Gu, et al. 2022. Pre-trained language model-based retrieval and ranking for web search. ACM Transactions on the Web 17, 1 (2022), 1–36.   
[463] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim Hacid. 2024. Falcon mamba: The first competitive attention-free 7b language model. arXiv preprint arXiv:2410.05355 (2024).