[
    {
        "type": "text",
        "text": "A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness ",
        "text_level": 1,
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "FALI WANG, ZHIWEI ZHANG, XIANREN ZHANG, and ZONGYU WU, The Pennsylvania State ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "University, USA   \nTZUHAO MO, University of Pennsylvania, USA   \nQIUHAO LU, WANJING WANG, and RUI LI, UTHealth Houston, USA   \nJUNJIE XU, The Pennsylvania State University, USA   \nXIANFENG TANG and QI HE, Amazon, USA   \nYAO MA, Rensselaer Polytechnic Institute, USA   \nMING HUANG, UTHealth Houston, USA   \nSUHANG WANG∗, The Pennsylvania State University, USA ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "Large language models (LLMs) have demonstrated emergent abilities in text generation, question answering, and reasoning, facilitating various tasks and domains. Despite their proficiency in various tasks, LLMs like PaLM 540B and Llama-3.1 405B face limitations due to large parameter sizes and computational demands, often requiring cloud API use which raises privacy concerns, limits real-time applications on edge devices, and increases fine-tuning costs. Additionally, LLMs often underperform in specialized domains such as healthcare and law due to insufficient domain-specific knowledge, necessitating specialized models. Therefore, Small Language Models (SLMs) are increasingly favored for their low inference latency, cost-effectiveness, efficient development, and easy customization and adaptability. These models are particularly well-suited for resource-limited environments and domain knowledge acquisition, addressing LLMs’ challenges and proving ideal for applications that require localized data handling for privacy, minimal inference latency for efficiency, and domain knowledge acquisition through lightweight fine-tuning. The rising demand for SLMs has spurred extensive research and development. However, a comprehensive survey investigating issues related to the definition, acquisition, application, enhancement, and reliability of SLM remains lacking, prompting us to conduct a detailed survey on these topics. The definition of SLMs varies widely, thus to standardize, we propose defining SLMs by their capability to perform specialized tasks and suitability for resource-constrained settings, setting boundaries based on the minimal size for emergent abilities and the maximum size sustainable under resource constraints. For other aspects, we provide a taxonomy of relevant models/methods and develop general frameworks for each category to enhance and utilize SLMs effectively. We have compiled the collected SLM models and related methods on GitHub: https://github.com/FairyFali/SLMs-Survey. ",
        "page_idx": 0
    },
    {
        "type": "text",
        "text": "CCS Concepts: $\\cdot$ Computing methodologies Natural language generation. ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "ACM Reference Format: ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Fali Wang, Zhiwei Zhang, Xianren Zhang, Zongyu Wu, TzuHao Mo, Qiuhao Lu, Wanjing Wang, Rui Li, Junjie Xu, Xianfeng Tang, Qi He, Yao Ma, Ming Huang, and Suhang Wang. 2018. A Comprehensive Survey of Small Language Models in the Era of Large Language Models: Techniques, Enhancements, Applications, Collaboration with LLMs, and Trustworthiness. J. ACM 37, 4, Article 111 (August 2018), 78 pages. https://doi.org/XXXXXXX.XXXXXXX ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "1 INTRODUCTION ",
        "text_level": 1,
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "The evolution of neural language models (LMs) from BERT’s [83] pre-training and fine-tuning paradigm to T5’s [284] pre-training plus prompting approach, and finally to GPT-3’s [37] pre-training plus in-context learning, has greatly enhanced natural language processing (NLP). These advancements have broadened NLP’s application across various fields, including language understanding [350], programming [255, 331], recommendation systems [369], information retrieval [44, 152, 228, 317], mobile-device control [87], scientific discovery [311, 436], medical question answering [34, 366], and legal question answering [11]. In particular, the recent emergence of proprietary commercial models including ChatGPT, Bard, and Claude, and open-sourced models such as Llama [94, 338, 339] has led to rapid growth in the development of large language models (LLMs). Even though neural networks consistently improve on various tasks with longer training times, larger datasets, and increased model sizes—a phenomenon known as a neural scaling law [166], these models unpredictably exhibit a sudden acquisition of versatile abilities, termed \"emergent ability,\" once they reach a critical scale threshold, thereby supporting the \"larger is better\" trend. This ability is not present in small-scale models. For instance, the latest Llama-3.1 model with 405 billion parameters performs better in dialogue, logical reasoning, and programming compared to the smaller 7B counterpart [94]. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Despite their prowess in complex tasks, LLMs’ huge parameters and computational needs impose significant limitations, hindering their adoption in many real-world applications. For example, the LLaMa 3.1 model with 405 billion parameters [94], trained on 16K H100 GPUs for 54 days, requires about 202.5 GB of GPU memory using int4 precision and has large inference latency. These issues present several challenges in specific contexts: (1) LLMs are generally hosted in the cloud and used via cloud-based APIs due to the large GPU memory and computational cost. Users need to upload their data to query LLMs, raising data leakage and privacy concerns, especially in high-stake scenarios such as healthcare, finance, and e-commerce; (2) Driven by personal agents, on-device deployment is a critical requirement. Several factors, including cloud costs, latency, and privacy concerns, hinder the on-device processing of cloud-based LLMs, and direct deployment is impractical due to their high parameter and cache requirements, which often exceed the capabilities of devices such as mobile phones; (3) Their large parameter count can cause inference delays from seconds to minutes, unsuitable for real-time applications. For instance, Llama 2 7B takes approximately 84 seconds to process 100 tokens on benchmarks including HellaSwag, TruthfulQA, MMLU, and Arc_C when run on a smartphone equipped with a Snapdragon 685 processor [336]; (4) To boost performance in specialized domains such as healthcare and law, where generic LLMs underperform, LLMs are often fine-tuned. However, this process is computationally expensive due to their large size. (5) Though general-purpose LLMs are powerful, many real-world applications require only specific abilities and domain knowledge, deploying general-purpose LLMs would be a waste of resources and such LLMs often cannot match the performance of models tailored for specific tasks [52, 125, 156, 275, 369]. ",
        "page_idx": 1
    },
    {
        "type": "text",
        "text": "Recently, small language models (SLMs) have shown great potential in alleviating these issues while achieving performance comparable to LLMs for domain-specific problems [1, 26, 116, 143, 223, 274, 333, 336, 402, 439]. Owing to fewer parameters, SLMs excel in efficiency, cost, flexibility, and customization. They provide significant computational Manuscript submitted to ACM ",
        "page_idx": 1
    },
    {
        "type": "image",
        "img_path": "images/77c4c32c1adca3a145486fc4768303feb5c12b34b9a1d2be0efc16a27cd435fa.jpg",
        "image_caption": [
            "Fig. 1. Overview of Small Language Models. "
        ],
        "image_footnote": [],
        "page_idx": 2
    },
    {
        "type": "image",
        "img_path": "images/df663fc0f0f03e94a63f985ffce187344c3d15b4e8e08b798ac33582aed8fd97.jpg",
        "image_caption": [
            "Fig. 2. Download Statistics Last Month in Huggingface for LLMs with Various Model Sizes, obtained on October 7, 2024. "
        ],
        "image_footnote": [],
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "savings in pre-training and inference with reduced memory and storage needs, which is vital for applications requiring efficient resource use. These small models are especially effective in resource-limited settings, performing well on low-power devices such as edge devices. Besides, SLMs improve on-device processing by enhancing privacy, security, response times, and personalization. This supports advanced personal assistants and cloud-independent applications, boosting energy efficiency and reducing carbon emissions. For example, the Llama 3.2 models (1B & 3B) demonstrate that local processing enables immediate execution of prompts and responses [7]. This approach protects privacy by keeping sensitive data such as patient health information (PHI), business data, personal messages, and calendar details local, enhancing confidentiality. It also allows for precise control over which queries are processed on-device versus those requiring cloud-based models. Therefore, small language models are gaining increasing attention as alternatives to LLMs, as indicated in Figure 2, which shows that SLMs are downloaded more frequently than larger models in the Hugging Face community, and Figure 3, which illustrates the growing popularity of SLM releases over time. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Typically, LMs that exhibit emergent abilities are classified as LLMs. However, the categorization of SLMs remains unclear. Studies vary in their contexts: some define SLMs as models with fewer than one billion parameters [223], while others consider the term “small language model” relative to the larger counterparts [183, 327, 369], with no consensus on a unified definition in the current landscape of LLMs. Research suggests SLMs for mobile devices, typically possessing around 6GB of memory, consist of sub-billion parameter models [223], whereas others classify models with up to 10 billion parameters as small, noting their lack of emergent abilities [105]. Given their use in resource-constrained environments and for specific tasks, we propose a generalized definition: Given specific tasks and resource constraints, we define SLMs as falling within a range where the lower bound is the minimum size at which the model exhibits emergent abilities for a specialized task, and the upper bound is the largest size manageable within limited resource conditions. This definition integrates various perspectives and addresses factors related to mobile computing and capability thresholds. ",
        "page_idx": 3
    },
    {
        "type": "text",
        "text": "Due to the growing demand for SLMs, extensive literature has emerged on various aspects of SLMs. For example, several resource-efficient techniques [398] and training methods optimized for SLMs, such as quantization-aware training [221, 357, 400] and selective architectural component choices [223, 280, 336], aim to enhance performance in specific applications [36, 52, 275, 292, 382]. These methods have led to the development of numerous open-source, generalpurpose, and domain-specific SLMs [3, 26, 34, 333, 402, 435]. Beyond their inherent capabilities, SLMs can enhance LLMs by serving as modules or effective proxies [246, 297, 383, 399, 413, 450]. Furthermore, the complementary advantages of SLMs and LLMs can be leveraged collectively to better complete tasks [79, 204, 234, 307, 437]. Despite the commendable performance of SLMs, it is crucial not to overlook their credibility issues, such as the risks of adversarial attacks, producing hallucinations, and privacy breaches [88, 95, 138, 176, 176, 222, 248, 254, 273, 351, 354, 370, 371, 425, 445]. However, currently, there is no comprehensive survey thoroughly exploring these works on SLMs in the era of LLMs. Therefore, this paper offers the first comprehensive survey that analyzes various aspects of SLMs in the LLM era and their future directions. The overview structure of our paper is shown in Figure 1. To summarize, our major contributions are: ",
        "page_idx": 3
    },
    {
        "type": "image",
        "img_path": "images/fe7a5deacc121dd09cb056dc09b2fd7d8e528d9525f679eca295d180628c994a.jpg",
        "image_caption": [
            "Fig. 3. A timeline of existing small language models. "
        ],
        "image_footnote": [],
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "• In Section 3, we examine various techniques for improving the performance of SLMs, including training from scratch, fine-tuning, knowledge distillation, quantization, and leveraging LLM-enhancing technologies to optimize SLMs.   \n• In Section 4, we discuss the tasks that SLMs can enhance and the deployment strategies that enable models to fit within the resource constraints of edge devices while maintaining acceptable inference speed.   \n• In Section 5, we collect SLMs with fewer than 7 billion parameters across both general-purpose and domain-specific applications, reviewing common architectural choices, training techniques, and datasets, and providing a comparative summary of performance across different model sizes. Recent SLMs are listed.   \n• In Section 6, we explore how SLMs can address key challenges faced by LLMs, such as high inference latency, labor-intensive fine-tuning, susceptibility to knowledge noise, and risks of copyright infringement.   \n• In Section 7, we survey two kinds of synergies between LLMs and SLMs: one involves cloud-based LLMs and local SLMs, while the other leverages the unique advantages of both to more effectively solve tasks.   \n• In Section 8, we investigate the trustworthiness issues of SLMs, including hallucination and privacy concerns, by providing a taxonomic summary of current evaluation methods. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "Concurrently with our survey, Lu et al. [229] evaluate open-source SLMs, focusing on their architectures, datasets, algorithms, and on-device performance metrics such as inference latency and memory usage. Van Nguyen et al. [345] delve into optimization strategies for SLMs, including model compression, pruning, and quantization. Chen and Varoquaux [49] investigate how SLMs enhance LLMs and vice versa. In contrast, our survey offers a more comprehensive review with the following differences: (1) we present a detailed taxonomy of recent advancements in SLMs in the era of LLMs; (2) we define SLMs based on emergent capabilities and device specifications, which refines previous unclear definitions related to LLMs; (3) we discuss SLM applications, especially in on-device tasks and deployment, topics previously unexplored; (4) we examine domain-specific SLMs previously overlooked; and (5) we additionally consider the synergy between SLMs and LLMs. ",
        "page_idx": 4
    },
    {
        "type": "text",
        "text": "2 FOUNDATIONAL CONCEPTS IN BUILDING LANGUAGE MODELS",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "This section will introduce foundational concepts and background knowledge for language models, including the concepts of architecture and the training process, as well as methods for obtaining SLMs from LLMs. The advanced training strategy to improve SLM performance will be introduced in Section 3. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.1 Architecture of SLMs ",
        "text_level": 1,
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "SLMs commonly employ the Transformer architecture [346] (see Figure 4), which utilizes self-attention mechanisms to manage long-range text dependencies, essential for maintaining performance with constrained resources. However, due to the attention mechanism, Transformers have large inference cost. Hence, to alleviate the issue, several subquadratictime architectures such as Mamba [117], Hymba [90], and xLSTM [25] are proposed. Next, we will give details of Transformer due to its popularity and briefly introduce newly emerged models. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "2.1.1 Transformer. The Transformer’s self-attention mechanism [346] allows language models to efficiently capture contextual information across longer sequences, even with limited resources. The Transformer generally adopts an encoder-decoder structure featuring self-attention mechanisms, feedforward networks, positional embeddings, and layer normalization. The Transformer architecture design tailored for SLMs is detailed in Section 5; this section will provide only foundational concepts. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Self-Attention Mechanism enables the model to evaluate the importance of tokens relative to each other. The self-attention mechanism is written as ",
        "page_idx": 5
    },
    {
        "type": "equation",
        "img_path": "images/3894a8927993b4576a6a54284923fe726c5e26ec8a2798c33b22908330f309b7.jpg",
        "text": "$$\n{ \\mathrm { A t t e n t i o n } } ( \\mathbf { Q } , \\mathbf { K } , \\mathbf { V } ) = { \\mathrm { s o f t m a x } } \\left( { \\frac { \\mathbf { Q } \\mathbf { K } ^ { \\top } } { \\sqrt { d _ { k } } } } \\right) \\mathbf { V }\n$$",
        "text_format": "latex",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "where ${ \\bf { Q } } , { \\bf { K } } ,$ and $\\mathbf { V }$ are query, key, and value matrices, scaled by $\\sqrt { d _ { k } }$ for stability where $d _ { k }$ is the dimension of key matrices. The dot product $\\mathbf { Q } \\mathbf { K } ^ { \\top }$ reflects the similarity between the query and key vectors. ",
        "page_idx": 5
    },
    {
        "type": "image",
        "img_path": "images/0ec07b9236260ae50875f629ad20462a0550afc9867c569b52597bc40aa089be.jpg",
        "image_caption": [
            "Fig. 4. Transformer architecture [346]. "
        ],
        "image_footnote": [],
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Multi-Head Attention (MHA) [346] is the first method that uses multiple heads to capture diverse information. MHA allows the model to attend to different parts of the input sequence using multiple attention heads as ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "$\\mathrm { M u l t i H e a d ( Q , K , V ) } = \\mathrm { C o n c a t } ( \\mathrm { h e a d } _ { 1 } , \\mathrm { h e a d } _ { 2 } , \\ldots , \\mathrm { h e a d } _ { h } ) { \\mathbf { W } } ^ { O } , \\mathrm { w i t h ~ h e a d } _ { i } = \\mathrm { A t t e n t i o n } ( \\mathbf { Q } { \\mathbf { W } } _ { i } ^ { Q } , \\mathbf { K } { \\mathbf { W } } _ { i } ^ { K } , \\mathbf { V } { \\mathbf { W } } _ { i } ^ { V } )$ (1) Each head in the Multi-Head Attention mechanism operates independently, allowing the model to capture diverse aspects of the data. The outputs are combined using learned projection matrices $\\mathbf { W } _ { i } ^ { Q } , \\mathbf { W } _ { i } ^ { K }$ , and $\\mathbf { W } _ { i } ^ { V }$ , concatenated, and passed through the output projection matrix $\\mathbf { W } ^ { O }$ . ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Building on this foundation, several modifications have been introduced to further optimize self-attention mechanisms for specific challenges such as memory efficiency and computational speed. To address the KV-cache bottleneck in MHA, Multi-Query Attention (MQA) [303] proposes that all attention heads share the same set of keys and values, which reduces the memory and computational overhead associated with storing and managing multiple key-value pairs. Grouped Query Attention (GQA) [8] serves as a middle ground between MHA and MQA. It introduces subgroups of query heads (fewer than the total number of attention heads), where each subgroup shares a single key and value head. ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Manuscript submitted to ACM ",
        "page_idx": 5
    },
    {
        "type": "text",
        "text": "Unlike MQA and GQA, which reduce the number of key and value heads, Multi-Head Latent Attention (MLA) [211] compresses the keys and values into a joint latent vector. This compression allows for efficient handling of key-value pairs while maintaining high performance, significantly reducing the KV-cache and improving inference efficiency. Flash Attention [73, 74] accelerates the self-attention mechanism by minimizing the memory overhead typical of standard attention calculations. This optimization allows SLMs to process longer sequences more efficiently, enhancing their functionality under strict hardware constraints. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Feedforward Network (FFN) comprises two linear transformations separated by a non-linearity, typically modeled as $\\mathrm { F F N } ( \\mathbf { x } ) \\ = \\ \\sigma ( \\mathbf { x } \\mathbf { W } _ { 1 } + b _ { 1 } ) \\mathbf { W } _ { 2 } + b _ { 2 }$ . where $\\mathbf { W } _ { 1 }$ and $\\mathbf { W } _ { 2 }$ are the weight matrices, and $b _ { 1 }$ and $b _ { 2 }$ are bias terms. $\\sigma$ is the activation function, which introduces non-linearity, allowing models to learn complex patterns. Generally, ReLU is used as the activation function. In addition to ReLU, activation functions such as GeLU and SiLU are also used in SLMs to improve performance. We give the details here: (i) ReLU (Rectified Linear Unit) [5] is defined as $\\sigma ( x ) = \\operatorname* { m a x } ( 0 , x )$ , which is commonly used for its simplicity and effectiveness. (ii) GELU (Gaussian Error Linear Unit) [135] is defined as ${ \\mathrm { G E L U } } ( x ) = x \\cdot \\Phi ( x ) = x \\cdot { \\textstyle { \\frac { 1 } { 2 } } } \\left[ 1 + \\operatorname { e r f } \\left( { \\frac { x } { \\sqrt { 2 } } } \\right) \\right] ,$ , where $\\Phi ( x )$ is the standard Gaussian CDF and erf is the error function. It is smoother than ReLU and widely used in models such as BERT [83] and GPT [282] for better gradient flow control. Since calculating the Gaussian error function for each neuron is computationally expensive and time-consuming, there are approximations using tanh and sigmoid functions, corresponding to $\\mathrm { G E L U } _ { \\mathrm { t a n h } }$ and SiLU: (iii) GELU with tanh is defined as $\\mathrm { G E L U } _ { \\mathrm { t a n h } } ( x ) = 0 . 5 \\cdot x \\cdot \\left( 1 + \\operatorname { t a n h } \\left( \\sqrt { \\frac { 2 } { \\pi } } \\cdot ( x + 0 . 0 4 4 7 1 5 \\cdot x ^ { 3 } ) \\right) \\right)$ This approximation uses the Tanh function to simplify computations. (iv) SiLU (Sigmoid Linear Unit) [97] is calculated as $\\mathrm { S i L U } ( x ) = x \\cdot \\mathrm { s i g m o i d } ( x ) = x \\cdot { \\frac { 1 } { 1 + e ^ { - x } } } .$ . It effectively combines the sigmoid function with its input, enhancing modeling capabilities. (v) SwiGLU (Swish-Gated Linear Units) [304] integrates the Swish activation function with Gated Linear Units, defined as ${ \\mathrm { S w i G L U } } ( x ) = { \\mathrm { S w i s h } } ( x \\cdot W + b ) \\odot ( x \\cdot V + c )$ where $W , V$ are the weight matrix and $b , c$ are the bias terms. The Swish function is expressed as $\\operatorname { S w i s h } ( x ) = x \\cdot \\operatorname { s i g m o i d } ( x )$ . This combination enhances expressiveness and computational efficiency, making it a preferred choice in advanced models such as the Qwen series [402]. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Positional Embeddings in Transformer models [346] are essential for capturing token order, providing context about relative positions within a sequence. Traditional positional embeddings in the Transformer architecture utilize a sinusoidal function, defined as: $\\begin{array} { r } { P E ( p o s , 2 i ) = \\sin \\left( \\frac { p o s } { 1 0 0 0 0 ^ { 2 i / d _ { \\mathrm { m o d e l } } } } \\right) } \\end{array}$ $\\begin{array} { r } { P E ( p o s , 2 i + 1 ) = \\cos \\left( \\frac { p o s } { 1 0 0 0 0 ^ { 2 i / d _ { \\mathrm { m o d e l } } } } \\right) } \\end{array}$ where ?????? represents the position within the sequence, $i$ is the dimension index, and $d _ { \\mathrm { m o d e l } }$ is the dimensionality of the model. To improve the model’s capacity for understanding the relative positions of tokens within a sequence, Rotary Positional Embedding (RoPE) [318] introduces a rotational matrix to the embeddings. RoPE significantly enhances the positional encoding by maintaining the relative distances through rotational transformations, thus optimizing the model’s interpretative ability regarding sequence dynamics. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "Layer Normalization [185] stabilizes the training process by normalizing layer outputs, accelerating convergence. Two types of layer normalization are commonly used [185]: (i) Non-Parametric Layer Norm normalizes inputs using the mean and variance calculated across the layer’s dimensions without learnable parameters as $\\textstyle \\operatorname { L N } ( x ) = { \\frac { x - \\mu } { \\sigma } }$ where $\\mu$ is the mean and $\\sigma$ is the standard deviation of the inputs. Its simplicity makes it ideal for SLMs. (ii) Parametric Layer Norm includes learnable parameters $\\gamma$ and $\\beta$ for adaptive scaling and bias, enhancing model flexibility: $\\begin{array} { r } { \\operatorname { P L N } ( x ) = \\gamma \\left( \\frac { x - \\mu } { \\sigma } \\right) + \\beta } \\end{array}$ Additionally, RMS Norm (Root Mean Square Layer Normalization) [431] simplifies the calculation by using the root mean square of inputs, reducing computational demands: $\\begin{array} { r } { \\mathrm { R M S N o r m } ( x ) = \\gamma \\frac { x } { \\sqrt { \\frac { 1 } { N } \\sum _ { i = 1 } ^ { N } x _ { i } ^ { 2 } + \\epsilon } } + \\beta } \\end{array}$ where $N$ is the number of inputs, $x _ { i }$ is the $i$ -th input, and $\\epsilon$ is a small constant to prevent division by zero. ",
        "page_idx": 6
    },
    {
        "type": "text",
        "text": "2.1.2 Mamba. The attention mechanism in Transformer suffers from a drawback: it requires recalculating attention scores with every previous token for each new token generated during inference, leading to quadratic time complexity. This increases the inference cost as sequence lengths grow. In contrast, Mamba [75, 117], based on state space models (SSMs) [164] which are a superclass of recurrent neural networks (RNNs), rely only on the last hidden state for generating the next token, enabling faster inference speeds, as shown in Figure 5. To address the Linear Time Invariant nature of traditional SSMs, which hinders their ability to focus on or ignore specific inputs, Mamba improves SSMs with a dynamic selection mechanism. This mechanism selectively filters out irrelevant information while retaining essential data, tailored to the content of the input. Leveraging this selective SSM foundation, Mamba adeptly captures complex global relationships within sequence data. Due to its focus on the immediate previous hidden state, as opposed to Transformer which requires access to all previous hidden states, Mamba achieves a higher utilization rate of model parameters. This makes it more suitable for SLMs. However, we identify two drawbacks of Mamba: (i) its focus on selectively capturing global information may compromise performance on tasks that require nuanced understanding, such as detailed sentiment analysis or complex entity recognition; and (ii) to balance inference speed, Mamba’s recurrent structure primarily encodes static global information, which limits its effectiveness in handling multi-round tasks within a single query, such as interactive dialogue systems or iterative problem-solving scenarios. ",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/c910d53cf633fd17f0283ebbd319032712214d9a42da2f28a15fe0577ca151de.jpg",
        "image_caption": [
            "Fig. 5. Mamba 1 architecture [117]. "
        ],
        "image_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "In language modeling, Mamba 1 [117] is pre-trained on the Pile dataset [108] using the training recipe from [37] and ranges from 125M to 1.3B parameters. It outperforms comparable models such as Pythia [31] and RWKV [270] in various tasks; for instance, Mamba-1.4B achieves a $3 2 . 8 \\%$ accuracy on the Arc-Challenge [65] dataset, surpassing Pythia-1.4B’s $2 8 . 5 \\%$ and RWKV-1.5B’s $2 9 . 4 \\%$ . Mamba 2 [75] develops a theoretical framework linking SSMs with attention mechanisms through structured semi-separable matrices, enhancing the selective SSM to achieve $2 { - } 8 \\mathrm { x }$ faster speeds while competing with Transformer models. Training and configuration for Mamba 2 align with Mamba 1. Additionally, Mamba-series models are applied widely across different fields [160, 280, 281, 463]. Other follow-up Mamba-based language models such as Falcon Mamba 7B [463] and Mistral 7B [160] also demonstrate the efficiency of the architecture for NLP tasks. Falcon Mamba 7B scales Mamba’s long-sequence processing capabilities to extensive language data, reducing memory overhead and making it ideal for long-form NLP tasks. Similarly, Mistral 7B incorporates Mamba’s efficient sequence handling ability to improve processing speed and computational efficiency in long-context NLP tasks, showcasing Mamba’s scalability and practicality for large-scale language modeling. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "2.1.3 Hymba. Attention heads in the Transformer facilitate high-resolution recall, while SSM heads in Mamba efficiently summarize context. To balance performance and efficiency for SLMs, Hymba [90] integrates both attention and SSM heads within the same layer, allowing for parallel and complementary processing of inputs, as depicted in Fig",
        "page_idx": 7
    },
    {
        "type": "image",
        "img_path": "images/6cd678b847173b848c1429c04ccfa78a775c56e46fec00eed0142c8567e93bcd.jpg",
        "image_caption": [
            "Fig. 6. Hymba [90] architecture. "
        ],
        "image_footnote": [],
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "ure 6. This hybrid-head approach enables each layer to simultaneously leverage the high-resolution recall of attention heads and the contextual summarization of SSMs, increasing the model’s flexibility and expressiveness in managing diverse information flows and memory access patterns. ",
        "page_idx": 7
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Hymba has been developed in models of varying sizes—125M, 350M, and 1.5B, trained on a combination of the DCLM-Baseline-1.0 [192], SmolLM-Corpus [27], and a proprietary high-quality dataset, with token counts of 1 trillion, 250 billion, and 50 billion, respectively. The models incorporate the Warmup-Stable-Decay (WSD) learning rate scheduler [143] and the data annealing technique [94] to ensure stable pretraining, conducted on 128 NVIDIA A100 GPUs. The 1.5B base model was post-trained using full finetuning (FFT), followed by direct preference optimization (DPO) [283] to develop the Hymba-1.5B-Instruct model. In commonsense reasoning tasks, the Hymba 1.5B model surpasses Llama3.2-3B [7] by achieving $1 . 3 2 \\%$ higher average accuracy, requiring an $1 1 . 6 7 \\times$ smaller cache size, and delivering a $3 . 4 9 \\times$ increase in processing speed. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "2.1.4 xLSTM. Long Short-Term Memory (LSTM) [137] shares a conceptual similarity with Mamba that achieves success in language modeling through the introduction of time-dependent weights. This similarity raises an intriguing question: how effective would LSTMs be at language modeling if scaled to billions of parameters, incorporating advanced techniques from modern LLMs while addressing known LSTM limitations? Inspired by this question, Beck et al. [25] propose xLSTM architecture, which performs favorably compared to state-of-the-art Transformers and SSMs in empirical evaluations. To address the limitations of LSTM, xLSTM designs exponential gates to enhance effectiveness with long sequences, expand memory cells from scalars to matrices to increase storage capacity, and remove memory mixing to enable parallel processing. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "To test the language modeling capabilities of xLSTM scaled to billions of parameters, it is trained on a large dataset comprising 300 billion tokens from SlimPajama [314] across various model sizes (125M, 350M, 760M, 1.3B). The performance of pre-trained xLSTM is compared against RWKV-4 [270], Llama [153], and Mamba [117] across 571 text domains of the PALOMA benchmark [237] and various downstream tasks. Across all model sizes and the majority of tasks, xLSTM consistently outperforms the others, suggesting that larger xLSTM models could become formidable competitors to existing Large Language Models that utilize Transformer technology. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "2.2 Training SLMs from Scratch ",
        "text_level": 1,
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Training SLMs from scratch entails several critical steps: (i) Pre-training, focused on acquiring general features and knowledge from the corpus; (ii) Fine-tuning, targeted at boosting the model’s abilities and performance for specific tasks; (iii) Decoding strategies, which involve the methods used for iteratively selecting the next token during generation. ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "2.2.1 Pre-training. Typically, the pre-training paradigm for language models is divided into encoder-based and decoderbased approaches. Encoder-based models, such as BERT [83], utilize Masked Language Modeling (MLM) tasks where the goal is to predict masked tokens within a sentence. This is achieved by maximizing: ",
        "page_idx": 8
    },
    {
        "type": "equation",
        "img_path": "images/88af859599a62dce2c28c3668d1cdea5e01201f40e87bdbce8df43504141cd84.jpg",
        "text": "$$\n\\begin{array} { r } { P ( \\mathrm { m a s k e d t o k e n } \\mid \\mathrm { c o n t e x t } ) = \\mathrm { s o f t m a x } ( \\mathbf { W } \\cdot \\mathbf { h } _ { \\mathrm { m a s k } } + b ) , } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "where masked token is the original token that has been masked, context represents the other unmasked tokens in the sentence, $\\mathbf { W }$ and $^ { b }$ are trainable parameters of a linear output layer, $\\mathbf { h } _ { \\mathrm { m a s k } }$ is the output from the transformer encoder for the masked position, and softmax is the activation function that converts logits to probabilities over the vocabulary. This process enhances the model’s language encoding capabilities. Decoder-based models, such as GPT [282], employ ",
        "page_idx": 8
    },
    {
        "type": "text",
        "text": "Next Token Prediction (NTP) tasks, aiming to model the distribution of the next token by maximizing: ",
        "page_idx": 9
    },
    {
        "type": "equation",
        "img_path": "images/94d107ba2821c802b7a73aa1983a1886476e568dfa1529db7ab8ea32a3f5338d.jpg",
        "text": "$$\n\\begin{array} { r } { P ( \\mathrm { n e x t ~ t o k e n } \\mid \\mathrm { c o n t e x t } ) = \\mathrm { s o f t m a x } ( \\mathbf { W } \\cdot \\mathbf { h } _ { \\mathrm { l a s t } } + b ) , } \\end{array}\n$$",
        "text_format": "latex",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "where next token is the token that the model aims to predict, context represents the sequence of tokens preceding the token to be predicted, and $\\mathbf { h } _ { \\mathrm { l a s t } }$ is the output from the transformer encoder for the last token in the context. Effective data preprocessing, crucial for optimizing the performance of SLMs trained from scratch, involves meticulous data cleaning and strategic tokenization. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Data Cleaning involves techniques such as filtering, deduplication, and noise reduction, which improve data quality and help the model generalize better. Filtering noisy or irrelevant data, addressing outliers, and handling imbalances in the dataset ensure that the training data is both representative and efficient. Deduplication, in particular, helps prevent overfitting by removing repeated instances, making the model more robust with efficient parameter usage. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Tokenization plays a vital role in handling diverse vocabularies without increasing model size. Advanced methods such as Byte-Pair Encoding (BPE) [106] and WordPiece [316] break text into subwords [83], allowing the model to manage rare and compound words efficiently. These strategies ensure that SLMs maintain a balance between vocabulary coverage and model compactness, crucial for improving generalization while minimizing computational demands. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "2.2.2 Fine-Tuning. After the initial training, SLMs are fine-tuned on specific tasks using task-specific data and loss functions. Parameter-efficient fine-tuning methods [118, 140, 142, 199], such as Low-Rank Adaptation (LoRA), prefixtuning, and adapter modules, are particularly effective for SLMs. Low-Rank Adaptation (LoRA) [142] modifies Transformer weights by introducing trainable low-rank matrices A and B for efficient fine-tuning, avoiding significant alterations to pre-trained weights. The update is represented as: $\\Delta \\mathbf { W } = \\mathbf { A } \\mathbf { B } ^ { \\top }$ The fine-tuned weight matrix used in Transformer operations then becomes: $\\mathbf { W } _ { \\mathrm { f t } } = \\mathbf { W } + \\alpha \\Delta \\mathbf { W }$ where $\\alpha$ is a scaling factor adjusting the adaptation’s impact, allowing fine-tuning on a smaller set of parameters while retaining the model’s foundational capabilities. Prefix-Tuning [199] prepends learnable prefixes to the input sequence, guiding the model’s attention without altering core model parameters. It is especially useful for generative tasks. Adapter Modules [140] are small, trainable layers inserted into the pre-trained model. These layers are fine-tuned on task-specific data, allowing the base model to remain fixed while the adapters learn the necessary adjustments. The typical structure of an adapter module includes a down-projection, a non-linearity, and an up-projection: Adapter $\\mathbf { \\Pi } ( \\mathbf { h } ) = \\mathbf { h } + \\mathbf { W } _ { \\mathrm { u p } } \\cdot \\sigma ( \\mathbf { W } _ { \\mathrm { d o w n } } \\cdot \\mathbf { h } + \\mathbf { b } _ { \\mathrm { d o w n } } ) + \\mathbf { b } _ { \\mathrm { u p } }$ where h is the input hidden state, $\\mathbf { W } _ { \\mathrm { d o w n } }$ and $\\mathbf { W } _ { \\mathrm { u p } }$ are the projection matrices, $\\mathbf { b } _ { \\mathrm { d o w n } }$ and ${ \\bf b } _ { \\mathrm { u p } }$ are the bias terms, and $\\sigma$ is a non-linear activation function. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "2.2.3 Decoding Strategies. After pre-training or fine-tuning, employing an effective decoding strategy is crucial for generating output from language models. Decoding, the process of text generation from SLMs, involves iteratively selecting the next word. A fundamental method is the greedy search, which predicts the most likely token at each step. This is formally modeled as: $x _ { i } = \\arg \\operatorname* { m a x } _ { x } P ( x \\mid x _ { < i } )$ , where $x _ { i }$ is the token with the highest probability at the $i$ -th step, conditioned on the preceding context $x _ { < i }$ . Other decoding strategies, such as beam search or top- $\\mathbf { \\nabla } \\cdot \\mathbf { k }$ sampling, are crucial for generating high-quality outputs. Beam search balances exploration and exploitation by considering multiple possible sequences simultaneously, while top- $\\mathbf { \\nabla } _ { \\cdot \\mathrm { k } }$ sampling introduces diversity and creativity in text generation. These strategies collectively ensure that SLMs are efficient and capable of delivering high performance across various natural language processing tasks. ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "Manuscript submitted to ACM ",
        "page_idx": 9
    },
    {
        "type": "text",
        "text": "2.3 Obtain SLMs from LLMs ",
        "text_level": 1,
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Obtaining an SLM from an LLM is crucial for deploying in resource-constrained environments. Instead of training from scratch, leveraging an LLM allows for knowledge transfer, enabling SLMs to retain much of the LLM’s linguistic and domain knowledge with reduced training time and data. To obtain SLMs from LLMs, three primary techniques are used: pruning, knowledge distillation, and quantization. Pruning removes less critical parameters, reducing model size while aiming to maintain performance. Knowledge distillation transfers knowledge from a large teacher model to a smaller student model, preserving much of the original model’s understanding. Quantization decreases parameter precision, significantly lowering memory and computation needs with minimal impact on accuracy. These methods balance size reduction, efficiency, and performance retention. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "2.3.1 Pruning. Pruning is a technique used to reduce a model’s size and computational requirements (e.g., LLMs) without significantly sacrificing its performance [126]. This process involves identifying and removing less important or redundant parameters and components from the model. The primary goal of LLM pruning is to make the model more efficient, faster, and suitable for deployment in resource-constrained environments. Typically, pruning can be categorized into two main types: unstructured pruning and structured pruning. An illustration of unstructured pruning and structured pruning is shown in Figure 7. ",
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/bc5d3036a35f80350c1a852c4442462e99358340f2f1f023976984b679b5e8f2.jpg",
        "image_caption": [
            "Fig. 7. Unstructured and structured pruning. "
        ],
        "image_footnote": [],
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Unstructured Pruning [76, 102, 201, 300, 321, 438, 442] prunes an LLM by removing weights individually without considering its internal structure. The least significant parameters are pruned according to specific criteria (e.g. magnitude or impact on the output). This method can achieve significant compression while maintaining performance. However, it can also lead to irregular memory access patterns and reduced hardware efficiency because the pruned model lacks a regular structure. SparseGPT [102] is a representative unstructured pruning method that can reduce large-scale GPT models like OPT-175B [440] and BLOOM-176B [181] to up to $6 0 \\%$ sparsity using a novel sparse regression solver. Wanda [321] combines weight magnitudes with input activations to efficiently identify and discard less impactful parameters. It operates in a single forward pass, rapidly achieving high sparsity without retraining. It is also worth noting that recent studies specifically address the compatibility issues between pruning and Low-rank Adaptation (LoRA) [142], such as LoRAPrune [438]. ",
        "page_idx": 10
    },
    {
        "type": "text",
        "text": "Structured Pruning [13, 19, 51, 110, 124, 188, 196, 232, 243, 305, 306, 385, 410, 446], which prunes an LLM by targeting entire structural components—such as neurons, channels, or layers—rather. This approach allows for a direct reduction in dimensionality, thus efficiently reducing model complexity and memory usage. Although structured pruning may lead to higher accuracy degradation than unstructured pruning, it simplifies implementation without requiring specialized hardware. ShortGPT [243] proposes the Block Influence (BI) metric, which measures the significance of each layer based on its transformation of hidden states. Essentially, a transformer block’s influence is measured by how much it alters the hidden states. By calculating BI scores, ShortGPT determines which layers contribute minimally to the overall performance and removes these low-importance layers. This simple yet effective layer removal strategy significantly reduces the model’s parameters and computational requirements. LLM Pruner [232] offers a method to efficiently prune LLMs without access to the original training dataset. It employs a three-step compression pipeline: Discovery (identifying interdependent structures), Estimation (evaluating the performance impact of each group), and Recovery (post-training to address performance loss). NutePrune [196] enhances structured pruning with a ",
        "page_idx": 10
    },
    {
        "type": "image",
        "img_path": "images/dac7bbe74f6604b3a7c2f738489f1e9f8bd44ea608360eca77d1fe9e35bf73c0.jpg",
        "image_caption": [
            "Fig. 8. Illustration of white-box and black-box knowledge distillation [245]. "
        ],
        "image_footnote": [],
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "Numerous-teacher method, employing variable sparsity masks and LoRA modules to guide the pruning process. This approach effectively reduces model size and complexity. COST-EFF [305] introduces a slenderized backbone—a form of structured pruning—and a multi-exit model that employs task-specific calibration through knowledge distillation. This slenderization reduces the model’s spatial footprint, while the multi-exit strategy effectively balances utility and runtime costs. To enhance the flexibility of structural pruning, DISP-LLM [110] breaks the structural dependencies in regular methods by allowing different layers to have different subsets of features along the embedding dimension. ",
        "page_idx": 11
    },
    {
        "type": "text",
        "text": "2.3.2 Knowledge Distillation. Knowledge distillation (KD) compresses a larger teacher model into a smaller student model by training the student to mimic the teacher’s outputs [136]. This enables the student to retain much of the teacher’s capabilities with fewer parameters, making it ideal for scaling down LLMs for resource-limited environments while maintaining performance. KD can be categorized into white-box and black-box approaches [361, 403, 457] as shown in Figure 8. In White-Box KD, the student has access to the teacher’s internal states or output distributions [6, 119, 157, 169, 173, 265, 381, 434]. Generalized Knowledge Distillation (GKD) [173] introduces skew KL divergence to stabilize gradients and enhance performance, using an adaptive off-policy approach to minimize noisy feedback and improve efficiency. Black-Box KD relies only on teacher outputs without having access to model internals [48, 182, 271, 360]. Methods like Distilling Step-by-Step [141] use teacher-generated rationales to train smaller models, improving performance with fewer examples. LaMini-LM [380] creates a diverse instruction dataset with GPT-3.5 Turbo responses, enabling robust performance in smaller models. ",
        "page_idx": 11
    },
    {
        "type": "image",
        "img_path": "images/f384686f4977d2530cb223bab852d8696ab57ce6dde9887671940026792ea987.jpg",
        "image_caption": [
            "Fig. 9. Illustration of quantization-aware training (QAT) and post-training quantization (PTQ). "
        ],
        "image_footnote": [],
        "page_idx": 11
    },
    {
        "type": "table",
        "img_path": "images/53be93c32e7dfda8e6f37a75c53b4f3413729758e8e135240489a3c89db07e90.jpg",
        "table_caption": [
            "Table 1. Representative quantization methods. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Methods</td><td>Bit</td><td>Type</td><td>Technical Contribution</td><td>Problems</td></tr><tr><td>SqueezeLLM[170]</td><td>3-bit</td><td>PTQ</td><td>Sensitivity-based non-uniform quantization, dense and sparse decomposition</td><td>ultra-low bit quantization</td></tr><tr><td>JSQ [122] FrameQuant [4]</td><td>Flexible Fractional bit</td><td>PTQ</td><td>Joint Sparsification and Quantization Fractional bit widths</td><td>better compression-accuracy trade-offs. better compression-accuracy trade-offs.</td></tr><tr><td>OneBit [400]</td><td>1-bit</td><td>PTQ PTQ</td><td>Quantization-aware knowledge distillation</td><td>1-bit quantization</td></tr><tr><td>BiLLM [147]</td><td>1-bit</td><td>PTQ</td><td>Crucial Weights Selection,Block-based error compensation</td><td>1-bit quantization</td></tr><tr><td>LQER [432] I-LLM[144]</td><td>Flexible Flexible</td><td>PTQ PTQ</td><td>Quantization Error Minimization Fully-Smooth Block-Reconstruction， Dy-</td><td>better compression-accuracy trade-offs Integer-only Quantization</td></tr><tr><td>PV-Tuning [239]</td><td></td><td></td><td>namic Integer-only MatMul and Integer-only Non-linear Operators</td><td></td></tr><tr><td>BitNet [357]</td><td>1-bit/2-bit 1-bit</td><td>PTQ</td><td>PV algorithm</td><td>better compression-accuracy trade-offs.</td></tr><tr><td>BitNet b1.58 [231]</td><td>{-1,0,1}</td><td>QAT</td><td>1-bit Transformer Architecture</td><td>1-bit quantization</td></tr><tr><td>PEQA[168]</td><td>Flexible</td><td>QAT</td><td>Ternary Parameters</td><td>1-bit quantization</td></tr><tr><td></td><td></td><td>QAT</td><td>Quantization Scales Optimization</td><td>Parameter-Effcient Finetuning</td></tr><tr><td>QLoRA[81]</td><td>NF4</td><td>QAT</td><td>4-bit NormalFloat and Double Quantization</td><td>Parameter-Efficient Finetuning</td></tr></table>",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "2.3.3 Quantization. Quantization reduces the storage and computational demands of LLMs by converting floatingpoint representations into lower-precision formats, significantly cutting both storage requirements and computational complexity. Existing methods fall into two categories: Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT). Figure 9 illustrates the two quantization methods. Post-Training Quantization, applied after training, simplifies model compression without altering the architecture or requiring retraining, though it may result in precision loss. Consider a group or block of weights w; the linear operation can be expressed as $y = \\mathbf { w x }$ , while the quantized version is given by ?? = ?? (w)x. Generally, the quantization function ?? is defined as [207]: ?? (w) = Δ · Round \u0000 wΔ \u0001 , Δ = max( |w| )2?? −1 , where $N$ is the number of quantization bits, and $\\Delta$ is the quantization scale factor determined by the absolute maximum value of w. Quantization-Aware Training (QAT) enhances LLM efficiency by directly incorporating quantization into the training process, often resulting in higher accuracy than PTQ. During QAT, the forward pass utilizes quantized weights $Q ( \\mathbf { W } )$ and activations $Q ( \\mathbf { X } )$ , while retaining full-precision values during the backward pass and for updating gradients to ensure stable learning dynamics. The comparisons of the post-training quantization methods are summarized in Table 1, detailing precision, addressed problems, and technical contributions of each method. ",
        "page_idx": 12
    },
    {
        "type": "text",
        "text": "2.3.4 Low-Rank Techniques. Low-rank techniques compress LLMs by approximating a high-dimensional weight matrix with two lower-dimensional matrices, reducing computational and memory requirements. A matrix $\\mathbf { W } \\in \\mathbb { R } ^ { m \\times n }$ is approximated as $\\mathbf { W } \\approx \\mathbf { A } \\times \\mathbf { B }$ , where $\\mathbf { A } \\in \\mathbb { R } ^ { m \\times r }$ and $\\mathbf { B } \\in \\mathbb { R } ^ { r \\times n }$ , with $r$ much smaller than $m$ or $n$ , reducing the number of parameters. Building on this concept, Ji et al. [158] propose a low-rank method tailored for LLMs, leveraging the observation that while LLMs have high-rank weights, their feature interactions tend to exhibit low-rank properties. The method estimates feature distributions using pooled covariance matrices and allocates distinct compression ratios to layers based on their sensitivity to low-rank compression. A Bayesian optimization strategy, using a Gaussian process as the surrogate model, optimizes the allocation of low-rank dimensions, ensuring the model maintains performance while achieving significant compression. Transitioning from model compression to fine-tuning, Cho et al. [62] tackles system and data heterogeneity with the HETLORA method, which uses heterogeneous low-rank approximations to accommodate the diverse capabilities of clients and data complexities. By combining local rank self-pruning with sparsityweighted aggregation, it balances high and low-rank LoRA modules, improving convergence speed and performance compared to uniform approaches. LLM-Neo [409] combines knowledge distillation with low-rank adaptation (LoRA) to improve the efficiency of transferring knowledge from a teacher LLM to a compact student model. ",
        "page_idx": 12
    },
    {
        "type": "table",
        "img_path": "images/0182c2e470b13d8149fe7417e49c57b59fa2cdc62824ea07749fc8a85c4cfda8.jpg",
        "table_caption": [
            "Table 2. Advanced enhancement methods for SLM. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Topic</td><td>Method</td><td>Main Contribution</td></tr><tr><td>Training from Scratch</td><td>MindLLM[412] MobiLlama [336] MobileLLM[223]</td><td>Bilingual models with advanced features. On-device SLM with dual objectives for efficiency and capability. Optimizes LLM deployment on mobile with advanced architecture.</td></tr><tr><td>Supervised Fine-tuning</td><td>MobileBERT322] Alpaca 7B [329] RLHF [264] DPO [283]</td><td>Compact BERT for efficient fine-tuning. Uses ChatGPT-generated tasks to tune Llama 7B. Trains using human-preferred data and reinforcement learning.</td></tr><tr><td>Data Quality in KD</td><td>TinyStory [96] AS-ES [384] Self-Amplify [29]</td><td>Dynamically adjusts log probabilities to prevent model degradation. Enhances narrative coherence in child-friendly datasets. Improves CoT by categorizing reasoning steps. Automates CoT data annotation for small models.</td></tr><tr><td>Distillation for SLM</td><td>GKD [6] DistiLLM [173] Adapt-and-Distill [414] SmoothQuant [386]</td><td>Aligns training and inference distributions using on-policy sequences. Uses skew KL divergence and adaptive off-policy for output utilization. Domain adapts both teacher and student models before distillation.</td></tr><tr><td>Quantization</td><td>BiLLM[147] LLM-QAT [221] PB-LLM [299] OneBit [400] BitNet [357] BitNetb1.58[231]</td><td>Balances quantization difficulty using per-channel scaling. Applies Hessian-based metrics for binary residual approximation. Uses data-free knowledge distilation and logit distilation forfine-tuning. Binarizes non-salient weights while preserving others in higher precision. Achieves near 1-bit quantization with minimal performance loss. Introduces 1-bit Transformer architecture with BitLinear layers.</td></tr><tr><td>LLM techniques</td><td>Moqt[234] SLM-RAG [215]</td><td>Implements a ternary weight system in enhanced BitNet. Copmpiesfiterianderanlighis oue Informatin ratin tas Shows that SLMs with RAG can match LLM performance.</td></tr></table>",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "3 ADVANCED ENHANCEMENT STRATEGIES FOR SMALL LANGUAGE MODELS ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "With the foundational concepts introduced in Section 2, this section explores various advanced techniques that enhance the performance of SLMs, including innovative training methods for training SLMs from scratch, supervised fine-tuning (SFT) to align SLMs to adhere to instructions, advanced knowledge distillation and quantization techniques, and techniques frequently used in LLMs such as mixture-of-experts to enhance SLM for specific applications. A summary of enhancement techniques is also summarized in Table 2. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "3.1 Innovative Training Methods for Small Language Models from Scratch ",
        "text_level": 1,
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "In scenarios with limited resources, we aim to train small language models to provide efficient, cost-effective solutions tailored for specific domains, while still maintaining competitive performance with larger models. Training small language models (SLMs) from scratch involves unique strategies that diverge significantly from those used for large language models (LLMs). This section synthesizes cutting-edge techniques tailored to optimize the inherent capabilities of SLMs, underscoring their potential to match or surpass larger counterparts in efficiency and effectiveness. As shown in Figure 10, the methods for training SLMs from scratch can be categorized into three primary categories: Architecture Design, Data Construction, and Optimization Strategy. Next, we introduce each category in detail. ",
        "page_idx": 13
    },
    {
        "type": "text",
        "text": "Architecture Design for SLMs When designing SLM architectures, parameter-sharing techniques are employed to minimize space usage and reduce the model’s size. As shown in the first part of Figure 10, parameter sharing is achieved by two approaches: (i) a single Feed-Forward Network (FFN) module is shared by every transformer layer. As shown in Figure 10 (1) middle, FFN layer sharing/reusing can maintain a smaller size while still benefiting from the depth and complexity gained through repeated processing of input data. This technique is firstly applied in MobiLlama [336] which surpasses the performance of existing SLMs of comparable size. (ii) Entire transformer blocks are shared. As shown in Manuscript submitted to ACM ",
        "page_idx": 13
    },
    {
        "type": "image",
        "img_path": "images/1d496ff147a34dad2f537bee9636ed019e3dc371882a648ff85001144a298f46.jpg",
        "image_caption": [
            "Fig. 10. Innovative Training Methods for Small Language Models from Scratch "
        ],
        "image_footnote": [],
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Figure 10 (1) right, Transformer Block-wise Sharing is another parameter-sharing approach that maintains depth and complexity. There are different transformer block-wise sharing strategies such as repeating the transformer blocks all over again or repeating the immediate transformer block. This technique is applied in MobileLLMs [223] which has 125M and 350M parameters. MobileLLMs demonstrate performance improvements of $2 . 7 \\%$ and $4 . 3 \\%$ , respectively, compared to previous models with equivalent parameters. Moreover, they exhibit accuracy comparable to LLaMa-2-7B on API call tasks, highlighting the capabilities of smaller models in mobile environments. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Data Construction For SLMs, the emphasis on data quality surpasses that of quantity and diversity [412]. Experiments demonstrate that using a quality filtering approach to remove low-quality data can lead to improved performance in SLMs [412]. Unlike large models, which can handle diverse and large datasets, SLMs benefit more from cleaner, high-quality data probably due to their limited capacity against noise. Generally, data processing has several steps: (i) Remove HTML, CSS, JS, and non-text elements for clean text; (ii) Filter low text-to-content ratio web pages; (iii) Deduplicate using SimHash [77, 293]; (iv) Exclude sensitive/offensive content with heuristics and token replacements; (v) Remove self-repeating phrases of advertisements to enhance dataset informativeness [47, 412]. These steps collectively ensure that training data has high-quality, informative texts. SLMs also significantly benefit from these techniques. For example, MindLLMs [412], which are bilingual lightweight language models (available in 1.3B and 3B versions), adopt these data processing techniques and achieve improved capability acquisition. ",
        "page_idx": 14
    },
    {
        "type": "text",
        "text": "Training Strategy for SLMs For LLMs, due to the large model size and data volume, LLMs are usually trained with one round. For SLMs, multiple-round training can be applied [328]. Considering some examples are hard to fit, hard examples can be trained with a high probability [328]. For each round of training, the data sampling probability is updated according to the overall loss of that sample. Experiments results show that two rounds of training and a $5 0 \\%$ sampling rate are a good trade-off between performance and training efficiency. Tang et al. [328] show that a deep and thin neural architecture and multiple-round training can enhance the performance of the trained Pangu 1.5B pro model. This model outperforms the conventionally trained Pangu 1.5B and a series of other comparable large language models with similar model sizes on multiple benchmark datasets, achieving an average performance increase of $8 . 8 7 \\%$ . ",
        "page_idx": 14
    },
    {
        "type": "image",
        "img_path": "images/bbd77bd34f4d7955af6799a68109e49f177df7268d73ddaf5fc5f74d572be046.jpg",
        "image_caption": [
            "Fig. 11. Fine-tuning for Enhancing SLMs "
        ],
        "image_footnote": [],
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "3.2 Supervised Fine-Tuning (SFT) for Enhancing SLM performance ",
        "text_level": 1,
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "Supervised Fine-Tuning (SFT) employs a training methodology similar to pre-training but is specifically tailored to align models to adhere to the instructions encapsulated within various instructional datasets. This approach is designed to refine the model’s responsiveness and appropriateness to given contexts as dictated by the training data. For example, various models, such as Alpaca [329], UltraChat [86], WizardLM [392], SlimOrca [203], ShareGPT [356], Capybara [72], Deita [216], and MetaMathQA [420], incorporates a suite of conversational datasets to enhance their capabilities in context-aware dialogue and instruction adherence. Usually, as shown in Figure 11, existing SFT methods can be categorized into three categories: ",
        "page_idx": 15
    },
    {
        "type": "text",
        "text": "• (i) Classical fine-tuning with downstream data [83, 282] trains SLMs on task-specific annotated data, transferring general language representations to specific tasks such as sentiment analysis. In the LLM era, this approach remains effective, such as enhancing LLMs by calibrating responses or assigning risk scores with smaller models such as BERT [450], or optimizing for mobile devices with MobileBERT [322].   \n• (ii) Instruction tuning with LLM-generated data [86, 203, 329] or human-generated questions with LLM annotations [356] aims to align generative models with specific instructions, enhancing their instruction-following and reasoning capabilities. For example, Alpaca 7B [329] uses 52k ChatGPT-generated instruction-following examples from 175 self-instructed seed tasks to tune Llama 7B [338]. Meanwhile, StableLM [26, 340] is trained on the Restruct-v1 dataset, which includes summarization, question-answering, and sentiment analysis tasks, using instruction data from [226].   \n• (iii) Preference optimization with human feedback [264, 283, 356] aims to better align language models with human preferences. Reinforcement Learning from Human Feedback (RLHF) [264] gathers human-preferred data, trains a reward model, and fine-tunes the LM using reinforcement learning. Direct Preference Optimization (DPO) [283] provides a simpler alternative to RLHF. Unlike RLHF, DPO avoids explicit reward modeling and reinforcement learning techniques. Instead, it adjusts the log probabilities of preferred versus non-preferred responses using a dynamic weighting mechanism, preventing model degradation issues typical of methods relying on probability ratios. For instance, Llama 3.2 1B & 3B apply SFT and DPO in post-training to enhance alignment with instructions and human preferences. ",
        "page_idx": 15
    },
    {
        "type": "image",
        "img_path": "images/40b74914fda927e82ca795290fe714f8e91ef13c65ad3e542a958b9d257aa21e.jpg",
        "image_caption": [
            "Fig. 12. Data Quality in Knowledge Distillation (KD) "
        ],
        "image_footnote": [],
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "3.3 Data Quality in Knowledge Distillation (KD) ",
        "text_level": 1,
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Transitioning from the discussion on training SLMs from scratch, this section delves into the critical role of data quality in Knowledge Distillation (KD). The motivation here is to highlight how high-quality data generated from LLMs can significantly enhance the learning efficiency and performance of SLMs. The central idea is that meticulously crafted datasets when used in KD, enable SLMs to more effectively mimic the advanced capabilities of their larger counterparts. As shown in Figure 12, the data can come either from (1) other strong LLMs (e.g., GPT-4 [2]) which are much larger and more powerful than the target SLM, or (2) the target SLM itself. ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Augment Data from LLMs. LLM-generated data could be categorized as pre-training data and fine-tuning data. Firstly, due to the limitations of model size, studies have shown that training SLMs requires simple and comprehensible data [96, 183, 187, 384]. As shown in Figure 12 (1) left, TinyStory [96] shows that small models (tens of millions of parameters) can generate coherent stories for 3-4-year-olds. GPT-3.5 or GPT-4 [2] prompts create simple stories from three keywords chosen from a 1,500-word vocabulary, which are then used to train SLMs for similar outputs. This approach shows that simple and comprehensible data can help smaller models exhibit behaviors similar to those of larger language models, such as obeying scaling laws and achieving enhanced performance. On the other hand, many efforts to enhance the Chain-of-Thought (CoT) capabilities of small models involve using LLMs to generate high-quality fine-tuning CoT data. As shown in Figure 12 (1) right, these data train small models end-to-end to mimic CoT reasoning [235, 384]. AS-ES Learning [384] highlights that small models struggle with complex reasoning, even when provided detailed steps, as these require nuanced extraction and abstraction. Therefore, the study introduces a paradigm splitting reasoning into extractive segments (context reminders) and abstractive segments (inferred insights). ",
        "page_idx": 16
    },
    {
        "type": "text",
        "text": "Augment Data from Itself. Besides distilling data from other LLMs, language models can also train on their own outputs [29, 145, 337]. Since voting strategies can improve the performance of LLMs, reasoning paths that lead to the majority answer can be further utilized to fine-tune LLMs [145]. Similarly, SLMs can generate their training data with the aid of existing rationale generation methods. Self-Amplify [29] notes that human annotation of Chain-of-Thought (CoT) data is very time-consuming; thus, automated rationale generation methods have been proposed. These methods Manuscript submitted to ACM involve three main steps: (1) Selection of samples $( x , y )$ that the model predicts correctly as few-shot examples; (2) Rationale generation, where rationales are produced using post hoc explanation methods; (3) Prompt design for SLMs, where the final prompt is crafted based on the previously generated rationales. ",
        "page_idx": 16
    },
    {
        "type": "image",
        "img_path": "images/5ef0bc349ca2896ceaffe910aeb9516f9094df1212bdea9facea688a55284eba.jpg",
        "image_caption": [
            "(c) Adapt-and-Distill Fig. 13. Distillation Techniques for Enhancing SLM Performance. On-policy means learning only use data from the current student (policy), while off-policy permits the use of previously gathered data. "
        ],
        "image_footnote": [],
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "3.4 Distillation Techniques for Enhancing SLM Performance ",
        "text_level": 1,
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Following the discussion on data quality in KD, this section reviews specialized KD training strategies designed to enhance the performance of SLMs. The motivation is to address the unique challenges and constraints involved in distilling knowledge from LLMs to SLMs, ensuring that the smaller models can maximize their performance gains. As shown in Figure 13, two main gaps between LLMs and SLMs lead to challenges in distillation: distribution mismatch and domain gap. Distribution mismatch [6, 173] occurs when the distribution of output sequences during training does not align with the distribution of sequences that SLMs produce during inference, leading to suboptimal performance of the student model. The domain gap [414] arises when there is a discrepancy between the domains or tasks on which the LLMs and SLMs are trained and applied. This gap can cause significant degradation in the performance of the student model if not properly addressed during the distillation process. To address these issues, specialized strategies involve first aligning the teacher and student models with the target domain before proceeding with knowledge distillation. To explore these challenges further, we now delve into the details of these two branches of methods. ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "Distribution Mismatch In original knowledge distillation, illustrated in Figure 13 Distribution Mismatch (a), the teacher and student are provided with the same input sequences $x$ and output labels $y$ , producing probability distributions for the next token $\\cdot q$ and $p$ ). The loss is calculated as the difference between these two distributions, $D ( q , p )$ . However, a key challenge arises due to distribution mismatch: the output sequences during training $( y )$ differ in distribution from those the SLMs produce during inference $( y ^ { \\prime } )$ . To address this challenge, various techniques have been proposed. As shown in Figure 13 Distribution Mismatch (b), one approach trains the student model using on-policy sequences—sequences generated by the student itself—guided by the teacher model’s feedback. Specifically, both the student and teacher take the same input $( x )$ and the student-generated output $( y ^ { \\prime } )$ , producing probability distributions Manuscript submitted to ACM ",
        "page_idx": 17
    },
    {
        "type": "text",
        "text": "for the next token ( $\\dot { \\boldsymbol { g } }$ and $\\mathcal { P }$ , respectively). The loss is calculated as the difference between these two distributions, $D ( q , p )$ . This approach helps the student model reduce the distribution gap between training and inference by learning from the teacher’s feedback on its own generated sequences. Generalized Knowledge Distillation (GKD) [6] is the first work using this technique and improves distillation outcomes. However, a drawback of this technique is that it requires the student to constantly produce new training sequences, which can be computationally expensive. To improve efficiency, as shown in Figure 13 Distribution Mismatch (c), an adaptive off-policy approach can be used to efficiently manage student-generated outputs by storing them in a replay buffer, thereby reducing computational costs. DistiLLM [173] employs this off-policy approach and improves the efficiency of KD. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Domain Gap When training an SLM in a specific domain that differs from the domain of the LLMs, the gap between the two domains becomes problematic. As illustrated in Figure 13 Domain Gap (a), domain adaptation fine-tunes a language model, initially trained on a general corpus, using a specialized dataset such as PubMed to enhance performance in that specific domain. As illustrated in Figure 13 Domain Gap (b), Knowledge distillation transfers knowledge from the larger model to the smaller one. However, because the teacher model may not produce high-quality outputs on specialized datasets, domain adaptation is needed prior to knowledge distillation. As illustrated in Figure 13 Domain Gap (c), Adapt-and-Distill [414] tackles the domain gap by distilling general large models into smaller ones. This paper introduces AdaLM and demonstrates that the “Adapt-and-Distill” strategy—first involving domain adaptation of both the large teacher model and the small student model, followed by distillation—is the most effective compared to three other strategies: training directly from scratch, distillation followed by adaptation, and adapting the teacher model before distillation into a general small student model. These innovative techniques are crucial for enhancing the capabilities of SLMs, making them more efficient and effective for various applications. However, adapting both the teacher (LLMs) and the student (SLMs) models to the target domain can be time-consuming. Future research could focus on efficiently solving the domain gap problem. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Insights: Here are some insights from distillation techniques: ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "• Sampling SLM outputs during the training process is the main approach to resolving distribution mismatch. • Techniques like Adapt-and-Distill address the domain gap by first adapting both the teacher (LLMs) and the student (SLMs) models to the target domain before proceeding with distillation. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "3.5 Performance Improvement through Quantization ",
        "text_level": 1,
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "As mentioned in Section 2, quantization is one of the most effective methods for adapting LLMs to SLMs. However, compression to smaller sizes often compromises performance. To address the performance drop associated with quantization, various methods have been proposed. This section examines how these quantization methods specifically enhance the performance of SLMs. While the general introduction to compression methods is discussed in the compression section, the focus here is on detailing those approaches that boost the efficiency and effectiveness of SLMs. As shown in Figure 9, we categorize these quantization methods into two main approaches: Post-Training Quantization (PTQ), where quantization is conducted on a well-trained fixed model, and Quantization-Aware Training (QAT), where quantization is integrated into the training process. This section introduces advanced techniques in PTQ and QAT respectively. ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "Post-Training Quantization (PTQ) primarily includes weight quantization and activation quantization. Weight quantization aims to quantize model parameters while preserving performance. GPTQ [103] compresses LLMs to 4-bit or 2-bit by quantizing weights layer-by-layer to minimize layer-wise quantization errors. PB-LLM [299], applicable to both Manuscript submitted to ACM ",
        "page_idx": 18
    },
    {
        "type": "text",
        "text": "PTQ and QAT, retains the most salient weights while binarizing the rest based on magnitudes. BiLLM [147], another PTQ method, uses a Hessian-based metric to identify salient and non-salient weights. Salient weights undergo binary residual approximation to minimize loss, while non-salient weights are divided into sparse and concentrated groups for separate binarization, reducing quantization errors. Activation quantization faces challenges with outliers that can stretch the quantization range, causing most values to cluster at few bits and introducing significant errors. To address this, LLM.int8() [80] isolates outlier features for 16-bit processing and handles the rest in 8-bit. SmoothQuant [386] circumvents per-channel quantization issues by employing a \"smoothing\" technique that shifts the quantization challenge from activations to weights through a per-channel scaling transformation. This balance between activating and weight quantization allows effective 8-bit quantization (W8A8), preserving accuracy while significantly reducing memory and computational costs. SmoothQuant thus enhances the efficiency of SLMs in resource-constrained environments. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Quantization-Aware Training (QAT) differs from PTQ in that it includes a training phase after the model has been quantized. When models are quantized to extremes, such as 2-bit or 1-bit, performance typically drops significantly, but further training can help the model retain its capabilities. For instance, to mitigate performance degradation from binarization, PB-LLM [299] selectively binarizes only non-salient weights, preserving the most salient ones at higher precision. This method effectively reduces the model size without significantly impacting performance. Salient weights are chosen based on their magnitude, ensuring that the most influential weights maintain higher precision to preserve the model’s reasoning capabilities. The paper explores both post-training quantization (PTQ) and quantization-aware training (QAT) to fine-tune and recover the performance of partially binarized models, achieving a balance between compression and accuracy. OneBit [400] and BitNet [357] address the severe performance degradation associated with 1-bit quantization by decomposing floating-point matrices and employing mixed-precision strategies. Specifically, OneBit introduces Sign-Value-Independent Decomposition (SVID), which decomposes a floating-point matrix into a 1-bit matrix and two floating-point vectors. This method allows LLMs to be quantized to a 1-bit level while minimizing performance loss. By retaining critical information with the floating-point vectors, OneBit effectively balances extreme compression with maintaining model accuracy. BitNet b1.58 [231] improves on the original BitNet by introducing a ternary matrix weight system -1, 0, 1, resulting in a 1.58-bit model. BitNet b1.58 matches the performance of full-precision models starting from a 3 billion parameter size while further reducing memory and latency costs. LLM-QAT [221] employs data-free knowledge distillation, where the pre-trained model itself generates data for fine-tuning the quantized model (student) using logit distillation from the full-precision model (teacher). This method incorporates quantization of weights, activations, and key-value cache, achieving accurate 4-bit quantization for weights and key-value caches, and 6-bit for activations, demonstrating substantial improvements over existing post-training quantization methods. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "Insights: Insights drawn from quantization strategies include: ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "• Post-Training Quantization techniques primarily focus on quantizing model weights, where selecting salient weights is crucial. Beyond weight quantization, handling outliers in activation signals is a significant challenge in quantizing activations.   \n• Quantization-Aware Training methods show that low-bit quantization (e.g., 1-bit models) requires additional tuning to maintain performance. Knowledge can be distilled from the model before quantization to the quantized model. ",
        "page_idx": 19
    },
    {
        "type": "text",
        "text": "3.6 Techniques in LLMs Contributing to SLMs ",
        "text_level": 1,
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "This subsection explores the potential of advanced techniques such as RAG and MoE, which enhance LLM performance, to also maintain or boost SLM performance within constrained computational budgets. However, effectively integrating these techniques into SLMs, which inherently possess limited capabilities, remains an unresolved challenge. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Retrieval Augmented Generation (RAG) enhances the capabilities of language models in knowledge-intensive tasks by incorporating a retrieval mechanism. This approach allows models to access relevant contextual information from a data repository in response to user queries. By integrating this retrieved data, RAG-equipped models better understand specific topics, enabling more informed and accurate outputs. For SLMs, a significant concern is whether they possess the capacity for long-context reasoning. A recent study [215] compares SLMs at the 7B level with RAG to larger models such as GPT-3.5 and GPT-4, suggesting that SLMs equipped with RAG can sometimes perform comparably or even better than LLMs. These findings indicate that RAG for SLMs is effective and represents a promising direction for future research. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "Mixture-of-Experts (MoE) [39] has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead in LLMs. The MoE framework is founded on a straightforward yet potent concept: distinct components of a model, referred to as “experts”, specialize in different tasks or data facets. In this paradigm, only the relevant experts are activated for a specific input, which manages computational costs while leveraging a vast pool of specialized knowledge. This scalable and adaptable approach enables increased model capacity without proportionally escalating computational demands. We argue that MoE is particularly suitable for SLM architectures [161] as it minimizes both computational load and memory overhead. However, research on MoE for SLMs remains sparse. Future studies could investigate how large LLM MoE architectures can be effectively compressed into small ones or how to develop an SLM with MoE tailored for specific devices from scratch. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "4 APPLICATIONS OF SMALL LANGUAGE MODELS ",
        "text_level": 1,
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "In this section, we delve into the applications of small language models (SLMs) across various NLP tasks and their deployment strategies. Due to benefits such as enhanced privacy, faster inference, and lower memory requirements, many NLP applications are now leveraging SLMs over LLMs. Additionally, deploying SLMs often involves considerations of memory and runtime efficiency, which are crucial for optimizing resource use on budget-constrained edge devices, particularly mobile phones. Then, we will discuss task-specific applications of SLMs and their deployment methods on mobile and edge devices. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "4.1 Task-specific SLM Applications ",
        "text_level": 1,
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "This subsection explores the diverse NLP tasks to which SLMs can contribute. Question-answering and coding represent generative tasks, while recommender systems and web search (though not strictly within the NLP domain) typically leverage the encoding capabilities of SLMs. Additionally, the application of SLMs on mobile devices is particularly well-suited due to constraints in memory and computing resources. The representative works are systematically organized in Table 3. ",
        "page_idx": 20
    },
    {
        "type": "text",
        "text": "4.1.1 SLM Applications in Question-Answering. Question-answering (QA) is a fundamental task in the NLP field, demanding language models to exhibit abilities in understanding language, reasoning, common sense, and recalling specialized knowledge. Typically, larger language models yield better QA performance. However, the substantial size of these models introduces challenges such as immense computational requirements, privacy concerns when using proprietary LLMs, and difficulties in customization. These issues lead researchers and developers to favor SLMs in scenarios that demand efficiency, privacy, and customization. Therefore, we explore methods to enhance the capabilities of SLMs in QA across three key areas: (i) Instruction Tuning of Generic SLMs for QA, (ii) Instruction Tuning of Domain-Specific SLMs for QA, and (iii) Enhancing SLMs for Out-of-Domain Questions. ",
        "page_idx": 20
    },
    {
        "type": "table",
        "img_path": "images/93144e9125c92890e1eada7cf6aca7e81e7846ea281be533d7a92908374ae4f4.jpg",
        "table_caption": [
            "Table 3. Task-specific SLM Applications "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Aspect</td><td>Representativework</td><td>Key point</td></tr><tr><td rowspan=\"2\"> SLM in QA</td><td>Alpaca [329] Stable Beluga 7B [238]</td><td>TuneLlama 7B[338] using 52k ChatGPT-generated examples.</td></tr><tr><td>Fine-tuned BioGPT[125] Financial SLMs [275] ColBERT[112] Rationale Ranking [128] T-SAS [156]</td><td>Employ explanation tuning to Llama-2 7B [339] on an Orca-style dataset. Fine-tuning BioGPT(1.6B)[230] on PubMedQA. Transfer financial knowledge from GPT-4 [2] to multiple SLMs. Fetch retrieval documents for SLMs to answer domain-specific questions. For unseen questions,combine retrieval with LLM-generated rationales.</td></tr><tr><td>SLM in Coding</td><td>Phi-3.5-mini [1] TinyLlama [439] CodeLlama [292] CodeGemma [331]</td><td>Enhance SLMs adaptability with self-generated pseudo labels. New addition to the Phi-3 series and focus on high-quality data. 1.1B Transformer model is trained on 3T corpus. A derivative of Llama 2 fine-tuned on domain-specific datasets. Fine-tuning Gemma to enhancing coding capabilities.</td></tr><tr><td>SLM in Recommen- dation</td><td>PromptRec [382] SLIM [369] BiLLP [309] ONCE [214] RecLoRA [455] Content encoder [44,152, 228]</td><td>Training on prompt templates Step-by-step Knowledge Distillation LLaMa-2-7B as planner and reflector LLaMa-2-7B as Content Encoder Personalized low-rank adaptation</td></tr><tr><td>SLM in Web Search</td><td>Ranker [63,260] Rewriter [233] Octopus [52]</td><td>Encode concatenated queries and documents. Re-rank retrieved documents using a specially SLM. Bridge the gap between queries and needed knowledge by rewriting inputs. Calling software APIs via learning in documents</td></tr><tr><td>SLM in Mobile- device</td><td>MobileAgent [87] α-UMI[307] Mobile Interaction [42] AutoDroid [376] M4 [423] Agent for Text Rewriting [458]</td><td>Standard Operating Procedure (SOP) SLMs serve as Multi-agents in tool uses. Text-to-action control and tests on 6GB and 4GB Android devices Interaction based on GUI and APP knowledge injection a foundation model handling all mobile AI tasks. Data Knowledge Distillation from LLMs</td></tr></table>",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 21
    },
    {
        "type": "text",
        "text": "Instruction Tuning Generic SLMs for QA. Despite the Phi series’ high question-answering capability, its training cost with over 3.4T tokens on 512 H100 GPUs for 10 days [1] is prohibitive for many researchers and developers. Instruction tuning [372] offers a cost-effective alternative, enhancing small models by fine-tuning on large model outputs. Alpaca 7B [329] tunes Llama 7B [338] with 52k ChatGPT-generated examples from 175 seed tasks. This behavior cloning mimics teacher models effectively but struggles in reasoning-intensive QA tasks where accuracy is key, not style [60]. To counter it, explanation tuning [238] enhances Llama-2 7B [339] using explanatory LLM answers to improve reasoning. However, its effectiveness varies with system instructions, and those effective for larger models like GPT-4 may not suit smaller ones. SLMs also struggle to identify optimal system instructions for different tasks. Therefore, Orca 2 [247] addresses this by promoting cautious reasoning, deciding which solution strategy to choose for a given task among direct answer generation, or “Slow Thinking” strategies (step-by-step, guess and check or explain-then-answer, etc.) and erasing specific system instructions during training. This involves (1) solution strategy is guided by the performance of Orca 1 [251], (2) writing task-specific system instructions corresponding to the chosen strategy to obtain teacher responses for each task, and (3) at training time, employing Prompt Erasing to replace student’s system instructions with generic ones vacated of details of how to approach the task, encouraging students learn not just task solutions but also deeper reasoning abilities. ",
        "page_idx": 21
    },
    {
        "type": "table",
        "img_path": "images/8baa83545500b3c7be9594e42743fe1ff1961b3ea45d6d6638141e7d356b2eec.jpg",
        "table_caption": [
            "Table 4. Comparison of instruction-tuned domain SLMs for QA and LLMs on FinQA [56] and PubMedQA [163]. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Size</td><td>Instruction tuned?</td><td>Task Name</td><td>Shot Type</td><td>Accuracy (%)</td></tr><tr><td>GPT-4 [2]</td><td>1</td><td>×</td><td>FinQA</td><td>Zero-shot</td><td>77.5</td></tr><tr><td>Phi-3-Mini [1]</td><td>2.7B</td><td>√</td><td>FinQA</td><td>Zero-shot</td><td>77.6</td></tr><tr><td>Meditron-70B[55]</td><td>70B</td><td>×</td><td>PubMedQA</td><td>Zero-shot</td><td>81.6</td></tr><tr><td>RankRAG-llama3-70B [421]</td><td>70B</td><td>×</td><td>PubMedQA</td><td>Zero-shot</td><td>79.8</td></tr><tr><td>Flan-PaLM [313]</td><td>540B</td><td>×</td><td>PubMedQA</td><td>Few-shot</td><td>79.0</td></tr><tr><td>GAL 120B [330]</td><td>120B</td><td>×</td><td>PubMedQA</td><td>Zero-shot</td><td>77.6</td></tr><tr><td>Flan-PaLM[313]</td><td>62B</td><td>×</td><td>PubMedQA</td><td>Few-shot</td><td>77.2</td></tr><tr><td>BioGPT[230]</td><td>345M</td><td>√</td><td>PubMedQA</td><td>Zero-shot</td><td>78.2</td></tr><tr><td>BioGPT-Large [230]</td><td>1.5B</td><td>√</td><td>PubMedQA</td><td>Zero-shot</td><td>81.0</td></tr></table>",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Instruction Tuning Domain SLMs for QA. Beyond instruction tuning for generic SLMs, tuning domain-specific SLMs is also crucial, as they provide specialized assistance where generic SLMs may underperform. Instruction-tuning generic SLMs can derive domain SLMs. We summarize some representatives in several domains. (1) In finance, Phogat et al. [275] transfer financial QA abilities from teacher LLMs such as GPT-4 [2] to specialized SLMs such as Phi-3-Mini [1], using datasets such as FinQA [56], ConvFinQA [57], and TATQA [454]. They train SLMs with Python programs created by the teacher model, which detail steps for financial reasoning, including concept comprehension, formula identification, entity extraction, and calculations. During inference, SLMs generate Python code that an external interpreter executes. (2) In the medical field, Guo et al. [125] enhance student SLMs, including domain-specific BioGPT (1.6B) [230] and general Llama 7B [338], by fine-tuning on enriched PubMedQA [163] data. This enhancement is achieved by generating new samples or rewriting existing ones using teacher LLMs, which include the highly knowledgeable GPT-4 and the relatively weaker ChatGPT. The best SLM, with under 1.6 billion parameters, achieves $7 5 . 4 \\%$ accuracy, surpassing GPT-4’s $7 4 . 4 \\%$ in few-shot settings on the PubmedQA test sets. It demonstrates that LLMs effectively refine and diversify question-answer pairs, leading to enhanced performance in a significantly smaller model after fine-tuning. We report the detailed results of comparisons of instruction-tuned domain-specific language models for QA and larger language models on FinQA [56] and PubMedQA [163], as shown in Table 4. ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "Enhancing SLMs for Out-of-Domain Questions. One of the major advantages of LLMs is their strong comprehension and logical reasoning abilities, which SLMs often struggle to match due to their limited parameters, especially when handling unseen or out-of-domain questions. Various methods have been developed to address this limitation, including Retrieval-Augmented Generation (RAG) and self-adaptive techniques. ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "(1) Retrieval-Augmented Generation (RAG): Incorporating External Knowledge for Domain-Specific QA. RAG addresses OOD questions by integrating external knowledge during inference, allowing models to access information beyond their pre-trained parameters. By retrieving relevant documents in real time, RAG enables small language models to provide accurate answers on specialized topics. In the telecommunications domain, Gichamba et al. [112] use ColBERT as a dense retrieval system to fetch documents from technical datasets. By encoding queries and documents separately, ColBERT computes relevance scores, helping small models like Phi-2 and Falcon-7B retrieve precise technical information to answer complex telecom-related queries. Rationale Ranking [128] addresses answering unseen questions using smaller language models by integrating external explanatory contexts from retrieval systems with reasoning rationales from LLMs. This method involves ranking both the retrieved explanatory contexts and LLM-generated rationales using a scoring module, which then combines them to form a cohesive context. Consequently, this integrated approach enhances the SLMs’ performance on unseen questions. ",
        "page_idx": 22
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "(2) Self-Adaptive Techniques: Enhancing Model Adaptability with Self-Generated Pseudo Labels. Fine-tuning, while effective in adapting domain knowledge, can be impractical in realistic scenarios where labeled datasets are scarce. To overcome this, self-adaptive techniques employ self-generated pseudo labels to activate specific aspects of the target tasks, thereby enhancing model adaptability [312, 347]. Test-time Self-Adaptive Small LMs (T-SAS) [156] first stochastically generates multiple answers for an unlabeled question. The most plausible answer is then selected via majority voting to enhance pseudo-label accuracy, serving as a pseudo-label for training during test-time. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Comparison between LLMs and SLMs for QA. When comparing LLMs such as GPT-4 [2] or BLOOM-175B [181] with fine-tuned SLMs in QA tasks, the benefits of SLMs are clear. LLMs, while versatile across multiple domains due to extensive pre-training, are computationally demanding, making them less ideal for resource-limited settings. SLMs, however, when fine-tuned for specific domains, often match or exceed the performance of larger models within those specialties. The trade-off is between large-scale models’ generalization and small-scale model’s specialization: LLMs handle diverse domains but may need additional techniques such as knowledge injection for domain-specific queries. In contrast, domain-specific SLMs, though less flexible, provide higher accuracy and more relevant responses, making them ideal for edge deployments where computational resources are scarce but domain precision is crucial. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "4.1.2 SLM Applications in Coding. The adoption of SLMs for coding offers an alternative to LLMs due to their lower computational needs and potential for domain-specific tuning. Despite LLMs’ proficiency in code generation and programming support, SLMs are advantageous for their faster inference, reduced operational costs, and suitability for real-time environments where rapid responses are crucial. Representative works are discussed next. The Phi series [1, 155, 200] showcase SLMs’ evolution in coding tasks. For instance, Phi-1 [120], a Transformer with 1.3B parameters, specializes in basic Python coding and achieves notable scores in benchmarks such as HumanEval [120], which includes 164 programming problems. Subsequent models, Phi-1.5 and Phi-2, have enhanced these capabilities, while Phi-3 demonstrated SLMs’ potential to rival larger models [1]. The latest model, Phi-3.5-mini, with 3.8B parameters, excels in long context tasks using advanced fine-tuning and optimization techniques, performing comparably to larger models such as Llama-3.1-8B-instruct [94] and surpassing smaller ones like Gemma-2 [333]. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Another avenue of development is the fine-tuning of general-purpose SLMs for coding tasks [23, 121, 227, 292, 331]. For instance, CodeLlama models [292], derivatives of Llama 2 [339], undergo a rigorous fine-tuning process on domain-specific datasets, enhancing their proficiency in specific programming languages such as Python. They are trained to handle tasks such as syntax error detection, code suggestion, and infilling, where they learn to predict and complete missing parts of the code. This specialized fine-tuning improves their ability to interpret and execute detailed programming instructions, making them highly effective in real-time code editing environments [292]. CodeGemma models [331], stemming from Google DeepMind’s Gemma framework, also exhibit a focused approach to enhancing coding capabilities through fine-tuning. These models are specifically engineered for high-performance code generation and infilling, underpinned by extensive training on a vast corpus of over 500 billion to 1 trillion tokens, predominantly consisting of code. This comprehensive dataset enables CodeGemma models to excel in mathematical reasoning and complex problem-solving within code contexts, setting new benchmarks in latency-sensitive applications such as real-time IDE support and automated code reviews [331]. ",
        "page_idx": 23
    },
    {
        "type": "text",
        "text": "Comparison between SLMs and LLMs on Coding. Table 5 provides a comparative analysis of SLMs and LLMs on coding benchmarks HumanEval [50] and MBPP [20]. Insights include: (i) Small SLMs (1.3B - 3.8B Parameters) like Phi-3.5-mini [342] achieve high scores, demonstrating the efficacy of small models. Mid-sized SLMs (6.7B - 9B Parameters), such as DeepSeekCoder 6.7B [121] and Llama 3.1 8B [94], show improved performance, indicating that larger model sizes and enhanced training contribute to better accuracy. Large models (33B and above) like Llama 3.1 405B [94], GPT-4o [263], and Claude 3.5 Sonnet [15] excel, supporting the idea that bigger models generalize better across diverse coding tasks; (ii) There’s a notable trade-off between computational efficiency and performance, with larger models requiring more resources, impacting their practical deployment in constrained environments; (iii) Specialized training and fine-tuning, as used in models like DeepSeek-Coder [121], are crucial for excelling in coding tasks, though such models may not handle complex requests as effectively, highlighting the versatility of general SLMs for broader applications. ",
        "page_idx": 24
    },
    {
        "type": "table",
        "img_path": "images/d571433f38a77ba849a7f2215c689dd115b7b307778fb86df6c4b41005282e82.jpg",
        "table_caption": [
            "Table 5. Performance comparison between SLMs and LLMs in coding benchmarks. All models listed are chat or instruct versions, and performance are sourced from respective research papers or technical reports [94, 121, 292, 331, 342]. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>Size</td><td>HumanEval</td><td>MBPP</td></tr><tr><td>DeepSeek-Coder [121]</td><td>1.3B</td><td>65.2</td><td>49.4</td></tr><tr><td>CodeGemma [331]</td><td>2B</td><td>37.8</td><td>49.2</td></tr><tr><td>Gemma 2 [333]</td><td>2B</td><td>17.7</td><td>40.2</td></tr><tr><td>Phi-3.5-mini [342]</td><td>3.8B</td><td>62.8</td><td>69.6</td></tr><tr><td>DeepSeek-Coder [121]</td><td>6.7B</td><td>78.6</td><td>65.4</td></tr><tr><td>CodeGemma [331]</td><td>7B</td><td>60.4</td><td>55.2</td></tr><tr><td>Llama 3.1 [94]</td><td>8B</td><td>66.5</td><td>69.4</td></tr><tr><td>Gemma 2 [333]</td><td>9B</td><td>61.0</td><td>69.3</td></tr><tr><td>GPT-3.5 Turbo</td><td>-</td><td>68.0</td><td>71.2</td></tr><tr><td>DeepSeek-Coder [121]</td><td>33B</td><td>79.3</td><td>70.0</td></tr><tr><td>Llama 3.1 [94]</td><td>70B</td><td>80.5</td><td>75.4</td></tr><tr><td>Llama 3.1 [94]</td><td>405B</td><td>89.0</td><td>78.8</td></tr><tr><td>GPT-4o OpenAI [263]</td><td></td><td>90.2</td><td>81.4</td></tr><tr><td>Claude 3.5 Sonnet [15]</td><td>-</td><td>92.0</td><td>76.6</td></tr></table>",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "4.1.3 SLM Applications in Recommender Systems. Recommender systems are essential in various online services, helping to manage information overload and meet users’ personal needs. SLMs enhance recommendation systems by (1) addressing the cold start problem; (2) reducing popularity bias; (3) improving long-term planning; (4) serving as personalized recommenders; and (5) acting as content encoders. These applications show the versatility and effectiveness of SLMs in boosting performance and personalization in recommendation. Next, we introduce the details. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "SLM for System Cold Start Problem. Traditional recommendation systems, which utilize historical user-item interactions such as clicks, purchases, and ratings to learn representations and match items to users, fail in scenarios lacking any user-item interactions, known as the cold-start recommendation problem, often occurring in start-up businesses [289]. Although LLMs address this with in-context learning, their slow and costly inference restricts real-time use. Thus, PromptRec [382] explores using SLMs as in-context recommenders for recommendation system cold-start problems. However, SLMs often struggle without emergent context-learning abilities. To overcome this, SLMs are enhanced by pre-training on relevant corpora, using a improved C4 corpus subset [284], and by developing training prompts for different domains, enhancing cold-start performance. Results show that enhanced SLMs like BERT-mini [83], with 11.3M parameters, achieve BERT-large’s performance in cold-start scenarios, with only $1 7 \\%$ of BERT-large’s inference time. Similarly, many studies have addressed the cold-start problem by leveraging BERT [133, 261, 441, 459]. For example, ADLRS [133] employs BERT to convert web-crawled item profiles into vectors that highlight key aspects, aiding recommender systems in acquiring essential initial information. ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "SLM for Mitigating Popularity Bias. Popularity bias in recommender systems, marked by discrepancies between item popularity in training datasets and the real world, often stems from using closed-loop datasets with limited information. Recent LLMs leverage their broad open-world knowledge to better reason about user-item interactions [206, 214], reducing this bias by providing recommenders with more extensive item details. Using the chain-of-thought (CoT) prompting, LLMs decompose complex tasks into intermediate reasoning steps, enhancing understanding of user behavior and interests. However, LLMs’ high resource demands limit their practical use. To overcome this, the ",
        "page_idx": 24
    },
    {
        "type": "text",
        "text": "Step-by-step Knowledge Distillation Framework for Recommendation (SLIM) [369] distills LLM reasoning capabilities into SLMs, keeping just $4 \\%$ of the original parameters, transitioning from ChatGPT to Llama 7B [338]. SLIM uses detailed LLM templates to extract reasoning steps and streamlined templates for fine-tuning, enabling SLMs to improve recommender systems by better reasoning on richer item information. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "SLM for Long-term Planning. Traditional recommender systems focus on optimizing immediate user responses, often maximizing short-term gains but overlooking long-term engagement. This can trap users in echo chambers and filter bubbles [107, 368]. To tackle this, integrating planning capabilities into recommendations to balance immediate and long-term outcomes is vital. LMs, with their extensive knowledge and reasoning abilities, are expected to enhance planning capabilities. BiLLP [309] adopts a hierarchical learning approach with macro and micro-learning phases. In macro-learning, a Planner and a Reflector, both as SLM instances like Llama-2-7B [339], operate; the Planner forms long-term plans using high-level experiences, while the Reflector updates plans based on past actions. Micro-learning uses an SLM-based Actor-Critic mechanism for personalized planning, with the Actor implementing plans and the Critic assessing actions for long-term benefits. The use of SLMs for long-term planning, similar to their use in cold-start scenarios, remains underexplored and merits further research. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "SLMs as a Personalized Recommender. Generative language model-based recommender systems require integrating user knowledge, typically achieved through fine-tuning. Fine-tuning techniques like LoRA [142] can incorporate extensive knowledge across all users by training an external module with a small number of parameters A and B, but this approach often overlooks individual user preferences. To address this, RecLoRA [455] utilizes Vicuna-7B [61] to integrate personalized knowledge into SLMs/LLMs tailored for recommendation tasks, as illustrated in Figure 14. Specifically, RecLoRA maintains a set of parallel, independent LoRA weights $( \\mathbf { A } _ { i } , \\mathbf { B } _ { i } )$ , allowing for the customization of language model parameters to match individual user preferences more effectively. ",
        "page_idx": 25
    },
    {
        "type": "image",
        "img_path": "images/3b43c2adad6b79f991c200f73540556e2caeebf4d3fa4388bf6043d9b05842b2.jpg",
        "image_caption": [
            "Fig. 14. The illustration of lifelong behavior sequence and personalized low-rank adaption (LoRA) for recommendation [455]. "
        ],
        "image_footnote": [],
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "SLM as a Content Encoder. Language models, particularly when deep, provide an effective starting point for finetuning on downstream tasks. In news recommendation systems, the representational capability of a model significantly impacts performance. Consequently, many news recommender systems now employ language models fine-tuned on specific datasets as text encoders. For example, Wu et al. [379] conducts pioneering work using a pre-trained language model to enhance large-scale news recommender systems by substituting traditional news encoders with a BERT model [83]. However, BERT may struggle to capture content as it is pre-trained on limited data. Therefore, ONCE [214] propose using Llama-2-7B [339] as an encoder to overcome the limitations of BERT in content-based recommendations. Additionally, the study explores the synergistic use of LLMs in recommendation systems, finding that SLMs optimized with LoRA [142] outperform the recommendation results of systems assisted by generic LLMs such as ChatGPT. ",
        "page_idx": 25
    },
    {
        "type": "text",
        "text": "4.1.4 SLM Applications in Web Search. Web search systems, involving retrieval and ranking, face challenges due to the diverse web documents and search queries. Traditional keyword-matching methods often fall short because of phrasing variations and the long-tail distribution of queries and content, complicating accurate semantic inference. Effective integration of retrieval and ranking models is also crucial. Language models, serving as content encoders, help overcome semantic challenges through their language understanding from pre-training [63, 89, 359]. Joint training of Manuscript submitted to ACM ",
        "page_idx": 25
    },
    {
        "type": "image",
        "img_path": "images/55caed2c4f7f78a5d857a38a853bc5c203ddb2ed2f0ce8cc74b86cc6c50f991c.jpg",
        "image_caption": [
            "Fig. 15. Roles of SLM in Web Search. "
        ],
        "image_footnote": [],
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "retrieval and ranking models addresses integration, with SLMs ranking retrieved documents and acting as re-rankers. Additionally, SLMs serve as rewriters in scenarios requiring enhanced query understanding. Thus, in web search, SLMs fulfill three roles: (1) content encoder, (2) ranker, and (3) rewriter, as depicted in Figure 15. Next, we give details. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "SLM as a Content Encoder. Text embeddings are vector representations that encode semantic information, widely used in retrieval; SLM-based dense retrieval utilizes pre-trained deep language understanding to effectively tackle semantic challenges. H-ERNIE [63] employs a hierarchical model that encodes queries and documents at multiple granularity—character, word, and phrase—to improve specificity and relevance in web search results by aggregating finer details into coarser layers, addressing issues like ambiguous queries. Implicit Interaction $( I ^ { 3 } )$ [89] uses BERT [83] as a content encoder, generating implicit pseudo-queries from passages to enable high online efficiency with offline caching of passage vectors. However, ERNIE and BERT-style models overlook advancements in SLMs such as context length extension [292]. Thus, Peng et al. [272] employs LLaMa-7B [338] and Vicuna-7B [61] as semantic encoders for embedding retrieval, demonstrating improved performance through soft prompt tuning. CoCondenser [109] addresses sensitivity to noisy data and large batch requirements during dense retriever training. Using the Condenser architecture with Transformer blocks, the model condenses information into dense vectors effectively. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "SLM as a Ranker. The reranking task improves the order of multiple candidates to enhance retrieval quality because rerankers are more accurate than embedding retrievers. InPars (Inquisitive Parrots for Search) [36] employs the T5 base 220M [284] as a re-ranker to enhance the BM25 retriever [291]. Initially, BM25 selects 1K candidates, re-ranked by a fine-tuned T5 model (monoT5) adapted as a binary classifier to assess document-query relevance. Training data, generated by GPT-3 [37], formulates queries and selects random negative examples. Experiments show the monoT5- enhanced retriever significantly outperforms GPT-3; for example, it achieves a 0.3599 MAP score on the TREC-DL 2020 dataset [69], surpassing GPT-3’s 0.3163. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "SLM as a Rewriter. Queries to the retriever, typically just a few keywords, may reveal a knowledge gap between the query and the knowledge needed for effective retrieval, thus limiting performance. To address this, the “rewriteretrieve-read” framework [233] uses T5-large [284] to bridge the knowledge gap in queries by rewriting them for more effective retrieval. This rewriter, trained via reinforcement learning with downstream LLM performance as a reward, outperforms general LLM rewrites. For example, it achieves a 45.97 F1 score on HotpotQA, surpassing the generic LLM’s 43.85 F1 score. ",
        "page_idx": 26
    },
    {
        "type": "text",
        "text": "4.1.5 SLM Applications in Mobile-device. The use of cloud-based LLMs on devices raises privacy concerns and their large size limits real-time responses in urgent scenarios such as medical emergencies. To overcome these issues, researchers are creating smaller, domain-specific models (SLMs) that offer accurate results and suit mobile use. This subsection discusses SLM applications on mobile devices, focusing on three aspects: (1) software API calls, (2) mobile control, and (3) basic NLP applications. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "SLM for Tool Learning. Integrating LLMs with APIs enhances capabilities but incurs high training costs, prompting a shift to smaller, task-specific models that cut costs but risk errors. In response, Octopus [52] uses a diverse dataset from over 30K APIs and curriculum learning [217] to improve API function accuracy. This method boosts API performance in models like Codellama-7b [292] and Google’s Gemma series [332]. PhoneLM-1.5B-Call [417] is fine-tuned on DroidCall [389] datasets and achieves comparable performance compared to GPT-4o-mini [262]. $\\alpha$ -UMI [307] employs SLMs as planners, callers, and summarizers within multi-agent systems, outperforming a single LLM in tool uses. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "SLM for Mobile Control. LM agents facilitate user-device interactions through taps, gestures, and text, automating tasks and enhancing user handsfree convenience. Unlike traditional developerbased approaches that require extensive developer effort to design interfaces and translate commands into API calls, LMs offer scalable automation via GUI-based text contents. MobileAgent [87] integrates instructions and Standard Operating Procedures (SOP) to improve SLMs for mobile control. As shown in Figure 16, it processes goals (e.g., booking a dental appointment) by analyzing screens, queries, prior actions, and UI elements, forming ",
        "page_idx": 27
    },
    {
        "type": "image",
        "img_path": "images/844ac2068793ef36b83eb6f0bf3c6390326b42f040099f022d3cd83076a77653.jpg",
        "image_caption": [
            "Fig. 16. An example workflow for an automated execution tool [87]. The screenshot in the left is taken from [87]. "
        ],
        "image_footnote": [],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "prompts to generate outputs and execute actions (e.g., selecting text, XPath). Fine-tuning Qwen-7B [23] on AIA medical data, it outperforms GPT-4 [2] on AitW [290], a key mobile benchmark, without extra inference costs. Carreira et al. [42] run a small offline model on mobile devices, fine-tuned with ChatGPT-3.5 data, enabling tasks like calls and web searches. RedPajama-INCITE-Chat-3B-v1 Computer [67], selected for its size and chat features, uses native code and quantization, performing well on 6GB and 4GB Android devices. ",
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "AutoDroid [376] improves Android app interactions via GUI automation. Figure 17 shows LLMpowered task automation (e.g., setting a laundry reminder for Aug 17) in four steps: (1) click ’New Event’, (2) enter ’laundry’ in ’Title’, (3) click ’Save’, (4) finish. Using Vicuna-7B and app-specific knowledge, AutoDroid generates privacy-filtered prompts for tasks. On its DroidTask benchmark, it outperforms GPT-3.5 $( 3 4 . 7 \\% )$ and GPT-4 $( 5 4 . 5 \\% )$ with $5 7 . 7 \\%$ accuracy. M4 (composable mobile foundation model) [423] introduces a 9.2B parameter model ",
        "page_idx": 27
    },
    {
        "type": "image",
        "img_path": "images/f0676f031f8be6ddbcdd421342e02c7aa2dd3e785aaeb77f576fd2a9803bf571.jpg",
        "image_caption": [
            "Fig. 17. An illustration of Vicuna-7B-powered mobile task automation [42] shows a user asking to be reminded about doing laundry on Aug 17. The figure is taken from [42]. "
        ],
        "image_footnote": [],
        "page_idx": 27
    },
    {
        "type": "text",
        "text": "(7.5GB memory) for mobile AI tasks, managed by the OS and hardware. Currently limited to high-end devices, its applicability will expand with increasing mobile memory/storage. These works highlight customizing smaller, domain-specific SLMs to address memory limits while preserving functionality in mobile environments. ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "SLM for Basic NLP Applications on Devices Performing basic NLP tasks such as text rewriting directly on the device can enable personalization while ensuring privacy. The sparse annotation on devices is a challenge. Qin et al. [278] utilizes self-supervised data selection and synthesis for on-device fine-tuning, leveraging sparse annotations and limited storage effectively. This approach, demonstrated in Figure 18, employs the Llama-3B model [338] and the LoRA fine-tuning method [142], enhancing personalization by efficiently managing data through metrics including embedding entropy and domain-specific scores. In mobile text rewriting, Zhu et al. [458] train the compact Palm 2-XXS model [14] using data generated by the larger Palm 2-L to ensure user privacy and accommodate device constraints. On its new benchmark, MESSAGEREWRITEEVAL, Palm 2-XXS achieves a BLEU score of 34.59, outperforming LLaMa-7B (16.65) [338]. tokens/s), proving its mobile efficiency for text rewriting. ",
        "page_idx": 28
    },
    {
        "type": "image",
        "img_path": "images/d4ef979fb35a43bc43f105144a9a822511b5247f2074c5314c41005aa974897d.jpg",
        "image_caption": [
            "Fig. 18. Overview of fine-tuning SLMs with synthesized and user data [278]. Data Synthesis generates semantically similar text via prompts, creating dialogue sets. Data Selection processes user data, tags domains, and calculates metrics (EOE, DSS, IDD). Selected data fine-tunes SLMs with user annotations. The iterative framework refines SLMs through continuous data generation and selection. "
        ],
        "image_footnote": [],
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Tests on the Samsung S23 show lower latency (29 tokens/s) than a 4-bit LLaMa-7B on a MacBook M1 Pro (18-22 ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "Insights: We draw several key insights from the development of task-specific SLMs: ",
        "page_idx": 28
    },
    {
        "type": "text",
        "text": "• There is considerable potential in enhancing the efficiency and effectiveness of small models by integrating self-adaptive techniques with further fine-tuning and optimization of RAG-based methods.   \n• The growing relevance of SLMs in coding highlights their cost-effectiveness and efficiency as alternatives to LLMs, providing quick processing and easy fine-tuning for specialized tasks; while LLMs handle complex tasks well, SLMs, optimized and fine-tuned on specific data, are increasingly essential in resource-limited settings.   \n• SLMs significantly enhance recommendation systems due to their robust generalization, reasoning abilities, and in-context learning, addressing key challenges such as cold-start problems and distribution biases. They support long-term planning, replace traditional encoders, and use parallel low-rank parameters to inject personalized user knowledge effectively.   \n• SLMs play a crucial role in web search such as document encoding, text reordering, and query rewriting, often outperforming LLMs through techniques such as supervised fine-tuning, soft prompt tuning, unsupervised contrastive loss, and reinforcement learning, thereby enhancing adaptability and efficiency.   \n• SLMs are utilized on mobile devices primarily for privacy and memory constraints, with applications in API calls and mobile control; they are typically developed by generating data with LLMs and fine-tuning with SLMs, or by using local SLMs to handle privacy with LLMs boosting performance, and their training involves innovative techniques like learning from data streams and managing non-IID time series data. ",
        "page_idx": 28
    },
    {
        "type": "table",
        "img_path": "images/f9283173cfd5cc41848dbdac8e8581006a4d40a4141bae56cd4912f55bdac1a1.jpg",
        "table_caption": [
            "Table 6. On-device Deployment Optimization Techniques "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Aspect</td><td>Representative Work</td><td>KeyPoint</td></tr><tr><td rowspan=\"10\"></td><td>EDGE-LLM[422]</td><td>Edge LLMs use LUC and adaptive tuning for efficiency</td></tr><tr><td>LLM-PQ [448]</td><td>Optimize quantization and layer partitioning for complex setups.</td></tr><tr><td>AWQ[207]</td><td>Preservekey weights based onactivation distribution.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td>DMC[256]</td><td>Adaptively compress KV cache, optimizing storage efficiency.</td></tr><tr><td>Transformer-Lite [193]</td><td>Optimize KV cache to reduce redundancy and memory use.</td></tr><tr><td>LLMaaS [419]</td><td>LLMaaS manages apps via chunk-wise KV cache optimization on mobiles.</td></tr><tr><td rowspan=\"5\">Runtime Efficiency Optimization</td><td>mllm-NPU [395]</td><td>On-device NPU (neural processing units) to reduce prefill latency.</td></tr><tr><td>COST-EFF [305]</td><td>Distill amulti-exit model from the original PLM.</td></tr><tr><td>LLMCad [394]</td><td>Use SLM for fast token generation and cloud verification.</td></tr><tr><td>EdgeMoE 416]</td><td>Predict expert needs, boosting inference speed and reducing latency.</td></tr><tr><td>LinguaLinked [447]</td><td>Optimize data flow and load,enhancing multi-threading efficiency.</td></tr></table>",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "4.2 SLM Deployment on Mobile and Edge Devices ",
        "text_level": 1,
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "On-device applications benefit uniquely from the memory-saving efficiency and rapid runtime performance of SLMs, which offer advantages over LLMs. However, devices with extremely limited resources may still struggle with the parameter sizes of SLMs. To ensure both memory and runtime remain within acceptable range while still maintaining performance, it is crucial to integrate technologies that facilitate the deployment of SLMs on resource-constrained devices. The primary challenge for memory-efficient technologies arises from the inherent size of the SLMs and their associated caches. To address this, we survey existing works focused on compressing SLMs and their caches. Additionally, the large size of models significantly impacts runtime efficiency due to the extensive computing workload and potential weight transfers between the memory buffer and RAM/GPU memory. Other challenges include switching the Mixture of Experts between CPU and GPU memory and managing resource scheduling when deploying SLMs across multiple local devices. Therefore, in this subsection, we review representative works that address these challenges under two aspects: memory efficiency optimization and runtime efficiency optimization, as systematically compiled in Table 6. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "4.2.1 Memory Efficiency Optimization. Memory efficiency involves minimizing the memory usage of both the model and the KV cache during deployment. This is typically achieved through model compression techniques such as quantization [207, 285, 422, 448], the cache of MoE experts [416], and KV cache compression [165]. ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Compression on model parameters. Quantization, a common method for deploying SLMs, lowers numerical precision to significantly save memory while preserving accuracy. We detail quantization strategies in Sections 2.3.3 and 3.5, focusing here on representative works for edge devices. EDGE-LLM [422] adapts LLMs for edge devices using a Layer-wise Unified Compression (LUC) method that combines layer-specific pruning and quantization to reduce computational demands and an Adaptive Layer Tuning and Voting scheme to optimize memory use while ensuring performance. Meanwhile, LLM-PQ [448] addresses quantization and model layer partitioning for heterogeneous clusters, incorporating a Cost Model and an Indicator Generator to optimize bit-width assignment and layer partitioning through integer linear programming, enhancing quantization for complex computational setups. Activation-aware Weight Quantization (AWQ) [207] is a hardware-friendly, low-bit, weight-only quantization method for on-device LLMs, preserving essential weights based on activation distribution to minimize quantization loss. MoE- $\\cdot \\mathbf { I } ^ { 2 }$ [404] prune less important experts and applies low-rank decomposition to the remaining experts to further optimize efficiency. Manuscript submitted to ACM ",
        "page_idx": 29
    },
    {
        "type": "text",
        "text": "Cache of MoE Experts. Beyond standard quantization, which reduces storage for all model parameters, another strategy involves caching a mixture of experts (MoE). Driven by the fact that memory storage is more cost-effective and scalable than computing capacity, the MoE architecture [154] boosts performance while minimizing computational costs by activating only portions of the LLM as needed. However, this approach incurs significant memory overhead, making it impractical for edge device memory constraints. For example, Switch Transformers [100], with 32 experts per layer, require 54GBs of memory for inference, exceeding the capacity of most edge devices. Yi et al. [416] notes that in the Switch Transformers model, although most of the model weights $( 8 6 . 5 \\% )$ are attributed to experts, these weights account for only a small fraction $( 2 6 . 4 \\% )$ of the computations. To address this, EdgeMoE [416] introduces a method where experts are loaded into an expert memory buffer only when activated, achieving approximately $3 \\times$ memory savings compared to the baseline where all weights are held in memory. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "KV Cache Compression. When serving LMs for inference, using a KV cache is common practice to avoid intensive recalculations and speed up generation [276]. However, cache memory consumption escalates with model size and sequence length, posing a challenge for edge devices. To manage this, offloading techniques transfer KV caches to CPU memory [12, 308], although this can introduce significant overhead due to the switching costs between GPUs and CPUs. Token dropping compresses cache size by keeping only key tokens, often identified by low attention scores [111, 220, 444]. However, this method struggles with complex tasks, especially at high compression levels, due to increased estimation errors in compressed KV values. GEAR [165] addresses these issues by enhancing KV cache quantization with error-reduction techniques, including: (i) quantizing caches of similar magnitudes to ultra-low precision, (ii) using a low-rank matrix for efficient quantization residual approximation, and (iii) employing a sparse matrix for correcting outliers. This approach separates coherent from incoherent approximation errors, enabling near-lossless KV cache compression and achieving up to $2 . 2 9 \\times$ peak memory reduction. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Besides, Dynamic Memory Compression (DMC) [256] adaptively compresses the KV cache by either adding new key and value representations directly or blending them with the top cache item using a weighted average. Transformer-Lite [193] tackles the redundancy of storing the KV cache twice in model inputs and outputs, which increases memory use. It optimizes storage by allocating a large tensor based on the maximum sequence length needed for inference. Sub-tensors are then created from this main tensor at different address offsets to serve as input and output KV caches, allowing direct writing to the correct locations during inference and removing extra copying steps. LLMaaS [419] introduces LLM as a Service for mobile devices, managing all apps through LLMS. This system uses chunk-wise KV cache compression and swapping, enabling efficient context switching within memory constraints. By segmenting the KV cache into independently compressed and swapped chunks, LLMS balances memory use and I/O bandwidth better than token-level or context-level management. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "4.2.2 Runtime Efficiency Optimization. The goal of decreasing computing workload aligns with enhancing memory efficiency through methods such as quantization, as mentioned in the previous section. Decreasing model weight precision or reducing the number of weights naturally lowers latency. Other runtime efficiency techniques of minimizing inference latency involve, reducing prefill latency, early exits, large and small model collaboration, decreasing switching time in MoE, and reducing latency in distributed SLMs. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Reducing prefill latency. mllm-NPU [395] is the first LLM inference system that leverages on-device NPU (neural processing units) to reduce prefill latency and energy consumption. It incorporates a chunk-sharing graph, shadow outlier execution, and out-of-order subgraph execution to enhance NPU offloading efficiency. Experiments have shown mllm-NPU’s superior performance benefits, including up to $4 3 . 6 \\times$ speedup and $5 9 . 5 \\times$ energy savings. ",
        "page_idx": 30
    },
    {
        "type": "text",
        "text": "Dynamic Early Exits A decoupled runtime saving technique is dynamic early exits. Originating from BranchyNet [335], which introduces exit branches after specific convolution layers in the CNN model, this concept has been adapted for PLMs as Transformer layer-wise early exiting [391]. Early exiting enables dynamic acceleration during inference and reduces temporal overhead by allowing exit without passing through all model layers. To address the inconsistency issue arising from exiting at different layers, COST-EFF [305] distills a multi-exit model from the original PLM. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Large and Small Model Collaboration Model collaboration, deploying SLMs on devices with cloud-based LLM support, enhances runtime efficiency. LLMs will increase latency when directly deployed via mobile engines like llama.cpp due to a large number of computing operations. LLMCad [394] addresses this by using a real-time, memoryresident SLM for simple tokens such as determiners and punctuation. The SLM generates tokens, while a cloud-based LLM verifies and corrects them, speeding up the process. LLMCad enhances token generation up to $9 . 3 \\times$ by pairing the memory-resident SLM, Llama 2 7B, with the cloud-based LLM, Llama 2 13B, cutting latency from 16.2 to 3.9 seconds on Xiaomi Pro for TruthfulQA tasks [208]. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Reducing MoE Switching Time. To reduce latency in MoE architectures caused by frequently switching experts in limited device memory, EdgeMoE [416] enhances runtime efficiency by preemptively predicting which expert will be needed, based on the observed long-tail distribution of unbalanced expert activations. It utilizes a statistical model, built offline, to estimate expert activation probabilities in transformer layers from previous activations. During inference, EdgeMoE preemptively loads the most likely needed expert, accelerating inference by $1 . 1 1 \\times$ to $2 . 7 8 \\times$ and significantly reducing latency. For instance, in a switch transformer model with 8 experts, latency drops from approximately 0.7s to 0.3s, outperforming the baseline method that preloads experts based on hit ratios. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Reducing Latency in Distributed SLMs. Distributing an SLM across smaller devices reduces the need for extensive model compression while preserving accuracy. However, this approach faces challenges that incur latency such as managing diverse device capabilities, handling data dependencies between model segments, and adapting to dynamic resource availability. To address these issues, LinguaLinked [447] addresses these issues by optimizing model assignment to match device capabilities and minimize data transmission, implementing runtime load balancing to redistribute tasks and prevent bottlenecks, and enhancing communication for efficient data exchange between segments. With multi-threading, the system improves, achieving acceleration rates between $1 . 7 3 \\times$ and $2 . 6 5 \\times$ for both quantized and full-precision models. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "Insights: We draw several key insights from the deployment of SLMs: ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "• Model size remains a bottleneck for both memory and runtime efficiency. A common solution is model quantization, which reduces model precision to save memory and lessen computing workload, thereby boosting inference speed [207, 223, 253, 285, 422, 448]. Similarly, KV cache compression also helps achieve these efficiency gains [165, 193, 256, 419].   \n• Mixture of Experts (MoE) is commonly used in SLMs to enhance performance using the same computing resources, but it results in increased memory usage. To address this, only activated experts are loaded into the memory buffer while the majority are stored cold on disk. However, the cost of switching can slow down inference. Designing a preemptive expert pre-load strategy could therefore accelerate the inference [416].   \n• Model collaboration between local SLMs and cloud-based LLMs enhances both memory and runtime efficiency by using smaller models on local devices, which are then verified by cloud LLMs to ensure performance is maintained. Using SLMs locally reduces memory usage and shortens the inference time from the local model. However, internet latency and delays in cloud LLM inference can still introduce latency. Verifying SLM outputs every $N$ tokens using LLMs can effectively mitigate this latency [394]. One deployment approach involves deploying SLMs/LLMs across multiple trusted local devices to maintain original performance while only loading a fraction of the model weights. However, this method can incur latency due to varying device capabilities and resource scheduling challenges. To address these issues, optimizing model assignment to align with device capabilities and minimizing data transmission are effective strategies [447]. ",
        "page_idx": 31
    },
    {
        "type": "text",
        "text": "5 GENERIC AND DOMAIN-SPECIFIC SMALL LANGUAGE MODELS",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "This section investigates small language models (with fewer than 7 billion parameters) in both general and specific domains. It details the methods of obtaining these small language models, the datasets, and the evaluation tasks, exploring the techniques for acquiring SLMs through compression, fine-tuning, or training from scratch. Additionally, we summarize the representative small language models, as detailed in Table 7 and 10. ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "5.1 Generic-domain SLMs ",
        "text_level": 1,
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "Overview. SLMs, with fewer parameters than LLMs, enhance computational efficiency in pre-training, fine-tuning, and inference, reducing memory and energy demands—crucial for resource-limited environments. Their compact, localized nature boosts privacy, personalization, and response times, making them ideal for low-power edge devices. Therefore, SLMs are attracting increasing attention, and various models are being developed. Table 7 summarizes current representative generic-domain 42 SLMs/SLM families. Although all chosen SLMs have similar architectures, they vary in specific training datasets and techniques, with some datasets not being openly available. Taking the latest Llama 3.2 1B models [7] in Figure 19 as an example, its parameter size and use of filtered high-quality training data, pruning-based initialization, knowledge distillation pre-training tasks, and training techniques such as Supervised Fine-Tuning (SFT), Rejection Sampling (RS), and Direct Preference Optimization (DPO) distinguish it from others. ",
        "page_idx": 32
    },
    {
        "type": "image",
        "img_path": "images/5c4d53a6a4c5f09f4ddfaecb6f9efc56e2048c001e55fb6f1444260ba8c674b2.jpg",
        "image_caption": [
            "Fig. 19. Llama 3.2 1B model card "
        ],
        "image_footnote": [],
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "5.1.1 Architecture Design. From Table 7, we observe several trends in component choices for SLMs: ",
        "page_idx": 32
    },
    {
        "type": "text",
        "text": "(1) Recent SLMs frequently employ Grouped Query Attention (GQA) in self-attention mechanisms because it can reduce computational complexity. GQA achieves this by sharing query representations across multiple heads while keeping key and value representations separate. This approach aligns with the goals of SLM to enhance efficiency without compromising functionality. ",
        "page_idx": 32
    },
    {
        "type": "table",
        "img_path": "images/7f16686f1105451014a37e7f8be3e8ce3938b447496f6b7451f897a2796e66ac.jpg",
        "table_caption": [
            "Table 7. High-level Overview and Training Details of Generic-domain Small Language Models. #Params means Parameter amounts. $\" > \"$ indicates parameters larger than 7B. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>#Params</td><td>Date</td><td>Paradigm</td><td>Domain</td><td>Training Datasets</td><td>Training Techniques</td></tr><tr><td>PhoneLM [417]</td><td>0.5B; 1.5B</td><td></td><td>2024.11Pre-train</td><td>Generic</td><td>DCLM-baseline[192],StarCoderData[195]; OpenWebMath[267],Dolma-algebraic and Dolma-arXiv [315]</td><td>RoPE,MHA,GtedFFNorm,ReLUDP,Faso 2,ZeRO</td></tr><tr><td>Llama 3.2 [7]</td><td>1B;3B</td><td>2024.9</td><td>Pre-train</td><td>Generic</td><td>no release (9T tokens)</td><td>GQA,SiLU, Multilingual Textand code,Shared embedding run- ing,Disilatio</td></tr><tr><td>Qwen 1[23]</td><td>1.8B; 7B; &gt;</td><td></td><td>2023.12Pre-train</td><td>Generic</td><td>no release</td><td>MHA; RoPE; SwiGLU; RMSNorm</td></tr><tr><td>Qwen 1.5 [23]</td><td>0.5B 1.8B;4B; 7B; </td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>no release</td><td>MHA; RoPE; SwiGLU; RMSNorm; Multilingual support</td></tr><tr><td>Qwen 2 402]</td><td>0.5B:1.5B; 7B; &gt;</td><td>2024.6</td><td>Pre-train</td><td>Generic</td><td>no release</td><td>GQA; RoPE;SwiGLU; RMSNorm; Multilingual support</td></tr><tr><td>Qwen 2.5 [402]</td><td>0.5B; 1.5B; 3B; 7B; &gt;</td><td>2024.9</td><td>Pre-train</td><td>Generic</td><td>no release</td><td>GQA; RoPE; SwiGLU; RMSNorm; Mulilingual support; Larger corpus</td></tr><tr><td>Gemma [332] Gemma 2[333]</td><td>2B;7B</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>Unknown</td><td>MHA, RoPE, GELUtanh</td></tr><tr><td></td><td>2B;</td><td>2024.7</td><td>Pre-train</td><td>Generic</td><td>Unknown</td><td>GQA;RoPE;GELUtalteatiaandGobion LogitSoft-Capping;RMSNormforPreandPost-Normalization</td></tr><tr><td>SmolLM[9]</td><td>135M; 360M; 1.7B</td><td>2024.7</td><td>Pre-train</td><td>Generic</td><td>smollm-corpus [27]</td><td>GQA,trapezoidalRscheduler</td></tr><tr><td>H2O-Danube3 [274]</td><td>500M; 4B</td><td>2024.7</td><td>Pre-train</td><td>Generic</td><td>Unknown</td><td>Thredifferent tainingstageswitdifferetdamixes</td></tr><tr><td>Fox-1[334]</td><td>1.6B</td><td>2024.6</td><td>Pre-train</td><td>Generic</td><td>Unknown(3T tokens)</td><td>GQA; Deeparchitecture</td></tr><tr><td>Rene[113] MiniCPM [143]</td><td>1.3B 1.2B; 2.4B</td><td>2024.5</td><td>Pre-train Pre-train</td><td>Generic Generic</td><td>Dolma-1.7 [315] Dolma[315]; C4[284]; Pile[84];stack[174];</td><td>Mamba-2layers,sliding-window attention(SWA)</td></tr><tr><td></td><td></td><td>2024.4</td><td></td><td></td><td>StarCoder[195]; UltraChat 86];Ossnstrut [373]; Evollnstrc[92]</td><td>Warmup-Stable-Decay (WSD)learning ate scheduler</td></tr><tr><td>CT-LLM [92]</td><td>2B</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>MAP-CC</td><td>Chinese, MHA,RoPE,SwiGLU,RMSNorm</td></tr><tr><td>OLMo[116]</td><td>1B;7B</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>Dolma[315]1</td><td>SwiGLU;RoPEameeri</td></tr><tr><td>TinyLlama [439]</td><td>1B</td><td>2024.1</td><td>Pre-train</td><td>Generic</td><td>SlimPajama [314]and StarCoder[195]</td><td>GQA, SiLU,FSDP,Flash Attention [73],xFormers [184]</td></tr><tr><td>Phi-1[120]</td><td>1.3B</td><td>2023.6</td><td>Pre-train</td><td>Coding</td><td>CodeTextBook[120] 2</td><td>MHA,GELUtanh,RoPE,FlashAttention</td></tr><tr><td>Pi-1.5[200]</td><td>1.3B</td><td>2023.9</td><td>Pre-train</td><td>Generic</td><td>CodeTextBook [120]; Synthetic Datasets (20B)</td><td>MHA,GELUtanh,RoPE,FlashAtentonDeep Otage</td></tr><tr><td>Phi-2[155]</td><td>2.7B</td><td></td><td>2023.12Pre-train</td><td>Generic</td><td>CodeTextBook[20];SyntheticDatasets ()</td><td>MHA,GELUtanh,RoPE,FlashAtentonDeep Otage</td></tr><tr><td>Phi-3[1]</td><td>3.8B; 7B; &gt;</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>Scaled-up datase from phi-2</td><td>MHA, SiLU, RoPE,FlashAttention, Deep ZeRO Stage 2</td></tr><tr><td>Phi-3.5[1]</td><td>3.8B; 4.2B; 6.6B</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>more multilingual and long-text data RefinedWeb269],duplicated]</td><td>Mltilingual; Vsion; HA, SiLU,RoPE,FlashAttetio,Ze</td></tr><tr><td>OpenELM [241]</td><td>270M: 450M 1.1B; 3B</td><td>2024.4</td><td>Pre-train</td><td>Generic</td><td>partial RedPajama [67],partialDolmav1.6 [315]</td><td>Nobiases inFClayers;Pre-norm:RMSNorm;Posencoding: RoPE; Attention: GQA; FFN:SwiGLU; Tokenizer:LLaMA-style</td></tr><tr><td>MobiLlama [336]</td><td>0.5B;0.8B</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>LLM360 Amber3</td><td>GQA; SwiGLUParameter-sharing</td></tr><tr><td>MobileLLM[22]</td><td>125M; 350M</td><td>2024.2</td><td>Pre-train</td><td>Generic</td><td>Unknown (1T tokens)</td><td>SwiGLUFN,endtinrchitectures,mbeddingsing and grouped queryattention</td></tr><tr><td></td><td></td><td>2</td><td></td><td></td><td></td><td>M</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Cerebras-GPT [84]</td><td>111M;256M; 590M; 1.3B; 2.7B; 6.7B; &gt;</td><td>2023.4</td><td>Pre-train</td><td>Generic</td><td>Pile [108]</td><td>MHA; GELU; Maximal Update Parameterization</td></tr><tr><td>Pythia [31]</td><td>14M;70M;160M:410M 1B:1.4B;2.8B;6.9B;&gt;</td><td>2023.4</td><td>Pre-train</td><td>Generic</td><td>Pile[108]</td><td>MHA; GELU; Flash Atention[74];RoPE[318]; ZeRO[286]</td></tr><tr><td>BLOOMZ [181]</td><td></td><td></td><td>2022.1Pre-train</td><td>Generic</td><td>ROOTS[180]and13programminglanguages</td><td></td></tr><tr><td>Galactica [330]</td><td>125M; 1.3B; 6.7B;&gt;</td><td></td><td>2022.1Pre-tran</td><td>Scientific</td><td>Open-access scientifc materials (106B tokens) but not released</td><td>MHA; GeLU;LearedPosinalEmbeddings</td></tr><tr><td>OPT[440]</td><td>125M350M;1.3B; 2.7B;5.7B</td><td>2022.5</td><td>Pre-train</td><td>Generic</td><td>Pile[108]andPushShift.io Redit[24]</td><td>MHA; ReLU</td></tr><tr><td>XGLM [209]</td><td>1.7B; 2.9B; &gt;</td><td></td><td>2021.12Pre-train</td><td>Generic</td><td>CC100-XL</td><td>-</td></tr><tr><td>GPT-Neo [33]</td><td>125M；350M；1.3B; 2.7B</td><td>2021.5</td><td>Pre-train</td><td>Generic</td><td>Pile[08]</td><td></td></tr><tr><td>Megatron-gpt2 [310]</td><td>355M;2.5B; &gt;</td><td>2019.9</td><td>Pre-train</td><td>Generic</td><td>Wikipedia[83],CC-Stories [341],RealNews [430],OpenWebtext</td><td>-</td></tr><tr><td>MINITRON [252]</td><td>4B；&gt;</td><td>2024.7</td><td>Distilation</td><td>Generic</td><td>8Ttokens inNemotron-4[26]</td><td>LR WSD Scheduer</td></tr><tr><td>Orca 2 [247]</td><td>7B</td><td>2023.11</td><td>Pruning Distillation</td><td>Generic</td><td>Orca 2 dataset</td><td>LLaMA-2-7B based; prompt erasing</td></tr><tr><td>Orca[251]</td><td>13B</td><td>2023.6</td><td>Distillation</td><td></td><td>FLAN-v2 [225]</td><td>From ChatGPT and GPT4, Explanation tuning; Progressive Learming</td></tr><tr><td>MINIMA [434]</td><td>3B</td><td></td><td>2023.11Distilation</td><td>Generic</td><td>ile[108],Wudao[424],Giub[67]</td><td>FromLlama-2-7B,Zero2,FlashAtention,Optimal teachrze</td></tr><tr><td>Dolly-v2 [68]</td><td>3B;7B; &gt;</td><td>2023.4</td><td>Instruction tuning</td><td>Generic</td><td>Databrcks-dolly-15[68]</td><td>from pythia</td></tr><tr><td>LaMini-LM [171]</td><td>61M-7B</td><td>2023.4</td><td>Distilation</td><td>Generic</td><td>LaMini instruction dataset</td><td>a colection ofSLMs distilld fromChatGPT-generated 2.58M instructions.</td></tr><tr><td>Specialized FlanT5[105]</td><td>250M; 760M; 3B</td><td>2023.1</td><td>Instruction Tuning</td><td>Generic (math）</td><td>GSM8K</td><td>Base modelis FlanT5</td></tr></table>",
        "page_idx": 33
    },
    {
        "type": "text",
        "text": "(2) The choice of activation function should balance model capability and efficiency. ReLU, known for its efficiency, introduces greater sparsity to the model, which facilitates faster coefficient calculations for inference acceleration. In contrast, SwiGLU’s parameters are learned during training, allowing the model to dynamically adapt to diverse tasks and datasets, thereby enhancing model capabilities and establishing it as a state-of-the-art option. SiLU, situated between these two, is favored for its balance of computational efficiency and model performance. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "(3) RMS normalization is commonly used than layer normalization due to its reduced computational demands. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "A basic introduction to these options is provided in Section 2. Apart from component choices, there are notable innovations in architecture for SLMs: ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "• Mobilellm [223] highlights that deeper models are more effective than wider ones for improving performance. • Embedding sharing [440] is crucial as embedding layers often constitute over $2 0 \\%$ of a model’s parameters—for example, with 512 dimensions and a $3 2 \\mathrm { k }$ vocabulary, each layer holds 16M parameters in a 125M-parameter model. Smaller models often reuse these weights for both input and output layers, enhancing efficiency and compactness. • Layer sharing [223] increases hidden layers in small Transformer models without additional storage costs. • Shared FFNs [336] make up about $6 5 \\%$ of all trainable parameters, with attention mechanisms and heads accounting for the rest. Sharing FFN parameters across all transformer layers of an SLM is proposed to increase efficiency. • Architecture search ahead of pre-training. PhoneLM [417] proposes a principle for constructing on-device small language models: searching for a resource-efficient architecture on a given hardware to optimize the speed-capacity trade-off before pretraining. This approach inspires the tailored selection of architectural components for on-device SLMs, based on specific compositional requirements such as computing efficiency, model capability, and safety. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "A detailed description of these architectural designs can be found in Section 3.1. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "5.1.2 Training Datasets. From Table 7, we can observe a set of widely used training datasets in SLM development. We provide the details below: ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "• Pile [108]: It comprises 22 smaller, high-quality diverse corpora from various domains, such as Pile-CC, PubMed Central, ArXiv, GitHub, and FreeLaw, designed to offer a comprehensive foundation for language model training. The dataset contains 207 billion tokens and totals 825 GB.   \n• C4 (Colossal Clean Crawled Corpus) [284]: This dataset includes 350 billion tokens, representing a cleaned version of the Common Crawl web corpus, intended to capture a wide snapshot of the internet 4.   \n• The Stack [174]: It contains 6 trillion tokens of source code from over 300 programming languages, useful for developing code-centric AI applications. Python-edu in smollm-corpus [27] consists of Python files that are scored 4 or more by the educational code model and are extracted from the stack-v2-train dataset.   \n• StarCoder [195]: It features 35 billion tokens, predominantly Python code, aimed at programming language understanding and generation.   \n• RedPajama [67]: This dataset encompasses 1.2 trillion tokens derived from over 100 billion text documents, processed using the CCNet pipeline to ensure a rich collection of web texts.   \n• RefinedWeb [269]: This dataset includes 5 trillion tokens of high-quality, extensively filtered web data, offering a valuable resource for training web-aware models.   \n• PushShift.io Reddit [24]: A around 5 billion tokens resource for social media data collection, analysis, and archiving, specifically of Reddit data, aiding research into social media dynamics.   \n• CulturaX [257]: It comprises 6.3 trillion tokens across 167 languages, supporting the development of models with extensive linguistic and cultural understanding.   \n• FineWeb [268], a large-scale (15-trillion tokens, 44 TB disk space) dataset for LLM pretraining. FineWeb is derived from 96 CommonCrawl snapshots. FineWeb-Edu is a subset of FineWeb constructed using scalable automated high-quality annotations for educational value. ",
        "page_idx": 34
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "From the analysis of these datasets, we can derive several critical insights regarding the development of SLMs: (i) Data quality is crucial for training effective SLMs, involving sophisticated filtering like removing duplicates or irrelevant content, often with another LLM’s help. For example, the TinyStories corpus [96] is tailored for simplicity, ideal for training models to handle straightforward narratives. RedPajama-V2 [67] uses the CCNet pipeline to process 30B documents, providing quality signals and IDs for creating a 20B deduplicated dataset. (ii) Code Data: Source code constitutes a significant component of valuable data for training models, particularly because of its structured nature and logical content. Training on code data enhances a model’s reasoning capabilities and supports its ability to generalize across multiple natural languages, which is crucial for applications requiring robust problem-solving and interpretation skills in diverse coding environments [17, 104, 120, 236] ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "5.1.3 Training Algorithms. To enhance the alignment of SLMs with desirable properties such as safety and reasoning, training algorithms, particularly during the fine-tuning phase, are crucial in evolving pre-trained SLMs. ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "• Direct Preference Optimization (DPO) [283] presents a simpler alternative to RLHF for optimizing language models based on human preferences, preventing explicit reward modeling and reinforcement learning. Instead, DPO modifies log probabilities of responses with a dynamic weighting mechanism to prevent model degradation common in probability ratio-focused methods. The DPO loss function is: ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "$\\begin{array} { r } { \\mathcal { L } _ { D P O } ( \\pi _ { \\theta } ; \\pi _ { \\mathrm { r e f } } ) = - \\mathbb { E } _ { ( x , y _ { w } , y _ { l } ) \\sim D } \\left[ \\log \\sigma \\left( \\beta \\log \\frac { \\pi _ { \\theta } ( y _ { w } | x ) } { \\pi _ { \\mathrm { r e f } } ( y _ { w } | x ) } - \\beta \\log \\frac { \\pi _ { \\theta } ( y _ { l } | x ) } { \\pi _ { \\mathrm { r e f } } ( y _ { l } | x ) } \\right) \\right] } \\end{array}$ where $\\pi _ { \\theta }$ is the policy being optimized, $\\pi _ { \\mathrm { r e f } }$ is the reference policy, $D$ includes tuples $( x , y _ { w } , y _ { l } )$ , $\\sigma$ is the sigmoid function, and $\\beta$ scales the log-ratios between $\\pi _ { \\theta }$ and $\\pi _ { \\mathrm { r e f } }$ , guiding the model towards human-preferred outputs. • Reinforcement Learning from Contrast Distillation (RLCD) [405] aims to calibrate generative SLMs/LLMs towards embodying harmless and beneficial characteristics. The process starts with an unaligned LM and initial prompts, which are modified into two variants ${ p + }$ and $\\hbar -$ , intended to promote and suppress, respectively, attributes like helpfulness and harmlessness. Upon inputting these prompts, the LM generates outputs $^ { o + }$ and $o -$ , with $^ { o + }$ automatically designated as the preferred response. This automation speeds up training by avoiding additional evaluative scoring. The training continues under the RLHF framework. • Conditioned Reinforcement Learning Fine-Tuning (C-RLFT), by OpenChat [356], enhances model performance by incorporating low-quality data during SFT. C-RLFT leverages varied data qualities with simple rewards (e.g., expert data at 1 credit, sub-optimal at 0.1), using distinct prompt tokens to condition data sources, eliminating costly human feedback. Similarly, Data Mix [274] trains on English text in three stages, reducing noisy web data progressively in each stage in favor of higher-quality data. • Explanation Tuning, proposed by Orca [251], addresses the limitations of standard instruction-based fine-tuning, which often restricts SLMs to style imitation rather than reasoning. It uses system prompts with instructions to direct GPT-4 to produce detailed explanations or perform step-by-step reasoning. The resulting instructions and the responses are used as a dataset for fine-tuning SLMs to have better ability of reasoning. ",
        "page_idx": 35
    },
    {
        "type": "text",
        "text": "• Progressive Learning, proposed by Orca [251], aims to bridge the capability gap between Orca and the more capable GPT-4. It starts with training on five million data points from ChatGPT, followed by one million from GPT-4. Research suggests that an intermediate-level teacher can improve distillation effects, enabling a stepwise learning approach where students start with simpler examples and gradually move to more complex ones, receiving improved reasoning and explanations from a more advanced teacher.   \n• Prompt Erasing introduced by Orac 2 [247], is a distillation strategy designed to enhance the independent reasoning capabilities of student SLMs. In this approach, a more capable teacher LLM is given intricate prompts intended to elicit specific strategic behaviors and more precise outcomes. During the training phase, the SLM is exposed only to the task instruction and the resulting behavior, without access to the original intricate prompts that initiate such responses. This technique, known as Prompt Erasing, positions the student model as a cautious reasoner because it not only learns to perform specific reasoning steps but also develops strategies for approaching tasks at a higher level.   \n• Maximal Update Parameterization $( \\mu \\mathbf { P } )$ optimizes control initialization, layer-wise learning rates, and activation magnitudes to ensure stable training regardless of model layer widths. This method enhances training stability and allows the same optimizer settings, especially learning rates, to be used across different model scales. For instance, Cerebras-GPT [314] employs $\\mu \\mathrm { P }$ to train its models efficiently. ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "5.1.4 Model Performance. To compare the performance of SLMs, we have extracted experimental results from two recent and concurrent studies published in June 2024, OLMo [116] and MobiLlama [336], and the recently proposed edge-device Llama 3.2 1B & 3B in September 2024 5. The extracted results are merged and shown in Table 8. From the table, we can find that the following evaluation benchmarks are commonly used: ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "(1) MMLU [134]: Evaluate broad knowledge across diverse fields such as humanities, science, technology, engineering, and management. It includes multiple-choice questions covering 57 tasks ranging from elementary mathematics to US history, computer science, law, and beyond, with a total of 14K test samples.   \n(2) HellaSwag [429]: Assesses the model’s ability to select the correct ending to scenarios from multiple options, testing common sense reasoning, including 10K test samples.   \n(3) ARC [65]: The AI2’s Reasoning Challenge (ARC) dataset features multiple-choice science exam questions for grades 3 to 9, divided into Easy and Challenge partitions, with the latter containing more complex questions necessitating reasoning. Most questions offer four answer choices. ARC includes a supporting knowledge base of 14.3M unstructured text passages, with 1.17K test samples in ARC_Challenge and 2.25K in ARC_Easy.   \n(4) PIQA [32]: A commonsense reasoning dataset designed to evaluate the physical knowledge of NLP models. It presents questions (goals) that require physical commonsense for correct resolution, alongside two detailed response options (sol1 and sol2). The dataset comprises 3,000 test samples.   \n(5) Winogrande [294]: a dataset structured as a fill-in-the-blank task with binary options, designed to assess commonsense reasoning. The dataset includes 1,767 test samples by default splits. ",
        "page_idx": 36
    },
    {
        "type": "text",
        "text": "Accuracy is used as the evaluation metric in the table. Open Language Model (OLMo) [116] is publicly available with its training data and code 6. MobiLlama [336] is a general-purpose SLM designed from scratch, available in 0.5B and 0.8B versions. It adopts a unique approach by using a shared FFN across all transformer blocks, enhancing efficiency. MobiLlama also show high efficiency on diverse hardware (Table 9). ",
        "page_idx": 36
    },
    {
        "type": "table",
        "img_path": "images/48eac85984db214f1239d2587a5c9212bbe0e83f4df0523301ac0d5c88ed7b07.jpg",
        "table_caption": [
            "Table 8. Performance of Various SLMs on Common Benchmarks: data from MobiLlama [336], OLMo [116], and Llama 3.2. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model Size Range</td><td>Model</td><td>MMLU</td><td>HellaSwag</td><td>ARC</td><td>PIQA</td><td>Winogrande</td></tr><tr><td rowspan=\"10\">&lt;1B</td><td>gpt-neo-125m</td><td>26.0</td><td>30.3</td><td>23.0</td><td>62.5</td><td>51.8</td></tr><tr><td>tiny-starcoder-170M</td><td>26.8</td><td>28.2</td><td>21.0</td><td>52.6</td><td>51.2</td></tr><tr><td>cerberas-gpt-256m</td><td>26.8</td><td>29.0</td><td>22.0</td><td>61.4</td><td>52.5</td></tr><tr><td>opt-350m</td><td>26.0</td><td>36.7</td><td>23.6</td><td>64.7</td><td>52.6</td></tr><tr><td>megatron-gpt2-345m</td><td>24.3</td><td>39.2</td><td>24.2</td><td>66.9</td><td>53.0</td></tr><tr><td>LiteLlama</td><td>26.2</td><td>38.5</td><td>24.9</td><td>67.7</td><td>49.9</td></tr><tr><td>gpt-sw3-356m</td><td>25.9</td><td>37.1</td><td>23.6</td><td>64.9</td><td>53.0</td></tr><tr><td>pythia-410m</td><td>27.3</td><td>40.9</td><td>26.2</td><td>67.2</td><td>53.1</td></tr><tr><td>xglm-564m</td><td>25.2</td><td>34.6</td><td>24.6</td><td>64.9</td><td>53.0</td></tr><tr><td>Lamini-GPT-LM0.59B</td><td>25.5</td><td>31.6</td><td>24.2</td><td>63.9</td><td>47.8</td></tr><tr><td></td><td>MobiLlama 0.5B</td><td>26.5</td><td>52.5</td><td>29.5</td><td>72.0</td><td>57.5</td></tr><tr><td></td><td>MobiLlama 0.8B</td><td>26.9</td><td>54.1</td><td>30.2</td><td>73.2</td><td>57.5</td></tr><tr><td rowspan=\"19\"></td><td>StableLM1.6B</td><td></td><td>68.2</td><td>43.8</td><td>74.0</td><td></td></tr><tr><td>Pythia 1B</td><td>=</td><td>44.7</td><td>33.1</td><td>69.1</td><td>=</td></tr><tr><td>TinyLlama 1.1B</td><td>=</td><td>58.7</td><td>34.8</td><td>71.1</td><td>=</td></tr><tr><td>OLMo-1B</td><td></td><td>62.5</td><td>34.5</td><td>73.7</td><td></td></tr><tr><td>OLMo 1.2B</td><td>25.9</td><td>62.5</td><td>34.5</td><td>-</td><td>58.9</td></tr><tr><td>Boomer 1B</td><td>25.4</td><td>31.6</td><td>22.3</td><td></td><td>51.0</td></tr><tr><td>Pythia-Dedup 1B</td><td>24.3</td><td>49.6</td><td>29.1</td><td>=</td><td>54.0</td></tr><tr><td>Falcon-RW 1B</td><td>25.4</td><td>63.1</td><td>35.1</td><td>=</td><td>61.9</td></tr><tr><td>Cerebras-GPT1.3B</td><td>26.7</td><td>38.5</td><td>26.1</td><td>=</td><td>53.6</td></tr><tr><td>Lamini 1.3B</td><td>28.5</td><td>38.1</td><td>26.6</td><td>=</td><td>50.6</td></tr><tr><td>OPT1.3B</td><td>24.6</td><td>54.5</td><td>29.6</td><td>=</td><td>59.7</td></tr><tr><td>GPT-NEO 1.3B</td><td>24.8</td><td>48.5</td><td>31.3</td><td>=</td><td>57.1</td></tr><tr><td>Pythia-Deduped 1.4B</td><td>25.5</td><td>55.0</td><td>32.6</td><td></td><td>56.9</td></tr><tr><td>MobiLlama 1.2B</td><td>24.8</td><td>63.0</td><td>34.6</td><td></td><td>62.0</td></tr><tr><td>Gemma 2 2B</td><td>57.8</td><td>61.1</td><td>76.7</td><td></td><td></td></tr><tr><td>Llama 3.2 1B</td><td>49.3</td><td>41.2</td><td>59.4</td><td></td><td></td></tr><tr><td>Llama 3.2 3B</td><td>63.4</td><td>69.8</td><td>78.6</td><td></td><td></td></tr><tr><td rowspan=\"9\"></td><td>Phi-3.5-mini 3.8B</td><td>69.0</td><td>81.4</td><td>87.4</td><td>=</td><td>=</td></tr><tr><td>Pythia 6.9B</td><td></td><td>63.8</td><td>44.1</td><td>75.1</td><td></td></tr><tr><td>Falcon-7B</td><td>=</td><td>75.9</td><td>47.5</td><td>78.5</td><td></td></tr><tr><td>LLaMA 7B</td><td>=</td><td>76.2</td><td>44.5</td><td>77.2</td><td></td></tr><tr><td>Llama 2 7B</td><td>=</td><td>76.8</td><td>48.5</td><td>76.7</td><td></td></tr><tr><td>MPT-7B</td><td>=</td><td>77.6</td><td>46.5</td><td>77.3</td><td></td></tr><tr><td>RPJ-INCITE-7B</td><td></td><td>70.3</td><td>42.8</td><td>76.0</td><td></td></tr><tr><td>OLMo-7B</td><td></td><td>76.4</td><td>48.5</td><td>78.4</td><td>=</td></tr><tr><td></td><td>=</td><td></td><td></td><td></td><td></td></tr></table>",
        "page_idx": 37
    },
    {
        "type": "table",
        "img_path": "images/7b09e9822fef32fc9cbff258819269c0c4e3644175f3124d16a3d7db4ff3a24f.jpg",
        "table_caption": [
            "Table 9. Comparison of MobiLlama 0.5B with large-base 1.2B, Llama2 7B, and Phi2-2.7B in terms of efficiency and resource consumption on low-end hardware devices [336]. "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Platform</td><td>Model</td><td>#Params</td><td>Precision</td><td>Avg Tokens/Sec</td><td>Avg Memory Consumption</td><td>Avg Battery Consumption /1k Tokens</td><td>CPU Utilization</td></tr><tr><td rowspan=\"4\">RTX2080Ti</td><td>Llama2</td><td>7B</td><td>bf16</td><td>14.85</td><td>27793MB</td><td>135.51 mAH</td><td>31.62%</td></tr><tr><td>Phi2</td><td>2.7B</td><td>bf16</td><td>32.19</td><td>12071 MB</td><td>59.13 mAH</td><td>24.73%</td></tr><tr><td>large-base</td><td>1.2B</td><td>bf16</td><td>50.61</td><td>6254MB</td><td>18.91 mAH</td><td>18.25%</td></tr><tr><td>MobiLlama</td><td>0.5B</td><td>bf16</td><td>63.38</td><td>3046MB</td><td>8.19 mAH</td><td>14.79%</td></tr><tr><td rowspan=\"4\">CPU-i7</td><td>Llama2</td><td>7B</td><td>4bit</td><td>5.96</td><td>4188MB</td><td>73.5 mAH</td><td>49.16%</td></tr><tr><td>Phi2</td><td>2.7B</td><td>4bit</td><td>22.14</td><td>1972 MB</td><td>27.36 mAH</td><td>34.92%</td></tr><tr><td>large-base</td><td>1.2B</td><td>4bit</td><td>29.23</td><td>1163MB</td><td>10.81 mAH</td><td>30.84%</td></tr><tr><td>MobiLlama</td><td>0.5B</td><td>4bit</td><td>36.32</td><td>799 MB</td><td>4.86 mAH</td><td>24.64%</td></tr><tr><td rowspan=\"4\">Snapdragon-685</td><td>Llama2</td><td>7B</td><td>4bit</td><td>1.193</td><td>4287MB</td><td>10.07 mAH</td><td>77.41%</td></tr><tr><td>Phi2</td><td>2.7B</td><td>4bit</td><td>2.882</td><td>1893MB</td><td>14.61 mAH</td><td>56.82%</td></tr><tr><td>large-base</td><td>1.2B</td><td>4bit</td><td>6.687</td><td>780MB</td><td>6.00 mAH</td><td>17.15%</td></tr><tr><td>MobiLlama</td><td>0.5B</td><td>4bit</td><td>7.021</td><td>770MB</td><td>5.32 mAH</td><td>13.02%</td></tr></table>",
        "page_idx": 37
    },
    {
        "type": "text",
        "text": "From Table 8 and 9, we can conclude that: (1) MobiLlama 0.5B and 0.8B demonstrate that a shared FFN design can facilitate excellent performance in SLMs with fewer than 1B parameters, even rivaling some models in the 1B-3B range. (2) The performance of MobiLlama 1.2B and OLMo 1.2B illustrates that advanced SLM architectures incorporating high-quality data, SwiGLU, non-parametric layer normalization, RoPE, BPE tokenization, and a shared FFN design can achieve competitive results among models with 1B-3B parameters. (3) MobiLlama demonstrates that SLMs can significantly reduce resource consumption on low-end hardware devices, achieving comparable performance while using a smaller proportion of battery power, memory, and GPU utilization. (4) Popular techniques such as pruning, quantization, distillation, SFT, and DPO, utilized in Llama 3.2, have substantially enhanced SLM performance. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Insights: We draw several key insights from the development of generic-domain SLMs: ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "• Typical SLM architectures generally incorporate features such as GQA, gated FFN with SiLU activations, RMS normalization, deep and thin architectures, embedding sharing, layer sharing, and shared FFNs.   \n• Although these components are widely used, current research has not yet thoroughly explored their specific contributions within SLMs.   \n• The importance of data quality in SLM research is increasingly emphasized, often considered more critical than the quantity of data and model architectural configurations.   \n• Post-pretraining, meticulous fine-tuning is often required to enhance the safety of SLMs, involving strategies to distill capabilities from LLMs better. Common strategies include explanatory tuning, progressive learning, and prompt erasing. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "5.2 Domain-Specific SLMs ",
        "text_level": 1,
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "Overview. The capability of LLMs to generate human-like text has significantly captured public interest, highlighting their potential in the field of general artificial intelligence. However, inefficiencies persist when integrating LLMs into specialized applications due to resource constraints. Unlike the need for extensive general knowledge and capabilities, domain-specific SLMs should focus on well-defined tasks and expertise pertinent to specific fields. For instance, specialized models can significantly impact biomedical research and healthcare by fine-tuning for interpretable mental health analysis, or assisting humans in legal dialogues and financial tasks through instruction tuning, showcasing their potential transformative influence. Given the limited number of SLMs specialized in specific domains, we demonstrate some existing SLMs individually across healthcare, science, finance, and law domains. ",
        "page_idx": 38
    },
    {
        "type": "text",
        "text": "5.2.1 SLMs for Healthcare. Hippocrates [3] is an open-source medical language model framework with free access to its data, codebase, checkpoints, and protocols 7. It utilizes a medical pre-training corpus from Medical Guidelines, PMCPatients [452], and PubMedQA-contexts [163], totaling about 300M tokens. The Hippo series, a 7B model, undergoes continuous pre-training, instruction tuning, and RLHF. Fine-tuned on Mistral and Llama-2, it rivals 70B models in some evaluation. For example, Hippo-Mistral 7B scores $5 9 . 9 \\%$ on MedQA, outperforming Meditron 70B [55] at $5 8 . 5 \\%$ . BioMedLM [34], a 2.7B GPT-style model trained on PubMed content [108], excels in biomedical QA after fine-tuning, achieving $5 7 . 3 \\%$ on MedMCQA (dev) and $6 9 . 0 \\%$ on MMLU medical genetics exams. Available on Hugging Face Hub 8. AdaLM [414] enhances domain-specific SLMs by continuing training on a medical-focused SLM atop a general pretrained model. It emperically validates adaptation then distillation is the most effective distillation way. AdaLM modified a BERT_base model (12 layers, 768 hidden size) [83] with a 16GB PubMed 9 abstracts corpus. MentalLLaMA [406] introduces the first IMHI dataset for mental health analysis and the first open-source LM for explainable analysis on social media. The IMHI is compiled from ten sources, totaling 105K samples. Expert-designed mental health analysis prompts are employed via ChatGPT for explanations. Based on Llama-2-7B, MentalLLaMA is instruction-tuned on this data and matches top methods in accuracy on the IMHI test set. Project code is available at 10. ",
        "page_idx": 38
    },
    {
        "type": "table",
        "img_path": "images/0a6052d959b55329f431979166c399bb48eb201513690234f56f7bc1ed2ca143.jpg",
        "table_caption": [
            "Table 10. High-level Overview and Training Details of Specific-domain Small Language Mode "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Model</td><td>#Params</td><td>Date</td><td>Base Models</td><td>Domain</td><td>Training Datasets</td><td>Train Techniques</td></tr><tr><td>Hippocrates [3]</td><td>7B</td><td>2024.4</td><td>Instruction Tuning (LLaMA2[339],Mistral [160])</td><td>Healthcare</td><td>Medical Guidelines, PMC-Patients [452], andPubMedQA- contexts [163]</td><td>Continual pre-training, in- struction tuning, RLHF</td></tr><tr><td>BioMedLM[34]</td><td>2.7B</td><td>2024.3</td><td>From scratch and Fine- tuning</td><td>Healthcare</td><td>PubMed[108]</td><td>FSDP</td></tr><tr><td>BioMistral[178]</td><td>7B</td><td>2024.2</td><td>Mistral[160]</td><td>Biomedicine</td><td>PubMed [108]</td><td>Continual pretraining</td></tr><tr><td> MentaLLaMA [406]</td><td>7B; 13B</td><td>2023.9</td><td>Instruction (LLaMA2 [339]) Tuning</td><td>Healthcare</td><td>IMHI dataset</td><td>RLHF; PEFT</td></tr><tr><td>AdaLM [414]</td><td>34M</td><td>2021.6</td><td>Distillation (BERT[83] or MiniLM[362])</td><td>Healthcare</td><td>PubMed [108]</td><td>Continual pretraining, Adapt-and-Distill</td></tr><tr><td>Rho-1 [210]</td><td>1B; 7B</td><td>2024.4</td><td>TinyLlama-1.1B [439], Mistral-7B[160]</td><td>Science (Math- ematics)</td><td>OpenWebMath [267]</td><td>Continual pretraining</td></tr><tr><td>ChemLLM [436]</td><td>7B</td><td>2024.4</td><td>Instruction Tuning (In- ternLM2)</td><td>Science (Chem- istry)</td><td>ChemData</td><td>Continual training and fine-tuning</td></tr><tr><td>SciGLM [435]</td><td>6B</td><td>2024.3</td><td>Instruction Tuning (ChatGLM-6B)</td><td>Science</td><td>SciInstruct</td><td>Self-reflective instruction annotation</td></tr><tr><td>Llemma [22]</td><td>7B</td><td>2023.10</td><td>Code Llama 7B</td><td>Science (Math- ematics)</td><td>Proof-Pile-2 [22]</td><td>Continual pre-training</td></tr><tr><td>OceanGPT [30]</td><td>2B； 7B； 14B</td><td>2023.10</td><td>LLaMA2 [339]</td><td>Science (Ocean)</td><td>Open-access litera- ture,DoINSTRUCT</td><td>Continual pre-training, In- struction tuning</td></tr><tr><td>AstroLLaMA [258]</td><td>7B</td><td>2023.9</td><td>Tuning (LLaMA-2-7B)</td><td>Science (As- tronomy)</td><td>arXiv abstracts from Kaggle</td><td>Continual traning</td></tr><tr><td>DARWIN [388]</td><td>7B</td><td>2023.8</td><td>LLaMa 7B</td><td>Science (physics, chem- istry, and material)</td><td>SciQ [375], Scien- tific paper[388], FAIR [388]</td><td>Fine-tuning</td></tr><tr><td>MindLLM [412]</td><td>1.3B; 3B</td><td>2023.10</td><td>From-scratch and Super- vised Fine-tuning</td><td>Law, Finance</td><td>Pile [108]，Wudao [424], CBooks</td><td>Train on Bilingual Mixture Data,SFT</td></tr></table>",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 39
    },
    {
        "type": "text",
        "text": "5.2.2 SLMs for Science. SciGLM [435] is a collegiate-level scientific language model overcoming data scarcity with a self-reflective instruction annotation framework. Utilizing GPT-4 [2], it generates detailed reasoning for unlabeled scientific problems through three steps with designed prompts in Table ??: (i) CoT prompt for step-by-step answers (Prompt 1), (ii) reflective prompt for correcting errors (Prompt 2), and (iii) integrating the correct answer for clarity (Prompt 3). The SciInstruct dataset spans physics, chemistry, math, and proofs, tuning ChatGLM-6B’s [93] reasoning abilities. SciGLM boosts the base model’s (ChatGLM3-6B-Base) scientific QA accuracy by $3 . 0 6 \\%$ on benchmarks such as CEval-Hard [149], CEval-Sci [149], MMLU-Sci [134], SciEval [319], and SciBench [363]. Llemma [22], an SLM derived from CodeLlama [292], specializes in mathematical reasoning. By continual pre-training, its 7B model is evolved on 55B tokens from the newly created Proof-Pile-2 dataset, which includes scientific papers, math web content, and mathematical code up until April 2023, to enhance few-shot capabilities. It excels in mathematical benchmarks like MATH [134], GSM8k [66], OCWCourses [186], MMLU-STEM [134], and SAT, surpassing all comparable open-weight models. ChemLLM [436] is a chemistry-focused language model that utilizes its proposed ChemData, a dataset designed for instruction tuning that transforms chemical data into dialogue format for training. ChemLLM is based on InternLM2-Base-7B [40], initially enhancing its language skills with a multi-corpus of 1.7 million Q&A pairs from Hugging Face, then fine-tunes using ChemData and the multi-corpus to maintain its general capabilities. ChemLLM excels in interdisciplinary chemical tasks within the proposed ChemBench, achieving results comparable to GPT-4 [2] and outperforming GPT-3.5 with a score of 92.6 in Mol2caption, slightly below that of GPT-4. AstroLLaMA [258] introduces an astronomy-focused language model. Based on Llama-2-7B [339] and enhanced via continual pre-training, it has been developed using over 300K astronomy abstracts from arXiv 11. AstroLLaMA achieves $3 0 \\%$ lower perplexity than Llama-2-7B, indicating substantial improvements in domain adaptability. AstroLLaMA is available 12 for tasks such as automated paper summarization and conversational agent development in astronomy. ",
        "page_idx": 39
    },
    {
        "type": "table",
        "img_path": "images/6095b11a617839e4830381c5584b8c024a668a165c10e234f2a66366d522a074.jpg",
        "table_caption": [
            "Table 11. Prompts for self-reflective instruction annotation framework "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Chain-of-Thought</td><td>[Prompt 1] The folowing input consists of a science problem, please generate an elaborate step- by-step solution to the problem.</td></tr><tr><td>ReflectiveGeneration</td><td>[Prompt 2] The following input comprises a science problem and a corresponding solution. However, this solution is incorrect, please reflect on its errors and then generate a correct step-by- step solution to the problem.</td></tr><tr><td>Prompt Answer</td><td>[Prompt 3] The following input consists of a science problem, a corresponding solution, and the real answer. The given solution is incorrect, please reflect on its errors and then generate a correct step-by-step solution to the problem based on the real answer.</td></tr></table>",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "5.2.3 SLMs for Finance and Law. MindLLM [412] introduces a bilingual (Chinese and English) SLM, pretrained on the Pile dataset [108] for English and WuDao [424], CBook, and various Chinese web content for Chinese. Bilingual training enhances capacity and prevents catastrophic forgetting. It explores specific domains such as law and finance through supervised fine-tuning. In law, it utilizes publicly available legal data, scenario-based Q&A from LaW-GPT [139], and NLP-based legal tasks from DISC-LawLLM [427]. In finance, EastMoney 13 is selected as the data source. ",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "Insights: We draw several key insights from the development of domain-specific SLMs: ",
        "page_idx": 40
    },
    {
        "type": "text",
        "text": "• Adapting SLMs to domain-specific data is a common practice for acquiring domain-specific SLMs, prompting many to create their datasets [258, 406, 435, 436]. These datasets are often annotated using LLMs like GPT-4 and used to continual pre-train or fine-tune general models such as LLaMa-2-7B [3, 34]. To ensure the data quality, specialized annotation frameworks are developed, such as SciGLM [435].   \n• In domains with abundant corpora, training a general model from scratch and fine-tuning it using SFT [412] is practical. Bilingual settings during training can prevent catastrophic forgetting.   \n• Distilling general capabilities from LLMs while integrating domain-specific knowledge from corpora is another method for developing domain-specific SLMs [414]. ",
        "page_idx": 40
    },
    {
        "type": "table",
        "img_path": "images/d7614a38b998519482b9ad9ab776369620dacab6d03d9d1eb88cbd8282792b50.jpg",
        "table_caption": [
            "Table 12. SLMs help LLMs in different aspects "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Aspect</td><td>Representative work</td><td>Key point</td></tr><tr><td rowspan=\"9\"> LM for relablee</td><td>APRICOT [343]</td><td>Trains a smallauxiliary model to predict LLM&#x27;s confidence using only textual inputs and outputs.</td></tr><tr><td>POLAR [450] Hallucination Detector in NMT</td><td>Using a BERT model to calibrate LLM responses. Using lightweight classifiers to detect hallucinations in Neural Ma-</td></tr><tr><td>[399]</td><td>chine Translation.</td></tr><tr><td>SAPIA</td><td></td></tr><tr><td>SuperICL [393]</td><td></td></tr><tr><td></td><td>SLM Plug-ins provide confidence and prediction for contextual ex- emplars to aid in-context learning.</td></tr><tr><td>SuperContext [408]</td><td>Specific SLM enhances ICL by providing confidence and predictions to overcome out-of-domain challenges.</td></tr><tr><td>Self-RAG [18]</td><td>A proxy model labels special tokens during RAG data generation for fine-tuning.</td></tr><tr><td>SKR [365]</td><td>Training a small model to detect its self-knowledge for better use of external knowledge.</td></tr><tr><td>SlimPLM [325]</td><td>Detecting missing knowledge in LLMs with a slim proxy model, enhancing the LLM&#x27;s knowledge integration.</td></tr><tr><td>In-Context RALM [287]</td><td>Training a RoBERTa-based reranker for top-k BM25 documents using LM signals to enhance LM gains.</td></tr><tr><td>CRAG [401]</td><td>Training a lightweight evaluator to assess document quality and trigger actions based on confidence levels.</td></tr><tr><td>GSR [148]</td><td>Traininga Generative Sub-graph Retriever (GSR) for relation chain in RAG when retrieving from knowledge graphs.</td></tr><tr><td>SLM for Prompt Extraction [443] extracting LLM</td><td>Small model trained to predict confidence of extracted system prompt from adversarial prompts.</td></tr><tr><td rowspan=\"4\"> prompts</td><td>Prompt Stealing Attacks [297]</td><td>galst</td></tr><tr><td>Output2prompt [433]</td><td>Using a sparse encoder-decoder-based T5 small model to reverse-</td></tr><tr><td>Model Purifying [197]</td><td>engineer LLM inputs from outputs. Using SLMs to ensemble with LLMs, mitigating negative effects</td></tr><tr><td>LP[242]</td><td>from uncurated data. Learning Percentage as a difficulty metric.</td></tr><tr><td rowspan=\"4\">SLM for Fine-tuning</td><td>Emulated Fine-tuning [246]</td><td></td></tr><tr><td>CROSSLM[79]</td><td> SLMs enhance LLMs by generating task-specific high-quality data.</td></tr><tr><td>Weak-to-Strong Search [453]</td><td>FramingLLMalignment asa test-time greedy search to maximize</td></tr><tr><td>SLCoLM[327]</td><td>the log-probability difference between tuned and untuned SLMs. Using SLM predictions to guide the LLM generation process in</td></tr><tr><td rowspan=\"3\">SLM for LLM applications SLM for LLM</td><td>HEF [413]</td><td>CingseEtyM&#x27;sacersai</td></tr><tr><td>Contrastive decoding [198]</td><td>Enhancing text quality by maximizing the difference between expert</td></tr><tr><td>Llama Guard [153]</td><td>and amateur log probabilities. An LLM-based input-output safeguard model geared towards</td></tr><tr><td rowspan=\"2\"> safety</td><td>SLM as Guardian [177]</td><td>Human-AI conversation use cases. Asmaller LLM for both harmful query detection and safeguard</td></tr><tr><td></td><td>response generation.</td></tr><tr><td rowspan=\"4\">SLM for LLM evaluation</td><td>SLIDE [449]</td><td>Utilizing SLMs trained via contrast learning to distinguish and score responses in dialogue scenarios effectively.</td></tr><tr><td>Kuhn et al. [175]</td><td> An SLM is used as the natural language inference classifier.</td></tr><tr><td>SelfCheckGPT[240]</td><td>An SLM is used to calculate BERTScore.</td></tr><tr><td>Factscore [244]</td><td>An SLM functions as the natural language inference classifier.</td></tr></table>",
        "page_idx": 41
    },
    {
        "type": "text",
        "text": "6 SLMS FOR LLMS ",
        "text_level": 1,
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "In this section, we provide a comprehensive review of how SLMs enhance LLMs. While LLMs are robust, they face challenges such as latency during inference, labor-intensive fine-tuning, noise filtration issues in retrieval, suboptimal zero-shot performance, copyright infringement risks, and evaluation difficulties. SLMs can help LLMs to alleviate these issues. Research in this field can be categorized into five primary areas: (i) using SLMs for reliable LLM generation; (ii) extracting prompts for LLMs using SLMs; (iii) fine-tuning LLMs with SLMs; (iv) applying SLMs in LLM applications; (v) utilizing SLMs as guardian; and (vi) evaluating LLMs using SLMs. A summary of representative work in each category along with their key point is given in Table 12. Next, we introduce each category in detail. ",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "6.1 SLM for Reliable LLM Generation ",
        "text_level": 1,
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "Although LLMs generally produce fluent and convincing text, they can occasionally generate erroneous responses [159, 349]. Additionally, LLMs are susceptible to privacy breaches from untrusted data collection, which can erode user trust or cause harm. To address these issues, recent studies have focused on using SLMs to calibrate LLM confidence, detect hallucinations, and improve retrieval-augmented LLMs and their reasoning capabilities. ",
        "page_idx": 42
    },
    {
        "type": "image",
        "img_path": "images/fd9856ea9edd8abf68bb3993615db91a48a01c7506699c07a2722ea28ad8f21a.jpg",
        "image_caption": [
            "Fig. 20. Architectures of Enhancing Calibration and Hallucination Detection of LLMs. "
        ],
        "image_footnote": [],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "Enhancing Calibration and Hallucination Detection of LLMs As illustrated in Figure 20 (a), to calibrate LLMs, an SLM processes both questions and LLM-generated answers to predict calibrated confidence. This training involves minimizing the discrepancy between estimated calibration error and predicted confidence score. For instance, APRICOT [343] uses an auxiliary DeBERTaV3 model [131] to assess LLM confidence in open-question scenarios, aiming to improve uncertainty expression and response adjustment. Similarly, POLAR [450] has developed a self-supervised approach that generates risk scores for each response to calibrate LLM confidence, utilizing a small BERT model [83] to synchronize LLM outputs with other weak supervision sources. As shown in Figure 20 (b), for hallucination detection, an SLM analyzes LLM internal states to output the likelihood of hallucination. This process uses supervised data obtained by testing the knowledge boundaries of the LLM. In neural machine translation, Xu et al. [399] develop a lightweight detector that analyzes token contributions to hallucinations, outperforming both model-free baselines and quality estimation classifiers. Furthermore, SAPLMA [21] found that LLM internal states can signal the truthfulness of statements, with a small BERT classifier trained to differentiate correct from incorrect predictions achieving accuracies of $7 1 \\%$ to $8 3 \\%$ . ",
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "Enhancing Retrieval-Augmented Generation Generally, as shown in Figure 21, SLMs can also serve as proxy models to evaluate the familiarity of LLMs with user queries, determining whether LLMs need to retrieve additional information or can respond directly. For example, SlimPLM [325] is a small proxy model that assesses the necessity for LLM retrieval by generating heuristic answers. High-quality responses indicate that LLMs can handle queries independently, whereas lower-quality outputs require further retrieval. Additionally, Self-Knowledge Guided Retrieval (SKR) [365] enables SLMs to autonomously decide when LLMs should operate independently, based on their self-assessment of knowledge limitations. Further, SELF-RAG [18] improves the factual accuracy and quality of LLM outputs through on-demand retrieval and self-reflection. This method employs a small critic language model to issue reflective markers and make binary decisions regarding the need for further information retrieval. Moreover, some studies utilize SLMs to evaluate the relevance of retrieved documents. LongLLMLingua [162] employs SLMs to calculate the relevance of documents to a query $x ^ { q u e }$ using perplexity, as formalized by the equation: ",
        "page_idx": 42
    },
    {
        "type": "image",
        "img_path": "images/a799fdedf2acbfe0cf778e6b9e31f6394c944c22af26e4e01b46328d0a6c4a24.jpg",
        "image_caption": [
            "Fig. 21. Architecture of SLM as a Heuristic RAG Prober. Manuscript submitted to ACM "
        ],
        "image_footnote": [],
        "page_idx": 42
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 43
    },
    {
        "type": "equation",
        "img_path": "images/ef74fcffda1a392094f0f94d29a585fc18aa9ad52d2ac41e1321d14a23c723e1.jpg",
        "text": "$$\nr _ { k } = - \\frac { 1 } { N _ { c } } \\sum _ { i } \\log \\phi _ { ^ { \\mathrm { S L M } } } ( x _ { i } ^ { q u e } | x _ { k } ^ { d o c } ) , \\quad k \\in \\{ 1 , 2 , . . . , K \\}\n$$",
        "text_format": "latex",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "where $x _ { i } ^ { q u e }$ is the $i \\cdot$ -th token in the query sequence, $x _ { k } ^ { d o c }$ is the retrieved document, and $N _ { c }$ is the total number of tokens in the query. $ { p _ { \\mathrm { S L M } } }$ represents the probability generated by an SLM. CRAG [401] employs SLMs as evaluators of document relevance in the same way. RA-ISF [219] trains a small language model that checks the base LLM in self-knowledge, relevance judgment, and question decomposition. In addition, some research employs SLMs as re-rankers to refine the order of documents provided by initial retrieval efforts such as BM25 [291]. In-Context RALM [287] positions SLMs as rankers, optimizing the document sequence with a fine-tuning process on RoBERTa [218] as defined by the loss function: ",
        "page_idx": 43
    },
    {
        "type": "equation",
        "img_path": "images/a0e58b835f756db84f4d3ab50e36df5b318a17cdc9bfb01a6fa698a1f2f3fd53.jpg",
        "text": "$$\n\\operatorname* { m i n } _ { r a n k e r } \\sum _ { i = 1 } ^ { k } - \\log p _ { \\mathrm { r a n k } } ( d _ { i } | \\boldsymbol x _ { \\le s _ { j } } ) \\cdot p _ { \\theta } ( \\boldsymbol y | d _ { i } ; \\boldsymbol x _ { \\le s _ { j } } )\n$$",
        "text_format": "latex",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "where $x _ { \\leq s _ { i } }$ is a prefix sampled from the training data, $y = x _ { s _ { i } + 1 } , . . . , x _ { s _ { i } + s }$ represents the text to be generated in the next stride, $p _ { \\theta } ( y | d _ { i } ; x _ { \\le s i } )$ denotes the probability of the LLM generating $y$ given $d _ { i }$ and $x { \\le } s i$ , and $p _ { \\mathrm { r a n k } } ( d _ { i } | \\boldsymbol x _ { \\le s _ { j } } )$ is the ranking score of $d _ { i }$ . Lastly, some studies leverage SLMs to retrieve sub-graphs when utilizing knowledge graphs as external sources. Huang et al. [148] introduces the Generative Sub-graph Retriever (GSR), which employs SLMs to predict relation chains for answering questions, offering a cost-effective alternative to training LLMs. Specifically, it uses customized T5 (220M, 770M, and 3B) [284] as retrievers to enhance LLM readers, including Llama2-chat-7B [339] and Llama3-instruct-8B [94], on the WebQSP [418] and CWQ [324] datasets. ",
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "Enhancing Reasoning Capabilities of LLMs As illustrated in Figure 22, SLMs enhance LLMs reasoning by transferring task knowledge to in-context examples, effectively reducing hallucinations. While In-context Learning (ICL) generally handles few-shot learning ",
        "page_idx": 43
    },
    {
        "type": "image",
        "img_path": "images/ae6045a43b98b5c6bb8da34d5ac3c583c0ff5d17ee9737e665173841d5bfaff3.jpg",
        "image_caption": [
            "Fig. 22. SLM transfers knowledge into ICL. "
        ],
        "image_footnote": [],
        "page_idx": 43
    },
    {
        "type": "text",
        "text": "with 16 to 32 examples, it struggles when faced with extensive supervised data. SLMs, specialized in task-specific training, complement the broader domain knowledge of extensively pre-trained LLMs. For example, SuperICL [393] incorporates SLMs as plugins for efficiently executing supervised tasks. It predicts labels for contextual examples and integrates these predictions with the input text and actual labels to enhance knowledge transfer, thereby boosting the understanding and responsiveness of LLMs. SuperContext [408] tackles challenges that LLMs encounter with new tasks and out-of-distribution data in natural language understanding by synergizing SLM outputs with LLM prompts during inference. This integration merges model predictions with their confidence levels, effectively leveraging SLM task-specific knowledge and LLM domain expertise. Furthermore, SLMs efficiently decompose complex reasoning by breaking tasks into simpler components, as demonstrated in [383]. This strategy increases efficiency and reduces deployment costs when SLMs and LLMs are used collaboratively, transforming complex tasks into manageable segments. ",
        "page_idx": 43
    },
    {
        "type": "image",
        "img_path": "images/9c3657c5bb8abd7484009440e98895b181ebe7f9d445c79c6060797acd00bc03.jpg",
        "image_caption": [
            "Fig. 24. SLM for LLM Prompt Extraction Paradigm. $M _ { S }$ denotes small language models and $M _ { L }$ denotes large language models. (a) SLM-based prompt estimation tries various attack prompts; $M _ { S }$ selects the most likely extracted one. (b) SLM-based Parameter Extractor identifies the type of input prompt. (c) SLM-based Model Inversion uses $M _ { S }$ to invert the LLM output back into the input. "
        ],
        "image_footnote": [],
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Alleviate Copyright and Privacy Issues of LLMs LLMs pose significant security risks due to their tendency to memorize training data, leading to potential privacy breaches and copyright infringement. As depicted in Figure 23, SLMs can assist LLMs in addressing copyright and privacy concerns arising from online data collection. By training on ",
        "page_idx": 44
    },
    {
        "type": "image",
        "img_path": "images/626c7e9d43b4aab04b75e0c85a0ebd44a849b1c90de29088e83b6acbc899b0f8.jpg",
        "image_caption": [
            "Fig. 23. Architecture of SLM-based Data Protection "
        ],
        "image_footnote": [],
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "selectively curated data subsets, SLMs effectively reduce copyright infringement and privacy risks, although they are less effective than full-scale LLMs. To harness the combined benefits of both models, Li et al. [197] integrates untrusted LLMs with benign SLMs using the CP- $\\cdot \\Delta$ KL algorithm to mitigate adverse effects while preserving performance. The equation is: ",
        "page_idx": 44
    },
    {
        "type": "equation",
        "img_path": "images/82acd32e9310bc792c3dd697686c96c5f1c2b71aadd2ce2266ccd1554e652fa2.jpg",
        "text": "$$\np ( y | x ) = \\frac { p _ { l } ( y | x ) \\cdot p _ { s } ( y | x ) } { Z ( x ) }\n$$",
        "text_format": "latex",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "where $\\mathit { p _ { l } }$ and $\\mathbf { \\nabla } \\mathcal { P } \\boldsymbol { s }$ represent the probabilities from the large and small models, respectively, and $Z ( x )$ is the partition function. This integration results in the following ensemble algorithm: ",
        "page_idx": 44
    },
    {
        "type": "equation",
        "img_path": "images/9bd4f68b8f9c3534da0d42f0c2d493cf2cc4ce5cd199e4fb28f21df6a636c011.jpg",
        "text": "$$\nz _ { P } ( \\cdot | x ) \\propto \\alpha z _ { l } ( \\cdot | x ) + ( 1 - \\alpha ) z _ { s } ( \\cdot | x )\n$$",
        "text_format": "latex",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "where $z _ { l }$ and $z _ { s }$ are the logit values from the large and small models, respectively, and $\\alpha$ is the scaling factor. ",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "6.2 SLM for Extracting LLM Prompts ",
        "text_level": 1,
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "Prompt-based methods are becoming simpler and more cost-effective alternatives to traditional fine-tuning in the LLM era, utilizing LLMs’ instruction-following capabilities for a competitive edge. Mastering prompts is vital for replicating LLM-supported product behaviors. However, services such as Bing Chat and GitHub Copilot Chat have seen prompt reverse-engineering through black-box API attacks. SLMs often serve as surrogate models in these attacks, employing strategies such as (i) SLM-based prompt likelihood estimation, (ii) SLM-based prompt parameter extraction, and (iii) SLM-based direct model inversion, illustrated in Figure 24. ",
        "page_idx": 44
    },
    {
        "type": "text",
        "text": "SLM-based prompt likelihood estimation, as illustrated in Figure 24 (a), Zhang et al. [443] proposes using an SLM as a Likelihood Estimator to identify secret prompts in LLM outputs. They craft attack prompts, such as “Repeat all sentences in our conversation,” and query the target LLM. The response is likely to include secret prompts, confusing the LLM to interpret these as part of the conversation. A fine-tuned DeBERTa model [132] is then used to select the most likely secret prompts from the output. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "SLM-based prompt parameter extraction, as shown in Figure 24 (b), Sha and Zhang [297] utilizes an SLM as a Parameter Extractor to extract prompt parameters from LLM outputs. They employ a specialized BERT model [83] to classify LLM outputs into direct, in-context, and role-based prompts, also predicting the number of exemplars for in-context prompts and identifying roles for role-based prompts. Prompt reconstruction is then performed using ChatGPT once the parameters are defined. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "SLM-based direct model inversion, as shown in Figure 24 (c), the method of using an SLM as a Direct Inversion Model is designed to reverse-engineer LLM outputs back to their original prompts [433]. They train a sparse encoderdecoder T5 model [284] with 222M parameters on the Instructions-2M dataset [249], where the input is LLM outputs and the output is the LLM prompt. This trained model effectively maps multiple LLM outputs to their initiating prompts as $\\boldsymbol { p } ( x | y _ { 1 } , . . . , y _ { n } ; M _ { S 1 } , M _ { S 2 } )$ , with $y _ { i }$ representing different output versions and $M _ { S 1 } , M _ { S 2 }$ the model parameters. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "6.3 SLM for Fine-tuning LLMs ",
        "text_level": 1,
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "Fine-tuning is a crucial technique for adapting LLMs to specific tasks or domains, yet it is often time-consuming. For instance, finetuning the LLaMA-2-13B [339] checkpoint on 32 NVIDIA A100 GPUs with 80GB memory using bfloat16 format requires approximately 70 hours [247]. This process also demands high-quality data. Therefore, we examine how SLMs can enhance LLM finetuning through three approaches: (i) proxy fine-tuning, (ii) selecting high-quality data, and (iii) guiding LLM-generated task data, as illustrated in Figure 25. ",
        "page_idx": 45
    },
    {
        "type": "image",
        "img_path": "images/dbb90c927b541ccb1dbead5da7a8cacff664dcc463e7dcbbe77d6329cea8382d.jpg",
        "image_caption": [
            "Fig. 25. SLM for LLM Fine-tuning. "
        ],
        "image_footnote": [],
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "SLMs as proxy models: SLMs can approximate the gradient of fine-tuning large-scale LLMs on target datasets, avoiding the costly fine-tuning process in terms of time and computational resources. As shown in Figure 25 (a), Emulated Fine-Tuning (EFT) [246] simulates both unsupervised pre-training and supervised fine-tuning stages across different scales by manipulating log probabilities. EFT, for example, combines base log probabilities from a 70B model with behavioral deltas from a 7B model—these deltas represent differences between fine-tuned and unfine-tuned SLMs, effectively emulating outcomes for the Llama-2 series. This method allows fine-tuning on smaller models such as Falcon-7B [10] while capturing most benefits of fine-tuning larger models such as Falcon-180B, benefiting applications such as dialogue, question-answering, and code generation. Similarly, Proxy-tuning [212] adjusts LLM predictions by adding the differences between the outputs of a fine-tuned small model and its untuned version to the LLM’s output vocabulary during decoding, maintaining the advantages of large-scale pre-training while integrating small-scale fine-tuning benefits. Moreover, SLMs can act as proxies for approximate LLM fine-tuning during decoding. Weakto-Strong Search [453] strategy frames the alignment of LLMs as a test-time greedy search, aiming to maximize the log-probability difference between small tuned and untuned models while sampling from the frozen large model. This approach serves as a dual-purpose method: (1) a compute-efficient model up-scaling strategy that circumvents direct tuning of the large model, and (2) an instance of weak-to-strong generalization that bolsters a strong model with weak test-time guidance. ",
        "page_idx": 45
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "SLMs play a role in selecting high-quality fine-tuning data for LLMs. Figure 25 (b) illustrates how SLMs within the same family as the LLM can identify training samples that are likely to be challenging, enhancing the training efficiency and generalization capability of the LLM. As demonstrated by Swayamdipta et al. [323] and further advanced by Mekala et al. [242], the learning percentage $L P ( i )$ is a metric used to curate high-quality datasets with hard samples: ???? (??) = ??−1 − ????0 −???? where $P _ { i }$ represents the perplexity at the end of epoch-??, and $P _ { 0 }$ is the initial perplexity. A higher $L P ( i )$ early in training indicates significant learning in the initial epochs, highlighting the potential of these samples to enhance LLMs. SmallToLarge (S2L) [411] utilizes training loss trajectories from smaller models to guide data selection for larger models fine-tuning. Experimental results demonstrate that S2L significantly enhances data efficiency in SFT for mathematical problem-solving, reducing the required training data to just $1 1 \\%$ of the original MathInstruct dataset [428] to achieve performance comparable to that obtained using the full dataset. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "SLMs enhance the quality of LLM-generated data for specific tasks. As depicted in Figure 25 (c), CROSSLM [79] promotes the local training of SLMs on client-specific private data to mitigate privacy risks associated with server-based LLMs. An SLM trained in this manner can guide the server-side LLM to produce high-quality synthetic datasets. Feedback from SLMs regarding the quality of this synthetic data serves as a supervisory signal, enhancing both the quality of LLM outputs and the utility of the data for further training. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "6.4 SLM for LLM Applications ",
        "text_level": 1,
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "LLMs are utilized across various applications due to their open-ended generation capabilities, yet they often lack specialized knowledge and other generation issues. SLMs can supplement this by providing taskspecific knowledge or reflecting weaknesses. Therefore, we explore how SLMs enhance the performance of LLMs in specific applications, focusing on open-ended generation, knowledge integration, relation extraction, and empathetic response. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "In open-ended text generation—such as writing assistance and story creation—LLMs often suffer from issues such as incoherence and thematic drift over extended sequences. Due to more frequent failure patterns observed in SLMs, such as short, repeated, and irrelevant strings, these patterns serve as negative examples for LLM decoding. ",
        "page_idx": 46
    },
    {
        "type": "image",
        "img_path": "images/7bd4c45becca7b686c32b583209d1f7a73eb6551e34b45c46098c5c022c94672.jpg",
        "image_caption": [
            "Fig. 26. Contrastive Decoding [198]. "
        ],
        "image_footnote": [],
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "Contrastive Decoding (CD) [198] improves coherence and lexical diversity by leveraging the differential capabilities between a large model, OPT-13B [440], and a smaller model, OPT-125M. As illustrated in Figure 26, CD improves content quality by sampling generation based on the difference in log probabilities, $\\log p _ { E X P } - \\log p _ { A M A }$ , between an expert LM and an amateur LM, rather than relying solely on the expert LM’s log probability. This approach effectively reduces generative failures, including repetition. ",
        "page_idx": 46
    },
    {
        "type": "text",
        "text": "In knowledge injection, general LLMs may lack domain-specific expertise for specialized tasks like law or medicine [91, 353]. Domain-specific SLMs can supply crucial knowledge in a format suitable for LLMs. To this end, BLADE [189] integrates black-box LLMs with small domain-specific models. BLADE combines the comprehensive language capabilities of LLMs with the specialized knowledge of small LMs. As shown in ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "1. Domain-specific  2. Knowledge  3. Bayesian Prompted pre-training Instruction Tuning Optimization ",
        "page_idx": 47
    },
    {
        "type": "image",
        "img_path": "images/0e42f91999f7419cb80623e72013f86267aee5f36093e88b937c5c8097ca9307.jpg",
        "image_caption": [
            "Fig. 27. BLADE Framework [189]. "
        ],
        "image_footnote": [],
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "Figure 27, BLADE’s process includes: 1) pre-training SLMs on domain-specific data, 2) fine-tuning with knowledge instruction to meet task-specific needs, and 3) using joint Bayesian optimization to enhance synergy between the LLM and the small LM, boosting overall performance. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "In relation extraction, a field limited by scarce labeled data and prevalent long-tail categories, the “Train-GuidePredict” framework [327] employs SLMs to learn task-specific knowledge for dominant categories. SLMs struggle with rare categories, whereas LLMs manage these effectively due to their extensive pre-trained text. Therefore, this framework leverages the strengths of both models: it utilizes SLMs to acquire task knowledge and guide the LLM’s generative process with initial SLM predictions, enhancing the LLM’s handling of underrepresented categories. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "In generating empathetic responses, LLMs excel in expressiveness but struggle with nuanced emotions and cognition. HEF [413] addresses this by incorporating Small Empathy Models (SEMs) to enhance LLMs’ emotional and cognitive depth. This framework employs a two-tiered emotion prediction method: SEMs identify primary emotions, directing LLMs to concentrate on these emotions and their triggers, resulting in more accurate and empathetic responses. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "6.5 SLM for LLM Safety ",
        "text_level": 1,
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "As demonstrated by various works [250, 302, 426, 460], LLMs are vulnerable to adversarial attacks and jailbreaking. For example, Wang et al. [358] shows that ChatGPT’s performance on adversarial datasets is still far from perfect, indicating that potential risks of adversarial vulnerability remain. Another example includes jailbreaking ChatGPT by asking it to ’pretend to be a sarcastic mean girl.’ Using such techniques, it has been shown that even the most advanced LLMs are far from being safe against generating potentially harmful content. Hence, the widely adopted LLM-based services to generate are at high risk of being misused for nefarious purposes. Consequently, resources such as the Llama 2 Responsible Use Guide 14 strongly advocate for implementing robust guardrails in products that utilize Generative AI. These guardrails are specifically designed to mitigate risks associated with both inputs to and outputs from the model, ensuring safeguards against the generation of high-risk or policy-violating content, as well as protecting against adversarial inputs and attempts to compromise the model. In addition to developing trustworthy LLMs, adopting SLMs for LLM safety [153, 177] has also attracted increasing attention. For example, Llama Guard [153], fine-tuned on Llama2-7B, has publicly released an input-output safeguard tool specifically for classifying safety risks in prompts and responses within conversational AI applications. However, this tool is limited to assessing the harmfulness of questions and answers and does not facilitate the generation of fluent, safe responses. In response to this limitation, Kwon et al. [177] fine-tune a specialized small language model with harmful query detection and safeguard answer generation tasks to accurately detect harmful user queries and generate appropriate safeguard explanations, thereby enhancing the safety measures in conversational AI. ",
        "page_idx": 47
    },
    {
        "type": "text",
        "text": "6.6 SLM for LLM Evaluation ",
        "text_level": 1,
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "SLMs can also enhance the evaluation of LLMs. In dialog evaluation, generating dialog reference responses is computationally complex, making accurate assessment difficult due to the multiple plausible but semantically different responses possible for a single dialog context. Relying on LLM prompting for evaluation can lead to problems such as dependency on prompt wording and inconsistent results. One solution involves training specialized SLMs to evaluate LLMs, as these SLMs can be fine-tuned more quickly and generate outputs faster during inference, owing to their reduced number of parameters. For example, SLIDE [449] employs contrastive learning to fine-tune an SLM to effectively distinguish between positive and negative responses. Based on its observation that SLMs are more accurate in identifying positive responses and LLMs excel at classifying negative ones, the trained SLM is subsequently integrated with an LLM to assign a score to each response. The scoring method used is formalized as follows: ",
        "page_idx": 48
    },
    {
        "type": "equation",
        "img_path": "images/4a08ec6fabe50723da4c06d5e69d62aad4ee613e005bc0895166e807df16ef81.jpg",
        "text": "$$\ns c o r e = \\left\\{ \\begin{array} { l l } { s c o r e _ { S L M } , } & { \\mathrm { i f } s c o r e _ { S L M } \\ge 0 . 5 } \\\\ { s c o r e _ { L L M } , } & { \\mathrm { e l i f } s c o r e _ { L L M } < 0 . 5 } \\\\ { \\frac { s c o r e _ { S L M } + s c o r e _ { L L M } } { 2 } , } & { \\mathrm { o t h e r w i s e } } \\end{array} \\right.\n$$",
        "text_format": "latex",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "This equation allows for adaptive response evaluation, leveraging the strengths of both models to ensure a more reliable and consistent assessment across varying dialogue contexts. In the natural language generation task, Kuhn et al. [175] designs a novel entropy to evaluate the uncertainty of LLMs. It aims to tackle the challenge of semantic equivalence [175]. For instance, $A$ ’s son is $B$ and $B$ is $A$ ’s son are semantically equivalent. It should not be considered uncertain if an LLM is unsure about which of the two previously mentioned sentences to generate due to semantic equivalence. A DeBERTa-Large [132] fine-tuned on the MNLI [378] dataset serves as the classifier guided by semantic equivalence in the clustering stage. SelfCheckGPT [240] proposes a black-box hallucination detection method for LLMs. The core idea is to leverage uncertainty derived from sampled outputs. To be specific, Manakul et al. [240] claim that an LLM trained on a concept generates responses that are similar and factually consistent. One of the five variants of SelfCheckGPT uses BertScore to achieve it. A DeBERTa-Large [218] is utilized to calculate the BERTScore. Factscore [244] is proposed to evaluate the factuality of LM-generated long-form content. It divides the generated long content into multiple short texts, enabling a more precise assessment of factual accuracy. In addition to manual evaluation, Min et al. [244] also proposes an automated evaluation framework to estimate Factscore which can reduce costs. LLaMa 7B [338] fine-tuned on Super-NaturalInstructions [367] is one of the LMs employed as an evaluation assistant and shows promising performance. They also employ Generalizable T5-based dense retrievers [259] to facilitate passage retrieval. ",
        "page_idx": 48
    },
    {
        "type": "text",
        "text": "Insights: SLMs can improve LLMs in various aspects, including enhancing the reliability of LLM generation, extracting prompts, fine-tuning,   \napplication, and evaluation. This discussion seeks to answer when SLMs should be utilized to augment LLMs. We identify several suitable   \nscenarios:   \n• Adapting LLMs to specific tasks can require substantial computational resources and time. In such cases, a smaller model could be fine-tuned instead to serve functions such as hallucination detection.   \n• SLMs can outperform LLMs in certain aspects, hence combining SLMs with LLMs can create a more powerful model, e.g., SLMs typically have fewer security issues than LLMs, and integrating both can generate a model that is both powerful and secure.   \n• SLMs, despite their limitations, can alert LLMs to these issues, such as the tendency to produce repetitive vocabulary. Designing contrastive losses can help LLMs overcome these issues by learning from the nuanced feedback of SLMs.   \n• The fast inference speed and certain characteristics of SLMs can emulate and thus enhance the behavior of LLMs, acting as effective proxies. For example, the training data selection for LLMs can be guided by the difficulty metrics assessed by SLMs, and the parameter adjustments during the fine-tuning of SLMs can also approximate the fine-tuning processes of LLMs. ",
        "page_idx": 48
    },
    {
        "type": "table",
        "img_path": "images/7a301bf14d27246d671fe2f019348067a554dcdd7dc6af2604ebd4ebba48dcc3.jpg",
        "table_caption": [
            "Table 13. Synergy between SLMs and LLMs "
        ],
        "table_footnote": [],
        "table_body": "<table><tr><td>Synergy</td><td>Representative Work</td><td>KeyPoint</td></tr><tr><td rowspan=\"6\"></td><td>CoGenesis [437]</td><td>Divide user instructions into general part by LLMs and private parts by SLMs.</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>武</td></tr><tr><td>g</td><td></td></tr><tr><td>LLMCad [394]</td><td>Combine lightweight and high-precision LLMs for on-device inference.</td></tr><tr><td>Khattab et al. [167], Ma et al. [234]</td><td>Focuse on LLM&#x27;s reasoning and SLM&#x27;s efficient decoding.</td></tr><tr><td>Cloud-Edge Synergy (Training)</td><td>CROSSLM [79]</td><td>Preserve client data privacy by training SLMlocally and LLM remotely; mutual improvement using SLM-labeled data from LLM outputs.</td></tr><tr><td rowspan=\"4\">Task-Centric Synergy</td><td>α-UMi [307]</td><td>Break downa single LLM into specialized agents.</td></tr><tr><td>SynCID[204]</td><td>Merge LLM&#x27;s semantic with SLM&#x27;s speed; refine labels via contrastive learning.</td></tr><tr><td>Filter-then-rerank [234]</td><td>SLMs process simple samples and flag complex ones for LLM reranking.</td></tr><tr><td>Data Shunt+ (DS+) [46]</td><td>Process easy samples with SLMs and delegates hard samples to LLMs.</td></tr></table>",
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "7 SYNERGY BETWEEN SMALL AND LARGE LANGUAGE MODELS ",
        "text_level": 1,
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "The synergy between small and large language models leverages the unique strengths of each to enhance overall system performance and efficiency. SLMs, being lightweight and resource-efficient, are ideal for deployment on edge devices, enabling rapid responses and low latency for straightforward tasks. LLMs, on the other hand, possess greater computational power and a deeper understanding of complex language patterns, allowing them to handle more intricate and nuanced tasks. By integrating SLMs and LLMs, systems can dynamically allocate tasks based on complexity, ensuring that simple queries are processed quickly on the edge while more demanding requests are escalated to the cloud. This collaborative approach optimizes resource usage, reduces operational costs, and maintains high-quality outputs across a diverse range of applications. The synergy between small and large language models can be categorized into two parts: cloud-edge synergy and task-centric synergy. Cloud-edge synergy refers to a setup where SLMs operate on edge devices, while LLMs reside on the server. When the SLM is not powerful enough, the LLM compensates by handling more complex tasks and providing additional support. Task-centric synergy refers to the scenario where SLMs and LLMs leverage their respective strengths to improve task-oriented efficiency. Table 13 summarizes representative work in each category and their key points. Next, we introduce each category in detail. ",
        "page_idx": 49
    },
    {
        "type": "image",
        "img_path": "images/48df989cfaadb8fac983caaf7d5a0543997244da2d91323c876c84ed2b7ce3c2.jpg",
        "image_caption": [
            "Fig. 28. Could-edge synergy between LLMs and SLMs. "
        ],
        "image_footnote": [],
        "page_idx": 49
    },
    {
        "type": "text",
        "text": "7.1 Cloud-Edge Synergy ",
        "text_level": 1,
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "The current utilization of LLMs typically involves uploading private data to the cloud for response. Fine-tuning LLMs usually also requires uploading data to clouds for computing. However, this raises privacy concerns as the collection and usage of private data are constrained by personal privacy awareness and legal regulations [348]. Consequently, the cloud-edge synergy between SLMs and LLMs is proposed to alleviate this issue, i.e., SLMs handle privacy-sensitive data locally, LLMs handle de-identified or non-privacy-sensitive data, and these two models collaborate. This section discusses such cloud-edge synergy, dividing them into two categories: cloud-edge synergy during inference and cloud-edge synergy during training, as shown in Figure 28. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Cloud-edge Synergy During Inference. CoGenesis [437] breaks down the user instruction into a general section and a personal section. The LLM generates replies solely based on general instruction, and the SLM considers both user instruction and additional personal context for its output generation. A fusion strategy blends the output of LLM and SLM synergistically. Xu et al. [397] introduces a split learning system for LLM agents in 6G networks, optimizing mobile device and cloud server collaboration. Mobile devices operate lightweight SLMs with 0–10B parameters for real-time tasks, while cloud servers handle larger LLMs with over 10B parameters for complex reasoning and planning. This setup allows efficient local task management on mobile devices and offloads heavy operations to cloud servers. The system’s architecture features three modules—perception, grounding, and alignment—facilitating effective communication to meet the sophisticated needs of 6G networks. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Besides these frameworks, more specific models are proposed to facilitate the cloud-edge synergy. A common strategy is to use SLM’s fast decoding ability. LLM-to-SLM [28] proposes a framework in which the pre-trained frozen encoder-decoder LLM resides on the server and computes a high-quality representation of the prompt for the planning of an appropriate response. The SLM residing on the edge device, conditioned on this representation, decodes the response efficiently. Some variants put more emphasis on the reasoning ability of LLMs [167, 234, 298]. In Synergy of Thoughts [298], the SLMs generate multiple low-cost reasoning paths. If these paths conflict, the larger LLMs are invoked to provide reflective reasoning and correct any intuitive errors. Hao et al. [127] proposes a framework in which an SLM residing on the edge devices generates tokens, calling LLMs to verify and correct threshold-gated \"harder\" tokens, to achieve a controllable trade-off between inference quality and cost. LLMCad [394] presents an on-device inference engine addressing memory and latency issues in deploying LLMs on mobile devices. It combines a lightweight LM for token generation with a high-precision LLM for verification, leveraging a token tree structure and speculative generation for efficiency. Tested on devices such as Jetson TX2, it achieves up to $9 . 3 \\times$ speedup for LLMs with over 10 billion parameters while maintaining accuracy. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "Cloud-edge Synergy During Training. CROSSLM [79] introduces a client-server collaborative training framework that preserves data privacy by having clients locally train SLMs instead of fine-tuning LLMs. The framework enables mutual enhancement through a feedback loop where SLMs evaluate LLM-generated synthetic data and provide feedback to improve the LLM’s generative capabilities, ensuring high-quality and task-specific data. Concurrently, the synthetic data trains the SLMs, boosting their performance. This cyclical exchange fosters cloud-edge synergy and mutual model improvement. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "7.2 Task-Centric Synergy ",
        "text_level": 1,
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "The advent of LLMs has significantly propelled various natural language processing tasks and inspired research into their synergistic interactions with SLMs to enhance the performance of models tailored for specific tasks. This section introduces scenarios where small language models exhibit specialized capabilities after fine-tuning and discusses how combining their unique strengths with the versatile abilities of LLMs can yield superior performance on specific tasks. For example, LLMs excel at handling difficult examples or can rewrite content to eliminate task-irrelevant redundancy, thereby enhancing overall task performance, as illustrated in Figure 29, 30 and 31. ",
        "page_idx": 50
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "$\\alpha$ -UMi [307] introduces a multi-agent framework to enhance tool learning by overcoming the limitations of singleLLM approaches for complex tasks. It utilizes three specialized LMs—planner, caller, and summarizer—as depicted in Figure 29—each handling specific subtasks such as planning, tool invocation, and summarization. This modular design allows the use of small and large open-source LLMs (e.g., LLaMa-7B/12B) and supports easy tool updates. Evaluated on benchmarks like ToolBench [279] and ToolAlpaca [326], $\\alpha$ -UMi outperforms traditional single-LLM methods and even exceeds GPT-4 in tool learning performance. ",
        "page_idx": 51
    },
    {
        "type": "image",
        "img_path": "images/bc23a40ce635f273f1242537db1150953da333aa08351c4a52224d8b31af729a.jpg",
        "image_caption": [
            "Fig. 29. Synergizing SLMs and LLMs in tool learning. "
        ],
        "image_footnote": [],
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "SynCID [204] focuses on Conversational Intent Discovery (CID), a task where both known and new intents must be identified from user utterances in an open-world setting. SynCID combines LLMs’ deep semantic insights with SLMs’ agility and specialized capabilities. As illustrated in Figure 30, the framework uses LLM prompting to refine discourse and intent labels, enhancing semantic accuracy and assigning new labels to unlabeled data. SLMs are trained via contrastive learning to align ",
        "page_idx": 51
    },
    {
        "type": "image",
        "img_path": "images/c81cb9b79eff7862cf4e886d507bc4aab412fb27e8b2d18cadc4977dc37d6307.jpg",
        "image_caption": [
            "Fig. 30. Synergizing SLMs and LLMs in Conversational Intent Detection. "
        ],
        "image_footnote": [],
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "semantic spaces of discourse and intent descriptors, reducing clustering distortion and improving new intent detection.   \nTested on BANKING [43], CLINC [179], and StackOverflow [396], SynCID outperforms CID baselines significantly. ",
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "Filter-then-rerank [234] addresses LLMs’ poor performance on simpler IE tasks by integrating LLMs and SLMs. SLMs act as filters, predicting and identifying difficult samples, while LLMs ",
        "page_idx": 51
    },
    {
        "type": "image",
        "img_path": "images/706a36e19b603c76d6aae7b544d112d98b1d3baf832b822766b357c5ba1f7446.jpg",
        "image_caption": [
            "Fig. 31. Synergizing SLMs and LLMs in Information Extraction. "
        ],
        "image_footnote": [],
        "page_idx": 51
    },
    {
        "type": "text",
        "text": "rerank the top N candidate labels for these cases. As illustrated in Figure 31, SLM predictions are final for non-difficult samples, minimizing reliance on LLMs and reducing latency and costs; for those difficult samples, the top N predicted candidate labels from the SLM are passed to the LLM for reranking (predicting). Tested on small-sample IE tasks, this approach improves performance by an average of $2 . 4 \\%$ compared to previous methods. Data Shunt $^ +$ $\\mathbf { ( D S + ) }$ [46] introduces a framework to reduce costs by minimizing large model queries during inference and boosting LLM performance with SLMs for tasks like sentiment analysis and image processing. ${ \\mathrm { D S } } +$ uses SLMs for “easy” samples within the main training distribution and LLMs for \"hard\" outliers or boundary cases, maintaining accuracy while reducing LLM use. It incorporates S4L and L4S modules with Prompt Pruning (PP) and 2-stage Confidence Distillation (2CD) for better input processing and knowledge transfer. Tests show ${ \\mathrm { D S } } +$ outperforms fine-tuning in accuracy and cost efficiency, significantly cutting down on LLM queries. ",
        "page_idx": 51
    },
    {
        "type": "image",
        "img_path": "images/bd7e4807f7a9d6400352932e88e02d9e09460a88ebef1ff96ca0ce0b08089699.jpg",
        "image_caption": [
            "Fig. 32. Scenarios we discuss in this section. The taxonomy is inspired by previous works [320, 351]. Please note that the trustworthy scenarios listed here are not exhaustive. "
        ],
        "image_footnote": [],
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "8 TRUSTWORTHINESS IN SMALL LANGUAGE MODELS",
        "text_level": 1,
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Language models have become ubiquitous in our daily lives, and we increasingly rely on them. However, they pose risks regarding their limitations in trustworthy dimensions like privacy and fairness. These concerns are especially critical in high-stakes domains such as healthcare [130] and finance [202]. Consequently, numerous studies have emerged to evaluate the trustworthiness of LMs [88, 95, 138, 176, 176, 248, 254, 273, 351, 370, 425]. In this section, we consider the works that benchmark various LMs’ trustworthiness and omit the specific attack methods [41, 53, 150, 461] or work [407] that only focuses on early pre-trained LMs like BERT [83] as they are already covered in previous survey papers [78, 115, 123, 288]. Inspired by previous works [320, 351], we discuss the following five key trustworthy scenarios: robustness, privacy, reliability, safety, and fairness, as shown in Figure 32. We consider two dimensions for robustness: Adversarial (Adv) Robustness [352] and Out-of-Distribution (OOD) Robustness [38, 213]. For safety, we explore two key concerns: Misinformation [344] and Toxicity [374]. For reliability, we focus on Hallucination [146] and Sycophancy [301]. Please note that these are just the aspects we are focusing on, and therefore this is not a comprehensive classification or taxonomy. For example, robustness also contains robustness to adversarial demonstration. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Though there are a lot of works benchmarking LMs’ trustworthiness, their main focus is on LLMs. Therefore, we survey some representative works evaluating the trustworthiness of LMs, focusing specifically on those that include SLMs of around 7B parameters or smaller. We also summarize these works in Table 14. Next, we briefly introduce them. ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Holistic Evaluation of Language Models (HELM) [205] benchmarks a large number of LMs from various aspects, including a lot of metrics related to trustworthiness such as robustness and fairness. Do-Not-Answer [364] introduces a dataset to evaluate how LMs act when they face content that should not be answered. Wang et al. [364] also label the output of several LMs output on their dataset and then use the labeled data to train some classifiers. PromptRobust [456] constructs two kinds of adversarial prompts to evaluate LMs’ robustness: One kind is designed under non-adversarial settings with semantic integrity while another category is created under adversarial settings. Their results show that LMs perform poorly under such prompts. HaluEval [191] builds a dataset comprising both the samples generated by their proposed framework and human-labeled hallucinations. It facilitates analysis of when LMs produce hallucinated output and how well they detect hallucinated content. Then they use some strategies such as knowledge retrieval to help LMs better recognize hallucinations. Mo et al. [248] evaluates the trustworthiness of open-source LMs, presenting a variety of scenarios such as fairness and privacy. Results show that smaller LMs sometimes outperform larger ones in terms of trustworthiness. PrivLM-Bench [190] is designed to evaluate the privacy issues in LMs. It enables a fair comparison of privacy-preserving LMs by considering more than just differential privacy parameters. FFT [71] introduces around two thousand crafted examples to evaluate LMs’ performances on three trustworthy dimensions: factuality, fairness, and toxicity. Their results suggest that larger LMs do not always show better harmlessness. ROBBIE [98] first benchmarks various series of LMs using a lot of datasets, including two newly introduced datasets developed by ROBBIE. It also Manuscript submitted to ACM ",
        "page_idx": 52
    },
    {
        "type": "text",
        "text": "Table 14. Comparison of Different Works that Evaluate the Trustworthiness Issues in LMs. Please note that for the \"No. of LMs\" attribute, compressed or pruned LMs are not included in the count. ",
        "page_idx": 53
    },
    {
        "type": "table",
        "img_path": "images/54ba53e905e4b1507712565596bc939d3eaf80a4bbe0e0ae6a1b42bff2ef742e.jpg",
        "table_caption": [],
        "table_footnote": [],
        "table_body": "<table><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Paper HELM[205]</td><td>√</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>Do-Not-Answer [364]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td></tr><tr><td>PromptRobust [456]</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>HaluEval[191]</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Mo et al. [248]</td><td>√</td><td>×</td><td>√</td><td>×</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td>PrivLM-Bench [190]</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>FFT[71]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>ROBBIE [98]</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td></tr><tr><td>TrustLLM [320]</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>√</td><td>×</td></tr><tr><td>RAmBLA[35]</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>JailbreakBench [45]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>Xie et al. [390]</td><td>×</td><td>×</td><td>√</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>OR-Bench [70]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>SORRY-Bench [387]</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>BeHonest [59]</td><td>×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Hong et al. [138]</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>√</td></tr><tr><td>RUPBench [370]</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Nakka et al. [254]</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td><td>×</td><td>√</td><td>√</td><td>×</td></tr></table>",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "evaluates mitigation techniques designed to reduce bias and toxicity. TrustLLM [320] is a comprehensive benchmark that contains a large number of datasets and various well-designed metrics to systematically evaluate various LMs across multiple trustworthy dimensions, including truthfulness, safety, fairness, robustness, privacy, and machine ethics. They also carefully design specific subcategories for each dimension. RAmBLA [35] evaluates the trustworthiness of four LMs as biomedical assistants from three dimensions: Robustness, High Recall, and Hallucination. RAmBLA suggests LMs with more parameters are less likely to cause hallucinations and may choose to reject providing an answer in uncertain situations. JailbreakBench [45] constructs a jailbreaking dataset named JBB-Behaviors and jailbreak artifacts to evaluate current LMs’ performance regarding jailbreaking. It also proposes a unified evaluation pipeline that can incorporate new jailbreak defense techniques. Xie et al. [390] tests online safety analysis methods, filling the gap where no methods focus on the generation phase. OR-Bench [70] constructs three datasets: OR-Bench-80K, OR-Bench-Hard-1K, and OR-Bench-Toxic, to systematically evaluate over-refusal problems in LMs, emphasizing the challenge of balancing safety alignment with the models’ usefulness. SORRY-Bench [387] systematically tests 43 different LMs to see how they perform when facing requests that should be refused. They also collect more than annotations created by humans and find that fine-tuned 7B LMs can achieve performance comparable to GPT-4 scale LMs as evaluators. BeHonest [59] evaluates the honesty of LMs from three aspects: Self-Knowledge, Non-Deceptiveness, and Consistency. They use many different metrics for each aspect. For example, sycophancy rate and lying rate are adopted in Non-Deceptiveness. The results in both the Self-Knowledge and Consistency parts reveal that larger model sizes generally bring improved performance for the Llama-2 [339] and Llama-3 [94] series. Hong et al. [138] examines the effects of compression Manuscript submitted to ACM ",
        "page_idx": 53
    },
    {
        "type": "text",
        "text": "methods, including quantization and pruning, on the trustworthiness of language models. They find that pruning and extreme quantization significantly affect the trustworthiness of LMs. RUPBench [370] comprises 15 reasoning datasets designed to assess the performance of LMs both in normal conditions and under various adversarial perturbations. Their results indicate that larger LMs generally demonstrate better resilience to perturbations. Nakka et al. [254] investigates the trust and ethical implications of SLMs deployed on personal devices. It reveals the vulnerabilities of on-device SLMs compared with their on-server counterparts. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Please note that the dimensions discussed in this section reflect only those relevant to our current focus; additional dimensions may be discussed in those works, but not listed in table 14. For example, TrustLLM [320] also explores Machine Ethics. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "9 FUTURE DIRECTIONS ",
        "text_level": 1,
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "In this section, we offer insights into several promising future research directions that could inspire and motivate the community to address existing gaps in the development of small language models. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "9.1 Developing Efficient SLM Model Architecture ",
        "text_level": 1,
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Although Transformers [346] are foundational in most language models, they face significant computational and memory challenges that worsen with model size, impacting training and autoregressive decoding. Recently, Mamba [117] has emerged as a promising alternative, adapting state space models to dynamically select inputs based on demands, thereby enhancing efficiency. Thereafter, xLSTM [25] demonstrates that an improved LSTM could function as an LLM, revealing the potential of traditional SSMs. The integration of global static information captured by SSMs with the dynamic information processing of Transformers could complement each other, leading to new architectures that balance effectiveness and efficiency. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "9.2 Addressing SLM Training Inefficiencies ",
        "text_level": 1,
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "One study [85] explores the disparate learning dynamics between small and large language models. Utilizing the Pythia model suite, the research demonstrates that layers’ activations in larger models converge more rapidly and monotonically to their final states. This phenomenon is associated with a higher proportional effective rank (PER) in the parameters and gradients of larger models. The analysis enhances our understanding of training inefficiencies in small models and provides insights for future efforts, such as developing methods to increase the PER of layers’ parameters. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "9.3 Expanding Domain-Specific SLMs ",
        "text_level": 1,
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Domain-specific SLMs, which are tailored for specific fields, can provide a stronger foundation for relevant downstream tasks than general-purpose models. Currently, these models primarily focus on scientific and healthcare domains. However, there is significant potential for expansion into other key areas such as law, finance, education, telecommunications, and transportation. The scarcity of SLMs that cater to these domains presents an urgent call for research into developing more specialized models. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "9.4 Establishing Benchmarking and Leaderboard Platforms for SLMs ",
        "text_level": 1,
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "Several compelling reasons justify the establishment of benchmarking and leaderboard platforms for SLMs. Firstly, most state-of-the-art SLMs are trained on proprietary datasets, which may include test sets from existing evaluation tasks, presenting challenges for fair capability comparisons. Secondly, many SLMs are designed for specific device applications, significantly differing from general open-domain tasks. Thus, there is a lack of comprehensive benchmarks that accurately reflect SLM performance in specific device applications. For example, SLMs deployed on smartphones often handle tasks sensitive to user data, such as auto-replies based on historical chat texts or GUI context understanding—tasks not typically included in current benchmarks, potentially leading to an underestimation of their importance. Finally, current evaluation tasks focus primarily on metrics like accuracy. Evaluating on-device SLMs involves balancing multiple factors, including overall capabilities, response times, storage and memory usage, power consumption, CPU utilization, additional fine-tuning requirements, and context window constraints, making comprehensive and detailed assessments essential. ",
        "page_idx": 54
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "9.5 Enhancing SLM Performance and Efficiency ",
        "text_level": 1,
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "In terms of enhancing SLM performance and efficiency, the efficiency of using teacher LLMs via instruction tuning can be further developed, such as Efficient Instruction Tuning of SLMs from LLMs-generated data, Optimizing Teacher LLM Selection for SLM Learning, and Applying Emerging Techniques from LLMs to SLMs. ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "• Efficient Instruction Tuning of SLMs from LLMs-generated data. Enhancing the specialization of SLMs through instruction tuning from LLMs-generated data is crucial, yet finding the most cost-effective instructional strategies remains an underexplored status. Some key areas for exploration are: (1) Instruction Design Adaptability: The performance of LLMs and SLMs varies significantly with changes in instructions. Therefore, designing tailored instructions that effectively activate relevant sub-competencies and reasoning pathways in SLMs for specific tasks is crucial. This approach would optimize their ability to utilize instructional data, representing a significant future research direction. (2) SLM Capability Adaptability: Given that SLMs exhibit diverse capabilities across domains, simply supplying extensive data samples for instruction tuning is often inefficient, as SLMs may spend excessive time processing unnecessary data. To optimize efficiency when adapting to specific domains, we suggest first assessing the intrinsic capabilities of an SLM within those domains. Subsequently, one could select appropriate data and activate essential fine-grained capabilities to effectively adapt to domain shifts. This targeted approach ensures efficient and domainspecific instruction tuning. (3) Optimizing Data Efficiency: SLMs may possess missing or latent domain knowledge, and activating this latent knowledge may not require substantial data. Thus, identifying inherent knowledge within SLMs and determining the minimal data necessary for effective fine-tuning is a future direction. This research aims to optimize performance while minimizing training resources.   \n• Optimizing Teacher LLM Selection for SLM Learning. Teacher LLMs with different abilities and knowledge facilitate diverse applications for SLM training, including data rewriting and generation. Selecting the appropriate teacher model based on specific use cases is crucial. This process requires evaluating the teacher’s capabilities and knowledge to ensure optimal application. For example, GPT-4 excels in generating domain-specific data, outperforming ChatGPT, which may produce inferior outcomes. Strategic selection of teacher LLMs is essential for future work to ensure their strengths are effectively utilized to enhance SLM performance.   \n• Applying Emerging Techniques from LLMs to SLMs. To improve LLM performance, techniques such as RetrievalAugmented Generation (RAG) and Mixture of Experts (MoE) are employed. The adoption of RAG in SLMs shows significant promise [215], suggesting benefits from further tailoring retrieved information for SLMs. Future research should account for SLMs’ constraints, such as limited context windows, and customize RAG accordingly. MoE uses ",
        "page_idx": 55
    },
    {
        "type": "text",
        "text": "multiple experts to enhance learning without increasing active neurons, but its storage demands pose challenges for SLM deployment, making this a promising area for exploration. Additionally, the application of LLM techniques such as in-context learning and prompting engineering to maximize SLM performance, while accounting for SLMs’ constraints, warrants further investigation. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "9.6 Applications of SLMs ",
        "text_level": 1,
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "In real-world applications, SLMs often need to provide personalized services and need to be updated periodically to reflect new needs and new knowledge. Hence, there are several promising directions in terms of the real-world application of SLMs, which are listed as follows: ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "• LoRA for Personalized Services. Companies often provide personalized services, but user-specific complexities can render simple rules ineffective. Training a separate SLM for each user is impractical. LoRA suggests a method of separable training weights alongside fixed original weights, enabling scalable customization. For instance, RecLoRA [455] integrates personalized knowledge into SLMs/LLMs tailored for recommendation tasks by maintaining a set of parallel, independent LoRA weights. This approach effectively customizes language model parameters to align with individual user preferences. This approach is a promising direction that inspires further investigation. • Lifelong On-device Learning for Knowledge Injection. SLMs on devices can access local data without risking data privacy through two main methods. The first method uses retrieval-augmented generation to integrate local data into prompts, requiring SLMs with advanced processing and reasoning capabilities. The second method fine-tunes SLMs with local data, integrating customized knowledge into the model’s weights. However, this approach demands significant device resources, including memory and energy. A promising solution is lifelong learning, where SLMs continuously learn and adapt while in use. • Strategic Use of SLMs and LLMs in Multi-Agent Systems. LLMs can function as agents; however, their extensive capabilities are often underutilized in many scenarios, leading to resource wastage. Consequently, strategically routing to appropriately capable SLMs and LLMs within multi-agent systems can optimize cost and functionality. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "9.7 Multimodal SLMs ",
        "text_level": 1,
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "Research on small language models also includes multimodal data. For example, SmolVLM [99] is a compact model that handles image and text inputs to produce text outputs, suitable for on-device use and various multimodal tasks. SOLO [54] integrates vision and language processing in a single 7B Transformer model. The limited scope of existing research on multimodal SLMs provides a compelling impetus for researchers to investigate the integration of various modalities, including audio and graphs. ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "9.8 SLMs Assisting LLMs ",
        "text_level": 1,
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "In Section 6, we introduced existing works on the use of SLMs to assist LLMs. For instance, EFT [246] emulates fine-tuning on LLMs by leveraging behavior deltas between SLMs’ pre-trained and fine-tuned weights to alleviate the time-cost issues associated with fine-tuning LLMs; SlimPLM [325] detects missing knowledge in LLMs using a slim proxy SLM to accelerate knowledge injection; Contrastive Decoding [198] enhances text quality by maximizing the difference between the log probabilities of an expert LLM and an amateur SLM to mitigate issues of low-quality generation. The research on adopting SLMs to assist LLMs is still in its early stages, with many promising directions yet to be explored. We list some as follows: ",
        "page_idx": 56
    },
    {
        "type": "text",
        "text": "• Enhancing LLM Performance Across Broader Tasks Through SLM Integration. SLMs can outperform LLMs in certain scenarios. For example, SLMs often present fewer security vulnerabilities compared to their larger counterparts and demonstrate superior performance on easier samples in specific tasks [197, 234]. Therefore, integrating SLMs with LLMs can promote the development of models that are not only more robust but also inherently safer. Currently, research in this domain is relatively sparse, suggesting that this collaborative framework could potentially be applied to a wider array of tasks. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• Efficient Enhancement of LLMs through Proxy SLMs. Existing research [18, 212, 246, 325] indicates that SLMs, owing to their accelerated fine-tuning and inference speeds, can effectively mimic the behaviors of LLMs, thereby serving as efficient proxies for optimization. However, the application of SLMs as operational proxies for LLMs is currently underexplored. This mimicry could potentially be expanded to include various aspects of LLM functionality, such as the optimization of prompts, the filtration and integration of supplementary knowledge, and the management of additional knowledge repositories. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• SLMs Assist in Managing Data Quality. LLMs tend to produce hallucinations and toxic content due to low-quality real-world training data. One solution is to remove these low-quality data [355]. However, directly eliminating low-quality content can diminish certain functionalities of LLMs, such as versatility [356]. Therefore, it is crucial to define more refined data quality assessment criteria across dimensions such as factuality, safety, and diversity [377] for real-world data. Researching efficient data selection methods using small models represents a valuable research direction. Additionally, while synthetic data serves as a vital complement to scarce human-generated data [224], the potential for small models to effectively manage synthetic data remains largely unexplored. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• SLMs Assist in LLM Assessment. LLMs are producing vast amounts of increasingly complex texts, such as specialized code and scientific papers, presenting challenges not only for human evaluators but also for traditional assessment metrics. Consequently, developing effective evaluators to assess various aspects of generated content, including factuality, safety, and uncertainty, becomes crucial. Given their proficiency in handling specific tasks, exploring the potential of SLMs to evaluate LLM outputs is a promising research direction. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• SLMs Optimize Query and Reduce Noise for LLM RAG. For Retrieval-Augmented Generation (RAG) using LLMs, differing query requirements between LLMs and search engines pose a challenge. The query for LLMs is often abstract and difficult for search engines to handle, so they require more detailed query keywords. Moreover, LLMs may not need all the information related to a query because they only require partial additional knowledge. Thus, intermediate agents are crucial to adapting LLM queries for search engines by clarifying the required detailed keywords that can search for necessary extra knowledge. Additionally, search engine outputs contain noises, requiring refinement to boost LLM efficiency. SLMs, skilled in a single task, are ideal for optimizing query rewriting and noise reduction in RAG systems, making their application in LLM RAG a promising research area. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "• SLMs safeguard LLM. Resources such as the Llama 2 Responsible Use Guide strongly advocate for the implementation of robust guardrails in products that utilize Generative AI. SLMs can be strategically designed to serve as such guardrails, mitigating risks associated with both inputs and outputs from the model. This approach ensures safeguards against the generation of high-risk or policy-violating content and provides protection against adversarial inputs and attempts to compromise the model. Future research can investigate the various safety roles that SLMs play in protecting LLMs. ",
        "page_idx": 57
    },
    {
        "type": "text",
        "text": "9.9 Synergy between Small and Large Language Models ",
        "text_level": 1,
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "In Section 7, we discussed how small and large language models can complement each other. For example, CoGenesis [437] integrates SLMs for private data and LLMs for broader context, while Synergy of Thoughts [298] uses SLMs for initial reasoning and LLMs for conflict resolution. CROSSLM [79] shows how privacy can be preserved by training SLMs locally to support LLMs without data exposure. Research in this area is still evolving, and we outline several promising future directions below: ",
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "• Refined Cloud-Edge Division of Labor. Current research mainly focuses on splitting tasks between edge-based SLMs and cloud-based LLMs along privacy-sensitive and non-sensitive data boundaries. A potential future direction involves more granular task partitioning: determining which subtasks should be handled locally by SLMs (e.g., initial data filtering, quick semantic parsing) and which should be delegated to the cloud-based LLM (e.g., advanced reasoning, complex generation). This approach can further optimize latency, privacy, and resource utilization. • Adaptive On-Device Specialization for Dynamic Environments. Although SLMs have shown the ability to handle private or personalized data locally, continuous changes in user preferences, application requirements, and data distributions pose challenges. Future work can explore adaptive strategies where edge-based SLMs dynamically specialize or update their parameters, guided by the cloud-based LLM. For instance, the LLM can periodically distill new knowledge into the SLM or provide feedback signals to help the SLM adapt to evolving scenarios. ",
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "9.10 Trustworthy SLMs ",
        "text_level": 1,
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "As SLMs are playing crucial roles in various aspects, understanding and improving the trustworthiness of SLMs are essential. Hence, two promising directions are: ",
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "• A Comprehensive Evaluation of SLMs’ Trustworthiness. While numerous studies address trustworthiness issues in LLMs, research on SLMs remains sparse. Most existing literature focuses on models with at least 7 billion parameters, leaving a gap in the comprehensive analysis of SLMs’ trustworthiness. Current evaluations typically cover only a fraction of the necessary aspects. Therefore, a systematic assessment, such as TrustLLM [320], is essential to thoroughly evaluate the trustworthiness of SLMs and understand their reliability across various applications. • Developing Trustworthy SLMs. Developing trustworthy SLMs is crucial, with three key research directions: (i) Training SLMs to be trustworthy from scratch; (ii) Ensuring SLMs retain or gain trustworthiness when compressed from LLMs—maintaining trustworthiness if the LLM is trustworthy and instilling trustworthiness if it is not; (iii) Fine-tuning non-trustworthy SLMs to enhance their robustness. ",
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "10 CONCLUSION ",
        "text_level": 1,
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "This paper provides a comprehensive survey of Small Language Models (SLMs) with up to 7 billion parameters. Initially, we address the need to clearly define SLMs due to existing ambiguities in their characterization. We then present the foundational concepts essential for constructing SLMs. The survey progresses to explore enhancement techniques, including knowledge distillation and quantization, as well as strategies for adapting Large Language Models (LLMs) to SLM contexts. We survey representative SLMs, both general-domain and domain-specific, discussing their preferred datasets and architectural decisions. We also assess their applications across various tasks and deployment strategies on devices. Further, we investigate their role in augmenting the capabilities of LLMs, serving as proxies for fine-tuning and facilitating two types of synergies: cloud-local and task-centric. Additionally, we discuss the critical aspect of their trustworthiness. The paper concludes with key insights aimed at guiding future research on small language models. ",
        "page_idx": 58
    },
    {
        "type": "text",
        "text": "REFERENCES ",
        "text_level": 1,
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. 2024. Phi-3 technical report: A highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219 (2024). [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023). [3] Emre Can Acikgoz, Osman Batur İnce, Rayene Bench, Arda Anıl Boz, İlker Kesen, Aykut Erdem, and Erkut Erdem. 2024. Hippocrates: An Open-Source Framework for Advancing Large Language Models in Healthcare. arXiv preprint arXiv:2404.16621 (2024).   \n[4] Harshavardhan Adepu, Zhanpeng Zeng, Li Zhang, and Vikas Singh. 2024. FrameQuant: Flexible Low-Bit Quantization for Transformers. arXiv preprint arXiv:2403.06082 (2024). [5] Abien Fred Agarap. 2018. Deep Learning using Rectified Linear Units (ReLU). CoRR abs/1803.08375 (2018). arXiv:1803.08375 http://arxiv.org/abs/ 1803.08375 [6] Rishabh Agarwal, Nino Vieillard, Yongchao Zhou, Piotr Stanczyk, Sabela Ramos Garea, Matthieu Geist, and Olivier Bachem. 2024. On-policy distillation of language models: Learning from self-generated mistakes. In The Twelfth International Conference on Learning Representations. [7] Meta AI. 2024. Llama 3.2: Revolutionizing edge AI and vision with open, customizable models. https://ai.meta.com/blog/llama-3-2-connect-2024- vision-edge-mobile-devices/ Accessed: 2024-9-25. [8] Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebrón, and Sumit Sanghai. 2023. GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints. arXiv:2305.13245 [cs.CL] https://arxiv.org/abs/2305.13245 [9] Loubna Ben Allal, Anton Lozhkov, Elie Bakouch, Leandro von Werra, and Thomas Wolf. 2024. SmolLM - blazingly fast and remarkably powerful.   \n[10] Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. 2023. The falcon series of open language models. arXiv preprint arXiv:2311.16867 (2023).   \n[11] Guilherme FCF Almeida, José Luiz Nunes, Neele Engelmann, Alex Wiegmann, and Marcelo de Araújo. 2024. Exploring the psychology of LLMs’ moral and legal reasoning. Artificial Intelligence 333 (2024), 104145.   \n[12] Reza Yazdani Aminabadi, Samyam Rajbhandari, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Olatunji Ruwase, Shaden Smith, Minjia Zhang, Jeff Rasley, et al. 2022. Deepspeed-inference: enabling efficient inference of transformer models at unprecedented scale. In SC22: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–15.   \n[13] Yongqi An, Xu Zhao, Tao Yu, Ming Tang, and Jinqiao Wang. 2024. Fluctuation-based adaptive structured pruning for large language models. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 10865–10873.   \n[14] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. 2023. Palm 2 technical report. arXiv preprint arXiv:2305.10403 (2023).   \n[15] AI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card 1 (2024).   \n[16] David Anugraha, Genta Indra Winata, Chenyue Li, Patrick Amadeus Irawan, and En-Shiun Annie Lee. 2024. ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models. arXiv preprint arXiv:2406.09334 (2024).   \n[17] Viraat Aryabumi, Yixuan Su, Raymond Ma, Adrien Morisot, Ivan Zhang, Acyr Locatelli, Marzieh Fadaee, Ahmet Üstün, and Sara Hooker. 2024. To Code, or Not To Code? Exploring Impact of Code in Pre-training. arXiv preprint arXiv:2408.10914 (2024).   \n[18] Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2024. Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=hSyW5go0v8   \n[19] Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, and James Hensman. 2024. SliceGPT: Compress Large Language Models by Deleting Rows and Columns. In The Twelfth International Conference on Learning Representations. https://openreview.net/ forum?id=vXxardq6db   \n[20] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732 (2021).   \n[21] Amos Azaria and Tom Mitchell. 2023. The Internal State of an LLM Knows When It’s Lying. In Findings of the Association for Computational Linguistics: EMNLP 2023. 967–976.   \n[22] Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2023. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631 (2023).   \n[23] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. arXiv:2309.16609 [cs.CL] https://arxiv.org/abs/2309.16609   \n[24] Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. In Proceedings of the international AAAI conference on web and social media, Vol. 14. 830–839.   \n[25] Maximilian Beck, Korbinian Pöppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, Günter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. 2024. xLSTM: Extended Long Short-Term Memory. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=ARAxPPIAhq   \n[26] Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan, James Baicoianu, Ben Brooks, Nathan Cooper, Ashish Datta, et al. 2024. Stable lm 2 1.6 b technical report. arXiv preprint arXiv:2402.17834 (2024).   \n[27] Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. 2024. SmolLM-Corpus. https://huggingface.co/ datasets/HuggingFaceTB/smollm-corpus   \n[28] Benjamin Bergner, Andrii Skliar, Amelie Royer, Tijmen Blankevoort, Yuki Asano, and Babak Ehteshami Bejnordi. 2024. Think Big, Generate Quick: LLM-to-SLM for Fast Autoregressive Decoding. arXiv preprint arXiv:2402.16844 (2024).   \n[29] Milan Bhan, Jean-Noel Vittaut, Nicolas Chesneau, and Marie-Jeanne Lesot. 2024. Self-AMPLIFY: Improving Small Language Models with Self Post Hoc Explanations. arXiv preprint arXiv:2402.12038 (2024).   \n[30] Zhen Bi, Ningyu Zhang, Yida Xue, Yixin Ou, Daxiong Ji, Guozhou Zheng, and Huajun Chen. 2023. Oceangpt: A large language model for ocean science tasks. arXiv preprint arXiv:2310.02031 (2023).   \n[31] Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. arXiv:2304.01373 [cs.CL] https://arxiv.org/abs/2304.01373   \n[32] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, Vol. 34. 7432–7439.   \n[33] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. 2021. GPT-Neo: Large Scale Autoregressive Language Modeling with MeshTensorflow. https://doi.org/10.5281/zenodo.5297715 If you use this software, please cite it using these metadata..   \n[34] Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et al. 2024. Biomedlm: A 2.7 b parameter language model trained on biomedical text. arXiv preprint arXiv:2403.18421 (2024).   \n[35] William James Bolton, Rafael Poyiadzi, Edward R Morrell, Gabriela van Bergen Gonzalez Bueno, and Lea Goetz. 2024. RAmBLA: A Framework for Evaluating the Reliability of LLMs as Assistants in the Biomedical Domain. arXiv preprint arXiv:2403.14578 (2024).   \n[36] Luiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144 (2022).   \n[37] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf   \n[38] Saikiran Bulusu, Bhavya Kailkhura, Bo Li, Pramod K Varshney, and Dawn Song. 2020. Anomalous example detection in deep learning: A survey. IEEE Access 8 (2020), 132330–132347.   \n[39] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. 2024. A survey on mixture of experts. Authorea Preprints (2024).   \n[40] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei Chu, et al. 2024. Internlm2 technical report. arXiv preprint arXiv:2403.17297 (2024).   \n[41] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. 2021. Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21). 2633–2650.   \n[42] Samuel Carreira, Tomás Marques, José Ribeiro, and Carlos Grilo. 2023. Revolutionizing Mobile Interaction: Enabling a 3 Billion Parameter GPT LLM on Mobile. arXiv:2310.01434 [cs.CL] https://arxiv.org/abs/2310.01434   \n[43] Iñigo Casanueva, Tadas Temčinas, Daniela Gerz, Matthew Henderson, and Ivan Vulić. 2020. Efficient Intent Detection with Dual Sentence Encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI. Association for Computational Linguistics, Online, 38–45.   \n[44] Wei-Cheng Chang, X Yu Felix, Yin-Wen Chang, Yiming Yang, and Sanjiv Kumar. [n. d.]. Pre-training Tasks for Embedding-based Large-scale Retrieval. In International Conference on Learning Representations.   \n[45] Patrick Chao, Edoardo Debenedetti, Alexander Robey, Maksym Andriushchenko, Francesco Croce, Vikash Sehwag, Edgar Dobriban, Nicolas Flammarion, George J Pappas, Florian Tramer, et al. 2024. Jailbreakbench: An open robustness benchmark for jailbreaking large language models. arXiv preprint arXiv:2404.01318 (2024).   \n[46] Dong Chen, Shuo Zhang, Yueting Zhuang, Siliang Tang, Qidong Liu, Hua Wang, and Mingliang Xu. 2024. Improving Large Models with Small models: Lower Costs and Better Performance. arXiv preprint arXiv:2406.15471 (2024).   \n[47] Dong Chen, Yueting Zhuang, Shuo Zhang, Jinfeng Liu, Su Dong, and Siliang Tang. 2024. Data Shunt: Collaboration of Small and Large Models for Lower Costs and Better Performance. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 11249–11257.   \n[48] Hongzhan Chen, Siyue Wu, Xiaojun Quan, Rui Wang, Ming Yan, and Ji Zhang. 2023. MCC-KD: Multi-CoT Consistent Knowledge Distillation. In Findings of the Association for Computational Linguistics: EMNLP 2023. 6805–6820.   \n[49] Lihu Chen and Gaël Varoquaux. 2024. What is the role of small models in the llm era: A survey. arXiv preprint arXiv:2409.06857 (2024).   \n[50] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374 (2021).   \n[51] Tianyi Chen, Tianyu Ding, Badal Yadav, Ilya Zharkov, and Luming Liang. 2023. Lorashear: Efficient large language model structured pruning and knowledge recovery. arXiv preprint arXiv:2310.18356 (2023).   \n[52] Wei Chen, Zhiyuan Li, and Mingyuan Ma. 2024. Octopus: On-device language model for function calling of software APIs. arXiv:2404.01549 [cs.CL] https://arxiv.org/abs/2404.01549   \n[53] Yangyi Chen, Fanchao Qi, Hongcheng Gao, Zhiyuan Liu, and Maosong Sun. 2022. Textual Backdoor Attacks Can Be More Harmful via Two Simple Tricks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP. 11215–11221.   \n[54] Yangyi Chen, Xingyao Wang, Hao Peng, and Heng Ji. 2024. A Single Transformer for Scalable Vision-Language Modeling. Transactions on Machine Learning Research (2024). https://openreview.net/forum?id=nuzFG0Rbhy   \n[55] Zeming Chen, Alejandro Hernández Cano, Angelika Romanou, Antoine Bonnet, Kyle Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Köpf, Amirkeivan Mohtashami, et al. 2023. MEDITRON-70B: Scaling Medical Pretraining for Large Language Models. arXiv preprint arXiv:2311.16079 (2023).   \n[56] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan R Routledge, et al. 2021. FinQA: A Dataset of Numerical Reasoning over Financial Data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 3697–3711.   \n[57] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the Chain of Numerical Reasoning in Conversational Finance Question Answering. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 6279–6292.   \n[58] Xiaoxue Cheng, Junyi Li, Wayne Xin Zhao, Hongzhi Zhang, Fuzheng Zhang, Di Zhang, Kun Gai, and Ji-Rong Wen. 2024. Small Agent Can Also Rock! Empowering Small Language Models as Hallucination Detector. arXiv preprint arXiv:2406.11277 (2024).   \n[59] Steffi Chern, Zhulin Hu, Yuqing Yang, Ethan Chern, Yuan Guo, Jiahe Jin, Binjie Wang, and Pengfei Liu. 2024. BeHonest: Benchmarking Honesty of Large Language Models. arXiv preprint arXiv:2406.13261 (2024).   \n[60] Yew Ken Chia, Pengfei Hong, Lidong Bing, and Soujanya Poria. 2024. InstructEval: Towards Holistic Evaluation of Instruction-Tuned Large Language Models. In Proceedings of the First edition of the Workshop on the Scaling Behavior of Large Language Models (SCALE-LLM 2024). 35–64.   \n[61] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with $9 0 \\% ^ { \\star }$ ChatGPT Quality. https://lmsys.org/blog/2023- 03-30-vicuna/   \n[62] Yae Jee Cho, Luyang Liu, Zheng Xu, Aldi Fahrezi, and Gauri Joshi. 2024. Heterogeneous LoRA for Federated Fine-tuning of On-Device Foundation Models. arXiv:2401.06432 [cs.LG] https://arxiv.org/abs/2401.06432   \n[63] Xiaokai Chu, Jiashu Zhao, Lixin Zou, and Dawei Yin. 2022. H-ERNIE: A multi-granularity pre-trained language model for web search. In Proceedings of the 45th International ACM SIGIR conference on research and development in information retrieval. 1478–1489.   \n[64] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research 25, 70 (2024), 1–53.   \n[65] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457 (2018).   \n[66] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168 (2021).   \n[67] Together Computer. 2023. RedPajama: an Open Dataset for Training Large Language Models. https://github.com/togethercomputer/RedPajama-Data   \n[68] Mike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie, Jun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell, Matei Zaharia, and Reynold Xin. 2023. Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned LLM. https://www.databricks.com/blog/2023/04/12/dolly-first-open-commerciallyviable-instruction-tuned-llm   \n[69] Nick Craswell, Bhaskar Mitra, Emine Yilmaz, and Daniel Campos. 2021. Overview of the TREC 2020 deep learning track. CoRR abs/2102.07662 (2021). arXiv:2102.07662 https://arxiv.org/abs/2102.07662   \n[70] Justin Cui, Wei-Lin Chiang, Ion Stoica, and Cho-Jui Hsieh. 2024. OR-Bench: An Over-Refusal Benchmark for Large Language Models. arXiv preprint arXiv:2405.20947 (2024).   \n[71] Shiyao Cui, Zhenyu Zhang, Yilong Chen, Wenyuan Zhang, Tianyun Liu, Siqi Wang, and Tingwen Liu. 2023. Fft: Towards harmlessness evaluation and analysis for llms with factuality, fairness, toxicity. arXiv preprint arXiv:2311.18580 (2023).   \n[72] Luigi Daniele and Suphavadeeprasit. 2023. Amplify-Instruct: Synthetically Generated Diverse Multi-turn Conversations for efficient LLM Training. arXiv preprint arXiv:(coming soon) (2023). https://huggingface.co/datasets/LDJnr/Capybara   \n[73] Tri Dao. [n. d.]. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In The Twelfth International Conference on Learning Representations.   \n[74] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems 35 (2022), 16344–16359.   \n[75] Tri Dao and Albert Gu. 2024. Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060 (2024).   \n[76] Rocktim Jyoti Das, Liqun Ma, and Zhiqiang Shen. 2023. Beyond size: How gradients shape pruning decisions in large language models. arXiv preprint arXiv:2311.04902 (2023).   \n[77] Anirban Dasgupta, Ravi Kumar, and Tamás Sarlós. 2011. Fast locality-sensitive hashing. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining. 1073–1081.   \n[78] Pieter Delobelle, Ewoenam Kwaku Tokpo, Toon Calders, and Bettina Berendt. 2022. Measuring fairness with biased rulers: A comparative study on bias metrics for pre-trained language models. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics. 1693–1706.   \n[79] Yongheng Deng, Ziqing Qiao, Ju Ren, Yang Liu, and Yaoxue Zhang. 2023. Mutual enhancement of large and small language models with cross-silo knowledge transfer. arXiv preprint arXiv:2312.05842 (2023).   \n[80] Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit Matrix Multiplication for Transformers at Scale. In Advances in Neural Information Processing Systems, Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (Eds.). https: //openreview.net/forum?id=dXiGWqBoxaD   \n[81] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2024. Qlora: Efficient finetuning of quantized llms. Advances in Neural Information Processing Systems 36 (2024).   \n[82] Tim Dettmers and Luke Zettlemoyer. 2023. The case for 4-bit precision: k-bit inference scaling laws. In International Conference on Machine Learning. PMLR, 7750–7774.   \n[83] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). 4171–4186.   \n[84] Nolan Dey, Gurpreet Gosal, Zhiming Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. 2023. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the Cerebras Wafer-Scale Cluster. CoRR abs/2304.03208 (2023).   \n[85] Richard Diehl Martinez, Pietro Lesci, and Paula Buttery. 2024. Tending Towards Stability: Convergence Challenges in Small Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3275–3286. https://doi.org/10.18653/v1/2024.findings-emnlp.187   \n[86] Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi Zheng, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. 2023. Enhancing chat language models by scaling high-quality instructional conversations. arXiv preprint arXiv:2305.14233 (2023).   \n[87] Tinghe Ding. 2024. MobileAgent: enhancing mobile control via human-machine interaction and SOP integration. arXiv:2401.04124 [cs.HC] https://arxiv.org/abs/2401.04124   \n[88] Ricardo Dominguez-Olmedo, Moritz Hardt, and Celestine Mendler-Dünner. 2023. Questioning the survey responses of large language models. arXiv preprint arXiv:2306.07951 (2023).   \n[89] Qian Dong, Yiding Liu, Qingyao Ai, Haitao Li, Shuaiqiang Wang, Yiqun Liu, Dawei Yin, and Shaoping Ma. 2023. I3 retriever: incorporating implicit interaction in pre-trained language models for passage retrieval. In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management. 441–451.   \n[90] Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. 2024. Hymba: A Hybrid-head Architecture for Small Language Models. arXiv preprint arXiv:2411.13676 (2024).   \n[91] Jason Xiaotian Dou, Haiyi Mao, Runxue Bao, Paul Pu Liang, Xiaoqing Tan, Shiyi Zhang, Minxue Jia, Pengfei Zhou, and Zhi-Hong Mao. 2023. The Measurement of Knowledge in Knowledge Graphs. In Proceedings of the AAAI 2023 Workshop on Representation Learning for Responsible Human-Centric AI (R2HCAI). Association for the Advancement of Artificial Intelligence (AAAI) Washington . . .   \n[92] Xinrun Du, Zhouliang Yu, Songyang Gao, Ding Pan, Yuyang Cheng, Ziyang Ma, Ruibin Yuan, Xingwei Qu, Jiaheng Liu, Tianyu Zheng, Xinchen Luo, Guorui Zhou, Wenhu Chen, and Ge Zhang. 2024. Chinese Tiny LLM: Pretraining a Chinese-Centric Large Language Model. arXiv:2404.04167 [cs.CL] https://arxiv.org/abs/2404.04167   \n[93] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General Language Model Pretraining with Autoregressive Blank Infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 320–335.   \n[94] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024).   \n[95] Kazuki Egashira, Mark Vero, Robin Staab, Jingxuan He, and Martin Vechev. 2024. Exploiting LLM Quantization. arXiv preprint arXiv:2405.18137 (2024).   \n[96] Ronen Eldan and Yuanzhi Li. 2023. Tinystories: How small can language models be and still speak coherent english? arXiv preprint arXiv:2305.07759 (2023).   \n[97] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. 2018. Sigmoid-weighted linear units for neural network function approximation in reinforcement learning. Neural networks 107 (2018), 3–11.   \n[98] David Esiobu, Xiaoqing Tan, Saghar Hosseini, Megan Ung, Yuchen Zhang, Jude Fernandes, Jane Dwivedi-Yu, Eleonora Presani, Adina Williams, and Eric Smith. 2023. ROBBIE: Robust bias evaluation of large generative language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 3764–3814. https://aclanthology.org/2023.emnlp-main.230   \n[99] Hugging Face. 2024. SmolVLM - small yet mighty Vision Language Model. https://huggingface.co/blog/smolvlm Accessed: 2024-11-26.   \n[100] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research 23, 120 (2022), 1–39.   \n[101] Shangbin Feng, Weijia Shi, Yuyang Bai, Vidhisha Balachandran, Tianxing He, and Yulia Tsvetkov. 2023. Knowledge Card: Filling LLMs’ Knowledge Gaps with Plug-in Specialized Language Models. arXiv preprint arXiv:2305.09955 (2023).   \n[102] Elias Frantar and Dan Alistarh. 2023. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International Conference on Machine Learning. PMLR, 10323–10337.   \n[103] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2023. GPTQ: Accurate Post-Training Quantization for Generative Pre-trained Transformers. In The Eleventh International Conference on Learning Representations.   \n[104] Hao Fu, Yao; Peng and Tushar Khot. 2022. How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources. Yao Fu’s Notion (Dec 2022). https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1   \n[105] Yao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and Tushar Khot. 2023. Specializing smaller language models towards multi-step reasoning. In International Conference on Machine Learning. PMLR, 10421–10430.   \n[106] Philip Gage. 1994. A new algorithm for data compression. The C Users Journal 12, 2 (1994), 23–38.   \n[107] Chongming Gao, Shiqi Wang, Shijun Li, Jiawei Chen, Xiangnan He, Wenqiang Lei, Biao Li, Yuan Zhang, and Peng Jiang. 2023. CIRS: Bursting filter bubbles by counterfactual interactive recommender system. ACM Transactions on Information Systems 42, 1 (2023), 1–27.   \n[108] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The Pile: An 800GB dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027 (2020).   \n[109] Luyu Gao and Jamie Callan. 2022. Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2843–2853.   \n[110] Shangqian Gao, Chi-Heng Lin, Ting Hua, Zheng Tang, Yilin Shen, Hongxia Jin, and Yen-Chang Hsu. 2024. DISP-LLM: Dimension-Independent Structural Pruning for Large Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https: //openreview.net/forum?id=YxaY6tHgg0   \n[111] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. 2024. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=uNrFpDPMyo   \n[112] Alex Gichamba, Tewodros Kederalah Idris, Brian Ebiyau, Eric Nyberg, and Teruko Mitamura. 2024. ColBERT Retrieval and Ensemble Response Scoring for Language Model Question Answering. arXiv:2408.10808 [cs.CL] https://arxiv.org/abs/2408.10808   \n[113] Karan Goel. 2024. The On-Device Intelligence Update. https://www.cartesia.ai/blog/on-device   \n[114] Aaron Gokaslan, Vanya Cohen, Ellie Pavlick, and Stefanie Tellex. 2019. Openwebtext corpus.   \n[115] Shreya Goyal, Sumanth Doddapaneni, Mitesh M Khapra, and Balaraman Ravindran. 2023. A survey of adversarial defenses and robustness in nlp. Comput. Surveys 55, 14s (2023), 1–39.   \n[116] Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838 (2024).   \n[117] Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752 (2023).   \n[118] Naibin Gu, Peng Fu, Xiyu Liu, Bowen Shen, Zheng Lin, and Weiping Wang. 2024. Light-PEFT: Lightening Parameter-Efficient Fine-Tuning via Early Pruning. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 7528–7541. https://doi.org/10.18653/v1/2024.findings-acl.447   \n[119] Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. 2024. MiniLLM: Knowledge distillation of large language models. In The Twelfth International Conference on Learning Representations.   \n[120] Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Harkirat Singh Behl, Xin Wang, Sébastien Bubeck, Ronen Eldan, Adam Tauman Kalai, Yin Tat Lee, and Yuanzhi Li. 2023. Textbooks Are All You Need. arXiv:2306.11644 [cs.CL] https://arxiv.org/abs/2306.11644   \n[121] Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Yu Wu, YK Li, et al. 2024. DeepSeek-Coder: When the Large Language Model Meets Programming–The Rise of Code Intelligence. arXiv preprint arXiv:2401.14196 (2024).   \n[122] Jinyang Guo, Jianyu Wu, Zining Wang, Jiaheng Liu, Ge Yang, Yifu Ding, Ruihao Gong, Haotong Qin, and Xianglong Liu. 2024. Compressing large language models by joint sparsification and quantization. In Forty-first International Conference on Machine Learning.   \n[123] Shangwei Guo, Chunlong Xie, Jiwei Li, Lingjuan Lyu, and Tianwei Zhang. 2022. Threats to pre-trained language models: Survey and taxonomy. arXiv preprint arXiv:2202.06862 (2022).   \n[124] Song Guo, Jiahang Xu, Li Lyna Zhang, and Mao Yang. 2023. Compresso: Structured pruning with collaborative prompting learns compact large language models. arXiv preprint arXiv:2310.05015 (2023).   \n[125] Zhen Guo, Peiqi Wang, Yanwei Wang, and Shangdi Yu. 2023. Improving Small Language Models on PubMedQA via Generative Data Augmentation. arXiv:2305.07804 [cs.CL] https://arxiv.org/abs/2305.07804   \n[126] Song Han, Jeff Pool, John Tran, and William Dally. 2015. Learning both weights and connections for efficient neural network. Advances in neural information processing systems 28 (2015).   \n[127] Zixu Hao, Huiqiang Jiang, Shiqi Jiang, Ju Ren, and Ting Cao. 2024. Hybrid SLM and LLM for Edge-Cloud Collaborative Inference. In Proceedings of the Workshop on Edge and Mobile Foundation Models. 36–41.   \n[128] Tim Hartill, Diana Benavides-Prado, Michael Witbrock, and Patricia J. Riddle. 2023. Answering Unseen Questions With Smaller Language Models Using Rationale Generation and Dense Retrieval. arXiv:2308.04711 [cs.CL] https://arxiv.org/abs/2308.04711   \n[129] Tim Hartill, Neset Tan, Michael Witbrock, and Patricia J. Riddle. 2023. Teaching Smaller Language Models To Generalise To Unseen Compositional Questions. arXiv:2308.00946 [cs.CL] https://arxiv.org/abs/2308.00946   \n[130] Kai He, Rui Mao, Qika Lin, Yucheng Ruan, Xiang Lan, Mengling Feng, and Erik Cambria. 2023. A survey of large language models for healthcare: from data, technology, and applications to accountability and ethics. arXiv preprint arXiv:2310.05694 (2023).   \n[131] Pengcheng He, Jianfeng Gao, and Weizhu Chen. 2023. DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with GradientDisentangled Embedding Sharing. In The Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=sE7- XhLxHA   \n[132] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. arXiv preprint arXiv:2006.03654 (2020).   \n[133] Narges Heidari, Parham Moradi, and Abbas Koochari. 2022. An attention-based deep learning method for solving the cold-start and sparsity issues of recommender systems. Knowledge-Based Systems 256 (2022), 109835.   \n[134] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations. https://openreview.net/forum?id=d7KBjmI3GmQ   \n[135] Dan Hendrycks and Kevin Gimpel. 2016. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 (2016).   \n[136] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).   \n[137] Sepp Hochreiter and Jürgen Schmidhuber. 1996. LSTM can solve hard long time lag problems. Advances in neural information processing systems 9 (1996).   \n[138] Junyuan Hong, Jinhao Duan, Chenhui Zhang, Zhangheng Li, Chulin Xie, Kelsey Lieberman, James Diffenderfer, Brian R. Bartoldson, Ajay Kumar Jaiswal, Kaidi Xu, Bhavya Kailkhura, Dan Hendrycks, Dawn Song, Zhangyang Wang, and Bo Li. 2024. Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression. In Proceedings of the Forty-first International Conference on Machine Learning, ICML. https://openreview.net/forum?id=e3Dpq3WdMv   \n[139] Yutong Meng Yuhao Wang Hongcheng Liu, Yusheng Liao. 2023. XieZhi: Chinese Law Large Language Model. https://github.com/LiuHC0428/ LAW_GPT.   \n[140] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In International Conference on Machine Learning. PMLR, 2790–2799.   \n[141] Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh, Hootan Nakhost, Yasuhisa Fujii, Alex Ratner, Ranjay Krishna, Chen-Yu Lee, and Tomas Pfister. 2023. Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. In Findings of the Association for Computational Linguistics: ACL 2023. 8003–8017.   \n[142] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).   \n[143] Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. 2024. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395 (2024).   \n[144] Xing Hu, Yuan Chen, Dawei Yang, Sifan Zhou, Zhihang Yuan, Jiangyong Yu, and Chen Xu. 2024. I-LLM: Efficient Integer-Only Inference for Fully-Quantized Low-Bit Large Language Models. arXiv preprint arXiv:2405.17849 (2024).   \n[145] Jiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. [n. d.]. Large Language Models Can Self-Improve. In The 2023 Conference on Empirical Methods in Natural Language Processing.   \n[146] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2023. A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv preprint arXiv:2311.05232 (2023).   \n[147] Wei Huang, Yangdong Liu, Haotong Qin, Ying Li, Shiming Zhang, Xianglong Liu, Michele Magno, and Xiaojuan Qi. 2024. Billm: Pushing the limit of post-training quantization for llms. arXiv preprint arXiv:2402.04291 (2024).   \n[148] Wenyu Huang, Guancheng Zhou, Hongru Wang, Pavlos Vougiouklis, Mirella Lapata, and Jeff Z. Pan. 2024. Less is More: Making Smaller Language Models Competent Subgraph Retrievers for Multi-hop KGQA. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 15787–15803. https://doi.org/10.18653/v1/2024.findings-emnlp.927   \n[149] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems 36 (2024).   \n[150] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic Jailbreak of Open-source LLMs via Exploiting Generation. arXiv preprint arXiv:2310.06987 (2023).   \n[151] Yuheng Huang, Jiayang Song, Zhijie Wang, Shengming Zhao, Huaming Chen, Felix Juefei-Xu, and Lei Ma. 2023. Look before you leap: An exploratory study of uncertainty measurement for large language models. arXiv preprint arXiv:2307.10236 (2023).   \n[152] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. [n. d.]. Poly-encoders: Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring. In International Conference on Learning Representations.   \n[153] Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, et al. 2023. Llama guard: Llm-based input-output safeguard for human-ai conversations. arXiv preprint arXiv:2312.06674 (2023).   \n[154] Robert A Jacobs, Michael I Jordan, Steven J Nowlan, and Geoffrey E Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 79–87.   \n[155] Mojan Javaheripi, Sébastien Bubeck, Marah Abdin, Jyoti Aneja, Sebastien Bubeck, Caio César Teodoro Mendes, Weizhu Chen, Allie Del Giorno, Ronen Eldan, Sivakanth Gopi, et al. 2023. Phi-2: The surprising power of small language models. Microsoft Research Blog (2023).   \n[156] Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Hwang, and Jong C Park. 2023. Test-Time Self-Adaptive Small Language Models for Question Answering. In Findings of the Association for Computational Linguistics: EMNLP 2023. 15459–15469.   \n[157] Ananya Harsh Jha, Tom Sherborne, Evan Pete Walsh, Dirk Groeneveld, Emma Strubell, and Iz Beltagy. 2024. Just CHOP: Embarrassingly Simple LLM Compression. arXiv:2305.14864 [cs.CL] https://arxiv.org/abs/2305.14864   \n[158] Yixin Ji, Yang Xiang, Juntao Li, Wei Chen, Zhongyi Liu, Kehai Chen, and Min Zhang. 2024. Feature-based Low-Rank Compression of Large Language Models via Bayesian Optimization. arXiv preprint arXiv:2405.10616 (2024).   \n[159] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung. 2023. Towards mitigating LLM hallucination via self reflection. In Findings of the Association for Computational Linguistics: EMNLP 2023. 1827–1843.   \n[160] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023).   \n[161] Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of experts. arXiv preprint arXiv:2401.04088 (2024).   \n[162] Huiqiang Jiang, Qianhui Wu, Xufang Luo, Dongsheng Li, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2024. LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. In ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models. https://openreview.net/forum?id=9YvfRrpmyw   \n[163] Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William Cohen, and Xinghua Lu. 2019. PubMedQA: A Dataset for Biomedical Research Question Answering. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). 2567–2577.   \n[164] Rudolph Emil Kalman. 1960. A new approach to linear filtering and prediction problems. (1960).   \n[165] Hao Kang, Qingru Zhang, Souvik Kundu, Geonhwa Jeong, Zaoxing Liu, Tushar Krishna, and Tuo Zhao. 2024. Gear: An efficient kv cache compression recipefor near-lossless generative inference of llm. arXiv preprint arXiv:2403.05527 (2024).   \n[166] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 (2020).   \n[167] Omar Khattab, Arnav Singhvi, Paridhi Maheshwari, Zhiyuan Zhang, Keshav Santhanam, Sri Vardhamanan, Saiful Haq, Ashutosh Sharma, Thomas T Joshi, Hanna Moazam, et al. 2023. Dspy: Compiling declarative language model calls into self-improving pipelines. arXiv preprint arXiv:2310.03714 (2023).   \n[168] Jeonghoon Kim, Jung Hyun Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, Se Jung Kwon, and Dongsoo Lee. 2024. Memory-efficient fine-tuning of compressed large language models via sub-4-bit integer quantization. Advances in Neural Information Processing Systems 36 (2024).   \n[169] Minsoo Kim, Sihwa Lee, Janghwan Lee, Sukjin Hong, Du-Seong Chang, Wonyong Sung, and Jungwook Choi. 2024. Token-scaled logit distillation for ternary weight generative language models. Advances in Neural Information Processing Systems 36 (2024).   \n[170] Sehoon Kim, Coleman Hooper, Amir Gholami, Zhen Dong, Xiuyu Li, Sheng Shen, Michael W Mahoney, and Kurt Keutzer. 2023. Squeezellm: Dense-and-sparse quantization. arXiv preprint arXiv:2306.07629 (2023).   \n[171] Yoon Kim and Alexander M Rush. 2016. Sequence-Level Knowledge Distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. 1317–1327.   \n[172] Young Jin Kim, Raffy Fahim, and Hany Hassan Awadalla. 2023. Mixture of Quantized Experts (MoQE): Complementary Effect of Low-bit Quantization and Robustness. arXiv preprint arXiv:2310.02410 (2023).   \n[173] Jongwoo Ko, Sungnyun Kim, Tianyi Chen, and Se-Young Yun. 2024. DistiLLM: Towards Streamlined Distillation for Large Language Models. arXiv preprint arXiv:2402.03898 (2024).   \n[174] Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Muñoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533 (2022).   \n[175] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty: Linguistic Invariances for Uncertainty Estimation in Natural Language Generation. In Proceedings of the Eleventh International Conference on Learning Representations. https://openreview.net/forum?id=VD-AYtP0dve   \n[176] Divyanshu Kumar, Anurakt Kumar, Sahil Agarwal, and Prashanth Harshangi. 2024. Fine-Tuning, Quantization, and LLMs: Navigating Unintended Outcomes. arXiv preprint arXiv:2404.04392 (2024).   \n[177] Ohjoon Kwon, Donghyeon Jeon, Nayoung Choi, Gyu-Hwung Cho, Hwiyeol Jo, Changbong Kim, Hyunwoo Lee, Inho Kang, Sun Kim, and Taiwoo Park. 2024. SLM as Guardian: Pioneering AI Safety with Small Language Model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track, Franck Dernoncourt, Daniel Preoţiuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US, 1333–1350. https://doi.org/10.18653/v1/2024.emnlp-industry.99   \n[178] Yanis Labrak, Adrien Bazoge, Emmanuel Morin, Pierre-Antoine Gourraud, Mickael Rouvier, and Richard Dufour. 2024. Biomistral: A collection of open-source pretrained large language models for medical domains. arXiv preprint arXiv:2402.10373 (2024).   \n[179] Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An Evaluation Dataset for Intent Classification and Out-of-Scope Prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China,   \nManuscript submitted to ACM 1311–1316. https://doi.org/10.18653/v1/D19-1131   \n[180] Hugo Laurençon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduardo González Ponferrada, Huu Nguyen, et al. 2022. The bigscience roots corpus: A 1.6 tb composite multilingual dataset. Advances in Neural Information Processing Systems 35 (2022), 31809–31826.   \n[181] Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilić, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2023. Bloom: A 176b-parameter open-access multilingual language model. (2023).   \n[182] Hojae Lee, Junho Kim, and SangKeun Lee. 2024. Mentor-KD: Making Small Language Models Better Multi-step Reasoners. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 17643–17658. https://doi.org/10.18653/v1/2024.emnlp-main.977   \n[183] Jooyoung Lee, Fan Yang, Thanh Tran, Qian Hu, Emre Barut, and Kai-Wei Chang. 2024. Can Small Language Models Help Large Language Models Reason Better?: LM-Guided Chain-of-Thought. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 2835–2843.   \n[184] Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. 2022. xFormers: A modular and hackable Transformer modelling library. https://github.com/facebookresearch/xformers.   \n[185] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. ArXiv e-prints (2016), arXiv–1607.   \n[186] Aitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al. 2022. Solving quantitative reasoning problems with language models. Advances in Neural Information Processing Systems 35 (2022), 3843–3857.   \n[187] Chenglin Li, Qianglong Chen, Liangyue Li, Caiyu Wang, Yicheng Li, Zulong Chen, and Yin Zhang. 2023. Mixed distillation helps smaller language model better reasoning. arXiv preprint arXiv:2312.10730 (2023).   \n[188] Guangyan Li, Yongqiang Tang, and Wensheng Zhang. 2024. LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models. arXiv preprint arXiv:2404.09695 (2024).   \n[189] Haitao Li, Qingyao Ai, Jia Chen, Qian Dong, Zhijing Wu, Yiqun Liu, Chong Chen, and Qi Tian. 2024. BLADE: Enhancing Black-box Large Language Models with Small Domain-Specific Models. arXiv:2403.18365 [cs.CL] https://arxiv.org/abs/2403.18365   \n[190] Haoran Li, Dadi Guo, Donghao Li, Wei Fan, Qi Hu, Xin Liu, Chunkit Chan, Duanyi Yao, Yuan Yao, and Yangqiu Song. 2024. PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics. 54–73.   \n[191] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Singapore, 6449–6464. https://doi.org/10.18653/v1/2023.emnlp-main.397   \n[192] Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Gadre, Hritik Bansal, Etash Guha, Sedrick Keh, Kushal Arora, et al. 2024. Datacomp-lm: In search of the next generation of training sets for language models. arXiv preprint arXiv:2406.11794 (2024).   \n[193] Luchang Li, Sheng Qian, Jie Lu, Lunxi Yuan, Rui Wang, and Qin Xie. 2024. Transformer-Lite: High-efficiency Deployment of Large Language Models on Mobile Phone GPUs. arXiv:2403.20041 [cs.CL] https://arxiv.org/abs/2403.20041   \n[194] Pingzhi Li, Xiaolong Jin, Yu Cheng, and Tianlong Chen. 2024. Examining Post-Training Quantization for Mixture-of-Experts: A Benchmark. arXiv preprint arXiv:2406.08155 (2024).   \n[195] Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023. StarCoder: may the source be with you! arXiv:2305.06161 [cs.CL] https://arxiv.org/abs/2305.06161   \n[196] Shengrui Li, Xueting Han, and Jing Bai. 2024. Nuteprune: Efficient progressive pruning with numerous teachers for large language models. arXiv preprint arXiv:2402.09773 (2024).   \n[197] Tianlin Li, Qian Liu, Tianyu Pang, Chao Du, Qing Guo, Yang Liu, and Min Lin. 2024. Purifying large language models by ensembling a small language model. arXiv preprint arXiv:2402.14845 (2024).   \n[198] Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori B Hashimoto, Luke Zettlemoyer, and Mike Lewis. 2023. Contrastive Decoding: Open-ended Text Generation as Optimization. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 12286–12312.   \n[199] Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 (2021).   \n[200] Yuanzhi Li, Sébastien Bubeck, Ronen Eldan, Allie Del Giorno, Suriya Gunasekar, and Yin Tat Lee. 2023. Textbooks Are All You Need II: phi-1.5 technical report. arXiv:2309.05463 [cs.CL] https://arxiv.org/abs/2309.05463   \n[201] Yun Li, Lin Niu, Xipeng Zhang, Kai Liu, Jianchen Zhu, and Zhanhui Kang. 2023. E-sparse: Boosting the large language model inference through entropy-based n: M sparsity. arXiv preprint arXiv:2310.15929 (2023).   \n[202] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large language models in finance: A survey. In Proceedings of the fourth ACM international conference on AI in finance. 374–382.   \n[203] Wing Lian, Guan Wang, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and \"Teknium\". 2023. SlimOrca: An Open Dataset of GPT-4 Augmented FLAN Reasoning Traces, with Verification. https://https://huggingface.co/Open-Orca/SlimOrca   \n[204] Jinggui Liang, Lizi Liao, Hao Fei, and Jing Jiang. 2024. Synergizing Large Language Models and Pre-Trained Smaller Models for Conversational Intent Discovery. In Findings of the Association for Computational Linguistics ACL 2024. 14133–14147.   \n[205] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re, Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic Evaluation of Language Models. Transactions on Machine Learning Research (2023). https://openreview.net/forum?id=iO4LZibEqW   \n[206] Jianghao Lin, Rong Shan, Chenxu Zhu, Kounianhua Du, Bo Chen, Shigang Quan, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Rella: Retrieval-enhanced large language models for lifelong sequential behavior comprehension in recommendation. In Proceedings of the ACM on Web Conference 2024. 3497–3508.   \n[207] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024. AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration. Proceedings of Machine Learning and Systems 6 (2024), 87–100.   \n[208] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 3214–3252.   \n[209] Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. 2022. Few-shot Learning with Multilingual Generative Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 9019–9052. https://doi.org/10.18653/v1/2022.emnlp-main.616   \n[210] Zhenghao Lin, Zhibin Gou, Yeyun Gong, Xiao Liu, Yelong Shen, Ruochen Xu, Chen Lin, Yujiu Yang, Jian Jiao, Nan Duan, et al. 2024. Rho-1: Not all tokens are what you need. arXiv preprint arXiv:2404.07965 (2024).   \n[211] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. 2024. Deepseek-v2: A strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434 (2024).   \n[212] Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia Tsvetkov, Yejin Choi, and Noah A Smith. 2024. Tuning language models by proxy. arXiv preprint arXiv:2401.08565 (2024).   \n[213] Jiashuo Liu, Zheyan Shen, Yue He, Xingxuan Zhang, Renzhe Xu, Han Yu, and Peng Cui. 2021. Towards out-of-distribution generalization: A survey. arXiv preprint arXiv:2108.13624 (2021).   \n[214] Qijiong Liu, Nuo Chen, Tetsuya Sakai, and Xiao-Ming Wu. 2024. Once: Boosting content-based recommendation with both open-and closed-source large language models. In Proceedings of the 17th ACM International Conference on Web Search and Data Mining. 452–461.   \n[215] Suqing Liu, Zezhu Yu, Feiran Huang, Yousef Bulbulia, Andreas Bergen, and Michael Liut. 2024. Can Small Language Models With RetrievalAugmented Generation Replace Large Language Models When Learning Computer Science? In Proceedings of the 2024 on Innovation and Technology in Computer Science Education V. 1. 388–393.   \n[216] Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023. What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning. arXiv preprint arXiv:2312.15685 (2023).   \n[217] Yinpeng Liu, Jiawei Liu, Xiang Shi, Qikai Cheng, and Wei Lu. 2024. Let’s Learn Step by Step: Enhancing In-Context Learning Ability with Curriculum Learning. arXiv preprint arXiv:2402.10738 (2024).   \n[218] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692 (2019).   \n[219] Yanming Liu, Xinyue Peng, Xuhong Zhang, Weihao Liu, Jianwei Yin, Jiannan Cao, and Tianyu Du. 2024. RA-ISF: Learning to Answer and Understand from Retrieval Augmentation via Iterative Self-Feedback. arXiv preprint arXiv:2403.06840 (2024).   \n[220] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. 2024. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. Advances in Neural Information Processing Systems 36 (2024).   \n[221] Zechun Liu, Barlas Oguz, Changsheng Zhao, Ernie Chang, Pierre Stock, Yashar Mehdad, Yangyang Shi, Raghuraman Krishnamoorthi, and Vikas Chandra. 2023. Llm-qat: Data-free quantization aware training for large language models. arXiv preprint arXiv:2305.17888 (2023).   \n[222] Zhengxiao Liu, Bowen Shen, Zheng Lin, Fali Wang, and Weiping Wang. 2023. Maximum Entropy Loss, the Silver Bullet Targeting Backdoor Attacks in Pre-trained Language Models. In Findings of the Association for Computational Linguistics: ACL 2023, Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 3850–3868. https://doi.org/10.18653/v1/2023.findings-acl.237   \n[223] Zechun Liu, Changsheng Zhao, Forrest Iandola, Chen Lai, Yuandong Tian, Igor Fedorov, Yunyang Xiong, Ernie Chang, Yangyang Shi, Raghuraman Krishnamoorthi, et al. 2024. Mobilellm: Optimizing sub-billion parameter language models for on-device use cases. arXiv preprint arXiv:2402.14905 (2024).   \n[224] Lin Long, Rui Wang, Ruixuan Xiao, Junbo Zhao, Xiao Ding, Gang Chen, and Haobo Wang. 2024. On LLMs-Driven Synthetic Data Generation, Curation, and Evaluation: A Survey. In Findings of the Association for Computational Linguistics ACL 2024. 11065–11082.   \n[225] Shayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. In International Conference on Machine Learning. PMLR, 22631–22648.   \n[226] Shayne Longpre, Robert Mahari, Anthony Chen, Naana Obeng-Marnu, Damien Sileo, William Brannon, Niklas Muennighoff, Nathan Khazam, Jad Kabbara, Kartik Perisetla, et al. 2023. The data provenance initiative: A large scale audit of dataset licensing & attribution in ai. arXiv preprint arXiv:2310.16787 (2023).   \n[227] Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. 2024. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173 (2024).   \n[228] Wenhao Lu, Jian Jiao, and Ruofei Zhang. 2020. Twinbert: Distilling knowledge to twin-structured compressed bert models for large-scale retrieval. In Proceedings of the 29th ACM International Conference on Information & Knowledge Management. 2645–2652.   \n[229] Zhenyan Lu, Xiang Li, Dongqi Cai, Rongjie Yi, Fangming Liu, Xiwen Zhang, Nicholas D Lane, and Mengwei Xu. 2024. Small language models: Survey, measurements, and insights. arXiv preprint arXiv:2409.15790 (2024).   \n[230] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in bioinformatics 23, 6 (2022), bbac409.   \n[231] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. 2024. The era of 1-bit llms: All large language models are in 1.58 bits. arXiv preprint arXiv:2402.17764 (2024).   \n[232] Xinyin Ma, Gongfan Fang, and Xinchao Wang. 2023. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems 36 (2023), 21702–21720.   \n[233] Xinbei Ma, Yeyun Gong, Pengcheng He, Hai Zhao, and Nan Duan. 2023. Query Rewriting in Retrieval-Augmented Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 5303–5315.   \n[234] Yubo Ma, Yixin Cao, YongChing Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! arXiv preprint arXiv:2303.08559 (2023).   \n[235] Yuhan Ma, Chenyou Fan, and Haiqi Jiang. 2023. Sci-cot: Leveraging large language models for enhanced knowledge distillation in small models for scientific qa. In 2023 9th International Conference on Computer and Communications (ICCC). IEEE, 2394–2398.   \n[236] YINGWEI MA, Yue Liu, Yue Yu, Yuanliang Zhang, Yu Jiang, Changjian Wang, and Shanshan Li. 2024. At Which Training Stage Does Code Data Help LLMs Reasoning?. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=KIPJKST4gw   \n[237] Ian Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, et al. 2023. Paloma: A benchmark for evaluating language model fit. arXiv preprint arXiv:2312.10523 (2023).   \n[238] Dakota Mahan, Ryan Carlow, Louis Castricato, Nathan Cooper, and Christian Laforte. [n. d.]. Stable Beluga models. [https://huggingface.co/ stabilityai/StableBeluga2](https://huggingface.co/stabilityai/StableBeluga2)   \n[239] Vladimir Malinovskii, Denis Mazur, Ivan Ilin, Denis Kuznedelev, Konstantin Burlachenko, Kai Yi, Dan Alistarh, and Peter Richtarik. 2024. PV-Tuning: Beyond Straight-Through Estimation for Extreme LLM Compression. arXiv preprint arXiv:2405.14852 (2024).   \n[240] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 9004–9017. https://doi.org/10.18653/v1/2023.emnlp-main.557   \n[241] Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, et al. 2024. Openelm: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II@ ICML2024.   \n[242] Dheeraj Mekala, Alex Nguyen, and Jingbo Shang. 2024. Smaller language models are capable of selecting instruction-tuning training data for larger language models. arXiv preprint arXiv:2402.10430 (2024).   \n[243] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. 2024. Shortgpt: Layers in large language models are more redundant than you expect. arXiv preprint arXiv:2403.03853 (2024).   \n[244] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text Generation. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 12076–12100. https://doi.org/10.18653/v1/2023.emnlp-main.741   \n[245] Go Min-su. 2024. Deep Learning Bible - 8. Large Language Models. WikiDocs. https://wikidocs.net/237419   \n[246] Eric Mitchell, Rafael Rafailov, Archit Sharma, Chelsea Finn, and Christopher D Manning. 2024. An Emulator for Fine-tuning Large Language Models using Small Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=Eo7kv0sllr   \n[247] Arindam Mitra, Luciano Del Corro, Shweti Mahajan, Andres Codas, Clarisse Simoes, Sahaj Agarwal, Xuxi Chen, Anastasia Razdaibiedina, Erik Jones, Kriti Aggarwal, et al. 2023. Orca 2: Teaching small language models how to reason. arXiv preprint arXiv:2311.11045 (2023).   \n[248] Lingbo Mo, Boshi Wang, Muhao Chen, and Huan Sun. 2024. How Trustworthy are Open-Source LLMs? An Assessment under Malicious Demonstrations Shows their Vulnerabilities. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). 2775–2792.   \n[249] John Xavier Morris, Wenting Zhao, Justin T Chiu, Vitaly Shmatikov, and Alexander M Rush. 2024. Language Model Inversion. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=t9dWHpGkPj   \n[250] Maximilian Mozes, Xuanli He, Bennett Kleinberg, and Lewis D Griffin. 2023. Use of llms for illicit purposes: Threats, prevention measures, and vulnerabilities. arXiv preprint arXiv:2308.12833 (2023).   \n[251] Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707 (2023).   \n[252] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. 2024. Compact language models via pruning and knowledge distillation. arXiv preprint arXiv:2407.14679 (2024).   \n[253] Rithesh Murthy, Liangwei Yang, Juntao Tan, Tulika Manoj Awalgaonkar, Yilun Zhou, Shelby Heinecke, Sachin Desai, Jason Wu, Ran Xu, Sarah Tan, Jianguo Zhang, Zhiwei Liu, Shirley Kokane, Zuxin Liu, Ming Zhu, Huan Wang, Caiming Xiong, and Silvio Savarese. 2024. MobileAIBench: Benchmarking LLMs and LMMs for On-Device Use Cases. arXiv:2406.10290 [cs.CL] https://arxiv.org/abs/2406.10290   \n[254] Kalyan Nakka, Jimmy Dani, and Nitesh Saxena. 2024. Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics in Small Language Models. arXiv preprint arXiv:2406.05364 (2024).   \n[255] Daye Nam, Andrew Macvean, Vincent Hellendoorn, Bogdan Vasilescu, and Brad Myers. 2024. Using an llm to help with code understanding. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 1–13.   \n[256] Piotr Nawrot, Adrian Łańcucki, Marcin Chochowski, David Tarjan, and Edoardo Ponti. 2024. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=tDRYrAkOB7   \n[257] Thuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A Rossi, and Thien Huu Nguyen. 2024. CulturaX: A Cleaned, Enormous, and Multilingual Dataset for Large Language Models in 167 Languages. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). 4226–4237.   \n[258] Tuan Dung Nguyen, Yuan-Sen Ting, Ioana Ciuca, Charles O’Neill, Ze-Chang Sun, Maja Jabłońska, Sandor Kruk, Ernest Perkowski, Jack Miller, Jason Jason Jingsh Li, et al. 2023. AstroLLaMA: Towards Specialized Foundation Models in Astronomy. In Proceedings of the Second Workshop on Information Extraction from Scientific Publications. 49–55.   \n[259] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, and Yinfei Yang. 2022. Large Dual Encoders Are Generalizable Retrievers. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 9844–9855. https://doi.org/10.18653/v1/2022.emnlp-main.669   \n[260] Rodrigo Nogueira and Kyunghyun Cho. 2019. Passage Re-ranking with BERT. arXiv preprint arXiv:1901.04085 (2019).   \n[261] A Noorian. 2024. A BERT-based sequential POI recommender system in social media. Computer Standards & Interfaces 87 (2024), 103766.   \n[262] OpenAI. 2024. GPT-4o mini: advancing cost-efficient intelligence. https://openai.com/index/gpt-4o-mini-advancing-cost-efficient-intelligence/ Accessed: 2024-7-18.   \n[263] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ Accessed: 2024-5-13.   \n[264] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in neural information processing systems 35 (2022), 27730–27744.   \n[265] Shankar Padmanabhan, Yasumasa Onoe, Michael Zhang, Greg Durrett, and Eunsol Choi. 2024. Propagating knowledge updates to lms through distillation. Advances in Neural Information Processing Systems 36 (2024).   \n[266] Jupinder Parmar, Shrimai Prabhumoye, Joseph Jennings, Mostofa Patwary, Sandeep Subramanian, Dan Su, Chen Zhu, Deepak Narayanan, Aastha Jhunjhunwala, Ayush Dattagupta, et al. 2024. Nemotron-4 15B Technical Report. arXiv preprint arXiv:2402.16819 (2024).   \n[267] Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. 2024. OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=jKHmjlpViu   \n[268] Guilherme Penedo, Hynek Kydlíček, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale. arXiv:2406.17557 [cs.CL]   \n[269] Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The RefinedWeb dataset for Falcon LLM: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116 (2023).   \n[270] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Leon Derczynski, Xingjian Du, Matteo Grella, Kranthi Gv, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartłomiej Koptyra, Hayden Lau, Jiaju Lin, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Guangyu Song, Xiangru Tang, Johan Wind, Stanisław Woźniak, Zhenyuan Zhang, Qinghua Zhou, Jian Zhu, and Rui-Jie Zhu. 2023. RWKV: Reinventing RNNs for the Transformer Era. In Findings of the Association for Computational Linguistics: EMNLP 2023, Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 14048–14077. https://doi.org/10.18653/v1/2023.findings-emnlp.936   \n[271] Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277 (2023).   \n[272] Zhiyuan Peng, Xuyang Wu, Qifan Wang, and Yi Fang. 2023. Soft prompt tuning for augmenting dense retrieval with large language models. arXiv preprint arXiv:2307.08303 (2023).   \n[273] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, Andy Jones, Anna Chen, Benjamin Mann, Brian Israel, Bryan Seethor, Cameron McKinnon, Christopher Olah, Da Yan, Daniela Amodei, Dario Amodei, Dawn Drain, Dustin Li, Eli Tran-Johnson, Guro Khundadze, Jackson Kernion, James Landis, Jamie Kerr, Jared Mueller, Jeeyoon Hyun, Joshua Landau, Kamal Ndousse, Landon Goldberg, Liane Lovitt, Martin Lucas, Michael Sellitto, Miranda Zhang, Neerav Kingsland, Nelson Elhage, Nicholas Joseph, Noemi Mercado, Nova DasSarma, Oliver Rausch, Robin Larson, Sam McCandlish, Scott Johnston, Shauna Kravec, Sheer El Showk, Tamera Lanham, Timothy Telleen-Lawton, Tom Brown, Tom Henighan, Tristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Jack Clark, Samuel R. Bowman, Amanda Askell, Roger Grosse, Danny Hernandez, Deep Ganguli, Evan Hubinger, Nicholas Schiefer, and Jared Kaplan. 2023. Discovering Language Model Behaviors with Model-Written Evaluations. In Findings of the Association for Computational Linguistics: ACL 2023. 13387–13434.   \n[274] Pascal Pfeiffer, Philipp Singer, Yauhen Babakhin, Gabor Fodor, Nischay Dhankhar, and Sri Satish Ambati. 2024. H2O-Danube3 Technical Report. arXiv preprint arXiv:2407.09276 (2024).   \n[275] Karmvir Singh Phogat, Sai Akhil Puranam, Sridhar Dasaratha, Chetan Harsha, and Shashishekar Ramakrishna. 2024. Fine-tuning Smaller Language Models for Question Answering over Financial Documents. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10528–10548. https://doi.org/10.18653/v1/2024.findings-emnlp.617   \n[276] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2023. Efficiently scaling transformer inference. Proceedings of Machine Learning and Systems 5 (2023), 606–624.   \n[277] Ofir Press, Noah Smith, and Mike Lewis. 2022. Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. In International Conference on Learning Representations. https://openreview.net/forum?id=R8sQPpGCv0   \n[278] Ruiyang Qin, Jun Xia, Zhenge Jia, Meng Jiang, Ahmed Abbasi, Peipei Zhou, Jingtong Hu, and Yiyu Shi. 2023. Enabling on-device large language model personalization with self-supervised data selection and synthesis. arXiv preprint arXiv:2311.12275 (2023).   \n[279] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. [n. d.]. ToolLLM: Facilitating Large Language Models to Master $1 6 0 0 0 +$ Real-world APIs. In The Twelfth International Conference on Learning Representations.   \n[280] Haohao $\\mathrm { Q u } .$ , Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, and Qing Li. 2024. A survey of mamba. arXiv preprint arXiv:2408.01129 (2024).   \n[281] Haohao Qu, Yifeng Zhang, Liangbo Ning, Wenqi Fan, and Qing Li. 2024. Ssd4rec: a structured state space duality model for efficient sequential recommendation. arXiv preprint arXiv:2409.01192 (2024).   \n[282] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog 1, 8 (2019), 9.   \n[283] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing Systems 36 (2024).   \n[284] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of machine learning research 21, 140 (2020), 1–67.   \n[285] Mohammad Wali Ur Rahman, Murad Mehrab Abrar, Hunter Gibbons Copening, Salim Hariri, Sicong Shao, Pratik Satam, and Soheil Salehi. 2023. Quantized Transformer Language Model Implementations on Edge Devices. arXiv:2310.03971 [cs.CL] https://arxiv.org/abs/2310.03971   \n[286] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis. IEEE, 1–16.   \n[287] Ori Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. 2023. In-Context RetrievalAugmented Language Models. Transactions of the Association for Computational Linguistics 11 (2023), 1316–1331.   \n[288] Krithika Ramesh, Arnav Chavan, Shrey Pandit, and Sunayana Sitaram. 2023. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 15762–15782. https://aclanthology.org/2023.acl-long.878   \n[289] Al Mamunur Rashid, George Karypis, and John Riedl. 2008. Learning preferences of new users in recommender systems: an information theoretic approach. Acm Sigkdd Explorations Newsletter 10, 2 (2008), 90–100.   \n[290] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana Riva, and Timothy Lillicrap. 2024. Androidinthewild: A large-scale dataset for android device control. Advances in Neural Information Processing Systems 36 (2024).   \n[291] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond. Foundations and Trends® in Information Retrieval 3, 4 (2009), 333–389.   \n[292] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950 (2023).   \n[293] Caitlin Sadowski and Greg Levin. 2007. Simhash: Hash-based similarity detection.   \n[294] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Commun. ACM 64, 9 (2021), 99–106.   \n[295] Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2024. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems 36 (2024).   \n[296] Rico Sennrich, Jannis Vamvas, and Alireza Mohammadshahi. 2023. Mitigating Hallucinations and Off-target Machine Translation with SourceContrastive and Language-Contrastive Decoding. arXiv preprint arXiv:2309.07098 (2023).   \n[297] Zeyang Sha and Yang Zhang. 2024. Prompt stealing attacks against large language models. arXiv preprint arXiv:2402.12959 (2024).   \n[298] Yu Shang, Yu Li, Fengli Xu, and Yong Li. 2024. Synergy-of-Thoughts: Eliciting Efficient Reasoning in Hybrid Language Models. arXiv preprint arXiv:2402.02563 (2024).   \n[299] Yuzhang Shang, Zhihang Yuan, Qiang Wu, and Zhen Dong. 2023. PB-LLM: Partially Binarized Large Language Models. arXiv:2310.00034 [cs.LG] https://arxiv.org/abs/2310.00034   \n[300] Hang Shao, Bei Liu, and Yanmin Qian. 2024. One-shot sensitivity-aware mixed sparsity pruning for large language models. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 11296–11300.   \n[301] Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R Bowman, Newton Cheng, Esin Durmus, Zac HatfieldDodds, Scott R Johnston, et al. 2023. Towards understanding sycophancy in language models. arXiv preprint arXiv:2310.13548 (2023).   \n[302] Erfan Shayegani, Md Abdullah Al Mamun, Yu Fu, Pedram Zaree, Yue Dong, and Nael Abu-Ghazaleh. 2023. Survey of vulnerabilities in large language models revealed by adversarial attacks. arXiv preprint arXiv:2310.10844 (2023).   \n[303] Noam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150 (2019).   \n[304] Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202 (2020).   \n[305] Bowen Shen, Zheng Lin, Yuanxin Liu, Zhengxiao Liu, Lei Wang, and Weiping Wang. 2022. COST-EFF: Collaborative Optimization of Spatial and Temporal Efficiency with Slenderized Multi-exit Language Models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 1719–1730. https://doi.org/10.18653/v1/2022.emnlp-main.112   \n[306] Bowen Shen, Zheng Lin, Daren Zha, Wei Liu, Jian Luan, Bin Wang, and Weiping Wang. 2024. Pruning Large Language Models to Intra-module Lowrank Architecture with Transitional Activations. In Findings of the Association for Computational Linguistics: ACL 2024, Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 9781–9793. https://doi.org/10.18653/v1/2024.findingsacl.582   \n[307] Weizhou Shen, Chenliang Li, Hongzhan Chen, Ming Yan, Xiaojun Quan, Hehong Chen, Ji Zhang, and Fei Huang. 2024. Small LLMs Are Weak Tool Learners: A Multi-LLM Agent. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Miami, Florida, USA, 16658–16680. https://doi.org/10.18653/v1/2024.emnlp-main.929   \n[308] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Beidi Chen, Percy Liang, Christopher Ré, Ion Stoica, and Ce Zhang. 2023. Flexgen: High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning. PMLR, 31094–31116.   \n[309] Wentao Shi, Xiangnan He, Yang Zhang, Chongming Gao, Xinyue Li, Jizhi Zhang, Qifan Wang, and Fuli Feng. 2024. Large language models are learnable planners for long-term recommendation. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. 1893–1903.   \n[310] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053 (2019).   \n[311] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K Reddy. 2024. Llm-sr: Scientific equation discovery via programming with large language models. arXiv preprint arXiv:2404.18400 (2024).   \n[312] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. 2022. Test-time prompt tuning for zero-shot generalization in vision-language models. Advances in Neural Information Processing Systems 35 (2022), 14274–14289.   \n[313] Karan Singhal, Shekoofeh Azizi, Tao Tu, S Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. 2023. Large language models encode clinical knowledge. Nature 620, 7972 (2023), 172–180.   \n[314] Daria Soboleva, Faisal Al-Khateeb, Robert Myers, Jacob R Steeves, Joel Hestness, and Nolan Dey. 2023. SlimPajama: A 627B token cleaned and deduplicated version of RedPajama. https://cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama. https://huggingface.co/datasets/cerebras/SlimPajama-627B   \n[315] Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Harsh Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research. arXiv preprint (2024).   \n[316] Xinying Song, Alex Salcianu, Yang Song, Dave Dopson, and Denny Zhou. 2021. Fast WordPiece Tokenization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 2089–2103.   \n[317] Sofia Eleni Spatharioti, David M Rothschild, Daniel G Goldstein, and Jake M Hofman. 2023. Comparing traditional and llm-based search for consumer choice: A randomized experiment. arXiv preprint arXiv:2307.03744 (2023).   \n[318] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. 2024. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing 568 (2024), 127063.   \n[319] Liangtai Sun, Yang Han, Zihan Zhao, Da Ma, Zhennan Shen, Baocai Chen, Lu Chen, and Kai Yu. 2024. Scieval: A multi-level large language model evaluation benchmark for scientific research. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 38. 19053–19061.   \n[320] Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, et al. 2024. Trustllm: Trustworthiness in large language models. arXiv preprint arXiv:2401.05561 (2024).   \n[321] Mingjie Sun, Zhuang Liu, Anna Bair, and J Zico Kolter. 2024. A Simple and Effective Pruning Approach for Large Language Models. In Proceedings of the Twelfth International Conference on Learning Representations, ICLR.   \n[322] Zhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu, Yiming Yang, and Denny Zhou. 2020. Mobilebert: a compact task-agnostic bert for resource-limited devices. arXiv preprint arXiv:2004.02984 (2020).   \n[323] Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, and Yejin Choi. 2020. Dataset Cartography: Mapping and Diagnosing Datasets with Training Dynamics. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 9275–9293.   \n[324] Alon Talmor and Jonathan Berant. 2018. The Web as a Knowledge-Base for Answering Complex Questions. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), Marilyn Walker, Heng Ji, and Amanda Stent (Eds.). Association for Computational Linguistics, New Orleans, Louisiana, 641–651. https://doi.org/10.18653/v1/N18-1059   \n[325] Jiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo, Kun Fang, and Ji-Rong Wen. 2024. Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs. arXiv preprint arXiv:2402.12052 (2024).   \n[326] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Boxi Cao, and Le Sun. 2023. Toolalpaca: Generalized tool learning for language models with 3000 simulated cases. arXiv preprint arXiv:2306.05301 (2023).   \n[327] Xuemei Tang, Jun Wang, and Qi Su. 2024. Small Language Model Is a Good Guide for Large Language Model in Chinese Entity Relation Extraction. arXiv preprint arXiv:2402.14373 (2024).   \n[328] Yehui Tang, Fangcheng Liu, Yunsheng Ni, Yuchuan Tian, Zheyuan Bai, Yi-Qi Hu, Sichao Liu, Shangling Jui, Kai Han, and Yunhe Wang. 2024. Rethinking optimization and architecture for tiny language models. arXiv preprint arXiv:2402.02791 (2024).   \n[329] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023. Stanford Alpaca: An Instruction-following LLaMA model. https://github.com/tatsu-lab/stanford_alpaca.   \n[330] Ross Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large language model for science. arXiv preprint arXiv:2211.09085 (2022).   \n[331] CodeGemma Team. 2024. Codegemma: Open code models based on gemma. arXiv preprint arXiv:2406.11409 (2024).   \n[332] Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295 (2024).   \n[333] Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024. Gemma 2: Improving open language models at a practical size. arXiv preprint arXiv:2408.00118 (2024).   \n[334] TensorOpera Team. 2024. TensorOpera Unveils Fox Foundation Model: A Pioneering Small Language Model (SLM) for Cloud and Edge. https: //blog.tensoropera.ai/tensoropera-unveils-fox-foundation-model-a-pioneering-open-source-slm-leading-the-way-against-tech-giants/ Accessed: 2024-6-13.   \n[335] Surat Teerapittayanon, Bradley McDanel, and Hsiang-Tsung Kung. 2016. Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR). IEEE, 2464–2469.   \n[336] Omkar Thawakar, Ashmal Vayani, Salman Khan, Hisham Cholakal, Rao M Anwer, Michael Felsberg, Tim Baldwin, Eric P Xing, and Fahad Shahbaz Khan. 2024. Mobillama: Towards accurate and lightweight fully transparent gpt. arXiv preprint arXiv:2402.16840 (2024).   \n[337] Ye Tian, Baolin Peng, Linfeng Song, Lifeng Jin, Dian Yu, Haitao Mi, and Dong Yu. 2024. Toward Self-Improvement of LLMs via Imagination, Searching, and Criticizing. arXiv preprint arXivko2024distillm:2404.12253 (2024).   \n[338] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971 (2023).   \n[339] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023).   \n[340] Jonathan Tow, Marco Bellagente, Dakota Mahan, and Carlos Riquelme. [n. d.]. StableLM 3B 4E1T. [https://huggingface.co/stabilityai/stablelm-3b4e1t](https://huggingface.co/stabilityai/stablelm-3b-4e1t)   \n[341] Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. arXiv preprint arXiv:1806.02847 (2018).   \n[342] Adina Trufinescu. 2024. Discover the New Multi-Lingual High-Quality Phi-3.5 SLMs. https://techcommunity.microsoft.com/t5/ai-azure-ai-servicesblog/discover-the-new-multi-lingual-high-quality-phi-3-5-slms/ba-p/4225280.   \n[343] Dennis Ulmer, Martin Gubri, Hwaran Lee, Sangdoo Yun, and Seong Joon Oh. 2024. Calibrating Large Language Models Using Their Generations Only. arXiv preprint arXiv:2403.05973 (2024).   \n[344] Sander Van Der Linden. 2022. Misinformation: susceptibility, spread, and interventions to immunize the public. Nature medicine 28, 3 (2022), 460–467.   \n[345] Chien Van Nguyen, Xuan Shen, Ryan Aponte, Yu Xia, Samyadeep Basu, Zhengmian Hu, Jian Chen, Mihir Parmar, Sasidhar Kunapuli, Joe Barrow, et al. 2024. A Survey of Small Language Models. arXiv preprint arXiv:2410.20011 (2024).   \n[346] A Vaswani. 2017. Attention is all you need. Advances in Neural Information Processing Systems (2017).   \n[347] Olga Veksler. 2023. Test time adaptation with regularized loss for weakly supervised salient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7360–7369.   \n[348] Paul Voigt and Axel Von dem Bussche. 2017. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing 10, 3152676 (2017), 10–5555.   \n[349] Yuxian Wan, Wenlin Zhang, and Zhen Li. 2023. Multi-Task Feature Self-Distillation for Semi-Supervised Machine Translation. In International Conference on Neural Information Processing. Springer, 238–254.   \n[350] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. 353–355.   \n[351] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. 2023. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. In Proceedings of the Annual Conference on Neural Information Processing Systems.   \n[352] Boxin Wang, Chejian Xu, Shuohang Wang, Zhe Gan, Yu Cheng, Jianfeng Gao, Ahmed Hassan Awadallah, and Bo Li. 2021. Adversarial glue: A multi-task benchmark for robustness evaluation of language models. arXiv preprint arXiv:2111.02840 (2021).   \n[353] Fali Wang, Runxue Bao, Suhang Wang, Wenchao Yu, Yanchi Liu, Wei Cheng, and Haifeng Chen. 2024. InfuserKI: Enhancing Large Language Models with Knowledge Graphs via Infuser-Guided Knowledge Integration. arXiv preprint arXiv:2402.11441 (2024).   \n[354] Fali Wang, Zheng Lin, Zhengxiao Liu, Mingyu Zheng, Lei Wang, and Daren Zha. 2021. Macrobert: Maximizing certified region of bert to adversarial word substitutions. In Database Systems for Advanced Applications: 26th International Conference, DASFAA 2021, Taipei, Taiwan, April 11–14, 2021, Proceedings, Part II 26. Springer, 253–261.   \n[355] Guan Wang, Sijie Cheng, Qiying Yu, and Changling Liu. 2023. OpenLLMs: Less is More for Open-source Models. https://doi.org/10.5281/zenodo.8105775   \n[356] Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024. OpenChat: Advancing Open-source Language Models with Mixed-Quality Data. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=AOJyfhWYHf   \n[357] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. 2023. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453 (2023).   \n[358] Jindong Wang, HU Xixu, Wenxin Hou, Hao Chen, Runkai Zheng, Yidong Wang, Linyi Yang, Wei Ye, Haojun Huang, Xiubo Geng, et al. [n. d.]. On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective. In ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models.   \n[359] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2024. Improving Text Embeddings with Large Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). Association for Computational Linguistics, 11897–11916. https://doi.org/10.18653/v1/2024.acl-long.642   \n[360] Peifeng Wang, Zhengyang Wang, Zheng Li, Yifan Gao, Bing Yin, and Xiang Ren. 2023. SCOTT: Self-Consistent Chain-of-Thought Distillation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 5546–5558.   \n[361] Wenxiao Wang, Wei Chen, Yicong Luo, Yongliu Long, Zhengkai Lin, Liye Zhang, Binbin Lin, Deng Cai, and Xiaofei He. 2024. Model compression and efficient inference for large language models: A survey. arXiv preprint arXiv:2402.09748 (2024).   \n[362] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers. arXiv:2002.10957 [cs.CL] https://arxiv.org/abs/2002.10957   \n[363] Xiaoxuan Wang, Ziniu Hu, Pan Lu, Yanqiao Zhu, Jieyu Zhang, Satyen Subramaniam, Arjun R Loomba, Shichang Zhang, Yizhou Sun, and Wei Wang. 2024. SciBench: Evaluating College-Level Scientific Problem-Solving Abilities of Large Language Models. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=bq1JEgioLr   \n[364] Yuxia Wang, Haonan Li, Xudong Han, Preslav Nakov, and Timothy Baldwin. 2024. Do-Not-Answer: Evaluating Safeguards in LLMs. In Findings of the Association for Computational Linguistics: EACL 2024. Association for Computational Linguistics, 896–911. https://aclanthology.org/2024.findingseacl.61   \n[365] Yile Wang, Peng Li, Maosong Sun, and Yang Liu. 2023. Self-Knowledge Guided Retrieval Augmentation for Large Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2023. 10303–10315.   \n[366] Yubo Wang, Xueguang Ma, and Wenhu Chen. 2023. Augmenting black-box llms with medical textbooks for clinical question answering. arXiv preprint arXiv:2309.02233 (2023).   \n[367] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi, Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-NaturalInstructions: Generalization via Declarative Instructions on $1 6 0 0 +$ NLP Tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 5085–5109. https://doi.org/10.18653/v1/2022.emnlp-main.340   \n[368] Yuyan Wang, Mohit Sharma, Can Xu, Sriraj Badam, Qian Sun, Lee Richardson, Lisa Chung, Ed H. Chi, and Minmin Chen. 2022. Surrogate for Long-Term User Experience in Recommender Systems. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Association for Computing Machinery, 4100–4109. https://doi.org/10.1145/3534678.3539073   \n[369] Yuling Wang, Changxin Tian, Binbin Hu, Yanhua Yu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou, Liang Pang, and Xiao Wang. 2024. Can Small Language Models be Good Reasoners for Sequential Recommendation?. In Proceedings of the ACM on Web Conference 2024. 3876–3887.   \n[370] Yuqing Wang and Yun Zhao. 2024. RUPBench: Benchmarking Reasoning Under Perturbations for Robustness Evaluation in Large Language Models. arXiv preprint arXiv:2406.11020 (2024).   \n[371] Zhepeng Wang, Runxue Bao, Yawen Wu, Jackson Taylor, Cao Xiao, Feng Zheng, Weiwen Jiang, Shangqian Gao, and Yanfu Zhang. 2024. Unlocking Memorization in Large Language Models with Dynamic Soft Prompting. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 9782–9796. https://doi.org/10.18653/v1/2024.emnlp-main.546   \n[372] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations. https://openreview.net/forum?id=gEZrGCozdqR   \n[373] Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. 2023. Magicoder: Source code is all you need. arXiv preprint arXiv:2312.02120 (2023).   \n[374] Johannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in Detoxifying Language Models. In Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics, 2447–2469. https://doi.org/10.18653/v1/2021.findings-emnlp.210   \n[375] Johannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text. 94–106.   \n[376] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2024. AutoDroid: LLM-powered Task Automation in Android. arXiv:2308.15272 [cs.AI] https://arxiv.org/abs/2308.15272   \n[377] Alexander Wettig, Aatmik Gupta, Saumya Malik, and Danqi Chen. 2024. QuRating: Selecting High-Quality Data for Training Language Models. In Forty-first International Conference on Machine Learning. https://openreview.net/forum?id=GLGYYqPwjy   \n[378] Adina Williams, Nikita Nangia, and Samuel Bowman. 2018. A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers). Association for Computational Linguistics, New Orleans, Louisiana, 1112–1122. https://doi.org/10.18653/v1/N18-1101   \n[379] Chuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng Huang. 2021. Empowering news recommendation with pre-trained language models. In Proceedings of the 44th international ACM SIGIR conference on research and development in information retrieval. 1652–1656.   \n[380] Minghao Wu, Abdul Waheed, Chiyu Zhang, Muhammad Abdul-Mageed, and Alham Fikri Aji. 2024. LaMini-LM: A Diverse Herd of Distilled Models from Large-Scale Instructions. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), Yvette Graham and Matthew Purver (Eds.). Association for Computational Linguistics, St. Julian’s, Malta, 944–964. https://aclanthology.org/2024.eacl-long.57   \n[381] Taiqiang Wu, Cheng Hou, Shanshan Lao, Jiayi Li, Ngai Wong, Zhe Zhao, and Yujiu Yang. 2024. Weight-Inherited Distillation for Task-Agnostic BERT Compression. In Findings of the Association for Computational Linguistics: NAACL 2024, Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 13–28. https://doi.org/10.18653/v1/2024.findings-naacl.2   \n[382] Xuansheng Wu, Huachi Zhou, Yucheng Shi, Wenlin Yao, Xiao Huang, and Ninghao Liu. 2024. Could Small Language Models Serve as Recommenders? Towards Data-centric Cold-start Recommendation. In Proceedings of the ACM on Web Conference 2024. 3566–3575.   \n[383] Zhuofeng Wu, He Bai, Aonan Zhang, Jiatao Gu, VG Vydiswaran, Navdeep Jaitly, and Yizhe Zhang. 2024. Divide-or-Conquer? Which Part Should You Distill Your LLM? arXiv preprint arXiv:2402.15000 (2024).   \n[384] Nuwa Xi, Yuhan Chen, Sendong Zhao, Haochun Wang, Bing Qin, and Ting Liu. 2024. AS-ES Learning: Towards Efficient CoT Learning in Small Models. arXiv preprint arXiv:2403.01969 (2024).   \n[385] Mengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi Chen. 2024. Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=09iOdaeOzp   \n[386] Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. 2023. Smoothquant: Accurate and efficient post-training quantization for large language models. In International Conference on Machine Learning. PMLR, 38087–38099.   \n[387] Tinghao Xie, Xiangyu Qi, Yi Zeng, Yangsibo Huang, Udari Madhushani Sehwag, Kaixuan Huang, Luxi He, Boyi Wei, Dacheng Li, Ying Sheng, et al. 2024. Sorry-bench: Systematically evaluating large language model safety refusal behaviors. arXiv preprint arXiv:2406.14598 (2024).   \n[388] Tong Xie, Yuwei Wan, Wei Huang, Zhenyu Yin, Yixuan Liu, Shaozhou Wang, Qingyuan Linghu, Chunyu Kit, Clara Grazian, Wenjie Zhang, et al. 2023. Darwin series: Domain specific large language models for natural science. arXiv preprint arXiv:2308.13565 (2023).   \n[389] Weikai Xie, Li Zhang, Shihe Wang, Rongjie Yi, and Mengwei Xu. 2024. DroidCall: A Dataset for LLM-powered Android Intent Invocation. arXiv preprint arXiv:2412.00402 (2024).   \n[390] Xuan Xie, Jiayang Song, Zhehua Zhou, Yuheng Huang, Da Song, and Lei Ma. 2024. Online Safety Analysis for LLMs: a Benchmark, an Assessment, and a Path Forward. arXiv preprint arXiv:2404.08517 (2024).   \n[391] Ji Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin. 2021. BERxiT: Early Exiting for BERT with Better Fine-Tuning and Extension to Regression. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, Paola Merlo, Jorg Tiedemann, and Reut Tsarfaty (Eds.). Association for Computational Linguistics, Online, 91–104. https://doi.org/10.18653/v1/2021.eacl-main.8   \n[392] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244 (2023).   \n[393] Canwen Xu, Yichong Xu, Shuohang Wang, Yang Liu, Chenguang Zhu, and Julian McAuley. 2023. Small models are valuable plug-ins for large language models. arXiv preprint arXiv:2305.08848 (2023).   \n[394] Daliang Xu, Wangsong Yin, Xin Jin, Ying Zhang, Shiyun Wei, Mengwei Xu, and Xuanzhe Liu. 2023. LLMCad: Fast and Scalable On-device Large Language Model Inference. arXiv:2309.04255 [cs.NI] https://arxiv.org/abs/2309.04255   \n[395] Daliang Xu, Hao Zhang, Liming Yang, Ruiqi Liu, Gang Huang, Mengwei Xu, and Xuanzhe Liu. 2024. Empowering 1000 tokens/second on-device llm prefilling with mllm-npu. arXiv preprint arXiv:2407.05858 (2024).   \n[396] Jiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun Zhao, Fangyuan Wang, and Hongwei Hao. 2015. Short Text Clustering via Convolutional Neural Networks. In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, Phil Blunsom, Shay Cohen, Paramveer Dhillon, and Percy Liang (Eds.). Association for Computational Linguistics, Denver, Colorado, 62–69. https://doi.org/10.3115/v1/W15-1509   \n[397] Minrui Xu, Niyato Dusit, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Dong In Kim, and Khaled B Letaief. 2024. When large language model agents meet 6G networks: Perception, grounding, and alignment. arXiv preprint arXiv:2401.07764 (2024).   \n[398] Mengwei Xu, Wangsong Yin, Dongqi Cai, Rongjie Yi, Daliang Xu, Qipeng Wang, Bingyang Wu, Yihao Zhao, Chen Yang, Shihe Wang, et al. 2024. A survey of resource-efficient llm and multimodal foundation models. arXiv preprint arXiv:2401.08092 (2024).   \n[399] Weijia Xu, Sweta Agrawal, Eleftheria Briakou, Marianna Martindale, and Marine Carpuat. 2023. Understanding and Detecting Hallucinations in Neural Machine Translation via Model Introspection. Transactions of the Association for Computational Linguistics 11 (2023), 546–564.   \n[400] Yuzhuang Xu, Xu Han, Zonghan Yang, Shuo Wang, Qingfu Zhu, Zhiyuan Liu, Weidong Liu, and Wanxiang Che. 2024. OneBit: Towards Extremely Low-bit Large Language Models. arXiv preprint arXiv:2402.11295 (2024).   \n[401] Shi-Qi Yan, Jia-Chen Gu, Yun Zhu, and Zhen-Hua Ling. 2024. Corrective retrieval augmented generation. arXiv preprint arXiv:2401.15884 (2024).   \n[402] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2 technical report. arXiv preprint arXiv:2407.10671 (2024).   \n[403] Chuanpeng Yang, Wang Lu, Yao Zhu, Yidong Wang, Qian Chen, Chenlong Gao, Bingjie Yan, and Yiqiang Chen. 2024. Survey on Knowledge Distillation for Large Language Models: Methods, Evaluation, and Application. arXiv preprint arXiv:2407.01885 (2024).   \n[404] Cheng Yang, Yang Sui, Jinqi Xiao, Lingyi Huang, Yu Gong, Yuanlin Duan, Wenqi Jia, Miao Yin, Yu Cheng, and Bo Yuan. 2024. MoE- $\\mathrm { I } ^ { 2 }$ : Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition. In Findings of the Association for Computational Linguistics: EMNLP 2024, Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10456–10466. https://doi.org/10.18653/v1/2024.findings-emnlp.612   \n[405] Kevin Yang, Dan Klein, Asli Celikyilmaz, Nanyun Peng, and Yuandong Tian. 2024. RLCD: Reinforcement Learning from Contrastive Distillation for LM Alignment. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=v3XXtxWKi6   \n[406] Kailai Yang, Tianlin Zhang, Ziyan Kuang, Qianqian Xie, Jimin Huang, and Sophia Ananiadou. 2024. MentaLLaMA: interpretable mental health analysis on social media with large language models. In Proceedings of the ACM on Web Conference 2024. 4489–4500.   \n[407] Linyi Yang, Shuibai Zhang, Libo Qin, Yafu Li, Yidong Wang, Hanmeng Liu, Jindong Wang, Xing Xie, and Yue Zhang. 2023. GLUE-X: Evaluating Natural Language Understanding Models from an Out-of-Distribution Generalization Perspective. In Findings of the Association for Computational Linguistics: ACL 2023. Association for Computational Linguistics, Toronto, Canada, 12731–12750. https://doi.org/10.18653/v1/2023.findings-acl.806   \n[408] Linyi Yang, Shuibai Zhang, Zhuohao Yu, Guangsheng Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Wei Ye, Xing Xie, Weizhu Chen, and Yue Zhang. 2024. Supervised Knowledge Makes Large Language Models Better In-context Learners. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=bAMPOUF227   \n[409] Runming Yang, Taiqiang Wu, Jiahao Wang, Pengfei Hu, Ngai Wong, and Yujiu Yang. 2024. LLM-Neo: Parameter Efficient Knowledge Distillation for Large Language Models. arXiv preprint arXiv:2411.06839 (2024).   \n[410] Yifei Yang, Zouying Cao, and Hai Zhao. 2024. Laco: Large language model pruning via layer collapse. arXiv preprint arXiv:2402.11187 (2024).   \n[411] Yu Yang, Siddhartha Mishra, Jeffrey N Chiang, and Baharan Mirzasoleiman. 2024. SmallToLarge (S2L): Scalable Data Selection for Fine-tuning Large Language Models by Summarizing Training Trajectories of Small Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/forum?id=K9IGlMQpif   \n[412] Yizhe Yang, Huashan Sun, Jiawei Li, Runheng Liu, Yinghao Li, Yuhang Liu, Heyan Huang, and Yang Gao. 2023. Mindllm: Pre-training lightweight large language model from scratch, evaluations and domain applications. arXiv preprint arXiv:2310.15777 (2023).   \n[413] Zhou Yang, Zhaochun Ren, Wang Yufeng, Shizhong Peng, Haizhou Sun, Xiaofei Zhu, and Xiangwen Liao. 2024. Enhancing Empathetic Response Generation by Augmenting LLMs with Small-scale Empathetic Models. arXiv preprint arXiv:2402.11801 (2024).   \n[414] Yunzhi Yao, Shaohan Huang, Wenhui Wang, Li Dong, and Furu Wei. 2021. Adapt-and-distill: Developing small, fast and effective pretrained language models for domains. arXiv preprint arXiv:2106.13474 (2021), 460–470.   \n[415] Mert Yazan, Suzan Verberne, and Frederik Situmeang. 2024. The Impact of Quantization on Retrieval-Augmented Generation: An Analysis of Small LLMs. arXiv preprint arXiv:2406.10251 (2024).   \n[416] Rongjie Yi, Liwei Guo, Shiyun Wei, Ao Zhou, Shangguang Wang, and Mengwei Xu. 2023. EdgeMoE: Fast On-Device Inference of MoE-based Large Language Models. arXiv:2308.14352 [cs.LG] https://arxiv.org/abs/2308.14352   \n[417] Rongjie Yi, Xiang Li, Weikai Xie, Zhenyan Lu, Chenghua Wang, Ao Zhou, Shangguang Wang, Xiwen Zhang, and Mengwei Xu. 2024. PhoneLM: an Efficient and Capable Small Language Model Family through Principled Pre-training. arXiv preprint arXiv:2411.05046 (2024).   \n[418] Wen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The Value of Semantic Parse Labeling for Knowledge Base Question Answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), Katrin Erk and Noah A. Smith (Eds.). Association for Computational Linguistics, Berlin, Germany, 201–206. https://doi.org/10.18653/v1/P16-2033   \n[419] Wangsong Yin, Mengwei Xu, Yuanchun Li, and Xuanzhe Liu. 2024. LLM as a System Service on Mobile Devices. arXiv:2403.11805 [cs.OS] https://arxiv.org/abs/2403.11805   \n[420] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2024. MetaMath: Bootstrap Your Own Mathematical Questions for Large Language Models. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=N8N0hgNDRt   \n[421] Yue Yu, Wei Ping, Zihan Liu, Boxin Wang, Jiaxuan You, Chao Zhang, Mohammad Shoeybi, and Bryan Catanzaro. 2024. Rankrag: Unifying context ranking with retrieval-augmented generation in llms. arXiv preprint arXiv:2407.02485 (2024).   \n[422] Zhongzhi Yu, Zheng Wang, Yuhan Li, Haoran You, Ruijie Gao, Xiaoya Zhou, Sreenidhi Reedy Bommu, Yang Katie Zhao, and Yingyan Celine Lin. 2024. EDGE-LLM: Enabling Efficient Large Language Model Adaptation on Edge Devices via Layerwise Unified Compression and Adaptive Layer Tuning and Voting. arXiv preprint arXiv:2406.15758 (2024).   \n[423] Jinliang Yuan, Chen Yang, Dongqi Cai, Shihe Wang, Xin Yuan, Zeling Zhang, Xiang Li, Dingge Zhang, Hanzi Mei, Xianqing Jia, et al. 2024. Mobile Foundation Model as Firmware. In Proceedings of the 30th Annual International Conference on Mobile Computing and Networking. 279–295.   \n[424] Sha Yuan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao Liu, Yukuo Cen, Xu Zou, Zhilin Yang, and Jie Tang. 2021. Wudaocorpora: A super large-scale chinese corpora for pre-training language models. AI Open 2 (2021), 65–68.   \n[425] Xiaohan Yuan, Jinfeng Li, Dongxia Wang, Yuefeng Chen, Xiaofeng Mao, Longtao Huang, Hui Xue, Wenhai Wang, Kui Ren, and Jingyi Wang. 2024. S-Eval: Automatic and Adaptive Test Generation for Benchmarking Safety Evaluation of Large Language Models. arXiv preprint arXiv:2405.14191 (2024).   \n[426] Youliang Yuan, Wenxiang Jiao, Wenxuan Wang, Jen tse Huang, Pinjia He, Shuming Shi, and Zhaopeng Tu. 2024. GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id= MbfAK4s61A   \n[427] Shengbin Yue, Wei Chen, Siyuan Wang, Bingxuan Li, Chenchen Shen, Shujun Liu, Yuxuan Zhou, Yao Xiao, Song Yun, Wei Lin, et al. 2023. Disc-lawllm: Fine-tuning large language models for intelligent legal services. arXiv preprint arXiv:2309.11325 (2023).   \n[428] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. [n. d.]. Mammoth: Building math generalist models through hybrid instruction tuning, 2023. URL https://arxiv. org/abs/2309.05653 ([n. d.]).   \n[429] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. HellaSwag: Can a Machine Really Finish Your Sentence?. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. 4791–4800.   \n[430] Rowan Zellers, Ari Holtzman, Hannah Rashkin, Yonatan Bisk, Ali Farhadi, Franziska Roesner, and Yejin Choi. 2019. Defending against neural fake news. Advances in neural information processing systems 32 (2019).   \n[431] Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems 32 (2019).   \n[432] Cheng Zhang, Jianyi Cheng, George A Constantinides, and Yiren Zhao. 2024. LQER: Low-Rank Quantization Error Reconstruction for LLMs. arXiv preprint arXiv:2402.02446 (2024).   \n[433] Collin Zhang, John X Morris, and Vitaly Shmatikov. 2024. Extracting Prompts by Inverting LLM Outputs. arXiv preprint arXiv:2405.15012 (2024).   \n[434] Chen Zhang, Dawei Song, Zheyu Ye, and Yan Gao. 2023. Towards the law of capacity gap in distilling language models. arXiv preprint arXiv:2311.07052 (2023).   \n[435] Dan Zhang, Ziniu Hu, Sining Zhoubian, Zhengxiao Du, Kaiyu Yang, Zihan Wang, Yisong Yue, Yuxiao Dong, and Jie Tang. 2024. Sciglm: Training scientific language models with self-reflective instruction annotation and tuning. arXiv preprint arXiv:2401.07950 (2024).   \n[436] Di Zhang, Wei Liu, Qian Tan, Jingdan Chen, Hang Yan, Yuliang Yan, Jiatong Li, Weiran Huang, Xiangyu Yue, Dongzhan Zhou, et al. 2024. Chemllm: A chemical large language model. arXiv preprint arXiv:2402.06852 (2024).   \n[437] Kaiyan Zhang, Jianyu Wang, Ermo Hua, Biqing Qi, Ning Ding, and Bowen Zhou. 2024. Cogenesis: A framework collaborating large and small language models for secure context-aware instruction following. arXiv preprint arXiv:2403.03129 (2024).   \n[438] Mingyang Zhang, Hao Chen, Chunhua Shen, Zhen Yang, Linlin Ou, Xinyi Yu, and Bohan Zhuang. 2023. Loraprune: Pruning meets low-rank parameter-efficient fine-tuning. arXiv preprint arXiv:2305.18403 (2023).   \n[439] Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, and Wei Lu. 2024. TinyLlama: An Open-Source Small Language Model. arXiv:2401.02385 [cs.CL] https://arxiv.org/abs/2401.02385   \n[440] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068 (2022).   \n[441] Xinran Zhang, Xin Yuan, Yunwei Li, and Yanru Zhang. 2019. Cold-Start representation learning: A recommendation approach with bert4Movie and movie2Vec. In Proceedings of the 27th ACM International Conference on Multimedia. 2612–2616.   \n[442] Yingtao Zhang, Haoli Bai, Haokun Lin, Jialin Zhao, Lu Hou, and Carlo Vittorio Cannistraci. 2024. Plug-and-play: An efficient post-training pruning method for large language models. In The Twelfth International Conference on Learning Representations.   \n[443] Yiming Zhang, Nicholas Carlini, and Daphne Ippolito. 2024. Effective Prompt Extraction from Language Models. arXiv:2307.06865 [cs.CL] https://arxiv.org/abs/2307.06865   \n[444] Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Ré, Clark Barrett, et al. 2024. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems 36 (2024).   \n[445] Zhiwei Zhang, Fali Wang, Xiaomin Li, Zongyu Wu, Xianfeng Tang, Hui Liu, Qi He, Wenpeng Yin, and Suhang Wang. 2024. Does your LLM truly unlearn? An embarrassingly simple approach to recover unlearned knowledge. arXiv preprint arXiv:2410.16454 (2024).   \n[446] Bowen Zhao, Hannaneh Hajishirzi, and Qingqing Cao. 2024. APT: Adaptive Pruning and Tuning Pretrained Language Models for Efficient Training and Inference. In Forty-first International Conference on Machine Learning.   \n[447] Junchen Zhao, Yurun Song, Simeng Liu, Ian G. Harris, and Sangeetha Abdu Jyothi. 2023. LinguaLinked: A Distributed Large Language Model Inference System for Mobile Devices. arXiv:2312.00388 [cs.LG] https://arxiv.org/abs/2312.00388   \n[448] Juntao Zhao, Borui Wan, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. LLM-PQ: Serving LLM on Heterogeneous Clusters with Phase-Aware Partition and Adaptive Quantization. arXiv preprint arXiv:2403.01136 (2024).   \n[449] Kun Zhao, Bohao Yang, Chen Tang, Chenghua Lin, and Liang Zhan. 2024. SLIDE: A Framework Integrating Small and Large Language Models for Open-Domain Dialogues Evaluation. arXiv preprint arXiv:2405.15924 (2024).   \n[450] Theodore Zhao, Mu Wei, J Samuel Preston, and Hoifung Poon. 2023. Automatic calibration and error correction for large language models via pareto optimal self-supervision. arXiv preprint arXiv:2306.16564 (2023).   \n[451] Youpeng Zhao, Ming Lin, Huadong Tang, Qiang Wu, and Jun Wang. 2024. Merino: Entropy-driven Design for Generative Language Models on IoT Devices. arXiv:2403.07921 [cs.LG] https://arxiv.org/abs/2403.07921   \n[452] Zhengyun Zhao, Qiao Jin, Fangyuan Chen, Tuorui Peng, and Sheng Yu. 2022. Pmc-patients: A large-scale dataset of patient summaries and relations for benchmarking retrieval-based clinical decision support systems. arXiv preprint arXiv:2202.13876 (2022).   \n[453] Zhanhui Zhou, Zhixuan Liu, Jie Liu, Zhichen Dong, Chao Yang, and Yu Qiao. 2024. Weak-to-Strong Search: Align Large Language Models via Searching over Small Language Models. In The Thirty-eighth Annual Conference on Neural Information Processing Systems. https://openreview.net/ forum?id=dOJ6CqWDf1   \n[454] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: A Question Answering Benchmark on a Hybrid of Tabular and Textual Content in Finance. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 3277–3287.   \n[455] Jiachen Zhu, Jianghao Lin, Xinyi Dai, Bo Chen, Rong Shan, Jieming Zhu, Ruiming Tang, Yong Yu, and Weinan Zhang. 2024. Lifelong Personalized Low-Rank Adaptation of Large Language Models for Recommendation. arXiv preprint arXiv:2408.03533 (2024).   \n[456] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Yue Zhang, Neil Zhenqiang Gong, et al. 2023. Promptbench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv preprint arXiv:2306.04528 (2023).   \n[457] Xunyu Zhu, Jian Li, Yong Liu, Can Ma, and Weiping Wang. 2023. A survey on model compression for large language models. arXiv preprint arXiv:2308.07633 (2023).   \n[458] Yun Zhu, Yinxiao Liu, Felix Stahlberg, Shankar Kumar, Yu hui Chen, Liangchen Luo, Lei Shu, Renjie Liu, Jindong Chen, and Lei Meng. 2023. Towards an On-device Agent for Text Rewriting. arXiv:2308.11807 [cs.CL] https://arxiv.org/abs/2308.11807   \n[459] Yuanyuan Zhuang and Jaekyeong Kim. 2021. A bert-based multi-criteria recommender system for hotel promotion management. Sustainability 13, 14 (2021), 8039.   \n[460] Terry Yue Zhuo, Yujin Huang, Chunyang Chen, and Zhenchang Xing. 2023. Red teaming chatgpt via jailbreaking: Bias, robustness, reliability and toxicity. arXiv preprint arXiv:2301.12867 (2023).   \n[461] Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. arXiv preprint arXiv:2307.15043 (2023).   \n[462] Lixin Zou, Weixue Lu, Yiding Liu, Hengyi Cai, Xiaokai Chu, Dehong Ma, Daiting Shi, Yu Sun, Zhicong Cheng, Simiu Gu, et al. 2022. Pre-trained language model-based retrieval and ranking for web search. ACM Transactions on the Web 17, 1 (2022), 1–36.   \n[463] Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hakim Hacid. 2024. Falcon mamba: The first competitive attention-free 7b language model. arXiv preprint arXiv:2410.05355 (2024). ",
        "page_idx": 59
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 60
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 61
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 62
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 63
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 64
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 65
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 66
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 67
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 68
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 69
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 70
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 71
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 72
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 73
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 74
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 75
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 76
    },
    {
        "type": "text",
        "text": "",
        "page_idx": 77
    }
]