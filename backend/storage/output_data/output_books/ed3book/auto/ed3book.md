# Speech and Language Processing

An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models

Third Edition draft

Daniel Jurafsky Stanford University

James H. Martin University of Colorado at Boulder

Copyright ©2024. All rights reserved.

Draft of January 12, 2025. Comments and typos welcome!

# Summary of Contents

# I Fundamental Algorithms for NLP 1

1 Introduction . . . 3   
2 Regular Expressions, Tokenization, Edit Distance . . 4   
3 N-gram Language Models . . 3256   
4 Naive Bayes, Text Classification, and Sentiment . . .   
5 Logistic Regression . . 77   
6 Vector Semantics and Embeddings . 101   
7 Neural Networks . . 132   
8 RNNs and LSTMs . . 158   
9 The Transformer . . 184   
10 Large Language Models . . . 203   
11 Masked Language Models . . 223   
12 Model Alignment, Prompting, and In-Context Learning . 242

# II NLP Applications 261

13 Machine Translation . . 263   
14 Question Answering, Information Retrieval, and RAG . 289   
15 Chatbots & Dialogue Systems . . . 309   
16 Automatic Speech Recognition and Text-to-Speech . . 331

# III Annotating Linguistic Structure 359

17 Sequence Labeling for Parts of Speech and Named Entities . . . . . . 362   
18 Context-Free Grammars and Constituency Parsing . . 387   
19 Dependency Parsing . . . . 411   
20 Information Extraction: Relations, Events, and Time. . . . 435   
21 Semantic Role Labeling . . . 461   
22 Lexicons for Sentiment, Affect, and Connotation . . 481   
23 Coreference Resolution and Entity Linking . . 501   
24 Discourse Coherence. . . 531   
Bibliography. . . . 553   
Subject Index . . . 585

# Contents

# I Fundamental Algorithms for NLP 1

1 Introduction 3

# 2 Regular Expressions, Tokenization, Edit Distance 4

2.1 Regular Expressions . 51315   
2.2 Words   
2.3 Corpora   
2.4 Simple Unix Tools for Word Tokenization 16   
2.5 Word and Subword Tokenization 18   
2.6 Word Normalization, Lemmatization and Stemming   
2.7 Sentence Segmentation   
2.8 Minimum Edit Distance   
2.9 Summary .   
Bibliographical and Historical Notes 3030   
Exercises

# 3 N-gram Language Models 32

3.1 N-Grams .   
3.2 Evaluating Language Models: Training and Test Sets   
3.3 Evaluating Language Models: Perplexity .   
3.4 Sampling sentences from a language model .   
3.5 Generalizing vs. overfitting the training set   
3.6 Smoothing, Interpolation, and Backoff   
3.7 Advanced: Perplexity’s Relation to Entropy   
3.8 Summary . 52   
Bibliographical and Historical Notes 5254   
Exercises

# 4 Naive Bayes, Text Classification, and Sentiment 56

4.1 Naive Bayes Classifiers 5760   
4.2 Training the Naive Bayes Classifier   
4.3 Worked example . . 61   
4.4 Optimizing for Sentiment Analysis 626465   
4.5 Naive Bayes for other text classification tasks   
4.6 Naive Bayes as a Language Model   
4.7 Evaluation: Precision, Recall, F-measure 66   
4.8 Test sets and Cross-validation 69   
4.9 Statistical Significance Testing 70   
4.10 Avoiding Harms in Classification   
4.11 Summary .   
Bibliographical and Historical Notes   
Exercises

# 5 Logistic Regression 77

5.1 The sigmoid function 7880   
5.2 Classification with Logistic Regression   
5.3 Multinomial logistic regression 84   
5.4 Learning in Logistic Regression 87   
5.5 The cross-entropy loss function 88   
5.6 Gradient Descent 89   
5.7 Regularization 95   
5.8 Learning in Multinomial Logistic Regression . 97   
5.9 Interpreting models 98   
5.10 Advanced: Deriving the Gradient Equation 98   
5.11 Summary . 99   
Bibliographical and Historical Notes 100   
Exercises 100

# 6 Vector Semantics and Embeddings 101

6.1 Lexical Semantics 102   
6.2 Vector Semantics 105   
6.3 Words and Vectors . 106   
6.4 Cosine for measuring similarity 110   
6.5 TF-IDF: Weighing terms in the vector 111   
6.6 Pointwise Mutual Information (PMI) 114   
6.7 Applications of the tf-idf or PPMI vector models 116   
6.8 Word2vec 117   
6.9 Visualizing Embeddings 123   
6.10 Semantic properties of embeddings 124   
6.11 Bias and Embeddings 126   
6.12 Evaluating Vector Models 127   
6.13 Summary . 128   
Bibliographical and Historical Notes 129   
Exercises 131

# 7 Neural Networks 132

7.1 Units . . 133   
7.2 The XOR problem 135   
7.3 Feedforward Neural Networks . 138   
7.4 Feedforward networks for NLP: Classification 142   
7.5 Training Neural Nets 145   
7.6 Feedforward Neural Language Modeling 152   
7.7 Training the neural language model 155   
7.8 Summary . 156   
Bibliographical and Historical Notes 157

# 8 RNNs and LSTMs 158

8.1 Recurrent Neural Networks 158   
8.2 RNNs as Language Models 162   
8.3 RNNs for other NLP tasks . 165   
8.4 Stacked and Bidirectional RNN architectures 168   
8.5 The LSTM . 171   
8.6 Summary: Common RNN NLP Architectures 174   
8.7 The Encoder-Decoder Model with RNNs 174   
8.8 Attention . 179   
8.9 Summary . 181   
Bibliographical and Historical Notes 182

# 9 The Transformer 184

9.1 Attention . . 185

9.2 Transformer Blocks . . 191   
9.3 Parallelizing computation using a single matrix $\pmb { \times }$ 194   
9.4 The input: embeddings for token and position 197   
9.5 The Language Modeling Head 199   
9.6 Summary . 201   
Bibliographical and Historical Notes 202

# 10 Large Language Models 203

10.1 Large Language Models with Transformers . 204   
10.2 Sampling for LLM Generation 207   
10.3 Pretraining Large Language Models 210   
10.4 Evaluating Large Language Models 214   
10.5 Dealing with Scale . . 216   
10.6 Potential Harms from Language Models 219   
10.7 Summary . . 220   
Bibliographical and Historical Notes 220

# 11 Masked Language Models 223

11.1 Bidirectional Transformer Encoders . . 223   
11.2 Training Bidirectional Encoders . 226   
11.3 Contextual Embeddings 231   
11.4 Fine-Tuning for Classification 235   
11.5 Fine-Tuning for Sequence Labelling: Named Entity Recognition 237   
11.6 Summary . . 241   
Bibliographical and Historical Notes 241

# 12 Model Alignment, Prompting, and In-Context Learning 242

12.1 Prompting . . 243   
12.2 Post-training and Model Alignment . 248   
12.3 Model Alignment: Instruction Tuning . 249   
12.4 Chain-of-Thought Prompting 254   
12.5 Automatic Prompt Optimization . 254   
12.6 Evaluating Prompted Language Models . 258   
12.7 Model Alignment with Human Preferences: RLHF and DPO 258   
12.8 Summary . 259   
Bibliographical and Historical Notes 259

# II NLP Applications 261

# 13 Machine Translation 263

13.1 Language Divergences and Typology . . 264   
13.2 Machine Translation using Encoder-Decoder 268   
13.3 Details of the Encoder-Decoder Model 272   
13.4 Decoding in MT: Beam Search 274   
13.5 Translating in low-resource situations . 278   
13.6 MT Evaluation . 280   
13.7 Bias and Ethical Issues 284   
13.8 Summary . 285   
Bibliographical and Historical Notes 286   
Exercises 288

# 14 Question Answering, Information Retrieval, and RAG

14.1 Information Retrieval 290   
14.2 Information Retrieval with Dense Vectors . 298   
14.3 Answering Questions with RAG 301   
14.4 Evaluating Question Answering 304   
14.5 Summary . 306   
Bibliographical and Historical Notes 306   
Exercises 308

# 15 Chatbots & Dialogue Systems 309

15.1 Properties of Human Conversation 311   
15.2 Frame-Based Dialogue Systems . 314   
15.3 Dialogue Acts and Dialogue State . 317   
15.4 Chatbots . 321   
15.5 Dialogue System Design . 325   
15.6 Summary . 327   
Bibliographical and Historical Notes 328   
Exercises 330

# 16 Automatic Speech Recognition and Text-to-Speech 331

16.1 The Automatic Speech Recognition Task . 332   
16.2 Feature Extraction for ASR: Log Mel Spectrum 334   
16.3 Speech Recognition Architecture 339   
16.4 CTC 341   
16.5 ASR Evaluation: Word Error Rate 346   
16.6 TTS 348   
16.7 Other Speech Tasks 353   
16.8 Summary . 354   
Bibliographical and Historical Notes 354   
Exercises 357

# III Annotating Linguistic Structure 359

# 17 Sequence Labeling for Parts of Speech and Named Entities 362

17.1 (Mostly) English Word Classes 363   
17.2 Part-of-Speech Tagging 365   
17.3 Named Entities and Named Entity Tagging 367   
17.4 HMM Part-of-Speech Tagging 369   
17.5 Conditional Random Fields (CRFs) 376   
17.6 Evaluation of Named Entity Recognition 381   
17.7 Further Details 381   
17.8 Summary . 383   
Bibliographical and Historical Notes 384   
Exercises 385

# 18 Context-Free Grammars and Constituency Parsing 387

18.1 Constituency . . 388   
18.2 Context-Free Grammars 388   
18.3 Treebanks 392   
18.4 Grammar Equivalence and Normal Form 394   
18.5 Ambiguity . 395   
18.6 CKY Parsing: A Dynamic Programming Approach 397   
18.7 Span-Based Neural Constituency Parsing . 403

# 18.8 Evaluating Parsers . . 405

18.9 Heads and Head-Finding 406   
18.10 Summary . 407   
Bibliographical and Historical Notes 408   
Exercises 409

# 19 Dependency Parsing 411

19.1 Dependency Relations . 412   
19.2 Transition-Based Dependency Parsing 416   
19.3 Graph-Based Dependency Parsing 425   
19.4 Evaluation 431   
19.5 Summary . 432   
Bibliographical and Historical Notes 433   
Exercises 434

# 20 Information Extraction: Relations, Events, and Time 435

20.1 Relation Extraction 436   
20.2 Relation Extraction Algorithms 438   
20.3 Extracting Events 446   
20.4 Representing Time . 447   
20.5 Representing Aspect . 450   
20.6 Temporally Annotated Datasets: TimeBank . 451   
20.7 Automatic Temporal Analysis . 452   
20.8 Template Filling 456   
20.9 Summary . 458   
Bibliographical and Historical Notes 459   
Exercises 460

# 21 Semantic Role Labeling 461

21.1 Semantic Roles 462   
21.2 Diathesis Alternations 462   
21.3 Semantic Roles: Problems with Thematic Roles 464   
21.4 The Proposition Bank 465   
21.5 FrameNet 466   
21.6 Semantic Role Labeling 468   
21.7 Selectional Restrictions 472   
21.8 Primitive Decomposition of Predicates 476   
21.9 Summary . 477   
Bibliographical and Historical Notes 478   
Exercises 480

# 22 Lexicons for Sentiment, Affect, and Connotation 481

22.1 Defining Emotion 482   
22.2 Available Sentiment and Affect Lexicons 484   
22.3 Creating Affect Lexicons by Human Labeling 485   
22.4 Semi-supervised Induction of Affect Lexicons 487   
22.5 Supervised Learning of Word Sentiment 490   
22.6 Using Lexicons for Sentiment Recognition 495   
22.7 Using Lexicons for Affect Recognition 496   
22.8 Lexicon-based methods for Entity-Centric Affect . 497   
22.9 Connotation Frames 497   
22.10 Summary . 499   
Bibliographical and Historical Notes 500   
Exercises 500

# 23 Coreference Resolution and Entity Linking 501

23.1 Coreference Phenomena: Linguistic Background . 504   
23.2 Coreference Tasks and Datasets 509   
23.3 Mention Detection . 510   
23.4 Architectures for Coreference Algorithms 513   
23.5 Classifiers using hand-built features . 515   
23.6 A neural mention-ranking algorithm 517   
23.7 Entity Linking . 520   
23.8 Evaluation of Coreference Resolution . 524   
23.9 Winograd Schema problems . 525   
23.10 Gender Bias in Coreference 526   
23.11 Summary . 527   
Bibliographical and Historical Notes 528   
Exercises 530

# 24 Discourse Coherence 531

24.1 Coherence Relations . 533   
24.2 Discourse Structure Parsing 536   
24.3 Centering and Entity-Based Coherence 540   
24.4 Representation learning models for local coherence 544   
24.5 Global Coherence 546   
24.6 Summary . 549   
Bibliographical and Historical Notes 550   
Exercises 552

# Bibliography 553

Subject Index 585

# Part I FUNDAMENTAL ALGORITHMS FOR NLP

In the first part of the book we introduce the fundamental suite of algorithmic tools that make up the modern neural language model that is the heart of end-to-end NLP systems. We begin with tokenization and preprocessing, as well as useful algorithms like computing edit distance, and then proceed to the tasks of classification, logistic regression, neural networks, proceeding through feedforward networks, recurrent networks, and then transformers. We’ll also see the role of embeddings as a model of word meaning.

# 1

# Introduction

La derniere chose qu’on trouve en faisant un ouvrage est de savoir celle qu’il faut \` mettre la premiere. \` [The last thing you figure out in writing a book is what to put first.] Pascal

# 2

# Regular Expressions, Tokenization, Edit Distance

User: I am unhappy.   
ELIZA: DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY User: I need some help, that much seems certain.   
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP   
User: Perhaps I could learn to get along with my mother. ELIZA: TELL ME MORE ABOUT YOUR FAMILY   
User: My mother takes care of me.   
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU   
User: My father.   
ELIZA: YOUR FATHER   
User: You are like my father in some ways.

Weizenbaum (1966)

chatbots

The dialogue above is from ELIZA, an early natural language processing system that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like “I need X” and translate them into suitable outputs like “What would it mean to you if you got X?”. This simple technique succeeds in this domain because ELIZA doesn’t actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. ELIZA’s mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA’s abilities even after the program’s operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion.

Of course modern conversational agents are much more than a diversion; they can answer questions, book flights, or find restaurants, functions for which they rely on a much more sophisticated understanding of the user’s intent, as we will see in Chapter 15. Nonetheless, the simple pattern-based methods that powered ELIZA and other chatbots play a crucial role in natural language processing.

text normalization

We’ll begin with the most important tool for describing text patterns: the regular expression. Regular expressions can be used to specify strings we might want to extract from a document, from transforming “I need X” in ELIZA above, to defining strings like $\$ 199$ or $\$ 24.99$ for extracting tables of prices from a document.

We’ll then turn to a set of tasks collectively called text normalization, in which regular expressions play an important part. Normalizing text means converting it to a more convenient, standard form. For example, most of what we are going to do with language relies on first separating out or tokenizing words or word parts from running text, the task of tokenization. English words are often separated from each other by whitespace, but whitespace is not always sufficient. New York and rock ’n’ roll are sometimes treated as large words despite the fact that they contain spaces, while sometimes we’ll need to separate $I ' m$ into the two words $I$ and am. For processing tweets or texts we’ll need to tokenize emoticons like :) or hashtags

lemmatization

like #nlproc. Some languages, like Japanese, don’t have spaces between words, so word tokenization becomes more difficult. And as we’ll see, for large language models we’ll use tokens that range greatly in size, from letters to subwords (parts of words) to words and even sometimes short phrases.

stemming sentence segmentation

Another part of text normalization is lemmatization, the task of determining that two words have the same root, despite their surface differences. For example, the words sang, sung, and sings are forms of the verb sing. The word sing is the common lemma of these words, and a lemmatizer maps from all of these to sing. Lemmatization is essential for processing morphologically complex languages like Arabic. Stemming refers to a simpler version of lemmatization in which we mainly just strip suffixes from the end of the word. Text normalization also includes sentence segmentation: breaking up a text into individual sentences, using cues like periods or exclamation points.

Finally, we’ll need to compare words and other strings. We’ll introduce a metric called edit distance that measures how similar two strings are based on the number of edits (insertions, deletions, substitutions) it takes to change one string into the other. Edit distance is an algorithm with applications throughout language processing, from spelling correction to speech recognition to coreference resolution.

# 2.1 Regular Expressions

# regular expression

# corpus

One of the most useful tools for text processing in computer science has been the regular expression (often shortened to regex), a language for specifying text search strings. This practical language is used in every computer language, in text processing tools like the Unix tools grep, and in editors like vim or Emacs. Formally, a regular expression is an algebraic notation for characterizing a set of strings. Regular expressions are particularly useful for searching in texts, when we have a pattern to search for and a corpus of texts to search through. A regular expression search function will search through the corpus, returning all texts that match the pattern. The corpus can be a single document or a collection. For example, the Unix command-line tool grep takes a regular expression and returns every line of the input document that matches the expression.

A search can be designed to return every match on a line, if there are more than one, or just the first match. In the following examples we generally underline the exact string that matches the regular expression and show only the first match. We’ll show regular expressions delimited by slashes but note that slashes are not part of the regular expressions.

Regular expressions come in many variants. We’ll be describing extended regular expressions; different regular expression parsers may only recognize subsets of these, or treat some expressions slightly differently. Using an online regular expression tester is a handy way to test out your expressions and explore these variations.

concatenation

# 2.1.1 Basic Regular Expression Patterns

The simplest kind of regular expression is a sequence of simple characters; putting characters in sequence is called concatenation. To search for woodchuck, we type /woodchuck/. The expression /Buttercup/ matches any string containing the substring Buttercup; grep with that expression would return the line I’m called little Buttercup. The search string can consist of a single character (like $/ ! / .$ ) or a

sequence of characters (like /urgl/) (see Fig. 2.1).   
Figure 2.1 Some simple regex searches.   

<table><tr><td>Regex</td><td>Example Patterns Matched</td></tr><tr><td>/woodchucks/</td><td>&quot;interesting links to woodchucks and lemurs&quot;</td></tr><tr><td>/a/</td><td>&quot;Mary Ann stopped by Mona&#x27;s”</td></tr><tr><td>/！/</td><td>“You&#x27;ve left the burglar behind again!” said Nori</td></tr></table>

Regular expressions are case sensitive; lower case $/ { \mathsf s } /$ is distinct from upper case /S/ (/s/ matches a lower case $s$ but not an upper case $S$ ). This means that the pattern /woodchucks/ will not match the string Woodchucks. We can solve this problem with the use of the square braces [ and ]. The string of characters inside the braces specifies a disjunction of characters to match. For example, Fig. 2.2 shows that the pattern /[wW]/ matches patterns containing either $w$ or $W$ .

<table><tr><td>Regex</td><td>Match</td><td>Example Patterns</td></tr><tr><td>/[wW]oodchuck/</td><td>Woodchuck or woodchuck</td><td>&quot;Woodchuck&quot;</td></tr><tr><td>/[abc]/</td><td>‘a’,&#x27;b&#x27;,or‘c&#x27;</td><td>&quot;In uomini, in soldati&quot;</td></tr><tr><td>/[1234567890]/</td><td>any digit</td><td>&quot;plenty of Z to 5&quot;</td></tr></table>

Figure 2.2 The use of the brackets [] to specify a disjunction of characters.

The regular expression /[1234567890]/ specifies any single digit. While such classes of characters as digits or letters are important building blocks in expressions, they can get awkward (e.g., it’s inconvenient to specify

# /[ABCDEFGHIJKLMNOPQRSTUVWXYZ]/

to mean “any capital letter”). In cases where there is a well-defined sequence associated with a set of characters, the brackets can be used with the dash $( - )$ to specify any one character in a range. The pattern /[2-5]/ specifies any one of the characters 2, 3, 4, or 5. The pattern $/ [ b \mathbf { - } \mathbf { 9 } ] /$ specifies one of the characters $b , c , d , e , f ,$ o r g. Some other examples are shown in Fig. 2.3.

<table><tr><td>Regex</td><td>Match</td><td>Example Patterns Matched</td></tr><tr><td>/[A-Z]/ /[a-z]/ /[0-9]/</td><td>an upper case letter a lower case letter</td><td>“we should callit ‘Drenched Blossoms’ “my beans were impatient to be hoed!” “Chapter 1: Down the Rabbit Hole&quot;</td></tr></table>

Figure 2.3 The use of the brackets [] plus the dash - to specify a range.

The square braces can also be used to specify what a single character cannot be, by use of the caret ˆ. If the caret ˆ is the first symbol after the open square brace [, the resulting pattern is negated. For example, the pattern $/ [ \hat { \textbf { a } } ] /$ matches any single character (including special characters) except $a$ . This is only true when the caret is the first symbol after the open square brace. If it occurs anywhere else, it usually stands for a caret; Fig. 2.4 shows some examples.

How can we talk about optional elements, like an optional $s$ in woodchuck and woodchucks? We can’t use the square brackets, because while they allow us to say “s or $S ^ { \ast }$ , they don’t allow us to say “s or nothing”. For this we use the question mark /?/, which means “the preceding character or nothing”, as shown in Fig. 2.5.

We can think of the question mark as meaning “zero or one instances of the previous character”. That is, it’s a way of specifying how many of something that we want, something that is very important in regular expressions. For example, consider the language of certain sheep, which consists of strings that look like the following:

<table><tr><td>Regex</td><td> Match (single characters)</td><td>Example Patterns Matched</td></tr><tr><td>/[^A-Z]/</td><td> not an upper case letter</td><td>&quot;Oyfn pripetchik&quot;</td></tr><tr><td>/[^ss]/</td><td>neither ‘S’ nor ‘s’</td><td>“I have no exquisite reason for&#x27;t&quot;</td></tr><tr><td>/[^.]/</td><td> not a period</td><td>“Qur resident Djinn”</td></tr><tr><td>/[e^]/</td><td>either ‘e’ or‘^’</td><td>“look up ^ now”</td></tr><tr><td>/a^b/</td><td> the pattern ‘a^b&#x27;</td><td>“look up a^ b now”</td></tr></table>

Figure 2.4 The caret ˆ for negation or just to mean ˆ. See below re: the backslash for escaping the period.

<table><tr><td>Regex</td><td>Match</td><td>Example Patterns Matched</td></tr><tr><td>/woodchucks?/</td><td> woodchuck or woodchucks</td><td>&quot;woodchuck&quot;</td></tr><tr><td>/colou?r/</td><td>color or colour</td><td>&quot;color”</td></tr></table>

Figure 2.5 The question mark ? marks optionality of the previous expression.

baa! baaa! baaaa!

# Kleene \*

This language consists of strings with a $^ b$ , followed by at least two a’s, followed by an exclamation point. The set of operators that allows us to say things like “some number of as” are based on the asterisk or \*, commonly called the Kleene \* (generally pronounced “cleany star”). The Kleene star means “zero or more occurrences of the immediately previous character or regular expression”. So $/ \mathsf { a } ^ { \ast } /$ means “any string of zero or more as”. This will match $a$ or aaaaaa, but it will also match the empty string at the start of Off Minor since the string Off Minor starts with zero a’s. So the regular expression for matching one or more $a$ is $/ \mathtt { a a } ^ { * } /$ , meaning one $a$ followed by zero or more as. More complex patterns can also be repeated. So $) / [ \mathsf { a b } ] ^ { * } /$ means “zero or more a’s or $b ^ { \prime } \mathbf { s } ^ { \prime }$ (not “zero or more right square braces”). This will match strings like aaaa or ababab or bbbb, as well as the empty string.

For specifying multiple digits (useful for finding prices) we can extend $\big / \big [ 0 - 9 \big ] \big /$ the regular expression for a single digit. An integer (a string of digits) is thus $/ [ 0 - 9 ] [ 0 - 9 ] ^ { * } / .$ . (Why isn’t it just $/ \left[ 0 - 9 \right] ^ { * } / ? )$

# Kleene $^ +$

Sometimes it’s annoying to have to write the regular expression for digits twice, so there is a shorter way to specify “at least one” of some character. This is the Kleene $^ +$ , which means “one or more occurrences of the immediately preceding character or regular expression”. Thus, the expression $/ \left[ 0 - 9 \right] + /$ is the normal way to specify “a sequence of digits”. There are thus two ways to specify the sheep language: $/ \mathsf { b a a a a } ^ { * } ! /$ or $/ 6 a a + ! /$ .

One very important special character is the period $( / . / )$ , a wildcard expression that matches any single character (except a carriage return), as shown in Fig. 2.6.

<table><tr><td>Regex</td><td>Match</td><td> Example Matches</td></tr><tr><td>/beg.n/</td><td> any character between beg and n</td><td>begin, beg&#x27;n, begun</td></tr></table>

Figure 2.6 The use of the period . to specify any character.

The wildcard is often used together with the Kleene star to mean “any string of characters”. For example, suppose we want to find any line in which a particular word, for example, aardvark, appears twice. We can specify this with the regular expression /aardvark.\*aardvark/.

Anchors are special characters that anchor regular expressions to particular places in a string. The most common anchors are the caret ˆ and the dollar sign \$. The caret ˆ matches the start of a line. The pattern /ˆThe/ matches the word The only at the start of a line. Thus, the caret ˆ has three uses: to match the start of a line, to indicate a negation inside of square brackets, and just to mean a caret. (What are the contexts that allow grep or Python to know which function a given caret is supposed to have?) The dollar sign $\$ 5$ matches the end of a line. So the pattern $\_ \ S$ is a useful pattern for matching a space at the end of a line, and $/ \hat { }$ The dog\.\$/ matches a line that contains only the phrase The dog. (We have to use the backslash here since we want the . to mean “period” and not the wildcard.)

Figure 2.7 Anchors in regular expressions.   

<table><tr><td>Regex</td><td>Match</td></tr><tr><td>A</td><td> start of line</td></tr><tr><td>$</td><td> end of line</td></tr><tr><td>\b</td><td> word boundary</td></tr><tr><td>\B</td><td> non-word boundary</td></tr></table>

There are also two other anchors: \b matches a word boundary, and \B matches a non word-boundary. Thus, /\bthe\b/ matches the word the but not the word other. A “word” for the purposes of a regular expression is defined based on the definition of words in programming languages as a sequence of digits, underscores, or letters. Thus /\b99\b/ will match the string 99 in There are 99 bottles of beer on the wall (because 99 follows a space) but not 99 in There are 299 bottles of beer on the wall (since 99 follows a number). But it will match 99 in $\$ 99$ (since 99 follows a dollar sign $( \$ )$ , which is not a digit, underscore, or letter).

# 2.1.2 Disjunction, Grouping, and Precedence

Suppose we need to search for texts about pets; perhaps we are particularly interested in cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can’t use the square brackets to search for “cat or dog” (why can’t we say /[catdog]/?), we need a new operator, the disjunction operator, also called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or the string dog.

Sometimes we need to use this disjunction operator in the midst of a larger sequence. For example, suppose I want to search for information about pet fish for my cousin David. How can I specify both guppy and guppies? We cannot simply say /guppy|ies/, because that would match only the strings guppy and ies. This is because sequences like guppy take precedence over the disjunction operator |. To make the disjunction operator apply only to a specific pattern, we need to use the parenthesis operators ( and ). Enclosing a pattern in parentheses makes it act like a single character for the purposes of neighboring operators like the pipe | and the Kleene\*. So the pattern /gupp(y|ies)/ would specify that we meant the disjunction only to apply to the suffixes y and ies.

The parenthesis operator ( is also useful when we are using counters like the Kleene\*. Unlike the | operator, the Kleene\* operator applies by default only to a single character, not to a whole sequence. Suppose we want to match repeated instances of a string. Perhaps we have a line that has column labels of the form

Column 1 Column 2 Column 3. The expression /Column $[ 0 - 9 ] + \ldots ^ { * } /$ will not match any number of columns; instead, it will match a single column followed by any number of spaces! The star here applies only to the space that precedes it, not to the whole sequence. With the parentheses, we could write the expression $/ ( \mathsf { C o l u m n \_ } [ \Theta - 9 ] + \ldots ^ { * } ) ^ { * } /$ to match the word Column, followed by a number and optional spaces, the whole pattern repeated zero or more times.

This idea that one operator may take precedence over another, requiring us to sometimes use parentheses to specify what we mean, is formalized by the operator precedence hierarchy for regular expressions. The following table gives the order of RE operator precedence, from highest precedence to lowest precedence.

<table><tr><td>Parenthesis Counters</td><td>O *+？{} Sequences and anchors the ^my end$</td></tr></table>

Thus, because counters have a higher precedence than sequences, $/ \mathrm { t h e ^ { * } } /$ matches theeeee but not thethe. Because sequences have a higher precedence than disjunction, /the|any/ matches the or any but not thany or theny.

Patterns can be ambiguous in another way. Consider the expression $/ \left[ { \tt a - z } \right] ^ { * } /$ when matching against the text once upon a time. Since $/ \left[ { \tt a - z } \right] ^ { * } /$ matches zero or more letters, this expression could match nothing, or just the first letter $o$ , on, onc, or once. In these cases regular expressions always match the largest string they can; we say that patterns are greedy, expanding to cover as much of a string as they can.

There are, however, ways to enforce non-greedy matching, using another meaning of the ? qualifier. The operator $\ast \ Y$ is a Kleene star that matches as little text as possible. The operator $+ ?$ is a Kleene plus that matches as little text as possible.

# 2.1.3 A Simple Example

Suppose we wanted to write a RE to find cases of the English article the. A simple (but incorrect) pattern might be:

One problem is that this pattern will miss the word when it begins a sentence and hence is capitalized (i.e., The). This might lead us to the following pattern:

But we will still overgeneralize, incorrectly return texts with the embedded in other words (e.g., other or there). So we need to specify that we want instances with a word boundary on both sides:

The simple process we just went through was based on fixing two kinds of errors: false positives, strings that we incorrectly matched like other or there, and false negatives, strings that we incorrectly missed, like The. Addressing these two kinds of errors comes up again and again in language processing. Reducing the overall error rate for an application thus involves two antagonistic efforts:

• Increasing precision (minimizing false positives) • Increasing recall (minimizing false negatives)

We’ll come back to precision and recall with more precise definitions in Chapter 4.

# 2.1.4 More Operators

Figure 2.8 shows some aliases for common ranges, which can be used mainly to save typing. Besides the Kleene \* and Kleene $^ +$ we can also use explicit numbers as counters, by enclosing them in curly brackets. The operator $/ \{ 3 \} /$ means “exactly 3 occurrences of the previous character or expression”. $\mathsf { S o / a } \backslash . \{ 2 4 \} \mathsf { z / }$ will match a followed by 24 dots followed by $z$ (but not $a$ followed by 23 or 25 dots followed by a z).

<table><tr><td>Regex</td><td>Expansion</td><td>Match</td><td>First Matches</td></tr><tr><td>Id</td><td>[0-9]</td><td> any digit</td><td>Party_of.5</td></tr><tr><td>\D</td><td>[^0-9]</td><td> any non-digit</td><td>Blue_moon</td></tr><tr><td>/w</td><td>[a-zA-Z0-9_]</td><td> any alphanumeric/underscore</td><td>Daiyu</td></tr><tr><td>\W</td><td>[^\w]</td><td> a non-alphanumeric</td><td>!!!!!</td></tr><tr><td>\s</td><td>[_\r\t\n\f]</td><td>whitespace (space, tab)</td><td>in_Concord</td></tr><tr><td>\s</td><td>[^\s]</td><td>Non-whitespace</td><td>in_Concord</td></tr></table>

Figure 2.8 Aliases for common sets of characters.

A range of numbers can also be specified. So $/ \{ \mathtt { n } , \mathtt { m } \} /$ specifies from $n$ to $m$ occurrences of the previous char or expression, and $/ \{ \boldsymbol { \mathfrak { n } } , \boldsymbol { \mathfrak { z } } /$ means at least $n$ occurrences of the previous expression. REs for counting are summarized in Fig. 2.9.

<table><tr><td>Regex</td><td>Match</td></tr><tr><td></td><td> zero or more occurrences of the previous char or expression</td></tr><tr><td>+</td><td> one or more occurrences of the previous char or expression</td></tr><tr><td>?</td><td> zero or one occurrence of the previous char or expression</td></tr><tr><td>{n}</td><td> exactly n occurrences of the previous char or expression</td></tr><tr><td>{n，m}</td><td> from n to m occurrences of the previous char or expression</td></tr><tr><td>{n,3</td><td> at least n occurrences of the previous char or expression</td></tr><tr><td>{，m}</td><td> up to m occurrences of the previous char or expression</td></tr></table>

Figure 2.9 Regular expression operators for counting.

# newline

Finally, certain special characters are referred to by special notation based on the backslash (\) (see Fig. 2.10). The most common of these are the newline character \n and the tab character \t. To refer to characters that are special themselves (like ., \*, [, and \), precede them with a backslash, (i.e., /\./, /\\*/, /\[/, and $/ \backslash \backslash / )$ .

<table><tr><td>Regex</td><td>Match</td><td>First Patterns Matched</td></tr><tr><td>1*</td><td> an asterisk &quot;*&quot;</td><td>&quot;K*A*P*L*A*N&quot;</td></tr><tr><td></td><td>a period&quot;&quot;</td><td>&quot;Dr. Livingston, I presume&quot;</td></tr><tr><td>\?</td><td> a question mark</td><td>“Why don’t they come and lend a hand?&quot;</td></tr><tr><td>\n</td><td>a newline</td><td></td></tr><tr><td>\t</td><td> a tab</td><td></td></tr></table>

Figure 2.10 Some characters that need to be escaped (via backslash).

# 2.1.5 A More Complex Example

Let’s try out a more significant example of the power of REs. Suppose our goal is help a user buy a computer on the Web who wants “at least 6 GHz and $5 0 0 \mathrm { G B }$ of disk space for less than $\$ 1000$ . To do this kind of retrieval, we first need to be

able to look for expressions like $6 \ : G H z$ or $5 0 0 G B$ or $\$ 999 .99$ . Let’s work out some regular expressions for this task.

First, let’s complete our regular expression for prices. Here’s a regular expression for a dollar sign followed by a string of digits:

$$
/ \$ [0-9]+/
$$

Note that the $\$ 5$ character has a different function here than the end-of-line function we discussed earlier. Most regular expression parsers are smart enough to realize that \$ here doesn’t mean end-of-line. (As a thought experiment, think about how regex parsers might figure out the function of $\$ 5$ from the context.)

Now we just need to deal with fractions of dollars. We’ll add a decimal point and two digits afterwards:

$$
/ { \ S } \left[ \Theta - 9 \right] + \backslash \cdot \left[ \Theta - 9 \right] \left[ \Theta - 9 \right] /
$$

This pattern only allows $\$ 199.99$ but not $\$ 199$ . We need to make the cents optional and to make sure we’re at a word boundary:

$$
/ ( \hat { \mathbf { \xi } } | \setminus \{ 0 \} \$ 0
$$

One last catch! This pattern allows prices like $\$ 199999.99$ which would be far too expensive! We need to limit the dollars:

$$
\big / ( \mathrm { \Large ~ \hat { ~ } { ~ } } | \mathrm { \large ~ \hat { W } ~ } ) \ S \left[ \varnothing - 9 \right] \{ \varnothing , 3 \} ( \mathrm { \Large ~ \hat { ~ } { ~ } } . [ \varnothing - 9 ] \left[ \varnothing - 9 \right] ) ? \backslash \ b \big /
$$

Further fixes (like avoiding matching a dollar sign with no price after it) are left as an exercise for the reader.

How about disk space? We’ll need to allow for optional fractions again $( 5 . 5 G B )$ ; note the use of ? for making the final s optional, and the use of $/ { \Gamma } _ { \Gamma } ^ { * } /$ to mean “zero or more spaces” since there might always be extra spaces lying around:

$$
/ \setminus \mathsf { b } \left[ \Theta - 9 \right] + ( \setminus . \left[ \Theta - 9 \right] + ) ? \ ^ { \ast } ( \mathsf { G B } \mid [ \mathsf { G 9 } ] \mathrm { i } \mathsf { g a b y t e s } ? ) \setminus \mathsf { b } /
$$

Modifying this regular expression so that it only matches more than $5 0 0 \mathrm { G B }$ is left as an exercise for the reader.

# 2.1.6 Substitution, Capture Groups, and ELIZA

An important use of regular expressions is in substitutions. For example, the substitution operator s/regexp1/pattern/ used in Python and in Unix commands like vim or sed allows a string characterized by a regular expression to be replaced by another string:

s/colour/color/

It is often useful to be able to refer to a particular subpart of the string matching the first pattern. For example, suppose we wanted to put angle brackets around all integers in a text, for example, changing the 35 boxes to the $< 3 5 >$ boxes. We’d like a way to refer to the integer we’ve found so that we can easily add the brackets. To do this, we put parentheses ( and ) around the first pattern and use the number operator \1 in the second pattern to refer back. Here’s how it looks:

$$
\mathsf { s } / ( [ 0 - 9 ] + ) / { < } \backslash 1 { > } /
$$

The parenthesis and number operators can also specify that a certain string or expression must occur twice in the text. For example, suppose we are looking for the pattern “the Xer they were, the Xer they will be”, where we want to constrain the two X’s to be the same string. We do this by surrounding the first X with the parenthesis operator, and replacing the second X with the number operator \1, as follows:

register

/the (.\*)er they were, the \1er they will be/

Here the \1 will be replaced by whatever string matched the first item in parentheses. So this will match the bigger they were, the bigger they will be but not the bigger they were, the faster they will be.

This use of parentheses to store a pattern in memory is called a capture group. Every time a capture group is used (i.e., parentheses surround a pattern), the resulting match is stored in a numbered register. If you match two different sets of parentheses, \2 means whatever matched the second capture group. Thus

non-capturing group

will match the faster they ran, the faster we ran but not the faster they ran, the faster we ate. Similarly, the third capture group is stored in \3, the fourth is \4, and so on.

Parentheses thus have a double function in regular expressions; they are used to group terms for specifying the order in which operators should apply, and they are used to capture something in a register. Occasionally we might want to use parentheses for grouping, but don’t want to capture the resulting pattern in a register. In that case we use a non-capturing group, which is specified by putting the special commands ?: after the open parenthesis, in the form (?: pattern ).

# /(?:some|a few) (people|cats) like some \1/

will match some cats like some cats but not some cats like some some.

Substitutions and capture groups are very useful in implementing simple chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates a Rogerian psychologist by carrying on conversations like the following:

User1: Men are all alike.   
$\mathrm { E L I Z A _ { 1 } }$ : IN WHAT WAY   
User2: They’re always bugging us about something or other. ELIZA2: CAN YOU THINK OF A SPECIFIC EXAMPLE User3: Well, my boyfriend made me come here.   
ELIZA3: YOUR BOYFRIEND MADE YOU COME HERE $\mathrm { U s e r } _ { 4 }$ : He says I’m depressed much of the time.   
ELIZA4: I AM SORRY TO HEAR YOU ARE DEPRESSED

ELIZA works by having a series or cascade of regular expression substitutions each of which matches and changes some part of the input lines. After the input is uppercased, substitutions change all instances of MY to YOUR, and I’M to YOU ARE, and so on. That way when ELIZA repeats back part of the user utterance, it will seem to be referring correctly to the user. The next set of substitutions matches and replaces other patterns in the input. Here are some examples:

s/.\* YOU ARE (depressed|sad) . $\ast / \ I$ AM SORRY TO HEAR YOU ARE \1/ s/.\* YOU ARE (depressed|sad) .\*/WHY DO YOU THINK YOU ARE \1/ s/.\* all .\*/IN WHAT WAY/ s/.\* always .\*/CAN YOU THINK OF A SPECIFIC EXAMPLE/

Since multiple substitutions can apply to a given input, substitutions are assigned a rank and applied in order. Creating patterns is the topic of Exercise 2.3, and we return to the details of the ELIZA architecture in Chapter 15.

# 2.1.7 Lookahead Assertions

zero-width

Finally, there will be times when we need to predict the future: look ahead in the text to see if some pattern matches, but not yet advance the pointer we always keep to where we are in the text, so that we can then deal with the pattern if it occurs, but if it doesn’t we can check for something else instead.

These lookahead assertions make use of the (? syntax that we saw in the previous section for non-capture groups. The operator ( $\ ? =$ pattern) is true if pattern occurs, but is zero-width, i.e. the match pointer doesn’t advance. The operator (?! pattern) only returns true if a pattern does not match, but again is zero-width and doesn’t advance the pointer. Negative lookahead is commonly used when we are parsing some complex pattern but want to rule out a special case. For example suppose we want to match, at the beginning of a line, any single word that doesn’t start with “Volcano”. We can use negative lookahead to do this:

$$
/ ^ { \cdot } ( ? ! \mathtt { V o l c a n o } ) [ \mathtt { A } \mathtt { - } Z \mathtt { a } \mathtt { - } Z ] + /
$$

# 2.2 Words

# corpus corpora

Before we talk about processing words, we need to decide what counts as a word. Let’s start by looking at one particular corpus (plural corpora), a computer-readable collection of text or speech. For example the Brown corpus is a million-word collection of samples from 500 written English texts from different genres (newspaper, fiction, non-fiction, academic, etc.), assembled at Brown University in 1963–64 (Kucera and Francis ˇ , 1967). How many words are in the following Brown sentence?

He stepped out into the hall, was delighted to encounter a water brother.

utterance

This sentence has 13 words if we don’t count punctuation marks as words, 15 if we count punctuation. Whether we treat period (“.”), comma (“,”), and so on as words depends on the task. Punctuation is critical for finding boundaries of things (commas, periods, colons) and for identifying some aspects of meaning (question marks, exclamation marks, quotation marks). For some tasks, like part-of-speech tagging or parsing or speech synthesis, we sometimes treat punctuation marks as if they were separate words.

The Switchboard corpus of American English telephone conversations between strangers was collected in the early 1990s; it contains 2430 conversations averaging 6 minutes each, totaling 240 hours of speech and about 3 million words (Godfrey et al., 1992). Such corpora of spoken language introduce other complications with regard to defining words. Let’s look at one utterance from Switchboard; an utterance is the spoken correlate of a sentence:

# I do uh main- mainly business data processing

This utterance has two kinds of disfluencies. The broken-off word main- is called a fragment. Words like uh and um are called fillers or filled pauses. Should we consider these to be words? Again, it depends on the application. If we are building a speech transcription system, we might want to eventually strip out the disfluencies.

But we also sometimes keep disfluencies around. Disfluencies like uh or um are actually helpful in speech recognition in predicting the upcoming word, because they may signal that the speaker is restarting the clause or idea, and so for speech recognition they are treated as regular words. Because different people use different disfluencies they can also be a cue to speaker identification. In fact Clark and Fox Tree (2002) showed that uh and um have different meanings. What do you think they are?

Perhaps most important, in thinking about what is a word, we need to distinguish two ways of talking about words that will be useful throughout the book. Word types are the number of distinct words in a corpus; if the set of words in the vocabulary is $V$ , the number of types is the vocabulary size $| V |$ . Word instances are the total number $N$ of running words.1 If we ignore punctuation, the following Brown sentence has 14 types and 16 instances:

They picnicked by the pool, then lay back on the grass and looked at the stars.

We still have decisions to make! For example, should we consider a capitalized string (like They) and one that is uncapitalized (like they) to be the same word type? The answer is that it depends on the task! They and they might be lumped together as the same type in some tasks, like speech recognition, where we care more about the sequence of words and less about the formatting, while for other tasks, such as deciding whether a particular word is a name of a person or location (namedentity tagging), capitalization is a useful feature and is retained. Sometimes we keep around two versions of a particular NLP model, one with capitalization and one without capitalization.

<table><tr><td>Corpus</td><td></td><td>Types = |V| Instances = N</td></tr><tr><td>Shakespeare</td><td>31 thousand</td><td>884 thousand</td></tr><tr><td>Brown corpus</td><td> 38 thousand</td><td>1 million</td></tr><tr><td> Switchboard telephone conversations</td><td> 20 thousand</td><td> 2.4 million</td></tr><tr><td>COCA</td><td> 2 million</td><td> 440 million</td></tr><tr><td>Google n-grams</td><td>13 million</td><td>1 trillion</td></tr></table>

Figure 2.11 Rough numbers of wordform types and instances for some English language corpora. The largest, the Google n-grams corpus, contains 13 million types, but this count only includes types appearing 40 or more times, so the true number would be much larger.

How many words are there in English? When we speak about the number of words in the language, we are generally referring to word types. Fig. 2.11 shows the rough numbers of types and instances computed from some English corpora. The larger the corpora we look at, the more word types we find, and in fact this relationship between the number of types $| V |$ and number of instances $N$ is called Herdan’s Law (Herdan, 1960) or Heaps’ Law (Heaps, 1978) after its discoverers (in linguistics and information retrieval respectively). It is shown in Eq. 2.16, where $k$ and $\beta$ are positive constants, and $0 < \beta < 1$ .

$$
| V | ~ = ~ k N ^ { \beta }
$$

The value of $\beta$ depends on the corpus size and the genre, but at least for the large corpora in Fig. 2.11, $\beta$ ranges from .67 to .75. Roughly then we can say that the vocabulary size for a text goes up significantly faster than the square root of its length in words.

lemma wordform

It’s sometimes useful to make a further distinction. Consider inflected forms like cats versus cat. We say these two words are different wordforms but have the same lemma. A lemma is a set of lexical forms having the same stem, and usually the same major part-of-speech. The wordform is the full inflected or derived form of the word. The two wordforms cat and cats thus have the same lemma, which we can represent as cat.

For morphologically complex languages like Arabic, we often need to deal with lemmatization. For most tasks in English, however, wordforms are sufficient, and when we talk about words in this book we almost always mean wordforms (although we will discuss basic algorithms for lemmatization and the related task of stemming below in Section 2.6). One of the situations even in English where we talk about lemmas is when we measure the number of words in a dictionary. Dictionary entries or boldface forms are a very rough approximation to (an upper bound on) the number of lemmas (since some lemmas have multiple boldface forms). The 1989 edition of the Oxford English Dictionary had 615,000 entries.

Finally, we should note that in practice, for many NLP applications (for example for neural language modeling) we don’t actually use words as our internal unit of representation at all! We instead tokenize the input strings into tokens, which can be words but can also be only parts of words. We’ll return to this tokenization question when we introduce the BPE algorithm in Section 2.5.2.

# 2.3 Corpora

Words don’t appear out of nowhere. Any particular piece of text that we study is produced by one or more specific speakers or writers, in a specific dialect of a specific language, at a specific time, in a specific place, for a specific function.

# AAE

Perhaps the most important dimension of variation is the language. NLP algorithms are most useful when they apply across many languages. The world has 7097 languages at the time of this writing, according to the online Ethnologue catalog (Simons and Fennig, 2018). It is important to test algorithms on more than one language, and particularly on languages with different properties; by contrast there is an unfortunate current tendency for NLP algorithms to be developed or tested just on English (Bender, 2019). Even when algorithms are developed beyond English, they tend to be developed for the official languages of large industrialized nations (Chinese, Spanish, Japanese, German etc.), but we don’t want to limit tools to just these few languages. Furthermore, most languages also have multiple varieties, often spoken in different regions or by different social groups. Thus, for example, if we’re processing text that uses features of African American English (AAE) or African American Vernacular English (AAVE)—the variations of English used by millions of people in African American communities (King 2020)—we must use NLP tools that function with features of those varieties. Twitter posts might use features often used by speakers of African American English, such as constructions like iont (I don’t in Mainstream American English (MAE)), or talmbout corresponding to MAE talking about, both examples that influence word segmentation (Blodgett et al. 2016, Jones 2015).

# MAE

It’s also quite common for speakers or writers to use multiple languages in a single communicative act, a phenomenon called code switching. Code switching is enormously common across the world; here are examples showing Spanish and (transliterated) Hindi code switching with English (Solorio et al. 2014, Jurgens et al. 2017):

(2.17) Por primera vez veo a $@$ username actually being hateful! it was beautiful:) [For the first time I get to see @username actually being hateful! it was beautiful:) $\jmath$   
(2.18) dost tha or ra- hega ... dont wory ... but dherya rakhe [“he was and will remain a friend ... don’t worry ... but have faith”]

Another dimension of variation is the genre. The text that our algorithms must process might come from newswire, fiction or non-fiction books, scientific articles, Wikipedia, or religious texts. It might come from spoken genres like telephone conversations, business meetings, police body-worn cameras, medical interviews, or transcripts of television shows or movies. It might come from work situations like doctors’ notes, legal text, or parliamentary or congressional proceedings.

Text also reflects the demographic characteristics of the writer (or speaker): their age, gender, race, socioeconomic class can all influence the linguistic properties of the text we are processing.

And finally, time matters too. Language changes over time, and for some languages we have good corpora of texts from different historical periods.

Because language is so situated, when developing computational models for language processing from a corpus, it’s important to consider who produced the language, in what context, for what purpose. How can a user of a dataset know all these details? The best way is for the corpus creator to build a datasheet (Gebru et al., 2020) or data statement (Bender et al., 2021) for each corpus. A datasheet specifies properties of a dataset like:

Motivation: Why was the corpus collected, by whom, and who funded it?

Situation: When and in what situation was the text written/spoken? For example, was there a task? Was the language originally spoken conversation, edited text, social media communication, monologue vs. dialogue?   
Language variety: What language (including dialect/region) was the corpus in?   
Speaker demographics: What was, e.g., the age or gender of the text’s authors?   
Collection process: How big is the data? If it is a subsample how was it sampled? Was the data collected with consent? How was the data pre-processed, and what metadata is available?   
Annotation process: What are the annotations, what are the demographics of the annotators, how were they trained, how was the data annotated?   
Distribution: Are there copyright or other intellectual property restrictions?

# 2.4 Simple Unix Tools for Word Tokenization

Before almost any natural language processing of a text, the text has to be normalized, a task called text normalization. At least three tasks are commonly applied as part of any normalization process:

1. Tokenizing (segmenting) words   
2. Normalizing word formats   
3. Segmenting sentences

In the next sections we walk through each of these tasks, but we’ll first start with an easy, if somewhat naive version of word tokenization and normalization (and frequency computation) that can be accomplished for English solely in a single Unix command-line, inspired by Church (1994). We’ll make use of some Unix commands: tr, used to systematically change particular characters in the input; sort, which sorts input lines in alphabetical order; and uniq, which collapses and counts adjacent identical lines.

For example let’s begin with the ‘complete words’ of Shakespeare in one file, sh.txt. We can use tr to tokenize the words by changing every sequence of nonalphabetic characters to a newline (’A-Za-z’ means alphabetic and the -c option complements to non-alphabet, so together they mean to change every non-alphabetic character into a newline. The -s (‘squeeze’) option is used to replace the result of multiple consecutive changes into a single output, so a series of non-alphabetic characters in a row would all be ‘squeezed’ into a single newline):

The output of this command will be:

THE   
SONNETS   
by   
William   
Shakespeare   
From   
fairest   
creatures   
We

Now that there is one word per line, we can sort the lines, and pass them to uniq -c which will collapse and count them:

tr -sc ’A-Za-z’ ’\n’ < sh.txt | sort | uniq -c with the following output:

1945 A   
72 AARON   
19 ABBESS   
25 Aaron   
6 Abate   
1 Abates   
5 Abbess   
6 Abbey   
3 Abbot

Alternatively, we can collapse all the upper case to lower case:   
tr -sc ’A-Za-z’ ’\n’ $<$ sh.txt | tr A-Z a-z | sort | uniq -c   
whose output is   
14725 a 97 aaron 1 abaissiez 10 abandon

2 abandoned   
2 abase   
1 abash   
14 abate   
3 abated   
3 abatement

Now we can sort again to find the frequent words. The $- \mathbf { n }$ option to sort means to sort numerically rather than alphabetically, and the $- \mathbf { r }$ option means to sort in reverse order (highest-to-lowest):

tr -sc ’A-Za-z’ ’\n’ < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r

The results show that the most frequent words in Shakespeare, as in any other corpus, are the short function words like articles, pronouns, prepositions:

27378 the   
26084 and   
22538 i   
19771 to   
17481 of   
14725 a   
13826 you   
12489 my   
11318 that   
11112 in   
...

Unix tools of this sort can be very handy in building quick word count statistics for any corpus in English. While in some versions of Unix these command-line tools also correctly handle Unicode characters and so can be used for many languages, in general for handling most languages outside English we use more sophisticated tokenization algorithms.

# 2.5 Word and Subword Tokenization

# tokenization

The simple Unix tools above were fine for getting rough word statistics but more sophisticated algorithms are generally necessary for tokenization, the task of segmenting running text into words. There are roughly two classes of tokenization algorithms. In top-down tokenization, we define a standard and implement rules to implement that kind of tokenization.

But more commonly instead of using words as the input to NLP algorithms we break up words into subword tokens, which can be words or parts of words or even individual letters. These are derived via bottom-up tokenization, in which we use simple statistics of letter sequences to come up with the vocabulary of subword tokens, and break up the input into those subwords.

# 2.5.1 Top-down (rule-based) tokenization

While the Unix command sequence just removed all the numbers and punctuation, for most NLP applications we’ll need to keep these in our tokenization. We often want to break off punctuation as a separate token; commas are a useful piece of information for parsers, and periods help indicate sentence boundaries. But we’ll often want to keep the punctuation that occurs word internally, in examples like m.p.h., Ph.D., AT&T, and cap’n. Special characters and numbers will need to be kept in prices $( \$ 45.55)$ and dates (01/02/06); we don’t want to segment that price into separate tokens of $\bullet \bullet 4 5 ^ { \prime \ }$ and $5 5 '$ . And there are URLs (https://www.stanford.edu), Twitter hashtags (#nlproc), or email addresses (someone@cs.colorado.edu).

Number expressions introduce complications; in addition to appearing at word boundaries, commas appear inside numbers in English, every three digits: 555,500.50. Tokenization differs by language; languages like Spanish, French, and German, for example, use a comma to mark the decimal point, and spaces (or sometimes periods) where English puts commas, for example, 555 500,50.

A tokenizer can also be used to expand clitic contractions that are marked by apostrophes, converting what’re to the two tokens what are, and we’re to we are. A clitic is a part of a word that can’t stand on its own, and can only occur when it is attached to another word. Such contractions occur in other alphabetic languages, including French pronouns (j’ai and articles l’homme).

Depending on the application, tokenization algorithms may also tokenize multiword expressions like New York or rock ’n’ roll as a single token, which requires a multiword expression dictionary of some sort. Tokenization is thus intimately tied up with named entity recognition, the task of detecting names, dates, and organizations (Chapter 17).

One commonly used tokenization standard is known as the Penn Treebank tokenization standard, used for the parsed corpora (treebanks) released by the Linguistic Data Consortium (LDC), the source of many useful datasets. This standard separates out clitics (doesn’t becomes does plus $n ^ { \prime } t$ ), keeps hyphenated words together, and separates out all punctuation (to save space we’re showing visible spaces ‘ ’ between tokens, although newlines is a more common output):

Input: "The San Francisco-based restaurant," they said, "doesn’t charge $\$ 10"$ .   
Output: " The San Francisco-based restaurant , " they said , " does n’t charge \$ 10 " .

In practice, since tokenization is run before any other language processing, it needs to be very fast. For word tokenization we generally use deterministic algorithms based on regular expressions compiled into efficient finite state automata. For example, Fig. 2.12 shows a basic regular expression that can be used to tokenize English with the nltk.regexp tokenize function of the Python-based Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org).

Carefully designed deterministic algorithms can deal with the ambiguities that arise, such as the fact that the apostrophe needs to be tokenized differently when used as a genitive marker (as in the book’s cover), a quotative as in ‘The other class’, she said, or in clitics like they’re.

# hanzi

Word tokenization is more complex in languages like written Chinese, Japanese, and Thai, which do not use spaces to mark potential word-boundaries. In Chinese, for example, words are composed of characters (called hanzi in Chinese). Each character generally represents a single unit of meaning (called a morpheme) and is pronounceable as a single syllable. Words are about 2.4 characters long on average. But deciding what counts as a word in Chinese is complex. For example, consider the following sentence:

$> > >$ text $=$ ’That U.S.A. poster-print costs \$12.40...’   
>>> patte $\mathbf { \vec { r } } \mathbf { \vec { n } } = \mathbf { r } ^ { \prime \prime \prime } ( \mathbf { \vec { \rho } } _ { : } ^ { \mathrm { ? } } \mathbf { x } )$ # set flag to allow verbose regexps $( ? : [ { \tt A } \mathrm { - } { \thinspace } 2 ] \setminus . ) +$ # abbreviations, e.g. U.S.A. $\begin{array} { r l } { | } & { { } \setminus \boldsymbol { w } + ( ? : - \setminus \boldsymbol { w } + ) ^ { * } } \end{array}$ # words with optional internal hyphens $1 \setminus \ S 3 : 1 1 + ( ? : \setminus \cdot \setminus \mathrm { d } + ) ? \% ?$ # currency, percentages, e.g. $\$ 12.40$ , $82 \%$ | \.\.\. # ellipsis | [][.,;"’?():_‘-] # these are separate tokens; includes ], [ ’’’   
$> > >$ nltk.regexp_tokenize(text, pattern)   
[’That’, ’U.S.A.’, ’poster-print’, ’costs’, ’\$12.40’, ’...’]

Figure 2.12 A Python trace of regular expression tokenization in the NLTK Python-based natural language processing toolkit (Bird et al., 2009), commented for readability; the $( ? \mathbf { x } )$ verbose flag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird et al. (2009).

(2.19) 明进入总决赛 yao m ´ ´ıng j\`ın ru z \` ong ju ˇ e s ´ ai\` 姚“Yao Ming reaches the finals”

As Chen et al. (2017b) point out, this could be treated as 3 words (‘Chinese Treebank’ segmentation):

(2.20) 明 进入 总决赛 姚YaoMing reaches finals   
or as 5 words (‘Peking University’ segmentation):   
(2.21) 明 进入 总 决赛 姚Yao Ming reaches overall finals

Finally, it is possible in Chinese simply to ignore words altogether and use characters as the basic elements, treating the sentence as a series of 7 characters:

(2.22) 明 进 入 总 决 赛姚Yao Ming enter enter overall decision game

In fact, for most Chinese NLP tasks it turns out to work better to take characters rather than words as input, since characters are at a reasonable semantic level for most applications, and since most word standards, by contrast, result in a huge vocabulary with large numbers of very rare words (Li et al., 2019b).

However, for Japanese and Thai the character is too small a unit, and so algorithms for word segmentation are required. These can also be useful for Chinese in the rare situations where word rather than character boundaries are required. For these situations we can use the subword tokenization algorithms introduced in the next section.

# 2.5.2 Byte-Pair Encoding: A Bottom-up Tokenization Algorithm

There is a third option to tokenizing text, one that is most commonly used by large language models. Instead of defining tokens as words (whether delimited by spaces or more complex algorithms), or as characters (as in Chinese), we can use our data to automatically tell us what the tokens should be. This is especially useful in dealing with unknown words, an important problem in language processing. As we will see in the next chapter, NLP algorithms often learn some facts about language from one corpus (a training corpus) and then use these facts to make decisions about a separate test corpus and its language. Thus if our training corpus contains, say the words low, new, newer, but not lower, then if the word lower appears in our test corpus, our system will not know what to do with it.

To deal with this unknown word problem, modern tokenizers automatically induce sets of tokens that include tokens smaller than words, called subwords. Subwords can be arbitrary substrings, or they can be meaning-bearing units like the morphemes -est or -er. (A morpheme is the smallest meaning-bearing unit of a language; for example the word unwashable has the morphemes un-, wash, and -able.) In modern tokenization schemes, most tokens are words, but some tokens are frequently occurring morphemes or other subwords like -er. Every unseen word like lower can thus be represented by some sequence of known subword units, such as low and er, or even as a sequence of individual letters if necessary.

Most tokenization schemes have two parts: a token learner, and a token segmenter. The token learner takes a raw training corpus (sometimes roughly preseparated into words, for example by whitespace) and induces a vocabulary, a set of tokens. The token segmenter takes a raw test sentence and segments it into the tokens in the vocabulary. Two algorithms are widely used: byte-pair encoding (Sennrich et al., 2016), and unigram language modeling (Kudo, 2018), There is also a SentencePiece library that includes implementations of both of these (Kudo and Richardson, 2018a), and people often use the name SentencePiece to simply mean unigram language modeling tokenization.

# BPE

In this section we introduce the simplest of the three, the byte-pair encoding or BPE algorithm (Sennrich et al., 2016); see Fig. 2.13. The BPE token learner begins with a vocabulary that is just the set of all individual characters. It then examines the training corpus, chooses the two symbols that are most frequently adjacent (say ‘A’, ‘B’), adds a new merged symbol ‘AB’ to the vocabulary, and replaces every adjacent ’A’ ’B’ in the corpus with the new ‘AB’. It continues to count and merge, creating new longer and longer character strings, until $k$ merges have been done creating $k$ novel tokens; $k$ is thus a parameter of the algorithm. The resulting vocabulary consists of the original set of characters plus $k$ new symbols.

The algorithm is usually run inside words (not merging across word boundaries), so the input corpus is first white-space-separated to give a set of strings, each corresponding to the characters of a word, plus a special end-of-word symbol , and its counts. Let’s see its operation on the following tiny input corpus of 18 word tokens with counts for each word (the word low appears 5 times, the word newer 6 times, and so on), which would have a starting vocabulary of 11 letters:

corpus vocabulary   
5 2 $\begin{array} { r l } & { \mathrm { \texttt { l o w \_ s } } } \\ & { \mathrm { \texttt { l o w e s t \_ } } } \\ & { \mathrm { \texttt { n e w e r \_ } } } \\ & { \mathrm { \texttt { w i d e r \_ } } } \\ & { \mathrm { \texttt { n e w \_ w } } } \end{array}$ , d, e, i, l, n, o, r, s, t, w   
6   
3   
2

The BPE algorithm first counts all pairs of adjacent symbols: the most frequent is the pair e r because it occurs in newer (frequency of 6) and wider (frequency of 3) for a total of 9 occurrences.2 We then merge these symbols, treating er as one symbol, and count again:

# corpus

# vocabulary

, d, e, i, l, n, o, r, s, t, w, er

Now the most frequent pair is er , which we merge; our system has learned that there should be a token for word-final er, represented as er :

# corpus

# vocabulary

$\begin{array} { r l } { 5 } & { { } \mathrm { ~ \mathbb ~ { ~ 1 ~ } ~ o ~ w ~ \_ ~ } } \\ { 2 } & { { } \mathrm { ~ \mathbb ~ { ~ 1 ~ } ~ o ~ w ~ \ e ~ s ~ \ t ~ \_ ~ } } \\ { 6 } & { { } \mathrm { ~ \mathbb ~ { ~ n ~ } ~ e ~ w ~ \ e r \_ ~ } } \\ { 3 } & { { } \mathrm { ~ \mathbb ~ { ~ w ~ i ~ } ~ d ~ \ e r \_ ~ } } \\ { 2 } & { { } \mathrm { ~ \mathbb ~ { ~ n ~ e ~ w ~ \ e ~ } ~ \ } } \end{array}$

, d, e, i, l, n, o, r, s, t, w, er, er

Next n e (total count of 8) get merged to ne:

# corpus

# vocabulary

5 l o w   
2 l o w e s t   
6 ne w er   
3 w i d er   
2 ne w

, d, e, i, l, n, o, r, s, t, w, er, er , ne

If we continue, the next merges are:

# merge

# current vocabulary

(ne, w) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new (l, o) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo (lo, w) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low (new, er ) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer (low, ) , d, e, i, l, n, o, r, s, t, w, er, er , ne, new, lo, low, newer , low function BYTE-PAIR ENCODING(strings $C$ , number of merges $k$ ) returns vocab $V$

$V $ all unique characters in $C$ # initial set of tokens is characters for $i = 1$ to $k$ do # merge tokens $k$ times $t _ { L }$ , $t _ { R } \gets \mathbf { M o s t }$ frequent pair of adjacent tokens in $C$ $\begin{array} { l } { { t _ { N E W }  t _ { L } + t _ { R } } } \\ { { V  V + t _ { N E W } } } \end{array}$ # make new token by concatenating # update the vocabulary Replace each occurrence of $t _ { L } , t _ { R }$ in $C$ with $t _ { N E W }$ # and update the corpus return $V$

Figure 2.13 The token learner part of the BPE algorithm for taking a corpus broken up into individual characters or bytes, and learning a vocabulary by iteratively merging tokens. Figure adapted from Bostrom and Durrett (2020).

Once we’ve learned our vocabulary, the token segmenter is used to tokenize a test sentence. The token segmenter just runs on the merges we have learned from the training data on the test data. It runs them greedily, in the order we learned them. (Thus the frequencies in the test data don’t play a role, just the frequencies in the training data). So first we segment each test sentence word into characters. Then we apply the first rule: replace every instance of e r in the test corpus with er, and then the second rule: replace every instance of er in the test corpus with er , and so on. By the end, if the test corpus contained the character sequence n e w e $\mathbf { r _ { \alpha } } - \mathbf { \partial }$ it would be tokenized as a full word. But the characters of a new (unknown) word like l o w e r would be merged into the two tokens low er .

Of course in real settings BPE is run with many thousands of merges on a very large input corpus. The result is that most words will be represented as full symbols, and only the very rare words (and unknown words) will have to be represented by their parts.

# 2.6 Word Normalization, Lemmatization and Stemming

# normalization case folding

Word normalization is the task of putting words or tokens in a standard format. The simplest case of word normalization is case folding. Mapping everything to lower case means that Woodchuck and woodchuck are represented identically, which is very helpful for generalization in many tasks, such as information retrieval or speech recognition. For sentiment analysis and other text classification tasks, information extraction, and machine translation, by contrast, case can be quite helpful and case folding is generally not done. This is because maintaining the difference between, for example, US the country and us the pronoun can outweigh the advantage in generalization that case folding would have provided for other words. Sometimes we produce both cased (i.e. including both upper and lower case words or tokens) and uncased versions of language models.

Systems that use BPE or other kinds of bottom-up tokenization may do no further word normalization. In other NLP systems, we may want to do further normalizations, like choosing a single normal form for words with multiple forms like USA and US or uh-huh and uhhuh. This standardization may be valuable, despite the spelling information that is lost in the normalization process. For information retrieval or information extraction about the US, we might want to see information from documents whether they mention the US or the USA.

# lemmatization

# 2.6.1 Lemmatization

For other natural language processing situations we also want two morphologically different forms of a word to behave similarly. For example in web search, someone may type the string woodchucks but a useful system might want to also return pages that mention woodchuck with no $s$ . This is especially common in morphologically complex languages like Polish, where for example the word Warsaw has different endings when it is the subject (Warszawa), or after a preposition like “in Warsaw” (w Warszawie), or “to Warsaw” (do Warszawy), and so on. Lemmatization is the task of determining that two words have the same root, despite their surface differences. The words am, are, and is have the shared lemma be; the words dinner and dinners both have the lemma dinner. Lemmatizing each of these forms to the same lemma will let us find all mentions of words in Polish like Warsaw. The lemmatized form of a sentence like He is reading detective stories would thus be He be read detective story.

How is lemmatization done? The most sophisticated methods for lemmatization involve complete morphological parsing of the word. Morphology is the study of the way words are built up from smaller meaning-bearing units called morphemes. Two broad classes of morphemes can be distinguished: stems—the central mor

stemming Porter stemmer

pheme of the word, supplying the main meaning—and affixes—adding “additional” meanings of various kinds. So, for example, the word fox consists of one morpheme (the morpheme fox) and the word cats consists of two: the morpheme cat and the morpheme -s. A morphological parser takes a word like cats and parses it into the two morphemes cat and $s$ , or parses a Spanish word like amaren (‘if in the future they would love’) into the morpheme amar ‘to love’, and the morphological features 3PL (third person plural) and future subjunctive.

# Stemming: The Porter Stemmer

Lemmatization algorithms can be complex. For this reason we sometimes make use of a simpler but cruder method, which mainly consists of chopping off wordfinal affixes. This naive version of morphological analysis is called stemming. For example, the classic Porter stemmer (Porter, 1980), when applied to the following paragraph:

This was not the map we found in Billy Bones’s chest, but an accurate copy, complete in all things-names and heights and soundings-with the single exception of the red crosses and the written notes.

produces the following stemmed output:

Thi wa not the map we found in Billi Bone s chest but an accur copi complet in all thing name and height and sound with the singl except of the red cross and the written not

The algorithm is based on rewrite rules run in series, with the output of each pass fed as input to the next pass. Some sample rules (more at https://tartarus.org/ martin/PorterStemmer/):

ATIONAL ATE (e.g., relational relate) $\mathrm { I N G } ~  ~ \epsilon$ if the stem contains a vowel (e.g., motoring motor) $\mathrm { S S E S } ~  ~ \mathrm { S S }$ (e.g., grasses grass)

Simple stemmers can be useful in cases where we need to collapse across different variants of the same lemma. Nonetheless, they are less commonly used in modern systems since they commit errors of both over-generalizing (lemmatizing policy to police) and under-generalizing (not lemmatizing European to Europe) (Krovetz, 1993).

# 2.7 Sentence Segmentation

Sentence segmentation is another important step in text processing. The most useful cues for segmenting a text into sentences are punctuation, like periods, question marks, and exclamation points. Question marks and exclamation points are relatively unambiguous markers of sentence boundaries. Periods, on the other hand, are more ambiguous. The period character “.” is ambiguous between a sentence boundary marker and a marker of abbreviations like $M r$ or Inc. The previous sentence that you just read showed an even more complex case of this ambiguity, in which the final period of Inc. marked both an abbreviation and the sentence boundary marker. For this reason, sentence tokenization and word tokenization may be addressed jointly.

In general, sentence tokenization methods work by first deciding (based on rules or machine learning) whether a period is part of the word or is a sentence-boundary marker. An abbreviation dictionary can help determine whether the period is part of a commonly used abbreviation; the dictionaries can be hand-built or machinelearned (Kiss and Strunk, 2006), as can the final sentence splitter. In the Stanford CoreNLP toolkit (Manning et al., 2014), for example sentence splitting is rule-based, a deterministic consequence of tokenization; a sentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped with other characters into a token (such as for an abbreviation or number), optionally followed by additional final quotes or brackets.

# 2.8 Minimum Edit Distance

Much of natural language processing is concerned with measuring how similar two strings are. For example in spelling correction, the user typed some erroneous string—let’s say graffe–and we want to know what the user meant. The user probably intended a word that is similar to graffe. Among candidate similar words, the word giraffe, which differs by only one letter from graffe, seems intuitively to be more similar than, say grail or graf, which differ in more letters. Another example comes from coreference, the task of deciding whether two strings such as the following refer to the same entity:

Stanford Arizona Cactus Garden Stanford University Arizona Cactus Garden

Again, the fact that these two strings are very similar (differing by only one word) seems like useful evidence for deciding that they might be coreferent. Finally, string similarity is commonly used to measure the quality of the transcription produced by a speech recognition system, by asking how similar (in words) the transcript is to a reference transcript. A system whose transcript is off by many words is measurably worse than one which is only off by a few words.

Edit distance gives us a way to quantify these intuitions about string similarity. More formally, the minimum edit distance between two strings is defined as the minimum number of editing operations (operations like insertion, deletion, substitution) needed to transform one string into another.

The gap between intention and execution, for example, is 5 (delete an i, substitute e for n, substitute x for t, insert c, substitute u for n). It’s much easier to see this by looking at the most important visualization for string distances, an alignment between the two strings, shown in Fig. 2.14. Given two sequences, an alignment is a correspondence between substrings of the two sequences. Thus, we say I aligns with the empty string, N with E, and so on. Beneath the aligned strings is another representation; a series of symbols expressing an operation list for converting the top string into the bottom string: d for deletion, s for substitution, i for insertion.

We can also assign a particular cost or weight to each of these operations. The Levenshtein distance between two sequences is the simplest weighting factor in which each of the three operations has a cost of 1 (Levenshtein, 1966)—we assume that the substitution of a letter for itself, for example, t for t, has zero cost. The Levenshtein distance between intention and execution is 5. Levenshtein also proposed an alternative version of his metric in which each insertion or deletion has a cost of 1 and substitutions are not allowed. (This is equivalent to allowing substitution, but giving each substitution a cost of 2 since any substitution can be represented by one insertion and one deletion). Using this version, the Levenshtein distance between intention and execution is 8.

![## Image Analysis: 844f167f64882a51071679b2ad26a534a6777b65add8176d0eef75a30774f5cd.jpg

**Conceptual Understanding:**
The image conceptually represents the 'minimum edit distance' between two strings by visualizing their alignment and the sequence of elementary edit operations required to transform one into the other. Its main purpose is to clearly illustrate the concept of an 'alignment' in the context of edit distance, showing which characters correspond, which are substituted, inserted, or deleted. The image communicates the idea that the cost of transforming one string into another can be broken down into a series of fundamental single-character changes, with each operation (deletion, substitution, insertion) contributing to the overall 'distance' or 'cost' of transformation. The goal is to find the path with the minimum total cost, represented by the most efficient sequence of `d`, `s`, and `i` operations.

**Content Interpretation:**
The image represents the concept of minimum edit distance between two strings, "INTENTION" and "EXECUTION", through an alignment. It explicitly shows the character-by-character correspondence and the sequence of edit operations required to transform the source string into the target string. The asterisks serve as placeholders or indicators for operations that don't involve a direct character-to-character match (e.g., deletion or insertion at a specific point). The sequence 'd s s i s' at the bottom systematically details these operations: 'd' for deletion, 's' for substitution, and 'i' for insertion. This visual breakdown highlights how the algorithm calculates the minimum number of single-character edits (deletions, insertions, or substitutions) needed to change one word into the other.

**Key Insights:**
The main takeaway from this image is the visual demonstration of how the minimum edit distance is calculated by aligning two strings and identifying the necessary transformation operations. It teaches that string similarity can be quantified by a sequence of explicit edits. Specifically, the image illustrates: 1. **Alignment Visualization:** How two strings can be aligned to show character correspondences and differences. 2. **Edit Operations:** The specific operations (deletion 'd', substitution 's', insertion 'i') are explicitly listed, providing a clear procedural understanding of string transformation. 3. **Gap Representation:** The use of asterisks '*' signifies positions where a character in one string does not directly correspond to a character in the other, indicating an insertion or deletion. For example, 'I N T E * N T I O N' implies a character was deleted at that position from the source, and '* E X E C U T I O N' implies a character was inserted at the beginning of the target string. The textual evidence `I N T E * N T I O N`, `* E X E C U T I O N`, and `d s s i s` directly supports these insights by showing the strings, their alignment, and the precise operations.

**Document Context:**
This image is crucial for Section 2.8, 'Minimum Edit Distance', as it provides a concrete visual example of how the concept is applied. The accompanying text 'Figure 2.14 Representing the minimum edit distance between two strings as an alignment. The final row gives the operation list for converting the top string into the bottom string: d for deletion, s for substitution, i for insertion.' directly explains the components of the image. It helps readers understand the mechanics of calculating edit distance by visually mapping the transformation steps and the specific operations (deletion, substitution, insertion) involved in converting one string ('INTENTION') into another ('EXECUTION'). The detailed alignment clarifies the often abstract concept of string similarity metrics.

**Summary:**
The image displays a textual representation of the alignment between two strings, "INTENTION" and "EXECUTION", illustrating the minimum edit distance. The top line shows the string "INTENTION" with an asterisk replacing a character in the fifth position. The middle section consists of vertical lines, indicating character alignment where a match occurs. The third line shows the string "EXECUTION", with an asterisk in the first position, signifying an insertion or deletion aligned with the corresponding asterisk in the top string. The final line provides a sequence of edit operations: 'd s s i s'. These operations describe how to transform the top string into the bottom string, where 'd' stands for deletion, 's' for substitution, and 'i' for insertion. For example, the first 'd' implies deleting the 'I' from "INTENTION", the 's' implies substituting 'N' with 'E', and so on. This visual representation helps to understand the character-by-character transformation process and the associated edit operations.](images/844f167f64882a51071679b2ad26a534a6777b65add8176d0eef75a30774f5cd.jpg)
Figure 2.14 Representing the minimum edit distance between two strings as an alignment. The final row gives the operation list for converting the top string into the bottom string: d for deletion, s for substitution, i for insertion.

dynamic programming

# 2.8.1 The Minimum Edit Distance Algorithm

How do we find the minimum edit distance? We can think of this as a search task, in which we are searching for the shortest path—a sequence of edits—from one string to another.

![## Image Analysis: 0659fefb2051557a0bbbcf7d241244eba0152e173e5181092c20efc5e1c1d5c9.jpg

**Conceptual Understanding:**
This image conceptually represents the initial steps of a search problem in the context of string manipulation, specifically illustrating the operations involved in calculating the minimum edit distance. The main purpose is to show how a base word can be transformed into different words through single-character edit operations. It communicates the key idea that from any given string, there are multiple immediate 'next states' that can be reached by applying fundamental edit operations like deletion, insertion, and substitution.

**Content Interpretation:**
The image displays a basic tree structure representing a single step in the process of calculating the minimum edit distance between strings. It shows the initial string "intention" at the root. From this root, three primary string edit operations are illustrated as branches: 'del' (deletion), 'ins' (insertion), and 'subst' (substitution). Each branch points to a new string that results from applying that specific operation to the original string "intention". The 'del' operation on "intention" results in "ntention". The 'ins' operation on "intention" results in "intecntion". The 'subst' operation on "intention" results in "inxention". This visually represents how different possible transformations of a string are generated, forming a search space for the edit distance algorithm.

**Key Insights:**
The main takeaway from this image is the explicit visualization of how a single string can be transformed into multiple different strings through a set of predefined edit operations (deletion, insertion, substitution). This demonstrates the concept of a search space in the context of string transformations, where each operation represents a 'move' to a new state (string). The image provides textual evidence of these transformations: starting with "intention", a 'del' operation yields "ntention", an 'ins' operation yields "intecntion", and a 'subst' operation yields "inxention". This illustrates that finding the minimum edit distance involves exploring these possible transformations to identify the shortest path.

**Document Context:**
This image directly supports the document section "2.8.1 The Minimum Edit Distance Algorithm" by visually explaining how the problem can be viewed as a search. It illustrates the fundamental operations (deletion, insertion, substitution) that are considered when transforming one string into another, which are the building blocks for computing edit distance. The caption "Figure 2.15 Finding the edit distance viewed as a search problem" further reinforces its role in demonstrating the search space exploration aspect of the algorithm.

**Summary:**
The image illustrates a conceptual search problem for finding the edit distance of the word "intention". It starts with the initial word at the top, and then branches out to show three possible single-character edit operations: deletion, insertion, and substitution. Each operation leads to a modified word. Specifically, applying a 'del' (deletion) operation results in the word "ntention" (removing the first 'i'). An 'ins' (insertion) operation results in "intecntion" (inserting 'c' after 'e'). A 'subst' (substitution) operation results in "inxention" (substituting 't' with 'x'). This diagram clearly demonstrates the immediate next steps or states that can be reached from a given word through basic edit operations, which is fundamental to understanding how the minimum edit distance algorithm explores the transformation space between two strings.](images/0659fefb2051557a0bbbcf7d241244eba0152e173e5181092c20efc5e1c1d5c9.jpg)
Figure 2.15 Finding the edit distance viewed as a search problem

The space of all possible edits is enormous, so we can’t search naively. However, lots of distinct edit paths will end up in the same state (string), so rather than recomputing all those paths, we could just remember the shortest path to a state each time we saw it. We can do this by using dynamic programming. Dynamic programming is the name for a class of algorithms, first introduced by Bellman (1957), that apply a table-driven method to solve problems by combining solutions to subproblems. Some of the most commonly used algorithms in natural language processing make use of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the CKY algorithm for parsing (Chapter 18).

The intuition of a dynamic programming problem is that a large problem can be solved by properly combining the solutions to various subproblems. Consider the shortest path of transformed words that represents the minimum edit distance between the strings intention and execution shown in Fig. 2.16.

minimum edit distance algorithm

Imagine some string (perhaps it is exention) that is in this optimal path (whatever it is). The intuition of dynamic programming is that if exention is in the optimal operation list, then the optimal sequence must also include the optimal path from intention to exention. Why? If there were a shorter path from intention to exention, then we could use it instead, resulting in a shorter overall path, and the optimal sequence wouldn’t be optimal, thus leading to a contradiction.

The minimum edit distance algorithm was named by Wagner and Fischer (1974) but independently discovered by many people (see the Historical Notes section of Chapter 17).

Let’s first define the minimum edit distance between two strings. Given two strings, the source string $X$ of length $n$ , and target string $Y$ of length $m$ , we’ll define

![## Image Analysis: bac1126707b695125b4f0bac14dce4e823b65abd3229c6a2873d5c60d0f2065a.jpg

**Conceptual Understanding:**
This image conceptually represents the step-by-step transformation of one character string into another, specifically illustrating the process of converting the word 'intention' into 'execution'. Its main purpose is to demonstrate the fundamental operations (deletion, substitution, insertion) that contribute to the 'edit distance' between two words. The image visually explains the sequence of changes necessary, providing a concrete example of how string manipulation algorithms work at a micro-level.

**Content Interpretation:**
The image presents a sequential process demonstrating how one string (the word 'intention') can be transformed into another string (the word 'execution') using a series of atomic edit operations. Each step shows the current state of the string and the specific operation applied to achieve the next state. The processes shown are string deletion ('delete i'), string substitution ('substitute n by e', 'substitute t by x', 'substitute n by c'), and string insertion ('insert u'). The significance of this presentation is to visually break down a complex string transformation into its fundamental components, which is a core concept in algorithms like Minimum Edit Distance.

**Key Insights:**
The main takeaway from this image is that complex string transformations can be broken down into a finite and ordered sequence of elementary edit operations: deletion, substitution, and insertion. The image specifically demonstrates that to transform 'intention' into 'execution', five distinct operations are performed: one deletion, three substitutions, and one insertion. This provides insight into the exact changes needed to convert one string into another, forming the basis for understanding string similarity measures. The textual evidence for these insights includes the initial word 'intention', the final word 'execution', and the explicit labels of the operations: 'delete i', 'substitute n by e', 'substitute t by x', 'insert u', and 'substitute n by c', along with all intermediate string states.

**Document Context:**
This image directly relates to Section 2.8.1, titled "The Minimum Edit Distance Algorithm," by providing a concrete and detailed example of how a string transformation occurs through a series of edit operations. It serves as a visual explanation for the "Path from intention to execution," as indicated in the text accompanying the image, illustrating the very mechanism by which the edit distance between two words (intention and execution) would be calculated. It clarifies the abstract concept of edit distance with a practical, step-by-step example.

**Summary:**
The image illustrates the process of transforming the word "intention" into "execution" through a series of edit operations. It visually maps out the step-by-step changes required. Starting with the word "intention," the first operation applied is to "delete i," which results in the intermediate word "ntention." Subsequently, the character "n" is "substitute[d] by e," transforming "ntention" into "etention." The process continues with a "substitute t by x" operation, changing "etention" to "exention." An "insert u" operation then modifies "exention" to "exenution." Finally, the character "n" is "substitute[d] by c," leading to the ultimate target word, "execution." Each step is clearly delineated by an arrow pointing from the current word to the operation performed, which then implicitly leads to the next transformed word. This detailed sequence makes it easy to understand the individual edit operations involved in string transformation.](images/bac1126707b695125b4f0bac14dce4e823b65abd3229c6a2873d5c60d0f2065a.jpg)
Figure 2.16 Path from intention to execution.

$D [ i , j ]$ as the edit distance between $X [ 1 . . i ]$ and $Y [ 1 . . j ]$ , i.e., the first $i$ characters of $X$ and the first $j$ characters of Y . The edit distance between $X$ and $Y$ is thus $D [ n , m ]$ .

We’ll use dynamic programming to compute $D [ n , m ]$ bottom up, combining solutions to subproblems. In the base case, with a source substring of length $i$ but an empty target string, going from $i$ characters to 0 requires $i$ deletes. With a target substring of length $j$ but an empty source going from 0 characters to $j$ characters requires $j$ inserts. Having computed $D [ i , j ]$ for small $i , j$ we then compute larger $D [ i , j ]$ based on previously computed smaller values. The value of $D [ i , j ]$ is computed by taking the minimum of the three possible paths through the matrix which arrive there:

$$
D [ i , j ] = \operatorname* { m i n } \left\{ \begin{array} { l l } { D [ i - 1 , j ] + \mathrm { d e l - c o s t } ( s o u r c e [ i ] ) } \\ { D [ i , j - 1 ] + \mathrm { i n s - c o s t } ( t a r g e t [ j ] ) } \\ { D [ i - 1 , j - 1 ] + \mathrm { s u b - c o s t } ( s o u r c e [ i ] , t a r g e t [ j ] ) } \end{array} \right.
$$

We mentioned above two versions of Levenshtein distance, one in which substitutions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion plus a deletion). Let’s here use that second version of Levenshtein distance in which the insertions and deletions each have a cost of $\begin{array} { r } { 1 \left( \operatorname* { i n s - c o s t } ( \cdot ) = \operatorname* { d e l - c o s t } ( \cdot ) = 1 \right) } \end{array}$ , and substitutions have a cost of 2 (except substitution of identical letters has zero cost). Under this version of Levenshtein, the computation for $D [ i , j ]$ becomes:

$$
D [ i , j ] = \operatorname* { m i n } \left\{ \begin{array} { l l } { D [ i - 1 , j ] + 1 } \\ { D [ i , j - 1 ] + 1 } \\ { D [ i - 1 , j - 1 ] + \left\{ \begin{array} { l l } { 2 ; } & { \mathrm { i f } \ s o u r c e [ i ] \neq t a r g e t [ j ] } \\ { 0 ; } & { \mathrm { i f } \ s o u r c e [ i ] = t a r g e t [ j ] } \end{array} \right. } \end{array} \right.
$$

The algorithm is summarized in Fig. 2.17; Fig. 2.18 shows the results of applying the algorithm to the distance between intention and execution with the version of Levenshtein in Eq. 2.24.

Alignment Knowing the minimum edit distance is useful for algorithms like finding potential spelling error corrections. But the edit distance algorithm is important in another way; with a small change, it can also provide the minimum cost alignment between two strings. Aligning two strings is useful throughout speech and language processing. In speech recognition, minimum edit distance alignment is used to compute the word error rate (Chapter 16). Alignment plays a role in machine translation, in which sentences in a parallel corpus (a corpus with a text in two languages) need to be matched to each other.

To extend the edit distance algorithm to produce an alignment, we can start by visualizing an alignment as a path through the edit distance matrix. Figure 2.19 function MIN-EDIT-DISTANCE(source, target) returns min-distance n← LENGTH(source) $m \gets$ LENGTH(target) Create a distance matrix $D [ n + I , m + I ]$ # Initialization: the zeroth row and column is the distance from the empty string $D [ 0 , 0 ] = 0$ for each row $i$ from 1 to $n$ do $D [ i , 0 J  D [ i - 1 , 0 ] + d e l - c o s t ( s o u r c e [ i ] )$ for each column $j$ from 1 to m do $D [ 0 , j ] \longleftarrow D [ 0 , j - 1 ] + i n s \ – c o s t ( t a r g e t [ j ] )$ # Recurrence relation: for each row $i$ from 1 to $n$ do for each column $j$ from 1 to $m$ do $\begin{array} { r l } & { D [ i , j ]  \mathrm { M I N } ( { D [ i - 1 , j ] } + d e l - c o s t ( s o u r c e [ i ] ) , } \\ & { \qquad D [ i - 1 , j - 1 ] + s u b - c o s t ( s o u r c e [ i ] , t a r g e t [ j ] ) , } \\ & { \qquad D [ i , j - 1 ] + i n s - c o s t ( t a r g e t [ j ] ) ) } \end{array}$ # Termination return $D [ \mathrm { n } , \mathrm { m } ]$

Figure 2.17 The minimum edit distance algorithm, an example of the class of dynamic programming algorithms. The various costs can either be fixed (e.g., $\forall x , \operatorname { i n s - c o s t } ( x ) = 1 ,$ ) or can be specific to the letter (to model the fact that some letters are more likely to be inserted than others). We assume that there is no cost for substituting a letter for itself (i.e., sub-cos $\mathfrak { t } ( x , x ) = 0 \quad$ ).

Figure 2.18 Computation of minimum edit distance between intention and execution with the algorithm of Fig. 2.17, using Levenshtein distance with cost of 1 for insertions or deletions, 2 for substitutions.   

<table><tr><td>Src\Tar</td><td>#</td><td>e</td><td>X</td><td>e</td><td>C</td><td>u</td><td>t</td><td>i</td><td>0</td><td>n</td></tr><tr><td>#</td><td>0</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td></tr><tr><td>i</td><td>1</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>6</td><td>7</td><td>8</td></tr><tr><td>n</td><td>2</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>7</td><td>8</td><td>7</td></tr><tr><td>t</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>7</td><td>8</td><td>9</td><td>8</td></tr><tr><td>e</td><td>4</td><td>3</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>9</td></tr><tr><td>n</td><td>5</td><td>4</td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>10</td></tr><tr><td></td><td></td><td>5</td><td>6</td><td>7</td><td>8</td><td>9</td><td>8</td><td>9</td><td>10</td><td>11</td></tr><tr><td>i</td><td>7</td><td>6</td><td>7</td><td>8</td><td>9</td><td>10</td><td>9</td><td>8</td><td>9</td><td>10</td></tr><tr><td>0</td><td>8</td><td>7</td><td>8</td><td>9</td><td>10</td><td>11</td><td>10</td><td>9</td><td>8</td><td>9</td></tr><tr><td>n</td><td>9</td><td>8</td><td>9</td><td>10</td><td>11</td><td>12</td><td>11</td><td>10</td><td>9</td><td>8</td></tr></table>

shows this path with boldfaced cells. Each boldfaced cell represents an alignment of a pair of letters in the two strings. If two boldfaced cells occur in the same row, there will be an insertion in going from the source to the target; two boldfaced cells in the same column indicate a deletion.

Figure 2.19 also shows the intuition of how to compute this alignment path. The computation proceeds in two steps. In the first step, we augment the minimum edit distance algorithm to store backpointers in each cell. The backpointer from a cell points to the previous cell (or cells) that we came from in entering the current cell. We’ve shown a schematic of these backpointers in Fig. 2.19. Some cells have multiple backpointers because the minimum extension could have come from multiple previous cells. In the second step, we perform a backtrace. In a backtrace, we start from the last cell (at the final row and column), and follow the pointers back through the dynamic programming matrix. Each complete path between the final cell and the initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the minimum edit distance algorithm to store the pointers and compute the backtrace to output an alignment.

<table><tr><td></td><td>#</td><td></td><td>e</td><td>X</td><td>e</td><td>C</td><td>u</td><td>t</td><td>i</td><td>0</td><td>n</td></tr><tr><td>#</td><td>0</td><td>←1</td><td>←2</td><td>←3</td><td>←4</td><td></td><td>←5←6</td><td></td><td>←7</td><td>←8←9</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>i↑1←↑2κ←↑3&lt;←↑4κ←↑5←↑6κ←↑7</td><td></td><td></td><td></td><td>6</td><td>←7←8</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>n↑2←↑3←↑4κ←↑5&lt;←↑6</td><td></td><td>K←↑7K←↑8</td><td></td><td></td><td>↑7←↑87</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td>t↑3←↑4K←↑5&lt;←↑6K←17</td><td></td><td>K←18K7</td><td></td><td></td><td>←↑8←↑9↑8</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td>e↑43←4←5</td><td>←6</td><td></td><td></td><td></td><td></td><td>←7←↑8←↑9←↑10↑9</td><td></td></tr><tr><td></td><td>n↑5</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>↑4←↑5κ←↑6κ←↑7κ←↑8κ←↑9κ←↑10κ←↑11↑10</td><td></td></tr><tr><td></td><td>t16</td><td></td><td></td><td></td><td>↑5←↑6K←17←↑8←198</td><td></td><td></td><td></td><td></td><td>←9←10←↑11</td><td></td></tr><tr><td></td><td>i17</td><td></td><td></td><td></td><td>↑6←↑7κ←↑8←↑9←↑10</td><td></td><td>19</td><td></td><td>8</td><td>←9←10</td><td></td></tr><tr><td></td><td>0↑8</td><td></td><td></td><td></td><td>↑7←↑8←↑9←↑10K←↑11</td><td></td><td>↑10</td><td></td><td>↑9</td><td>8←9</td><td></td></tr><tr><td></td><td>n↑9</td><td></td><td></td><td></td><td>↑8←↑9←↑10←↑11←↑12</td><td></td><td></td><td>个11</td><td>↑10</td><td></td><td>↑98</td></tr></table>

Figure 2.19 When entering a value in each cell, we mark which of the three neighboring cells we came from with up to three arrows. After the table is full we compute an alignment (minimum edit path) by using a backtrace, starting at the 8 in the lower-right corner and following the arrows back. The sequence of bold cells represents one possible minimum cost alignment between the two strings, again using Levenshtein distance with cost of 1 for insertions or deletions, 2 for substitutions. Diagram design after Gusfield (1997).

While we worked our example with simple Levenshtein distance, the algorithm in Fig. 2.17 allows arbitrary weights on the operations. For spelling correction, for example, substitutions are more likely to happen between letters that are next to each other on the keyboard. The Viterbi algorithm is a probabilistic extension of minimum edit distance. Instead of computing the “minimum edit distance” between two strings, Viterbi computes the “maximum probability alignment” of one string with another. We’ll discuss this more in Chapter 17.

# 2.9 Summary

This chapter introduced a fundamental tool in language processing, the regular expression, and showed how to perform basic text normalization tasks including word segmentation and normalization, sentence segmentation, and stemming. We also introduced the important minimum edit distance algorithm for comparing strings. Here’s a summary of the main points we covered about these ideas:

• The regular expression language is a powerful tool for pattern-matching. • Basic operations in regular expressions include concatenation of symbols, disjunction of symbols ([], |), counters $( \ast , + ,$ , and $^ \mathrm { \{ n , m \} }$ ), anchors $( \hat { \textbf { \xi } } , \pmb { \ S } )$ and precedence operators ((,)). • Word tokenization and normalization are generally done by cascades of simple regular expression substitutions or finite automata. • The Porter algorithm is a simple and efficient way to do stemming, stripping off affixes. It does not have high accuracy but may be useful for some tasks.

• The minimum edit distance between two strings is the minimum number of operations it takes to edit one into the other. Minimum edit distance can be computed by dynamic programming, which also results in an alignment of the two strings.

# Bibliographical and Historical Notes

Kleene 1951; 1956 first defined regular expressions and the finite automaton, based on the McCulloch-Pitts neuron. Ken Thompson was one of the first to build regular expressions compilers into editors for text searching (Thompson, 1968). His editor ed included a command “g/regular expression/p”, or Global Regular Expression Print, which later became the Unix grep utility.

Text normalization algorithms have been applied since the beginning of the field. One of the earliest widely used stemmers was Lovins (1968). Stemming was also applied early to the digital humanities, by Packard (1973), who built an affix-stripping morphological parser for Ancient Greek. Currently a wide variety of code for tokenization and normalization is available, such as the Stanford Tokenizer (https://nlp.stanford.edu/software/tokenizer.shtml) or specialized tokenizers for Twitter (O’Connor et al., 2010), or for sentiment (http: //sentiment.christopherpotts.net/tokenizing.html). See Palmer (2012) for a survey of text preprocessing. NLTK is an essential tool that offers both useful Python libraries (https://www.nltk.org) and textbook descriptions (Bird et al., 2009) of many algorithms including text normalization and corpus interfaces.

For more on Herdan’s law and Heaps’ Law, see Herdan (1960, p. 28), Heaps (1978), Egghe (2007) and Baayen (2001); For more on edit distance, see Gusfield (1997). Our example measuring the edit distance from ‘intention’ to ‘execution’ was adapted from Kruskal (1983). There are various publicly available packages to compute edit distance, including Unix diff and the NIST sclite program (NIST, 2005).

In his autobiography Bellman (1984) explains how he originally came up with the term dynamic programming:

“...The 1950s were not good years for mathematical research. [the] Secretary of Defense ...had a pathological fear and hatred of the word, research... I decided therefore to use the word, “programming”. I wanted to get across the idea that this was dynamic, this was multistage... I thought, let’s ... take a word that has an absolutely precise meaning, namely dynamic... it’s impossible to use the word, dynamic, in a pejorative sense. Try thinking of some combination that will possibly give it a pejorative meaning. It’s impossible. Thus, I thought dynamic programming was a good name. It was something not even a Congressman could object to.”

# Exercises

2.1 Write regular expressions for the following languages.

1. the set of all alphabetic strings;   
2. the set of all lower case alphabetic strings ending in a $^ b$ ;

3. the set of all strings from the alphabet $^ { a , b }$ such that each $a$ is immediately preceded by and immediately followed by a $^ b$ ;

2.2 Write regular expressions for the following languages. By “word”, we mean an alphabetic string separated from other words by whitespace, any relevant punctuation, line breaks, and so forth.

1. the set of all strings with two consecutive repeated words (e.g., “Humbert Humbert” and “the the” but not “the bug” or “the big bug”);   
2. all strings that start at the beginning of the line with an integer and that end at the end of the line with a word;   
3. all strings that have both the word grotto and the word raven in them (but not, e.g., words like grottos that merely contain the word grotto);   
4. write a pattern that places the first word of an English sentence in a register. Deal with punctuation.   
2.3 Implement an ELIZA-like program, using substitutions such as those described on page 12. You might want to choose a different domain than a Rogerian psychologist, although keep in mind that you would need a domain in which your program can legitimately engage in a lot of simple repetition.   
2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution cost 1) of “leda” to “deal”. Show your work (using the edit distance grid).   
2.5 Figure out whether drive is closer to brief or to divers and what the edit distance is to each. You may use any version of distance that you like.   
2.6 Now implement a minimum edit distance algorithm and use your hand-computed results to check your code.   
2.7 Augment the minimum edit distance algorithm to output an alignment; you will need to store pointers and add a stage to compute the backtrace.

# 3

# N-gram Language Models

“You are uniformly charming!” cried he, with a smile of associating and now and then I bowed and they perceived a chaise and four to wish for. Random sentence generated from a Jane Austen trigram model

Predicting is difficult—especially about the future, as the old quip goes. But how about predicting something that seems much easier, like the next word someone is going to say? What word, for example, is likely to follow

The water of Walden Pond is so beautifully ...

LM

You might conclude that a likely word is blue, or green, or clear, but probably not refrigerator nor this. In this chapter we formalize this intuition by introducing language models or LMs. A language model is a machine learning model that predicts upcoming words. More formally, a language model assigns a probability to each possible next word, or equivalently gives a probability distribution over possible next works. Language models can also assign a probability to an entire sentence. Thus an LM could tell us that the following sequence has a much higher probability of appearing in a text:

all of a sudden I notice three guys standing on the sidewalk than does this same set of words in a different order:

on guys all I of notice sidewalk three a sudden standing the

# AAC

Why would we want to predict upcoming words, or know the probability of a sentence? One reason is for generation: choosing contextually better words. For example we can correct grammar or spelling errors like Their are two midterms, in which There was mistyped as Their, or Everything has improve, in which improve should have been improved. The phrase There are is more probable than Their are, and has improved than has improve, so a language model can help users select the more grammatical variant. Or for a speech system to recognize that you said I will be back soonish and not I will be bassoon dish, it helps to know that back soonish is a more probable sequence. Language models can also help in augmentative and alternative communication (Trnka et al. 2007, Kane et al. 2017). People can use AAC systems if they are physically unable to speak or sign but can instead use eye gaze or other movements to select words from a menu. Word prediction can be used to suggest likely words for the menu.

Word prediction is also central to NLP for another reason: large language models are built just by training them to predict words!! As we’ll see in chapters 7-9, large language models learn an enormous amount about language solely from being trained to predict upcoming words from neighboring words.

In this chapter we introduce the simplest kind of language model: the n-gram language model. An n-gram is a sequence of $n$ words: a 2-gram (which we’ll call bigram) is a two-word sequence of words like The water, or water of, and a 3- gram (a trigram) is a three-word sequence of words like The water of, or water of Walden. But we also (in a bit of terminological ambiguity) use the word ‘ngram’ to mean a probabilistic model that can estimate the probability of a word given the n-1 previous words, and thereby also to assign probabilities to entire sequences.

In later chapters we will introduce the much more powerful neural large language models, based on the transformer architecture of Chapter 9. But because n-grams have a remarkably simple and clear formalization, we use them to introduce some major concepts of large language modeling, including training and test sets, perplexity, sampling, and interpolation.

# 3.1 N-Grams

Let’s begin with the task of computing $P ( w | h )$ , the probability of a word $w$ given some history $h$ . Suppose the history $h$ is “The water of Walden Pond is so beautifully ” and we want to know the probability that the next word is blue:

P(blue|The water of Walden Pond is so beautifully)

One way to estimate this probability is directly from relative frequency counts: take a very large corpus, count the number of times we see The water of Walden Pond is so beautifully, and count the number of times this is followed by blue. This would be answering the question “Out of the times we saw the history $h$ , how many times was it followed by the word $w ^ { \prime \prime }$ , as follows:

$$
\begin{array} { r } { P \mathrm { ( b l u e / T h e ~ w a t e r ~ o f ~ w a l d e n ~ P o n d ~ i s ~ s o ~ b e a u t i f u l l y ) = } } \\ { \frac { C \mathrm { ( T h e ~ w a t e r ~ o f ~ W a l d e n ~ P o n d ~ i s ~ s o ~ b e a u t i f u l l y ~ b l u e ) } } { C \mathrm { ( T h e ~ w a t e r ~ o f ~ W a l d e n ~ P o n d ~ i s ~ s o ~ b e a u t i f u l l y ) } } } \end{array}
$$

If we had a large enough corpus, we could compute these two counts and estimate the probability from Eq. 3.2. But even the entire web isn’t big enough to give us good estimates for counts of entire sentences. This is because language is creative; new sentences are invented all the time, and we can’t expect to get accurate counts for such large objects as entire sentences. For this reason, we’ll need more clever ways to estimate the probability of a word $w$ given a history $h$ , or the probability of an entire word sequence $W$ .

Let’s start with some notation. First, throughout this chapter we’ll continue to refer to words, although in practice we usually compute language models over tokens like the BPE tokens of page 20. To represent the probability of a particular random variable $X _ { i }$ taking on the value “the”, or $P ( X _ { i } = " \mathrm { t h e } ^ { \prime \prime } )$ , we will use the simplification $P ( t h e )$ . We’ll represent a sequence of $n$ words either as $w _ { 1 } \ldots w _ { n }$ or $w _ { 1 : n }$ . Thus the expression $w _ { 1 : n - 1 }$ means the string $w _ { 1 } , w _ { 2 } , . . . , w _ { n - 1 }$ , but we’ll also be using the equivalent notation $w _ { < n }$ , which can be read as “all the elements of $w$ from $w _ { 1 }$ up to and including $w _ { n - 1 } { } ^ { , }$ . For the joint probability of each word in a sequence having a particular value $P ( X _ { 1 } = w _ { 1 } , X _ { 2 } = w _ { 2 } , X _ { 3 } = w _ { 3 } , . . . , X _ { n } = w _ { n } )$ we’ll use $P ( w _ { 1 } , w _ { 2 } , . . . , w _ { n } )$ .

Now, how can we compute probabilities of entire sequences like $P ( w _ { 1 } , w _ { 2 } , . . . , w _ { n } ) ?$ One thing we can do is decompose this probability using the chain rule of proba

bility:

$$
{ \begin{array} { l } { P ( X _ { 1 } \ldots X _ { n } ) \ = \ P ( X _ { 1 } ) P ( X _ { 2 } | X _ { 1 } ) P ( X _ { 3 } | X _ { 1 : 2 } ) \ldots P ( X _ { n } | X _ { 1 : n - 1 } ) } \\ { \ = \ \displaystyle \prod _ { k = 1 } ^ { n } P ( X _ { k } | X _ { 1 : k - 1 } ) } \end{array} }
$$

Applying the chain rule to words, we get

$$
\begin{array} { l } { P ( w _ { 1 : n } ) ~ = ~ P ( w _ { 1 } ) P ( w _ { 2 } | w _ { 1 } ) P ( w _ { 3 } | w _ { 1 : 2 } ) \dots P ( w _ { n } | w _ { 1 : n - 1 } ) } \\ { ~ = ~ \displaystyle \prod _ { k = 1 } ^ { n } P ( w _ { k } | w _ { 1 : k - 1 } ) } \end{array}
$$

The chain rule shows the link between computing the joint probability of a sequence and computing the conditional probability of a word given previous words. Equation 3.4 suggests that we could estimate the joint probability of an entire sequence of words by multiplying together a number of conditional probabilities. But using the chain rule doesn’t really seem to help us! We don’t know any way to compute the exact probability of a word given a long sequence of preceding words, $P ( w _ { n } | w _ { 1 : n - 1 } )$ . As we said above, we can’t just estimate by counting the number of times every word occurs following every long string in some corpus, because language is creative and any particular context might have never occurred before!

# 3.1.1 The Markov assumption

The intuition of the $\mathbf { n }$ -gram model is that instead of computing the probability of a word given its entire history, we can approximate the history by just the last few words.

The bigram model, for example, approximates the probability of a word given all the previous words $P ( w _ { n } | w _ { 1 : n - 1 } )$ by using only the conditional probability given the preceding word $P ( w _ { n } | w _ { n - 1 } )$ . In other words, instead of computing the probability

P(blue|The water of Walden Pond is so beautifully)

we approximate it with the probability

$$
P ( { \mathrm { b l u e } } | { \mathrm { b e a u t i f u l l y } } )
$$

When we use a bigram model to predict the conditional probability of the next word, we are thus making the following approximation:

$$
P ( w _ { n } | w _ { 1 : n - 1 } ) \approx P ( w _ { n } | w _ { n - 1 } )
$$

The assumption that the probability of a word depends only on the previous word is called a Markov assumption. Markov models are the class of probabilistic models that assume we can predict the probability of some future unit without looking too far into the past. We can generalize the bigram (which looks one word into the past) to the trigram (which looks two words into the past) and thus to the $\mathbf { n }$ -gram (which looks $n - 1$ words into the past).

Let’s see a general equation for this n-gram approximation to the conditional probability of the next word in a sequence. We’ll use $N$ here to mean the n-gram size, so $N = 2$ means bigrams and $N = 3$ means trigrams. Then we approximate the probability of a word given its entire context as follows:

$$
P ( w _ { n } | w _ { 1 : n - 1 } ) \approx P ( w _ { n } | w _ { n - N + 1 : n - 1 } )
$$

Given the bigram assumption for the probability of an individual word, we can compute the probability of a complete word sequence by substituting Eq. 3.7 into Eq. 3.4:

$$
P ( w _ { 1 : n } ) \approx \prod _ { k = 1 } ^ { n } P ( w _ { k } | w _ { k - 1 } )
$$

# 3.1.2 How to estimate probabilities

How do we estimate these bigram or n-gram probabilities? An intuitive way to estimate probabilities is called maximum likelihood estimation or MLE. We get the MLE estimate for the parameters of an n-gram model by getting counts from a corpus, and normalizing the counts so that they lie between 0 and 1. For probabilistic models, normalizing means dividing by some total count so that the resulting probabilities fall between 0 and 1 and sum to 1.

For example, to compute a particular bigram probability of a word $w _ { n }$ given a previous word $w _ { n - 1 }$ , we’ll compute the count of the bigram $C ( w _ { n - 1 } w _ { n } )$ and normalize by the sum of all the bigrams that share the same first word $w _ { n - 1 }$ :

$$
P ( w _ { n } | w _ { n - 1 } ) = \frac { C \big ( w _ { n - 1 } w _ { n } \big ) } { \sum _ { w } C \big ( w _ { n - 1 } w \big ) }
$$

We can simplify this equation, since the sum of all bigram counts that start with a given word $w _ { n - 1 }$ must be equal to the unigram count for that word $w _ { n - 1 }$ (the reader should take a moment to be convinced of this):

$$
P ( w _ { n } | w _ { n - 1 } ) = { \frac { C ( w _ { n - 1 } w _ { n } ) } { C ( w _ { n - 1 } ) } }
$$

Let’s work through an example using a mini-corpus of three sentences. We’ll first need to augment each sentence with a special symbol ${ \bf \zeta } < { \bf s } >$ at the beginning of the sentence, to give us the bigram context of the first word. We’ll also need a special end-symbol $< / \mathsf { s } >$ . 1

$\begin{array} { c } { { < s > \mathrm { ~ I ~ } \mathrm { a m ~ } \ S \mathrm { a m ~ } < / s > } } \\ { { < s > \mathrm { ~ S a m ~ I ~ } \mathrm { a m ~ } < / s > } } \\ { { < s > \mathrm { ~ I ~ } \mathrm { d o ~ n o t ~ } \mathrm { 1 i k e } } } \end{array}$ green eggs and ham $< / { \mathsf { s } } { \mathsf { > } }$

Here are the calculations for some of the bigram probabilities from this corpus

$$
{ \begin{array} { r l r l } & { P ( \mathrm { I } | < \mathsf { s } > ) = { \frac { 2 } { 3 } } = 0 . 6 7 } & & { P ( \mathsf { S } \mathbf { a m } | < \mathsf { s } > ) = { \frac { 1 } { 3 } } = 0 . 3 3 } & & { P ( \mathbf { a m } | \mathrm { I } ) = { \frac { 2 } { 3 } } = 0 . 6 7 } \\ & { P ( < / \mathbf { s } > | \mathsf { S } \mathbf { a m } ) = { \frac { 1 } { 2 } } = 0 . 5 } & & { P ( \mathsf { S } \mathbf { a m } | \mathbf { a m } ) = { \frac { 1 } { 2 } } = 0 . 5 } & & { P ( \mathbf { d o } | \mathrm { I } ) = { \frac { 1 } { 3 } } = 0 . 3 3 } \end{array} }
$$

For the general case of MLE $\mathbf { n }$ -gram parameter estimation:

$$
P ( w _ { n } | w _ { n - N + 1 : n - 1 } ) = { \frac { C ( w _ { n - N + 1 : n - 1 } ~ w _ { n } ) } { C ( w _ { n - N + 1 : n - 1 } ) } }
$$

Equation 3.12 (like Eq. 3.11) estimates the n-gram probability by dividing the observed frequency of a particular sequence by the observed frequency of a prefix. This ratio is called a relative frequency. We said above that this use of relative frequencies as a way to estimate probabilities is an example of maximum likelihood estimation or MLE. In MLE, the resulting parameter set maximizes the likelihood of the training set $T$ given the model $M$ (i.e., $P ( { \cal T } | M ) )$ . For example, suppose the word Chinese occurs 400 times in a corpus of a million words. What is the probability that a random word selected from some other text of, say, a million words will be the word Chinese? The MLE of its probability is $\frac { 4 0 0 } { 1 0 0 0 0 0 0 }$ or 0.0004. Now 0.0004 is not the best possible estimate of the probability of Chinese occurring in all situations; it might turn out that in some other corpus or context Chinese is a very unlikely word. But it is the probability that makes it most likely that Chinese will occur 400 times in a million-word corpus. We present ways to modify the MLE estimates slightly to get better probability estimates in Section 3.6.

Let’s move on to some examples from a real but tiny corpus, drawn from the now-defunct Berkeley Restaurant Project, a dialogue system from the last century that answered questions about a database of restaurants in Berkeley, California (Jurafsky et al., 1994). Here are some sample user queries (text-normalized, by lower casing and with punctuation striped) (a sample of 9332 sentences is on the website):

can you tell me about any good cantonese restaurants close by tell me about chez panisse i’m looking for a good place to eat breakfast when is caffe venezia open during the day

Figure 3.1 shows the bigram counts from part of a bigram grammar from textnormalized Berkeley Restaurant Project sentences. Note that the majority of the values are zero. In fact, we have chosen the sample words to cohere with each other; a matrix selected from a random set of eight words would be even more sparse.   

<table><tr><td></td><td>i</td><td>want</td><td>to</td><td>eat</td><td>chinese</td><td>food</td><td>lunch</td><td> spend</td></tr><tr><td>i</td><td>5</td><td>827</td><td>0</td><td>9</td><td>0</td><td>0</td><td>0</td><td>2</td></tr><tr><td> want</td><td>2</td><td>0</td><td>608</td><td>1</td><td>6</td><td>6</td><td>5</td><td>1</td></tr><tr><td>to</td><td>2</td><td>0</td><td>4</td><td>686</td><td>2</td><td>0</td><td>6</td><td>211</td></tr><tr><td>eat</td><td>0</td><td>0</td><td>2</td><td>0</td><td>16</td><td>2</td><td>42</td><td>0</td></tr><tr><td>chinese</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>82</td><td>1</td><td>0</td></tr><tr><td>food</td><td>15</td><td>0</td><td>15</td><td>0</td><td>1</td><td>4</td><td>0</td><td>0</td></tr><tr><td> lunch</td><td>2</td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>0</td><td>0</td></tr><tr><td> spend</td><td>1</td><td>0</td><td>1</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

Figure 3.1 Bigram counts for eight of the words (out of $V = 1 4 4 6$ in the Berkeley Restaurant Project corpus of 9332 sentences. Zero counts are in gray. Each cell shows the count of the column label word following the row label word. Thus the cell in row i and column want means that want followed i 827 times in the corpus.

Figure 3.2 shows the bigram probabilities after normalization (dividing each cell in Fig. 3.1 by the appropriate unigram for its row, taken from the following set of unigram counts):   

<table><tr><td>i</td><td>want</td><td>to</td><td>eat</td><td>chinese</td><td>food</td><td>lunch spend</td><td></td></tr><tr><td>2533</td><td>927</td><td>2417</td><td>746</td><td>158</td><td>1093</td><td>341</td><td>278</td></tr></table>

<table><tr><td></td><td>i</td><td>want</td><td>to</td><td>eat</td><td>chinese</td><td>food</td><td>lunch</td><td> spend</td></tr><tr><td>i</td><td>0.002</td><td>0.33</td><td>0</td><td>0.0036</td><td>0</td><td>0</td><td>0</td><td>0.00079</td></tr><tr><td> want</td><td>0.0022</td><td>0</td><td>0.66</td><td>0.0011</td><td>0.0065</td><td>0.0065</td><td>0.0054</td><td>0.0011</td></tr><tr><td>to</td><td>0.00083</td><td>0</td><td>0.0017</td><td>0.28</td><td>0.00083</td><td>0</td><td>0.0025</td><td>0.087</td></tr><tr><td>eat</td><td>0</td><td>0</td><td>0.0027</td><td>0</td><td>0.021</td><td>0.0027</td><td>0.056</td><td>0</td></tr><tr><td>chinese</td><td>0.0063</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.52</td><td>0.0063</td><td>0</td></tr><tr><td>food</td><td>0.014</td><td>0</td><td>0.014</td><td>0</td><td>0.00092</td><td>0.0037</td><td>0</td><td>0</td></tr><tr><td> lunch</td><td>0.0059</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0.0029</td><td>0</td><td>0</td></tr><tr><td>spend</td><td>0.0036</td><td>0</td><td>0.0036</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td></tr></table>

Figure 3.2 Bigram probabilities for eight words in the Berkeley Restaurant Project corpus of 9332 sentences. Zero probabilities are in gray.

Here are a few other useful probabilities:

$$
\begin{array} { r l } { P ( \mathbf { i } | < \mathbf { s } > ) = 0 . 2 5 \ : \ : \ : \ : \ : } & { { } P ( \mathbf { e n g l i s h } | \mathbf { w a n t } ) = 0 . 0 0 1 1 } \\ { P ( \mathbf { f o o d } | \mathbf { e n g l i s h } ) = 0 . 5 \ : \ : \ : } & { { } P ( < / \mathbf { s } > | \mathbf { f o o d } ) = 0 . 6 8 } \end{array}
$$

Now we can compute the probability of sentences like $I$ want English food or I want Chinese food by simply multiplying the appropriate bigram probabilities together, as follows:

$$
\begin{array} { r l } { P ( < \mathsf { s } > \mathrm { ~ i ~ } \mathsf { w a n t ~ e n g l i s h ~ f o o d ~ } < / \mathsf { s } > ) } & { } \\ { = } & { P ( \mathrm { i } | < \mathsf { s } > ) P ( \mathsf { w a n t ~ } | \mathrm { i } ) P ( \mathsf { e n g l i s h } | \mathsf { w a n t } ) } \\ & { \qquad P ( \pounds { \mathsf { o o d } } | \mathsf { e n g l i s h } ) P ( < / \mathsf { s } > | \pounds { \mathsf { o o d } } ) } \\ { = } & { 0 . 2 5 \times 0 . 3 3 \times 0 . 0 0 1 1 \times 0 . 5 \times 0 . 6 8 } \\ { = } & { 0 . 0 0 0 0 3 1 } \end{array}
$$

We leave it as Exercise 3.2 to compute the probability of $i$ want chinese food.

What kinds of linguistic phenomena are captured in these bigram statistics? Some of the bigram probabilities above encode some facts that we think of as strictly syntactic in nature, like the fact that what comes after eat is usually a noun or an adjective, or that what comes after $t o$ is usually a verb. Others might be a fact about the personal assistant task, like the high probability of sentences beginning with the words $I .$ And some might even be cultural rather than linguistic, like the higher probability that people are looking for Chinese versus English food.

# 3.1.3 Dealing with scale in large n-gram models

In practice, language models can be very large, leading to practical issues.

Log probabilities Language model probabilities are always stored and computed in log space as log probabilities. This is because probabilities are (by definition) less than or equal to 1, and so the more probabilities we multiply together, the smaller the product becomes. Multiplying enough n-grams together would result in numerical underflow. Adding in log space is equivalent to multiplying in linear space, so we combine log probabilities by adding them. By adding log probabilities instead of multiplying probabilities, we get results that are not as small. We do all computation and storage in log space, and just convert back into probabilities if we need to report probabilities at the end by taking the exp of the logprob:

$$
p _ { 1 } \times p _ { 2 } \times p _ { 3 } \times p _ { 4 } = \exp ( \log p _ { 1 } + \log p _ { 2 } + \log p _ { 3 } + \log p _ { 4 } )
$$

In practice throughout this book, we’ll use log to mean natural log (ln) when the base is not specified.

Longer context Although for pedagogical purposes we have only described bigram models, when there is sufficient training data we use trigram models, which condition on the previous two words, or 4-gram or 5-gram models. For these larger n-grams, we’ll need to assume extra contexts to the left and right of the sentence end. For example, to compute trigram probabilities at the very beginning of the sentence, we use two pseudo-words for the first trigram (i.e., $P ( \mathbb { I } | < { \mathsf { s } } > < { \mathsf { s } } > )$ .

Some large n-gram datasets have been created, like the million most frequent n-grams drawn from the Corpus of Contemporary American English (COCA), a curated 1 billion word corpus of American English (Davies, 2020), Google’s Web 5-gram corpus from 1 trillion words of English web text (Franz and Brants, 2006), or the Google Books Ngrams corpora (800 billion tokens from Chinese, English, French, German, Hebrew, Italian, Russian, and Spanish) (Lin et al., 2012a)).

It’s even possible to use extremely long-range n-gram context. The infini-gram (∞-gram) project (Liu et al., 2024) allows n-grams of any length. Their idea is to avoid the expensive (in space and time) pre-computation of huge n-gram count tables. Instead, n-gram probabilities with arbitrary n are computed quickly at inference time by using an efficient representation called suffix arrays. This allows computing of n-grams of every length for enormous corpora of 5 trillion tokens.

Efficiency considerations are important when building large n-gram language models. It is standard to quantize the probabilities using only 4-8 bits (instead of 8-byte floats), store the word strings on disk and represent them in memory only as a 64-bit hash, and represent n-grams in special data structures like ‘reverse tries’. It is also common to prune n-gram language models, for example by only keeping n-grams with counts greater than some threshold or using entropy to prune lessimportant n-grams (Stolcke, 1998). Efficient language model toolkits like KenLM (Heafield 2011, Heafield et al. 2013) use sorted arrays and use merge sorts to efficiently build the probability tables in a minimal number of passes through a large corpus.

# 3.2 Evaluating Language Models: Training and Test Sets

# extrinsic evaluation

The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to know if a particular improvement in the language model (or any component) is really going to help the task at hand. Thus for evaluating n-gram language models that are a component of some task like speech recognition or machine translation, we can compare the performance of two candidate language models by running the speech recognizer or machine translator twice, once with each language model, and seeing which gives the more accurate transcription.

# intrinsic evaluation

Unfortunately, running big NLP systems end-to-end is often very expensive. Instead, it’s helpful to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that measures the quality of a model independent of any application. In the next section we’ll introduce perplexity, which is the standard intrinsic metric for measuring language model performance, both for simple n-gram language models and for the more sophisticated neural large language models of Chapter 9.

training set   
development set test set

In order to evaluate any machine learning model, we need to have at least three distinct data sets: the training set, the development set, and the test set.

The training set is the data we use to learn the parameters of our model; for simple n-gram language models it’s the corpus from which we get the counts that we normalize into the probabilities of the n-gram language model.

The test set is a different, held-out set of data, not overlapping with the training set, that we use to evaluate the model. We need a separate test set to give us an unbiased estimate of how well the model we trained can generalize when we apply it to some new unknown dataset. A machine learning model that perfectly captured the training data, but performed terribly on any other data, wouldn’t be much use when it comes time to apply it to any new data or problem! We thus measure the quality of an n-gram model by its performance on this unseen test set or test corpus.

How should we choose a training and test set? The test set should reflect the language we want to use the model for. If we’re going to use our language model for speech recognition of chemistry lectures, the test set should be text of chemistry lectures. If we’re going to use it as part of a system for translating hotel booking requests from Chinese to English, the test set should be text of hotel booking requests. If we want our language model to be general purpose, then the test set should be drawn from a wide variety of texts. In such cases we might collect a lot of texts from different sources, and then divide it up into a training set and a test set. It’s important to do the dividing carefully; if we’re building a general purpose model, we don’t want the test set to consist of only text from one document, or one author, since that wouldn’t be a good measure of general performance.

Thus if we are given a corpus of text and want to compare the performance of two different n-gram models, we divide the data into training and test sets, and train the parameters of both models on the training set. We can then compare how well the two trained models fit the test set.

But what does it mean to “fit the test set”? The standard answer is simple: whichever language model assigns a higher probability to the test set—which means it more accurately predicts the test set—is a better model. Given two probabilistic models, the better model is the one that better predicts the details of the test data, and hence will assign a higher probability to the test data.

Since our evaluation metric is based on test set probability, it’s important not to let the test sentences into the training set. Suppose we are trying to compute the probability of a particular “test” sentence. If our test sentence is part of the training corpus, we will mistakenly assign it an artificially high probability when it occurs in the test set. We call this situation training on the test set. Training on the test set introduces a bias that makes the probabilities all look too high, and causes huge inaccuracies in perplexity, the probability-based metric we introduce below.

Even if we don’t train on the test set, if we test our language model on the test set many times after making different changes, we might implicitly tune to its characteristics, by noticing which changes seem to make the model better. For this reason, we only want to run our model on the test set once, or a very few number of times, once we are sure our model is ready.

For this reason we normally instead have a third dataset called a development test set or, devset. We do all our testing on this dataset until the very end, and then we test on the test set once to see how good our model is.

How do we divide our data into training, development, and test sets? We want our test set to be as large as possible, since a small test set may be accidentally unrepresentative, but we also want as much training data as possible. At the minimum, we would want to pick the smallest test set that gives us enough statistical power to measure a statistically significant difference between two potential models. It’s important that the devset be drawn from the same kind of text as the test set, since its goal is to measure how we would do on the test set.

# 3.3 Evaluating Language Models: Perplexity

We said above that we evaluate language models based on which one assigns a higher probability to the test set. A better model is better at predicting upcoming words, and so it will be less surprised by (i.e., assign a higher probability to) each word when it occurs in the test set. Indeed, a perfect language model would correctly guess each next word in a corpus, assigning it a probability of 1, and all the other words a probability of zero. So given a test corpus, a better language model will assign a higher probability to it than a worse language model.

But in fact, we do not use raw probability as our metric for evaluating language models. The reason is that the probability of a test set (or any sequence) depends on the number of words or tokens in it; the probability of a test set gets smaller the longer the text. We’d prefer a metric that is per-word, normalized by length, so we could compare across texts of different lengths. The metric we use is, a function of probability called perplexity, is one of the most important metrics in NLP, used for evaluating large language models as well as n-gram models.

The perplexity (sometimes abbreviated as PP or PPL) of a language model on a test set is the inverse probability of the test set (one over the probability of the test set), normalized by the number of words (or tokens). For this reason it’s sometimes called the per-word or per-token perplexity. We normalize by the number of words $N$ by taking the Nth root. For a test set $W = w _ { 1 } w _ { 2 } \dots w _ { N } ,$ :

$$
\begin{array} { r c l } { { \mathrm { p e r p l e x i t y } ( W ) } } & { { = } } & { { P ( w _ { 1 } w _ { 2 } \ldots \ldots w _ { N } ) ^ { - \frac { 1 } { N } } } } \\ { { } } & { { = } } & { { \sqrt [ N ] { \frac { 1 } { P ( w _ { 1 } w _ { 2 } \ldots w _ { N } ) } } } } \end{array}
$$

Or we can use the chain rule to expand the probability of $W$ :

$$
\mathrm { p e r p l e x i t y } ( W ) ~ = ~ \sqrt [ N ] { \prod _ { i = 1 } ^ { N } \frac { 1 } { P ( w _ { i } | w _ { 1 } \ldots w _ { i - 1 } ) } }
$$

Note that because of the inverse in Eq. 3.15, the higher the probability of the word sequence, the lower the perplexity. Thus the the lower the perplexity of a model on the data, the better the model. Minimizing perplexity is equivalent to maximizing the test set probability according to the language model. Why does perplexity use the inverse probability? It turns out the inverse arises from the original definition of perplexity from cross-entropy rate in information theory; for those interested, the explanation is in the advanced Section 3.7. Meanwhile, we just have to remember that perplexity has an inverse relationship with probability.

The details of computing the perplexity of a test set $W$ depends on which language model we use. Here’s the perplexity of $W$ with a unigram language model (just the geometric mean of the inverse of the unigram probabilities):

$$
\mathrm { p e r p l e x i t y } ( W ) = \sqrt [ N ] { \prod _ { i = 1 } ^ { N } \frac { 1 } { P ( w _ { i } ) } }
$$

The perplexity of $W$ computed with a bigram language model is still a geometric mean, but now of the inverse of the bigram probabilities:

$$
\mathrm { p e r p l e x i t y } ( W ) = \sqrt [ N ] { \prod _ { i = 1 } ^ { N } \frac { 1 } { P ( w _ { i } | w _ { i - 1 } ) } }
$$

What we generally use for word sequence in Eq. 3.15 or Eq. 3.17 is the entire sequence of words in some test set. Since this sequence will cross many sentence boundaries, if our vocabulary includes a between-sentence token ${ \tt - E O S } >$ or separate begin- and end-sentence markers ${ \tt < s > }$ and $< / { \mathsf { s } } { \mathsf { > } }$ then we can include them in the probability computation. If we do, then we also include one token per sentence in the total count of word tokens $N$ .2

We mentioned above that perplexity is a function of both the text and the language model: given a text $W$ , different language models will have different perplexities. Because of this, perplexity can be used to compare different language models. For example, here we trained unigram, bigram, and trigram grammars on 38 million words from the Wall Street Journal newspaper. We then computed the perplexity of each of these models on a WSJ test set using Eq. 3.16 for unigrams, Eq. 3.17 for bigrams, and the corresponding equation for trigrams. The table below shows the perplexity of the 1.5 million word test set according to each of the language models.

<table><tr><td></td><td>Unigram Bigram</td><td> Trigram</td></tr><tr><td>Perplexity 962</td><td>170</td><td>109</td></tr></table>

As we see above, the more information the n-gram gives us about the word sequence, the higher the probability the n-gram will assign to the string. A trigram model is less surprised than a unigram model because it has a better idea of what words might come next, and so it assigns them a higher probability. And the higher the probability, the lower the perplexity (since as Eq. 3.15 showed, perplexity is related inversely to the probability of the test sequence according to the model). So a lower perplexity tells us that a language model is a better predictor of the test set.

Note that in computing perplexities, the language model must be constructed without any knowledge of the test set, or else the perplexity will be artificially low. And the perplexity of two language models is only comparable if they use identical vocabularies.

An (intrinsic) improvement in perplexity does not guarantee an (extrinsic) improvement in the performance of a language processing task like speech recognition or machine translation. Nonetheless, because perplexity usually correlates with task improvements, it is commonly used as a convenient evaluation metric. Still, when possible a model’s improvement in perplexity should be confirmed by an end-to-end evaluation on a real task.

# 3.3.1 Perplexity as Weighted Average Branching Factor

It turns out that perplexity can also be thought of as the weighted average branching factor of a language. The branching factor of a language is the number of possible next words that can follow any word. For example consider a mini artificial

language that is deterministic (no probabilities), any word can follow any word, and whose vocabulary consists of only three colors:

$$
L = \{ { \bf r e d } , { \bf b l u e } , { \bf g r e e n } \}
$$

The branching factor of this language is 3.

Now let’s make a probabilistic version of the same LM, let’s call it $A$ , where each word follows each other with equal probability $\frac 1 3$ (it was trained on a training set with equal counts for the 3 colors), and a test set $T =$ “red red red red blue”.

Let’s first convince ourselves that if we compute the perplexity of this artificial digit language on this test set (or any such test set) we indeed get 3. By Eq. 3.15, the perplexity of $A$ on $T$ is:

$$
{ \begin{array} { l } { { \mathrm { p e r p l e x i t y } } _ { A } ( T ) \ = \ P _ { A } ( { \mathrm { r e d ~ r e d ~ r e d ~ r e d ~ b l u e } } ) ^ { - { \frac { 1 } { 5 } } } } \\ { \ = \ \left( \left( { \frac { 1 } { 3 } } \right) ^ { 5 } \right) ^ { - { \frac { 1 } { 5 } } } } \\ { \ = \ \left( { \frac { 1 } { 3 } } \right) ^ { - 1 } = 3 } \end{array} }
$$

But now suppose red was very likely in the training set a different $\mathrm { ~ L M } B$ , and so $B$ has the following probabilities:

$$
P ( { \bf r e d } ) = 0 . 8 P ( { \bf g r e e n } ) = 0 . 1 P ( { \bf b l u e } ) = 0 . 1
$$

We should expect the perplexity of the same test set red red red red blue for language model $B$ to be lower since most of the time the next color will be red, which is very predictable, i.e. has a high probability. So the probability of the test set will be higher, and since perplexity is inversely related to probability, the perplexity will be lower. Thus, although the branching factor is still 3, the perplexity or weighted branching factor is smaller:

$$
\begin{array} { l } { \mathrm { p e r p l e x i t y } _ { B } ( T ) \ = \ P _ { B } ( \mathrm { r e d \ r e d \ r e d \ r e d \ b l u e } ) ^ { - 1 / 5 } } \\ { \ = \ 0 . 0 4 0 9 6 ^ { - \frac { 1 } { 5 } } } \\ { \ = \ 0 . 5 2 7 ^ { - 1 } = 1 . 8 9 } \end{array}
$$

# 3.4 Sampling sentences from a language model

sampling

One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points according to their likelihood. Thus sampling from a language model—which represents a distribution over sentences—means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability.

This technique of visualizing a language model by sampling was first suggested very early on by Shannon (1948) and Miller and Selfridge (1950). It’s simplest to visualize how this works for the unigram case. Imagine all the words of the English language covering the number line between 0 and 1, each word covering an interval proportional to its frequency. Fig. 3.3 shows a visualization, using a unigram LM computed from the text of this book. We choose a random value between 0 and 1, find that point on the probability line, and print the word whose interval includes this chosen value. We continue choosing random numbers and generating words until we randomly generate the sentence-final token $< / { \mathsf { s } } { \mathsf { > } }$ .

![## Image Analysis: a42b8d64865a480fa554e276ef3dc3249a2aff101d4ec6bf2b2db7d522d1c1a8.jpg

**Conceptual Understanding:**
This image conceptually represents a cumulative probability distribution of words in a language. Its main purpose is to visualize how individual word probabilities are mapped to segments of a continuous interval (0 to 1) to enable random sampling. The key idea conveyed is that words with higher probabilities (e.g., "the", "of") are assigned larger segments on this interval, making them more likely to be selected when a random number is drawn, while rare words (e.g., "polyphonic") occupy tiny segments, making their selection much less probable. This directly illustrates the weighted random selection process for unigrams in a language model context.

**Content Interpretation:**
The image displays a sampling distribution for unigrams (single words) based on their probabilities or relative frequencies. It shows common words like "the" (0.06), "of" (0.03), "a" (0.02), "to" (0.02), and "in" (0.02) occupying initial segments of a cumulative probability number line from "0" to "1". The cumulative probabilities are marked at ".06", ".09", ".11", ".13", ".15". Ellipses (". . .") indicate the presence of many other words. Less frequent words, such as "however" (p=0.0003) and "polyphonic" (p=0.000018), are shown to occupy progressively smaller segments further along the number line, with their approximate positions marked by dashed arrows pointing to locations like around ".66" and near ".99" towards "1". The blue bars visually represent the individual probabilities, and their corresponding cumulative segments on the number line illustrate how a random number can be used to select a word, with larger segments corresponding to higher probabilities. The disparity in segment sizes (e.g., "0.06" for "the" versus "p=0.000018" for "polyphonic") highlights the differing likelihoods of words being sampled.

**Key Insights:**
1.  **Probability Distribution for Sampling:** The image clearly illustrates how a probability distribution is used to select items (words) in a weighted random fashion, where each word is assigned an interval on a 0-1 scale proportional to its probability. This is evidenced by the "0" to "1" number line with intermediate cumulative values like ".06", ".09", ".11", ".13", ".15", ".66", ".99".
2.  **Frequency Dictates Selection Likelihood:** Words with higher frequencies or probabilities occupy larger segments of the cumulative probability line, making them more probable candidates for selection. For example, "the" with a segment of "0.06" has a significantly larger portion than "polyphonic" with "p=0.000018", directly showing that "the" is far more likely to be sampled.
3.  **Visualizing Rarity:** The visualization effectively highlights the extreme rarity of certain words. "polyphonic" with "p=0.000018" is shown to occupy a minuscule segment near "1" on the number line, emphasizing its infrequent selection through this sampling method. The explicit probabilities for "however (p=0.0003)" and "polyphonic p=0.000018" provide concrete textual evidence of their low likelihoods.
4.  **Foundation of Language Model Sampling:** This diagram serves as a fundamental illustration of how words might be sampled from a language model based on their learned probabilities, forming the building blocks for generating sentences. The sequential arrangement of words like "the", "of", "a", "to", "in" and their associated probabilities within the blue bars, alongside their cumulative probabilities on the number line, visually demonstrate this core principle.

**Document Context:**
This image is crucial for understanding Section 3.4, titled "Sampling sentences from a language model," as it visually explains the fundamental mechanism for selecting individual words (unigrams) based on their probabilities. The visualization directly supports the concept of "repeatedly sampling unigrams" by showing how each word is assigned a proportional interval on a 0-1 probability line. The text's explanation about the "blue bar represents the relative frequency of each word" and how a "random number between 0 and 1" falling into an "interval corresponding to some word" selects that word, perfectly aligns with the diagram. It explicitly demonstrates why frequent words (like "the", "of", "a") have a much higher chance of being sampled than rare words (like "polyphonic"), which is a core principle in language model sentence generation.

**Summary:**
This image illustrates a method for sampling words, like those a language model might generate, by mapping their probabilities to segments on a number line. At the top, several common words—"the", "of", "a", "to", "in"—are shown, each with a blue bar underneath indicating its relative frequency or probability: "the" has "0.06", "of" has "0.03", and "a", "to", "in" each have "0.02". These are examples of the most frequent words, and their probabilities sum up to the first part of the distribution. Below this, a number line spans from "0" on the left to "1" on the right, representing the total cumulative probability. Tick marks along this line indicate the cumulative probabilities: ".06", ".09", ".11", ".13", ".15". For instance, "the" occupies the segment from 0 to ".06". "of" occupies the segment from ".06" to ".09", and so on. Further to the right, indicated by ellipses "...", the diagram shows examples of less frequent words. "however" is presented with a probability "(p=0.0003)", and a dashed arrow points from it to a tiny segment on the number line around ".66". Even further right, another ellipsis "..." leads to "polyphonic" with an even smaller probability of "p=0.000018". A dashed arrow points from "polyphonic" to an extremely small segment on the number line, very close to "1" (specifically after ".99"). The core idea is that to sample a word, one would pick a random number between 0 and 1. Whichever segment this random number falls into determines the word that is selected. Words with higher probabilities, like "the" (0.06), are allocated much larger segments on this number line, making it much more likely for a random number to fall within their range. Conversely, words with very low probabilities, such as "polyphonic" (p=0.000018), are assigned extremely small segments, meaning they are very rarely selected. The visualization effectively demonstrates that the size of a word's interval on the cumulative probability line directly corresponds to its likelihood of being chosen.](images/a42b8d64865a480fa554e276ef3dc3249a2aff101d4ec6bf2b2db7d522d1c1a8.jpg)
Figure 3.3 A visualization of the sampling distribution for sampling sentences by repeatedly sampling unigrams. The blue bar represents the relative frequency of each word (we’ve ordered them from most frequent to least frequent, but the choice of order is arbitrary). The number line shows the cumulative probabilities. If we choose a random number between 0 and 1, it will fall in an interval corresponding to some word. The expectation for the random number to fall in the larger intervals of one of the frequent words (the, of, a) is much higher than in the smaller interval of one of the rare words (polyphonic).

We can use the same technique to generate bigrams by first generating a random bigram that starts with ${ \tt < s > }$ (according to its bigram probability). Let’s say the second word of that bigram is $w$ . We next choose a random bigram starting with $w$ (again, drawn according to its bigram probability), and so on.

# 3.5 Generalizing vs. overfitting the training set

The n-gram model, like many statistical models, is dependent on the training corpus. One implication of this is that the probabilities often encode specific facts about a given training corpus. Another implication is that n-grams do a better and better job of modeling the training corpus as we increase the value of $N$ .

We can use the sampling method from the prior section to visualize both of these facts! To give an intuition for the increasing power of higher-order n-grams, Fig. 3.4 shows random sentences generated from unigram, bigram, trigram, and 4- gram models trained on Shakespeare’s works.

The longer the context, the more coherent the sentences. The unigram sentences show no coherent relation between words nor any sentence-final punctuation. The bigram sentences have some local word-to-word coherence (especially considering punctuation as words). The trigram sentences are beginning to look a lot like Shakespeare. Indeed, the 4-gram sentences look a little too much like Shakespeare. The words It cannot be but so are directly from King John. This is because, not to put the knock on Shakespeare, his oeuvre is not very large as corpora go $( N = 8 8 4 , 6 4 7 , V = 2 9 , 0 6 6 )$ , and our n-gram probability matrices are ridiculously sparse. There are $V ^ { 2 } = 8 4 4 , 0 0 0 , 0 0 0$ possible bigrams alone, and the number of possible 4-grams is $V ^ { 4 } = 7 \times 1 0 ^ { 1 7 }$ . Thus, once the generator has chosen the first 3-gram ${ \mathbf { } } I t$ cannot be), there are only seven possible next words for the 4th element (but, I, that, thus, this, and the period).

To get an idea of the dependence on the training set, let’s look at LMs trained on a completely different corpus: the Wall Street Journal (WSJ) newspaper. Shakespeare and the WSJ are both English, so we might have expected some overlap between our n-grams for the two genres. Fig. 3.5 shows sentences generated by unigram, bigram, and trigram grammars trained on 40 million words from WSJ.

Figure 3.4 Eight sentences randomly generated from four n-grams computed from Shakespeare’s works. All characters were mapped to lower-case and punctuation marks were treated as words. Output is hand-corrected for capitalization to improve readability.   

<table><tr><td>1 gram</td><td>-To him swallowed confess hear both. Which. Of save on trail for are ay device and rote life have -Hill he late speaks; or! a more to leg less first you enter -Why dost stand forth thycanopy,forsooth; he is this palpable hit the King Henry. Live</td></tr><tr><td>2 gram</td><td>king. Follow. -What means, sir. I confess she? then allsorts, he is trim, captain.</td></tr><tr><td>3</td><td>-Fly, and willrid me these news of price. Therefore the sadnessof parting, as they say, &#x27;tis done.</td></tr><tr><td>gram</td><td>-This shall forbid it should be branded, if renown made it empty.</td></tr><tr><td>4</td><td>-King Henry. What! I will go seek the traitor Gloucester. Exeunt some of the watch. A</td></tr><tr><td>gram</td><td>great banquet serv&#x27;d in; -It cannot be but so.</td></tr></table>

<table><tr><td>1 gram 2</td><td>Months the m andansue of yearforegnqure toshxe&#x27;sutytembr Last December through the way to preserve the Hudson corporation N. B. E. C. Taylor would seem to complete the major central planners one</td></tr><tr><td>3 gram</td><td>point five percent of U. S. E. has already old M. X. corporation of living on information such as more frequently fishing to keep her They also point to ninety nine point six billion dollars from two hundred four oh six three percent of the rates of interest stores as Mexico and gram Brazil on market conditions</td></tr></table>

Figure 3.5 Three sentences randomly generated from three n-gram models computed from 40 million words of the Wall Street Journal, lower-casing all characters and treating punctuation as words. Output was then hand-corrected for capitalization to improve readability.

Compare these examples to the pseudo-Shakespeare in Fig. 3.4. While they both model “English-like sentences”, there is no overlap in the generated sentences, and little overlap even in small phrases. Statistical models are pretty useless as predictors if the training sets and the test sets are as different as Shakespeare and the WSJ.

How should we deal with this problem when we build n-gram models? One step is to be sure to use a training corpus that has a similar genre to whatever task we are trying to accomplish. To build a language model for translating legal documents, we need a training corpus of legal documents. To build a language model for a question-answering system, we need a training corpus of questions.

It is equally important to get training data in the appropriate dialect or variety, especially when processing social media posts or spoken transcripts. For example some tweets will use features of African American English (AAE)— the name for the many variations of language used in African American communities (King, 2020). Such features can include words like finna—an auxiliary verb that marks immediate future tense —that don’t occur in other varieties, or spellings like den for then, in tweets like this one (Blodgett and O’Connor, 2017):

(3.22) Bored af den my phone finna die!!!

while tweets from English-based languages like Nigerian Pidgin have markedly different vocabulary and n-gram patterns from American English (Jurgens et al., 2017):

(3.23) $@$ username R u a wizard or wat gan sef: in d mornin - u tweet, afternoon - u tweet, nyt gan u dey tweet. beta get ur IT placement wiv twitter

Is it possible for the testset nonetheless to have a word we have never seen before? What happens if the word Jurafsky never occurs in our training set, but pops up in the test set? The answer is that although words might be unseen, we normally run our NLP algorithms not on words but on subword tokens. With subword tokenization (like the BPE algorithm of Chapter 2) any word can be modeled as a sequence of known smaller subwords, if necessary by a sequence of tokens corresponding to individual letters. So although for convenience we’ve been referring to words in this chapter, the language model vocabulary is normally the set of tokens rather than words, and in this way the test set can never contain unseen tokens.

# 3.6 Smoothing, Interpolation, and Backoff

# zeros

There is a problem with using maximum likelihood estimates for probabilities: any finite training corpus will be missing some perfectly acceptable English word sequences. That is, cases where a particular n-gram never occurs in the training data but appears in the test set. Perhaps our training corpus has the words ruby and slippers in it but just happens not to have the phrase ruby slippers.

These unseen sequences or zeros—sequences that don’t occur in the training set but do occur in the test set—are a problem for two reasons. First, their presence means we are underestimating the probability of word sequences that might occur, which hurts the performance of any application we want to run on this data. Second, if the probability of any word in the test set is 0, the probability of the whole test set is 0. Perplexity is defined based on the inverse probability of the test set. Thus if some words in context have zero probability, we can’t compute perplexity at all, since we can’t divide by 0!

smoothing discounting

The standard way to deal with putative “zero probability n-grams” that should really have some non-zero probability is called smoothing or discounting. Smoothing algorithms shave off a bit of probability mass from some more frequent events and give it to unseen events. Here we’ll introduce some simple smoothing algorithms: Laplace (add-one) smoothing, stupid backoff, and n-gram interpolation.

# Laplace smoothing

# 3.6.1 Laplace Smoothing

The simplest way to do smoothing is to add one to all the n-gram counts, before we normalize them into probabilities. All the counts that used to be zero will now have a count of 1, the counts of 1 will be 2, and so on. This algorithm is called Laplace smoothing. Laplace smoothing does not perform well enough to be used in modern n-gram models, but it usefully introduces many of the concepts that we see in other smoothing algorithms, gives a useful baseline, and is also a practical smoothing algorithm for other tasks like text classification (Chapter 4).

Let’s start with the application of Laplace smoothing to unigram probabilities. Recall that the unsmoothed maximum likelihood estimate of the unigram probability of the word $w _ { i }$ is its count $c _ { i }$ normalized by the total number of word tokens $N$ :

$$
P ( w _ { i } ) = \frac { c _ { i } } { N }
$$

Laplace smoothing merely adds one to each count (hence its alternate name addone smoothing). Since there are $V$ words in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra $V$ observations. (What happens to our $P$ values if we don’t increase the denominator?)

$$
P _ { \mathrm { L a p l a c e } } ( w _ { i } ) = \frac { c _ { i } + 1 } { N + V }
$$

Now that we have the intuition for the unigram case, let’s smooth our Berkeley Restaurant Project bigrams. Figure 3.6 shows the add-one smoothed counts for the bigrams in Fig. 3.1.

<table><tr><td></td><td>i</td><td>want</td><td>to</td><td>eat</td><td>chinese</td><td>food</td><td>lunch</td><td>spend</td></tr><tr><td>i</td><td>6</td><td>828</td><td>1</td><td>10</td><td>1</td><td>1</td><td>1</td><td>3</td></tr><tr><td> want</td><td>3</td><td>1</td><td>609</td><td>2</td><td>7</td><td>7</td><td>6</td><td>2</td></tr><tr><td>to</td><td>3</td><td>1</td><td>5</td><td>687</td><td>3</td><td>1</td><td>7</td><td>212</td></tr><tr><td>eat</td><td>1</td><td>1</td><td>3</td><td>1</td><td>17</td><td>3</td><td>43</td><td>1</td></tr><tr><td>chinese</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>83</td><td>2</td><td>1</td></tr><tr><td>food</td><td>16</td><td>1</td><td>16</td><td>1</td><td>2</td><td>5</td><td>1</td><td>1</td></tr><tr><td> lunch</td><td>3</td><td>1</td><td>1</td><td>1</td><td>1</td><td>2</td><td>1</td><td>1</td></tr><tr><td> spend</td><td>2</td><td>1</td><td>2</td><td>1</td><td>1</td><td>1</td><td>1</td><td>1</td></tr></table>

Figure 3.6 Add-one smoothed bigram counts for eight of the words (out of $V = 1 4 4 6$ ) in the Berkeley Restaurant Project corpus of 9332 sentences. Previously-zero counts are in gray.

Figure 3.7 shows the add-one smoothed probabilities for the bigrams in Fig. 3.2, computed by Eq. 3.26 below. Recall that normal bigram probabilities are computed by normalizing each row of counts by the unigram count:

$$
P _ { \mathrm { M L E } } ( w _ { n } | w _ { n - 1 } ) = { \frac { C ( w _ { n - 1 } w _ { n } ) } { C ( w _ { n - 1 } ) } }
$$

For add-one smoothed bigram counts, we need to augment the unigram count in the denominator by the number of total word types in the vocabulary $V$ . We can see why this is in the following equation, which makes it explicit that the unigram count in the denominator is really the sum over all the bigrams that start with $w _ { n - 1 }$ . Since we add one to each of these, and there are $V$ of them, we add a total of $V$ to the denominator:

$$
P _ { \mathrm { L a p l a c e } } ( w _ { n } | w _ { n - 1 } ) = { \frac { C ( w _ { n - 1 } w _ { n } ) + 1 } { \sum _ { w } { ( C ( w _ { n - 1 } w ) + 1 ) } } } = { \frac { C ( w _ { n - 1 } w _ { n } ) + 1 } { C ( w _ { n - 1 } ) + V } }
$$

Thus, each of the unigram counts given on page 36 will need to be augmented by $V =$ 1446. The result, using Eq. 3.26, is the smoothed bigram probabilities in Fig. 3.7.

One useful visualization technique is to reconstruct an adjusted count matrix so we can see how much a smoothing algorithm has changed the original counts. This adjusted count $C ^ { * }$ is the count that, if divided by $C ( w _ { n - 1 } )$ , would result in the smoothed probability. This adjusted count is easier to compare directly with the MLE counts. That is, the Laplace probability can equally be expressed as the adjusted count divided by the (non-smoothed) denominator from Eq. 3.25:

$$
P _ { \mathrm { L a p l a c e } } ( w _ { n } | w _ { n - 1 } ) = { \frac { C ( w _ { n - 1 } w _ { n } ) + 1 } { C ( w _ { n - 1 } ) + V } } = { \frac { C ^ { * } ( w _ { n - 1 } w _ { n } ) } { C ( w _ { n - 1 } ) } }
$$

<table><tr><td></td><td>i</td><td>want</td><td>to</td><td>eat</td><td>chinese</td><td>food</td><td>lunch</td><td> spend</td></tr><tr><td>i</td><td>0.0015</td><td>0.21</td><td>0.00025</td><td>0.0025</td><td>0.00025</td><td>0.00025</td><td>0.00025</td><td>0.00075</td></tr><tr><td> want</td><td>0.0013</td><td>0.00042</td><td>0.26</td><td>0.00084</td><td>0.0029</td><td>0.0029</td><td>0.0025</td><td>0.00084</td></tr><tr><td>to</td><td>0.00078</td><td>0.00026</td><td>0.0013</td><td>0.18</td><td>0.00078</td><td>0.00026</td><td>0.0018</td><td>0.055</td></tr><tr><td>eat</td><td>0.00046</td><td>0.00046</td><td>0.0014</td><td>0.00046</td><td>0.0078</td><td>0.0014</td><td>0.02</td><td>0.00046</td></tr><tr><td>chinese</td><td>0.0012</td><td>0.00062</td><td>0.00062</td><td>0.00062</td><td>0.00062</td><td>0.052</td><td>0.0012</td><td>0.00062</td></tr><tr><td>food</td><td>0.0063</td><td>0.00039</td><td>0.0063</td><td>0.00039</td><td>0.00079</td><td>0.002</td><td>0.00039</td><td>0.00039</td></tr><tr><td> lunch</td><td>0.0017</td><td>0.00056</td><td>0.00056</td><td>0.00056</td><td>0.00056</td><td>0.0011</td><td>0.00056</td><td>0.00056</td></tr><tr><td> spend</td><td>0.0012</td><td>0.00058</td><td>0.0012</td><td>0.00058</td><td>0.00058</td><td>0.00058</td><td>0.00058</td><td>0.00058</td></tr></table>

Figure 3.7 Add-one smoothed bigram probabilities for eight of the words (out of $V = 1 4 4 6$ ) in the BeRP corpus of 9332 sentences computed by Eq. 3.26. Previously-zero probabilities are in gray.

Rearranging terms, we can solve for $C ^ { * } { \left( w _ { n - 1 } w _ { n } \right) }$ :

$$
C ^ { * } ( w _ { n - 1 } w _ { n } ) = \frac { \left[ C ( w _ { n - 1 } w _ { n } ) + 1 \right] \times C ( w _ { n - 1 } ) } { C ( w _ { n - 1 } ) + V }
$$

Figure 3.8 shows the reconstructed counts, computed by Eq. 3.27.   

<table><tr><td></td><td>i</td><td>want</td><td>to</td><td>eat</td><td>chinese</td><td>food</td><td>lunch</td><td>spend</td></tr><tr><td>i</td><td>3.8</td><td>527</td><td>0.64</td><td>6.4</td><td>0.64</td><td>0.64</td><td>0.64</td><td>1.9</td></tr><tr><td> want</td><td>1.2</td><td>0.39</td><td>238</td><td>0.78</td><td>2.7</td><td>2.7</td><td>2.3</td><td>0.78</td></tr><tr><td>to</td><td>1.9</td><td>0.63</td><td>3.1</td><td>430</td><td>1.9</td><td>0.63</td><td>4.4</td><td>133</td></tr><tr><td>eat</td><td>0.34</td><td>0.34</td><td>1</td><td>0.34</td><td>5.8</td><td>1</td><td>15</td><td>0.34</td></tr><tr><td>chinese</td><td>0.2</td><td>0.098</td><td>0.098</td><td>0.098</td><td>0.098</td><td>8.2</td><td>0.2</td><td>0.098</td></tr><tr><td>food</td><td>6.9</td><td>0.43</td><td>6.9</td><td>0.43</td><td>0.86</td><td>2.2</td><td>0.43</td><td>0.43</td></tr><tr><td> lunch</td><td>0.57</td><td>0.19</td><td>0.19</td><td>0.19</td><td>0.19</td><td>0.38</td><td>0.19</td><td>0.19</td></tr><tr><td> spend</td><td>0.32</td><td>0.16</td><td>0.32</td><td>0.16</td><td>0.16</td><td>0.16</td><td>0.16</td><td>0.16</td></tr></table>

Figure 3.8 Add-one reconstituted counts for eight words (of $V = 1 4 4 6$ ) in the BeRP corpus of 9332 sentences, computed by Eq. 3.27. Previously-zero counts are in gray.

Note that add-one smoothing has made a very big change to the counts. Comparing Fig. 3.8 to the original counts in Fig. 3.1, we can see that $C ( w a n t t o )$ changed from 608 to 238! We can see this in probability space as well: $P ( t o | w a n t )$ decreases from 0.66 in the unsmoothed case to 0.26 in the smoothed case. Looking at the discount $d$ , defined as the ratio between new and old counts, shows us how strikingly the counts for each prefix word have been reduced; the discount for the bigram want $t o$ is 0.39, while the discount for Chinese food is 0.10, a factor of 10! The sharp change occurs because too much probability mass is moved to all the zeros.

# 3.6.2 Add-k smoothing

One alternative to add-one smoothing is to move a bit less of the probability mass from the seen to the unseen events. Instead of adding 1 to each count, we add a fractional count $k \left( 0 . 5 ? 0 . 0 1 ? \right)$ . This algorithm is therefore called add- $\mathbf { k }$ smoothing.

$$
P _ { \mathrm { A d d - k } } ^ { * } ( w _ { n } | w _ { n - 1 } ) = \frac { C ( w _ { n - 1 } w _ { n } ) + k } { C ( w _ { n - 1 } ) + k V }
$$

Add- $\mathbf { \nabla } \cdot \mathbf { k }$ smoothing requires that we have a method for choosing $k$ ; this can be done, for example, by optimizing on a devset. Although add- $\mathbf { \nabla } \cdot \mathbf { k }$ is useful for some tasks (including text classification), it turns out that it still doesn’t work well for

language modeling, generating counts with poor variances and often inappropriate discounts (Gale and Church, 1994).

# 3.6.3 Language Model Interpolation

There is an alternative source of knowledge we can draw on to solve the problem of zero frequency n-grams. If we are trying to compute $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ but we have no examples of a particular trigram $w _ { n - 2 } w _ { n - 1 } w _ { n }$ , we can instead estimate its probability by using the bigram probability $P ( w _ { n } | w _ { n - 1 } )$ . Similarly, if we don’t have counts to compute $P ( w _ { n } | w _ { n - 1 } )$ , we can look to the unigram $P ( w _ { n } )$ . In other words, sometimes using less context can help us generalize more for contexts that the model hasn’t learned much about.

The most common way to use this $\mathbf { n }$ -gram hierarchy is called interpolation: computing a new probability by interpolating (weighting and combining) the trigram, bigram, and unigram probabilities.3 In simple linear interpolation, we combine different order n-grams by linearly interpolating them. Thus, we estimate the trigram probability $P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } )$ by mixing together the unigram, bigram, and trigram probabilities, each weighted by a $\lambda$ :

$$
\begin{array} { r c l } { { \hat { P } ( w _ { n } | w _ { n - 2 } w _ { n - 1 } ) } } & { { = } } & { { \lambda _ { 1 } P ( w _ { n } ) } } \\ { { } } & { { } } & { { + \lambda _ { 2 } P ( w _ { n } | w _ { n - 1 } ) } } \\ { { } } & { { } } & { { + \lambda _ { 3 } P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } ) } } \end{array}
$$

The $\lambda \mathrm { s }$ must sum to 1, making Eq. 3.29 equivalent to a weighted average. In a slightly more sophisticated version of linear interpolation, each $\lambda$ weight is computed by conditioning on the context. This way, if we have particularly accurate counts for a particular bigram, we assume that the counts of the trigrams based on this bigram will be more trustworthy, so we can make the $\lambda \mathrm { s }$ for those trigrams higher and thus give that trigram more weight in the interpolation. Equation 3.30 shows the equation for interpolation with context-conditioned weights, where each lambda takes an argument that is the two prior word context:

$$
\begin{array} { r c l } { { \hat { P } ( w _ { n } | w _ { n - 2 } w _ { n - 1 } ) ~ = } } & { { \lambda _ { 1 } ( w _ { n - 2 : n - 1 } ) P ( w _ { n } ) } } & { { } } \\ { { } } & { { } } & { { + \lambda _ { 2 } ( w _ { n - 2 : n - 1 } ) P ( w _ { n } | w _ { n - 1 } ) } } \\ { { } } & { { } } & { { + \lambda _ { 3 } ( w _ { n - 2 : n - 1 } ) P ( w _ { n } | w _ { n - 2 } w _ { n - 1 } ) } } \end{array}
$$

How are these $\lambda$ values set? Both the simple interpolation and conditional interpolation $\lambda \mathrm { s }$ are learned from a held-out corpus. A held-out corpus is an additional training corpus, so-called because we hold it out from the training data, that we use to set these $\lambda$ values.4 We do so by choosing the $\lambda$ values that maximize the likelihood of the held-out corpus. That is, we fix the $\mathbf { n }$ -gram probabilities and then search for the $\lambda$ values that—when plugged into Eq. 3.29—give us the highest probability of the held-out set. There are various ways to find this optimal set of $\lambda \mathrm { s }$ . One way is to use the EM algorithm, an iterative learning algorithm that converges on locally optimal $\lambda \mathrm { s }$ (Jelinek and Mercer, 1980).

# 3.6.4 Stupid Backoff

# backoff

discount

An alternative to interpolation is backoff. In a backoff model, if the n-gram we need has zero counts, we approximate it by backing off to the (n-1)-gram. We continue backing off until we reach a history that has some counts. For a backoff model to give a correct probability distribution, we have to discount the higher-order n-grams to save some probability mass for the lower order n-grams. In practice, instead of discounting, it’s common to use a much simpler non-discounted backoff algorithm called stupid backoff (Brants et al., 2007).

# stupid backoff

Stupid backoff gives up the idea of trying to make the language model a true probability distribution. There is no discounting of the higher-order probabilities. If a higher-order n-gram has a zero count, we simply backoff to a lower order n-gram, weighed by a fixed (context-independent) weight. This algorithm does not produce a probability distribution, so we’ll follow Brants et al. (2007) in referring to it as $s$ :

$$
S ( w _ { i } | w _ { i - N + 1 : i - 1 } ) = \left\{ \begin{array} { l } { { \displaystyle { \frac { \mathrm { c o u n t } ( w _ { i - N + 1 : i } ) } { \mathrm { c o u n t } ( w _ { i - N + 1 : i - 1 } ) } } \quad \mathrm { ~ i f ~ c o u n t } ( w _ { i - N + 1 : i } ) > 0 } } \\ { { \lambda S ( w _ { i } | w _ { i - N + 2 : i - 1 } ) \quad \mathrm { ~ o t h e r w i s e } } } \end{array} \right.
$$

The backoff terminates in the unigram, which has score S(w) = count(w)N . Brants et al.   
(2007) find that a value of 0.4 worked well for $\lambda$ .

# 3.7 Advanced: Perplexity’s Relation to Entropy

# Entropy

We introduced perplexity in Section 3.3 as a way to evaluate n-gram models on a test set. A better n-gram model is one that assigns a higher probability to the test data, and perplexity is a normalized version of the probability of the test set. The perplexity measure actually arises from the information-theoretic concept of cross-entropy, which explains otherwise mysterious properties of perplexity (why the inverse probability, for example?) and its relationship to entropy. Entropy is a measure of information. Given a random variable $X$ ranging over whatever we are predicting (words, letters, parts of speech), the set of which we’ll call $\chi$ , and with a particular probability function, call it $p ( x )$ , the entropy of the random variable $X$ is:

$$
H ( X ) = - \sum _ { x \in \chi } p ( x ) \log _ { 2 } p ( x )
$$

The log can, in principle, be computed in any base. If we use log base 2, the resulting value of entropy will be measured in bits.

One intuitive way to think about entropy is as a lower bound on the number of bits it would take to encode a certain decision or piece of information in the optimal coding scheme. Consider an example from the standard information theory textbook Cover and Thomas (1991). Imagine that we want to place a bet on a horse race but it is too far to go all the way to Yonkers Racetrack, so we’d like to send a short message to the bookie to tell him which of the eight horses to bet on. One way to encode this message is just to use the binary representation of the horse’s number as the code; thus, horse 1 would be 001, horse 2 010, horse $3 \ \mathfrak { \sigma } 1 1$ , and so on, with horse 8 coded as 000. If we spend the whole day betting and each horse is coded with 3 bits, on average we would be sending 3 bits per race.

Can we do better? Suppose that the spread is the actual distribution of the bets placed and that we represent it as the prior probability of each horse as follows:

<table><tr><td>Horse 1 1-21- Horse 2 1-41-816 Horse 3 Horse 4</td><td>Horse 5 Horse 6 Horse 7 Horse 8</td><td>1-41-41-41-4</td></tr></table>

The entropy of the random variable $X$ that ranges over horses gives us a lower bound on the number of bits and is

$$
{ \begin{array} { r c l } { H ( X ) } & { = } & { \displaystyle - \sum _ { i = 1 } ^ { i = 8 } p ( i ) \log _ { 2 } p ( i ) } \\ & { = } & { \displaystyle - { \frac { 1 } { 2 } } \log _ { 2 } { \frac { 1 } { 2 } } - { \frac { 1 } { 4 } } \log _ { 2 } { \frac { 1 } { 4 } } - { \frac { 1 } { 8 } } \log _ { 2 } { \frac { 1 } { 8 } } - { \frac { 1 } { 1 6 } } \log _ { 2 } { \frac { 1 } { 1 6 } } - 4 ( { \frac { 1 } { 6 4 } } \log _ { 2 } { \frac { 1 } { 6 4 } } ) } \\ & { = } & { 2 \operatorname { b i t s } } \end{array} }
$$

A code that averages 2 bits per race can be built with short encodings for more probable horses, and longer encodings for less probable horses. For example, we could encode the most likely horse with the code 0, and the remaining horses as 10, then 110, 1110, 111100, 111101, 111110, and 111111.

What if the horses are equally likely? We saw above that if we used an equallength binary code for the horse numbers, each horse took 3 bits to code, so the average was 3. Is the entropy the same? In this case each horse would have a probability of $\frac { 1 } { 8 }$ . The entropy of the choice of horses is then

$$
H ( X ) = - \sum _ { i = 1 } ^ { i = 8 } { \frac { 1 } { 8 } } \log _ { 2 } { \frac { 1 } { 8 } } = - \log _ { 2 } { \frac { 1 } { 8 } } = 3 { \mathrm { ~ b i t s } }
$$

Until now we have been computing the entropy of a single variable. But most of what we will use entropy for involves sequences. For a grammar, for example, we will be computing the entropy of some sequence of words $W = \{ w _ { 1 } , w _ { 2 } , . . . , w _ { n } \}$ . One way to do this is to have a variable that ranges over sequences of words. For example we can compute the entropy of a random variable that ranges over all sequences of words of length $n$ in some language $L$ as follows:

$$
H ( w _ { 1 } , w _ { 2 } , . . . , w _ { n } ) = - \sum _ { w _ { 1 : n } \in L } p ( w _ { 1 : n } ) \log p ( w _ { 1 : n } )
$$

entropy rate

We could define the entropy rate (we could also think of this as the per-word entropy) as the entropy of this sequence divided by the number of words:

$$
\frac { 1 } { n } H ( w _ { 1 : n } ) = - \frac { 1 } { n } \sum _ { w _ { 1 : n } \in L } p ( w _ { 1 : n } ) \log p ( w _ { 1 : n } )
$$

But to measure the true entropy of a language, we need to consider sequences of infinite length. If we think of a language as a stochastic process $L$ that produces a sequence of words, and allow $W$ to represent the sequence of words $w _ { 1 } , \ldots , w _ { n }$ , then $L$ ’s entropy rate $H ( L )$ is defined as

$$
\begin{array} { l } { { \displaystyle H ( L ) ~ = ~ \operatorname* { l i m } _ { n  \infty } \frac 1 n H ( w _ { 1 : n } ) } } \\ { { \displaystyle ~ = ~ - \operatorname* { l i m } _ { n  \infty } \frac 1 n \sum _ { W \in L } p ( w _ { 1 : n } ) \log p ( w _ { 1 : n } ) } } \end{array}
$$

The Shannon-McMillan-Breiman theorem (Algoet and Cover 1988, Cover and Thomas 1991) states that if the language is regular in certain ways (to be exact, if it is both stationary and ergodic),

$$
H ( L ) = \operatorname* { l i m } _ { n \to \infty } - { \frac { 1 } { n } } \log p ( w _ { 1 : n } )
$$

That is, we can take a single sequence that is long enough instead of summing over all possible sequences. The intuition of the Shannon-McMillan-Breiman theorem is that a long-enough sequence of words will contain in it many other shorter sequences and that each of these shorter sequences will reoccur in the longer sequence according to their probabilities.

A stochastic process is said to be stationary if the probabilities it assigns to a sequence are invariant with respect to shifts in the time index. In other words, the probability distribution for words at time $t$ is the same as the probability distribution at time $t + 1$ . Markov models, and hence n-grams, are stationary. For example, in a bigram, $P _ { i }$ is dependent only on $P _ { i - 1 }$ . So if we shift our time index by $x ,$ , $P _ { i + x }$ is still dependent on $P _ { i + x - 1 }$ . But natural language is not stationary, since as we show in Appendix D, the probability of upcoming words can be dependent on events that were arbitrarily distant and time dependent. Thus, our statistical models only give an approximation to the correct distributions and entropies of natural language.

To summarize, by making some incorrect but convenient simplifying assumptions, we can compute the entropy of some stochastic process by taking a very long sample of the output and computing its average log probability.

Now we are ready to introduce cross-entropy. The cross-entropy is useful when we don’t know the actual probability distribution $p$ that generated some data. It allows us to use some $m$ , which is a model of $p$ (i.e., an approximation to $p$ ). The cross-entropy of $m$ on $p$ is defined by

$$
H ( p , m ) = \operatorname* { l i m } _ { n \to \infty } - { \frac { 1 } { n } } \sum _ { W \in L } p ( w _ { 1 } , \ldots , w _ { n } ) \log m ( w _ { 1 } , \ldots , w _ { n } )
$$

That is, we draw sequences according to the probability distribution $p$ , but sum the log of their probabilities according to $m$ .

Again, following the Shannon-McMillan-Breiman theorem, for a stationary ergodic process:

$$
H ( p , m ) = \operatorname* { l i m } _ { n \to \infty } - { \frac { 1 } { n } } \log m ( w _ { 1 } w _ { 2 } \ldots w _ { n } )
$$

This means that, as for entropy, we can estimate the cross-entropy of a model $m$ on some distribution $p$ by taking a single sequence that is long enough instead of summing over all possible sequences.

What makes the cross-entropy useful is that the cross-entropy $H ( p , m )$ is an upper bound on the entropy $H ( p )$ . For any model $m$ :

$$
H ( p ) \leq H ( p , m )
$$

This means that we can use some simplified model $m$ to help estimate the true entropy of a sequence of symbols drawn according to probability $p$ . The more accurate $m$ is, the closer the cross-entropy $H ( p , m )$ will be to the true entropy $H ( p )$ . Thus, the difference between $H ( p , m )$ and $H ( p )$ is a measure of how accurate a model is. Between two models $m _ { 1 }$ and $m _ { 2 }$ , the more accurate model will be the one with the lower cross-entropy. (The cross-entropy can never be lower than the true entropy, so a model cannot err by underestimating the true entropy.)

We are finally ready to see the relation between perplexity and cross-entropy as we saw it in Eq. 3.40. Cross-entropy is defined in the limit as the length of the observed word sequence goes to infinity. We approximate this cross-entropy by relying on a (sufficiently long) sequence of fixed length. This approximation to the cross-entropy of a model $M = P ( w _ { i } | w _ { i - N + 1 : i - 1 } )$ on a sequence of words $W$ is

$$
H ( W ) = - { \frac { 1 } { N } } \log P ( w _ { 1 } w _ { 2 } \ldots w _ { N } )
$$

# perplexity

The perplexity of a model $P$ on a sequence of words $W$ is now formally defined as 2 raised to the power of this cross-entropy:

$$
{ \begin{array} { l } { { \mathrm { P e r p l e x i t y } } ( W ) ~ = ~ 2 ^ { H ( W ) } } \\ { ~ = ~ P ( w _ { 1 } w _ { 2 } \ldots w _ { N } ) ^ { - { \frac { 1 } { N } } } } \\ { ~ = ~ { \sqrt [ { N } ] { \frac { 1 } { P ( w _ { 1 } w _ { 2 } \ldots w _ { N } ) } } } } \end{array} }
$$

# 3.8 Summary

This chapter introduced language modeling via the n-gram model, a classic model that allows us to introduce many of the basic concepts in language modeling.

• Language models offer a way to assign a probability to a sentence or other sequence of words or tokens, and to predict a word or token from preceding words or tokens.   
• N-grams are perhaps the simplest kind of language model. They are Markov models that estimate words from a fixed window of previous words. N-gram models can be trained by counting in a training corpus and normalizing the counts (the maximum likelihood estimate).   
• N-gram language models can be evaluated on a test set using perplexity.   
• The perplexity of a test set according to a language model is a function of the probability of the test set: the inverse test set probability according to the model, normalized by the length.   
• Sampling from a language model means to generate some sentences, choosing each sentence according to its likelihood as defined by the model.   
• Smoothing algorithms provide a way to estimate probabilities for events that were unseen in training. Commonly used smoothing algorithms for n-grams include add-1 smoothing, or rely on lower-order n-gram counts through interpolation.

# Bibliographical and Historical Notes

The underlying mathematics of the $\mathbf { n }$ -gram was first proposed by Markov (1913), who used what are now called Markov chains (bigrams and trigrams) to predict whether an upcoming letter in Pushkin’s Eugene Onegin would be a vowel or a consonant. Markov classified 20,000 letters as $\mathrm { v }$ or C and computed the bigram and trigram probability that a given letter would be a vowel given the previous one or two letters. Shannon (1948) applied n-grams to compute approximations to English word sequences. Based on Shannon’s work, Markov models were commonly used in engineering, linguistic, and psychological work on modeling word sequences by the 1950s. In a series of extremely influential papers starting with Chomsky (1956) and including Chomsky (1957) and Miller and Chomsky (1963), Noam Chomsky argued that “finite-state Markov processes”, while a possibly useful engineering heuristic, were incapable of being a complete cognitive model of human grammatical knowledge. These arguments led many linguists and computational linguists to ignore work in statistical modeling for decades.

The resurgence of n-gram language models came from Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, who were influenced by Shannon, and James Baker at CMU, who was influenced by the prior, classified work of Leonard Baum and colleagues on these topics at labs like the US Institute for Defense Analyses (IDA) after they were declassified. Independently these two labs successfully used n-grams in their speech recognition systems at the same time (Baker 1975b, Jelinek et al. 1975, Baker 1975a, Bahl et al. 1983, Jelinek 1990). The terms “language model” and “perplexity” were first used for this technology by the IBM group. Jelinek and his colleagues used the term language model in a pretty modern way, to mean the entire set of linguistic influences on word sequence probabilities, including grammar, semantics, discourse, and even speaker characteristics, rather than just the particular n-gram model itself.

Add-one smoothing derives from Laplace’s 1812 law of succession and was first applied as an engineering solution to the zero frequency problem by Jeffreys (1948) based on an earlier Add-K suggestion by Johnson (1932). Problems with the addone algorithm are summarized in Gale and Church (1994).

A wide variety of different language modeling and smoothing techniques were proposed in the 80s and 90s, including Good-Turing discounting—first applied to the n-gram smoothing at IBM by Katz (Nadas ´ 1984, Church and Gale 1991)— WittenBell discounting (Witten and Bell, 1991), and varieties of class-based n-gram models that used information about word classes. Starting in the late 1990s, Chen and Goodman performed a number of carefully controlled experiments comparing different algorithms and parameters (Chen and Goodman 1999, Goodman 2006, inter alia). They showed the advantages of Modified Interpolated Kneser-Ney, which became the standard baseline for n-gram language modeling around the turn of the century, especially because they showed that caches and class-based models provided only minor additional improvement. SRILM (Stolcke, 2002) and KenLM (Heafield 2011, Heafield et al. 2013) are publicly available toolkits for building ngram language models.

Large language models are based on neural networks rather than n-grams, enabling them to solve the two major problems with n-grams: (1) the number of parameters increases exponentially as the n-gram order increases, and (2) n-grams have no way to generalize from training examples to test set examples unless they use identical words. Neural language models instead project words into a continuous space in which words with similar contexts have similar representations. We’ll introduce transformer-based large language models in Chapter 9, along the way introducing feedforward language models (Bengio et al. 2006, Schwenk 2007) in Chapter 7 and recurrent language models (Mikolov, 2012) in Chapter 8.

# Exercises

3.1 Write out the equation for trigram probability estimation (modifying Eq. 3.11). Now write out all the non-zero trigram probabilities for the I am Sam corpus on page 35.

3.2 Calculate the probability of the sentence i want chinese food. Give two probabilities, one using Fig. 3.2 and the ‘useful probabilities’ just below it on page 37, and another using the add-1 smoothed table in Fig. 3.7. Assume the additional add-1 smoothed probabilities $P ( \mathbf { i } | < \mathbf { s } > ) = 0 . 1 9$ and $P ( < / { \mathsf { s } } > | \pm { \mathsf { o o d } } ) =$ 0.40.

3.3 Which of the two probabilities you computed in the previous exercise is higher, unsmoothed or smoothed? Explain why.

3.4 We are given the following corpus, modified from the one in the chapter:

$< s > \ I$ am Sam $< / { \mathsf { s } } { \mathsf { > } }$   
${ \tt < s > }$ Sam I am $< / { \mathsf { s } } { \mathsf { > } }$   
$< s > \ I$ am Sam $< / { \mathsf { s } } { \mathsf { > } }$   
$< s > \ I$ do not like green eggs and Sam $< / { \mathsf { s } } { \mathsf { > } }$

Using a bigram language model with add-one smoothing, what is $\mathrm { P } ( \mathrm { S a m } \mid$ am)? Include ${ \tt < s > }$ and $< / \mathsf { s } >$ in your counts just like any other token.

3.5 Suppose we didn’t use the end-symbol $< / { \mathsf { s } } { \mathsf { > } }$ . Train an unsmoothed bigram grammar on the following training corpus without using the end-symbol $< / { \mathsf { s } } { \mathsf { > } }$ :

${ \tt < s > }$ a b ${ \tt < s > }$ b b ${ \tt < s > }$ b a ${ \tt < s > }$ a a

Demonstrate that your bigram model does not assign a single probability distribution across all sentence lengths by showing that the sum of the probability of the four possible 2 word sentences over the alphabet $\{ \mathrm { a } , \mathrm { b } \}$ is 1.0, and the sum of the probability of all possible 3 word sentences over the alphabet $\{ \mathrm { a } , \mathrm { b } \}$ is also 1.0.

3.6 Suppose we train a trigram language model with add-one smoothing on a given corpus. The corpus contains V word types. Express a formula for estimating $\mathrm { P } ( \mathrm { w } 3 | \mathrm { w } 1 , \mathrm { w } 2 )$ , where w3 is a word which follows the bigram (w1,w2), in terms of various n-gram counts and V. Use the notation $\mathbf { c } ( \mathbf { w } 1 , \mathbf { w } 2 , \mathbf { w } 3 )$ to denote the number of times that trigram (w1,w2,w3) occurs in the corpus, and so on for bigrams and unigrams.

3.7 We are given the following corpus, modified from the one in the chapter:

$< s > \ I$ am Sam $< / { \mathsf { s } } { \mathsf { > } }$   
${ \tt < s > }$ Sam I am $< / { \mathsf { s } } { \mathsf { > } }$   
$< s > \ I$ am Sam $< / { \mathsf { s } } { \mathsf { > } }$   
$< s > \ I$ do not like green eggs and Sam $< / { \mathsf { s } } { \mathsf { > } }$

If we use linear interpolation smoothing between a maximum-likelihood bigram model and a maximum-likelihood unigram model with $\begin{array} { r } { \lambda _ { 1 } = \frac { 1 } { 2 } } \end{array}$ and $\lambda _ { 2 } =$ $\frac { 1 } { 2 }$ , what is $\mathrm { P } ( \mathrm { S a m } | \mathrm { a m } ) ^ { \cdot }$ ? Include ${ \tt < s > }$ and $< / { \mathsf { s } } { \mathsf { > } }$ in your counts just like any other token.

3.8 Write a program to compute unsmoothed unigrams and bigrams.

3.9 Run your n-gram program on two different small corpora of your choice (you might use email text or newsgroups). Now compare the statistics of the two corpora. What are the differences in the most common unigrams between the two? How about interesting differences in bigrams?   
3.10 Add an option to your program to generate random sentences.   
3.11 Add an option to your program to compute the perplexity of a test set.   
3.12 You are given a training set of 100 numbers that consists of 91 zeros and 1 each of the other digits 1-9. Now we see the following test set: 0 0 0 0 0 3 0 0 0 0. What is the unigram perplexity?

# 4

# Naive Bayes, Text Classification, and Sentiment

sentiment analysis

Classification lies at the heart of both human and machine intelligence. Deciding what letter, word, or image has been presented to our senses, recognizing faces or voices, sorting mail, assigning grades to homeworks; these are all examples of assigning a category to an input. The potential challenges of this task are highlighted by the fabulist Jorge Luis Borges (1964), who imagined classifying animals into:

(a) those that belong to the Emperor, (b) embalmed ones, (c) those that are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray dogs, (h) those that are included in this classification, (i) those that tremble as if they were mad, (j) innumerable ones, (k) those drawn with a very fine camel’s hair brush, (l) others, (m) those that have just broken a flower vase, (n) those that resemble flies from a distance.

Many language processing tasks involve classification, although luckily our classes are much easier to define than those of Borges. In this chapter we introduce the naive Bayes algorithm and apply it to text categorization, the task of assigning a label or category to an entire text or document.

We focus on one common text categorization task, sentiment analysis, the extraction of sentiment, the positive or negative orientation that a writer expresses toward some object. A review of a movie, book, or product on the web expresses the author’s sentiment toward the product, while an editorial or political text expresses sentiment toward a candidate or political action. Extracting consumer or public sentiment is thus relevant for fields from marketing to politics.

The simplest version of sentiment analysis is a binary classification task, and the words of the review provide excellent cues. Consider, for example, the following phrases extracted from positive and negative reviews of movies and restaurants. Words like great, richly, awesome, and pathetic, and awful and ridiculously are very informative cues:

spam detection

$^ +$ ...zany characters and richly applied satire, and some great plot twists − It was pathetic. The worst part about it was the boxing scenes... $^ +$ ...awesome caramel sauce and sweet toasty almonds. I love this place! ...awful pizza and ridiculously overpriced...

Spam detection is another important commercial application, the binary classification task of assigning an email to one of the two classes spam or not-spam. Many lexical and other features can be used to perform this classification. For example you might quite reasonably be suspicious of an email containing phrases like “online pharmaceutical” or “WITHOUT ANY COST” or “Dear Winner”.

Another thing we might want to know about a text is the language it’s written in. Texts on social media, for example, can be in any number of languages and we’ll need to apply different processing. The task of language id is thus the first step in most language processing pipelines. Related text classification tasks like authorship attribution— determining a text’s author— are also relevant to the digital humanities, social sciences, and forensic linguistics.

Finally, one of the oldest tasks in text classification is assigning a library subject category or topic label to a text. Deciding whether a research paper concerns epidemiology or instead, perhaps, embryology, is an important component of information retrieval. Various sets of subject categories exist, such as the MeSH (Medical Subject Headings) thesaurus. In fact, as we will see, subject category classification is the task for which the naive Bayes algorithm was invented in 1961 (Maron, 1961).

Classification is essential for tasks below the level of the document as well. We’ve already seen period disambiguation (deciding if a period is the end of a sentence or part of a word), and word tokenization (deciding if a character should be a word boundary). Even language modeling can be viewed as classification: each word can be thought of as a class, and so predicting the next word is classifying the context-so-far into a class for each next word. A part-of-speech tagger (Chapter 17) classifies each occurrence of a word in a sentence as, e.g., a noun or a verb.

The goal of classification is to take a single observation, extract some useful features, and thereby classify the observation into one of a set of discrete classes. One method for classifying text is to use rules handwritten by humans. Handwritten rule-based classifiers can be components of state-of-the-art systems in language processing. But rules can be fragile, as situations or data change over time, and for some tasks humans aren’t necessarily good at coming up with the rules.

The most common way of doing text classification in language processing is instead via supervised machine learning, the subject of this chapter. In supervised learning, we have a data set of input observations, each associated with some correct output (a ‘supervision signal’). The goal of the algorithm is to learn how to map from a new observation to a correct output.

Formally, the task of supervised classification is to take an input $x$ and a fixed set of output classes $Y = \{ y _ { 1 } , y _ { 2 } , . . . , y _ { M } \}$ and return a predicted class $y \in Y$ . For text classification, we’ll sometimes talk about $c$ (for “class”) instead of $y$ as our output variable, and $d$ (for “document”) instead of $x$ as our input variable. In the supervised situation we have a training set of $N$ documents that have each been handlabeled with a class: $\{ ( d _ { 1 } , c _ { 1 } ) , . . . . , ( d _ { N } , c _ { N } ) \}$ . Our goal is to learn a classifier that is capable of mapping from a new document $d$ to its correct class $c \in C$ , where $C$ is some set of useful document classes. A probabilistic classifier additionally will tell us the probability of the observation being in the class. This full distribution over the classes can be useful information for downstream decisions; avoiding making discrete decisions early on can be useful when combining systems.

Many kinds of machine learning algorithms are used to build classifiers. This chapter introduces naive Bayes; the following one introduces logistic regression. These exemplify two ways of doing classification. Generative classifiers like naive Bayes build a model of how a class could generate some input data. Given an observation, they return the class most likely to have generated the observation. Discriminative classifiers like logistic regression instead learn what features from the input are most useful to discriminate between the different possible classes. While discriminative systems are often more accurate and hence more commonly used, generative classifiers still have a role.

# 4.1 Naive Bayes Classifiers

In this section we introduce the multinomial naive Bayes classifier, so called because it is a Bayesian classifier that makes a simplifying (naive) assumption about

bag of words

how the features interact.

The intuition of the classifier is shown in Fig. 4.1. We represent a text document as if it were a bag of words, that is, an unordered set of words with their position ignored, keeping only their frequency in the document. In the example in the figure, instead of representing the word order in all the phrases like “I love this movie” and “I would recommend it”, we simply note that the word $I$ occurred 5 times in the entire excerpt, the word $i t 6$ times, the words love, recommend, and movie once, and so on.

![## Image Analysis: 3e02c6543f41be5ade4a65b6e2e31f7f0753fce7fdcafb2133422383908057ff.jpg

**Conceptual Understanding:**
This image conceptually represents the 'bag-of-words' assumption as applied to text processing, specifically in the context of analyzing a movie review for classification. The main purpose is to illustrate how natural language text is simplified into a quantitative format suitable for machine learning algorithms. It conveys the key idea that the order and grammatical structure of words in a document are discarded, and only the presence and frequency of individual words are considered for a given task, such as sentiment analysis or topic classification, as a precursor to using classifiers like Naive Bayes.

**Content Interpretation:**
The image demonstrates the initial stages of natural language processing (NLP) for text classification, specifically the feature extraction process under the bag-of-words model. It shows: 1. Text Input: A raw movie review text as the starting point. 2. Tokenization and Bag-of-Words Transformation: The text is broken into individual words (tokens), and their order is ignored, represented by an unordered 'bag' of words. This collection includes 'fairy', 'always', 'love', 'to', 'it', 'it', 'whimsical', 'it', 'I', 'and', 'seen', 'are', 'anyone', 'friend', 'happy', 'dialogue', 'recommend', 'adventure', 'sweet', 'of', 'satirical', 'who', 'it', 'but', 'to', 'movie', 'it', 'I', 'romantic', 'I', 'several', 'yet', 'again', 'it', 'the', 'humor', 'the', 'seen', 'would', 'to', 'scenes', 'I', 'the', 'manages', 'fun', 'I', 'and', 'about', 'times', 'and', 'whenever', 'while', 'conventions', 'have', 'with'. 3. Frequency Counting: The occurrences of each unique word from the bag are counted, resulting in a structured numerical representation. The core concept illustrated is the Bag-of-Words Model, where a document is treated as an unordered collection of its words, disregarding grammar and word order. This process is crucial for feature engineering in text, converting text into numerical features (word counts) for machine learning algorithms. The final word frequency list ('it 6', 'I 5', 'the 4', etc.) is significant as it forms the basis for classifiers like Naive Bayes, where the frequency of descriptive words can be indicative of sentiment or other classifications.

**Key Insights:**
Key takeaways are: 1. Text Simplification for Machine Learning: Complex, unstructured text must be converted into a numerical, structured format for algorithms. Evidence: The transformation from a movie review paragraph to a word frequency list ('it 6', 'I 5', 'the 4', etc.). 2. The 'Bag-of-Words' Assumption: Text is simplified by treating documents as an unordered collection of words, ignoring grammar and word order. Evidence: The visual representation of words scattered in a 'bag' ('fairy', 'always', 'love', 'to', 'it', 'it', etc.) rather than in sentence structure. 3. Importance of Word Frequencies: For certain NLP tasks and models (like Naive Bayes), word frequency is a crucial feature. Evidence: The final word frequency list provides quantitative counts, which are essential inputs for classification. 4. Loss of Contextual Information: The process sacrifices detailed semantic and syntactic information present in the original sentence structure, focusing solely on individual word counts. Evidence: The transition from the flowing review text to the jumbled words in the bag and then just their counts.

**Document Context:**
This image directly supports the '4.1 Naive Bayes Classifiers' section of the document, as stated in the context. The accompanying text explicitly links it to the 'intuition of the multinomial naive Bayes classifier applied to a movie review' and highlights the 'bag-of-words assumption' and the use of 'frequency of each word'. It serves as a foundational illustration, demonstrating the essential preprocessing step where raw text is transformed into a quantifiable feature vector (word frequencies) that a Naive Bayes model can then utilize for classification tasks, such as sentiment analysis. The figure clarifies how the input data for such a classifier is prepared.

**Summary:**
This diagram illustrates the fundamental process of converting a written document, specifically a movie review, into a numerical representation suitable for machine learning, a technique often used in text classification. The process begins on the left with the original movie review text. This text is a natural language paragraph: "I love this movie! It's sweet, but with satirical humor. The dialogue is great and the adventure scenes are fun... It manages to be whimsical and romantic while laughing at the conventions of the fairy tale genre. I would recommend it to just about anyone. I've seen it several times, and I'm always happy to see it again whenever I have a friend who hasn't seen it yet!" This is the raw input data. A blue arrow then points to the middle section, which represents a "bag of words." This visual metaphor signifies a crucial step in text processing: taking all the words from the review and treating them as an unordered collection. In this "bag," you can see individual words extracted from the review, such as "fairy", "always", "love", "to", "it", "it", "whimsical", "it", "I", "and", "seen", "are", "anyone", "friend", "happy", "dialogue", "recommend", "adventure", "sweet", "of", "satirical", "who", "it", "but", "to", "movie", "it", "I", "romantic", "I", "several", "yet", "again", "it", "the", "humor", "the", "seen", "would", "to", "scenes", "I", "the", "manages", "fun", "I", "and", "about", "times", "and", "whenever", "while", "conventions", "have", "with". The key idea here is that the original word order, grammar, and sentence structure are ignored; only the words themselves are retained. This is known as the "bag-of-words" assumption. Finally, another blue arrow leads to the rightmost section, which is a word frequency list. This list is generated by counting how many times each unique word appears in the "bag of words." It provides a quantitative summary of the text. For example, the list shows "it" appears 6 times, "I" appears 5 times, "the" appears 4 times, "to" and "and" both appear 3 times, "seen" appears 2 times, and words like "yet", "would", "whimsical", "times", "sweet", "satirical", "adventure", "genre", "fairy", "humor", "have", and "great" each appear 1 time. The "..." indicates that more words from the review would continue down this list. This word frequency list is the numerical representation of the movie review, which can then be used as input for algorithms like the Multinomial Naive Bayes classifier to perform tasks such as sentiment analysis (determining if the review is positive or negative).](images/3e02c6543f41be5ade4a65b6e2e31f7f0753fce7fdcafb2133422383908057ff.jpg)
Figure 4.1 Intuition of the multinomial naive Bayes classifier applied to a movie review. The position of the words is ignored (the bag-of-words assumption) and we make use of the frequency of each word.

# argmax

Naive Bayes is a probabilistic classifier, meaning that for a document $d$ , out of all classes $c \in C$ the classifier returns the class $\hat { c }$ which has the maximum posterior probability given the document. In Eq. 4.1 we use the hat notation ˆ to mean “our estimate of the correct class”, and we use argmax to mean an operation that selects the argument (in this case the class $c$ ) that maximizes a function (in this case the probability $P ( c | d )$ .

$$
{ \hat { c } } = \operatorname * { a r g m a x } _ { c \in C } P ( c | d )
$$

# Bayesian inference

This idea of Bayesian inference has been known since the work of Bayes (1763), and was first applied to text classification by Mosteller and Wallace (1964). The intuition of Bayesian classification is to use Bayes’ rule to transform Eq. 4.1 into other probabilities that have some useful properties. Bayes’ rule is presented in Eq. 4.2; it gives us a way to break down any conditional probability $P ( x | y )$ into three other probabilities:

$$
P ( x | y ) = { \frac { P ( y | x ) P ( x ) } { P ( y ) } }
$$

We can then substitute Eq. 4.2 into Eq. 4.1 to get Eq. 4.3:

$$
\hat { c } = \underset { c \in C } { \operatorname { a r g m a x } } P ( c | d ) = \underset { c \in C } { \operatorname { a r g m a x } } \frac { P ( d | c ) P ( c ) } { P ( d ) }
$$

We can conveniently simplify Eq. 4.3 by dropping the denominator $P ( d )$ . This is possible because we will be computing P(d|c)P(c) for each possible class. But P(d) doesn’t change for each class; we are always asking about the most likely class for the same document $d$ , which must have the same probability $P ( d )$ . Thus, we can choose the class that maximizes this simpler formula:

$$
\hat { c } = \underset { c \in C } { \operatorname { a r g m a x } } P ( c | d ) = \underset { c \in C } { \operatorname { a r g m a x } } P ( d | c ) P ( c )
$$

We call Naive Bayes a generative model because we can read Eq. 4.4 as stating a kind of implicit assumption about how a document is generated: first a class is sampled from $P ( c )$ , and then the words are generated by sampling from $P ( d | c )$ . (In fact we could imagine generating artificial documents, or at least their word counts, by following this process). We’ll say more about this intuition of generative models in Chapter 5.

To return to classification: we compute the most probable class $\hat { c }$ given some document $d$ by choosing the class which has the highest product of two probabilities: the prior probability of the class $P ( c )$ and the likelihood of the document $P ( d | c )$ :

$$
{ \hat { c } } = \underset { c \in C } { \operatorname { a r g m a x } } \ \overbrace { P ( d | c ) } ^ { ( \bullet \bullet \qquad \widehat { P ( c ) }  } \ \overbrace { P ( c ) } ^ {  \widehat { } }
$$

Without loss of generality, we can represent a document $d$ as a set of features $f _ { 1 } , f _ { 2 } , . . . , f _ { n }$ :

$$
{ \hat { c } } = \underset { c \in C } { \operatorname { a r g m a x } } \overbrace { P ( f _ { 1 } , f _ { 2 } , . . . . , f _ { n } | c ) } ^ {  \langle { \mathrm { 1 } } \rangle } \ \overbrace { P ( c ) } ^ {  \langle { \mathrm { 2 } } \rangle } 
$$

Unfortunately, Eq. 4.6 is still too hard to compute directly: without some simplifying assumptions, estimating the probability of every possible combination of features (for example, every possible set of words and positions) would require huge numbers of parameters and impossibly large training sets. Naive Bayes classifiers therefore make two simplifying assumptions.

The first is the bag-of-words assumption discussed intuitively above: we assume position doesn’t matter, and that the word “love” has the same effect on classification whether it occurs as the 1st, $2 0 \mathrm { { t h } }$ , or last word in the document. Thus we assume that the features $f _ { 1 } , f _ { 2 } , . . . , f _ { n }$ only encode word identity and not position.

The second is commonly called the naive Bayes assumption: this is the conditional independence assumption that the probabilities $P ( f _ { i } | c )$ are independent given the class $c$ and hence can be ‘naively’ multiplied as follows:

$$
P ( f _ { 1 } , f _ { 2 } , . . . . , f _ { n } | c ) ~ = ~ P ( f _ { 1 } | c ) \cdot P ( f _ { 2 } | c ) \cdot . . . \cdot P ( f _ { n } | c )
$$

The final equation for the class chosen by a naive Bayes classifier is thus:

$$
c _ { N B } = \underset { c \in C } { \mathrm { a r g m a x } } P ( c ) \prod _ { f \in F } P ( f | c )
$$

To apply the naive Bayes classifier to text, we will use each word in the documents as a feature, as suggested above, and we consider each of the words in the document by walking an index through every word position in the document:

$$
\begin{array} { r c l } { { \mathrm { p o s i t i o n s } } } & { {  } } & { { \mathrm { a l l ~ w o r d ~ p o s i t i o n s ~ i n ~ t e s t ~ d o c v } } } \\ { { c _ { N B } } } & { { = } } & { { \displaystyle \operatorname * { a r g m a x } _ { c \in { \cal C } } P ( c ) \prod _ { i \in p o s i t i o n s } P ( w _ { i } | c ) } } \end{array}
$$

Naive Bayes calculations, like calculations for language modeling, are done in log space, to avoid underflow and increase speed. Thus Eq. 4.9 is generally instead expressed1 as

$$
c _ { N B } ~ = ~ { \underset { c \in C } { \operatorname { a r g m a x } } } \log P ( c ) + \sum _ { i \in p o s i t i o n s } \log P ( w _ { i } | c )
$$

By considering features in log space, Eq. 4.10 computes the predicted class as a linear function of input features. Classifiers that use a linear combination of the inputs to make a classification decision —like naive Bayes and also logistic regression— are called linear classifiers.

# 4.2 Training the Naive Bayes Classifier

How can we learn the probabilities $P ( c )$ and $P ( \mathbb { f } _ { i } | c ) \hat { ! }$ Let’s first consider the maximum likelihood estimate. We’ll simply use the frequencies in the data. For the class prior $P ( c )$ we ask what percentage of the documents in our training set are in each class $c$ . Let $N _ { c }$ be the number of documents in our training data with class $c$ and $N _ { d o c }$ be the total number of documents. Then:

$$
\hat { P } ( c ) = \frac { N _ { c } } { N _ { d o c } }
$$

To learn the probability $P ( f _ { i } | c )$ , we’ll assume a feature is just the existence of a word in the document’s bag of words, and so we’ll want $P ( w _ { i } | c )$ , which we compute as the fraction of times the word $w _ { i }$ appears among all words in all documents of topic $c$ . We first concatenate all documents with category $c$ into one big “category $c ^ { \prime \prime }$ text. Then we use the frequency of $w _ { i }$ in this concatenated document to give a maximum likelihood estimate of the probability:

$$
\hat { P } ( w _ { i } | { c } ) ~ = ~ \frac { c o u n t ( w _ { i } , c ) } { \sum _ { w \in V } c o u n t ( w , c ) }
$$

Here the vocabulary $\mathrm { v }$ consists of the union of all the word types in all classes, not just the words in one class $c$ .

There is a problem, however, with maximum likelihood training. Imagine we are trying to estimate the likelihood of the word “fantastic” given class positive, but suppose there are no training documents that both contain the word “fantastic” and are classified as positive. Perhaps the word “fantastic” happens to occur (sarcastically?) in the class negative. In such a case the probability for this feature will be zero:

$$
\hat { P } ( ^ { \mathrm { * } } \mathrm { f a n t a s t i c } ^ { \mathrm { , } } | \mathrm { p o s i t i v e } ) = \frac { c o u n t ( ^ { \mathrm { * } } \mathrm { f a n t a s t i c } ^ { \mathrm { , } } , \mathrm { p o s i t i v e } ) } { \sum _ { w \in V } c o u n t ( w , \mathrm { p o s i t i v e } ) } = 0
$$

But since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence!

The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization:

$$
\hat { P } ( w _ { i } | c ) ~ = ~ \frac { c o u n t ( w _ { i } , c ) + 1 } { \sum _ { w \in V } \left( c o u n t ( w , c ) + 1 \right) } = \frac { c o u n t ( w _ { i } , c ) + 1 } { \left( \sum _ { w \in V } c o u n t ( w , c ) \right) + | V | }
$$

Note once again that it is crucial that the vocabulary $\mathrm { v }$ consists of the union of all the word types in all classes, not just the words in one class $c$ (try to convince yourself why this must be true; see the exercise at the end of the chapter).

What do we do about words that occur in our test data but are not in our vocabulary at all because they did not occur in any training document in any class? The solution for such unknown words is to ignore them—remove them from the test document and not include any probability for them at all.

stop words

Finally, some systems choose to completely ignore another class of words: stop words, very frequent words like the and $a$ . This can be done by sorting the vocabulary by frequency in the training set, and defining the top 10–100 vocabulary entries as stop words, or alternatively by using one of the many predefined stop word lists available online. Then each instance of these stop words is simply removed from both training and test documents as if it had never occurred. In most text classification applications, however, using a stop word list doesn’t improve performance, and so it is more common to make use of the entire vocabulary and not use a stop word list.

Fig. 4.2 shows the final algorithm.

# 4.3 Worked example

Let’s walk through an example of training and testing naive Bayes with add-one smoothing. We’ll use a sentiment analysis domain with the two classes positive $( + )$ and negative (-), and take the following miniature training and test documents simplified from actual movie reviews.

<table><tr><td colspan="2">Cat</td></tr><tr><td rowspan="6">Training</td><td> just plain boring</td></tr><tr><td> entirely predictable and lacks energy</td></tr><tr><td> no surprises and very few laughs</td></tr><tr><td>+ very powerful</td></tr><tr><td></td></tr><tr><td>？</td></tr><tr><td>+ Test</td><td>the most fun film of the summer predictable with no fun</td></tr></table>

The prior $P ( c )$ for the two classes is computed via Eq. 4.11 as $\frac { N _ { c } } { N _ { d o c } }$ Nc :

$$
P ( - ) = \frac { 3 } { 5 } P ( + ) = \frac { 2 } { 5 }
$$

The word with doesn’t occur in the training set, so we drop it completely (as mentioned above, we don’t use unknown word models for naive Bayes). The likelihoods from the training set for the remaining three words “predictable”, “no”, and

![## Image Analysis: 840b0178ade8c04a1d2231ac370fa9273c247c1d55cc164514933d6086796cac.jpg

**Conceptual Understanding:**
This image conceptually represents the Naive Bayes classification algorithm, specifically demonstrating its training and testing phases. The main purpose is to provide a programmatic (pseudocode) outline of how to implement a Naive Bayes classifier for text or document classification, incorporating add-1 smoothing to handle unseen words and prevent zero probabilities. It communicates the core ideas of calculating prior probabilities of classes and conditional probabilities of words given a class, and then using these probabilities to predict the class of a new, unseen document.

**Content Interpretation:**
The image depicts two key processes within the Naive Bayes system: `TRAIN NAIVE BAYES` and `TEST NAIVE BAYES`.

**Training Phase (`TRAIN NAIVE BAYES`):**
*   **Concepts:** This section calculates the necessary probabilities from a training dataset (`D`) for different classes (`C`).
*   **Prior Probabilities:** The lines `Ndoc = number of documents in D`, `Nc = number of documents from D in class c`, and `logprior[c] ← log Nc / Ndoc` explicitly show the calculation of the log-prior probability for each class `c`. This is the probability of encountering a document from a specific class `c` in the training data, expressed in logarithmic form.
*   **Vocabulary and Document Aggregation:** `V ← vocabulary of D` and `bigdoc[c] ← append(d) for d ∈ D with class c` illustrate the steps to build the unique word list and aggregate documents by class for subsequent word counting.
*   **Conditional (Likelihood) Probabilities:** The core of the training involves calculating `loglikelihood[w,c] ← log (count(w,c) + 1) / (Σw' in V (count(w',c) + 1))`. This line, along with `count(w,c) ← # of occurrences of w in bigdoc[c]`, shows how the log-conditional probability of a word `w` appearing in a document of class `c` is computed. The `+ 1` terms are crucial: they signify "add-1 smoothing" (Laplace smoothing). This technique ensures that even if a word `w` does not appear in any document of a particular class `c` during training (making `count(w,c)` zero), its `loglikelihood[w,c]` will not be negative infinity, thus preventing a zero probability from completely negating the probability of a class for a test document. The denominator `Σw' in V (count(w',c) + 1)` represents the total count of all words (plus 1 for each vocabulary word) in documents belonging to class `c`'s `bigdoc[c]` (the aggregated documents for class `c`).

**Testing Phase (`TEST NAIVE BAYES`):**
*   **Concepts:** This section uses the trained `logprior` and `loglikelihood` values to predict the most probable class for a new `testdoc`.
*   **Score Accumulation:** For each class `c`, `sum[c]` is initialized with `logprior[c]`. Then, `for each position i in testdoc`, if `word ← testdoc[i]` is `if word ∈ V`, its corresponding `loglikelihood[word,c]` is added to `sum[c]`. This process accumulates the evidence for each class. By summing log-probabilities, the algorithm effectively multiplies probabilities (since `log(A*B) = log(A) + log(B)`), which is numerically more stable than multiplying many small floating-point numbers.
*   **Classification Decision:** Finally, `return argmaxc sum[c]` indicates that the class with the highest accumulated `sum[c]` is selected as the predicted class for the `testdoc`. This is based on the Naive Bayes assumption that features (words) are conditionally independent given the class.

**Key Insights:**
**Main Takeaways/Lessons:**

*   **Naive Bayes Algorithm Structure:** The image clearly outlines the two fundamental phases of a Naive Bayes classifier: training (learning probabilities from data) and testing (applying learned probabilities to classify new data).
*   **Logarithmic Probabilities for Stability:** The use of `log` in `logprior[c] ← log Nc / Ndoc` and `loglikelihood[w,c] ← log ...` highlights a common practice in NLP and machine learning to work with log-probabilities. This is to avoid underflow issues when multiplying many small probabilities, as summing log-probabilities is equivalent to multiplying original probabilities.
*   **Add-1 Smoothing for Robustness:** The term `+ 1` in `loglikelihood[w,c] ← log (count(w,c) + 1) / (Σw' in V (count(w',c) + 1))` is critical. It demonstrates how "add-1 smoothing" (Laplace smoothing) is applied to handle the "zero-frequency problem" where a word might appear in a test document but not in the training data for a specific class. Without smoothing, such a word would result in a zero conditional probability, causing the entire posterior probability for that class to become zero, regardless of other strong evidence. Add-1 smoothing assigns a small, non-zero probability, making the model more robust.
*   **Classification by Maximum A Posteriori:** The `return argmaxc sum[c]` line shows that the classification decision is made by selecting the class that yields the maximum posterior probability (or, equivalently, the maximum sum of log-posterior probabilities, thanks to the log-transform and the Naive Bayes assumption).

**Specific Textual Evidence for Insights:**

*   `function TRAIN NAIVE BAYES(D, C) returns V, log P(c), log P(w|c)`: Explicitly states the inputs, purpose, and outputs of the training phase, including the types of probabilities learned.
*   `# Calculate P(c) terms` and `logprior[c] ← log Nc / Ndoc`: Directly indicates the computation of class prior probabilities.
*   `# Calculate P(w|c) terms` and `loglikelihood[w,c] ← log (count(w,c) + 1) / (Σw' in V (count(w',c) + 1))`: Clearly shows the calculation of word-given-class conditional probabilities, and the `+ 1` terms provide explicit evidence of add-1 smoothing.
*   `function TEST NAIVE BAYES(...) returns best c`: Defines the purpose of the testing phase – to return the optimal class.
*   `sum[c] ← logprior[c]` and `sum[c] ← sum[c] + loglikelihood[word,c]`: Illustrates the accumulation of log-probabilities to form a score for each class.
*   `return argmaxc sum[c]`: Provides evidence for the final decision rule based on selecting the class with the highest accumulated score.

**Document Context:**
This image serves as a "Worked example" (Section 4.3) and is labeled as "Figure 4.2 The naive Bayes algorithm, using add-1 smoothing." It directly demonstrates the algorithmic steps for implementing a Naive Bayes classifier as discussed in the surrounding document. The text after the image, "To use add- α smoothing instead, change the + 1 to + α for loglikelihood counts in training," further reinforces the specific smoothing technique used in the depicted pseudocode and offers a clear modification instruction for a variant. This figure is crucial for understanding the practical application and mathematical basis of the Naive Bayes model within the document's context, especially for text classification tasks.

**Summary:**
This figure presents the step-by-step pseudocode for implementing a Naive Bayes algorithm, broken down into two main parts: training and testing. The algorithm is designed for classification tasks, such as determining the category of a document.

**Part 1: Training the Naive Bayes Model (`TRAIN NAIVE BAYES` Function)**

1.  **Inputs and Outputs:** This function takes a set of training documents (`D`) and the possible classes (`C`) as input. It outputs the learned vocabulary (`V`), and the log-probabilities that will be used for classification: `logprior` for each class and `loglikelihood` for each word given a class.
2.  **Calculating Class Prior Probabilities:** For each class `c`:
    *   It first counts the total number of documents in the dataset (`Ndoc`).
    *   Then, it counts how many of these documents belong to the current class `c` (`Nc`).
    *   The `logprior[c]` is calculated as the logarithm of `Nc` divided by `Ndoc`. This essentially represents the general likelihood of a document belonging to class `c` before even looking at its content.
3.  **Building Vocabulary and Class-Specific Data:**
    *   A comprehensive `vocabulary (V)` of all unique words found across all training documents is created.
    *   For each class `c`, all documents belonging to that class are effectively combined into a single, large document called `bigdoc[c]`.
4.  **Calculating Word Conditional Probabilities (Log-Likelihoods):** For every unique word `w` in the `vocabulary (V)`:
    *   It counts how many times `w` appears in the `bigdoc[c]` for the current class `c` (`count(w,c)`).
    *   The `loglikelihood[w,c]` is calculated as the logarithm of a fraction. The numerator is `count(w,c) + 1`. The `+ 1` is important: it's a technique called "add-1 smoothing" (also known as Laplace smoothing). This prevents any word from having a zero probability if it didn't appear in a particular class's training documents, which would otherwise make it impossible to classify a test document containing that word.
    *   The denominator of this fraction is the sum of `(count(w',c) + 1)` for all words `w'` in the entire vocabulary `V`. This essentially represents the total number of word occurrences in `bigdoc[c]`, adjusted for the smoothing factor.
5.  **Return Values:** The function concludes by returning these calculated `logprior` values, `loglikelihood` values, and the `vocabulary (V)`. These are the trained parameters of our classifier.

**Part 2: Testing the Naive Bayes Model (`TEST NAIVE BAYES` Function)**

1.  **Inputs and Outputs:** This function takes a new document to be classified (`testdoc`), the `logprior` and `loglikelihood` values learned during training, the set of classes (`C`), and the `vocabulary (V)`. It outputs the `best c`, which is the predicted class for the `testdoc`.
2.  **Initializing Class Scores:** For each possible class `c`:
    *   A score `sum[c]` is initialized with the `logprior[c]` value. This is the starting "likelihood" that the `testdoc` belongs to class `c`.
3.  **Processing Words in the Test Document:** The algorithm then goes through each word (`word`) in the `testdoc`:
    *   **Vocabulary Check:** It first checks `if word ∈ V` (if the word was seen during training and is part of the learned vocabulary).
    *   **Updating Class Scores (If Word is Known):** If the word is in the vocabulary, its `loglikelihood[word,c]` (the log-probability of this word appearing given class `c`) is added to `sum[c]`. This step accumulates evidence for each class based on the words present in the `testdoc`. Adding log-probabilities is mathematically equivalent to multiplying the original probabilities, which helps avoid numerical underflow.
    *   **Ignoring Unknown Words:** If a word from the `testdoc` is *not* in the training `vocabulary (V)`, it is simply skipped; it does not contribute to the `sum[c]` score.
4.  **Final Classification:** After all words in the `testdoc` have been processed, the function returns the class `c` for which `sum[c]` is the highest (`argmaxc sum[c]`). This class is the most probable classification for the `testdoc` according to the Naive Bayes model.

This comprehensive explanation, derived directly from the pseudocode, illustrates how the Naive Bayes algorithm learns from data and then applies that learning to classify new information, explicitly showing the role of add-1 smoothing for robustness.](images/840b0178ade8c04a1d2231ac370fa9273c247c1d55cc164514933d6086796cac.jpg)
Figure 4.2 The naive Bayes algorithm, using add-1 smoothing. To use add- $\alpha$ smoothing instead, change the $+ 1$ to $+ \alpha$ for loglikelihood counts in training.

“fun”, are as follows, from Eq. 4.14 (computing the probabilities for the remainder of the words in the training set is left as an exercise for the reader):

$$
{ \begin{array} { r l } { P ( ^ { \mathrm { * } } { \mathrm { p r e d i c t a b l e } } ^ { \mathrm { , * } } | - ) = { \frac { 1 + 1 } { 1 4 + 2 0 } } } & { P ( ^ { \mathrm { * } } { \mathrm { p r e d i c t a b l e } } ^ { \mathrm { , * } } | + ) = { \frac { 0 + 1 } { 9 + 2 0 } } } \\ { P ( ^ { \mathrm { * } } { \mathrm { n o } } ^ { \mathrm { , * } } | - ) = { \frac { 1 + 1 } { 1 4 + 2 0 } } } & { P ( ^ { \mathrm { * } } { \mathrm { n o } } ^ { \mathrm { , * } } | + ) = { \frac { 0 + 1 } { 9 + 2 0 } } } \\ { P ( ^ { \mathrm { * } } { \mathrm { f u n } } ^ { \mathrm { , * } } | - ) = { \frac { 0 + 1 } { 1 4 + 2 0 } } } & { P ( ^ { \mathrm { * } } { \mathrm { f u n } } ^ { \mathrm { , * } } | + ) = { \frac { 1 + 1 } { 9 + 2 0 } } } \end{array} }
$$

For the test sentence $\mathbf { S } =$ “predictable with no fun”, after removing the word ‘with’, the chosen class, via Eq. 4.9, is therefore computed as follows:

$$
{ \begin{array} { r l } { P ( - ) P ( S | - ) } & { = { \begin{array} { l } { 3 } \\ { 5 } \end{array} } \times { \frac { 2 \times 2 \times 1 } { 3 4 ^ { 3 } } } = 6 . 1 \times 1 0 ^ { - 5 } } \\ { P ( + ) P ( S | + ) } & { = { \begin{array} { l } { 2 } \\ { 5 } \end{array} } \times { \frac { 1 \times 1 \times 2 } { 2 9 ^ { 3 } } } = 3 . 2 \times 1 0 ^ { - 5 } } \end{array} }
$$

The model thus predicts the class negative for the test sentence.

# 4.4 Optimizing for Sentiment Analysis

While standard naive Bayes text classification can work well for sentiment analysis, some small changes are generally employed that improve performance.

First, for sentiment classification and a number of other text classification tasks, whether a word occurs or not seems to matter more than its frequency. Thus it often improves performance to clip the word counts in each document at 1 (see the end of the chapter for pointers to these results). This variant is called binary multinomial naive Bayes or binary naive Bayes. The variant uses the same algorithm as in Fig. 4.2 except that for each document we remove all duplicate words before concatenating them into the single big document during training and we also remove duplicate words from test documents. Fig. 4.3 shows an example in which a set of four documents (shortened and text-normalized for this example) are remapped to binary, with the modified counts shown in the table on the right. The example is worked without add-1 smoothing to make the differences clearer. Note that the results counts need not be 1; the word great has a count of 2 even for binary naive Bayes, because it appears in multiple documents.

# Four original documents:

− it was pathetic the worst part was the boxing scenes no plot twists or great scenes   
$^ +$ and satire and great plot twists   
$^ +$ great scenes great film

# After per-document binarization:

<table><tr><td></td><td>NB CountsCounts + 1</td><td>Binary +</td></tr><tr><td>and</td><td></td><td>2010</td></tr><tr><td>boxing film</td><td></td><td>1010</td></tr><tr><td>great 3121</td><td></td><td>0</td></tr><tr><td>it no or part</td><td>0 1 0 1 0 1 0 1</td><td>1 0 1 0 1 0 1</td></tr><tr><td>pathetic plot</td><td>0 1 1 1</td><td>0 1 1 1</td></tr><tr><td>satire scenes</td><td>1 0 1 2</td><td>1 0 1 2</td></tr><tr><td>the</td><td>02</td><td>01</td></tr><tr><td>twists</td><td></td><td>0201</td></tr><tr><td>was</td><td></td><td></td></tr><tr><td>worst</td><td>0</td><td></td></tr></table>

− it was pathetic the worst part boxing scenes   
一 no plot twists or great scenes   
$^ +$ and satire great plot twists   
$^ +$ great scenes film

A second important addition commonly made when doing text classification for sentiment is to deal with negation. Consider the difference between I really like this movie (positive) and I didn’t like this movie (negative). The negation expressed by didn’t completely alters the inferences we draw from the predicate like. Similarly, negation can modify a negative word to produce a positive review (don’t dismiss this film, doesn’t let us get bored).

A very simple baseline that is commonly used in sentiment analysis to deal with negation is the following: during text normalization, prepend the prefix $N O T _ { - }$ to every word after a token of logical negation (n’t, not, no, never) until the next punctuation mark. Thus the phrase

didn’t like this movie , but I becomes

didn’t NOT_like NOT_this NOT_movie , but I

Newly formed ‘words’ like NOT like, NOT recommend will thus occur more often in negative document and act as cues for negative sentiment, while words like NOT bored, NOT dismiss will acquire positive associations. Syntactic parsing (Chapter 18) can be used deal more accurately with the scope relationship between

General Inquirer LIWC

these negation words and the predicates they modify, but this simple baseline works quite well in practice.

Finally, in some situations we might have insufficient labeled training data to train accurate naive Bayes classifiers using all words in the training set to estimate positive and negative sentiment. In such cases we can instead derive the positive and negative word features from sentiment lexicons, lists of words that are preannotated with positive or negative sentiment. Four popular lexicons are the General Inquirer (Stone et al., 1966), LIWC (Pennebaker et al., 2007), the opinion lexicon of Hu and Liu (2004a) and the MPQA Subjectivity Lexicon (Wilson et al., 2005).

For example the MPQA subjectivity lexicon has 6885 words each marked for whether it is strongly or weakly biased positive or negative. Some examples:

+ : admirable, beautiful, confident, dazzling, ecstatic, favor, glee, great − : awful, bad, bias, catastrophe, cheat, deny, envious, foul, harsh, hate

A common way to use lexicons in a naive Bayes classifier is to add a feature that is counted whenever a word from that lexicon occurs. Thus we might add a feature called ‘this word occurs in the positive lexicon’, and treat all instances of words in the lexicon as counts for that one feature, instead of counting each word separately. Similarly, we might add as a second feature ‘this word occurs in the negative lexicon’ of words in the negative lexicon. If we have lots of training data, and if the test data matches the training data, using just two features won’t work as well as using all the words. But when training data is sparse or not representative of the test set, using dense lexicon features instead of sparse individual-word features may generalize better.

We’ll return to this use of lexicons in Chapter 22, showing how these lexicons can be learned automatically, and how they can be applied to many other tasks beyond sentiment classification.

# 4.5 Naive Bayes for other text classification tasks

In the previous section we pointed out that naive Bayes doesn’t require that our classifier use all the words in the training data as features. In fact features in naive Bayes can express any property of the input text we want.

Consider the task of spam detection, deciding if a particular piece of email is an example of spam (unsolicited bulk email)—one of the first applications of naive Bayes to text classification (Sahami et al., 1998).

A common solution here, rather than using all the words as individual features, is to predefine likely sets of words or phrases as features, combined with features that are not purely linguistic. For example the open-source SpamAssassin tool2 predefines features like the phrase “one hundred percent guaranteed”, or the feature mentions millions of dollars, which is a regular expression that matches suspiciously large sums of money. But it also includes features like HTML has a low ratio of text to image area, that aren’t purely linguistic and might require some sophisticated computation, or totally non-linguistic features about, say, the path that the email took to arrive. More sample SpamAssassin features:

• Email subject line contains “online pharmaceutical” • HTML has unbalanced “head” tags • Claims you can be removed from the list

# language id

For other tasks, like language id—determining what language a given piece of text is written in—the most effective naive Bayes features are not words at all, but character n-grams, 2-grams (‘zw’) 3-grams (‘nya’, ‘ Vo’), or 4-grams (‘ie $\mathbf { z } '$ , ‘thei’), or, even simpler byte n-grams, where instead of using the multibyte Unicode character representations called codepoints, we just pretend everything is a string of raw bytes. Because spaces count as a byte, byte n-grams can model statistics about the beginning or ending of words. A widely used naive Bayes system, langid.py (Lui and Baldwin, 2012) begins with all possible n-grams of lengths 1-4, using feature selection to winnow down to the most informative 7000 final features.

Language ID systems are trained on multilingual text, such as Wikipedia (Wikipedia text in 68 different languages was used in (Lui and Baldwin, 2011)), or newswire. To make sure that this multilingual text correctly reflects different regions, dialects, and socioeconomic classes, systems also add Twitter text in many languages geotagged to many regions (important for getting world English dialects from countries with large Anglophone populations like Nigeria or India), Bible and Quran translations, slang websites like Urban Dictionary, corpora of African American Vernacular English (Blodgett et al., 2016), and so on (Jurgens et al., 2017).

# 4.6 Naive Bayes as a Language Model

As we saw in the previous section, naive Bayes classifiers can use any sort of feature: dictionaries, URLs, email addresses, network features, phrases, and so on. But if, as in Section 4.3, we use only individual word features, and we use all of the words in the text (not a subset), then naive Bayes has an important similarity to language modeling. Specifically, a naive Bayes model can be viewed as a set of class-specific unigram language models, in which the model for each class instantiates a unigram language model.

Since the likelihood features from the naive Bayes model assign a probability to each word $P ( w o r d | c )$ , the model also assigns a probability to each sentence:

$$
P ( s | c ) = \prod _ { i \in p o s i t i o n s } P ( w _ { i } | c )
$$

Thus consider a naive Bayes model with the classes positive $( + )$ and negative (-) and the following model parameters:

<table><tr><td>W</td><td>P(w|+) P(w|-)</td><td></td></tr><tr><td>I 0.1 love 0.1</td><td>0.2 0.001</td><td></td></tr><tr><td>this</td><td>0.01</td><td>0.01</td></tr><tr><td>fun film</td><td>0.05</td><td>0.005</td></tr><tr><td></td><td>0.1</td><td>0.1</td></tr><tr><td>·</td><td>：</td><td></td></tr></table>

Each of the two columns above instantiates a language model that can assign a probability to the sentence “I love this fun film”:

$$
\begin{array} { r l } & { P \big ( \mathrm { ^ { \langle * } I ~ l o v e ~ t h i s ~ f u n ~ f u l m ^ { \prime } \vert + \rangle } \ = \ 0 . 1 \times 0 . 1 \times 0 . 0 1 \times 0 . 0 5 \times 0 . 1 = 5 \times 1 0 ^ { - 7 } } \\ & { P \big ( \mathrm { ^ { \langle * } I ~ l o v e ~ t h i s ~ f u n ~ f i l m ^ { \prime } \vert - \rangle } \ = \ 0 . 2 \times 0 . 0 0 1 \times 0 . 0 1 \times 0 . 0 0 5 \times 0 . 1 = 1 . 0 \times 1 0 ^ { - 9 } } \end{array}
$$

As it happens, the positive model assigns a higher probability to the sentence: $P ( s | p o s ) > P ( s | n e g )$ . Note that this is just the likelihood part of the naive Bayes model; once we multiply in the prior a full naive Bayes model might well make a different classification decision.

# 4.7 Evaluation: Precision, Recall, $\mathrm { F }$ -measure

gold labels

# confusion matrix

To introduce the methods for evaluating text classification, let’s first consider some simple binary detection tasks. For example, in spam detection, our goal is to label every text as being in the spam category (“positive”) or not in the spam category (“negative”). For each item (email document) we therefore need to know whether our system called it spam or not. We also need to know whether the email is actually spam or not, i.e. the human-defined labels for each document that we are trying to match. We will refer to these human labels as the gold labels.

Or imagine you’re the CEO of the Delicious Pie Company and you need to know what people are saying about your pies on social media, so you build a system that detects tweets concerning Delicious Pie. Here the positive class is tweets about Delicious Pie and the negative class is all other tweets.

In both cases, we need a metric for knowing how well our spam detector (or pie-tweet-detector) is doing. To evaluate any system for detecting things, we start by building a confusion matrix like the one shown in Fig. 4.4. A confusion matrix is a table for visualizing how an algorithm performs with respect to the human gold labels, using two dimensions (system output and gold labels), and each cell labeling a set of possible outcomes. In the spam detection case, for example, true positives are documents that are indeed spam (indicated by human-created gold labels) that our system correctly said were spam. False negatives are documents that are indeed spam but our system incorrectly labeled as non-spam.

To the bottom right of the table is the equation for accuracy, which asks what percentage of all the observations (for the spam or pie examples that means all emails or tweets) our system labeled correctly. Although accuracy might seem a natural metric, we generally don’t use it for text classification tasks. That’s because accuracy doesn’t work well when the classes are unbalanced (as indeed they are with spam, which is a large majority of email, or with tweets, which are mainly not about pie).

To make this more explicit, imagine that we looked at a million tweets, and let’s say that only 100 of them are discussing their love (or hatred) for our pie, while the other 999,900 are tweets about something completely unrelated. Imagine a simple classifier that stupidly classified every tweet as “not about pie”. This classifier would have 999,900 true negatives and only 100 false negatives for an accuracy of 999,900/1,000,000 or $9 9 . 9 9 \%$ ! What an amazing accuracy level! Surely we should be happy with this classifier? But of course this fabulous ‘no pie’ classifier would be completely useless, since it wouldn’t find a single one of the customer comments we are looking for. In other words, accuracy is not a good metric when the goal is to discover something that is rare, or at least not completely balanced in frequency, which is a very common situation in the world.

<table><tr><td colspan="5">gold standard labels</td></tr><tr><td rowspan="3">soutpm sysitie labels nsygtime</td><td> gold positive</td><td>gold negative false positive</td><td>precision</td><td></td></tr><tr><td> true positive</td><td></td><td></td><td></td></tr><tr><td>false negative tp recall =</td><td>true negative tp+fn</td><td>accuracy tp+fp+tn+fn</td><td>tp+tn</td></tr></table>

That’s why instead of accuracy we generally turn to two other metrics shown in Fig. 4.4: precision and recall. Precision measures the percentage of the items that the system detected (i.e., the system labeled as positive) that are in fact positive (i.e., are positive according to the human gold labels). Precision is defined as

$$
\mathbf { P r e c i s i o n } = { \frac { \mathbf { u u c \ p o s s t u v e s } } { \mathrm { t r u e \ p o s i t i v e s } + \mathrm { f a l s e \ p o s i t i v e s } } }
$$

Recall measures the percentage of items actually present in the input that were correctly identified by the system. Recall is defined as

$$
\mathbf { R e c a l l } = { \frac { \mathrm { t r u e ~ p o s i t i v e s } } { \mathrm { t r u e ~ p o s i t i v e s } + \mathrm { f a l s e ~ n e g a t i v e s } } }
$$

Precision and recall will help solve the problem with the useless “nothing is pie” classifier. This classifier, despite having a fabulous accuracy of $9 9 . 9 9 \%$ , has a terrible recall of 0 (since there are no true positives, and 100 false negatives, the recall is 0/100). You should convince yourself that the precision at finding relevant tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize true positives: finding the things that we are supposed to be looking for.

There are many ways to define a single metric that incorporates aspects of both precision and recall. The simplest of these combinations is the $\mathbf { F }$ -measure (van Rijsbergen, 1975) , defined as:

$$
F _ { \beta } = \frac { ( \beta ^ { 2 } + 1 ) P R } { \beta ^ { 2 } P + R }
$$

The $\beta$ parameter differentially weights the importance of recall and precision, based perhaps on the needs of an application. Values of $\beta > 1$ favor recall, while values of $\beta < 1$ favor precision. When $\beta = 1$ , precision and recall are equally balanced; this is the most frequently used metric, and is called $\mathrm { F } _ { \beta = 1 }$ or just $\mathrm { F } _ { 1 }$ :

$$
\displaystyle \mathrm { F } _ { 1 } = \frac { 2 P R } { P + R }
$$

F-measure comes from a weighted harmonic mean of precision and recall. The harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of reciprocals:

$$
\mathrm { H a r m o n i c M e a n ( a _ { 1 } , a _ { 2 } , a _ { 3 } , a _ { 4 } , . . . , a _ { n } ) = \frac { n } { \frac { 1 } { a _ { 1 } } + \frac { 1 } { a _ { 2 } } + \frac { 1 } { a _ { 3 } } + . . . + \frac { 1 } { a _ { n } } } }
$$

and hence F-measure is

$$
F = \frac { 1 } { \alpha \frac { 1 } { P } + ( 1 - \alpha ) \frac { 1 } { R } } \mathrm { o r } \left( \mathrm { w i t h } \beta ^ { 2 } = \frac { 1 - \alpha } { \alpha } \right) F = \frac { ( \beta ^ { 2 } + 1 ) P R } { \beta ^ { 2 } P + R }
$$

Harmonic mean is used because the harmonic mean of two values is closer to the minimum of the two values than the arithmetic mean is. Thus it weighs the lower of the two numbers more heavily, which is more conservative in this situation.

# 4.7.1 Evaluating with more than two classes

Up to now we have been describing text classification tasks with only two classes. But lots of classification tasks in language processing have more than two classes. For sentiment analysis we generally have 3 classes (positive, negative, neutral) and even more classes are common for tasks like part-of-speech tagging, word sense disambiguation, semantic role labeling, emotion detection, and so on. Luckily the naive Bayes algorithm is already a multi-class classification algorithm.

<table><tr><td rowspan="8"></td><td colspan="5">gold labels</td></tr><tr><td>urgent</td><td>urgent 8</td><td>normal 10</td><td>spam 1</td><td>precisionu= 101</td></tr><tr><td>Soutput normal</td><td>5</td><td>60</td><td>50</td><td>precisionm=5+60-50</td></tr><tr><td>spam</td><td>3</td><td>30</td><td>200</td><td>precisions=330200</td></tr><tr><td></td><td>8</td><td>60</td><td>recallu = recalln = recalls = 200</td><td></td></tr><tr><td>8+5+3</td><td></td><td></td><td>10+60+30 1+50+200&#x27;</td><td></td></tr></table>

But we’ll need to slightly modify our definitions of precision and recall. Consider the sample confusion matrix for a hypothetical 3-way one-of email categorization decision (urgent, normal, spam) shown in Fig. 4.5. The matrix shows, for example, that the system mistakenly labeled one spam document as urgent, and we have shown how to compute a distinct precision and recall value for each class. In order to derive a single metric that tells us how well the system is doing, we can combine these values in two ways. In macroaveraging, we compute the performance for each class, and then average over classes. In microaveraging, we collect the decisions for all classes into a single confusion matrix, and then compute precision and recall from that table. Fig. 4.6 shows the confusion matrix for each class separately, and shows the computation of microaveraged and macroaveraged precision.

As the figure shows, a microaverage is dominated by the more frequent class (in this case spam), since the counts are pooled. The macroaverage better reflects the statistics of the smaller classes, and so is more appropriate when performance on all the classes is equally important.

<table><tr><td colspan="3">Class 1: Urgent true true</td><td colspan="3">Class 2: Normal true</td><td colspan="3">Class 3: Spam</td><td colspan="3">Pooled true</td></tr><tr><td colspan="3">urgent</td><td colspan="3">true normal</td><td colspan="3">true true not</td><td colspan="3">true</td></tr><tr><td colspan="3"></td><td colspan="3"></td><td colspan="3">spam</td><td colspan="3">yes</td></tr><tr><td colspan="3">sustem</td><td>system</td><td colspan="3">not 60 55</td><td colspan="3">system20033</td><td></td><td>99</td><td>no</td></tr><tr><td colspan="3">8 system 8</td><td>11 340</td><td colspan="3">system</td><td>system</td><td>51 83</td><td></td><td>sysesm268 system 99</td><td></td><td>635</td></tr><tr><td colspan="3">preision 8=12:42</td><td></td><td></td><td colspan="3">40 212 preision= 60+5=-52</td><td>preision= 2003-.86</td><td></td><td>microavserage</td><td>29</td><td>= .73</td></tr></table>

Figure 4.6 Separate confusion matrices for the 3 classes from the previous figure, showing the pooled confusion matrix and the microaveraged and macroaveraged precision.

# 4.8 Test sets and Cross-validation

development test set devset

The training and testing procedure for text classification follows what we saw with language modeling (Section 3.2): we use the training set to train the model, then use the development test set (also called a devset) to perhaps tune some parameters, and in general decide what the best model is. Once we come up with what we think is the best model, we run it on the (hitherto unseen) test set to report its performance.

While the use of a devset avoids overfitting the test set, having a fixed training set, devset, and test set creates another problem: in order to save lots of data for training, the test set (or devset) might not be large enough to be representative. Wouldn’t it be better if we could somehow use all our data for training and still use all our data for test? We can do this by cross-validation.

cross-validation

folds

10-fold cross-validation

In cross-validation, we choose a number $k$ , and partition our data into $k$ disjoint subsets called folds. Now we choose one of those $k$ folds as a test set, train our classifier on the remaining $k - 1$ folds, and then compute the error rate on the test set. Then we repeat with another fold as the test set, again training on the other $k - 1$ folds. We do this sampling process $k$ times and average the test set error rate from these $k$ runs to get an average error rate. If we choose $k = 1 0$ , we would train 10 different models (each on $90 \%$ of our data), test the model 10 times, and average these 10 values. This is called 10-fold cross-validation.

The only problem with cross-validation is that because all the data is used for testing, we need the whole corpus to be blind; we can’t examine any of the data to suggest possible features and in general see what’s going on, because we’d be peeking at the test set, and such cheating would cause us to overestimate the performance of our system. However, looking at the corpus to understand what’s going on is important in designing NLP systems! What to do? For this reason, it is common to create a fixed training set and test set, then do 10-fold cross-validation inside the training set, but compute error rate the normal way in the test set, as shown in Fig. 4.7.

![## Image Analysis: 1ae78e45d8dc29be77374877543f9eadc97e4f087b8a3e3c2cb6e2fae6e5f38c.jpg

**Conceptual Understanding:**
This image conceptually represents the methodology of 10-fold cross-validation in machine learning model development and evaluation. It illustrates how a dataset is repeatedly split into training and validation subsets across multiple iterations, while maintaining a separate, untouched test set for final, unbiased evaluation.

The main purpose of this image is to visually demonstrate a robust technique for assessing the performance of a machine learning model. It aims to explain how to reduce the risk of overfitting and how to obtain a more reliable estimate of a model's generalization capability by systematically partitioning data. It explicitly highlights the importance of keeping a truly independent "Test Set" completely separate from the development and validation process.

The key ideas and concepts being communicated are:
1.  **Iterative Data Partitioning:** The process of dividing the data into multiple folds and iteratively using different folds for validation and training.
2.  **Rotation of Validation Set:** How each segment of the data gets an opportunity to serve as the validation set, ensuring a comprehensive assessment.
3.  **Distinction between Validation and Testing:** The critical difference between the "Dev" (validation) set used for internal model tuning and the "Test Set" reserved for final, unbiased performance evaluation.
4.  **Robustness and Bias Reduction:** The underlying principle that cross-validation provides a more stable and less biased estimate of model performance compared to a single arbitrary data split.

**Content Interpretation:**
The image systematically illustrates the process of 10-fold cross-validation combined with an independent test set for machine learning model evaluation. It shows how the data is partitioned and utilized across multiple iterations for training and validation, and then finally for unbiased testing. 

**Processes, concepts, relationships, or systems shown:**
*   **Data Splitting:** The image shows an initial dataset conceptually divided into ten segments, with one segment at a time being allocated for "Dev" (development or validation) and the rest for "Training" across the "Training Iterations". A completely separate "Test Set" is also shown, signifying its independent role.
*   **K-Fold Cross-Validation (specifically 10-fold):** The "Training Iterations" section, with its 10 distinct rows labeled "1" through "10", explicitly demonstrates the core of k-fold cross-validation. In each iteration, a different segment (fold) of the data is designated as the "Dev" set, while the remaining nine folds are combined and used as the "Training" set. This rotational allocation ensures comprehensive internal validation.
*   **Model Training and Validation:** In each iteration, a hypothetical model would be trained on the data labeled "Training" and subsequently evaluated on the data labeled "Dev". This iterative process is crucial for hyperparameter tuning, model selection, and identifying potential overfitting during development.
*   **Independent Testing:** The "Testing" section, with its "Test Set" box, unequivocally highlights that a portion of the data is held out from the entire cross-validation process. This "Test Set" is reserved for a single, final evaluation of the selected model, providing an unbiased estimate of its performance on truly unseen data.
*   **Relationship between Dev, Training, and Test Set:** The "Dev" set is a segment of the data used for internal model validation during the iterative development process, while the "Training" data is used to fit the model. Crucially, both are distinct from the "Test Set", which is used only for the ultimate, unbiased performance assessment.

**Significance of data/information presented:**
*   The **10 "Training Iterations"** (numbered 1-10) signify that the model evaluation is repeated ten times, each with a different segment of the data acting as the validation set. This approach yields a more robust and less variance-prone estimate of model performance compared to a single train-validation split.
*   The **rotating "Dev" blocks** across the iterations (e.g., in iteration 1, "Dev" is at the beginning; in iteration 5, it's in the middle; in iteration 10, it's at the end) are significant because they visually confirm that every segment of the development data serves as the validation set exactly once. This ensures that the model's performance is comprehensively assessed across the entire dataset.
*   The consistently labeled **"Training" blocks** indicate that the majority of the data (9 out of 10 folds) is consistently used for fitting the model parameters in each iteration.
*   The **separate "Test Set"** in the "Testing" section is critically important. Its complete isolation from the "Training Iterations" prevents any form of data leakage, thereby guaranteeing that the final reported performance metric is a truly unbiased reflection of how the model would generalize and perform on completely new, previously unseen data in a real-world application.

**Key Insights:**
The image effectively conveys several key takeaways and insights regarding machine learning model evaluation:

**Main Takeaways/Lessons:**
1.  **Robust Model Performance Estimation:** The diagram demonstrates that 10-fold cross-validation is a superior method for obtaining a reliable estimate of model performance compared to a single train-validation split. By iterating 10 times and systematically rotating the validation set, the impact of random data partitioning on performance metrics is minimized. (Evidence: The "Training Iterations" labeled 1 through 10, with the "Dev" block cycling through different positions, and the "Training" blocks making up the remainder of the data, illustrate this iterative and comprehensive approach).
2.  **Mitigation of Overfitting Bias and Improved Model Selection:** The iterative validation process using the "Dev" set during development helps in identifying and mitigating overfitting. By repeatedly testing the model on different, unseen (within the development context) data segments, the cross-validation process allows for more effective hyperparameter tuning and model selection, leading to a model that generalizes better. (Evidence: The distinct "Dev" block in each iteration serves as the validation data for that specific training run, allowing for internal performance assessment without touching the final evaluation data).
3.  **Critical Role of an Untouched Test Set for Unbiased Evaluation:** The image strongly emphasizes that a truly independent test set is indispensable for an unbiased assessment of a model's generalization ability. This set must remain entirely unused during any part of the model's training, hyperparameter tuning, or cross-validation process, ensuring the final performance metric is a realistic indicator of real-world performance. (Evidence: The clearly separated "Testing" section with the "Test Set" box, physically distinct from all "Training Iterations", underscores its independent and final evaluation role).

**Conclusions/Insights Supported:**
*   The overall data available for model development (i.e., the data used in the 10 iterations) is conceptually divided into 10 equally sized segments or folds.
*   Each of these 10 segments gets a unique opportunity to act as the validation set in one of the iterations.
*   The performance results obtained from these 10 iterations (e.g., average accuracy, precision, recall) are typically aggregated (e.g., averaged) to provide a single, more stable cross-validation score, which represents the model's expected performance during the development phase.
*   The cross-validation process is primarily employed for model selection and hyperparameter optimization, distinct from the ultimate, final performance evaluation, which is reserved for the independent test set.

**Textual Evidence for These Insights:**
*   The numbered "Training Iterations" (1-10) and the shifting horizontal position of the "Dev" block relative to the "Training" blocks within each iteration explicitly demonstrate the 10-fold rotational aspect of the validation set.
*   The repeated appearance of the "Training" text in multiple segments within each iteration (covering 90% of the data in each pass) visually confirms that the vast majority of the data is used for learning the model's parameters.
*   The clear header "Testing" and the distinct, isolated box labeled "Test Set" unequivocally highlight its separate and critical function for final, unbiased performance evaluation, distinct from the iterative development and validation process.

**Document Context:**
As indicated by the document context "Section: 4.8 Test sets and Cross-validation" and the text after the image "Figure 4.7 10-fold cross-validation", this image serves as a direct visual explanation of how 10-fold cross-validation is implemented. It precisely illustrates the methodology discussed in this section, aiding readers in understanding the practical partitioning and iterative use of data for robust machine learning model evaluation. Therefore, it strongly supports the document's broader narrative concerning reliable model assessment techniques in academic, technical, and research contexts.

**Summary:**
This image titled "10-fold cross-validation" illustrates a standard and robust method for evaluating machine learning models, ensuring a reliable assessment of their performance. The diagram is divided into two main conceptual areas: "Training Iterations" on the left and "Testing" on the right. 

The "Training Iterations" section demonstrates the 10 cycles of cross-validation. Each horizontal row, labeled numerically from "1" to "10", represents one iteration. In each iteration, the overall dataset (excluding a separate test set) is divided into 10 equal parts or "folds."

For **Iteration 1**, the first fold is designated as the "Dev" (development or validation) set, while the remaining nine folds are combined to form the "Training" set. 
For **Iteration 2**, the second fold becomes the "Dev" set, and the other nine folds make up the "Training" set.
This pattern continues, with the "Dev" set systematically shifting to a different fold in each subsequent iteration: **Iteration 3** uses the third fold as "Dev", **Iteration 4** uses the fourth, and so on, until **Iteration 10** uses the tenth fold as the "Dev" set. 
This rotation ensures that by the end of the process, every single fold of the data has served as the "Dev" set exactly once. In each iteration, the model would be trained on the data marked "Training" and then evaluated on the "Dev" set to tune hyperparameters or compare different model versions. The performance metrics from these 10 iterations are typically averaged to provide a more stable and less biased estimate of the model's performance during its development phase.

On the right side of the diagram, under the header "Testing", there is a distinct box labeled "Test Set". This represents a portion of the original data that has been kept entirely separate from all the "Training Iterations." The "Test Set" is crucial because it is only used once, after the cross-validation process is complete and the final model has been selected and optimized. Evaluating the final model on this previously unseen "Test Set" provides an unbiased and objective measure of how well the model is expected to perform on truly new, real-world data, thus avoiding potential data leakage or an overly optimistic performance estimate. This comprehensive approach ensures that the model's generalization ability is rigorously assessed.](images/1ae78e45d8dc29be77374877543f9eadc97e4f087b8a3e3c2cb6e2fae6e5f38c.jpg)
Figure 4.7 10-fold cross-validation

# 4.9 Statistical Significance Testing

In building systems we often need to compare the performance of two systems. How can we know if the new system we just built is better than our old one? Or better than some other system described in the literature? This is the domain of statistical hypothesis testing, and in this section we introduce tests for statistical significance for NLP classifiers, drawing especially on the work of Dror et al. (2020) and BergKirkpatrick et al. (2012).

Suppose we’re comparing the performance of classifiers $A$ and $B$ on a metric $M$ such as $\mathrm { F } _ { 1 }$ , or accuracy. Perhaps we want to know if our logistic regression sentiment classifier $A$ (Chapter 5) gets a higher $\mathrm { F } _ { 1 }$ score than our naive Bayes sentiment classifier $B$ on a particular test set $x$ . Let’s call $M ( A , x )$ the score that system $A$ gets on test set $x .$ , and $\delta ( x )$ the performance difference between $A$ and $B$ on $x$ :

$$
\delta ( x ) = M ( A , x ) - M ( B , x )
$$

effect size

We would like to know if $\delta ( x ) > 0$ , meaning that our logistic regression classifier has a higher $\mathrm { F } _ { 1 }$ than our naive Bayes classifier on $x$ . $\delta ( x )$ is called the effect size; a bigger $\delta$ means that $A$ seems to be way better than $B$ ; a small $\delta$ means $A$ seems to be only a little better.

Why don’t we just check if $\delta ( x )$ is positive? Suppose we do, and we find that the $\mathrm { F } _ { 1 }$ score of $A$ is higher than $B$ ’s by .04. Can we be certain that $A$ is better? We cannot! That’s because $A$ might just be accidentally better than $B$ on this particular $x$ . We need something more: we want to know if $A$ ’s superiority over $B$ is likely to hold again if we checked another test set $x ^ { \prime }$ , or under some other set of circumstances.

In the paradigm of statistical hypothesis testing, we test this by formalizing two hypotheses.

$$
\begin{array} { r } { { H _ { 0 } } : \delta ( x ) \le 0 } \\ { { H _ { 1 } } : \delta ( x ) > 0 } \end{array}
$$

# null hypothesis

The hypothesis $H _ { 0 }$ , called the null hypothesis, supposes that $\delta ( x )$ is actually negative or zero, meaning that $A$ is not better than $B$ . We would like to know if we can confidently rule out this hypothesis, and instead support $H _ { 1 }$ , that $A$ is better.

We do this by creating a random variable $X$ ranging over all test sets. Now we ask how likely is it, if the null hypothesis $H _ { 0 }$ was correct, that among these test sets we would encounter the value of $\delta ( x )$ that we found, if we repeated the experiment a great many times. We formalize this likelihood as the $\mathbf { p }$ -value: the probability, assuming the null hypothesis $H _ { 0 }$ is true, of seeing the $\delta ( x )$ that we saw or one even greater

$$
P ( \delta ( X ) \geq \delta ( x ) | H _ { 0 } { \mathrm { i s ~ t r u e } } )
$$

statistically significant

So in our example, this p-value is the probability that we would see $\delta ( x )$ assuming $A$ is not better than $B$ . If $\delta ( x )$ is huge (let’s say $A$ has a very respectable $\mathrm { F } _ { 1 }$ of .9 and $B$ has a terrible $\mathrm { F } _ { 1 }$ of only .2 on $x _ { \cdot }$ ), we might be surprised, since that would be extremely unlikely to occur if $H _ { 0 }$ were in fact true, and so the $\mathsf { p }$ -value would be low (unlikely to have such a large $\delta$ if $A$ is in fact not better than $B$ ). But if $\delta ( x )$ is very small, it might be less surprising to us even if $H _ { 0 }$ were true and $A$ is not really better than $B$ , and so the p-value would be higher.

A very small p-value means that the difference we observed is very unlikely under the null hypothesis, and we can reject the null hypothesis. What counts as very small? It is common to use values like .05 or .01 as the thresholds. A value of .01 means that if the p-value (the probability of observing the $\delta$ we saw assuming $H _ { 0 }$ is true) is less than .01, we reject the null hypothesis and assume that $A$ is indeed better than $B$ . We say that a result (e.g., $^ { 6 6 } A$ is better than $B ^ { \prime \prime }$ ) is statistically significant if the $\delta$ we saw has a probability that is below the threshold and we therefore reject this null hypothesis.

How do we compute this probability we need for the p-value? In NLP we generally don’t use simple parametric tests like t-tests or ANOVAs that you might be familiar with. Parametric tests make assumptions about the distributions of the test statistic (such as normality) that don’t generally hold in our cases. So in NLP we usually use non-parametric tests based on sampling: we artificially create many versions of the experimental setup. For example, if we had lots of different test sets $x ^ { \prime }$ we could just measure all the $\delta ( x ^ { \prime } )$ for all the $x ^ { \prime }$ . That gives us a distribution. Now we set a threshold (like .01) and if we see in this distribution that $9 9 \%$ or more of those deltas are smaller than the delta we observed, i.e., that p-value $( x )$ —the probability of seeing a $\delta ( x )$ as big as the one we saw—is less than .01, then we can reject the null hypothesis and agree that $\delta ( x )$ was a sufficiently surprising difference and $A$ is really a better algorithm than $B$ .

approximate randomization

paired

There are two common non-parametric tests used in NLP: approximate randomization (Noreen, 1989) and the bootstrap test. We will describe bootstrap below, showing the paired version of the test, which again is most common in NLP. Paired tests are those in which we compare two sets of observations that are aligned: each observation in one set can be paired with an observation in another. This happens naturally when we are comparing the performance of two systems on the same test set; we can pair the performance of system $A$ on an individual observation $x _ { i }$ with the performance of system $B$ on the same $x _ { i }$ .

bootstrap test bootstrapping

# 4.9.1 The Paired Bootstrap Test

The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from precision, recall, or F1 to the BLEU metric used in machine translation. The word bootstrapping refers to repeatedly drawing large numbers of samples with replacement (called bootstrap samples) from an original set. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population.

Consider a tiny text classification example with a test set $x$ of 10 documents. The first row of Fig. 4.8 shows the results of two classifiers (A and B) on this test set. Each document is labeled by one of the four possibilities (A and B both right, both wrong, A right and B wrong, A wrong and B right). A slash through a letter $\textcircled { \mathsf { B } }$ means that that classifier got the answer wrong. On the first document both A and B get the correct class (AB), while on the second document A got it right but B got it wrong $( \mathbf { A } , \mathbf { \Lambda } ^ { \prime } )$ . If we assume for simplicity that our metric is accuracy, A has an accuracy of .70 and B of .50, so $\delta ( x )$ is .20.

Now we create a large number $^ b$ (perhaps $1 0 ^ { 5 }$ ) of virtual test sets $x ^ { ( i ) }$ , each of size $n = 1 0$ . Fig. 4.8 shows a couple of examples. To create each virtual test set $x ^ { ( i ) }$ , we repeatedly ${ \mathrm { \Delta } n = 1 0 }$ times) select a cell from row $x$ with replacement. For example, to create the first cell of the first virtual test set $x ^ { ( 1 ) }$ , if we happened to randomly select the second cell of the $x$ row; we would copy the value $\mathsf { A } \mathsf { B }$ into our new cell, and move on to create the second cell of $x ^ { ( 1 ) }$ , each time sampling (randomly choosing) from the original $x$ with replacement.

<table><tr><td></td><td></td><td>12 3456789 10A%B%δ()</td></tr><tr><td>X x(1）</td><td>AB AB AB AB AB AB AB AB AB AB .70 .50 .20</td></tr><tr><td>AB AB AB AB AB AB AB AB AB AB .60 .60 .00</td></tr><tr><td>x（2)</td></tr><tr><td> AB AB AB AB AB AB AB AB AB AB .60 .70 -.10</td></tr></table>

Now that we have the $^ b$ test sets, providing a sampling distribution, we can do statistics on how often $A$ has an accidental advantage. There are various ways to compute this advantage; here we follow the version laid out in Berg-Kirkpatrick et al. (2012). Assuming $H _ { 0 }$ (A isn’t better than $B$ ), we would expect that $\delta ( X )$ , estimated over many test sets, would be zero or negative; a much higher value would be surprising, since $H _ { 0 }$ specifically assumes $A$ isn’t better than $B$ . To measure exactly how surprising our observed $\delta ( x )$ is, we would in other circumstances compute the p-value by counting over many test sets how often $\delta ( \boldsymbol { x } ^ { ( i ) } )$ exceeds the expected zero value by $\delta ( x )$ or more:

$$
\operatorname { p - v a l u e } ( x ) = \frac { 1 } { b } \sum _ { i = 1 } ^ { b } \mathbb { 1 } \left( \delta ( x ^ { ( i ) } ) - \delta ( x ) \geq 0 \right)
$$

(We use the notation $\mathbb { 1 } ( x )$ to mean $^ { * * } 1$ if $x$ is true, and 0 otherwise”.) However, although it’s generally true that the expected value of $\delta ( X )$ over many test sets, (again assuming $A$ isn’t better than $B$ ) is 0, this isn’t true for the bootstrapped test sets we created. That’s because we didn’t draw these samples from a distribution with 0 mean; we happened to create them from the original test set $x _ { \ast }$ , which happens to be biased (by .20) in favor of $A$ . So to measure how surprising is our observed $\delta ( x )$ , we actually compute the p-value by counting over many test sets how often $\delta ( \boldsymbol { x } ^ { ( i ) } )$ exceeds the expected value of $\delta ( x )$ by $\delta ( x )$ or more:

$$
\begin{array} { l } { \displaystyle \mathrm { p } { \mathrm { - v a l u e } } ( x ) ~ = ~ \frac 1 b \sum _ { i = 1 } ^ { b } \mathbb { 1 } \left( \delta ( x ^ { ( i ) } ) - \delta ( x ) \geq \delta ( x ) \right) } \\ { ~ = ~ \displaystyle \frac 1 b \sum _ { i = 1 } ^ { b } \mathbb { 1 } \left( \delta ( x ^ { ( i ) } ) \geq 2 \delta ( x ) \right) } \end{array}
$$

So if for example we have 10,000 test sets $x ^ { ( i ) }$ and a threshold of .01, and in only 47 of the test sets do we find that A is accidentally better $\delta ( x ^ { ( i ) } ) \geq 2 \delta ( x )$ , the resulting p-value of .0047 is smaller than .01, indicating that the delta we found, $\delta ( x )$ is indeed sufficiently surprising and unlikely to have happened by accident, and we can reject the null hypothesis and conclude $A$ is better than $B$ .

function BOOTSTRAP(test set $x ,$ , num of samples $^ b$ ) returns $p$ -value(x)   
Calculate $\delta ( x )$ # how much better does algorithm A do than B on $x$   
$s = 0$   
for $i = ~ 1$ to $^ b$ do for $j = 1$ to $n$ do # Draw a bootstrap sample $x ^ { ( i ) }$ of size n Select a member of $x$ at random and add it to $x ^ { ( i ) }$ Calculate $\delta ( \boldsymbol { x } ^ { ( i ) } )$ # how much better does algorithm A do than B on $x ^ { ( i ) }$ $s  s + 1$ if $\delta ( x ^ { ( i ) } ) \geq 2 \delta ( x )$   
$\begin{array} { r } { \mathbf { p } \mathbf { - v a l u e } ( x ) \approx \frac { s } { b } } \end{array}$ # on what $\%$ of the b samples did algorithm A beat expectations?   
return p-value(x) # if very few did, our observed $\delta$ is probably not accidental

The full algorithm for the bootstrap is shown in Fig. 4.9. It is given a test set $x$ , a number of samples $^ b$ , and counts the percentage of the $^ b$ bootstrap test sets in which $\delta ( x ^ { * ( i ) } ) > 2 \delta ( \bar { x } )$ . This percentage then acts as a one-sided empirical p-value.

# 4.10 Avoiding Harms in Classification

representational harms

It is important to avoid harms that may result from classifiers, harms that exist both for naive Bayes classifiers and for the other classification algorithms we introduce in later chapters.

One class of harms is representational harms (Crawford 2017, Blodgett et al. 2020), harms caused by a system that demeans a social group, for example by perpetuating negative stereotypes about them. For example Kiritchenko and Mohammad (2018) examined the performance of 200 sentiment analysis systems on pairs of sentences that were identical except for containing either a common African American first name (like Shaniqua) or a common European American first name (like Stephanie), chosen from the Caliskan et al. (2017) study discussed in Chapter 6. They found that most systems assigned lower sentiment and more negative emotion to sentences with African American names, reflecting and perpetuating stereotypes that associate African Americans with negative emotions (Popp et al., 2003).

In other tasks classifiers may lead to both representational harms and other harms, such as silencing. For example the important text classification task of toxicity detection is the task of detecting hate speech, abuse, harassment, or other kinds of toxic language. While the goal of such classifiers is to help reduce societal harm, toxicity classifiers can themselves cause harms. For example, researchers have shown that some widely used toxicity classifiers incorrectly flag as being toxic sentences that are non-toxic but simply mention identities like women (Park et al., 2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018; Dias Oliva et al., 2021), or simply use linguistic features characteristic of varieties like African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019). Such false positive errors could lead to the silencing of discourse by or about these groups.

These model problems can be caused by biases or other problems in the training data; in general, machine learning systems replicate and even amplify the biases in their training data. But these problems can also be caused by the labels (for example due to biases in the human labelers), by the resources used (like lexicons, or model components like pretrained embeddings), or even by model architecture (like what the model is trained to optimize). While the mitigation of these biases (for example by carefully considering the training data sources) is an important area of research, we currently don’t have general solutions. For this reason it’s important, when introducing any NLP model, to study these kinds of factors and make them clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for each version of a model. A model card documents a machine learning model with information like:

# model card

• training algorithms and parameters   
• training data sources, motivation, and preprocessing   
• evaluation data sources, motivation, and preprocessing   
• intended use and users   
• model performance across different demographic or other groups and environmental situations

# 4.11 Summary

This chapter introduced the naive Bayes model for classification and applied it to the text categorization task of sentiment analysis.

• Many language processing tasks can be viewed as tasks of classification.   
• Text categorization, in which an entire text is assigned a class from a finite set, includes such tasks as sentiment analysis, spam detection, language identification, and authorship attribution.   
• Sentiment analysis classifies a text as reflecting the positive or negative orientation (sentiment) that a writer expresses toward some object.   
• Naive Bayes is a generative model that makes the bag-of-words assumption (position doesn’t matter) and the conditional independence assumption (words are conditionally independent of each other given the class)   
• Naive Bayes with binarized features seems to work better for many text classification tasks.   
• Classifiers are evaluated based on precision and recall.   
• Classifiers are trained using distinct training, dev, and test sets, including the use of cross-validation in the training set.

• Statistical significance tests should be used to determine whether we can be confident that one version of a classifier is better than another. • Designers of classifiers should carefully consider harms that may be caused by the model, including its training data and other components, and report model characteristics in a model card.

# Bibliographical and Historical Notes

Multinomial naive Bayes text classification was proposed by Maron (1961) at the RAND Corporation for the task of assigning subject categories to journal abstracts. His model introduced most of the features of the modern form presented here, approximating the classification task with one-of categorization, and implementing add-δ smoothing and information-based feature selection.

The conditional independence assumptions of naive Bayes and the idea of Bayesian analysis of text seems to have arisen multiple times. The same year as Maron’s paper, Minsky (1961) proposed a naive Bayes classifier for vision and other artificial intelligence problems, and Bayesian techniques were also applied to the text classification task of authorship attribution by Mosteller and Wallace (1963). It had long been known that Alexander Hamilton, John Jay, and James Madison wrote the anonymously-published Federalist papers in 1787–1788 to persuade New York to ratify the United States Constitution. Yet although some of the 85 essays were clearly attributable to one author or another, the authorship of 12 were in dispute between Hamilton and Madison. Mosteller and Wallace (1963) trained a Bayesian probabilistic model of the writing of Hamilton and another model on the writings of Madison, then computed the maximum-likelihood author for each of the disputed essays. Naive Bayes was first applied to spam detection in Heckerman et al. (1998).

Metsis et al. (2006), Pang et al. (2002), and Wang and Manning (2012) show that using boolean attributes with multinomial naive Bayes works better than full counts. Binary multinomial naive Bayes is sometimes confused with another variant of naive Bayes that also uses a binary representation of whether a term occurs in a document: Multivariate Bernoulli naive Bayes. The Bernoulli variant instead estimates $P ( w | c )$ as the fraction of documents that contain a term, and includes a probability for whether a term is not in a document. McCallum and Nigam (1998) and Wang and Manning (2012) show that the multivariate Bernoulli variant of naive Bayes doesn’t work as well as the multinomial algorithm for sentiment or other text tasks.

There are a variety of sources covering the many kinds of text classification tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012). Stamatatos (2009) surveys authorship attribute algorithms. On language identification see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural system. The task of newswire indexing was often used as a test case for text classification algorithms, based on the Reuters-21578 collection of newswire articles.

See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classification; classification in general is covered in machine learning textbooks (Hastie et al. 2001, Witten and Frank 2005, Bishop 2006, Murphy 2012).

Non-parametric methods for computing statistical significance were used first in NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work has focused on issues including multiple test sets and multiple metrics (Søgaard et al.

# 2014, Dror et al. 2017).

Feature selection is a method of removing features that are unlikely to generalize well. Features are generally ranked by how informative they are about the classification decision. A very common metric, information gain, tells us how many bits of information the presence of the word gives us for guessing the class. Other feature selection metrics include $\chi ^ { 2 }$ , pointwise mutual information, and GINI index; see Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an introduction to feature selection.

# Exercises

4.1 Assume the following likelihoods for each word being part of a positive or negative movie review, and equal prior probabilities for each class.

<table><tr><td></td><td>pos</td><td>neg</td></tr><tr><td>I</td><td>0.09</td><td>0.16</td></tr><tr><td>always</td><td>0.07</td><td>0.06</td></tr><tr><td>like</td><td>0.29</td><td>0.06</td></tr><tr><td>foreign</td><td>0.04</td><td>0.15</td></tr><tr><td>films</td><td>0.08</td><td>0.11</td></tr></table>

What class will Naive bayes assign to the sentence “I always like foreign films.”?

4.2 Given the following short movie reviews, each labeled with a genre, either comedy or action:

1. fun, couple, love, love comedy   
2. fast, furious, shoot action   
3. couple, fly, fast, fun, fun comedy   
4. furious, shoot, shoot, fun action   
5. fly, fast, shoot, love action

and a new document D:

fast, couple, shoot, fly compute the most likely class for D. Assume a naive Bayes classifier and use add-1 smoothing for the likelihoods.

4.3 Train two models, multinomial naive Bayes and binarized naive Bayes, both with add-1 smoothing, on the following document counts for key sentiment words, with positive or negative class assigned as noted.

<table><tr><td>doc</td><td></td><td></td><td>“good”“poor”“great”(class)</td><td></td></tr><tr><td>d1.</td><td>3</td><td>0</td><td>3</td><td>pos</td></tr><tr><td>d2.</td><td>0</td><td>1</td><td>2</td><td>pos</td></tr><tr><td>d3.</td><td>1</td><td>3</td><td>0</td><td>neg</td></tr><tr><td>d4.</td><td>1</td><td>5</td><td>2</td><td>neg</td></tr><tr><td>d5.</td><td>0</td><td>2</td><td>0</td><td>neg</td></tr></table>

Use both naive Bayes models to assign a class (pos or neg) to this sentence:

A good, good plot and great characters, but poor acting.

Recall from page 61 that with naive Bayes text classification, we simply ignore (throw out) any word that never occurred in the training document. (We don’t throw out words that appear in some classes but not others; that’s what add-one smoothing is for.) Do the two models agree or disagree?

# CHAPTER Logistic Regression

“And how do you know that these fine begonias are not of equal importance?” Hercule Poirot, in Agatha Christie’s The Mysterious Affair at Styles

Detective stories are as littered with clues as texts are with words. Yet for the poor reader it can be challenging to know how to weigh the author’s clues in order to make the crucial classification task: deciding whodunnit.

In this chapter we introduce an algorithm that is admirably suited for discovering the link between features or clues and some particular outcome: logistic regression. Indeed, logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks. As we will see in Chapter 7, a neural network can be viewed as a series of logistic regression classifiers stacked on top of each other. Thus the classification and machine learning techniques introduced here will play an important role throughout the book.

Logistic regression can be used to classify an observation into one of two classes (like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes. Because the mathematics for the two-class case is simpler, we’ll describe this special case of logistic regression first in the next few sections, and then briefly summarize the use of multinomial logistic regression for more than two classes in Section 5.3.

We’ll introduce the mathematics of logistic regression in the next few sections. But let’s begin with some high-level issues.

Generative and Discriminative Classifiers: The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier.

These are two very different frameworks for how to build a machine learning model. Consider a visual metaphor: imagine we’re trying to distinguish dog images from cat images. A generative model would have the goal of understanding what dogs look like and what cats look like. You might literally ask such a model to ‘generate’, i.e., draw, a dog. Given a test image, the system then asks whether it’s the cat model or the dog model that better fits (is less surprised by) the image, and chooses that as its label.

![## Image Analysis: 2456159405e30574cbc5eb6ca46055bd2728fd417a0479931613a04a05d71f5a.jpg

**Conceptual Understanding:**
This image conceptually represents a domestic cat. Its main purpose is to showcase the animal's physical characteristics, particularly its facial features and fur patterns, in a direct and engaging manner. The image communicates the visual detail and presence of the cat, with a focused gaze that suggests alertness or direct engagement with the viewer.

**Content Interpretation:**
This image presents a detailed close-up of a cat's face. The content is purely visual, depicting the cat's facial features, fur pattern, and eye color. There are no processes, concepts, relationships, or systems being shown; it is a direct photographic representation.

**Key Insights:**
The main takeaway from this image is a detailed visual understanding of a specific cat's facial appearance. No academic, technical, or research-oriented knowledge can be extracted, as the image lacks any textual information, data, diagrams, or conceptual representations relevant to a document on 'Logistic Regression'. It serves solely as a visual depiction of an animal.

**Document Context:**
The provided document context is 'CHAPTER Logistic Regression'. Given that this image is a photograph of a cat and contains no textual or diagrammatic information, its relevance to a chapter on 'Logistic Regression' is highly questionable. It appears to be an unrelated image, possibly a placeholder, an accidental inclusion, or a visual break with no direct academic or technical connection to the topic of logistic regression. It does not enhance document comprehension regarding the stated academic subject matter.

**Summary:**
The image is a close-up, head-on portrait of a domestic cat. The cat has thick, brindled fur, predominantly in shades of dark brown and golden-brown, forming distinct tabby stripes across its forehead, cheeks, and around its eyes. Its eyes are a striking green-amber color, intensely focused and looking directly forward. Prominent white whiskers extend outwards from both sides of its muzzle. The cat's nose is a soft pinkish-brown. The background is a solid, warm orange color, providing a strong contrast to the cat's fur. There are no textual elements, labels, or annotations present anywhere in the image.](images/2456159405e30574cbc5eb6ca46055bd2728fd417a0479931613a04a05d71f5a.jpg)

![## Image Analysis: 432217383836e29ff60d225752b8694ac47e805d0d244a07a026dce37469f350.jpg

**Conceptual Understanding:**
The image conceptually represents a domestic animal, specifically a beagle dog. The main purpose conveyed by the image is to visually present this particular dog. There are no key ideas or complex concepts communicated beyond the simple visual presence of the animal. It is a straightforward photograph of a pet, lacking any abstract or symbolic meaning without further context.

**Content Interpretation:**
The image captures a static portrait of a beagle dog. No processes, concepts, relationships, or systems are depicted beyond the visual representation of the animal itself. There is no data, trends, or specific information presented for interpretation, nor are there any text elements to support further analysis. The significance is purely visual, illustrating the appearance of a beagle. The image solely presents a visual depiction of an animal, without any associated processes, diagrams, or textual information.

**Key Insights:**
The primary knowledge extracted from this image is the visual identification of a beagle dog. There are no processes, patterns, relationships, or insights related to the academic or technical domain provided by the image. The image offers a visual example of a specific dog breed, but no further information, data, or concepts are presented that could lead to broader conclusions or takeaways within an academic context. There are no textual elements to support any deeper insights related to the document's stated topic.

**Document Context:**
Given the document context 'CHAPTER Logistic Regression,' this image of a dog appears to be entirely irrelevant and out of place. Logistic regression is a statistical model used for binary classification, and an image of a pet dog has no direct or apparent connection to this academic subject matter. The image does not contribute to understanding the concepts, methods, or applications typically discussed in a chapter on logistic regression.

**Summary:**
The image displays a close-up, side profile of a beagle dog. The dog is looking towards the left side of the frame with a focused, gentle expression. Its fur is a mix of tan, white, and dark brown, characteristic of the breed, with prominent tan markings on its head and ears, white on its muzzle and chest, and darker brown on its back. The dog is wearing a black collar with a metal buckle and what appears to be a small, dark tag or charm hanging from it. The background is softly blurred, showing hints of green foliage and a lighter, possibly paved, area, suggesting an outdoor setting. There is no discernible text, process flow, or diagrammatic elements present in this image. Given the document context of 'CHAPTER Logistic Regression,' the image of a dog appears to be entirely unrelated to the surrounding academic content.](images/432217383836e29ff60d225752b8694ac47e805d0d244a07a026dce37469f350.jpg)

A discriminative model, by contrast, is only trying to learn to distinguish the classes (perhaps without learning much about them). So maybe all the dogs in the training data are wearing collars and the cats aren’t. If that one feature neatly separates the classes, the model is satisfied. If you ask such a model what it knows about cats all it can say is that they don’t wear collars.

More formally, recall that the naive Bayes assigns a class $c$ to a document $d$ not by directly computing $P ( c | d )$ but by computing a likelihood and a prior

$$
{ \hat { c } } = \underset { c \in C } { \operatorname { a r g m a x } } \ \overbrace { P ( d | c ) } ^ { ( \bullet \bullet \qquad \widehat { P ( c ) }  } \ \overbrace { P ( c ) } ^ {  \widehat { } }
$$

discriminative model

A generative model like naive Bayes makes use of this likelihood term, which expresses how to generate the features of a document if we knew it was of class $c$ .

By contrast a discriminative model in this text categorization scenario attempts to directly compute $P ( c | d )$ . Perhaps it will learn to assign a high weight to document features that directly improve its ability to discriminate between possible classes, even if it couldn’t generate an example of one of the classes.

Components of a probabilistic machine learning classifier: Like naive Bayes, logistic regression is a probabilistic classifier that makes use of supervised machine learning. Machine learning classifiers require a training corpus of $m$ input/output pairs $( \bar { x ^ { ( i ) } } , y ^ { ( i ) } )$ . (We’ll use superscripts in parentheses to refer to individual instances in the training set—for sentiment classification each instance might be an individual document to be classified.) A machine learning system for classification then has four components:

1. A feature representation of the input. For each input observation $x ^ { ( i ) }$ , this will be a vector of features $\left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \right]$ . We will generally refer to feature $i$ for input $x ^ { ( j ) }$ as $x _ { i } ^ { ( j ) }$ , sometimes simplified as $x _ { i }$ , but we will also see the notation $f _ { i } , f _ { i } ( x )$ , or, for multiclass classification, $f _ { i } ( c , x )$ .   
2. A classification function that computes $\hat { y }$ , the estimated class, via $p ( y | x )$ . In the next section we will introduce the sigmoid and softmax tools for classification.   
3. An objective function that we want to optimize for learning, usually involving minimizing a loss function corresponding to error on training examples. We will introduce the cross-entropy loss function.   
4. An algorithm for optimizing the objective function. We introduce the stochastic gradient descent algorithm.

Logistic regression has two phases:

training: We train the system (specifically the weights $w$ and $^ b$ , introduced below) using stochastic gradient descent and the cross-entropy loss. test: Given a test example $x$ we compute $p ( y | x )$ and return the higher probability label $y = 1$ or $y = 0$ .

# 5.1 The sigmoid function

The goal of binary logistic regression is to train a classifier that can make a binary decision about the class of a new input observation. Here we introduce the sigmoid classifier that will help us make this decision.

Consider a single input observation $x _ { \ast }$ , which we will represent by a vector of features $\left[ x _ { 1 } , x _ { 2 } , . . . , x _ { n } \right]$ . (We’ll show sample features in the next subsection.) The classifier output $y$ can be 1 (meaning the observation is a member of the class) or 0 (the observation is not a member of the class). We want to know the probability

$P ( y = 1 | x )$ that this observation is a member of the class. So perhaps the decision is “positive sentiment” versus “negative sentiment”, the features represent counts of words in a document, $P ( y = 1 | x )$ is the probability that the document has positive sentiment, and $P ( y = 0 | x )$ is the probability that the document has negative sentiment.

Logistic regression solves this task by learning, from a training set, a vector of weights and a bias term. Each weight $w _ { i }$ is a real number, and is associated with one of the input features $x _ { i }$ . The weight $w _ { i }$ represents how important that input feature is to the classification decision, and can be positive (providing evidence that the instance being classified belongs in the positive class) or negative (providing evidence that the instance being classified belongs in the negative class). Thus we might expect in a sentiment task the word awesome to have a high positive weight, and abysmal to have a very negative weight. The bias term, also called the intercept, is another real number that’s added to the weighted inputs.

To make a decision on a test instance—after we’ve learned the weights in training— the classifier first multiplies each $x _ { i }$ by its weight $w _ { i }$ , sums up the weighted features, and adds the bias term $^ b$ . The resulting single number $z$ expresses the weighted sum of the evidence for the class.

$$
z ~ = ~ \left( \sum _ { i = 1 } ^ { n } w _ { i } x _ { i } \right) + b
$$

In the rest of the book we’ll represent such sums using the dot product notation from linear algebra. The dot product of two vectors a and $\mathbf { b }$ , written as $\mathbf { a } \cdot \mathbf { b }$ , is the sum of the products of the corresponding elements of each vector. (Notice that we represent vectors using the boldface notation b). Thus the following is an equivalent formation to Eq. 5.2:

$$
z = { \pmb w } \cdot { \pmb x } + b
$$

But note that nothing in Eq. 5.3 forces $z$ to be a legal probability, that is, to lie between 0 and 1. In fact, since weights are real-valued, the output might even be negative; z ranges from $- \infty$ to $\infty$ .

![## Image Analysis: 7c62b7153b838784c4a0ef5632a814750cae577d5709f65a7f163176256d3ec9.jpg

**Conceptual Understanding:**
The image conceptually represents the sigmoid (or logistic) function, a mathematical function that takes any real number as input and outputs a value between 0 and 1. Its main purpose is to visualize how this function transforms inputs, demonstrating its characteristic S-shape, its range, and its behavior across different input values. It illustrates that the function acts as a 'squashing' or 'normalization' function, mapping extreme inputs towards its asymptotic limits of 0 and 1, while providing a steeper gradient for inputs closer to the center (around 0).

**Content Interpretation:**
The image shows the graphical representation of the sigmoid function, σ(z) = 1 / (1 + e^(-z)). This function is a common activation function in neural networks and logistic regression. The graph illustrates how the function takes any real-valued input 'z' and transforms it into an output 'σ(z)' that lies strictly within the range (0, 1). The S-shaped curve demonstrates the function's behavior: it has a very small gradient (flattens out) for extreme positive or negative values of 'z', indicating that large positive inputs are mapped close to 1, and large negative inputs are mapped close to 0. Conversely, around z=0, the curve is steepest, showing that small changes in 'z' in this region result in more significant changes in 'σ(z)'. The point (0, 0.5) is the inflection point where the function's output is exactly 0.5. The smooth, monotonic increase of the function from 0 to 1 is clearly depicted.

**Key Insights:**
1.  **Mapping Real Values to (0,1) Range:** The graph explicitly shows the sigmoid function, σ(z) = 1 / (1 + e^(-z)), mapping all input 'z' values (from -8 to 8 on the x-axis) to output 'σ(z)' values strictly between 0 and 1 on the y-axis (from 0.0 to 1.0). For example, at z = -8, σ(z) is very close to 0, and at z = 8, σ(z) is very close to 1. This confirms the function's primary role as a squashing or normalization function. 
2.  **Non-Linearity and Saturation:** For large positive values of 'z' (e.g., z > 4), the curve flattens out, and σ(z) approaches 1.0. Similarly, for large negative values of 'z' (e.g., z < -4), the curve flattens out, and σ(z) approaches 0.0. This 'squashing' behavior, where extreme inputs yield outputs very close to the limits, is a key characteristic of the sigmoid function, as highlighted by the document text 'outlier values get squashed toward 0 or 1'. 
3.  **Linearity Around Zero:** Around z = 0, the curve is steepest. At z = 0, σ(z) = 0.5, and the curve shows a relatively linear increase in this central region. This supports the document's statement that the function 'is nearly linear around 0', indicating that small changes in 'z' around zero result in proportionally larger changes in 'σ(z)' compared to the tails.

**Document Context:**
This image is presented in Section 5.1 of a document, specifically discussing 'The sigmoid function'. The text following the image explicitly states, 'Figure 5.1 The sigmoid function σ(z) = 1 / (1 + e^(-z)) takes a real value and maps it to the range (0, 1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.' The graph directly visualizes this description, showing the S-shaped curve that maps 'z' values to 'σ(z)' values between 0 and 1. The visual representation of the curve's flattening at the extremes and its steepness around z=0 perfectly illustrates the textual explanation of how outlier values are 'squashed' towards 0 or 1 and how it is 'nearly linear around 0'. This image serves as a fundamental visual aid to understand the behavior and properties of the sigmoid function, which is critical for comprehending its role in machine learning models.

**Summary:**
The image displays a graph of the sigmoid function, mathematically defined as σ(z) = 1 / (1 + e^(-z)). The horizontal axis is labeled 'z' and ranges from -8 to 8, with tick marks at -8, -6, -4, -2, 0, 2, 4, 6, and 8. The vertical axis is labeled 'σ(z)' and ranges from 0.0 to 1.0, with tick marks at 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0. The blue curve represents the sigmoid function. It starts near 0.0 for large negative 'z' values, gradually increases, passes through 0.5 at z=0, and approaches 1.0 for large positive 'z' values. The curve demonstrates a characteristic S-shape, showing a steep incline around z=0, and flattening out towards the asymptotic limits of 0 and 1 as 'z' moves towards negative and positive infinity, respectively. The function equation, σ(z) = 1/(1 + e^(-z)), is prominently displayed on the graph itself, aligning with the visual representation of the curve.](images/7c62b7153b838784c4a0ef5632a814750cae577d5709f65a7f163176256d3ec9.jpg)
Figure 5.1 The sigmoid function $\begin{array} { r } { \sigma ( z ) = \frac { 1 } { 1 + e ^ { - z } } } \end{array}$ takes a real value and maps it to the range $( 0 , 1 )$ . It is nearly linear around 0 but outlier values get squashed toward 0 or 1.

To create a probability, we’ll pass $z$ through the sigmoid function, $\sigma ( z )$ . The sigmoid function (named because it looks like an $s$ ) is also called the logistic function, and gives logistic regression its name. The sigmoid has the following equation, shown graphically in Fig. 5.1:

$$
\sigma ( z ) = \frac { 1 } { 1 + e ^ { - z } } = \frac { 1 } { 1 + \exp \left( - z \right) }
$$

(For the rest of the book, we’ll use the notation $\exp ( x )$ to mean $e ^ { x }$ .) The sigmoid has a number of advantages; it takes a real-valued number and maps it into the range $( 0 , 1 )$ , which is just what we want for a probability. Because it is nearly linear around 0 but flattens toward the ends, it tends to squash outlier values toward 0 or 1. And it’s differentiable, which as we’ll see in Section 5.10 will be handy for learning.

We’re almost there. If we apply the sigmoid to the sum of the weighted features, we get a number between 0 and 1. To make it a probability, we just need to make sure that the two cases, $p ( y = 1 )$ and $p ( y = 0 )$ , sum to 1. We can do this as follows:

$$
\begin{array} { l } { P ( y = 1 ) ~ = ~ \sigma ( { \mathbf w } \cdot { \mathbf x } + b ) } \\ { ~ = ~ { \cfrac { 1 } { 1 + \exp ( - ( { \mathbf w } \cdot { \mathbf x } + b ) ) } } } \\ { P ( y = 0 ) ~ = ~ 1 - \sigma ( { \mathbf w } \cdot { \mathbf x } + b ) } \\ { ~ = ~ 1 - { \cfrac { 1 } { 1 + \exp ( - ( { \mathbf w } \cdot { \mathbf x } + b ) ) } } } \\ { ~ = ~ { \cfrac { \exp ( - ( { \mathbf w } \cdot { \mathbf x } + b ) ) } { 1 + \exp ( - ( { \mathbf w } \cdot { \mathbf x } + b ) ) } } } \end{array}
$$

The sigmoid function has the property

$$
1 - \sigma ( x ) = \sigma ( - x )
$$

so we could also have expressed $P ( y = 0 )$ as $\pmb { \sigma } ( - ( \mathbf { w } \cdot \mathbf { x } + b ) )$ .

logit

Finally, one terminological point. The input to the sigmoid function, the score ${ z = { \mathbf { w } } \cdot { \mathbf { x } } + b }$ from Eq. 5.3, is often called the logit. This is because the logit function is the inverse of the sigmoid. The logit function is the log of the odds ratio $\frac { p } { 1 - p }$ :

$$
\operatorname { l o g i t } ( p ) = \sigma ^ { - 1 } ( p ) = \ln { \frac { p } { 1 - p } }
$$

Using the term logit for $z$ is a way of reminding us that by using the sigmoid to turn $z$ (which ranges from $- \infty$ to $\infty$ ) into a probability, we are implicitly interpreting $z$ as not just any real-valued number, but as specifically a log odds.

# 5.2 Classification with Logistic Regression

The sigmoid function from the prior section thus gives us a way to take an instance $x$ and compute the probability $P ( y = 1 | x )$ .

How do we make a decision about which class to apply to a test instance $x ?$ For a given $x _ { \ast }$ , we say yes if the probability $P ( y = 1 | x )$ is more than .5, and no otherwise. We call .5 the decision boundary:

$$
\mathrm { d e c i s i o n } ( x ) ~ = ~ \left\{ \begin{array} { l l } { 1 \mathrm { i f } P ( y = 1 | x ) > 0 . 5 } \\ { 0 \mathrm { o t h e r w i s e } } \end{array} \right.
$$

Let’s have some examples of applying logistic regression as a classifier for language tasks.

# 5.2.1 Sentiment Classification

Suppose we are doing binary sentiment classification on movie review text, and we would like to know whether to assign the sentiment class $^ +$ or to a review document doc. We’ll represent each input observation by the 6 features $x _ { 1 } \ldots x _ { 6 }$ of the input shown in the following table; Fig. 5.2 shows the features in a sample mini test document.

<table><tr><td>Var</td><td>Definition</td><td>Value in Fig. 5.2</td></tr><tr><td>X1</td><td>count(positive lexicon words ∈ doc)</td><td>3</td></tr><tr><td>x2</td><td>count(negative lexicon words E doc)</td><td>2</td></tr><tr><td>x3</td><td>{1 if “no”∈doc {0 otherwise</td><td>1</td></tr><tr><td>X4</td><td>count(1st and 2nd pronouns E doc)</td><td>3</td></tr><tr><td>X5</td><td>{1 if“!&quot;∈doc 亻0otherwise</td><td>0</td></tr><tr><td>X6</td><td>ln(word count of doc)</td><td>ln(66) = 4.19</td></tr></table>

![## Image Analysis: 1fdcedcb072231653bb7b086fed57b95daebde0b216a240649f1893f1600899e.jpg

**Conceptual Understanding:**
This image conceptually represents the **feature extraction stage in Natural Language Processing (NLP)**, specifically as applied to **sentiment analysis**. Its main purpose is to visually demonstrate how a raw text document (a customer review, in this case) is analyzed to identify key words and phrases, which are then transformed into a numerical feature vector. The image conveys the idea that subjective textual information can be systematically converted into a structured, quantifiable format (`x_i` variables with assigned values) that can be processed by machine learning algorithms for tasks like determining the overall sentiment of the text. It highlights the bridge between human language and computational interpretation for sentiment classification.

**Content Interpretation:**
The image illustrates the process of feature extraction from a text document. It shows how specific words and phrases are identified and mapped to numerical features, represented as `x_i` variables with associated values. This demonstrates the conversion of qualitative textual data into quantitative features suitable for computational analysis, specifically for tasks like sentiment classification.

Key elements contributing to this interpretation:
- **Textual Content:** The core input is a sample review: "It's chokey. There are virtually no surprises, and the writing is second-rate. So why was it so enjoyable? For one thing, the cast is great. Another nice touch is the music I was overcome with the urge to get off the couch and start dancing. It sucked me in, and it'll do the same to you."
- **Highlighted Sentiment Indicators:** Words and phrases such as "chokey," "no surprises," "second-rate" (negative indicators) and "enjoyable," "great," "nice" (positive indicators) are highlighted, suggesting their importance in determining sentiment.
- **Feature Variables and Values:** Each highlighted element is linked to a variable `x_i` with a specific numerical value (e.g., `x_2=2`, `x_3=1`, `x_1=3`, `x_5=0`, `x_6=4.19`, `x_4=3`). These represent the extracted features, where the values could signify counts, sentiment scores, or weights.
- **Many-to-One Mapping:** Multiple words can map to the same feature (e.g., "enjoyable" and "great" both map to `x_1=3`; "second-rate," "me," and "you" map to `x_4=3`), indicating that different terms can contribute to a common semantic or sentiment feature.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Textual data is quantifiable:** The image clearly demonstrates that subjective natural language can be converted into discrete, measurable numerical features for computational analysis.
2.  **Specific lexical units are key features:** Individual words and phrases (like "chokey", "enjoyable", "second-rate") are identified as critical components for extracting information, particularly sentiment.
3.  **Numerical representation is fundamental for ML:** The assignment of numerical values (`x_i = value`) to these textual features illustrates the creation of a feature vector, which is the standard input format for machine learning models.
4.  **Feature values can represent different aspects:** The varying numerical values (e.g., `0`, `1`, `2`, `3`, `4.19`) suggest that features can encapsulate different types of information, such as presence, count, intensity, or a specific weighted score. For example, `x_5=0` for "nice" might imply a zero count or a negligible weight in a specific feature's context.
5.  **Different terms can contribute to the same feature:** The fact that "enjoyable" and "great" both map to `x_1=3`, and "second-rate," "me," and "you" map to `x_4=3`, indicates that a feature is not necessarily tied to a single word but can aggregate contributions from multiple related terms, implying a more generalized semantic or sentiment category.

**Evidence from Textual Elements:**
- The sample text provides the raw data.
- The highlighted words and phrases such as "chokey," "no," "second-rate," "enjoyable," "great," "nice," "I," "me," and "you" are the direct evidence of feature identification.
- The explicit assignments (`x_2=2.`, `x_3=1`, `x_1=3`, `x_5=0`, `x_6=4.19`, `x_4=3`) are the evidence for the numerical quantification and the resulting feature vector components. The repeated `x_1=3` and `x_4=3` assignments for multiple words substantiate the insight about different terms contributing to the same feature.

**Document Context:**
This image is directly relevant to the "5.2.1 Sentiment Classification" section of the document. As indicated by the surrounding text, "Figure 5.2 A sample mini test document showing the extracted features in the vector $x$," the image serves as a visual example of the initial, crucial step in sentiment classification: transforming raw text into a numerical feature vector. It demonstrates how a qualitative piece of text (the mini test document) is processed to extract quantifiable features (represented by `x_i` and their values) that a machine learning model can use to classify its sentiment. Thus, it grounds the theoretical discussion of sentiment classification in a practical, illustrative example of feature engineering.

**Summary:**
This image illustrates the process of feature extraction from a sample text document for sentiment classification. The image displays a multi-sentence review, where specific words and phrases are highlighted and linked to numerical feature variables (x_i) with assigned values. 

The document content is: "It's chokey. There are virtually no surprises, and the writing is second-rate. So why was it so enjoyable? For one thing, the cast is great. Another nice touch is the music I was overcome with the urge to get off the couch and start dancing. It sucked me in, and it'll do the same to you."

Each highlighted word or phrase is shown to contribute to a specific feature:
- The word "chokey" is linked to feature `x_2` with a value of `2`.
- The word "no" (from "no surprises") is linked to feature `x_3` with a value of `1`.
- The words "enjoyable" and "great" are both linked to feature `x_1` with a value of `3`.
- The word "nice" is linked to feature `x_5` with a value of `0`.
- The word "I" (from "music I was overcome") is linked to feature `x_6` with a value of `4.19`.
- The phrase "second-rate", and the words "me" (from "sucked me in") and "you" (from "to you"), are all linked to feature `x_4` with a value of `3`.

This visualization demonstrates how raw textual data, which is subjective and qualitative, is transformed into a structured, quantifiable format (a feature vector `x`). This numerical representation is crucial for machine learning algorithms to process and understand the sentiment expressed in the text, forming the basis for tasks like sentiment classification. The different `x_i` values represent components of this feature vector, which would then be used as input for a classifier.](images/1fdcedcb072231653bb7b086fed57b95daebde0b216a240649f1893f1600899e.jpg)
Figure 5.2 A sample mini test document showing the extracted features in the vector $x$ .

Let’s assume for the moment that we’ve already learned a real-valued weight for each of these features, and that the 6 weights corresponding to the 6 features are [2.5 $, - 5 . 0 , - 1 . 2 , 0 . 5 , 2 . 0 , 0 . 7 ]$ , while $b = 0 . 1$ . (We’ll discuss in the next section how the weights are learned.) The weight $w _ { 1 }$ , for example indicates how important a feature the number of positive lexicon words (great, nice, enjoyable, etc.) is to a positive sentiment decision, while $w _ { 2 }$ tells us the importance of negative lexicon words. Note that $w _ { 1 } = 2 . 5$ is positive, while $w _ { 2 } = - 5 . 0$ , meaning that negative words are negatively associated with a positive sentiment decision, and are about twice as important as positive words.

Given these 6 features and the input review $x$ , $P ( + | x )$ and $P ( - | x )$ can be computed using Eq. 5.5:

$$
\begin{array} { l c l } { p ( + | x ) = P ( y = 1 | x ) } & { = \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) } \\ & { = \sigma ( [ 2 . 5 , - 5 . 0 , - 1 . 2 , 0 . 5 , 2 . 0 , 0 . 7 ] \cdot [ 3 , 2 , 1 , 3 , 0 , 4 . 1 9 ] + 0 . 1 ) } \\ & { = \sigma ( . 8 3 3 ) } \\ & { = } & { 0 . 7 0 } \\ { p ( - | x ) = P ( y = 0 | x ) } & { = } & { 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) } \\ & { = } & { 0 . 3 0 } \end{array}
$$

# 5.2.2 Other classification tasks and features

feature templates

Logistic regression is applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes, EOS (end-of-sentence) and not-EOS. We might use features like $x _ { 1 }$ below expressing that the current word is lower case, perhaps with a positive weight. Or a feature expressing that the current word is in our abbreviations dictionary (“Prof.”), perhaps with a negative weight. A feature can also express a combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is $S t .$ and the previous word is capitalized then the period is likely part of a shortening of the word street following a street name.

feature interactions

$$
\begin{array} { r l } & { x _ { 1 } \ = \ \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } \ \cdots { } C a s e ( w _ { i } ) = \mathrm { L o w e r } ^ { , } , } \\ { 0 } & { \mathrm { o t h e r w i s e } } \end{array} \right. } \\ & { x _ { 2 } \ = \ \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } \ \cdots w _ { i } \in \mathrm { A c r o n y m D i c t } ^ { , } , } \\ { 0 } & { \mathrm { o t h e r w i s e } } \end{array} \right. } \\ & { x _ { 3 } \ = \ \left\{ \begin{array} { l l } { 1 } & { \mathrm { i f } \ \cdots w _ { i } = \mathrm { S t . } \ \& { } C a s e ( w _ { i - 1 } ) = \mathrm { U p p e r } ^ { , } , } \\ { 0 } & { \mathrm { o t h e r w i s e } } \end{array} \right. } \end{array}
$$

Designing versus learning features: In classic models, features are designed by hand by examining the training set with an eye to linguistic intuitions and literature, supplemented by insights from error analysis on the training set of an early version of a system. We can also consider (feature interactions), complex features that are combinations of more primitive features. We saw such a feature for period disambiguation above, where a period on the word $S t .$ was less likely to be the end of the sentence if the previous word was capitalized. Features can be created automatically via feature templates, abstract specifications of features. For example a bigram template for period disambiguation might create a feature for every pair of words that occurs before a period in the training set. Thus the feature space is sparse, since we only have to create a feature if that n-gram exists in that position in the training set. The feature is generally created as a hash from the string descriptions. A user description of a feature as, “bigram(American breakfast)” is hashed into a unique integer i that becomes the feature number $f _ { i }$ .

It should be clear from the prior paragraph that designing features by hand requires extensive human effort. For this reason, recent NLP systems avoid handdesigned features and instead focus on representation learning: ways to learn features automatically in an unsupervised way from the input. We’ll introduce methods for representation learning in Chapter 6 and Chapter 7.

Scaling input features: When different input features have extremely different ranges of values, it’s common to rescale them so they have comparable ranges. We standardize input values by centering them to result in a zero mean and a standard deviation of one (this transformation is sometimes called the $\mathbf { z }$ -score). That is, if $\mu _ { i }$ is the mean of the values of feature $x _ { i }$ across the $m$ observations in the input dataset, and $\sigma _ { i }$ is the standard deviation of the values of features $x _ { i }$ across the input dataset, we can replace each feature $x _ { i }$ by a new feature $x _ { i } ^ { \prime }$ computed as follows:

$$
\begin{array} { c c c } { \displaystyle \mu _ { i } = \frac { 1 } { m } \sum _ { j = 1 } ^ { m } x _ { i } ^ { ( j ) } } & { \displaystyle \sigma _ { i } = \sqrt { \frac { 1 } { m } \sum _ { j = 1 } ^ { m } \left( x _ { i } ^ { ( j ) } - \mu _ { i } \right) ^ { 2 } } } \\ { \displaystyle x _ { i } ^ { \prime } ~ = ~ \frac { x _ { i } - \mu _ { i } } { \sigma _ { i } } } & { } \end{array}
$$

Alternatively, we can normalize the input features values to lie between 0 and 1:

$$
x _ { i } ^ { \prime } = { \frac { x _ { i } - \operatorname* { m i n } ( x _ { i } ) } { \operatorname* { m a x } ( x _ { i } ) - \operatorname* { m i n } ( x _ { i } ) } }
$$

Having input data with comparable range is useful when comparing values across features. Data scaling is especially important in large neural networks, since it helps speed up gradient descent.

# 5.2.3 Processing many examples at once

We’ve shown the equations for logistic regression for a single example. But in practice we’ll of course want to process an entire test set with many examples. Let’s suppose we have a test set consisting of $m$ test examples each of which we’d like to classify. We’ll continue to use the notation from page 78, in which a superscript value in parentheses refers to the example index in some set of data (either for training or for test). So in this case each test example $x ^ { ( i ) }$ has a feature vector $\mathbf { x } ^ { ( i ) }$ , $1 \leq i \leq m$ . (As usual, we’ll represent vectors and matrices in bold.)

One way to compute each output value $\hat { y } ^ { ( i ) }$ is just to have a for-loop, and compute each test example one at a time:

$$
\begin{array} { r l } { \mathbf { f o r e a c h } } & { x ^ { ( i ) } \mathrm { i n i n p u t } [ x ^ { ( 1 ) } , x ^ { ( 2 ) } , . . . , x ^ { ( m ) } ] } \\ & { y ^ { ( i ) } = \sigma ( \mathbf { w } \cdot \mathbf { x } ^ { ( \mathfrak { i } ) } + b ) } \end{array}
$$

For the first 3 test examples, then, we would be separately computing the predicted $\hat { y } ^ { ( i ) }$ as follows:

$$
\begin{array} { r l } { P ( y ^ { ( 1 ) } = 1 | x ^ { ( 1 ) } ) ~ = ~ \sigma ( \mathbf { w } \cdot \mathbf { x } ^ { ( 1 ) } + b ) ~ } & { } \\ { P ( y ^ { ( 2 ) } = 1 | x ^ { ( 2 ) } ) ~ = ~ \sigma ( \mathbf { w } \cdot \mathbf { x } ^ { ( 2 ) } + b ) ~ } & { } \\ { P ( y ^ { ( 3 ) } = 1 | x ^ { ( 3 ) } ) ~ = ~ \sigma ( \mathbf { w } \cdot \mathbf { x } ^ { ( 3 ) } + b ) ~ } & { } \end{array}
$$

But it turns out that we can slightly modify our original equation Eq. 5.5 to do this much more efficiently. We’ll use matrix arithmetic to assign a class to all the examples with one matrix operation!

First, we’ll pack all the input feature vectors for each input $x$ into a single input matrix $\pmb { \times }$ , where each row $i$ is a row vector consisting of the feature vector for input example $x ^ { ( i ) }$ (i.e., the vector $\mathbf { x } ^ { ( i ) }$ ). Assuming each example has $f$ features and weights, $\pmb { \times }$ will therefore be a matrix of shape $[ m \times f ]$ , as follows:

$$
\begin{array} { r l r } { \textsf { \bf X } } & { = } & { \left[ \begin{array} { l } { x _ { 1 } ^ { ( 1 ) } x _ { 2 } ^ { ( 1 ) } \dots x _ { f } ^ { ( 1 ) } } \\ { x _ { 1 } ^ { ( 2 ) } x _ { 2 } ^ { ( 2 ) } \dots x _ { f } ^ { ( 2 ) } } \\ { x _ { 1 } ^ { ( 3 ) } x _ { 2 } ^ { ( 3 ) } \dots x _ { f } ^ { ( 3 ) } } \\ { \dots } \end{array} \right] } \\ & { } & \end{array}
$$

Now if we introduce $\mathbf { b }$ as a vector of length $m$ which consists of the scalar bias term $^ b$ repeated $m$ times, $\boldsymbol { \mathsf { b } } = [ b , b , . . . , b ]$ , and $\pmb { \hat { y } } = [ \hat { y } ^ { ( 1 ) } , \hat { y } ^ { ( 2 ) } . . . , \hat { y } ^ { ( m ) } ]$ as the vector of outputs (one scalar $\hat { y } ^ { ( i ) }$ for each input $x ^ { ( i ) }$ and its feature vector $\mathbf { x } ^ { ( i ) }$ ), and represent the weight vector $\boldsymbol { \mathsf { W } }$ as a column vector, we can compute all the outputs with a single matrix multiplication and one addition:

$$
\bf y _ { \tau } = \bf x _ { w + \bf b }
$$

You should convince yourself that Eq. 5.13 computes the same thing as our for-loop in Eq. 5.11. For example $\hat { y } ^ { ( 1 ) }$ , the first entry of the output vector $\pmb { \ y }$ , will correctly be:

$$
\hat { y } ^ { ( 1 ) } = [ x _ { 1 } ^ { ( 1 ) } , x _ { 2 } ^ { ( 1 ) } , . . . , x _ { f } ^ { ( 1 ) } ] \cdot [ w _ { 1 } , w _ { 2 } , . . . , w _ { f } ] + b
$$

Note that we had to reorder $\pmb { \times }$ and $\boldsymbol { \mathsf { w } }$ from the order they appeared in in Eq. 5.5 to make the multiplications come out properly. Here is Eq. 5.13 again with the shapes shown:

$$
\begin{array} { c } { { \pmb { y } } \ = \ { \pmb { x } } \qquad { \pmb { w } } \quad + \quad { \pmb { \ b } } } \\ { ( { \pmb { m } } \times 1 ) \qquad ( { \pmb { m } } \times { \pmb f } ) ( { \pmb f } \times 1 ) \ ( { \pmb { m } } \times 1 ) } \end{array}
$$

Modern compilers and compute hardware can compute this matrix operation very efficiently, making the computation much faster, which becomes important when training or testing on very large datasets.

Note by the way that we could have kept $\pmb { \times }$ and $\pmb { w }$ in the original order $( \pmb { \mathsf { y } } =$ $\mathbf { \boldsymbol { x } } \mathbf { \boldsymbol { w } } + \mathbf { \boldsymbol { b } } )$ if we had chosen to define $\pmb { \times }$ differently as a matrix of column vectors, one vector for each input example, instead of row vectors, and then it would have shape $[ f \times m ]$ . But we conventionally represent inputs as rows.

# 5.2.4 Choosing a classifier

Logistic regression has a number of advantages over naive Bayes. Naive Bayes has overly strong conditional independence assumptions. Consider two features which are strongly correlated; in fact, imagine that we just add the same feature $f _ { 1 }$ twice. Naive Bayes will treat both copies of $f _ { 1 }$ as if they were separate, multiplying them both in, overestimating the evidence. By contrast, logistic regression is much more robust to correlated features; if two features $f _ { 1 }$ and $f _ { 2 }$ are perfectly correlated, regression will simply assign part of the weight to $w _ { 1 }$ and part to $w _ { 2 }$ . Thus when there are many correlated features, logistic regression will assign a more accurate probability than naive Bayes. So logistic regression generally works better on larger documents or datasets and is a common default.

Despite the less accurate probabilities, naive Bayes still often makes the correct classification decision. Furthermore, naive Bayes can work extremely well (sometimes even better than logistic regression) on very small datasets $\mathrm { N g }$ and Jordan, 2002) or short documents (Wang and Manning, 2012). Furthermore, naive Bayes is easy to implement and very fast to train (there’s no optimization step). So it’s still a reasonable approach to use in some situations.

# 5.3 Multinomial logistic regression

Sometimes we need more than two classes. Perhaps we might want to do 3-way sentiment classification (positive, negative, or neutral). Or we could be assigning some of the labels we will introduce in Chapter 17, like the part of speech of a word (choosing from 10, 30, or even 50 different parts of speech), or the named entity type of a phrase (choosing from tags like person, location, organization).

In such cases we use multinomial logistic regression, also called softmax regression (in older NLP literature you will sometimes see the name maxent classifier). In multinomial logistic regression we want to label each observation with a class $k$ from a set of $K$ classes, under the stipulation that only one of these classes is the correct one (sometimes called hard classification; an observation can not be in multiple classes). Let’s use the following representation: the output $\pmb { \ y }$ for each input $\pmb { \times }$ will be a vector of length $K$ . If class $c$ is the correct class, we’ll set $y _ { c } = 1$ , and set all the other elements of $\pmb { \ y }$ to be 0, i.e., $y _ { c } = 1$ and $y _ { j } = 0 \forall j \neq c$ . A vector like this $\pmb { y }$ , with one value $^ { = 1 }$ and the rest 0, is called a one-hot vector. The job of the classifier is to produce an estimate vector $\hat { \pmb { y } }$ . For each class $k$ , the value $\hat { y } _ { k }$ will be the classifier’s estimate of the probability $p ( y _ { k } = 1 | \mathbf { x } )$ .

# 5.3.1 Softmax

softmax

The multinomial logistic classifier uses a generalization of the sigmoid, called the softmax function, to compute $p ( y _ { k } = 1 | \mathbf { x } )$ . The softmax function takes a vector $\mathbf { z } = [ z _ { 1 } , z _ { 2 } , . . . , z _ { K } ]$ of $K$ arbitrary values and maps them to a probability distribution, with each value in the range [0,1], and all the values summing to 1. Like the sigmoid, it is an exponential function.

For a vector $\mathbf { z }$ of dimensionality $K$ , the softmax is defined as:

$$
\operatorname { s o f t m a x } ( z _ { i } ) ~ = ~ { \frac { \exp { ( z _ { i } ) } } { \sum _ { j = 1 } ^ { K } \exp { ( z _ { j } ) } } } ~ 1 \leq i \leq K
$$

The softmax of an input vector $\pmb { z } = [ z _ { 1 } , z _ { 2 } , . . . , z _ { K } ]$ is thus a vector itself:

$$
\mathrm { s o f t m a x } ( \mathbf { z } ) ~ = ~ \left[ \frac { \exp \left( z _ { 1 } \right) } { \sum _ { i = 1 } ^ { K } \exp \left( z _ { i } \right) } , \frac { \exp \left( z _ { 2 } \right) } { \sum _ { i = 1 } ^ { K } \exp \left( z _ { i } \right) } , . . . , \frac { \exp \left( z _ { K } \right) } { \sum _ { i = 1 } ^ { K } \exp \left( z _ { i } \right) } \right]
$$

The denominator $\textstyle \sum _ { i = 1 } ^ { K } \exp ( \pmb { z } _ { i } )$ is used to normalize all the values into probabilities. Thus for example given a vector:

$$
\mathbf { z } = [ 0 . 6 , 1 . 1 , - 1 . 5 , 1 . 2 , 3 . 2 , - 1 . 1 ]
$$

the resulting (rounded) softmax $( \pmb { z } )$ is

$$
\left[ 0 . 0 5 , 0 . 0 9 , 0 . 0 1 , 0 . 1 , 0 . 7 4 , 0 . 0 1 \right]
$$

Like the sigmoid, the softmax has the property of squashing values toward 0 or 1. Thus if one of the inputs is larger than the others, it will tend to push its probability toward 1, and suppress the probabilities of the smaller inputs.

Finally, note that, just as for the sigmoid, we refer to $\mathbf { z }$ , the vector of scores that is the input to the softmax, as logits (see Eq. 5.7).

# 5.3.2 Applying softmax in logistic regression

When we apply softmax for logistic regression, the input will (just as for the sigmoid) be the dot product between a weight vector $\pmb { w }$ and an input vector $\pmb { \times }$ (plus a bias). But now we’ll need separate weight vectors ${ \pmb w } _ { k }$ and bias $b _ { k }$ for each of the $K$ classes. The probability of each of our output classes $\hat { y } _ { k }$ can thus be computed as:

$$
p ( y _ { k } = 1 | { \bf x } ) ~ = ~ \frac { \exp { ( { \bf w } _ { k } \cdot { \bf x } + b _ { k } ) } } { \displaystyle \sum _ { j = 1 } ^ { K } \exp { ( { \bf w } _ { j } \cdot { \bf x } + b _ { j } ) } }
$$

The form of Eq. 5.18 makes it seem that we would compute each output separately. Instead, it’s more common to set up the equation for more efficient computation by modern vector processing hardware. We’ll do this by representing the set of $K$ weight vectors as a weight matrix $W$ and a bias vector b. Each row $k$ of $\boldsymbol { \mathsf { W } }$ corresponds to the vector of weights $w _ { k }$ . $\boldsymbol { \mathsf { W } }$ thus has shape $[ K \times f ]$ , for $K$ the number of output classes and $f$ the number of input features. The bias vector $\mathbf { b }$ has one value for each of the $K$ output classes. If we represent the weights in this way, we can compute $\hat { \mathbf { y } }$ , the vector of output probabilities for each of the $K$ classes, by a single elegant equation:

$$
\hat { \mathbf { y } } = \operatorname { s o f t m a x } ( \mathbf { W } \mathbf { x } + \mathbf { b } )
$$

If you work out the matrix arithmetic, you can see that the estimated score of the first output class $\hat { y } _ { 1 }$ (before we take the softmax) will correctly turn out to be $\pmb { w } _ { 1 } \cdot \pmb { x } + b _ { 1 }$ .

One helpful interpretation of the weight matrix $\boldsymbol { \mathsf { W } }$ is to see each row $\boldsymbol { \mathsf { w } } _ { k }$ as a prototype of class $k$ . The weight vector $\boldsymbol { \mathsf { w } } _ { k }$ that is learned represents the class as a kind of template. Since two vectors that are more similar to each other have a higher dot product with each other, the dot product acts as a similarity function. Logistic regression is thus learning an exemplar representation for each class, such that incoming vectors are assigned the class $k$ they are most similar to from the $K$ classes.

Fig. 5.3 shows the difference between binary and multinomial logistic regression by illustrating the weight vector versus weight matrix in the computation of the output class probabilities.

# 5.3.3 Features in Multinomial Logistic Regression

Features in multinomial logistic regression act like features in binary logistic regression, with the difference mentioned above that we’ll need separate weight vectors and biases for each of the $K$ classes. Recall our binary exclamation point feature $x _ { 5 }$ from page 81:

$$
x _ { 5 } ~ = ~ \left\{ { \begin{array} { l l } { 1 } & { { \mathrm { i f } } ~ \cdots { \mathrm { p } } ^ { \prime \prime } \in \mathrm { d o c } } \\ { 0 } & { { \mathrm { o t h e r w i s e } } } \end{array} } \right.
$$

In binary classification a positive weight $w _ { 5 }$ on a feature influences the classifier toward $y = 1$ (positive sentiment) and a negative weight influences it toward $y = 0$ (negative sentiment) with the absolute value indicating how important the feature is. For multinomial logistic regression, by contrast, with separate weights for each class, a feature can be evidence for or against each individual class.

In 3-way multiclass sentiment classification, for example, we must assign each document one of the 3 classes $^ +$ , −, or 0 (neutral). Now a feature related to exclamation marks might have a negative weight for 0 documents, and a positive weight for $^ +$ or documents:

$$
\frac { \mathrm { F e a t u r e } \mathrm { D e f i n i t i o n } } { f _ { 5 } ( x ) } \quad \{ \begin{array} { l l } { { \mathrm { 1 \ i f \ ^ {  } { : } \ ^ {  } { : } } \in \mathrm { d o c } } } & { { \begin{array} { l l l } { w _ { 5 , + } } & { w _ { 5 , - } } & { w _ { 5 , 0 } } \\ { { } } & { { } } \end{array} } } \\ { { 0 \mathrm { o t h e r w i s e } } } & { { 3 . 5 \quad 3 . 1 \quad - 5 . 3 } } \end{array} 
$$

Because these feature weights are dependent both on the input text and the output class, we sometimes make this dependence explicit and represent the features themselves as $f ( x , y )$ : a function of both the input and the class. Using such a notation

![## Image Analysis: 25d582188abb680bd9bdc550b78baeb03267046324d3c096d4eccb6379c39d8a.jpg

**Conceptual Understanding:**
This image conceptually illustrates the fundamental architectural differences between binary and multinomial logistic regression models. Its main purpose is to demonstrate how the internal structure, specifically the weight parameters and output layer, adapts based on whether the model is performing a two-class classification or a multi-class classification, while processing the same input features. The image visually conveys the contrast between a single-output, single-weight-vector system for binary prediction and a multiple-output, weight-matrix system for multinomial prediction.

**Content Interpretation:**
The image illustrates the architectural differences between Binary Logistic Regression and Multinomial Logistic Regression models. It details how the input features, derived from textual data, are processed through different weight structures to produce distinct output formats for classification tasks. The core distinction lies in the dimensionality of the output (scalar vs. vector) and the corresponding structure of the weight parameters (single vector vs. matrix) based on the number of classes being predicted. Both models use common text-derived features like word count, positive lexicon words, and counts of specific negative words.

**Key Insights:**
1. **Binary Logistic Regression for Two Classes:** This model uses a single weight vector (W [1xf]) to combine 'f' input features into a scalar output (ŷ [scalar]). It computes a probability for one class, with the other class probability being its complement (p(+) = 1- p(-)), typically using a sigmoid activation function.
2. **Multinomial Logistic Regression for Multiple Classes:** This model employs a weight matrix (W [Kxf]), where 'K' represents the number of classes. Each of the 'K' output nodes (e.g., ŷ1, ŷ2, ŷ3 for positive, negative, neutral) has its own corresponding weight vector (a row in W), leading to a vector output (ŷ [Kx1]) of probabilities for each class, usually utilizing a softmax activation function.
3. **Consistent Feature Extraction:** Both binary and multinomial logistic regression models can utilize the same type of input feature vector (X [f x 1]) derived from the raw input data, such as text. Examples provided include 'wordcount', 'positive lexicon words', and 'count of 'no''. This indicates that the feature engineering step can be common, while the model's internal structure adapts to the number of classes.
4. **Class-Specific Prototypes in Multinomial Regression:** The explicit annotation, "These f red weights are a row of W corresponding to weight vector w3, (= weights for class 3, = a prototype of class 3)", highlights that each row of the weight matrix in multinomial regression can be interpreted as a 'prototype' or learned representation for a specific class. This means the model learns distinct patterns for each outcome category.

The extracted text clearly distinguishes the 'Output' (sigmoid vs. softmax), 'Weight vector' vs. 'Weight matrix' and their dimensions ([1xf] vs. [Kxf]), and the 'Input feature vector' (X [f x 1]) which remains consistent across both models, providing strong evidence for these insights.

**Document Context:**
This image directly supports the document's section 5.3.3, "Features in Multinomial Logistic Regression," by providing a clear visual explanation of how multinomial logistic regression differs from binary logistic regression. It clarifies the conceptual shift from a single weight vector and scalar output for two classes to a weight matrix and vector output for multiple classes, thereby enhancing the understanding of how features are handled and processed in these models, particularly for text classification.

**Summary:**
The image presents a comparison of Binary Logistic Regression and Multinomial Logistic Regression, illustrating their structural differences, particularly concerning output, weight parameters, and input feature processing. Both models start with the same input words, "dessert was great", which are converted into an input feature vector. For instance, 'x1' represents 'wordcount = 3', 'x2' represents 'positive lexicon words = 1', and 'x3' represents 'count of "no" = 0'.

In **Binary Logistic Regression** (top section):
- The 'Output' is 'sigmoid', producing a scalar output denoted as 'ŷ' (y-hat) with a dimension of '[scalar]'. The probabilities are defined as 'p(+) = 1- p(-)'.
- The 'Weight vector' is 'W' with a dimension of '[1xf]', indicating a single weight vector connecting the 'f' input features to the single output 'ŷ'.
- The 'Input feature vector' is 'X' with a dimension of '[f x 1]', comprising features 'x1', 'x2', 'x3', ..., 'xf'. These features, as derived from "dessert was great", are detailed as: 'x1 wordcount = 3', 'x2 positive lexicon words = 1', and 'x3 count of "no" = 0'.

In **Multinomial Logistic Regression** (bottom section):
- The 'Output' is 'softmax', producing a vector output denoted as 'ŷ' (y-hat), specifically 'ŷ1', 'ŷ2', 'ŷ3' in the example, with a dimension of '[Kx1]'. The probabilities are shown for multiple classes: 'p(+)', 'p(-)', 'p(neut)'.
- The 'Weight matrix' is 'W' with a dimension of '[Kxf]', indicating a matrix of weights where 'K' is the number of classes and 'f' is the number of features. Each output node (ŷ1, ŷ2, ŷ3) receives input from all input features via its own set of weights.
- The 'Input feature vector' is 'X' with a dimension of '[f x 1]', identical to the binary regression, comprising 'x1', 'x2', 'x3', ..., 'xf'. The features from "dessert was great" are again 'x1 wordcount = 3', 'x2 positive lexicon words = 1', and 'x3 count of "no" = 0'.
- A specific annotation indicates an arrow pointing to the connections for 'ŷ3' (p(neut) output): "These f red weights are a row of W corresponding to weight vector w3, (= weights for class 3, = a prototype of class 3)". This highlights that each output node in multinomial regression has its own dedicated weight vector, which forms a row in the overall weight matrix W.](images/25d582188abb680bd9bdc550b78baeb03267046324d3c096d4eccb6379c39d8a.jpg)
Figure 5.3 Binary versus multinomial logistic regression. Binary logistic regression uses a single weight vector $\boldsymbol { \mathsf { w } }$ , and has a scalar output $\hat { y }$ . In multinomial logistic regression we have $K$ separate weight vectors corresponding to the $K$ classes, all packed into a single weight matrix $\boldsymbol { \mathsf { W } }$ , and a vector output $\hat { \bf y }$ . We omit the biases from both figures for clarity.

$f _ { 5 } ( x )$ above could be represented as three features $f _ { 5 } ( x , + )$ , $f _ { 5 } ( x , - )$ , and $f _ { 5 } ( x , 0 )$ , each of which has a single weight. We’ll use this kind of notation in our description of the CRF in Chapter 17.

# 5.4 Learning in Logistic Regression

How are the parameters of the model, the weights $\boldsymbol { \mathsf { W } }$ and bias $^ b$ , learned? Logistic regression is an instance of supervised classification in which we know the correct label $y$ (either 0 or 1) for each observation $x$ . What the system produces via Eq. 5.5 is $\hat { y }$ , the system’s estimate of the true $y$ . We want to learn parameters (meaning $\boldsymbol { \mathsf { W } }$ and $^ b$ ) that make $\hat { y }$ for each training observation as close as possible to the true $y$ .

This requires two components that we foreshadowed in the introduction to the chapter. The first is a metric for how close the current label $( \hat { y } )$ is to the true gold label y. Rather than measure similarity, we usually talk about the opposite of this: the distance between the system output and the gold output, and we call this distance the loss function or the cost function. In the next section we’ll introduce the loss function that is commonly used for logistic regression and also for neural networks, the cross-entropy loss.

The second thing we need is an optimization algorithm for iteratively updating the weights so as to minimize this loss function. The standard algorithm for this is gradient descent; we’ll introduce the stochastic gradient descent algorithm in the following section.

We’ll describe these algorithms for the simpler case of binary logistic regression in the next two sections, and then turn to multinomial logistic regression in Section 5.8.

# 5.5 The cross-entropy loss function

We need a loss function that expresses, for an observation $x$ , how close the classifier output $( \widehat { \boldsymbol { y } } = \sigma ( \boldsymbol { \mathsf { w } } \cdot \boldsymbol { \mathsf { x } } + b ) )$ is to the correct output $( y ,$ , which is 0 or 1). We’ll call this:

$$
{ \cal L } ( \hat { y } , y ) ~ = ~ \mathrm { H o w ~ m u c h } ~ \hat { y } ~ \mathrm { d i f f e r s ~ f r o m ~ t h e ~ t r u e ~ } y
$$

We do this via a loss function that prefers the correct class labels of the training examples to be more likely. This is called conditional maximum likelihood estimation: we choose the parameters $w , b$ that maximize the log probability of the true $y$ labels in the training data given the observations $x$ . The resulting loss function is the negative log likelihood loss, generally called the cross-entropy loss.

Let’s derive this loss function, applied to a single observation $x$ . We’d like to learn weights that maximize the probability of the correct label $p ( y | x )$ . Since there are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can express the probability $p ( y | x )$ that our classifier produces for one observation as the following (keeping in mind that if $y = 1$ , Eq. 5.21 simplifies to $\hat { y }$ ; if $y = 0$ , Eq. 5.21 simplifies to $1 - \hat { y } )$ :

$$
p ( y | x ) \ = \ \hat { y } ^ { y } ( 1 - \hat { y } ) ^ { 1 - y }
$$

Now we take the log of both sides. This will turn out to be handy mathematically, and doesn’t hurt us; whatever values maximize a probability will also maximize the log of the probability:

$$
\begin{array} { l } { \log p ( y | x ) \ = \ \log \left[ \hat { y } ^ { y } \left( 1 - \hat { y } \right) ^ { 1 - y } \right] } \\ { \ = \ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) } \end{array}
$$

Eq. 5.22 describes a log likelihood that should be maximized. In order to turn this into a loss function (something that we need to minimize), we’ll just flip the sign on Eq. 5.22. The result is the cross-entropy loss $L _ { \mathrm { C E } }$ :

$$
L _ { \mathrm { C E } } ( \hat { y } , y ) = - \log p ( y | x ) ~ = ~ - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]
$$

Finally, we can plug in the definition of $\hat { y } = \sigma ( { \boldsymbol { \mathsf { w } } } \cdot { \boldsymbol { \mathsf { x } } } + b )$ :

$$
{ \cal L } _ { \mathrm { C E } } ( \hat { y } , y ) ~ = ~ - [ y \log \sigma ( { \bf w } \cdot { \bf x } + b ) + ( 1 - y ) \log \left( 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) \right) ]
$$

Let’s see if this loss function does the right thing for our example from Fig. 5.2. We want the loss to be smaller if the model’s estimate is close to correct, and bigger if the model is confused. So first let’s suppose the correct gold label for the sentiment example in Fig. 5.2 is positive, i.e., $y = 1$ . In this case our model is doing well, since from Eq. 5.8 it indeed gave the example a higher probability of being positive (.70) than negative (.30). If we plug $\sigma ( \mathbf { w } \cdot \mathbf { x } + b ) = . 7 0$ and $y = 1$ into Eq. 5.24, the right side of the equation drops out, leading to the following loss (we’ll use log to mean natural log when the base is not specified):

$$
\begin{array} { r l r l } { L _ { \mathrm { C E } } ( \hat { y } , y ) = } & { } & & { - [ y \log \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) + ( 1 - y ) \log \left( 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) \right) ] } \\ { = } & { } & & { - \left[ \log \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) \right] } \\ { = } & { } & & { - \log ( \cdot 7 0 ) } \\ { = } & { } & & { 3 6 } \end{array}
$$

By contrast, let’s pretend instead that the example in Fig. 5.2 was actually negative, i.e., $y = 0$ (perhaps the reviewer went on to say “But bottom line, the movie is terrible! I beg you not to see it!”). In this case our model is confused and we’d want the loss to be higher. Now if we plug $y = 0$ and $1 - \sigma ( { \boldsymbol { \mathsf { w } } } \cdot { \boldsymbol { \mathsf { x } } } + b ) = . 3 0$ from Eq. 5.8 into Eq. 5.24, the left side of the equation drops out:

$$
\begin{array} { r l r } { L _ { \mathrm { C E } } ( \hat { y } , y ) = } & { } & { - [ y \log \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) + ( 1 - y ) \log \left( 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) \right) ] } \\ { = } & { } & { - \left[ \log \left( 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) \right) \right] } \\ { = } & { } & { - \log \left( . 3 0 \right) } \\ { = } & { } & { 1 . 2 } \end{array}
$$

Sure enough, the loss for the first classifier (.36) is less than the loss for the second classifier (1.2).

Why does minimizing this negative log probability do what we want? A perfect classifier would assign probability 1 to the correct outcome $\mathrm { \Delta } y = 1$ or $y = 0$ ) and probability 0 to the incorrect outcome. That means if $y$ equals 1, the higher $\hat { y }$ is (the closer it is to 1), the better the classifier; the lower $\hat { y }$ is (the closer it is to 0), the worse the classifier. If $y$ equals 0, instead, the higher $1 - \hat { y }$ is (closer to 1), the better the classifier. The negative log of $\hat { y }$ (if the true $y$ equals 1) or $1 - \hat { y }$ (if the true $y$ equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss) to infinity (negative log of 0, infinite loss). This loss function also ensures that as the probability of the correct answer is maximized, the probability of the incorrect answer is minimized; since the two sum to one, any increase in the probability of the correct answer is coming at the expense of the incorrect answer. It’s called the crossentropy loss, because Eq. 5.22 is also the formula for the cross-entropy between the true probability distribution $y$ and our estimated distribution $\hat { y }$ .

Now we know what we want to minimize; in the next section, we’ll see how to find the minimum.

# 5.6 Gradient Descent

Our goal with gradient descent is to find the optimal weights: minimize the loss function we’ve defined for the model. In Eq. 5.25 below, we’ll explicitly represent the fact that the cross-entropy loss function $L _ { \mathrm { C E } }$ is parameterized by the weights. In

machine learning in general we refer to the parameters being learned as $\theta$ ; in the case of logistic regression $\boldsymbol { \theta } = \{ \mathbf { w } , b \}$ . So the goal is to find the set of weights which minimizes the loss function, averaged over all examples:

$$
\hat { \theta } \ = \ \underset { \theta } { \operatorname { a r g m i n } } \frac { 1 } { m } \sum _ { i = 1 } ^ { m } L _ { \mathrm { { C E } } } ( f ( x ^ { ( i ) } ; \theta ) , y ^ { ( i ) } )
$$

How shall we find the minimum of this (or any) loss function? Gradient descent is a method that finds a minimum of a function by figuring out in which direction (in the space of the parameters $\theta$ ) the function’s slope is rising the most steeply, and moving in the opposite direction. The intuition is that if you are hiking in a canyon and trying to descend most quickly down to the river at the bottom, you might look around yourself in all directions, find the direction where the ground is sloping the steepest, and walk downhill in that direction.

For logistic regression, this loss function is conveniently convex. A convex function has at most one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.)

Although the algorithm (and the concept of gradient) are designed for direction vectors, let’s first consider a visualization of the case where the parameter of our system is just a single scalar $w$ , shown in Fig. 5.4.

Given a random initialization of $w$ at some value $w ^ { 1 }$ , and assuming the loss function $L$ happened to have the shape in Fig. 5.4, we need the algorithm to tell us whether at the next iteration we should move left (making $w ^ { 2 }$ smaller than $w ^ { 1 }$ ) or right (making $w ^ { 2 }$ bigger than $w ^ { 1 }$ ) to reach the minimum.

![## Image Analysis: cefb8645882083f189508126bbd621c343d42dca3281b4fefc097dbb69d70d5d.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental mechanism of the first step of the gradient descent optimization algorithm. Its main purpose is to visually demonstrate how the algorithm iteratively adjusts a parameter ('w') to minimize a 'Loss' function. The image communicates the key idea that the direction of adjustment for the parameter is determined by the negative of the slope (gradient) of the loss function, guiding the parameter towards the minimum loss value (the 'goal').

**Content Interpretation:**
The image displays a graph of a 'Loss' function (blue curve) against a parameter 'w'. It illustrates the initial step of the gradient descent optimization algorithm. Key concepts shown include:

1.  **Loss Function:** The blue curve represents the 'Loss' function, where the Y-axis indicates the 'Loss' value and the X-axis indicates the parameter 'w'. The curve shows a clear minimum point.
2.  **Initial Parameter Value and Loss:** The black dot on the curve corresponds to the initial parameter value 'w^1' (which is '0').
3.  **Gradient (Slope):** A dashed green line represents the tangent to the loss curve at 'w^1'. The annotation "slope of loss at w^1 is negative" explicitly states the direction of the gradient at this initial point.
4.  **Gradient Descent Step:** A red curved arrow, labeled "one step of gradient descent", demonstrates the movement from the initial black dot to a new point (gray dot) further down and to the right along the curve. This visually represents 'w' being updated in a direction that decreases the 'Loss'.
5.  **Optimization Goal:** The point 'w^min', labeled as "(goal)", indicates the parameter value where the 'Loss' function is minimized.

The significance is that the negative slope at 'w^1' dictates a positive movement in 'w' to decrease 'Loss', which is precisely what "one step of gradient descent" depicts. The movement from the black dot to the gray dot confirms the algorithm's principle of moving against the gradient to find the minimum.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Gradient Descent Principle:** The image visually confirms that gradient descent operates by moving the parameter ('w') in the direction opposite to the slope (gradient) of the loss function to minimize it. This is evidenced by the 'slope of loss at w^1 is negative' leading to 'one step of gradient descent' moving 'w' to the right (positive direction).
2.  **Iterative Optimization:** It highlights that gradient descent is an iterative process aimed at reaching a minimum. The 'one step' suggests that multiple such steps would be taken to reach the ultimate 'w^min (goal)'.
3.  **Slope Interpretation:** A negative slope of the loss function indicates that increasing the parameter 'w' will reduce the loss, guiding the algorithm's direction. This is directly supported by the green annotation 'slope of loss at w^1 is negative' and the subsequent rightward movement.

**Document Context:**
This image directly supports Section 5.6 'Gradient Descent' by providing a visual explanation of the algorithm's first step. It clarifies the text stating that the algorithm 'moves w in the reverse direction from the slope of the function' and that 'Since the slope is negative, we need to move w in a positive direction, to the right.' The graph precisely illustrates this scenario, showing the initial state where the 'slope of loss at w^1 is negative' and the subsequent 'one step of gradient descent' moving 'w' to the right towards 'w^min (goal)'. The labels 'w^1', '0', and 'w^min' align with the document's notation for learning steps and the initial value of 'w'.

**Summary:**
This image is a 2D graph illustrating the first step of the gradient descent algorithm used to minimize a loss function. The vertical axis is labeled "Loss", representing the value of the function to be minimized. The horizontal axis is labeled "w", representing a single parameter (often a weight in a model) that is being optimized.

A blue U-shaped curve depicts the loss function, indicating a minimum point. On the horizontal axis, three specific points are marked:
1.  "w^1" is an initial value for the parameter, with "0" written directly below it, indicating that w^1 is initially 0.
2.  "w^min" represents the parameter value at which the loss function reaches its absolute minimum. Below it, "(goal)" is written, signifying that reaching w^min is the objective of the optimization.

At the initial parameter value "w^1", there is a black dot on the blue loss curve. A dashed green line is drawn tangent to the curve at this black dot. A green arrow points from the text "slope of loss at w^1 is negative" towards this dashed green tangent line, explicitly stating that the gradient of the loss function at w^1 is negative. This negative slope indicates that increasing 'w' (moving to the right on the x-axis) will cause the 'Loss' to decrease.

A red curved arrow starts near the black dot (at w^1) and arcs downwards and to the right along the blue curve, ending at a gray dot. This red arrow is labeled "one step of gradient descent", demonstrating how the algorithm moves the parameter 'w' from its initial value to a new value in a single iteration. Because the slope was negative, gradient descent moves 'w' in a positive direction (to the right) to reduce the loss, leading to the position of the gray dot where the loss value is lower than at the black dot.

In summary, the image effectively visualizes how gradient descent takes an initial parameter value, assesses the slope of the loss function at that point, and then moves the parameter in the opposite direction of the slope to iteratively minimize the loss towards the "w^min (goal)".](images/cefb8645882083f189508126bbd621c343d42dca3281b4fefc097dbb69d70d5d.jpg)
Figure 5.4 The first step in iteratively finding the minimum of this loss function, by moving $w$ in the reverse direction from the slope of the function. Since the slope is negative, we need to move $w$ in a positive direction, to the right. Here superscripts are used for learning steps, so $w ^ { 1 }$ means the initial value of $w$ (which is 0), $w ^ { 2 }$ the value at the second step, and so on.

The gradient descent algorithm answers this question by finding the gradient of the loss function at the current point and moving in the opposite direction. The gradient of a function of many variables is a vector pointing in the direction of the greatest increase in a function. The gradient is a multi-variable generalization of the slope, so for a function of one variable like the one in Fig. 5.4, we can informally think of the gradient as the slope. The dotted line in Fig. 5.4 shows the slope of this hypothetical loss function at point $w = w ^ { 1 }$ . You can see that the slope of this dotted line is negative. Thus to find the minimum, gradient descent tells us to go in the opposite direction: moving $w$ in a positive direction.

The magnitude of the amount to move in gradient descent is the value of the slope $\begin{array} { r } { \frac { d } { d w } L ( \bar { f } ( x ; w ) , y ) } \end{array}$ weighted by a learning rate $\eta$ . A higher (faster) learning rate means that we should move $w$ more on each step. The change we make in our parameter is the learning rate times the gradient (or the slope, in our single-variable example):

$$
w ^ { t + 1 } = w ^ { t } - \eta \frac { d } { d w } L ( f ( x ; w ) , y )
$$

Now let’s extend the intuition from a function of one scalar variable $w$ to many variables, because we don’t just want to move left or right, we want to know where in the N-dimensional space (of the $N$ parameters that make up $\theta$ ) we should move. The gradient is just such a vector; it expresses the directional components of the sharpest slope along each of those $N$ dimensions. If we’re just imagining two weight dimensions (say for one weight $w$ and one bias $^ b$ ), the gradient might be a vector with two orthogonal components, each of which tells us how much the ground slopes in the $w$ dimension and in the $^ b$ dimension. Fig. 5.5 shows a visualization of the value of a 2-dimensional gradient vector taken at the red point.

In an actual logistic regression, the parameter vector $\pmb { w }$ is much longer than 1 or 2, since the input feature vector $\pmb { \times }$ can be quite long, and we need a weight $w _ { i }$ for each $x _ { i }$ . For each dimension/variable $w _ { i }$ in $\boldsymbol { \mathsf { w } }$ (plus the bias $b$ ), the gradient will have a component that tells us the slope with respect to that variable. In each dimension $w _ { i }$ , we express the slope as a partial derivative $\frac { \partial } { \partial w _ { i } }$ of the loss function. Essentially we’re asking: “How much would a small change in that variable $w _ { i }$ influence the total loss function $L$ ?”

Formally, then, the gradient of a multi-variable function $f$ is a vector in which each component expresses the partial derivative of $f$ with respect to one of the variables. We’ll use the inverted Greek delta symbol $\nabla$ to refer to the gradient, and represent $\hat { y }$ as $f ( x ; \theta )$ to make the dependence on $\theta$ more obvious:

![## Image Analysis: d1fa3c62fa821cccb3b91423f14e36b0a199fb7eaa6f1cc91e9f0ca8edaa57c2.jpg

**Conceptual Understanding:**
The image conceptually represents the 'cost surface' of a function dependent on two parameters, 'w' and 'b', which are typically weights and bias in a machine learning context. The main purpose is to visualize how a cost function behaves in 3D and, more specifically, to illustrate one step of the Gradient Descent optimization algorithm. It conveys the key idea that to minimize a cost, one must move from the current parameter values (the red dot's projection) in a direction (the red arrow) that leads to a lower cost, which is the opposite direction of the gradient of the function at that point. The bowl shape emphasizes that there is a minimum to be found.

**Content Interpretation:**
The image primarily shows a 3D plot of a cost function, `Cost(w,b)`, which is a fundamental concept in machine learning and optimization. The surface's parabolic, bowl-like shape signifies a convex function with a single global minimum, where the cost is lowest. The variables 'w' and 'b' are model parameters (weights and bias, respectively) that the cost function depends on. The red dot represents the current state of these parameters and their associated cost during an optimization process. The red dashed line projects this point onto the `w-b` parameter plane. The solid red arrow in the `w-b` plane indicates the direction in which the parameters 'w' and 'b' should be adjusted to decrease the cost. This direction is precisely opposite to the gradient vector at the current point, guiding the optimization towards the minimum of the cost function, as described by the text 'the opposite direction of the gradient'.

**Key Insights:**
The main takeaway from this image is a visual understanding of how a cost function for two parameters (`w` and `b`) can be represented in 3D, creating an 'optimization landscape' (the bowl shape). The image effectively demonstrates a single step of the Gradient Descent algorithm: from a given point on this landscape (the red dot), the parameters are updated by moving in the direction opposite to the gradient, which is the path of steepest descent, as indicated by the red arrow. This movement aims to iteratively reduce the cost until the minimum of the function is reached. The labels 'Cost(w,b)', 'w', and 'b' are critical textual evidence confirming that this is an illustration of parameter optimization for a cost function.

**Document Context:**
This image is placed within the "5.6 Gradient Descent" section of the document, directly illustrating the core concept of how gradient descent works. The accompanying text, 'Figure 5.5 Visualization of the gradient vector at the red point in two dimensions $w$ and $^b$ , showing a red arrow in the x-y plane pointing in the direction we will go to look for the minimum: the opposite direction of the gradient (recall that the gradient points in the direction of increase not decrease),' confirms its role in visually explaining how the algorithm iteratively adjusts parameters ('w' and 'b') by moving in the direction of steepest decrease on the cost function landscape. It provides a visual foundation for understanding the mechanics of finding optimal parameters in machine learning models.

**Summary:**
This image presents a 3D visualization of a cost function, explicitly labeled as "Cost(w,b)" at the top. The function's output, representing the cost, is shown as a blue mesh surface in three-dimensional space. The two independent variables or parameters that determine this cost are labeled on the horizontal axes: "w" (weight) extending towards the front-left, and "b" (bias) extending towards the front-right. The surface forms a parabolic, bowl-like shape, characteristic of a convex function, which implies a single global minimum exists. A red dot is positioned on this blue cost surface, indicating a specific combination of 'w' and 'b' values and their corresponding cost at a particular iteration of an optimization process. A dashed red vertical line extends from this red dot down to its projection on the 'w-b' parameter plane. From this projected point on the 'w-b' plane, a solid red arrow points towards the approximate center of the bowl. This arrow visually represents the direction in which the parameters 'w' and 'b' would be simultaneously adjusted to move towards a lower cost value, illustrating a single step in a gradient descent optimization process. The direction of the arrow signifies the path of steepest descent on the cost surface, guiding the optimization towards the function's minimum.](images/d1fa3c62fa821cccb3b91423f14e36b0a199fb7eaa6f1cc91e9f0ca8edaa57c2.jpg)
Figure 5.5 Visualization of the gradient vector at the red point in two dimensions $w$ and $^ b$ , showing a red arrow in the x-y plane pointing in the direction we will go to look for the minimum: the opposite direction of the gradient (recall that the gradient points in the direction of increase not decrease).

$$
\nabla L ( f ( x ; \theta ) , y ) ~ = ~ \left[ \begin{array} { c } { \frac { \partial } { \partial w _ { 1 } } L ( f ( x ; \theta ) , y ) } \\ { \frac { \partial } { \partial w _ { 2 } } L ( f ( x ; \theta ) , y ) } \\ { \vdots } \\ { \frac { \partial } { \partial w _ { n } } L ( f ( x ; \theta ) , y ) } \\ { \frac { \partial } { \partial b } L ( f ( x ; \theta ) , y ) } \end{array} \right]
$$

The final equation for updating $\theta$ based on the gradient is thus

$$
\pmb { \theta } ^ { t + 1 } = \pmb { \theta } ^ { t } - \eta \nabla L ( f ( x ; \pmb { \theta } ) , y )
$$

# 5.6.1 The Gradient for Logistic Regression

In order to update $\theta$ , we need a definition for the gradient $\nabla L ( f ( x ; \theta ) , y )$ . Recall that for logistic regression, the cross-entropy loss function is:

$$
{ \cal L } _ { \mathrm { C E } } ( \hat { y } , y ) ~ = ~ - [ y \log \sigma ( { \bf w } \cdot { \bf x } + b ) + ( 1 - y ) \log \left( 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) \right) ]
$$

It turns out that the derivative of this function for one observation vector $x$ is Eq. 5.30 (the interested reader can see Section 5.10 for the derivation of this equation):

$$
\begin{array} { r l r } { \frac { \partial L _ { \mathrm { C E } } ( \hat { y } , y ) } { \partial w _ { j } } } & { = } & { [ \pmb { \sigma } ( \mathbf { w } \cdot \mathbf { x } + b ) - y ] x _ { j } } \\ & { } & { = ~ ( \hat { y } - y ) x _ { j } } \end{array}
$$

You’ll also sometimes see this equation in the equivalent form:

$$
\frac { \partial { \cal L } _ { \mathrm { C E } } ( \hat { y } , y ) } { \partial w _ { j } } ~ = ~ - ( y - \hat { y } ) x _ { j }
$$

Note in these equations that the gradient with respect to a single weight $w _ { j }$ represents a very intuitive value: the difference between the true $y$ and our estimated $\hat { y } = \sigma ( { \boldsymbol { \mathsf { w } } } \cdot { \boldsymbol { \mathsf { x } } } + b )$ for that observation, multiplied by the corresponding input value $x _ { j }$ .

# 5.6.2 The Stochastic Gradient Descent Algorithm

Stochastic gradient descent is an online algorithm that minimizes the loss function by computing its gradient after each training example, and nudging $\theta$ in the right direction (the opposite direction of the gradient). (An “online algorithm” is one that processes its input example by example, rather than waiting until it sees the entire input.) Stochastic gradient descent is called stochastic because it chooses a single random example at a time; in Section 5.6.4 we’ll discuss other versions of gradient descent that batch many examples at once. Fig. 5.6 shows the algorithm.

The learning rate $\eta$ is a hyperparameter that must be adjusted. If it’s too high, the learner will take steps that are too large, overshooting the minimum of the loss function. If it’s too low, the learner will take steps that are too small, and take too long to get to the minimum. It is common to start with a higher learning rate and then slowly decrease it, so that it is a function of the iteration $k$ of training; the notation $\eta _ { k }$ can be used to mean the value of the learning rate at iteration $k$ .

function STOCHASTIC GRADIENT DESCENT $\cdot ( L ( ) , f ( ) , x , y )$ returns θ # where: L is the loss function # f is a function parameterized by $\theta$ # x is the set of training inputs x(1), x(2), ..., x(m) # y is the set of training outputs (labels) y(1), y(2), ..., y(m)   
$\theta \gets 0$ # (or small random values)   
repeat til done # see caption For each training tuple $( \boldsymbol x ^ { ( i ) } , \boldsymbol y ^ { ( i ) } )$ (in random order) 1. Optional (for reporting): # How are we doing on this tuple? Compute $\hat { y } ^ { ( i ) } = f ( x ^ { ( i ) } ; \theta )$ # What is our estimated output $\hat { y } ?$ Compute the loss $L ( \hat { y } ^ { ( i ) } , y ^ { ( i ) } )$ # How far off is $\hat { y } ^ { ( i ) }$ from the true output $y ^ { ( i ) }$ ? 2 $\cdot g  \bar { \nabla _ { \theta } } L ( f ( x ^ { ( i ) } ; \theta ) , y ^ { ( i ) } )$ # How should we move $\theta$ to maximize loss? 3. $\theta  \theta \mathrm { ~ - ~ } \eta \mathrm { ~ g ~ }$ # Go the other way instead   
return θ

Figure 5.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used mainly to report how well we are doing on the current tuple; we don’t need to compute the loss in order to compute the gradient. The algorithm can terminate when it converges (when the gradient norm $< \epsilon$ ), or when progress halts (for example when the loss starts going up on a held-out set). Weights are initialized to 0 for logistic regression, but to small random values for neural networks, as we’ll see in Chapter 7.

We’ll discuss hyperparameters in more detail in Chapter 7, but in short, they are a special kind of parameter for any machine learning model. Unlike regular parameters of a model (weights like $w$ and $b$ ), which are learned by the algorithm from the training set, hyperparameters are special parameters chosen by the algorithm designer that affect how the algorithm works.

# 5.6.3 Working through an example

Let’s walk through a single step of the gradient descent algorithm. We’ll use a simplified version of the example in Fig. 5.2 as it sees a single observation $x$ , whose correct value is $y = 1$ (this is a positive review), and with a feature vector $\mathbf { x } = [ x _ { 1 } , x _ { 2 } ]$ consisting of these two features:

$$
\begin{array} { l l } { { x _ { 1 } ~ = ~ 3 ~ } } & { { ( \mathrm { c o u n t ~ o f ~ p o s i t i v e ~ l e x i c o n ~ w o r d s } ) } } \\ { { x _ { 2 } ~ = ~ 2 ~ } } & { { ( \mathrm { c o u n t ~ o f ~ n e g a t i v e ~ l e x i c o n ~ w o r d s } ) } } \end{array}
$$

Let’s assume the initial weights and bias in $\theta ^ { 0 }$ are all set to 0, and the initial learning rate $\eta$ is 0.1:

$$
\begin{array} { r } { w _ { 1 } = w _ { 2 } = b \ = \ 0 } \\ { \eta \ = \ 0 . 1 } \end{array}
$$

The single update step requires that we compute the gradient, multiplied by the learning rate

$$
\theta ^ { t + 1 } = \theta ^ { t } - \eta \nabla _ { \theta } L ( f ( x ^ { ( i ) } ; \theta ) , y ^ { ( i ) } )
$$

In our mini example there are three parameters, so the gradient vector has 3 dimensions, for $w _ { 1 } , w _ { 2 }$ , and $^ b$ . We can compute the first gradient as follows:

$$
\nabla _ { w , b } L = \left[ \begin{array} { l } { \frac { \partial L _ { \mathrm { C } } ( \hat { y } , y ) } { \partial w _ { 1 } } } \\ { \frac { \partial L _ { \mathrm { C } } ( \hat { y } , y ) } { \partial w _ { 2 } } } \\ { \frac { \partial L _ { \mathrm { C } } ( \hat { y } , y ) } { \partial b } } \end{array} \right] = \left[ \begin{array} { l } { ( \boldsymbol { \sigma } ( \mathbf { w } \cdot \mathbf { x } + b ) - y ) \boldsymbol { x } _ { 1 } } \\ { ( \boldsymbol { \sigma } ( \mathbf { w } \cdot \mathbf { x } + b ) - y ) \boldsymbol { x } _ { 2 } } \\ { \boldsymbol { \sigma } ( \mathbf { w } \cdot \mathbf { x } + b ) - y } \end{array} \right] = \left[ \begin{array} { l } { ( \boldsymbol { \sigma } ( 0 ) - 1 ) \boldsymbol { x } _ { 1 } } \\ { ( \boldsymbol { \sigma } ( 0 ) - 1 ) \boldsymbol { x } _ { 2 } } \\ { \boldsymbol { \sigma } ( 0 ) - 1 } \end{array} \right] = \left[ \begin{array} { l } { - 0 . 5 x _ { 1 } } \\ { - 0 . 5 x _ { 2 } } \\ { - 0 . 5 } \end{array} \right] = \left[ \begin{array} { l } { - 1 . 5 y } \\ { - 1 . 0 } \\ { - 0 . 5 } \end{array} \right]
$$

Now that we have a gradient, we compute the new parameter vector $\theta ^ { 1 }$ by moving $\theta ^ { 0 }$ in the opposite direction from the gradient:

$$
\theta ^ { 1 } = { \left[ \begin{array} { l } { w _ { 1 } } \\ { w _ { 2 } } \\ { b } \end{array} \right] } - \eta \left[ { - 1 . 5 } \atop { - 1 . 0 } \right] = { \left[ \begin{array} { l } { . 1 5 } \\ { . 1 } \\ { . 0 5 } \end{array} \right] }
$$

So after one step of gradient descent, the weights have shifted to be: $w _ { 1 } = . 1 5$ $w _ { 2 } = . 1$ , and $b = . 0 5$ .

Note that this observation $x$ happened to be a positive example. We would expect that after seeing more negative examples with high counts of negative words, that the weight $\boldsymbol { \mathsf { w } } _ { 2 }$ would shift to have a negative value.

# 5.6.4 Mini-batch training

Stochastic gradient descent is called stochastic because it chooses a single random example at a time, moving the weights so as to improve performance on that single example. That can result in very choppy movements, so it’s common to compute the gradient over batches of training instances rather than a single instance.

For example in batch training we compute the gradient over the entire dataset. By seeing so many examples, batch training offers a superb estimate of which direction to move the weights, at the cost of spending a lot of time processing every single example in the training set to compute this perfect direction.

A compromise is mini-batch training: we train on a group of $m$ examples (perhaps 512, or 1024) that is less than the whole dataset. (If $m$ is the size of the dataset, then we are doing batch gradient descent; if $m = 1$ , we are back to doing stochastic gradient descent.) Mini-batch training also has the advantage of computational efficiency. The mini-batches can easily be vectorized, choosing the size of the minibatch based on the computational resources. This allows us to process all the examples in one mini-batch in parallel and then accumulate the loss, something that’s not possible with individual or batch training.

We just need to define mini-batch versions of the cross-entropy loss function we defined in Section 5.5 and the gradient in Section 5.6.1. Let’s extend the crossentropy loss for one example from Eq. 5.23 to mini-batches of size $m$ . We’ll continue to use the notation that $x ^ { ( i ) }$ and $\boldsymbol { y } ^ { ( i ) }$ mean the ith training features and training label, respectively. We make the assumption that the training examples are independent:

$$
\begin{array} { l } { \displaystyle \log p \big ( \mathrm { t r a i n i n g ~ l a b e l s } \big ) = \log \prod _ { i = 1 } ^ { m } p \big ( y ^ { ( i ) } | x ^ { ( i ) } \big ) } \\ { = \displaystyle \sum _ { i = 1 } ^ { m } \log p \big ( y ^ { ( i ) } | x ^ { ( i ) } \big ) } \\ { = - \displaystyle \sum _ { i = 1 } ^ { m } L _ { \mathrm {  { \mathrm { C E } } } } \big ( \hat { y } ^ { ( i ) } , y ^ { ( i ) } \big ) } \end{array}
$$

Now the cost function for the mini-batch of $m$ examples is the average loss for each example:

$$
\begin{array} { l } { { { \cal C } o s t ( \hat { y } , y ) = { \displaystyle { \frac { 1 } { m } } \sum _ { i = 1 } ^ { m } } { \cal L } _ { \mathrm { C E } } ( \hat { y } ^ { ( i ) } , y ^ { ( i ) } ) } \ ~ } \\ { { \displaystyle = - { \frac { 1 } { m } } \sum _ { i = 1 } ^ { m } y ^ { ( i ) } \log \sigma ( { \bf w } \cdot { \bf x } ^ { ( i ) } + b ) + ( 1 - y ^ { ( i ) } ) \log \left( 1 - \sigma ( { \bf w } \cdot { \bf x } ^ { ( i ) } + b ) \right) } } \end{array}
$$

The mini-batch gradient is the average of the individual gradients from Eq. 5.30:

$$
\frac { \partial C o s t ( \hat { y } , y ) } { \partial w _ { j } } ~ = ~ \frac { 1 } { m } \sum _ { i = 1 } ^ { m } \left[ \sigma ( { \bf w } \cdot { \bf x } ^ { ( i ) } + b ) - y ^ { ( i ) } \right] x _ { j } ^ { ( i ) }
$$

Instead of using the sum notation, we can more efficiently compute the gradient in its matrix form, following the vectorization we saw on page 83, where we have a matrix $\pmb { \times }$ of size $[ m \times f ]$ representing the $m$ inputs in the batch, and a vector $\pmb { \ y }$ of size $[ m \times 1 ]$ representing the correct outputs:

$$
\begin{array} { l } { \displaystyle \frac { \partial C o s t ( \hat { y } , y ) } { \partial \mathbf { w } } = \frac { 1 } { m } ( \hat { \pmb { y } } - \mathbf { y } ) ^ { \intercal } \mathbf { x } } \\ { = \displaystyle \frac { 1 } { m } ( \sigma ( \mathbf { \pmb { X } } \mathbf { w } + \mathbf { b } ) - \mathbf { y } ) ^ { \intercal } \mathbf { x } } \end{array}
$$

# 5.7 Regularization

Numquam ponenda est pluralitas sine necessitate ‘Plurality should never be proposed unless needed’ William of Occam

overfitting generalize regularization

There is a problem with learning weights that make the model perfectly match the training data. If a feature is perfectly predictive of the outcome because it happens to only occur in one class, it will be assigned a very high weight. The weights for features will attempt to perfectly fit details of the training set, in fact too perfectly, modeling noisy factors that just accidentally correlate with the class. This problem is called overfitting. A good model should be able to generalize well from the training data to the unseen test set, but a model that overfits will have poor generalization.

To avoid overfitting, a new regularization term $R ( \theta )$ is added to the loss function in Eq. 5.25, resulting in the following loss for a batch of $m$ examples (slightly rewritten from Eq. 5.25 to be maximizing log probability rather than minimizing loss, and removing the $\frac { 1 } { m }$ term which doesn’t affect the argmax):

$$
\hat { \theta } \ = \ \underset { \theta } { \mathrm { a r g m a x } } \sum _ { i = 1 } ^ { m } \log P ( y ^ { ( i ) } | x ^ { ( i ) } ) - \alpha R ( \theta )
$$

The new regularization term $R ( \theta )$ is used to penalize large weights. Thus a setting of the weights that matches the training data perfectly— but uses many weights with high values to do so—will be penalized more than a setting that matches the data a little less well, but does so using smaller weights. There are two common ways to compute this regularization term $R ( \theta )$ . L2 regularization is a quadratic function of the weight values, named because it uses the (square of the) L2 norm of the weight values. The L2 norm, $| | \theta | | _ { 2 }$ , is the same as the Euclidean distance of the vector $\theta$ from the origin. If $\theta$ consists of $n$ weights, then:

$$
R ( \theta ) ~ = ~ | | \theta | | _ { 2 } ^ { 2 } = \sum _ { j = 1 } ^ { n } \theta _ { j } ^ { 2 }
$$

The L2 regularized loss function becomes:

$$
\hat { \theta } \ = \ \underset { \theta } { \operatorname { a r g m a x } } \left[ \sum _ { i = 1 } ^ { m } \log P ( y ^ { ( i ) } | x ^ { ( i ) } ) \right] - \alpha \sum _ { j = 1 } ^ { n } \theta _ { j } ^ { 2 }
$$

L1 regularization is a linear function of the weight values, named after the L1 norm $| | W | | _ { 1 }$ , the sum of the absolute values of the weights, or Manhattan distance (the Manhattan distance is the distance you’d have to walk between two points in a city with a street grid like New York):

$$
R ( \theta ) ~ = ~ | | \theta | | _ { 1 } = \sum _ { i = 1 } ^ { n } | \theta _ { i } |
$$

The L1 regularized loss function becomes:

lasso ridge

$$
\hat { \theta } \ = \ \underset { \theta } { \operatorname { a r g m a x } } \left[ \sum _ { i = 1 } ^ { m } \log P ( y ^ { ( i ) } | x ^ { ( i ) } ) \right] - \alpha \sum _ { j = 1 } ^ { n } | \theta _ { j } |
$$

These kinds of regularization come from statistics, where L1 regularization is called lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression, and both are commonly used in language processing. L2 regularization is easier to optimize because of its simple derivative (the derivative of $\theta ^ { 2 }$ is just 2θ ), while L1 regularization is more complex (the derivative of $| \theta |$ is non-continuous at zero). But while L2 prefers weight vectors with many small weights, L1 prefers sparse solutions with some larger weights but many more weights set to zero. Thus L1 regularization leads to much sparser weight vectors, that is, far fewer features.

Both L1 and L2 regularization have Bayesian interpretations as constraints on the prior of how weights should look. L1 regularization can be viewed as a Laplace prior on the weights. L2 regularization corresponds to assuming that weights are distributed according to a Gaussian distribution with mean $\mu = 0$ . In a Gaussian or normal distribution, the further away a value is from the mean, the lower its probability (scaled by the variance $\sigma$ ). By using a Gaussian prior on the weights, we are saying that weights prefer to have the value 0. A Gaussian for a weight $\theta _ { j }$ is

$$
\frac { 1 } { \sqrt { 2 \pi \sigma _ { j } ^ { 2 } } } \exp \left( - \frac { ( \theta _ { j } - \mu _ { j } ) ^ { 2 } } { 2 \sigma _ { j } ^ { 2 } } \right)
$$

If we multiply each weight by a Gaussian prior on the weight, we are thus maximizing the following constraint:

$$
\hat { \theta } ~ = ~ \operatorname * { a r g m a x } _ { \theta } \prod _ { i = 1 } ^ { m } P ( y ^ { ( i ) } | x ^ { ( i ) } ) \times \prod _ { j = 1 } ^ { n } \frac { 1 } { \sqrt { 2 \pi \sigma _ { j } ^ { 2 } } } \exp \left( - \frac { ( \theta _ { j } - \mu _ { j } ) ^ { 2 } } { 2 \sigma _ { j } ^ { 2 } } \right)
$$

which in log space, with $\mu = 0$ , and assuming $2 \sigma ^ { 2 } = 1$ , corresponds to

$$
\hat { \theta } \ = \ \underset { \theta } { \operatorname { a r g m a x } } \sum _ { i = 1 } ^ { m } \log P ( y ^ { ( i ) } | x ^ { ( i ) } ) - \alpha \sum _ { j = 1 } ^ { n } \theta _ { j } ^ { 2 }
$$

which is in the same form as Eq. 5.38.

# 5.8 Learning in Multinomial Logistic Regression

The loss function for multinomial logistic regression generalizes the loss function for binary logistic regression from 2 to $K$ classes. Recall that that the cross-entropy loss for binary logistic regression (repeated from Eq. 5.23) is:

$$
L _ { \mathrm { C E } } ( \hat { y } , y ) = - \log p ( y | x ) ~ = ~ - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]
$$

The loss function for multinomial logistic regression generalizes the two terms in Eq. 5.44 (one that is non-zero when $y = 1$ and one that is non-zero when $y = 0$ ) to $K$ terms. As we mentioned above, for multinomial regression we’ll represent both $\pmb { \ y }$ and $\hat { \mathbf { y } }$ as vectors. The true label $\pmb { y }$ is a vector with $K$ elements, each corresponding to a class, with $y _ { c } = 1$ if the correct class is $c$ , with all other elements of $\pmb { \ y }$ being 0. And our classifier will produce an estimate vector with $K$ elements $\hat { \mathbf { y } }$ , each element $\hat { y } _ { k }$ of which represents the estimated probability $p ( y _ { k } = 1 | \mathbf { x } )$ .

The loss function for a single example $\pmb { \times }$ , generalizing from binary logistic regression, is the sum of the logs of the $K$ output classes, each weighted by the indicator function $\mathbf { y } _ { k }$ (Eq. 5.45). This turns out to be just the negative log probability of the correct class $c$ (Eq. 5.46):

$$
\begin{array} { l } { { \displaystyle { \cal L } _ { \mathrm { C E } } ( \hat { \bf y } , { \bf y } ) ~ = ~ - \sum _ { k = 1 } ^ { K } y _ { k } \log \hat { y } _ { k } } } \\ { ~ = ~ - \log \hat { y } _ { c } , ~ \mathrm { ( w h e r e ~ } c \mathrm { ~ i s ~ t h e ~ c o r r e c t ~ c l a s s ) } } \\ { ~ = ~ - \log \hat { p } ( y _ { c } = 1 | { \bf x } ) ~ \mathrm { ( w h e r e ~ } c \mathrm { ~ i s ~ t h e ~ c o r r e c t ~ c l a s s ) } } \\ { ~ = ~ - \log \frac { \exp \left( { \bf w } _ { \mathrm { c } } \cdot { \bf x } + b _ { c } \right) } { \sum _ { j = 1 } ^ { K } \exp \left( { \bf w } _ { j } \cdot { \bf x } + b _ { j } \right) } ~ ( c \mathrm { ~ i s ~ t h e ~ c o r r e c t ~ c l a s s ) } } \end{array}
$$

How did we get from Eq. 5.45 to Eq. 5.46? Because only one class (let’s call it $c$ ) is the correct one, the vector $\pmb { y }$ takes the value 1 only for this value of $k$ , i.e., has $y _ { c } = 1$ and $y _ { j } = 0 \forall j \neq c$ . That means the terms in the sum in Eq. 5.45 will all be 0 except for the term corresponding to the true class $c$ . Hence the cross-entropy loss is simply the log of the output probability corresponding to the correct class, and we therefore also call Eq. 5.46 the negative log likelihood loss.

Of course for gradient descent we don’t need the loss, we need its gradient. The gradient for a single example turns out to be very similar to the gradient for binary logistic regression, $( { \hat { y } } - y ) x$ , that we saw in Eq. 5.30. Let’s consider one piece of the gradient, the derivative for a single weight. For each class $k$ , the weight of the ith element of input $\pmb { \times }$ is $w _ { k , i }$ . What is the partial derivative of the loss with respect to $w _ { k , i } 2$ This derivative turns out to be just the difference between the true value for the class $k$ (which is either 1 or 0) and the probability the classifier outputs for class $k$ , weighted by the value of the input $x _ { i }$ corresponding to the ith element of the weight vector for class $k$ :

$$
\begin{array} { r c l } { \displaystyle \frac { \partial L _ { \mathrm { C E } } } { \partial w _ { k , i } } ~ = ~ - ( y _ { k } - \hat { y } _ { k } ) x _ { i } } \\ { \displaystyle } & { = ~ - ( y _ { k } - p ( y _ { k } = 1 | x ) ) x _ { i } } \\ { \displaystyle } & { = ~ - \left( y _ { k } - \frac { \exp { ( \mathbf { w _ { k } \cdot x } + b _ { k } ) } } { \sum _ { j = 1 } ^ { K } \exp { ( \mathbf { w _ { j } \cdot x } + b _ { j } ) } } \right) x _ { i } } \end{array}
$$

We’ll return to this case of the gradient for softmax regression when we introduce neural networks in Chapter 7, and at that time we’ll also discuss the derivation of this gradient in equations Eq. 7.33–Eq. 7.41.

# 5.9 Interpreting models

interpretable

Often we want to know more than just the correct classification of an observation. We want to know why the classifier made the decision it did. That is, we want our decision to be interpretable. Interpretability can be hard to define strictly, but the core idea is that as humans we should know why our algorithms reach the conclusions they do. Because the features to logistic regression are often human-designed, one way to understand a classifier’s decision is to understand the role each feature plays in the decision. Logistic regression can be combined with statistical tests (the likelihood ratio test, or the Wald test); investigating whether a particular feature is significant by one of these tests, or inspecting its magnitude (how large is the weight $w$ associated with the feature?) can help us interpret why the classifier made the decision it makes. This is enormously important for building transparent models.

Furthermore, in addition to its use as a classifier, logistic regression in NLP and many other fields is widely used as an analytic tool for testing hypotheses about the effect of various explanatory variables (features). In text classification, perhaps we want to know if logically negative words (no, not, never) are more likely to be associated with negative sentiment, or if negative reviews of movies are more likely to discuss the cinematography. However, in doing so it’s necessary to control for potential confounds: other factors that might influence sentiment (the movie genre, the year it was made, perhaps the length of the review in words). Or we might be studying the relationship between NLP-extracted linguistic features and non-linguistic outcomes (hospital readmissions, political outcomes, or product sales), but need to control for confounds (the age of the patient, the county of voting, the brand of the product). In such cases, logistic regression allows us to test whether some feature is associated with some outcome above and beyond the effect of other features.

# 5.10 Advanced: Deriving the Gradient Equation

In this section we give the derivation of the gradient of the cross-entropy loss function $L _ { \mathrm { C E } }$ for logistic regression. Let’s start with some quick calculus refreshers. First, the derivative of $\ln ( x )$ :

$$
{ \frac { d } { d x } } \ln ( x ) = { \frac { 1 } { x } }
$$

Second, the (very elegant) derivative of the sigmoid:

$$
\frac { d \sigma ( z ) } { d z } = \sigma ( z ) ( 1 - \sigma ( z ) )
$$

Finally, the chain rule of derivatives. Suppose we are computing the derivative of a composite function $f ( x ) = u ( \nu ( x ) )$ . The derivative of $f ( x )$ is the derivative of $u ( x )$ with respect to $\nu ( x )$ times the derivative of $\nu ( x )$ with respect to $x$ :

$$
{ \frac { d f } { d x } } \ = \ { \frac { d u } { d \nu } } \cdot { \frac { d \nu } { d x } }
$$

First, we want to know the derivative of the loss function with respect to a single weight $w _ { j }$ (we’ll need to compute it for each weight, and for the bias):

$$
\begin{array} { l } { { \displaystyle \frac { \partial { \cal L } _ { \mathrm { C E } } } { \partial w _ { j } } ~ = ~ \frac { \partial } { \partial w _ { j } } - \left[ y \log \sigma ( { \bf w } \cdot { \bf x } + b ) + ( 1 - y ) \log \left( 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) \right) \right] } } \\ { { ~ = ~ - \left[ \frac { \partial } { \partial w _ { j } } y \log \sigma ( { \bf w } \cdot { \bf x } + b ) + \frac { \partial } { \partial w _ { j } } ( 1 - y ) \log \left[ 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) \right] \right] } } \end{array}
$$

Next, using the chain rule, and relying on the derivative of log:

$$
\frac { \partial L _ { \mathrm { C E } } } { \partial w _ { j } } \ = \ - \frac { y } { \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) } \frac { \partial } { \partial w _ { j } } \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) - \frac { 1 - y } { 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) } \frac { \partial } { \partial w _ { j } } 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b )
$$

Rearranging terms:

$$
\frac { \partial L _ { \mathrm { C E } } } { \partial w _ { j } } \ = \ - \left[ \frac { y } { \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) } - \frac { 1 - y } { 1 - \sigma ( \mathbf { w } \cdot \mathbf { x } + b ) } \right] \frac { \partial } { \partial w _ { j } } \sigma ( \mathbf { w } \cdot \mathbf { x } + b )
$$

And now plugging in the derivative of the sigmoid, and using the chain rule one more time, we end up with Eq. 5.55:

$$
\begin{array} { l l } { \displaystyle \frac { \partial L _ { \mathrm { C E } } } { \partial w _ { j } } } & { = \displaystyle - \left[ \frac { y - \sigma \big ( { \bf w } \cdot { \bf x } + b \big ) } { \sigma ( { \bf w } \cdot { \bf x } + b ) [ 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) ] } \right] \sigma ( { \bf w } \cdot { \bf x } + b ) [ 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) ] \frac { \partial ( { \bf w } \cdot { \bf x } + b ) } { \partial w _ { j } } } \\ & { = \displaystyle - \left[ \frac { y - \sigma \big ( { \bf w } \cdot { \bf x } + b \big ) } { \sigma ( { \bf w } \cdot { \bf x } + b ) [ 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) ] } \right] \sigma ( { \bf w } \cdot { \bf x } + b ) [ 1 - \sigma ( { \bf w } \cdot { \bf x } + b ) ] x _ { j } } \\ & { = \displaystyle - [ y - \sigma ( { \bf w } \cdot { \bf x } + b ) ] x _ { j } } \\ & { = \ : [ \sigma ( { \bf w } \cdot { \bf x } + b ) - y ] x _ { j } } \end{array}
$$

# 5.11 Summary

This chapter introduced the logistic regression model of classification.

• Logistic regression is a supervised machine learning classifier that extracts real-valued features from the input, multiplies each by a weight, sums them, and passes the sum through a sigmoid function to generate a probability. A threshold is used to make a decision.

• Logistic regression can be used with two classes (e.g., positive and negative sentiment) or with multiple classes (multinomial logistic regression, for example for n-ary text classification, part-of-speech labeling, etc.).   
• Multinomial logistic regression uses the softmax function to compute probabilities.   
• The weights (vector $w$ and bias $b$ ) are learned from a labeled training set via a loss function, such as the cross-entropy loss, that must be minimized.   
• Minimizing this loss function is a convex optimization problem, and iterative algorithms like gradient descent are used to find the optimal weights.   
• Regularization is used to avoid overfitting.   
• Logistic regression is also one of the most useful analytic tools, because of its ability to transparently study the importance of individual features.

# Bibliographical and Historical Notes

Logistic regression was developed in the field of statistics, where it was used for the analysis of binary data by the 1960s, and was particularly common in medicine (Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one of the formal foundations of the study of linguistic variation (Sankoff and Labov, 1979).

Nonetheless, logistic regression didn’t become common in natural language processing until the 1990s, when it seems to have appeared simultaneously from two directions. The first source was the neighboring fields of information retrieval and speech processing, both of which had made use of regression, and both of which lent many other statistical techniques to NLP. Indeed a very early use of logistic regression for document routing was one of the first NLP applications to use (LSI) embeddings as word representations (Schutze et al. ¨ , 1995).

# maximum entropy

At the same time in the early 1990s logistic regression was developed and applied to NLP at IBM Research under the name maximum entropy modeling or maxent (Berger et al., 1996), seemingly independent of the statistical literature. Under that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution (Kehler, 1997b), and text classification (Nigam et al., 1999).

More on classification can be found in machine learning textbooks (Hastie et al. 2001, Witten and Frank 2005, Bishop 2006, Murphy 2012).

# Exercises

# 6

# Vector Semantics and Embeddings

荃者 以在鱼， 鱼而 荃 Nets are for fish; Once you get the fish, you can forget the net.   
者 以在意， 意而 Words are for meaning; Once you get the meaning, you can forget the words

庄子(Zhuangzi), Chapter 26

The asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly recognizable by its long canines. Five million years ago or so, a completely different saber-tooth tiger called Thylacosmilus lived in Argentina and other parts of South America. Thylacosmilus was a marsupial whereas Smilodon was a placental mammal, but Thylacosmilus had the same long upper canines and, like Smilodon, had a protective bone flange on the lower jaw. The similarity of these two mammals is one of many examples

vector semantics embeddings

of parallel or convergent evolution, in which particular contexts or environments lead to the evolution of very similar structures in different species (Gould, 1980).

![## Image Analysis: 5a69d66947050fa00ab6bc0072cd80def6699af15b007a5e93fa70875e3a82cf.jpg

**Conceptual Understanding:**
The image represents a realistic, artistic depiction of a sabertooth tiger, likely a Smilodon, one of the most famous prehistoric apex predators. Conceptually, it illustrates ancient life, extinction, and the untamed power of nature during the Ice Age. The main purpose of the image is to visually portray this extinct animal in a dynamic and striking manner, evoking a sense of its ferocity and majesty in its natural habitat. It communicates the key ideas of prehistoric wildlife, the visual characteristics of a sabertooth, and the raw, powerful aspects of ancient ecosystems.

**Content Interpretation:**
The image exclusively depicts a sabertooth tiger (Smilodon) in a natural, rugged environment, specifically on a cliff edge. It focuses on illustrating the physical characteristics and presumed demeanor of this extinct apex predator. The open mouth and exposed canines are significant, emphasizing its predatory nature and strength. No processes, relationships, or systems are explicitly shown; rather, it is a static, artistic representation. The significance lies in visually bringing to life a creature from the Pleistocene epoch, allowing viewers to conceptualize its appearance and habitat, and possibly its formidable presence in the ancient ecosystem. The artistic style, with its dramatic lighting and natural setting, enhances the creature's perceived power and wildness.

**Key Insights:**
The main takeaway from this image is a visual understanding of a sabertooth tiger (Smilodon), an iconic extinct mammal. It teaches about its characteristic features, such as the prominent canines, and suggests its natural habitat on rocky, wild terrains. The painting conveys a sense of the formidable power and wildness associated with these prehistoric predators. It serves as a visual reminder of Earth's ancient megafauna. No specific conclusions or insights directly related to 'Vector Semantics and Embeddings' can be drawn from the image content itself, as it is a representational artwork rather than a diagram or data visualization. The knowledge extracted is purely zoological and historical in nature, relating to the appearance and suggested behavior of a Smilodon.

**Document Context:**
Given the document context 'Vector Semantics and Embeddings,' the image, being a painting of a sabertooth tiger, does not directly depict concepts related to vector semantics or embeddings. However, it could serve as a visual example of an entity (a sabertooth tiger) that could be represented, classified, or embedded within a semantic space. For instance, an image of a sabertooth tiger could be analyzed by a computer vision model, and its features could then be mapped to a vector space for tasks like image retrieval, categorization, or understanding its relationship to other animal concepts based on visual similarity or textual descriptions that might accompany such an image in a dataset. Without explicit accompanying text or a caption that links it to the section's theme, its immediate relevance is as a visual subject that can be processed and understood through computational methods, potentially generating semantic vectors.

**Summary:**
The image is a painting depicting a sabertooth tiger (Smilodon) standing prominently on a rugged cliff edge. The animal, which is the central focus, is shown in profile, facing towards the right side of the frame with its mouth open in what appears to be a snarl or roar, clearly showcasing its iconic elongated canines. Its fur is a tawny brown, and its powerful, muscular body is captured in a dynamic stance as if in motion or poised for action. To the left, a dense cluster of dark green foliage or trees partially frames the scene, suggesting a natural, wild environment. The cliff face itself is textured with shades of brown and ochre, indicating rock and earth. In the background, beyond the precipice, a vast, expansive landscape stretches under a dramatic sky. The sky transitions from a lighter, perhaps yellowish or pale blue hue near the horizon to deeper shades of blue and purple, with hints of clouds or atmospheric conditions, possibly suggesting either dawn or dusk. The overall impression is one of untamed wilderness and the formidable power of a prehistoric predator.](images/5a69d66947050fa00ab6bc0072cd80def6699af15b007a5e93fa70875e3a82cf.jpg)

The role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was first formulated in the 1950s by linguists like Joos (1950), Harris (1954), and Firth (1957), who noticed that words which are synonyms (like oculist and eye-doctor) tended to occur in the same environment (e.g., near words like eye or examined) with the amount of meaning difference between two words “corresponding roughly to the amount of difference in their environments” (Harris, 1954, p. 157).

In this chapter we introduce vector semantics, which instantiates this linguistic hypothesis by learning representations of the meaning of words, called embeddings, directly from their distributions in texts. These representations are used in every natural language processing application that makes use of meaning, and the static embeddings we introduce here underlie the more powerful dynamic or contextualized embeddings like BERT that we will see in Chapter 11.

These word representations are also the first example in this book of representation learning, automatically learning useful representations of the input text. Finding such self-supervised ways to learn representations of the input, instead of creating representations by hand via feature engineering, is an important focus of NLP research (Bengio et al., 2013).

# 6.1 Lexical Semantics

Let’s begin by introducing some basic principles of word meaning. How should we represent the meaning of a word? In the n-gram models of Chapter 3, and in classical NLP applications, our only representation of a word is as a string of letters, or an index in a vocabulary list. This representation is not that different from a tradition in philosophy, perhaps you’ve seen it in introductory logic classes, in which the meaning of words is represented by just spelling the word with small capital letters; representing the meaning of “dog” as DOG, and “cat” as CAT, or by using an apostrophe (DOG’).

Representing the meaning of a word by capitalizing it is a pretty unsatisfactory model. You might have seen a version of a joke due originally to semanticist Barbara Partee (Carlson, 1977):

Q: What’s the meaning of life? A: LIFE’

lexical semantics

Surely we can do better than this! After all, we’ll want a model of word meaning to do all sorts of things for us. It should tell us that some words have similar meanings (cat is similar to dog), others are antonyms (cold is the opposite of hot), some have positive connotations (happy) while others have negative connotations (sad). It should represent the fact that the meanings of buy, sell, and pay offer differing perspectives on the same underlying purchasing event. (If I buy something from you, you’ve probably sold it to me, and I likely paid you.) More generally, a model of word meaning should allow us to draw inferences to address meaning-related tasks like question-answering or dialogue.

In this section we summarize some of these desiderata, drawing on results in the linguistic study of word meaning, which is called lexical semantics; we’ll return to and expand on this list in Appendix G and Chapter 21.

Lemmas and Senses Let’s start by looking at how one word (we’ll choose mouse) might be defined in a dictionary (simplified from the online dictionary WordNet): mouse (N)   
1. any of numerous small rodents...   
2. a hand-operated device that controls a cursor...

lemma citation form wordform

Here the form mouse is the lemma, also called the citation form. The form mouse would also be the lemma for the word mice; dictionaries don’t have separate definitions for inflected forms like mice. Similarly sing is the lemma for sing, sang, sung. In many languages the infinitive form is used as the lemma for the verb, so Spanish dormir “to sleep” is the lemma for duermes “you sleep”. The specific forms sung or carpets or sing or duermes are called wordforms.

As the example above shows, each lemma can have multiple meanings; the lemma mouse can refer to the rodent or the cursor control device. We call each of these aspects of the meaning of mouse a word sense. The fact that lemmas can be polysemous (have multiple senses) can make interpretation difficult (is someone who types “mouse info” into a search engine looking for a pet or a tool?). Chapter 11 and Appendix G will discuss the problem of polysemy, and introduce word sense disambiguation, the task of determining which sense of a word is being used in a particular context.

Synonymy One important component of word meaning is the relationship between word senses. For example when one word has a sense whose meaning is identical to a sense of another word, or nearly identical, we say the two senses of those two words are synonyms. Synonyms include such pairs as

# couch/sofa vomit/throw up filbert/hazelnut car/automobile

principle of contrast

A more formal definition of synonymy (between words rather than senses) is that two words are synonymous if they are substitutable for one another in any sentence without changing the truth conditions of the sentence, the situations in which the sentence would be true.

While substitutions between some pairs of words like car / automobile or water / $H _ { 2 } O$ are truth preserving, the words are still not identical in meaning. Indeed, probably no two words are absolutely identical in meaning. One of the fundamental tenets of semantics, called the principle of contrast (Girard 1718, Breal´ 1897, Clark 1987), states that a difference in linguistic form is always associated with some difference in meaning. For example, the word $H _ { 2 } O$ is used in scientific contexts and would be inappropriate in a hiking guide—water would be more appropriate— and this genre difference is part of the meaning of the word. In practice, the word synonym is therefore used to describe a relationship of approximate or rough synonymy.

similarity

Word Similarity While words don’t have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. In moving from synonymy to similarity, it will be useful to shift from talking about relations between word senses (like synonymy) to relations between words (like similarity). Dealing with words avoids having to commit to a particular representation of word senses, which will turn out to simplify our task.

The notion of word similarity is very useful in larger semantic tasks. Knowing how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement):

relatedness association

<table><tr><td>vanish</td><td>disappear impression 5.95</td><td>9.8</td></tr><tr><td>belief muscle</td><td>bone</td><td>3.65</td></tr><tr><td></td><td></td><td></td></tr><tr><td>modest</td><td>flexible</td><td>0.98</td></tr><tr><td>hole</td><td> agreement</td><td>0.3</td></tr></table>

Word Relatedness The meaning of two words can be related in ways other than similarity. One such class of connections is called word relatedness (Budanitsky and Hirst, 2006), also traditionally called word association in psychology.

Consider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).

One common kind of relatedness between words is if they belong to the same semantic field. A semantic field is a set of words which cover a particular semantic domain and bear structured relations with each other. For example, words might be

topic models

related by being in the semantic field of hospitals (surgeon, scalpel, nurse, anesthetic, hospital), restaurants (waiter, menu, plate, food, chef), or houses (door, roof, kitchen, family, bed). Semantic fields are also related to topic models, like Latent Dirichlet Allocation, LDA, which apply unsupervised learning on large sets of texts to induce sets of associated words from text. Semantic fields and topic models are very useful tools for discovering topical structure in documents.

In Appendix G we’ll introduce more relations between senses like hypernymy or IS-A, antonymy (opposites) and meronymy (part-whole relations).

semantic frame

Semantic Frames and Roles Closely related to semantic fields is the idea of a semantic frame. A semantic frame is a set of words that denote perspectives or participants in a particular type of event. A commercial transaction, for example, is a kind of event in which one entity trades money to another entity in return for some good or service, after which the good changes hands or perhaps the service is performed. This event can be encoded lexically by using verbs like buy (the event from the perspective of the buyer), sell (from the perspective of the seller), pay (focusing on the monetary aspect), or nouns like buyer. Frames have semantic roles (like buyer, seller, goods, money), and words in a sentence can take on these roles.

Knowing that buy and sell have this relation makes it possible for a system to know that a sentence like Sam bought the book from Ling could be paraphrased as Ling sold the book to Sam, and that Sam has the role of the buyer in the frame and Ling the seller. Being able to recognize such paraphrases is important for question answering, and can help in shifting perspective for machine translation.

# connotations

Connotation Finally, words have affective meanings or connotations. The word connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. For example some words have positive connotations (wonderful) while others have negative connotations (dreary). Even words whose meanings are similar in other ways can vary in connotation; consider the difference in connotations between fake, knockoff, forgery, on the one hand, and copy, replica, reproduction on the other, or innocent (positive connotation) and naive (negative connotation). Some words describe positive evaluation (great, love) and others negative evaluation (terrible, hate). Positive or negative evaluation language is called sentiment, as we saw in Chapter 4, and word sentiment plays a role in important tasks like sentiment analysis, stance detection, and applications of NLP to the language of politics and consumer reviews.

# sentiment

Early work on affective meaning (Osgood et al., 1957) found that words varied along three important dimensions of affective meaning:

valence: the pleasantness of the stimulus arousal: the intensity of emotion provoked by the stimulus dominance: the degree of control exerted by the stimulus

Thus words like happy or satisfied are high on valence, while unhappy or annoyed are low on valence. Excited is high on arousal, while calm is low on arousal. Controlling is high on dominance, while awed or influenced are low on dominance. Each word is thus represented by three numbers, corresponding to its value on each of the three dimensions:

<table><tr><td>Valence</td><td></td><td> Arousal Dominance</td></tr><tr><td>courageous 8.05</td><td>5.5</td><td>7.38</td></tr><tr><td>music 7.67</td><td>5.57</td><td>6.5</td></tr><tr><td>heartbreak 2.45</td><td>5.65</td><td>3.58</td></tr><tr><td>cub 6.71</td><td>3.95</td><td>4.24</td></tr></table>

Osgood et al. (1957) noticed that in using these 3 numbers to represent the meaning of a word, the model was representing each word as a point in a threedimensional space, a vector whose three dimensions corresponded to the word’s rating on the three scales. This revolutionary idea that word meaning could be represented as a point in space (e.g., that part of the meaning of heartbreak can be represented as the point [2.45, 5.65, 3.58]) was the first expression of the vector semantics models that we introduce next.

# 6.2 Vector Semantics

# vector semantics

Vector semantics is the standard way to represent word meaning in NLP, helping us model many of the aspects of word meaning we saw in the previous section. The roots of the model lie in the 1950s when two big ideas converged: Osgood’s 1957 idea mentioned above to use a point in three-dimensional space to represent the connotation of a word, and the proposal by linguists like Joos (1950), Harris (1954), and Firth (1957) to define the meaning of a word by its distribution in language use, meaning its neighboring words or grammatical environments. Their idea was that two words that occur in very similar distributions (whose neighboring words are similar) have similar meanings.

For example, suppose you didn’t know the meaning of the word ongchoi (a recent borrowing from Cantonese) but you see it in the following contexts:

(6.1) Ongchoi is delicious sauteed with garlic.   
(6.2) Ongchoi is superb over rice.   
(6.3) ...ongchoi leaves with salty sauces...

And suppose that you had seen many of these context words in other contexts:

(6.4) ...spinach sauteed with garlic over rice... (6.5) ...chard stems and leaves are delicious... (6.6) ...collard greens and other salty leafy greens

The fact that ongchoi occurs with words like rice and garlic and delicious and salty, as do words like spinach, chard, and collard greens might suggest that ongchoi is a leafy green similar to these other leafy greens.1 We can do the same thing computationally by just counting words in the context of ongchoi.

The idea of vector semantics is to represent a word as a point in a multidimensional semantic space that is derived (in ways we’ll see) from the distributions of word neighbors. Vectors for representing words are called embeddings (although the term is sometimes more strictly applied only to dense vectors like word2vec (Section 6.8), rather than sparse tf-idf or PPMI vectors (Section 6.3-Section 6.6)). The word “embedding” derives from its mathematical sense as a mapping from one space or structure to another, although the meaning has shifted; see the end of the chapter.

![## Image Analysis: 902f44b9cb3a7e1a82001458d2f255037076f761f67867099d8b6e7d7ba532d7.jpg

**Conceptual Understanding:**
The image conceptually represents the visualization of word embeddings in a lower-dimensional space. It illustrates the principle of semantic similarity, where words and phrases that share similar meanings or sentiment are mapped to nearby points. The main purpose is to demonstrate that word embeddings, especially those trained for sentiment analysis, can effectively capture the semantic relationships between words, including their sentiment polarity. The key idea communicated is that linguistic meaning can be represented mathematically in a vector space, and these representations allow for the quantification and visualization of semantic relationships, such as grouping words by sentiment (positive, negative, neutral).

**Content Interpretation:**
The image displays a two-dimensional t-SNE (t-distributed Stochastic Neighbor Embedding) projection of word and phrase embeddings. This projection is a visualization of high-dimensional data (original 60-dimensional embeddings) in a lower, more interpretable dimension (2D). The content shows three distinct clusters of words and phrases, color-coded: 1. Blue cluster: Located in the top-left and middle-left, these words are primarily functional words or pronouns, such as 'to', 'by', 'that', 'now', 'a', 'i', 'than', 'with', ''s', 'are', 'you', 'is'. This cluster signifies a group of words that typically carry less strong sentiment and serve grammatical or general connective purposes. 2. Red cluster: Positioned in the top-right, these words express negative sentiment, including 'not good', 'bad', 'dislike', 'worst', 'incredibly bad', 'worse'. The proximity of these words to each other indicates their strong semantic similarity in terms of negativity. 3. Green cluster: Located in the bottom-middle and bottom-right, these words convey positive sentiment, such as 'very good', 'incredibly good', 'amazing', 'terrific', 'fantastic', 'wonderful', 'nice', 'good'. Their close grouping reflects their shared positive semantic meaning. The significance of this visualization is to demonstrate that word embeddings, trained for tasks like sentiment analysis, effectively capture semantic relationships. Words with similar meanings, or in this case, similar sentiment polarities, are represented by vectors that are close in the embedding space. The t-SNE projection makes these high-dimensional relationships visible in an intuitive two-dimensional plot, confirming that sentiment is a prominent dimension captured by the embeddings.

**Key Insights:**
1. Word embeddings effectively capture semantic similarity: Words and phrases with similar meanings (e.g., 'amazing' and 'fantastic') are clustered together in the embedding space. 2. Sentiment is a quantifiable dimension in word embeddings: The clear separation of positive (green), negative (red), and neutral/functional (blue) word clusters demonstrates that sentiment is a strong semantic feature embedded in these representations. 3. Dimensionality reduction techniques like t-SNE can make complex high-dimensional relationships interpretable: The original 60-dimensional embeddings are made visually understandable in a 2D projection, revealing underlying semantic structures. 4. Functional words form their own distinct cluster, separate from sentiment-laden words, indicating their unique semantic role. These insights are directly supported by the distinct and coherent clustering of words based on their sentiment or functional role, as explicitly transcribed in the content interpretation.

**Document Context:**
This image directly supports the 'vector semantics' section by visually demonstrating how words are represented as vectors in a semantic space, and how these vector representations can capture meaning, specifically sentiment. It illustrates the concept that words with similar meanings or semantic functions are geographically closer in this vector space. The image provides concrete visual evidence for the abstract concept of 'embeddings' mentioned in the surrounding text, showing a simplified projection of 60-dimensional embeddings used for sentiment analysis. The colors added for explanation enhance the understanding of the sentiment categories.

**Summary:**
This image displays a two-dimensional t-SNE projection of word and phrase embeddings, illustrating how words with similar meanings are positioned closely together in space. The words are color-coded to highlight different semantic clusters, specifically relating to sentiment. Neutral or functional words are shown in blue, negative sentiment words in red, and positive sentiment words in green. The spatial arrangement clearly demonstrates that the embedding process effectively groups words by their semantic properties, particularly their sentiment polarity. For instance, words like 'amazing' and 'wonderful' are clustered closely and colored green, indicating positive sentiment, while 'bad' and 'worst' are clustered in red, representing negative sentiment. Functional words like 'to' and 'that' form a separate cluster in blue, showing their distinct semantic role.](images/902f44b9cb3a7e1a82001458d2f255037076f761f67867099d8b6e7d7ba532d7.jpg)
Figure 6.1 A two-dimensional (t-SNE) projection of embeddings for some words and phrases, showing that words with similar meanings are nearby in space. The original 60- dimensional embeddings were trained for sentiment analysis. Simplified from Li et al. (2015) with colors added for explanation.

Fig. 6.1 shows a visualization of embeddings learned for sentiment analysis, showing the location of selected words projected down from 60-dimensional space into a two dimensional space. Notice the distinct regions containing positive words, negative words, and neutral function words.

The fine-grained model of word similarity of vector semantics offers enormous power to NLP applications. NLP applications like the sentiment classifiers of Chapter 4 or Chapter 5 depend on the same words appearing in the training and test sets. But by representing words as embeddings, a classifier can assign sentiment as long as it sees some words with similar meanings. And as we’ll see, vector semantic models can be learned automatically from text without supervision.

In this chapter we’ll introduce the two most commonly used models. In the tf-idf model, an important baseline, the meaning of a word is defined by a simple function of the counts of nearby words. We will see that this method results in very long vectors that are sparse, i.e. mostly zeros (since most words simply never occur in the context of others). We’ll introduce the word2vec model family for constructing short, dense vectors that have useful semantic properties. We’ll also introduce the cosine, the standard way to use embeddings to compute semantic similarity, between two words, two sentences, or two documents, an important tool in practical applications like question answering, summarization, or automatic essay grading.

# 6.3 Words and Vectors

“The most important attributes of a vector in 3-space are {Location, Location, Location}” Randall Munroe, https://xkcd.com/2358/

Vector or distributional models of meaning are generally based on a co-occurrence matrix, a way of representing how often words co-occur. We’ll look at two popular matrices: the term-document matrix and the term-term matrix.

# 6.3.1 Vectors and documents

In a term-document matrix, each row represents a word in the vocabulary and each column represents a document from some collection of documents. Fig. 6.2 shows a small selection from a term-document matrix showing the occurrence of four words in four plays by Shakespeare. Each cell in this matrix represents the number of times

a particular word (defined by the row) occurs in a particular document (defined by the column). Thus fool appeared 58 times in Twelfth Night.   

<table><tr><td></td><td>As You Like It</td><td> Twelfth Night</td><td> Julius Caesar</td><td>Henry V</td></tr><tr><td>battle</td><td>1</td><td>0</td><td>7</td><td>13</td></tr><tr><td>good</td><td>114</td><td>80</td><td>62</td><td>89</td></tr><tr><td>fool</td><td>36</td><td>58</td><td>1</td><td>4</td></tr><tr><td>wit</td><td>20</td><td>15</td><td>2</td><td>3</td></tr></table>

Figure 6.2 The term-document matrix for four words in four Shakespeare plays. Each cell contains the number of times the (row) word occurs in the (column) document.

The term-document matrix of Fig. 6.2 was first defined as part of the vector space model of information retrieval (Salton, 1971). In this model, a document is represented as a count vector, a column in Fig. 6.3.

vector

To review some basic linear algebra, a vector is, at heart, just a list or array of numbers. So As You Like It is represented as the list [1,114,36,20] (the first column vector in Fig. 6.3) and Julius Caesar is represented as the list [7,62,1,2] (the third column vector). A vector space is a collection of vectors, and is characterized by its dimension. Vectors in a 3-dimensional vector space have an element for each dimension of the space. We will loosely refer to a vector in a 4-dimensional space as a 4-dimensional vector, with one element along each dimension. In the example in Fig. 6.3, we’ve chosen to make the document vectors of dimension 4, just so they fit on the page; in real term-document matrices, the document vectors would have dimensionality $| V |$ , the vocabulary size.

The ordering of the numbers in a vector space indicates the different dimensions on which documents vary. The first dimension for both these vectors corresponds to the number of times the word battle occurs, and we can compare each dimension, noting for example that the vectors for As You Like It and Twelfth Night have similar values (1 and 0, respectively) for the first dimension.

<table><tr><td></td><td> As You Like It</td><td>Twelfth Night</td><td></td><td> Julius Caesar</td><td>Henry V</td></tr><tr><td>battle</td><td>£4 30</td><td></td><td>0 30585</td><td></td><td>3843</td></tr><tr><td> good</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>fool</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>wit</td><td></td><td></td><td></td><td></td><td></td></tr></table>

Figure 6.3 The term-document matrix for four words in four Shakespeare plays. The red boxes show that each document is represented as a column vector of length four.

We can think of the vector for a document as a point in $| V |$ -dimensional space; thus the documents in Fig. 6.3 are points in 4-dimensional space. Since 4-dimensional spaces are hard to visualize, Fig. 6.4 shows a visualization in two dimensions; we’ve arbitrarily chosen the dimensions corresponding to the words battle and fool.

Term-document matrices were originally defined as a means of finding similar documents for the task of document information retrieval. Two documents that are similar will tend to have similar words, and if two documents have similar words their column vectors will tend to be similar. The vectors for the comedies As You Like It [1,114,36,20] and Twelfth Night [0,80,58,15] look a lot more like each other (more fools and wit than battles) than they look like Julius Caesar [7,62,1,2] or Henry V [13,89,4,3]. This is clear with the raw numbers; in the first dimension (battle) the comedies have low numbers and the others have high numbers, and we can see it visually in Fig. 6.4; we’ll see very shortly how to quantify this intuition more formally.

![## Image Analysis: 6a5d1e1b43b4a933e3d0647bb3a1737ab592abb592339d1e2b1eaef1b7cace63.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified vector space model used in natural language processing and information retrieval. Its main purpose is to visually demonstrate how text documents (specifically Shakespearean plays) can be represented as points or vectors in a multi-dimensional space, where each dimension corresponds to the frequency of a particular word. The key idea being communicated is that by quantifying word occurrences, documents can be mapped into a geometric space, allowing for the analysis of their semantic similarity or thematic grouping based on their proximity or direction in that space. In this specific case, it illustrates the use of 'battle' and 'fool' as distinguishing features for different plays, separating those with more conflict-oriented themes from those with more comedic or light-hearted themes.

**Content Interpretation:**
The image displays a two-dimensional vector space model for text documents, specifically four Shakespearean plays. The 'fool' dimension (x-axis) quantifies the occurrence of the word 'fool', while the 'battle' dimension (y-axis) quantifies the occurrence of the word 'battle'. Each play is represented as a vector (point) in this space, with its coordinates reflecting the frequency counts of these two words within the document. The blue arrows originating from the origin (0,0) to each play's label visually depict these document vectors. For instance, "Henry V [4,13]" indicates 4 occurrences of 'fool' and 13 of 'battle'. This visualization directly supports the concept of representing textual content as numerical vectors to enable quantitative analysis and comparison between documents. The separation of the plays into two distinct groups based on these dimensions suggests a thematic difference, aligning with the plays' known genres.

**Key Insights:**
The main takeaway is that textual documents can be effectively represented as numerical vectors based on the frequencies of specific words. This vector representation allows for the spatial visualization and comparison of documents. The image demonstrates that word frequencies can serve as features to differentiate between documents, potentially reflecting their genre or thematic content. For example, plays like "As You Like It [36,1]" and "Twelfth Night [58,0]" are clearly separated from "Henry V [4,13]" and "Julius Caesar [1,7]" based on the 'fool' and 'battle' dimensions. This empirically shows how the occurrence of certain words ('fool' often associated with comedies, 'battle' with histories/tragedies) can cluster documents with similar characteristics, validating the use of vector space models for document classification or similarity analysis. The comedies are distinctly positioned by their high 'fool' and low 'battle' counts, while the other plays show the opposite trend.

**Document Context:**
This image directly illustrates the concept introduced in Section 6.3.1, 'Vectors and documents,' by providing a concrete example of how documents (Shakespearean plays) can be transformed into numerical vectors. It makes the abstract idea of a 'document vector' tangible by plotting them in a simple 2D space. The text accompanying the figure, 'Figure 6.4 A spatial visualization of the document vectors for the four Shakespeare play documents, showing just two of the dimensions, corresponding to the words battle and fool. The comedies have high values for the fool dimension and low values for the battle dimension,' provides crucial context. The visualization visually confirms this statement, showing the comedies ('As You Like It' and 'Twelfth Night') indeed having high 'fool' values and low 'battle' values, aligning them with the broader narrative of text analysis techniques.

**Summary:**
The image is a 2D scatter plot, or a spatial visualization of document vectors, illustrating how four Shakespearean plays are represented in a vector space defined by the frequency of two specific words: "fool" and "battle". The horizontal axis is labeled "fool" and ranges from 0 to 60, with major tick marks every 5 units. The vertical axis is labeled "battle" and ranges from 0 up to 40, with major tick marks at 5, 10, 15, and 40. Each play is represented as a point (and implicitly a vector from the origin), with its name and corresponding coordinates: "Henry V [4,13]" is positioned at (fool=4, battle=13), indicating a relatively low frequency of "fool" and a higher frequency of "battle". "Julius Caesar [1,7]" is at (fool=1, battle=7), showing an even lower "fool" frequency and a moderate "battle" frequency. "As You Like It [36,1]" is at (fool=36, battle=1), indicating a high frequency of "fool" and a very low frequency of "battle". Finally, "Twelfth Night [58,0]" is at (fool=58, battle=0), showing the highest frequency of "fool" and a zero frequency of "battle". Blue arrows originate from the (0,0) point, pointing towards each play's respective coordinate, visually representing the vectors. The visualization clearly separates plays, with "Henry V" and "Julius Caesar" clustered towards the "battle" axis, and "As You Like It" and "Twelfth Night" clustered towards the "fool" axis.](images/6a5d1e1b43b4a933e3d0647bb3a1737ab592abb592339d1e2b1eaef1b7cace63.jpg)
Figure 6.4 A spatial visualization of the document vectors for the four Shakespeare play documents, showing just two of the dimensions, corresponding to the words battle and fool. The comedies have high values for the fool dimension and low values for the battle dimension.

A real term-document matrix, of course, wouldn’t just have 4 rows and columns, let alone 2. More generally, the term-document matrix has $| V |$ rows (one for each word type in the vocabulary) and $D$ columns (one for each document in the collection); as we’ll see, vocabulary sizes are generally in the tens of thousands, and the number of documents can be enormous (think about all the pages on the web).

Information retrieval (IR) is the task of finding the document $d$ from the $D$ documents in some collection that best matches a query $q$ . For IR we’ll therefore also represent a query by a vector, also of length $| V |$ , and we’ll need a way to compare two vectors to find how similar they are. (Doing IR will also require efficient ways to store and manipulate these vectors by making use of the convenient fact that these vectors are sparse, i.e., mostly zeros).

Later in the chapter we’ll introduce some of the components of this vector comparison process: the tf-idf term weighting, and the cosine similarity metric.

# 6.3.2 Words as vectors: document dimensions

We’ve seen that documents can be represented as vectors in a vector space. But vector semantics can also be used to represent the meaning of words. We do this by associating each word with a word vector— a row vector rather than a column vector, hence with different dimensions, as shown in Fig. 6.5. The four dimensions of the vector for fool, [36,58,1,4], correspond to the four Shakespeare plays. Word counts in the same four dimensions are used to form the vectors for the other 3 words: wit, [20,15,2,3]; battle, [1,0,7,13]; and good [114,80,62,89].

<table><tr><td></td><td>As You Like It</td><td>Twelfth Night</td><td>Julius Caesar</td><td>HenryV</td></tr><tr><td>battle</td><td></td><td>0</td><td></td><td>13</td></tr><tr><td> good</td><td>114</td><td>80</td><td>62</td><td>89</td></tr><tr><td>fool</td><td>36</td><td>58</td><td></td><td>4</td></tr><tr><td>wit</td><td>C20</td><td>15</td><td></td><td>3</td></tr></table>

Figure 6.5 The term-document matrix for four words in four Shakespeare plays. The red boxes show that each word is represented as a row vector of length four.

For documents, we saw that similar documents had similar vectors, because similar documents tend to have similar words. This same principle applies to words: similar words have similar vectors because they tend to occur in similar documents. The term-document matrix thus lets us represent the meaning of a word by the documents it tends to occur in.

# 6.3.3 Words as vectors: word dimensions

An alternative to using the term-document matrix to represent words as vectors of document counts, is to use the term-term matrix, also called the word-word matrix or the term-context matrix, in which the columns are labeled by words rather than documents. This matrix is thus of dimensionality $| V | \times | V |$ and each cell records the number of times the row (target) word and the column (context) word co-occur in some context in some training corpus. The context could be the document, in which case the cell represents the number of times the two words appear in the same document. It is most common, however, to use smaller contexts, generally a window around the word, for example of 4 words to the left and 4 words to the right, in which case the cell represents the number of times (in some training corpus) the column word occurs in such a $\pm 4$ word window around the row word. Here are four examples of words in their windows:

is traditionally followed by cherry pie, a traditional dessert often mixed, such as strawberry rhubarb pie. Apple pie computer peripherals and personal digital assistants. These devices usually a computer. This includes information available on the internet

If we then take every occurrence of each word (say strawberry) and count the context words around it, we get a word-word co-occurrence matrix. Fig. 6.6 shows a simplified subset of the word-word co-occurrence matrix for these four words computed from the Wikipedia corpus (Davies, 2015).

<table><tr><td></td><td>aardvark</td><td>·</td><td>computer</td><td>data</td><td>result</td><td>pie</td><td> sugar</td><td>·</td></tr><tr><td rowspan="3">cherry strawberry</td><td>0</td><td></td><td>2</td><td>8</td><td>9</td><td>442</td><td>25</td><td></td></tr><tr><td>0</td><td></td><td>0</td><td>0</td><td>1</td><td>60</td><td>19</td><td></td></tr><tr><td>0</td><td></td><td>1670</td><td>1683</td><td>85</td><td>5</td><td>4</td><td></td></tr><tr><td>digital information</td><td>0</td><td>·</td><td>3325</td><td>3982</td><td>378</td><td>5</td><td>13</td><td></td></tr></table>

Figure 6.6 Co-occurrence vectors for four words in the Wikipedia corpus, showing six of the dimensions (hand-picked for pedagogical purposes). The vector for digital is outlined in red. Note that a real vector would have vastly more dimensions and thus be much sparser.

Note in Fig. 6.6 that the two words cherry and strawberry are more similar to each other (both pie and sugar tend to occur in their window) than they are to other words like digital; conversely, digital and information are more similar to each other than, say, to strawberry. Fig. 6.7 shows a spatial visualization.

![## Image Analysis: 379649e1887170bdbf61a9b00a7e7c89a8432949aa939f0c009eb80584527ed4.jpg

**Conceptual Understanding:**
This image conceptually represents the idea of "word vectors" or "word embeddings" in natural language processing. Its main purpose is to spatially visualize how different words can be positioned in a multi-dimensional semantic space. Specifically, it illustrates the words "digital" and "information" as vectors within a two-dimensional plane defined by the semantic dimensions "data" (x-axis) and "computer" (y-axis). The image communicates the key idea that words can be represented numerically, and their positions in this vector space reflect their semantic relationships and associations with the defining dimensions. The coordinates associated with each word vector (`[1683,1670]` for 'digital' and `[3982,3325]` for 'information') provide a quantitative measure of their meaning along these specific dimensions, demonstrating how semantic properties are encoded numerically.

**Content Interpretation:**
The image displays a two-dimensional representation of word vectors, specifically for the words 'digital' and 'information'. These vectors are plotted in a semantic space defined by two dimensions: 'data' (x-axis) and 'computer' (y-axis). The purpose is to visually demonstrate how word embeddings can quantify the semantic relationship between words.

The word 'digital' is represented by a green vector pointing to the coordinates [1683,1670]. This indicates that 'digital' has a score of 1683 in the 'data' dimension and 1670 in the 'computer' dimension. This suggests a moderate association with both concepts.

The word 'information' is represented by a blue vector pointing to the coordinates [3982,3325]. This indicates that 'information' has a higher score of 3982 in the 'data' dimension and 3325 in the 'computer' dimension, implying a stronger association with these concepts compared to 'digital' in this specific two-dimensional projection. The longer length of the 'information' vector also suggests a greater overall magnitude or relevance in this semantic space.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Words as Vectors:** The core idea is that words can be numerically represented as vectors in a multi-dimensional space, capturing their semantic properties. This is evidenced by the explicit labels 'data' and 'computer' as dimensions and the words 'digital' and 'information' as vectors with specific coordinate values.
2.  **Semantic Dimensions:** Each axis represents a semantic dimension (e.g., 'data', 'computer'), and a word's coordinate along an axis indicates its association with that dimension. For example, 'digital' at `[1683,1670]` shows its numerical scores in relation to 'data' and 'computer'.
3.  **Quantifiable Relationships:** The coordinates `[1683,1670]` for 'digital' and `[3982,3325]` for 'information' provide a quantitative measure of their positions within this semantic space. These numbers allow for comparison of semantic proximity or strength of association.
4.  **Relative Association:** By comparing the vectors, it can be observed that 'information' (coordinates `[3982,3325]`) has higher values along both the 'data' and 'computer' dimensions than 'digital' (coordinates `[1683,1670]`). This suggests that, in this specific two-dimensional representation, 'information' is more strongly associated with 'data' and 'computer' than 'digital' is. The longer vector for 'information' further supports a stronger overall presence or relevance in this particular semantic context.

**Document Context:**
This image directly supports the document section "6.3.3 Words as vectors: word dimensions" by providing a concrete visual example of the abstract concept of word vectors. The text immediately following the image, "Figure 6.7 A spatial visualization of word vectors for digital and information, showing just two of the dimensions, corresponding to the words data and computer," explicitly states the image's purpose and content. It serves to illustrate how words like 'digital' and 'information' can be mapped into a multi-dimensional space (here, reduced to two dimensions of 'data' and 'computer') where their positions and relationships convey semantic meaning. This helps readers grasp the practical application of word embeddings in representing the semantic properties of words.

**Summary:**
This image is a two-dimensional Cartesian coordinate plot titled "A spatial visualization of word vectors for digital and information, showing just two of the dimensions, corresponding to the words data and computer." The horizontal axis (x-axis) is labeled "data" and ranges from 0 to 4000, with tick marks at 1000, 2000, 3000, and 4000. The vertical axis (y-axis) is labeled "computer" and also ranges from 0 to 4000, with tick marks at 1000, 2000, 3000, and 4000. Two vectors originate from the origin (0,0).

The first vector, colored green, is labeled "digital" and points to the coordinate [1683,1670]. This indicates that the word "digital" has a value of 1683 along the 'data' dimension and 1670 along the 'computer' dimension.

The second vector, colored blue, is labeled "information" and points to the coordinate [3982,3325]. This signifies that the word "information" has a value of 3982 along the 'data' dimension and 3325 along the 'computer' dimension.

The plot visually represents how words are embedded in a semantic space, using numerical vectors to quantify their relationships to specific dimensions (in this case, 'data' and 'computer'). The relative positions and magnitudes of the vectors for 'digital' and 'information' demonstrate their semantic associations within this defined space.](images/379649e1887170bdbf61a9b00a7e7c89a8432949aa939f0c009eb80584527ed4.jpg)
Figure 6.7 A spatial visualization of word vectors for digital and information, showing just two of the dimensions, corresponding to the words data and computer.

Note that $| V |$ , the dimensionality of the vector, is generally the size of the vocabulary, often between 10,000 and 50,000 words (using the most frequent words in the training corpus; keeping words after about the most frequent 50,000 or so is generally not helpful). Since most of these numbers are zero these are sparse vector representations; there are efficient algorithms for storing and computing with sparse matrices.

Now that we have some intuitions, let’s move on to examine the details of computing word similarity. Afterwards we’ll discuss methods for weighting cells.

# 6.4 Cosine for measuring similarity

dot product inner product

To measure similarity between two target words $\nu$ and $w$ , we need a metric that takes two vectors (of the same dimensionality, either both with words as dimensions, hence of length $| V |$ , or both with documents as dimensions, of length $| D | )$ and gives a measure of their similarity. By far the most common similarity metric is the cosine of the angle between the vectors.

The cosine—like most measures for vector similarity used in NLP—is based on the dot product operator from linear algebra, also called the inner product:

$$
\operatorname* { d o t } \operatorname { p r o d u c t } ( \mathbf { v } , \mathbf { w } ) = \mathbf { v } \cdot \mathbf { w } = \sum _ { i = 1 } ^ { N } \nu _ { i } w _ { i } = \nu _ { 1 } w _ { 1 } + \nu _ { 2 } w _ { 2 } + \ldots + \nu _ { N } w _ { N }
$$

The dot product acts as a similarity metric because it will tend to be high just when the two vectors have large values in the same dimensions. Alternatively, vectors that have zeros in different dimensions—orthogonal vectors—will have a dot product of 0, representing their strong dissimilarity.

vector length

This raw dot product, however, has a problem as a similarity metric: it favors long vectors. The vector length is defined as

$$
| \mathbf { v } | ~ = ~ \sqrt { \sum _ { i = 1 } ^ { N } \nu _ { i } ^ { 2 } }
$$

The dot product is higher if a vector is longer, with higher values in each dimension. More frequent words have longer vectors, since they tend to co-occur with more words and have higher co-occurrence values with each of them. The raw dot product thus will be higher for frequent words. But this is a problem; we’d like a similarity metric that tells us how similar two words are regardless of their frequency.

We modify the dot product to normalize for the vector length by dividing the dot product by the lengths of each of the two vectors. This normalized dot product turns out to be the same as the cosine of the angle between the two vectors, following from the definition of the dot product between two vectors a and b:

$$
{ \begin{array} { r l } { \mathbf { a } \cdot \mathbf { b } } & { = \ | \mathbf { a } | | \mathbf { b } | \cos \theta } \\ { \left. { \frac { \mathbf { a } \cdot \mathbf { b } } { | \mathbf { a } | | \mathbf { b } | } } \right. } & { = \ \cos \theta } \end{array} }
$$

The cosine similarity metric between two vectors $\pmb { v }$ and $\boldsymbol { \mathsf { W } }$ thus can be computed as:

$$
\mathrm { c o s i n e } ( \mathbf { v } , \mathbf { w } ) = \frac { \mathbf { v } \cdot \mathbf { w } } { | \mathbf { v } | | \mathbf { w } | } = \frac { \displaystyle \sum _ { i = 1 } ^ { N } \nu _ { i } w _ { i } } { \sqrt { \displaystyle \sum _ { i = 1 } ^ { N } \nu _ { i } ^ { 2 } } \sqrt { \displaystyle \sum _ { i = 1 } ^ { N } w _ { i } ^ { 2 } } }
$$

unit vector

For some applications we pre-normalize each vector, by dividing it by its length, creating a unit vector of length 1. Thus we could compute a unit vector from a by dividing it by |a|. For unit vectors, the dot product is the same as the cosine.

The cosine value ranges from 1 for vectors pointing in the same direction, through 0 for orthogonal vectors, to $^ { - 1 }$ for vectors pointing in opposite directions. But since raw frequency values are non-negative, the cosine for these vectors ranges from 0–1.

Let’s see how the cosine computes which of the words cherry or digital is closer in meaning to information, just using raw counts from the following shortened table:

<table><tr><td></td><td>pie</td><td>data</td><td>computer</td></tr><tr><td>cherry</td><td>442</td><td>8</td><td>2</td></tr><tr><td>digital</td><td>5</td><td>1683</td><td>1670</td></tr><tr><td>information</td><td>5</td><td>3982</td><td>3325</td></tr></table>

$$
{ \begin{array} { r l } { \cos ( { \mathrm { c h e r r y } } , { \mathrm { i n f o r m a t i o n } } ) \ = \ { \frac { 4 4 2 * 5 + 8 * 3 9 8 2 + 2 * 3 3 2 5 } { { \sqrt { 4 4 2 ^ { 2 } + 8 ^ { 2 } + 2 ^ { 2 } } } { \sqrt { 5 ^ { 2 } + 3 9 8 2 ^ { 2 } + 3 3 2 5 ^ { 2 } } } } } = . 0 1 8 } & { } \\ { \cos ( { \mathrm { d i g i t a l } } , { \mathrm { i n f o r m a t i o n } } ) \ = \ { \frac { 5 * 5 + 1 6 8 3 * 3 9 8 2 + 1 6 7 0 * 3 3 2 5 } { { \sqrt { 5 ^ { 2 } + 1 6 8 3 ^ { 2 } + 1 6 7 0 ^ { 2 } } } { \sqrt { 5 ^ { 2 } + 3 9 8 2 ^ { 2 } + 3 3 2 5 ^ { 2 } } } } } = . 9 9 6 } & { } \end{array} }
$$

The model decides that information is way closer to digital than it is to cherry, a result that seems sensible. Fig. 6.8 shows a visualization.

![## Image Analysis: 03cf08b19bc38766a0ca890c86b64c7ee538056055b37411220ec6ed1abee200.jpg

**Conceptual Understanding:**
This image represents a graphical demonstration of word vectors in a two-dimensional space, specifically illustrating the concept of cosine similarity. The main purpose is to show how the angular relationship between these word vectors corresponds to their semantic similarity. The key idea communicated is that words can be quantified and positioned in a vector space where their 'closeness' (measured by the angle between their vectors) indicates how similar their meanings or contexts are. The smaller the angle, the more similar the words are considered.

**Content Interpretation:**
This image illustrates the concept of representing words as vectors in a multi-dimensional space, specifically a two-dimensional space defined by the words 'pie' (Dimension 1) and 'computer' (Dimension 2). The position of each word vector ('cherry', 'digital', 'information') in this space indicates its co-occurrence or association with the defining words of the dimensions. For instance, the 'cherry' vector has a significant component along the 'pie' dimension, suggesting a stronger association with 'pie' than 'computer'. Conversely, 'digital' and 'information' vectors are heavily aligned with the 'computer' dimension, indicating a strong association with 'computer'. The significance of the image lies in its visual demonstration of cosine similarity through the angles between these vectors. A smaller angle between two vectors, such as between 'digital' and 'information', implies higher semantic similarity between those words. The larger angle between 'cherry' and the 'computer' axis (and implicitly 'information') suggests a lower similarity. The explicit labels 'Dimension 1: 'pie'', 'Dimension 2: 'computer'', and the word labels 'cherry', 'digital', 'information' are crucial in understanding what concepts are being mapped and compared.

**Key Insights:**
The main takeaway from this image is that semantic similarity between words can be represented and measured geometrically using vectors. Words that are semantically similar will have vectors that point in similar directions, resulting in smaller angles between them. Conversely, words that are less similar will have vectors with larger angles. The image specifically shows that 'digital' and 'information' are more similar to each other (smaller angle) than 'cherry' is to 'information' (larger implied angle with 'information' or the 'computer' dimension). The numerical scale '500', '1000', '1500', '2000', '2500', '3000' on the 'computer' dimension and '500' on the 'pie' dimension provide quantitative context for the word associations, although the exact values are less critical than the angular relationships for cosine similarity. The labels 'Dimension 1: 'pie'' and 'Dimension 2: 'computer'' establish the basis of this semantic space.

**Document Context:**
This image directly supports the document's section '6.4 Cosine for measuring similarity' by providing a visual, simplified example of how word vectors are used to calculate similarity. It helps the reader visualize the abstract concept of vectors and angles in a semantic space, making the explanation of cosine similarity more concrete. The textual labels for the dimensions ('pie', 'computer') and the words ('cherry', 'digital', 'information') provide specific examples that tie into the document's discussion of word co-occurrence and similarity metrics.

**Summary:**
This image displays a two-dimensional Cartesian coordinate system designed to graphically demonstrate cosine similarity between word vectors. The vertical axis is labeled 'Dimension 1: 'pie'', with a tick mark at '500'. The horizontal axis is labeled 'Dimension 2: 'computer'', with tick marks at '500', '1000', '1500', '2000', '2500', and '3000'. Three distinct vectors originate from the origin (0,0) and represent the words 'cherry', 'digital', and 'information'. The 'cherry' vector extends primarily along the 'pie' dimension and partially along the 'computer' dimension. The 'digital' and 'information' vectors extend predominantly along the 'computer' dimension, with 'digital' being shorter than 'information'. Two green arcs are depicted: one indicates the angle between the 'cherry' vector and the 'computer' axis, and another smaller arc indicates the angle between the 'digital' vector and the 'information' vector, highlighting their relative angular proximity.](images/03cf08b19bc38766a0ca890c86b64c7ee538056055b37411220ec6ed1abee200.jpg)
Figure 6.8 A (rough) graphical demonstration of cosine similarity, showing vectors for three words (cherry, digital, and information) in the two dimensional space defined by counts of the words computer and pie nearby. The figure doesn’t show the cosine, but it highlights the angles; note that the angle between digital and information is smaller than the angle between cherry and information. When two vectors are more similar, the cosine is larger but the angle is smaller; the cosine has its maximum (1) when the angle between two vectors is smallest $( 0 ^ { \circ } )$ ; the cosine of all other angles is less than 1.

# 6.5 TF-IDF: Weighing terms in the vector

The co-occurrence matrices above represent each cell by frequencies, either of words with documents (Fig. 6.5), or words with other words (Fig. 6.6). But raw frequency is not the best measure of association between words. Raw frequency is very skewed and not very discriminative. If we want to know what kinds of contexts are shared by cherry and strawberry but not by digital and information, we’re not going to get good discrimination from words like the, it, or they, which occur frequently with all sorts of words and aren’t informative about any particular word. We saw this also in Fig. 6.3 for the Shakespeare corpus; the dimension for the word good is not very discriminative between plays; good is simply a frequent word and has roughly equivalent high frequencies in each of the plays.

It’s a bit of a paradox. Words that occur nearby frequently (maybe pie nearby cherry) are more important than words that only appear once or twice. Yet words that are too frequent—ubiquitous, like the or good— are unimportant. How can we balance these two conflicting constraints?

There are two common solutions to this problem: in this section we’ll describe the tf-idf weighting, usually used when the dimensions are documents. In the next section we introduce the PPMI algorithm (usually used when the dimensions are words).

The tf-idf weighting (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, each term capturing one of these two intuitions:

The first is the term frequency (Luhn, 1957): the frequency of the word $t$ in the document $d$ . We can just use the raw count as the term frequency:

$$
\mathsf { t f } _ { t , d } ~ = ~ \mathsf { c o u n t } ( t , d )
$$

More commonly we squash the raw frequency a bit, by using the $\log _ { 1 0 }$ of the frequency instead. The intuition is that a word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document. We also need to do something special with counts of 0, since we can’t take the log of 0.2

$$
{ \mathrm { t f } } _ { t , d } = { \left\{ \begin{array} { l l } { 1 + \log _ { 1 0 } \operatorname { c o u n t } ( t , d ) } & { { \mathrm { ~ i f ~ } } \operatorname { c o u n t } ( t , d ) > 0 } \\ { 0 } & { { \mathrm { ~ o t h e r w i s e } } } \end{array} \right. }
$$

If we use log weighting, terms which occur 0 times in a document would have $\operatorname { t f } = 0$ , 1 times in a document $\mathrm { t f } = 1 + \log _ { 1 0 } ( 1 ) = 1 + 0 = 1$ , 10 times in a document $\mathbf { t } \mathbf { = }$ $1 + \log _ { 1 0 } ( 1 0 ) = 2$ , 100 times $\mathrm { t f } = 1 + \log _ { 1 0 } ( 1 0 0 ) = 3$ , 1000 times $\mathrm { t f } = 4$ , and so on.

The second factor in tf-idf is used to give a higher weight to words that occur only in a few documents. Terms that are limited to a few documents are useful for discriminating those documents from the rest of the collection; terms that occur frequently across the entire collection aren’t as helpful. The document frequency $\mathbf { d f } _ { t }$ of a term $t$ is the number of documents it occurs in. Document frequency is not the same as the collection frequency of a term, which is the total number of times the word appears in the whole collection in any document. Consider in the collection of Shakespeare’s 37 plays the two words Romeo and action. The words have identical collection frequencies (they both occur 113 times in all the plays) but very different document frequencies, since Romeo only occurs in a single play. If our goal is to find documents about the romantic tribulations of Romeo, the word Romeo should be highly weighted, but not action:

<table><tr><td></td><td>Collection Frequency Document Frequency</td></tr><tr><td>Romeo 113</td><td>1</td></tr><tr><td>action 113</td><td>31</td></tr></table>

We emphasize discriminative words like Romeo via the inverse document frequency or idf term weight (Sparck Jones, 1972). The idf is defined using the fraction $N / \mathrm { d f } _ { t }$ , where $N$ is the total number of documents in the collection, and $\operatorname { d f } _ { t }$ is the number of documents in which term $t$ occurs. The fewer documents in which a term occurs, the higher this weight. The lowest weight of 1 is assigned to terms that occur in all the documents. It’s usually clear what counts as a document: in Shakespeare we would use a play; when processing a collection of encyclopedia articles like Wikipedia, the document is a Wikipedia page; in processing newspaper articles, the document is a single article. Occasionally your corpus might not have appropriate document divisions and you might need to break up the corpus into documents yourself for the purposes of computing idf.

Because of the large number of documents in many collections, this measure too is usually squashed with a log function. The resulting definition for inverse document frequency (idf) is thus

$$
\mathrm { i d f } _ { t } \ = \ \log _ { 1 0 } \left( { \frac { N } { \mathrm { d f } _ { t } } } \right)
$$

Here are some idf values for some words in the Shakespeare corpus, (along with the document frequency df values on which they are based) ranging from extremely informative words which occur in only one play like Romeo, to those that occur in a few like salad or Falstaff, to those which are very common like fool or so common as to be completely non-discriminative since they occur in all 37 plays like good or sweet.3

<table><tr><td>Word</td><td>df idf</td></tr><tr><td>Romeo</td><td>1 1.57</td></tr><tr><td> salad</td><td>2 1.27</td></tr><tr><td> Falstaff</td><td>4 0.967</td></tr><tr><td>forest</td><td>12 0.489</td></tr><tr><td>battle</td><td>21 0.246</td></tr><tr><td>wit</td><td>34 0.037</td></tr><tr><td>fool</td><td>36 0.012</td></tr><tr><td> good</td><td>37 0</td></tr><tr><td> sweet</td><td>37 0</td></tr></table>

The tf-idf weighted value $w _ { t , d }$ for word $t$ in document $d$ thus combines term frequency $\mathrm { t f } _ { t , d }$ (defined either by Eq. 6.11 or by Eq. 6.12) with idf from Eq. 6.13:

$$
w _ { t , d } = \mathrm { t f } _ { t , d } \times \mathrm { i d f } _ { t }
$$

Fig. 6.9 applies tf-idf weighting to the Shakespeare term-document matrix in Fig. 6.2, using the tf equation Eq. 6.12. Note that the tf-idf values for the dimension corresponding to the word good have now all become 0; since this word appears in every document, the tf-idf weighting leads it to be ignored. Similarly, the word fool, which appears in 36 out of the 37 plays, has a much lower weight.

The tf-idf weighting is the way for weighting co-occurrence matrices in information retrieval, but also plays a role in many other aspects of natural language processing. It’s also a great baseline, the simple thing to try first. We’ll look at other weightings like PPMI (Positive Pointwise Mutual Information) in Section 6.6.

<table><tr><td></td><td> As You Like It</td><td>Twelfth Night</td><td> Julius Caesar</td><td>Henry V</td></tr><tr><td>battle</td><td>0.246</td><td>0</td><td>0.454</td><td>0.520</td></tr><tr><td>good</td><td>0</td><td>0</td><td>0</td><td>0</td></tr><tr><td>fool</td><td>0.030</td><td>0.033</td><td>0.0012</td><td>0.0019</td></tr><tr><td>wit</td><td>0.085</td><td>0.081</td><td>0.048</td><td>0.054</td></tr></table>

Figure 6.9 A portion of the tf-idf weighted term-document matrix for four words in Shakespeare plays, showing a selection of 4 plays, using counts from Fig. 6.2. For example the 0.085 value for wit in As You Like It is the product of $\mathrm { t f } = 1 + \log _ { 1 0 } ( 2 0 ) = 2 . 3 0 1$ and idf $= . 0 3 7$ . Note that the idf weighting has eliminated the importance of the ubiquitous word good and vastly reduced the impact of the almost-ubiquitous word fool.

# 6.6 Pointwise Mutual Information (PMI)

An alternative weighting function to tf-idf, PPMI (positive pointwise mutual information), is used for term-term-matrices, when the vector dimensions correspond to words rather than documents. PPMI draws on the intuition that the best way to weigh the association between two words is to ask how much more the two words co-occur in our corpus than we would have a priori expected them to appear by chance.

Pointwise mutual information (Fano, $1 9 6 1 ) ^ { 4 }$ is one of the most important concepts in NLP. It is a measure of how often two events $x$ and $y$ occur, compared with what we would expect if they were independent:

$$
I ( x , y ) = \log _ { 2 } \frac { P ( x , y ) } { P ( x ) P ( y ) }
$$

The pointwise mutual information between a target word $w$ and a context word $c$ (Church and Hanks 1989, Church and Hanks 1990) is then defined as:

$$
\mathrm { P M I } ( w , c ) = \log _ { 2 } \frac { P ( w , c ) } { P ( w ) P ( c ) }
$$

The numerator tells us how often we observed the two words together (assuming we compute probability by using the MLE). The denominator tells us how often we would expect the two words to co-occur assuming they each occurred independently; recall that the probability of two independent events both occurring is just the product of the probabilities of the two events. Thus, the ratio gives us an estimate of how much more the two words co-occur than we expect by chance. PMI is a useful tool whenever we need to find words that are strongly associated.

PMI values range from negative to positive infinity. But negative PMI values (which imply things are co-occurring less often than we would expect by chance) tend to be unreliable unless our corpora are enormous. To distinguish whether two words whose individual probability is each $1 0 ^ { - 6 }$ occur together less often than chance, we would need to be certain that the probability of the two occurring together is significantly less than $1 0 ^ { - 1 2 }$ , and this kind of granularity would require an enormous corpus. Furthermore it’s not clear whether it’s even possible to evaluate such scores of ‘unrelatedness’ with human judgments. For this reason it is more

$$
I ( X , Y ) = \sum _ { x } \sum _ { y } P ( x , y ) \log _ { 2 } { \frac { P ( x , y ) } { P ( x ) P ( y ) } }
$$

PPMI common to use Positive PMI (called PPMI) which replaces all negative PMI values with zero (Church and Hanks 1989, Dagan et al. 1993, Niwa and Nitta $1 9 9 4 ) ^ { 5 }$ :

$$
\mathrm { P P M I } ( w , c ) = \operatorname* { m a x } ( \log _ { 2 } \frac { P ( w , c ) } { P ( w ) P ( c ) } , 0 )
$$

More formally, let’s assume we have a co-occurrence matrix $\mathrm { F }$ with $\mathbf { W }$ rows (words) and C columns (contexts), where $f _ { i j }$ gives the number of times word $w _ { i }$ occurs with context $c _ { j }$ . This can be turned into a PPMI matrix where $\mathrm { P P M I } _ { i j }$ gives the PPMI value of word $w _ { i }$ with context $c _ { j }$ (which we can also express as $\mathsf { P P M I } ( \boldsymbol { w } _ { i } , \boldsymbol { c } _ { j } )$ o r $\mathrm { P P M I } ( w = i , c = j ) )$ ) as follows:

$$
\begin{array} { r } { p _ { i j } = \frac { f _ { i j } } { \sum _ { i = 1 } ^ { W } \sum _ { j = 1 } ^ { C } f _ { i j } } , ~ p _ { i * } = \frac { \sum _ { j = 1 } ^ { C } f _ { i j } } { \sum _ { i = 1 } ^ { W } \sum _ { j = 1 } ^ { C } f _ { i j } } , ~ p _ { * j } = \frac { \sum _ { i = 1 } ^ { W } f _ { i j } } { \sum _ { i = 1 } ^ { W } \sum _ { j = 1 } ^ { C } f _ { i j } } } \end{array}
$$

$$
\mathbf { P P M I } _ { i j } = \operatorname* { m a x } ( \log _ { 2 } \frac { p _ { i j } } { p _ { i * } p _ { * j } } , 0 )
$$

Let’s see some PPMI calculations. We’ll use Fig. 6.10, which repeats Fig. 6.6 plus all the count marginals, and let’s pretend for ease of calculation that these are the only words/contexts that matter.

<table><tr><td></td><td>computer</td><td>data</td><td>result</td><td>pie</td><td> sugar</td><td>count(w)</td></tr><tr><td>cherry</td><td>2</td><td>8</td><td>9</td><td>442</td><td>25</td><td>486</td></tr><tr><td> strawberry</td><td>0</td><td>0</td><td>1</td><td>60</td><td>19</td><td>80</td></tr><tr><td> digital</td><td>1670</td><td>1683</td><td>85</td><td>5</td><td>4</td><td>3447</td></tr><tr><td> information</td><td>3325</td><td>3982</td><td>378</td><td>5</td><td>13</td><td>7703</td></tr><tr><td>count(context)</td><td>4997</td><td>5673</td><td>473</td><td>512</td><td>61</td><td>11716</td></tr></table>

Figure 6.10 Co-occurrence counts for four words in 5 contexts in the Wikipedia corpus, together with the marginals, pretending for the purpose of this calculation that no other words/contexts matter.

Thus for example we could compute PPMI(information,data), assuming we pretended that Fig. 6.6 encompassed all the relevant word contexts/dimensions, as follows:

$$
\begin{array} { r c l } { { \displaystyle P ( \mathrm { w = i n f o r m a t i o n } , \mathsf { c = d a t a } ) ~ = ~ \frac { 3 9 8 2 } { 1 1 7 1 6 } = . 3 3 9 9 } } \\ { { \displaystyle P ( \mathrm { w = i n f o r m a t i o n } ) ~ = ~ \frac { 7 7 0 3 } { 1 1 7 1 6 } = . 6 5 7 5 } } \\ { { \displaystyle P ( \mathrm { c = d a t a } ) ~ = ~ \frac { 5 6 7 3 } { 1 1 7 1 6 } = . 4 8 4 2 } } \\ { { \displaystyle \mathrm { P P M I ( i n f o r m a t i o n , d a t a ) } ~ = ~ \log _ { 2 } ( . 3 3 9 9 / ( . 6 5 7 5 * . 4 8 4 2 ) ) = . 0 9 4 4 } } \end{array}
$$

Fig. 6.11 shows the joint probabilities computed from the counts in Fig. 6.10, and Fig. 6.12 shows the PPMI values. Not surprisingly, cherry and strawberry are highly associated with both pie and sugar, and data is mildly associated with information. PMI has the problem of being biased toward infrequent events; very rare words tend to have very high PMI values. One way to reduce this bias toward low frequency

<table><tr><td colspan="6"> p(w,context)</td><td>p(w)</td></tr><tr><td></td><td>computer</td><td>data</td><td>result</td><td>pie</td><td> sugar</td><td>p(w)</td></tr><tr><td>cherry</td><td>0.0002</td><td>0.0007</td><td>0.0008</td><td>0.0377</td><td>0.0021</td><td>0.0415</td></tr><tr><td>strawberry</td><td>0.0000</td><td>0.0000</td><td>0.0001</td><td>0.0051</td><td>0.0016</td><td>0.0068</td></tr><tr><td> digital</td><td>0.1425</td><td>0.1436</td><td>0.0073</td><td>0.0004</td><td>0.0003</td><td>0.2942</td></tr><tr><td>information</td><td>0.2838</td><td>0.3399</td><td>0.0323</td><td>0.0004</td><td>0.0011</td><td>0.6575</td></tr><tr><td> p(context)</td><td>0.4265</td><td>0.4842</td><td>0.0404</td><td>0.0437</td><td>0.0052</td><td></td></tr></table>

Figure 6.11 Replacing the counts in Fig. 6.6 with joint probabilities, showing the marginals in the right column and the bottom row.

<table><tr><td></td><td>computer</td><td>data</td><td>result</td><td>pie</td><td> sugar</td></tr><tr><td>cherry</td><td>0</td><td>0</td><td>0</td><td>4.38</td><td>3.30</td></tr><tr><td>strawberry</td><td>0</td><td>0</td><td>0</td><td>4.10</td><td>5.51</td></tr><tr><td>digital</td><td>0.18</td><td>0.01</td><td>0</td><td>0</td><td>0</td></tr><tr><td>information</td><td>0.02</td><td>0.09</td><td>0.28</td><td>0</td><td>0</td></tr></table>

Figure 6.12 The PPMI matrix showing the association between words and context words, computed from the counts in Fig. 6.11. Note that most of the 0 PPMI values are ones that had a negative PMI; for example PMI(cherry,computer) $= - 6 . 7$ , meaning that cherry and computer co-occur on Wikipedia less often than we would expect by chance, and with PPMI we replace negative values by zero.

events is to slightly change the computation for $P ( c )$ , using a different function $P _ { \alpha } ( c )$ that raises the probability of the context word to the power of $\alpha$ :

$$
\mathrm { P P M I } _ { \alpha } ( w , c ) = \operatorname* { m a x } ( \log _ { 2 } \frac { P ( w , c ) } { P ( w ) P _ { \alpha } ( c ) } , 0 )
$$

$$
P _ { \alpha } ( c ) = { \frac { c o u n t ( c ) ^ { \alpha } } { \sum _ { c } c o u n t ( c ) ^ { \alpha } } }
$$

Levy et al. (2015) found that a setting of $\alpha = 0 . 7 5$ improved performance of embeddings on a wide range of tasks (drawing on a similar weighting used for skipgrams described below in Eq. 6.32). This works because raising the count to $\alpha =$ 0.75 increases the probability assigned to rare contexts, and hence lowers their PMI $( P _ { \alpha } ( c ) > P ( c )$ when $c$ is rare).

Another possible solution is Laplace smoothing: Before computing PMI, a small constant $k$ (values of 0.1-3 are common) is added to each of the counts, shrinking (discounting) all the non-zero values. The larger the $k$ , the more the non-zero counts are discounted.

# 6.7 Applications of the tf-idf or PPMI vector models

In summary, the vector semantics model we’ve described so far represents a target word as a vector with dimensions corresponding either to the documents in a large collection (the term-document matrix) or to the counts of words in some neighboring window (the term-term matrix). The values in each dimension are counts, weighted by tf-idf (for term-document matrices) or PPMI (for term-term matrices), and the vectors are sparse (since most values are zero).

The model computes the similarity between two words $x$ and $y$ by taking the cosine of their tf-idf or PPMI vectors; high cosine, high similarity. This entire model is sometimes referred to as the tf-idf model or the PPMI model, after the weighting function.

The tf-idf model of meaning is often used for document functions like deciding if two documents are similar. We represent a document by taking the vectors of all the words in the document, and computing the centroid of all those vectors. The centroid is the multidimensional version of the mean; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set. Given $k$ word vectors $w _ { 1 } , w _ { 2 } , . . . , w _ { k }$ , the centroid document vector $d$ is:

$$
d = { \frac { w _ { 1 } + w _ { 2 } + \ldots + w _ { k } } { k } }
$$

Given two documents, we can then compute their document vectors $d _ { 1 }$ and $d _ { 2 }$ , and estimate the similarity between the two documents by $\cos ( d _ { 1 } , d _ { 2 } )$ . Document similarity is also useful for all sorts of applications; information retrieval, plagiarism detection, news recommender systems, and even for digital humanities tasks like comparing different versions of a text to see which are similar to each other.

Either the PPMI model or the tf-idf model can be used to compute word similarity, for tasks like finding word paraphrases, tracking changes in word meaning, or automatically discovering meanings of words in different corpora. For example, we can find the 10 most similar words to any target word $w$ by computing the cosines between $w$ and each of the $V - 1$ other words, sorting, and looking at the top 10.

# 6.8 Word2vec

In the previous sections we saw how to represent a word as a sparse, long vector with dimensions corresponding to words in the vocabulary or documents in a collection. We now introduce a more powerful word representation: embeddings, short dense vectors. Unlike the vectors we’ve seen so far, embeddings are short, with number of dimensions $d$ ranging from 50-1000, rather than the much larger vocabulary size $| V |$ or number of documents $D$ we’ve seen. These $d$ dimensions don’t have a clear interpretation. And the vectors are dense: instead of vector entries being sparse, mostly-zero counts or functions of counts, the values will be real-valued numbers that can be negative.

It turns out that dense vectors work better in every NLP task than sparse vectors. While we don’t completely understand all the reasons for this, we have some intuitions. Representing words as 300-dimensional dense vectors requires our classifiers to learn far fewer weights than if we represented words as 50,000-dimensional vectors, and the smaller parameter space possibly helps with generalization and avoiding overfitting. Dense vectors may also do a better job of capturing synonymy. For example, in a sparse vector representation, dimensions for synonyms like car and automobile dimension are distinct and unrelated; sparse vectors may thus fail to capture the similarity between a word with car as a neighbor and a word with automobile as a neighbor.

In this section we introduce one method for computing embeddings: skip-gram with negative sampling, sometimes called SGNS. The skip-gram algorithm is one of two algorithms in a software package called word2vec, and so sometimes the algorithm is loosely referred to as word2vec (Mikolov et al. 2013a, Mikolov et al. 2013b). The word2vec methods are fast, efficient to train, and easily available on

static embeddings

line with code and pretrained embeddings. Word2vec embeddings are static embeddings, meaning that the method learns one fixed embedding for each word in the vocabulary. In Chapter 11 we’ll introduce methods for learning dynamic contextual embeddings like the popular family of BERT representations, in which the vector for each word is different in different contexts.

The intuition of word2vec is that instead of counting how often each word $w$ occurs near, say, apricot, we’ll instead train a classifier on a binary prediction task: “Is word $w$ likely to show up near apricot?” We don’t actually care about this prediction task; instead we’ll take the learned classifier weights as the word embeddings.

self-supervision

The revolutionary intuition here is that we can just use running text as implicitly supervised training data for such a classifier; a word $c$ that occurs near the target word apricot acts as gold ‘correct answer’ to the question “Is word $c$ likely to show up near apricot?” This method, often called self-supervision, avoids the need for any sort of hand-labeled supervision signal. This idea was first proposed in the task of neural language modeling, when Bengio et al. (2003) and Collobert et al. (2011) showed that a neural language model (a neural network that learned to predict the next word from prior words) could just use the next word in running text as its supervision signal, and could be used to learn an embedding representation for each word as part of doing this prediction task.

We’ll see how to do neural networks in the next chapter, but word2vec is a much simpler model than the neural network language model, in two ways. First, word2vec simplifies the task (making it binary classification instead of word prediction). Second, word2vec simplifies the architecture (training a logistic regression classifier instead of a multi-layer neural network with hidden layers that demand more sophisticated training algorithms). The intuition of skip-gram is:

1. Treat the target word and a neighboring context word as positive examples.   
2. Randomly sample other words in the lexicon to get negative samples.   
3. Use logistic regression to train a classifier to distinguish those two cases.   
4. Use the learned weights as the embeddings.

# 6.8.1 The classifier

Let’s start by thinking about the classification task, and then turn to how to train. Imagine a sentence like the following, with a target word apricot, and assume we’re using a window of $\pm 2$ context words:

Our goal is to train a classifier such that, given a tuple $( w , c )$ of a target word $w$ paired with a candidate context word $c$ (for example (apricot, jam), or perhaps (apricot, aardvark)) it will return the probability that $c$ is a real context word (true for jam, false for aardvark):

$$
P ( + | w , c )
$$

The probability that word $c$ is not a real context word for $w$ is just 1 minus Eq. 6.24:

$$
P ( - | w , c ) = 1 - P ( + | w , c )
$$

How does the classifier compute the probability $P ?$ The intuition of the skipgram model is to base this probability on embedding similarity: a word is likely to occur near the target if its embedding vector is similar to the target embedding. To compute similarity between these dense embeddings, we rely on the intuition that two vectors are similar if they have a high dot product (after all, cosine is just a normalized dot product). In other words:

$$
S i m i l a r i t y ( w , c ) \approx \mathbf { c } \cdot \mathbf { w }
$$

The dot product $\mathbf { c } \cdot \mathbf { w }$ is not a probability, it’s just a number ranging from $- \infty$ to $\infty$ (since the elements in word2vec embeddings can be negative, the dot product can be negative). To turn the dot product into a probability, we’ll use the logistic or sigmoid function $\sigma ( x )$ , the fundamental core of logistic regression:

$$
\sigma ( x ) = \frac { 1 } { 1 + \exp \left( - x \right) }
$$

We model the probability that word $c$ is a real context word for target word $w$ as:

$$
P ( + | w , c ) = \sigma ( \pmb { \mathrm { c } } \cdot \pmb { \mathrm { w } } ) = \frac { 1 } { 1 + \exp \left( - \pmb { \mathrm { c } } \cdot \pmb { \mathrm { w } } \right) }
$$

The sigmoid function returns a number between 0 and 1, but to make it a probability we’ll also need the total probability of the two possible events ( $c$ is a context word, and $c$ isn’t a context word) to sum to 1. We thus estimate the probability that word $c$ is not a real context word for $w$ as:

$$
\begin{array} { l } { P ( - | w , c ) ~ = ~ 1 - P ( + | w , c ) } \\ { ~ } \\ { \displaystyle = ~ \sigma ( - \bar { \bf { c } } \cdot { \bf { w } } ) = \frac { 1 } { 1 + \exp \left( \bar { \bf { c } } \cdot \bar { \bf { w } } \right) } } \end{array}
$$

Equation 6.28 gives us the probability for one word, but there are many context words in the window. Skip-gram makes the simplifying assumption that all context words are independent, allowing us to just multiply their probabilities:

$$
\begin{array} { l } { { \displaystyle { \cal P } ( + | w , c _ { 1 : L } ) ~ = ~ \prod _ { i = 1 } ^ { L } \sigma ( \bar { \bf c } _ { i } \cdot { \bf w } ) } } \\ { { \displaystyle ~ \log { \cal P } ( + | w , c _ { 1 : L } ) ~ = ~ \sum _ { i = 1 } ^ { L } \log \sigma ( \bar { \bf c } _ { i } \cdot { \bf w } ) } } \end{array}
$$

In summary, skip-gram trains a probabilistic classifier that, given a test target word $w$ and its context window of $L$ words $c _ { 1 : L }$ , assigns a probability based on how similar this context window is to the target word. The probability is based on applying the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. To compute this probability, we just need embeddings for each target word and context word in the vocabulary.

Fig. 6.13 shows the intuition of the parameters we’ll need. Skip-gram actually stores two embeddings for each word, one for the word as a target, and one for the word considered as context. Thus the parameters we need to learn are two matrices $\boldsymbol { \mathsf { W } }$ and C, each containing an embedding for every one of the $| V |$ words in the vocabulary $V$ .6 Let’s now turn to learning these embeddings (which is the real goal of training this classifier in the first place).

![## Image Analysis: a380879cb7bb6a6cffceb471addfa5bfa79a84168a88388a7e6ad0f7e7006289.jpg

**Conceptual Understanding:**
This image conceptually represents the composite parameter matrix `θ` learned by a skip-gram model in natural language processing. Its main purpose is to visually illustrate how the model stores two distinct types of embeddings for each word in a vocabulary: one for its role as a "target word" and another for its role as a "context or noise word." The diagram shows these two sets of embeddings, `W` and `C`, are vertically concatenated to form the complete `θ` matrix.

**Content Interpretation:**
The image demonstrates the **structure of the parameter matrix `θ`** used in word embedding models, particularly skip-gram.
*   **Processes/Concepts Shown:**
    *   **Word Embeddings:** Each word (e.g., "aardvark", "apricot", "zebra") is associated with a vector of numerical values (represented by the three red circles), which is its embedding. The label `1..d` at the top indicates that these are d-dimensional vectors.
    *   **Skip-gram Model Parameters:** The entire matrix `θ` comprises all the learned parameters.
    *   **Dual Embeddings:** For each word in the vocabulary, two different embeddings are learned and stored. This is evident from "aardvark" appearing in both the green (`W`) and purple (`C`) sections, each with its own embedding representation.
    *   **Target Word Embeddings (`W`):** The top (green) section, explicitly labeled `W target words`, shows embeddings used when a word acts as a central "target" word in the skip-gram objective. The indices `1` to `|V|` indicate that this section contains embeddings for all `|V|` words in the vocabulary.
    *   **Context & Noise Word Embeddings (`C`):** The bottom (purple) section, explicitly labeled `C context & noise words`, illustrates embeddings used when a word acts as a "context" word or a "noise" word. This section also spans `|V|` entries, from index `|V|+1` to `2V`, corresponding to all `|V|` words in the vocabulary in their context/noise role.
    *   **Matrix Concatenation:** The visual representation of `θ` as a single tall vertical bar divided into two distinct colored sections (`W` green, `C` purple) directly shows that `θ` is formed by concatenating the target word embeddings and the context/noise word embeddings.

*   **Significance:** The separation into `W` and `C` highlights a crucial aspect of the skip-gram model: it learns distinct representations for words based on whether they are being predicted (target) or are part of the predicting context (context/noise). This dual representation allows for more nuanced and powerful learning of word relationships.

*   **Supporting Evidence from Transcription:**
    *   `θ =`: Defines the overall parameter matrix.
    *   `W target words`: Explicitly labels the upper part of the matrix.
    *   `C context & noise words`: Explicitly labels the lower part of the matrix.
    *   `aardvark`, `apricot`, `...`, `zebra`: Examples of words whose embeddings are stored. The repetition of "aardvark" and "apricot" across `W` and `C` sections clearly shows the dual embedding concept.
    *   `1..d`: Indicates the dimensionality `d` of each individual embedding vector.
    *   `1`, `|V|`, `|V|+1`, `2V`: These indices precisely delineate the ranges for the `|V|` target word embeddings and the `|V|` context/noise word embeddings, confirming that `θ` contains `2|V|` total embeddings.
    *   The visual separation by color (green for `W`, purple for `C`) further emphasizes the distinct nature and concatenation of the two matrices.

**Key Insights:**
*   **Main Takeaways/Lessons:**
    *   **Dual Nature of Word Embeddings in Skip-gram:** The skip-gram model does not just learn one embedding per word; it learns two distinct embeddings: one as a potential "target" and another as a potential "context/noise" word. This is evident from the explicit labels `W target words` and `C context & noise words` and the identical list of words (e.g., "aardvark", "apricot") appearing in both `W` and `C` sections.
    *   **Structure of the Model's Parameters:** The entire set of learnable parameters (`θ`) in the skip-gram model is a large matrix formed by vertically concatenating these two sets of embeddings. The `θ =` symbol and the vertical concatenation of the green (`W`) and purple (`C`) blocks graphically represent this.
    *   **Dimensionality and Vocabulary Size:** For a vocabulary of size `|V|`, the total parameter matrix `θ` contains `2|V|` embeddings. Each embedding vector has a dimension `d`. This is directly supported by the numerical indices `1`, `|V|`, `|V|+1`, `2V` and the `1..d` label, signifying `|V|` target embeddings and `|V|` context embeddings, each of `d` dimensions.

*   **Conclusions/Insights:**
    *   The complexity of word representation is handled by learning different vector spaces (or different views) for words based on their functional role (target vs. context) in the prediction task.
    *   The `θ` matrix is a fundamental component for storing all the learned knowledge about word relationships within the skip-gram framework.

*   **Specific Text Elements as Evidence:**
    *   `θ =`: The symbolic representation of the complete parameter matrix.
    *   `W target words`: Designates the embeddings for words when they are the focus of prediction.
    *   `C context & noise words`: Designates the embeddings for words when they appear in the surrounding context or as negative samples.
    *   `aardvark`, `apricot`, `zebra`: These recurring words in both `W` and `C` sections serve as direct evidence that each word has both a target and a context embedding.
    *   `1`, `|V|`, `|V|+1`, `2V`: These indices confirm the count of embeddings: `|V|` for target words and `|V|` for context words, totaling `2|V|` embedding vectors in `θ`.
    *   `1..d`: Indicates that each of these `2|V|` embeddings is a `d`-dimensional vector.

**Document Context:**
This image directly and clearly visualizes the explanation provided in the document text: "The parameter θ that the algorithm learns is thus a matrix of $2 | V |$ vectors, each of dimension $d$ , formed by concatenating two matrices, the target embeddings $\boldsymbol { \mathsf { W } }$ and the context+noise embeddings C." The diagram serves as a perfect visual aid to understand the structure of `θ`, explicitly showing the concatenation of `W` and `C` matrices and illustrating the concept of `2|V|` vectors of dimension `d`. It makes the abstract mathematical description concrete, showing which words are associated with which part of the matrix and their corresponding indices.

**Summary:**
The image depicts the complete parameter matrix, denoted as `θ`, which is a fundamental component learned by a skip-gram model in natural language processing. This matrix visually illustrates how words are represented through numerical embeddings.

The `θ` matrix is structured as a tall, vertical bar, which is divided into two distinct sections. The top section, colored green, represents the **target word embeddings**, denoted by `W`. The bottom section, colored purple, represents the **context & noise word embeddings**, denoted by `C`. These two matrices, `W` and `C`, are vertically concatenated to form the complete `θ` parameter matrix.

Let's break down each part:

1.  **Overall Structure (`θ =`):** The entire diagram shows `θ` as a single, combined structure.
2.  **Dimensionality (`1..d`):** At the top, `1..d` indicates that each individual word embedding (represented by the small red circles within a rectangular shape) is a vector of `d` dimensions.
3.  **Target Word Embeddings (`W target words`):**
    *   This is the upper, green portion of the `θ` matrix.
    *   It contains embeddings for words when they are considered "target" words in the skip-gram learning process.
    *   Examples of words shown here are "aardvark" (at index `1`), "apricot", and "zebra" (at index `|V|`). The `...` signifies other words in the vocabulary.
    *   The indices on the right (`1` through `|V|`) indicate that this section holds embeddings for all `|V|` words in the vocabulary when they function as target words.
4.  **Context & Noise Word Embeddings (`C context & noise words`):**
    *   This is the lower, purple portion of the `θ` matrix.
    *   It stores embeddings for words when they act as "context" words (surrounding a target word) or "noise" words (negative samples).
    *   Interestingly, the same words appear here as in the target section: "aardvark" (starting at index `|V|+1`), "apricot", and "zebra" (ending at index `2V`). This clearly demonstrates that each word has *two* distinct embeddings associated with it—one for its target role and one for its context/noise role.
    *   The indices (`|V|+1` through `2V`) show that this section also holds `|V|` embeddings, covering all words in the vocabulary in their context/noise capacity.

In summary, the image effectively communicates that the `θ` parameter matrix is a comprehensive collection of `2|V|` word embeddings, each of `d` dimensions, where `|V|` represents the size of the vocabulary. These embeddings are carefully organized into two sets: `W` for target word representations and `C` for context and noise word representations, concatenated vertically to form the complete set of parameters learned by the model.](images/a380879cb7bb6a6cffceb471addfa5bfa79a84168a88388a7e6ad0f7e7006289.jpg)
Figure 6.13 The embeddings learned by the skipgram model. The algorithm stores two embeddings for each word, the target embedding (sometimes called the input embedding) and the context embedding (sometimes called the output embedding). The parameter θ that the algorithm learns is thus a matrix of $2 | V |$ vectors, each of dimension $d$ , formed by concatenating two matrices, the target embeddings $\boldsymbol { \mathsf { W } }$ and the context+noise embeddings C.

# 6.8.2 Learning skip-gram embeddings

The learning algorithm for skip-gram embeddings takes as input a corpus of text, and a chosen vocabulary size N. It begins by assigning a random embedding vector for each of the $\mathbf { N }$ vocabulary words, and then proceeds to iteratively shift the embedding of each word $w$ to be more like the embeddings of words that occur nearby in texts, and less like the embeddings of words that don’t occur nearby. Let’s start by considering a single piece of training data:

... lemon, a [tablespoon of apricot jam, a] pinch ... c1 c2 w c3 c4

This example has a target word $w$ (apricot), and 4 context words in the $L = \pm 2$ window, resulting in 4 positive training instances (on the left below):

<table><tr><td colspan="2">positive examples +</td><td colspan="4">negative examples -</td></tr><tr><td>W Cpos</td><td></td><td>W</td><td>Cneg</td><td>W</td><td>Cneg</td></tr><tr><td>apricot tablespoon</td><td></td><td></td><td>apricot aardvark apricot seven</td><td></td><td></td></tr><tr><td>apricot of</td><td></td><td>apricot my</td><td></td><td></td><td>apricot forever</td></tr><tr><td>apricot jam</td><td></td><td></td><td>apricot where</td><td>apricot</td><td>dear</td></tr><tr><td>apricot a</td><td></td><td></td><td>apricot coaxial</td><td>apricot if</td><td></td></tr></table>

For training a binary classifier we also need negative examples. In fact skipgram with negative sampling (SGNS) uses more negative examples than positive examples (with the ratio between them set by a parameter $k$ ). So for each of these $\left( w , c _ { p o s } \right)$ training instances we’ll create $k$ negative samples, each consisting of the target $w$ plus a ‘noise word’ $c _ { n e g }$ . A noise word is a random word from the lexicon, constrained not to be the target word $w$ . The right above shows the setting where $k = 2$ , so we’ll have 2 negative examples in the negative training set − for each positive example $w , c _ { p o s }$ .

The noise words are chosen according to their weighted unigram frequency $p _ { \alpha } ( w )$ , where $\alpha$ is a weight. If we were sampling according to unweighted frequency $p ( w )$ , it would mean that with unigram probability $p ( \tilde { } { } ^ { \ast } t h e ^ { \prime } \tilde { } { } )$ we would choose the word the as a noise word, with unigram probability $p ( \cdots a a r d \nu a r k ^ { \cdots } )$ we would choose aardvark, and so on. But in practice it is common to set $\alpha = 0 . 7 5$ , i.e. use the weighting $p _ { \frac { 3 } { 4 } } ( w )$ :

$$
P _ { \alpha } ( w ) = \frac { c o u n t ( w ) ^ { \alpha } } { \sum _ { w ^ { \prime } } c o u n t ( w ^ { \prime } ) ^ { \alpha } }
$$

Setting $\alpha = . 7 5$ gives better performance because it gives rare noise words slightly higher probability: for rare words, $P _ { \alpha } ( w ) > P ( w )$ . To illustrate this intuition, it might help to work out the probabilities for an example with $\alpha = . 7 5$ and two events, $P ( a ) = 0 . 9 9$ and $P ( b ) = 0 . 0 1$ :

$$
\begin{array} { l } { { P _ { \alpha } ( a ) ~ = ~ { \frac { . 9 9 ^ { . 7 5 } } { . 9 9 ^ { . 7 5 } + . 0 1 ^ { . 7 5 } } } = 0 . 9 7 } } \\ { { P _ { \alpha } ( b ) ~ = ~ { \frac { . 0 1 ^ { . 7 5 } } { . 9 9 ^ { . 7 5 } + . 0 1 ^ { . 7 5 } } } = 0 . 0 3 } } \end{array}
$$

Thus using $\alpha = . 7 5$ increases the probability of the rare event $^ b$ from 0.01 to 0.03.

Given the set of positive and negative training instances, and an initial set of embeddings, the goal of the learning algorithm is to adjust those embeddings to

• Maximize the similarity of the target word, context word pairs $( w , c _ { p o s } )$ drawn from the positive examples • Minimize the similarity of the $\left( w , c _ { n e g } \right)$ pairs from the negative examples.

If we consider one word/context pair (w, cpos) with its k noise words cneg1 ...cnegk , we can express these two goals as the following loss function $L$ to be minimized (hence the ); here the first term expresses that we want the classifier to assign the real context word $c _ { p o s }$ a high probability of being a neighbor, and the second term expresses that we want to assign each of the noise words $c _ { n e g _ { i } }$ a high probability of being a non-neighbor, all multiplied because we assume independence:

$$
\begin{array} { r l } { L \ = \ - \log \left[ P ( + | w , c _ { p o s } ) \prod _ { i = 1 } ^ { k } P ( - | w , c _ { n e g _ { i } } ) \right] } \\ { \ = \ - \left[ \log P ( + | w , c _ { p o s } ) + \displaystyle \sum _ { i = 1 } ^ { k } \log P ( - | w , c _ { n e g _ { i } } ) \right] } \\ { \ = \ - \left[ \log P ( + | w , c _ { p o s } ) + \displaystyle \sum _ { i = 1 } ^ { k } \log \big ( 1 - P ( + | w , c _ { n e g _ { i } } ) \big ) \right] } \\ { \ = \ - \left[ \log \sigma ( c _ { p o s } , w ) + \displaystyle \sum _ { i = 1 } ^ { k } \log \sigma ( - c _ { n e g _ { i } } , w ) \right] } \end{array}
$$

That is, we want to maximize the dot product of the word with the actual context words, and minimize the dot products of the word with the $k$ negative sampled nonneighbor words.

We minimize this loss function using stochastic gradient descent. Fig. 6.14 shows the intuition of one step of learning.

To get the gradient, we need to take the derivative of Eq. 6.34 with respect to the different embeddings. It turns out the derivatives are the following (we leave the

![## Image Analysis: eae332a3e07a67fec6c7336852a975dcae460e900efeb9b0523410b756c77e57.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental mechanism of updating word embeddings in a skip-gram model using negative sampling during a single step of gradient descent. Its main purpose is to visually explain how the model adjusts the positions of word vectors in an embedding space to reflect their co-occurrence probabilities. Specifically, it shows that positive word pairs are drawn closer together, while negative (noise) word pairs are pushed further apart, thereby improving the quality of the learned embeddings. The image conveys the idea of iterative refinement of word representations based on their contextual relationships.

**Content Interpretation:**
The image depicts the core mechanics of how skip-gram word embeddings are updated during a single step of gradient descent. It illustrates the 'push and pull' mechanism where a target word's embedding ('apricot' labeled 'w') is adjusted relative to its positive context word ('jam' labeled 'c_pos') and negative (noise) context words ('matrix' labeled 'c_neg1' and 'Tolstoy' labeled 'c_neg2'). The 'W' and 'C' sections likely represent the word and context embedding matrices, respectively, with 'θ' encompassing both. The 'k=2' indicates that two negative samples are used for this update step. The green arrow and its text indicate that the 'apricot' and 'jam' embeddings are moved closer to each other, aiming to increase their dot product (c_pos * W). Conversely, the red arrows and their accompanying text demonstrate that 'apricot' is moved further away from 'matrix' and 'Tolstoy', aiming to decrease their dot products (c_neg1 * W and c_neg2 * W). The small red dots next to each word visually represent the multi-dimensional vectors for each word's embedding.

**Key Insights:**
The main takeaway is that skip-gram embedding learning involves a directed adjustment of word vectors in a high-dimensional space. Specifically, for a given target word (e.g., 'apricot') and its observed context word (e.g., 'jam'), their embeddings are updated to become more similar (closer), which is directly tied to 'increasing c_pos * W'. Simultaneously, the target word's embedding is updated to become less similar (further apart) from randomly sampled 'noise' words (e.g., 'matrix', 'Tolstoy'), indicated by 'decreasing c_neg1 * W' and 'decreasing c_neg2 * W'. The parameter 'k=2' explicitly shows that two negative samples are used in this particular step. This push-and-pull mechanism, driven by gradient descent, iteratively refines the word embeddings to capture semantic and syntactic relationships based on word co-occurrence.

**Document Context:**
This image serves as a visual explanation for Section 6.8.2, titled 'Learning skip-gram embeddings'. The preceding text describes the intuition behind skip-gram and the necessity of adjusting word embeddings. The text immediately following the image, 'Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift embeddings so the target embeddings (here for apricot) are closer to (have a higher dot product with) context embeddings for nearby words (here jam) and further from (lower dot product with) context embeddings for noise words that don’t occur nearby (here Tolstoy and matrix)', perfectly aligns with and elaborates on the visual elements. The diagram provides a clear graphical representation of how the model achieves its objective of learning meaningful word relationships by moving related words closer in the embedding space and unrelated words further apart.

**Summary:**
The image illustrates a conceptual representation of one step in the gradient descent process for learning skip-gram embeddings, focusing on how word embeddings are adjusted. On the left, there's a vertical bar representing an embedding space, conceptually divided into two sections by a large Greek letter theta (θ). The upper section is labeled 'W' and contains words like 'aardvark', 'apricot', and 'zebra'. The word 'apricot' is highlighted and explicitly labeled 'w', signifying its role as a target word embedding. The lower section is labeled 'C' and contains words like 'aardvark', 'jam', 'matrix', 'Tolstoy', and 'zebra'. Within the 'C' section, 'jam' is labeled 'c_pos' (positive context word), 'matrix' is labeled 'c_neg1' (negative context word 1), and 'Tolstoy' is labeled 'c_neg2' (negative context word 2). A bracket next to these words indicates 'k=2', likely referring to the number of negative samples. To the right, the phrase "...apricot jam..." is shown, indicating the positive word pair being considered. Dashed arrows illustrate the adjustments made to the embeddings based on this pair and selected noise words. A green dashed arrow originates from 'apricot' (w) and points to 'jam' (c_pos), with the associated text "move apricot and jam closer, increasing c_pos * W". Two red dashed arrows also originate from 'apricot' (w). One points to 'matrix' (c_neg1) with the text "move apricot and matrix apart decreasing c_neg1 * W". The other red dashed arrow points to 'Tolstoy' (c_neg2) with the text "move apricot and Tolstoy apart decreasing c_neg2 * W". Each word in the embedding space is shown with three small red dots, visually representing its embedding vector.](images/eae332a3e07a67fec6c7336852a975dcae460e900efeb9b0523410b756c77e57.jpg)
Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift embeddings so the target embeddings (here for apricot) are closer to (have a higher dot product with) context embeddings for nearby words (here jam) and further from (lower dot product with) context embeddings for noise words that don’t occur nearby (here Tolstoy and matrix).

proof as an exercise at the end of the chapter):

$$
\begin{array} { r l } { \displaystyle \frac { \partial L } { \partial c _ { p o s } } } & { = \big [ \boldsymbol { \sigma } ( \mathbf { c } _ { p o s } \cdot \mathbf { w } ) - 1 \big ] \mathbf { w } } \\ { \displaystyle \frac { \partial L } { \partial c _ { n e g } } } & { = \big [ \boldsymbol { \sigma } ( \mathbf { c } _ { n e g } \cdot \mathbf { w } ) \big ] \mathbf { w } } \\ { \displaystyle \frac { \partial L } { \partial w } } & { = \big [ \boldsymbol { \sigma } ( \mathbf { c } _ { p o s } \cdot \mathbf { w } ) - 1 \big ] \mathbf { c } _ { p o s } + \sum _ { i = 1 } ^ { k } [ \boldsymbol { \sigma } ( \mathbf { c } _ { \mathbf { n } e g _ { i } } \cdot \mathbf { w } ) \big ] \mathbf { c } _ { n e g _ { i } } } \end{array}
$$

The update equations going from time step $t$ to $t + 1$ in stochastic gradient descent are thus:

$$
\begin{array} { r l } { { \mathbf { c } _ { p o s } ^ { t + 1 } } } & { = \mathbf { c } _ { p o s } ^ { t } - \eta [ \boldsymbol { \sigma } ( \mathbf { c } _ { p o s } ^ { t } \cdot \mathbf { w } ^ { t } ) - 1 ] \mathbf { w } ^ { t } } \\ { { \mathbf { c } _ { n e g } ^ { t + 1 } } } & { = \mathbf { c } _ { n e g } ^ { t } - \eta [ \boldsymbol { \sigma } ( \mathbf { c } _ { n e g } ^ { t } \cdot \mathbf { w } ^ { t } ) ] \mathbf { w } ^ { t } } \\ { { \mathbf { w } ^ { t + 1 } } } & { = \mathbf { w } ^ { t } - \eta \left[ [ \boldsymbol { \sigma } ( \mathbf { c } _ { p o s } ^ { t } \cdot \mathbf { w } ^ { t } ) - 1 ] \mathbf { c } _ { p o s } ^ { t } + \sum _ { i = 1 } ^ { k } [ \boldsymbol { \sigma } ( \mathbf { c } _ { \mathfrak { n e g } _ { i } } ^ { t } \cdot \mathbf { w } ^ { t } ) ] \mathbf { c } _ { n e g _ { i } } ^ { t } \right] } \end{array}
$$

Just as in logistic regression, then, the learning algorithm starts with randomly initialized $\boldsymbol { \mathsf { W } }$ and C matrices, and then walks through the training corpus using gradient descent to move $\boldsymbol { \mathsf { W } }$ and C so as to minimize the loss in Eq. 6.34 by making the updates in (Eq. 6.38)-(Eq. 6.40).

Recall that the skip-gram model learns two separate embeddings for each word $i$ : the target embedding $\boldsymbol { \mathsf { w } } _ { i }$ and the context embedding $\mathbf { c } _ { i }$ , stored in two matrices, the target matrix $\boldsymbol { \mathsf { W } }$ and the context matrix C. It’s common to just add them together, representing word $i$ with the vector $\pmb { \mathsf { w } } _ { i } + \pmb { \mathsf { c } } _ { i }$ . Alternatively we can throw away the C matrix and just represent each word $i$ by the vector $\boldsymbol { \mathsf { w } } _ { i }$ .

As with the simple count-based methods like tf-idf, the context window size $L$ affects the performance of skip-gram embeddings, and experiments often tune the parameter $L$ on a devset.

# 6.8.3 Other kinds of static embeddings

There are many kinds of static embeddings. An extension of word2vec, fasttext (Bojanowski et al., 2017), addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words—words that appear in a test corpus but were unseen in the training corpus. A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols $<$ and $>$ added to each word. For example, with $n = 3$ the word where would be represented by the sequence <where> plus the character n-grams:

<wh, whe, her, ere, re>

Then a skipgram embedding is learned for each constituent n-gram, and the word where is represented by the sum of all of the embeddings of its constituent n-grams. Unknown words can then be presented only by the sum of the constituent n-grams. A fasttext open-source library, including pretrained embeddings for 157 languages, Modeling Word Meaning Using Lexical Co-Occurrence is available at https://fasttext.cc.

Another very widely used static embedding model is GloVe (Pennington et al., 2014), short for Global Vectors, because the model is based on capturing global corpus statistics. GloVe is based on ratios of probabilities from the word-word co-CHINA FRANCE occurrence matrix, combining the intuitions of count-based models like PPMI whileEUROPE also capturing the linear structures used by methods like word2vec.AMERICAAFRICA

It turns out that dense embeddings like word2vec actually have an elegant math-MOSCOW ematical relationship with sparse embeddings like PPMI, in which word2vec can be seen as implicitly optimizing a function of a PPMI matrix (Levy and Goldberg,HAWAIITOKYO 2014c).

# 6.9DOGCAT Visualizing Embeddings

# “I see well in many dimensions as long as the dimensions are around two.” The late economist Martin Shubik

Visualizing embeddings is an important goal in helping understand, apply, andcaling for three noun classes. improve these models of word meaning. But how can we visualize a (for example) 100-dimensional vector?

The simplest way to visualize the meaning of a word $w$ embedded in a space is to list the most similar words to $w$ by sorting the vectors for all words in the vocabulary by their cosine with the vector for $w$ . For example the 7 closest words to frog using a particular embeddings computed with the GloVe algorithm are: frogs, toad, litoria, leptodactylidae, rana, lizard, and eleutherodactylus (Pennington et al., 2014).

Yet another visualization method is to use a clustering algorithm to show a hierarchical representation of which words are similar to others in the embedding space. The uncaptioned figure on the left uses hierarchical clustering of some embedding vectors for nouns as a visualization

![## Image Analysis: efd7716cafde4b030ca254a05b1b35bacd938fc0483a8dcf24dc2047e232e043.jpg

**Conceptual Understanding:**
The image conceptually represents a dendrogram, which is a visual output of hierarchical clustering. This diagram illustrates how a collection of discrete items (in this case, words) are grouped into progressively larger and more abstract clusters based on their degree of similarity or dissimilarity. The branching structure of the 'tree' indicates the fusion points of clusters, with the length of the horizontal lines often representing the 'distance' or dissimilarity at which those clusters were merged. The main purpose of this image is to visually demonstrate the inherent hierarchical structure and natural groupings that exist within a diverse set of words. It aims to convey that even seemingly disparate concepts can be systematically organized into meaningful categories based on their semantic relationships. Key ideas communicated include the principles of hierarchical classification, the concept of similarity measurement, and the reduction of complex, multi-dimensional relationships into an easily understandable two-dimensional visual format.

**Content Interpretation:**
The image demonstrates a conceptual system of categorization and relationship discovery among a list of terms. It's showing the output of a clustering algorithm that identifies implicit relationships. The 'clustering process' illustrates how terms are iteratively grouped (e.g., 'DOG' and 'CAT' are first grouped, then 'PUPPY' and 'KITTEN', and these smaller groups then merge into larger 'animal' clusters). 'Hierarchical relationships' are explicitly shown by the tree structure, with individual terms as leaves and joining points as internal nodes representing higher-level categories. 'Similarity/Dissimilarity' is implicitly conveyed by the branching structure; terms joined by shorter horizontal lines (e.g., 'DOG' and 'CAT') are more similar than those joined at higher levels in the tree. The primary 'data' is the set of words and their arrangement. The significance lies in how these words, which are seemingly unrelated at first glance, reveal clear semantic categories when subjected to clustering. The distinct separation into three major branches (Body Parts, Animals, Places) is a significant trend, indicating strong internal coherence within these categories and strong dissimilarity between them. This interpretation is supported by the verbatim transcription of all labels. For instance, the 'Body Parts Cluster' with terms like "WRIST", "ANKLE", "SHOULDER", "ARM", "LEG", "HAND", "FOOT", "HEAD", "NOSE", "FINGER", "TOE", "FACE", "EAR", "EYE" clearly shows a coherent grouping of anatomical terms. Their close proximity, especially "WRIST" with "ANKLE" or "FACE", "EAR", "EYE", directly supports the interpretation of a human body parts category. Similarly, the 'Animals Cluster' ('TOOTH', 'DOG', 'CAT', 'PUPPY', 'KITTEN', 'COW', 'MOUSE', 'TURTLE', 'OYSTER', 'LION', 'BULL') and 'Geographical Locations Cluster' ('CHICAGO', 'ATLANTA', 'MONTREAL', 'NASHVILLE', 'TOKYO', 'CHINA', 'RUSSIA', 'AFRICA', 'ASIA', 'EUROPE', 'AMERICA', 'BRAZIL', 'MOSCOW', 'FRANCE', 'HAWAII') form distinct categories, with internal clustering (e.g., 'DOG' and 'CAT' together, cities together) reinforcing the semantic similarities.

**Key Insights:**
The main takeaways from this image are: 1. Semantic Organization: The image clearly demonstrates that seemingly unrelated words can be organized into meaningful, hierarchical categories based on their intrinsic semantic relationships. 2. Implicit Relationships: It reveals how underlying relationships between concepts can be discovered and visualized through clustering analysis, even if those connections are not immediately obvious from a flat list. 3. Clustering as a Discovery Tool: The dendrogram is presented as a powerful visual tool for exploring and understanding the inherent structure within a dataset of qualitative labels, facilitating the discovery of knowledge. The conclusions and insights supported by this image include: 1. Clear Categorization: The most prominent insight is the clear and distinct categorization of the listed terms into three primary domains: human anatomy, animals, and global geography. 2. Varying Granularity of Similarity: The tree structure highlights that similarity exists at different levels of granularity. For example, 'DOG' and 'CAT' are very similar, and 'PUPPY' and 'KITTEN' are very similar, but these groups then merge at higher levels, indicating broader categories with less immediate similarity. 3. Dimensionality Reduction for Understanding: The diagram effectively reduces complex, multi-dimensional semantic relationships into an easily interpretable two-dimensional tree structure. These insights are directly evidenced by the complete list of transcribed words. For instance, the consistent clustering of terms like "WRIST", "ANKLE", "SHOULDER" into one major branch, separate from "DOG", "CAT" or "CHINA", "RUSSIA", provides strong evidence for distinct semantic categories. The specific grouping of "PUPPY" and "KITTEN" before merging with "COW" and then "MOUSE" within the animal cluster illustrates the varying degrees of similarity. Similarly, the separation of cities like "CHICAGO" from continents/countries like "CHINA" within the geographical cluster further supports the hierarchical organization of knowledge.

**Document Context:**
The image's relevance to the document's broader narrative is strongly linked to the accompanying quote: "I see well in many dimensions as long as the dimensions are around two." by Martin Shubik. This dendrogram serves as a powerful visual example of how complex, multi-dimensional data – in this case, the abstract semantic relationships between a diverse set of words – can be effectively represented and understood within a two-dimensional framework. The process of hierarchical clustering, which a dendrogram visualizes, takes high-dimensional data (where each word could be represented by many features) and reduces it to a comprehensible 2D tree structure. This aligns perfectly with Shubik's statement, demonstrating how one can 'see well' and derive clear insights by simplifying complex relationships into a lower-dimensional, manageable visual format. The image illustrates a practical application of the principle articulated in the quote, showing how conceptual clarity can be achieved by distilling multi-dimensional information into a readily interpretable, two-dimensional display.

**Summary:**
This image displays a dendrogram, which is a tree-like diagram used to represent the hierarchical clustering of a set of items. In this particular dendrogram, the items are a collection of words, and their arrangement illustrates their semantic similarities and differences. The structure moves from individual terms (the “leaves” on the right) towards progressively broader categories (merging to the left). The length of the horizontal lines indicates the distance or dissimilarity between the items or clusters they connect; shorter lines suggest greater similarity, while longer lines indicate more significant differences. The dendrogram clearly organizes the approximately 38 transcribed words into three primary, distinct semantic clusters, each further sub-divided: 1. Human Body Parts Cluster (Top): This large group comprises various anatomical terms, showing how they naturally associate. "WRIST" and "ANKLE" are closely grouped, likely due to both being joints. "SHOULDER" and "ARM" cluster together. "LEG", "HAND", and "FOOT" form another sub-group, representing appendages. A distinct sub-cluster includes "HEAD", "NOSE", "FINGER", "TOE", "FACE", "EAR", and "EYE". Within this, "FINGER" and "TOE" show close proximity, as do "FACE", "EAR", and "EYE", highlighting a grouping of sensory and facial features. 2. Animals and Related Terms Cluster (Middle): This cluster groups animal names and one related concept. "TOOTH" forms a branch that joins the animal kingdom. "DOG" and "CAT" are very closely related, as are "PUPPY" and "KITTEN" (offspring). "COW" joins the "PUPPY"/"KITTEN" sub-group. "MOUSE" is a distinct animal that joins a broader animal grouping. "TURTLE" and "OYSTER" are closely linked, possibly representing aquatic or less common terrestrial animals. "LION" and "BULL" are also closely linked, perhaps representing larger, powerful animals. These sub-groups then merge into a comprehensive animal category. 3. Geographical Locations Cluster (Bottom): This cluster categorizes various places, including cities, countries, continents, and a specific region. A group of cities includes "CHICAGO", "ATLANTA", "MONTREAL", "NASHVILLE", and "TOKYO". "ATLANTA" and "MONTREAL" are very similar, as are "NASHVILLE" and "TOKYO". A large group of countries and continents forms another sub-cluster: "CHINA", "RUSSIA" (closely related), "AFRICA", "ASIA" (closely related), and "EUROPE", "AMERICA" (closely related). "BRAZIL" and "MOSCOW" are grouped. "FRANCE" and "HAWAII" are grouped. The image effectively illustrates how complex semantic relationships can be visually organized into a comprehensible two-dimensional structure, reflecting the insight that information can be "seen well" when its dimensions are reduced. Each term's position and its connections to other terms provide specific evidence for these classifications and the hierarchical organization of knowledge.](images/efd7716cafde4b030ca254a05b1b35bacd938fc0483a8dcf24dc2047e232e043.jpg)

method (Rohde et al., 2006).

Probably the most common visualization method, however, is to project the 100 dimensions of a word down into 2 dimensions. Fig. 6.1 showed one such visualization, as does Fig. 6.16, using a projection method called t-SNE (van der

# 6.10 Semantic properties of embeddings

In this section we briefly summarize some of the semantic properties of embeddings that have been studied.

Different types of similarity or association: One parameter of vector semantic models that is relevant to both sparse PPMI vectors and dense word2vec vectors is the size of the context window used to collect counts. This is generally between 1 and 10 words on each side of the target word (for a total context of 2-20 words).

first-order co-occurrence

The choice depends on the goals of the representation. Shorter context windows tend to lead to representations that are a bit more syntactic, since the information is coming from immediately nearby words. When the vectors are computed from short context windows, the most similar words to a target word $w$ tend to be semantically similar words with the same parts of speech. When vectors are computed from long context windows, the highest cosine words to a target word $w$ tend to be words that are topically related but not similar.

parallelogram model

For example Levy and Goldberg (2014a) showed that using skip-gram with a window of $\pm 2$ , the most similar words to the word Hogwarts (from the Harry Potter series) were names of other fictional schools: Sunnydale (from Buffy the Vampire Slayer) or Evernight (from a vampire series). With a window of $\pm 5$ , the most similar words to Hogwarts were other words topically related to the Harry Potter series: Dumbledore, Malfoy, and half-blood.

second-order co-occurrence

It’s also often useful to distinguish two kinds of similarity or association between words (Schutze and Pedersen ¨ , 1993). Two words have first-order co-occurrence (sometimes called syntagmatic association) if they are typically nearby each other. Thus wrote is a first-order associate of book or poem. Two words have second-order co-occurrence (sometimes called paradigmatic association) if they have similar neighbors. Thus wrote is a second-order associate of words like said or remarked.

Analogy/Relational Similarity: Another semantic property of embeddings is their ability to capture relational meanings. In an important early vector space model of cognition, Rumelhart and Abrahamson (1973) proposed the parallelogram model for solving simple analogy problems of the form a is to b as $a ^ { * }$ is to what?. In such problems, a system is given a problem like apple:tree::grape:?, i.e., apple is to tree as grape is to , and must fill in the word vine. In the parallelogram model, illustrated in Fig. 6.15, the vector from the word apple to the word tree $( =$ $\overrightarrow { \mathrm { t r e e } } - \overrightarrow { \mathrm { a p p l e } } )$ is added to the vector for grape ( grape); the nearest word to that point is returned.

In early work with sparse embeddings, scholars showed that sparse vector models of meaning could solve such analogy problems (Turney and Littman, 2005), but the parallelogram method received more modern attention because of its success with word2vec or GloVe vectors (Mikolov et al. 2013c, Levy and Goldberg# » 2014b, Pennington et al. 2014). For example, the result of the expression $\overrightarrow { \mathrm { k i n g } } - \Bigg .$

![## Image Analysis: d45bab812864266253225d997179aeaf3c297affa813f4b0f01d6eb0e36ec784.jpg

**Conceptual Understanding:**
This image represents the 'parallelogram model' for solving analogy problems using vector representations, often associated with word embeddings. Conceptually, it illustrates that a semantic analogy, such as 'apple is to tree as grape is to vine,' can be understood as a geometric relationship in a multi-dimensional space. The main purpose is to show that the vector difference between two words in an analogous pair is approximately the same as the vector difference between another analogous pair. Specifically, the relationship from 'apple' to 'tree' (represented by a vector) is parallel and equal to the relationship from 'grape' to 'vine' (represented by another vector).

**Content Interpretation:**
This image visually represents the 'parallelogram model' for analogy problems within the context of semantic embeddings. It demonstrates how semantic relationships can be captured and manipulated through vector operations. The specific concepts shown are 'apple', 'tree', 'grape', and 'vine', each represented as a point in a conceptual space. The solid arrows indicate directed vectors: one from 'apple' to 'tree', and another from 'grape' to 'vine'. The parallelism of these vectors, forming a parallelogram (indicated by the solid and dashed lines enclosing a green area), signifies that the semantic relationship or transformation from 'apple' to 'tree' is analogous to the semantic relationship or transformation from 'grape' to 'vine'. The text labels 'apple', 'tree', 'grape', and 'vine' are crucial as they define the specific entities involved in this analogy.

**Key Insights:**
The main takeaway from this image is that semantic analogies can be effectively modeled and solved using vector arithmetic in an embedding space. The image demonstrates that if the semantic relationship between a pair of words (e.g., 'apple' to 'tree') can be represented as a vector, then an analogous relationship from a third word (e.g., 'grape') can be found by applying the same vector transformation, leading to a fourth word (e.g., 'vine'). The specific text labels 'apple', 'tree', 'grape', and 'vine' provide the concrete instances for this abstract model, illustrating that the vector connecting 'apple' to 'tree' is equivalent to the vector connecting 'grape' to 'vine' in a semantic space. This principle underlies the capability of word embeddings to perform tasks like 'king - man + woman = queen'.

**Document Context:**
This image is directly relevant to Section 6.10, 'Semantic properties of embeddings,' as it provides a clear visual example of how word embeddings can encode and enable the solution of analogy problems. It illustrates the 'parallelogram model' mentioned in the accompanying text, showing how a target word's embedding (e.g., 'vine') can be found by performing vector arithmetic on related word embeddings (e.g., 'tree - apple + grape'). This visual metaphor helps to concretize the abstract concept of semantic relationships in high-dimensional vector spaces.

**Summary:**
The image displays a parallelogram model, visually representing how semantic analogies can be solved using vector arithmetic in an embedding space. Four points are labeled: 'apple' at the top-left, 'tree' at the top-right, 'grape' at the bottom-left, and 'vine' at the bottom-right. A solid black line with an arrow connects 'apple' to 'tree', indicating a vector from 'apple' to 'tree'. Another solid black line with an arrow connects 'grape' to 'vine', indicating a vector from 'grape' to 'vine'. The remaining two sides of the parallelogram are depicted by dashed lines: one connecting 'apple' to 'grape', and another connecting 'tree' to 'vine'. The entire area enclosed by these four points is shaded light green. This diagram illustrates the principle that the vector relationship between 'apple' and 'tree' is parallel and equal in magnitude to the vector relationship between 'grape' and 'vine', allowing for the deduction of 'vine's' position based on the other three.](images/d45bab812864266253225d997179aeaf3c297affa813f4b0f01d6eb0e36ec784.jpg)
Figure 6.15 The parallelogram model for analogy problems (Rumelhart and Abrahamson, 1973): the location of  vine can be found by subtracting  apple from  tree and adding  grape.

$\overrightarrow { \mathrm { m a n } } + \overrightarrow { \mathrm { w o m a n } }$ is a vector close to # » queen. Similarly,  Paris − France + Italy results# » in a vector that is close to  Rome. The embedding model thus seems to be extracting representations of relations like MALE-FEMALE, or CAPITAL-CITY-OF, or even COMPARATIVE/SUPERLATIVE, as shown in Fig. 6.16 from GloVe.

![## Image Analysis: 28f384f4e3c99f72d548f561b3efe2511e697a657bacdf4de5398d9344ce1900.jpg

**Conceptual Understanding:**
This image conceptually represents the 'semantic properties' of word embeddings, specifically from the GloVe (Global Vectors for Word Representation) model. It illustrates how these high-dimensional word vectors, when projected into a 2D space, effectively capture and organize complex linguistic relationships.

The main purpose of the image is to demonstrate that word embeddings encode meaning and relationships in a structured, quantifiable way. It shows that semantic analogies (like gender differences) and morphological variations (like comparative and superlative forms of adjectives) manifest as consistent geometric patterns (specifically, parallel vector offsets) within the embedding space.

Key ideas communicated are:
1.  **Vector Algebra for Semantics:** Analogical reasoning can be performed using vector arithmetic (e.g., `king - man + woman ≈ queen`).
2.  **Linguistic Regularities:** Embeddings capture not just individual word meanings but also the rules and patterns governing how words relate to each other (e.g., adding a 'gender vector' or a 'comparative vector').
3.  **Structured Representation:** The embedding space is not random but has an inherent structure that reflects human language's semantic and syntactic regularities.

**Content Interpretation:**
The image, composed of two scatter plots, demonstrates the ability of GloVe word embeddings to capture intricate semantic and morphological relationships between words within a vector space. Each point represents a word's embedding projected onto two dimensions, and dashed lines connect semantically or morphologically related words.

Sub-figure (a) illustrates semantic analogies, primarily focusing on gender and familial relationships. Words like 'king' and 'man' are related, and similarly, 'queen' and 'woman'. The dashed line connecting 'man' to 'king' is parallel to the line connecting 'woman' to 'queen', visually confirming the textual evidence that the vector difference (king - man) is approximately equal to (queen - woman), or that 'king - man + woman' is close to 'queen'. This shows that the vector space encodes semantic regularities where a consistent vector offset represents a specific relationship (e.g., 'male to female'). Other analogous pairs include 'brother' and 'sister', 'nephew' and 'niece', 'uncle' and 'aunt', demonstrating how familial roles are also captured. The section including 'emperor', 'duke', 'earl', 'sir' and their female counterparts 'empress', 'duchess', 'countess', 'madam', 'heiress' further reinforces the capturing of gender roles and hierarchical titles through parallel vector relationships.

Sub-figure (b) showcases the representation of morphological relationships, specifically comparative and superlative forms of adjectives. Words like 'short', 'shorter', and 'shortest' are positioned such that the vector from 'short' to 'shorter' and 'shorter' to 'shortest' are consistently oriented and spaced. This pattern is repeated for other adjective sets: 'slow', 'slower', 'slowest'; 'strong', 'stronger', 'strongest'; 'loud', 'louder', 'loudest'; 'clear', 'clearer', 'clearest'; 'soft', 'softer', 'softest'; and 'dark', 'darker', 'darkest'. The consistent 'steps' in the vector space for these morphological transformations indicate that the GloVe embeddings successfully capture these linguistic regularities as additive offsets. This means that a specific vector can be added to an adjective's embedding to obtain its comparative or superlative form.

**Key Insights:**
The main takeaways from this image are:

1.  **Semantic Analogies through Vector Offsets:** GloVe word embeddings capture complex semantic relationships (like gender, familial roles, and hierarchical titles) as consistent vector differences. For example, the vector from 'man' to 'king' is approximately parallel to the vector from 'woman' to 'queen', illustrating that the operation `vector(king) - vector(man) + vector(woman)` results in a vector very close to `vector(queen)`. This is supported by the parallel dashed lines connecting 'man' to 'king' and 'woman' to 'queen' in sub-figure (a), and similar patterns for other gendered or familial pairs like 'brother'/'sister', 'nephew'/'niece', 'uncle'/'aunt', 'heir'/'heiress', and 'emperor'/'empress'.

2.  **Morphological Regularities as Vector Progressions:** The embeddings also effectively represent morphological transformations (like comparative and superlative forms) as consistent additive offsets in the vector space. This is evident in sub-figure (b) where sequences like 'short' -> 'shorter' -> 'shortest' show a clear, consistent vector progression. The text elements 'slow', 'slower', 'slowest'; 'strong', 'stronger', 'strongest'; 'loud', 'louder', 'loudest'; 'clear', 'clearer', 'clearest'; 'soft', 'softer', 'softest'; and 'dark', 'darker', 'darkest' all visually demonstrate these uniform 'steps' in the 2D projection, indicating that the embeddings capture these grammatical rules.

3.  **Visualizability of Abstract Vector Spaces:** Even when projected onto a 2D plane, the relationships encoded in high-dimensional word embeddings remain discernible and interpretable, allowing for a visual understanding of their semantic properties. The consistent arrangement of related words and the parallel nature of the connecting vectors provide strong evidence for the structured nature of these learned representations.

These insights demonstrate that word embeddings are powerful tools for natural language processing because they not only represent words but also encode intricate linguistic knowledge in a mathematically tractable form.

**Document Context:**
This image is highly relevant to the document's section '6.10 Semantic properties of embeddings' as it provides visual evidence and concrete examples of how word embeddings, specifically GloVe vectors, encode rich semantic and morphological information. The accompanying text explicitly states that the figure demonstrates 'Relational properties of the GloVe vector space' and refers to the 'king - man + woman' analogy and 'comparative and superlative morphology'.

Sub-figure (a) directly supports the concept of semantic analogies, showing how relationships like gender (e.g., king-man, queen-woman) are represented as consistent vector differences. This illustrates the idea that operations in the vector space can correspond to meaningful linguistic transformations. Sub-figure (b) further extends this by demonstrating the capture of morphological properties, such as comparative and superlative forms of adjectives. This reinforces the broader point that word embeddings are not merely representations but contain learnable and extractable linguistic regularities, enhancing the understanding of how words relate to each other beyond simple co-occurrence counts. The image serves as a powerful visual aid to comprehend the abstract notion of semantic and morphological 'properties' within high-dimensional vector spaces.

**Summary:**
The image displays two scatter plots, (a) and (b), each illustrating relational properties within the GloVe vector space by projecting word vectors onto two dimensions. The plots show how semantic and morphological relationships between words are represented as consistent vector offsets.

Sub-figure (a) focuses on semantic analogies, specifically gender and familial relationships. It shows clusters of related words and dashed lines connecting them, indicating vector relationships. For example, a line connects 'man' to 'king' and a parallel line connects 'woman' to 'queen', visually representing the analogy 'man is to king as woman is to queen'. Other parallel relationships include 'brother' to 'nephew' and 'sister' to 'niece', 'uncle' to 'nephew', and 'aunt' to 'niece'. Another set of words forms an analogous relationship where 'heir' is connected to 'heiress', and 'madam' is nearby, suggesting gender-based differences. Similarly, words like 'earl', 'duke', and 'emperor' are connected to 'countess', 'duchess', and 'empress' respectively, again demonstrating gender transformation as a consistent vector offset in the embedding space.

Sub-figure (b) illustrates how GloVe vectors capture comparative and superlative morphology. It shows groups of words representing an adjective, its comparative form, and its superlative form, connected by dashed lines. For instance, 'short' is connected to 'shorter', which is then connected to 'shortest', forming a progression. Similar progressions are observed for other word sets: 'slow' -> 'slower' -> 'slowest'; 'strong' -> 'stronger' -> 'strongest'; 'loud' -> 'louder' -> 'loudest'; 'clear' -> 'clearer' -> 'clearest'; 'soft' -> 'softer' -> 'softest'; and 'dark' -> 'darker' -> 'darkest'. The consistent direction and length of the dashed lines for each progression indicate that the morphological transformation (e.g., from 'short' to 'shorter' or 'shorter' to 'shortest') corresponds to a relatively fixed vector offset in the projected space. Both plots use numerical scales for the x and y axes, without explicit text labels, ranging from approximately -0.5 to 0.5 for the x-axis and -0.2 to 0.5 for the y-axis in (a), and -0.4 to 0.6 for the x-axis and -0.3 to 0.5 for the y-axis in (b).](images/28f384f4e3c99f72d548f561b3efe2511e697a657bacdf4de5398d9344ce1900.jpg)
Figure 6.16 Relational properties of the GloVe vector space, shown by projecting vectors onto two dimensions. (a) ${ \overrightarrow { \mathrm { k i n g } } } - { \overrightarrow { \mathrm { m a n } } } + { \overrightarrow { \mathrm { w o m a n } } }$ is close to  queen. (b) offsets seem to capture comparative and superlative morphology (Pennington et al., 2014).

For a $\mathbf { a } : \mathbf { b } : : \mathbf { a } ^ { * } : \mathbf { b } ^ { * }$ problem, meaning the algorithm is given vectors a, b, and $\mathbf { a } ^ { * }$ and must find $\mathbf { b } ^ { * }$ , the parallelogram method is thus:

$$
\hat { \mathbf { b } } ^ { * } = \underset { \mathbf { x } } { \mathrm { a r g m i n ~ d i s t a n c e } } ( \mathbf { x } , \mathbf { b } - \mathbf { a } + \mathbf { a } ^ { * } )
$$

with some distance function, such as Euclidean distance.

There are some caveats. For example, the closest value returned by the parallelogram algorithm in word2vec or GloVe embedding spaces is usually not in fact $\mathbf { b } ^ { * }$ but one of the 3 input words or their morphological variants (i.e., cherry:red :: potato: $_ x$ returns potato or potatoes instead of brown), so these must be explicitly excluded. Furthermore while embedding spaces perform well if the task involves frequent words, small distances, and certain relations (like relating countries with their capitals or verbs/nouns with their inflected forms), the parallelogram method with embeddings doesn’t work as well for other relations (Linzen 2016, Gladkova et al. 2016, Schluter 2018, Ethayarajh et al. 2019a), and indeed Peterson et al. (2020) argue that the parallelogram method is in general too simple to model the human cognitive process of forming analogies of this kind.

# 6.10.1 Embeddings and Historical Semantics

Embeddings can also be a useful tool for studying how meaning changes over time, by computing multiple embedding spaces, each from texts written in a particular time period. For example Fig. 6.17 shows a visualization of changes in meaning in English words over the last two centuries, computed by building separate embedding spaces for each decade from historical corpora like Google n-grams (Lin et al.,CHAPTER 5. DYNAMIC SOCIAL REPRESENTATIONS OF WORD MEANING 2012b) and the Corpus of Historical American English (Davies, 2012).

![## Image Analysis: 4eda66f987fcee06083b4be877ac4ccd52a1879461514e9db3b3bfd5f6beb33f.jpg

**Conceptual Understanding:**
This image conceptually represents the phenomenon of **semantic change** (also known as semantic shift or lexical evolution) in the English language. It visually illustrates how the meaning of words evolves over historical periods. The main purpose of the image is to demonstrate this linguistic phenomenon using three distinct examples: the words "gay," "broadcast," and "awful." It shows how the associated concepts and contextual usage of these words have transformed over the 19th and 20th centuries.

The key ideas being communicated are:
1.  **Fluidity of Language:** Word meanings are not static but are dynamic and subject to change over time.
2.  **Context Dependency:** The meaning of a word is heavily influenced by the words it frequently appears with (its semantic neighbors).
3.  **Historical Linguistics through Computation:** Advanced computational methods, specifically word embeddings and dimensionality reduction (t-SNE), can be used to model and visualize these historical semantic shifts.

**Content Interpretation:**
The image shows three distinct instances of semantic change in the English language for the words "gay," "broadcast," and "awful" over various periods. These visualizations are likely generated using word embeddings (like SGword2vec as mentioned in the document context) and a dimensionality reduction technique such as t-SNE to represent semantic relationships in a 2D space. The proximity of context words to the main word indicates semantic relatedness at a given time.

**A. Semantic Shift of "gay" (1900s to 1990s):**
- In the 1900s, "gay" was associated with positive descriptors like "daft," "flaunting," "tasteful," "sweet," "cheerful," "pleasant," "frolicsome," and "witty." This indicates an original meaning related to cheerfulness, lightheartedness, or perhaps showiness.
- By the 1950s, the primary associations were still largely positive, including "cheerful," "pleasant," "frolicsome," "witty," and "bright," suggesting a continuation of its earlier sense, though perhaps becoming more focused.
- The shift to the 1990s shows a dramatic change, with "gay" becoming associated with terms like "gays," "bisexual," "homosexual," and "lesbian," clearly illustrating its modern primary association with homosexuality. This is a clear example of semantic specialization and reappropriation.

**B. Semantic Shift of "broadcast" (1850s to 1990s):**
- In the 1850s, "broadcast" was associated with agricultural terms such as "spread," "sow," "seed," "sows," and "scatter," along with "circulated," reflecting its original meaning related to scattering seeds over a wide area.
- By the 1900s, while "circulated" remained, new associations emerged like "newspapers," "television," and "radio," indicating a transition towards the meaning of transmitting information or media.
- The 1990s further solidified this modern sense, with strong associations like "bbc," "radio," "television," and "newspapers," clearly defining its role in media transmission.

**C. Semantic Shift of "awful" (1850s to 1990s):**
- In the 1850s, "awful" was linked to terms such as "solemn," "majestic," "awe," and "dread," as well as "pensive" and "gloomy." This suggests a meaning related to inspiring awe, reverence, or deep respect (or sometimes fear).
- By the 1900s, while "awe" and "dread" were still present, the word began to associate with negative terms like "horrible," "appalling," and "terrible," alongside "gloomy" and "wonderful," showing a shift towards its pejorative sense, but possibly still retaining some positive associations.
- The 1990s decisively moved towards its negative, pejorative meaning, with strong associations like "horrible," "appalling," "terrible," "awfully," and "weird." The term "wonderful" also appears, which could indicate its use as an intensifier even in negative contexts (e.g., "awful lot").

Overall, the image effectively visualizes the dynamic nature of language, where word meanings are not static but evolve significantly over time, often driven by changes in technology, culture, and social understanding. The context words provide concrete evidence for these shifts, acting as semantic neighbors that define the core meaning of the target word at each historical juncture.

**Key Insights:**
The main takeaways from this image are:

1.  **Words are not static entities; their meanings evolve significantly over time.** This is evident in all three examples: "gay" shifts from cheerfulness to sexuality, "broadcast" from agriculture to media, and "awful" from reverence to negativity.
2.  **Semantic change can involve various processes:**
    *   **Specialization/Reappropriation (e.g., gay):** A word's meaning can narrow or shift to a specific, often new, domain. The transition from general positive emotional states ("cheerful," "frolicsome") to specific sexual identity ("homosexual," "lesbian") for "gay" is a prime example.
    *   **Metaphorical Extension/Domain Shift (e.g., broadcast):** Meanings can extend from a concrete original sense to an abstract or technical one, often driven by technological advancements. "Broadcast" moving from scattering seeds to disseminating information via electronic media illustrates this.
    *   **Pejoration (e.g., awful):** A word's meaning can become more negative or derogatory over time. "Awful" changing from inspiring "awe" or "majesty" to signifying something "horrible" or "terrible" is a clear case of pejoration.
3.  **Context words are crucial indicators of a word's meaning at a given time.** The surrounding grey words define the semantic space of the target word. For instance, the context words for "gay (1900s)" like "cheerful" and "witty" clearly differ from "gay (1990s)" context words like "homosexual" and "lesbian," providing direct textual evidence for the semantic shift.
4.  **Word embeddings provide a quantitative way to visualize and study semantic change.** The t-SNE visualization demonstrates how these computational methods can capture the gradual or abrupt shifts in word meanings, allowing researchers to track historical linguistic evolution. The blue arrows visually represent the trajectory of these changes through time.

These insights are directly supported by the verbatim text extraction, which details the specific context words and time periods for each word's semantic journey.

**Document Context:**
This image directly supports and visualizes the discussion in Section 6.10.1 "Embeddings and Historical Semantics" by providing concrete examples of semantic change in English words. The preceding and following text explicitly refers to these examples:

- It explains the shift of "gay" from "cheerful" to "homosexuality." The image shows "cheerful," "frolicsome," "witty" in earlier periods transitioning to "gays," "bisexual," "homosexual," "lesbian" by the 1990s.
- It describes the development of "broadcast" from "sowing seeds" to "transmitting signals." The image clearly shows "sow," "seed," "sows," "scatter" for the 1850s transitioning to "newspapers," "television," "radio," "bbc" for later periods.
- It illustrates the pejoration of "awful" from "full of awe" to "terrible or appalling." The image demonstrates this by showing "solemn," "majestic," "awe," "dread" in the 1850s shifting to "horrible," "appalling," "terrible" by the 1990s.

The figure acts as a visual demonstration of how word embedding techniques (like SGword2vec mentioned in the text) can be used to track and represent these historical semantic changes, thereby enhancing the reader's understanding of the theoretical concepts discussed in the section. It visually confirms the claims made in the text, making the abstract idea of semantic evolution tangible.

**Summary:**
This image, titled "A t-SNE visualization of the semantic change of 3 words in English," presents three distinct visualizations (A, B, C) illustrating the historical semantic evolution of the words "gay," "broadcast," and "awful" across different time periods. Each visualization uses a t-SNE plot to show the word's position in an embedding space relative to its most frequent context words at specific historical points, with blue arrows indicating the direction of semantic shift over time. The main words for each time period are in bold, and their associated context words are in light grey. The document context states that the modern sense of each word and its grey context words are computed from the most recent (modern) time-point embedding space, while earlier points are computed from earlier historical embedding spaces. The image demonstrates how word meanings are fluid and evolve, influenced by cultural and societal changes.](images/4eda66f987fcee06083b4be877ac4ccd52a1879461514e9db3b3bfd5f6beb33f.jpg)
Figure 6.17 A t-SNE visualization of the semantic change of 3 words in English using Figure 5.1: Two-dimensional visualization of semantic change in English using SGword2vec vectors. The modern sense of each word, and the grey context words, are computed from the most recent (modern) time-point embedding space. Earlier points are comfrom meaning “cheerful” or “frolicsome” to referring to homosexuality. A, In the earputed from earlier historical embedding spaces. The visualizations show the changes in the 20th century broadcast referred to “casting out seeds”; with the rise of television aword gay from meanings related to “cheerful” or “frolicsome” to referring to homosexuality, radio its meaning shifted to “transmitting signals”. C, Awful underwent a processthe development of the modern “transmission” sense of broadcast from its original sense of pejoration, as it shifted from meaning “full of awe” to meaning “terrible or appallinsowing seeds, and the pejoration of the word awful as it shifted from meaning “full of awe” [212].to meaning “terrible or appalling” (Hamilton et al., 2016b).

# where they shift from objective s6.11 Bias and Embeddings

# allocational harm

In addition to their ability to learn word meaning from text, embeddings, alas, also reproduce the implicit biases and stereotypes that were latent in the text. As 5.2.2 Computational linguistic studiesthe prior section just showed, embeddings can roughly model relational similarity: ‘queen’ as the closest word to ‘king’ - ‘man’ $^ +$ ‘woman’ implies the analogy man:woman::king:queen. But these same embedding analogies also exhibit gender methods. [200] use latent semantic analysis to analyze how word meanings broadstereotypes. For example Bolukbasi et al. (2016) find that the closest occupation and narrow over time. [113] use rto ‘computer programmer’ - ‘man’ $^ +$ co-occurrence vectors to perform a number ‘woman’ in word2vec embeddings trained on news text is ‘homemaker’, and that the embeddings similarly suggest the analogy ‘father’ is to ‘doctor’ as ‘mother’ is to ‘nurse’. This could result in what Crawford (2017) and Blodgett et al. (2020) call an allocational harm, when a system alloinformation-based embeddings and found that semantic changes uncovered by thcates resources (jobs or credit) unfairly to different groups. For example algorithms method had reasonable agreement with human judgments. [129] and [119] use “neurthat use embeddings as part of a search for hiring potential programmers or doctors might thus incorrectly downweight documents with women’s names.

bias amplification

It turns out that embeddings don’t just reflect the statistics of their input, but also historical co-occurrences to test whether synonyms tend to change in similar waysamplify bias; gendered terms become more gendered in embedding space than they were in the input text statistics (Zhao et al. 2017, Ethayarajh et al. 2019b, Jia et al. 2020), and biases are more exaggerated than in actual labor employment statistics (Garg et al., 2018).

Embeddings also encode the implicit associations that are a property of human reasoning. The Implicit Association Test (Greenwald et al., 1998) measures peo

representational harm

ple’s associations between concepts (like ‘flowers’ or ‘insects’) and attributes (like ‘pleasantness’ and ‘unpleasantness’) by measuring differences in the latency with which they label words in the various categories.7 Using such methods, people in the United States have been shown to associate African-American names with unpleasant words (more than European-American names), male names more with mathematics and female names with the arts, and old people’s names with unpleasant words (Greenwald et al. 1998, Nosek et al. 2002a, Nosek et al. 2002b). Caliskan et al. (2017) replicated all these findings of implicit associations using GloVe vectors and cosine similarity instead of human latencies. For example African-American names like ‘Leroy’ and ‘Shaniqua’ had a higher GloVe cosine with unpleasant words while European-American names (‘Brad’, ‘Greg’, ‘Courtney’) had a higher cosine with pleasant words. These problems with embeddings are an example of a representational harm (Crawford 2017, Blodgett et al. 2020), which is a harm caused by a system demeaning or even ignoring some social groups. Any embedding-aware algorithm that made use of word sentiment could thus exacerbate bias against African Americans.

debiasing

Recent research focuses on ways to try to remove these kinds of biases, for example by developing a transformation of the embedding space that removes gender stereotypes but preserves definitional gender (Bolukbasi et al. 2016, Zhao et al. 2017) or changing the training procedure (Zhao et al., 2018b). However, although these sorts of debiasing may reduce bias in embeddings, they do not eliminate it (Gonen and Goldberg, 2019), and this remains an open problem.

Historical embeddings are also being used to measure biases in the past. Garg et al. (2018) used embeddings from historical texts to measure the association between embeddings for occupations and embeddings for names of various ethnicities or genders (for example the relative cosine similarity of women’s names versus men’s to occupation words like ‘librarian’ or ‘carpenter’) across the 20th century. They found that the cosines correlate with the empirical historical percentages of women or ethnic groups in those occupations. Historical embeddings also replicated old surveys of ethnic stereotypes; the tendency of experimental participants in 1933 to associate adjectives like ‘industrious’ or ‘superstitious’ with, e.g., Chinese ethnicity, correlates with the cosine between Chinese last names and those adjectives using embeddings trained on 1930s text. They also were able to document historical gender biases, such as the fact that embeddings for adjectives related to competence (‘smart’, ‘wise’, ‘thoughtful’, ‘resourceful’) had a higher cosine with male than female words, and showed that this bias has been slowly decreasing since 1960. We return in later chapters to this question about the role of bias in natural language processing.

# 6.12 Evaluating Vector Models

The most important evaluation metric for vector models is extrinsic evaluation on tasks, i.e., using vectors in an NLP task and seeing whether this improves performance over some other model.

Nonetheless it is useful to have intrinsic evaluations. The most common metric is to test their performance on similarity, computing the correlation between an algorithm’s word similarity scores and word similarity ratings assigned by humans. WordSim-353 (Finkelstein et al., 2002) is a commonly used set of ratings from 0 to 10 for 353 noun pairs; for example (plane, car) had an average score of 5.77. SimLex-999 (Hill et al., 2015) is a more complex dataset that quantifies similarity (cup, mug) rather than relatedness (cup, coffee), and includes concrete and abstract adjective, noun and verb pairs. The TOEFL dataset is a set of 80 questions, each consisting of a target word with 4 additional word choices; the task is to choose which is the correct synonym, as in the example: Levied is closest in meaning to: imposed, believed, requested, correlated (Landauer and Dumais, 1997). All of these datasets present words without context.

Slightly more realistic are intrinsic similarity tasks that include context. The Stanford Contextual Word Similarity (SCWS) dataset (Huang et al., 2012) and the Word-in-Context (WiC) dataset (Pilehvar and Camacho-Collados, 2019) offer richer evaluation scenarios. SCWS gives human judgments on 2,003 pairs of words in their sentential context, while WiC gives target words in two sentential contexts that are either in the same or different senses; see Appendix G. The semantic textual similarity task (Agirre et al. 2012, Agirre et al. 2015) evaluates the performance of sentence-level similarity algorithms, consisting of a set of pairs of sentences, each pair with human-labeled similarity scores.

Another task used for evaluation is the analogy task, discussed on page 124, where the system has to solve problems of the form a is to b as a\* is to $b ^ { * }$ , given a, b, and $a ^ { * }$ and having to find $b ^ { * }$ (Turney and Littman, 2005). A number of sets of tuples have been created for this task (Mikolov et al. 2013a, Mikolov et al. 2013c, Gladkova et al. 2016), covering morphology (city:cities::child:children), lexicographic relations (leg:table::spout:teapot) and encyclopedia relations (Beijing:China::Dublin:Ireland), some drawing from the SemEval-2012 Task 2 dataset of 79 different relations (Jurgens et al., 2012).

All embedding algorithms suffer from inherent variability. For example because of randomness in the initialization and the random negative sampling, algorithms like word2vec may produce different results even from the same dataset, and individual documents in a collection may strongly impact the resulting embeddings (Tian et al. 2016, Hellrich and Hahn 2016, Antoniak and Mimno 2018). When embeddings are used to study word associations in particular corpora, therefore, it is best practice to train multiple embeddings with bootstrap sampling over documents and average the results (Antoniak and Mimno, 2018).

# 6.13 Summary

• In vector semantics, a word is modeled as a vector—a point in high-dimensional space, also called an embedding. In this chapter we focus on static embeddings, where each word is mapped to a fixed embedding. • Vector semantic models fall into two classes: sparse and dense. In sparse models each dimension corresponds to a word in the vocabulary $V$ and cells are functions of co-occurrence counts. The term-document matrix has a row for each word (term) in the vocabulary and a column for each document. The word-context or term-term matrix has a row for each (target) word in the vocabulary and a column for each context term in the vocabulary. Two sparse weightings are common: the tf-idf weighting which weights each cell by its term frequency and inverse document frequency, and PPMI (pointwise positive mutual information), which is most common for word-context matrices.

• Dense vector models have dimensionality 50–1000. Word2vec algorithms like skip-gram are a popular way to compute dense embeddings. Skip-gram trains a logistic regression classifier to compute the probability that two words are ‘likely to occur nearby in text’. This probability is computed from the dot product between the embeddings for the two words.   
• Skip-gram uses stochastic gradient descent to train the classifier, by learning embeddings that have a high dot product with embeddings of words that occur nearby and a low dot product with noise words.   
• Other important embedding algorithms include GloVe, a method based on ratios of word co-occurrence probabilities.   
• Whether using sparse or dense vectors, word and document similarities are computed by some function of the dot product between vectors. The cosine of two vectors—a normalized dot product—is the most popular such metric.

# Bibliographical and Historical Notes

The idea of vector semantics arose out of research in the 1950s in three distinct fields: linguistics, psychology, and computer science, each of which contributed a fundamental aspect of the model.

The idea that meaning is related to the distribution of words in context was widespread in linguistic theory of the 1950s, among distributionalists like Zellig Harris, Martin Joos, and J. R. Firth, and semioticians like Thomas Sebeok. As Joos (1950) put it,

the linguist’s “meaning” of a morpheme. . . is by definition the set of conditional probabilities of its occurrence in context with all other morphemes.

The idea that the meaning of a word might be modeled as a point in a multidimensional semantic space came from psychologists like Charles E. Osgood, who had been studying how people responded to the meaning of words by assigning values along scales like happy/sad or hard/soft. Osgood et al. (1957) proposed that the meaning of a word in general could be modeled as a point in a multidimensional Euclidean space, and that the similarity of meaning between two words could be modeled as the distance between these points in the space.

A final intellectual source in the 1950s and early 1960s was the field then called mechanical indexing, now known as information retrieval. In what became known as the vector space model for information retrieval (Salton 1971, Sparck Jones 1986), researchers demonstrated new ways to define the meaning of words in terms of vectors (Switzer, 1965), and refined methods for word similarity based on measures of statistical association between words like mutual information (Giuliano, 1965) and idf (Sparck Jones, 1972), and showed that the meaning of documents could be represented in the same vector spaces used for words. Around the same time, (Cordier, 1965) showed that factor analysis of word association probabilities could be used to form dense vector representations of words.

Some of the philosophical underpinning of the distributional way of thinking came from the late writings of the philosopher Wittgenstein, who was skeptical of the possibility of building a completely formal theory of meaning definitions for each word. Wittgenstein suggested instead that “the meaning of a word is its use in the language” (Wittgenstein, 1953, PI 43). That is, instead of using some logical language to define each word, or drawing on denotations or truth values, Wittgenstein’s idea is that we should define a word by how it is used by people in speaking and understanding in their day-to-day interactions, thus prefiguring the movement toward embodied and experiential models in linguistics and NLP (Glenberg and Robertson 2000, Lake and Murphy 2021, Bisk et al. 2020, Bender and Koller 2020).

More distantly related is the idea of defining words by a vector of discrete features, which has roots at least as far back as Descartes and Leibniz (Wierzbicka 1992, Wierzbicka 1996). By the middle of the 20th century, beginning with the work of Hjelmslev (Hjelmslev, 1969) (originally 1943) and fleshed out in early models of generative grammar (Katz and Fodor, 1963), the idea arose of representing meaning with semantic features, symbols that represent some sort of primitive meaning. For example words like hen, rooster, or chick, have something in common (they all describe chickens) and something different (their age and sex), representable as:

hen $^ +$ female, $^ +$ chicken, +adult rooster -female, $^ +$ chicken, $^ +$ adult chick $^ +$ chicken, -adult

The dimensions used by vector models of meaning to define words, however, are only abstractly related to this idea of a small fixed number of hand-built dimensions. Nonetheless, there has been some attempt to show that certain dimensions of embedding models do contribute some specific compositional aspect of meaning like these early semantic features.

The use of dense vectors to model word meaning, and indeed the term embedding, grew out of the latent semantic indexing (LSI) model (Deerwester et al., 1988) recast as LSA (latent semantic analysis) (Deerwester et al., 1990). In LSA singular value decomposition—SVD— is applied to a term-document matrix (each cell weighted by log frequency and normalized by entropy), and then the first 300 dimensions are used as the LSA embedding. Singular Value Decomposition (SVD) is a method for finding the most important dimensions of a data set, those dimensions along which the data varies the most. LSA was then quickly widely applied: as a cognitive model Landauer and Dumais (1997), and for tasks like spell checking (Jones and Martin, 1997), language modeling (Bellegarda 1997, Coccaro and Jurafsky 1998, Bellegarda 2000), morphology induction (Schone and Jurafsky 2000, Schone and Jurafsky 2001b), multiword expressions (MWEs) (Schone and Jurafsky, 2001a), and essay grading (Rehder et al., 1998). Related models were simultaneously developed and applied to word sense disambiguation by Schutze ¨ (1992b). LSA also led to the earliest use of embeddings to represent words in a probabilistic classifier, in the logistic regression document router of Schutze et al. ¨ (1995). The idea of SVD on the term-term matrix (rather than the term-document matrix) as a model of meaning for NLP was proposed soon after LSA by Schutze ¨ (1992b). Schutze applied the low-rank (97-dimensional) embeddings produced by SVD to the ¨ task of word sense disambiguation, analyzed the resulting semantic space, and also suggested possible techniques like dropping high-order dimensions. See Schutze ¨ (1997).

A number of alternative matrix models followed on from the early SVD work, including Probabilistic Latent Semantic Indexing (PLSI) (Hofmann, 1999), Latent

Dirichlet Allocation (LDA) (Blei et al., 2003), and Non-negative Matrix Factorization (NMF) (Lee and Seung, 1999).

The LSA community seems to have first used the word “embedding” in Landauer et al. (1997), in a variant of its mathematical meaning as a mapping from one space or mathematical structure to another. In LSA, the word embedding seems to have described the mapping from the space of sparse count vectors to the latent space of SVD dense vectors. Although the word thus originally meant the mapping from one space to another, it has metonymically shifted to mean the resulting dense vector in the latent space, and it is in this sense that we currently use the word.

By the next decade, Bengio et al. (2003) and Bengio et al. (2006) showed that neural language models could also be used to develop embeddings as part of the task of word prediction. Collobert and Weston (2007), Collobert and Weston (2008), and Collobert et al. (2011) then demonstrated that embeddings could be used to represent word meanings for a number of NLP tasks. Turian et al. (2010) compared the value of different kinds of embeddings for different NLP tasks. Mikolov et al. (2011) showed that recurrent neural nets could be used as language models. The idea of simplifying the hidden layer of these neural net language models to create the skipgram (and also CBOW) algorithms was proposed by Mikolov et al. (2013a). The negative sampling training algorithm was proposed in Mikolov et al. (2013b). There are numerous surveys of static embeddings and their parameterizations (Bullinaria and Levy 2007, Bullinaria and Levy 2012, Lapesa and Evert 2014, Kiela and Clark 2014, Levy et al. 2015).

See Manning et al. (2008) and Chapter 14 for a deeper understanding of the role of vectors in information retrieval, including how to compare queries with documents, more details on tf-idf, and issues of scaling to very large datasets. See Kim (2019) for a clear and comprehensive tutorial on word2vec. Cruse (2004) is a useful introductory linguistic text on lexical semantics.

# Exercises

# Neural Networks

“[M]achines of this character can behave in a very complicated manner when the number of units is large.”

Alan Turing (1948) “Intelligent Machines”, page 6

Neural networks are a fundamental computational tool for language processing, and a very old one. They are called neural because their origins lie in the McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. But the modern use in language processing no longer draws on these early biological inspirations.

Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iteratively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers).

Neural networks share much of the same mathematics as logistic regression. But neural networks are a more powerful classifier than logistic regression, and indeed a minimal neural network (technically one with a single ‘hidden layer’) can be shown to learn any function.

Neural net classifiers are different from logistic regression in another way. With logistic regression, we applied the regression classifier to many different tasks by developing many rich kinds of feature templates based on domain knowledge. When working with neural networks, it is more common to avoid most uses of rich handderived features, instead building neural networks that take raw words as inputs and learn to induce features as part of the process of learning to classify. We saw examples of this kind of representation learning for embeddings in Chapter 6. Nets that are very deep are particularly good at representation learning. For that reason deep neural nets are the right tool for tasks that offer sufficient data to learn features automatically.

In this chapter we’ll introduce feedforward networks as classifiers, and also apply them to the simple task of language modeling: assigning probabilities to word sequences and predicting upcoming words. In subsequent chapters we’ll introduce many other aspects of neural models, such as recurrent neural networks (Chapter 8), the Transformer (Chapter 9), and masked language modeling (Chapter 11).

# 7.1 Units

# bias term

The building block of a neural network is a single computational unit. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output.

At its heart, a neural unit is taking a weighted sum of its inputs, with one additional term in the sum called a bias term. Given a set of inputs $x _ { 1 } . . . x _ { n }$ , a unit has a set of corresponding weights $w _ { 1 } . . . w _ { n }$ and a bias $^ b$ , so the weighted sum $z$ can be represented as:

$$
z = b + \sum _ { i } w _ { i } x _ { i }
$$

# vector

Often it’s more convenient to express this weighted sum using vector notation; recall from linear algebra that a vector is, at heart, just a list or array of numbers. Thus we’ll talk about $z$ in terms of a weight vector $w$ , a scalar bias $^ b$ , and an input vector $x$ , and we’ll replace the sum with the convenient dot product:

$$
{ z = { \mathbf { w } } \cdot { \mathbf { x } } + b }
$$

# activation

As defined in Eq. 7.2, $z$ is just a real valued number.

Finally, instead of using $z$ , a linear function of $x _ { \ast }$ , as the output, neural units apply a non-linear function $f$ to $z$ . We will refer to the output of this function as the activation value for the unit, $a$ . Since we are just modeling a single unit, the activation for the node is in fact the final output of the network, which we’ll generally call $y$ . So the value $y$ is defined as:

$$
y = a = f ( z )
$$

sigmoid

We’ll discuss three popular non-linear functions $f$ below (the sigmoid, the tanh, and the rectified linear unit or ReLU) but it’s pedagogically convenient to start with the sigmoid function since we saw it in Chapter 5:

$$
y = \sigma ( z ) = \frac { 1 } { 1 + e ^ { - z } }
$$

The sigmoid (shown in Fig. 7.1) has a number of advantages; it maps the output into the range $( 0 , 1 )$ , which is useful in squashing outliers toward 0 or 1. And it’s differentiable, which as we saw in Section 5.10 will be handy for learning.

![## Image Analysis: 8debc2c3f74c984dc6b8dcb05370029605abbad6ce975b63dc0344bd974fd858.jpg

**Conceptual Understanding:**
The image conceptually represents the sigmoid activation function, a fundamental component in neural networks and logistic regression. Its main purpose is to graphically illustrate how this mathematical function transforms any real-valued input into an output value constrained within the range of (0, 1). The key idea being communicated is the function's characteristic S-shape, demonstrating its non-linear behavior, where inputs close to zero are mapped with a relatively linear response, while extreme positive or negative inputs are 'squashed' towards the limits of 1 or 0, respectively. This squashing property is crucial for interpreting outputs as probabilities or for introducing non-linearity into models.

**Content Interpretation:**
The image shows the graphical representation of the sigmoid activation function, defined by the equation \["\sigma(z) = 1 / (1 + e^{-z})"\] (as transcribed directly from the image). The x-axis represents the input 'z' to the function, ranging from -8 to 8, with labeled intervals. The y-axis represents the output '\sigma(z)', ranging from 0.0 to 1.0, also with labeled intervals. The curve itself is S-shaped, which is characteristic of the sigmoid function. It visually demonstrates that as the input 'z' decreases significantly, the output '\sigma(z)' approaches 0. Conversely, as 'z' increases significantly, '\sigma(z)' approaches 1. At z=0, the curve passes through \sigma(z)=0.5. The central portion of the curve around z=0 exhibits a relatively steep slope, indicating a more linear-like behavior for inputs close to zero, while the tails of the curve flatten out, signifying that very large or very small inputs are compressed towards 0 or 1, respectively. This 'squashing' behavior is a key characteristic of the sigmoid function, allowing it to normalize outputs within a bounded range.

**Key Insights:**
The main takeaways from this image are:1.  **Definition of the Sigmoid Function:** The image explicitly provides the mathematical formula: \["\sigma(z) = 1 / (1 + e^{-z})"\] (extracted from the graph). This is the foundational definition for understanding its behavior.2.  **Input and Output Range:** The graph visually demonstrates that the sigmoid function takes any real number as input (x-axis 'z' ranging from -8 to 8 and implicitly beyond) and maps it to an output range between 0 and 1 (y-axis '\sigma(z)' ranging from 0.0 to 1.0). This is directly shown by the bounds of the plotted curve and the axis labels.3.  **Non-linear and 'Squashing' Behavior:** The S-shaped curve illustrates that the function is non-linear. Specifically, it shows a steep, almost linear, slope around z=0, meaning small changes in 'z' lead to significant changes in '\sigma(z)' near this point. For large positive or negative values of 'z' (outliers), the curve flattens out, indicating that the function 'squashes' these extreme inputs towards 0 or 1, respectively. For example, at z=-8, \sigma(z) is very close to 0, and at z=8, \sigma(z) is very close to 1 (evidenced by the curve's behavior at the extremes of the x-axis).4.  **Midpoint Value:** The graph clearly shows that at z=0, the output \sigma(z) is 0.5 (where the curve crosses the y-axis).

**Document Context:**
This image directly supports the document's "activation" section by providing a visual and mathematical definition of the sigmoid function. The accompanying text, "Figure 7.1 The sigmoid function takes a real value and maps it to the range (0, 1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1," explicitly links the graph to its purpose. In the context of neural networks, activation functions like sigmoid introduce non-linearity, which is essential for learning complex patterns. By illustrating how the sigmoid function maps inputs to a bounded range (0 to 1), the image helps readers understand its role in tasks such as binary classification (where outputs can be interpreted as probabilities) or gating mechanisms within network layers. It visually explains the mathematical operation that transforms an arbitrary real-valued input into a standardized output, which is fundamental to the concept of activation in artificial neural networks.

**Summary:**
The image displays a two-dimensional graph illustrating the sigmoid function, which is a common activation function in machine learning. The horizontal axis is labeled "z" and represents the input values, with tick marks at -8, -6, -4, -2, 0, 2, 4, 6, and 8. The vertical axis is labeled "\[\sigma(z)\]" and represents the output of the function, with tick marks at 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0. A prominent blue S-shaped curve is plotted on these axes. The mathematical definition of the function, "\[\sigma(z) = 1 / (1 + e^{-z})\]", is clearly displayed above the curve. The graph shows that for negative values of z, the output \sigma(z) starts near 0 and gradually increases. Around z=0, the curve shows a steep ascent, indicating that the function is highly sensitive to changes in input. At z=0, \sigma(z) is exactly 0.5. For positive values of z, the output \sigma(z) continues to increase but flattens out, asymptotically approaching 1 as z becomes larger. This visualization clearly demonstrates how the sigmoid function maps any real number input (z) to an output value between 0 and 1, effectively squashing extreme values towards these limits while showing a near-linear response around the origin.](images/8debc2c3f74c984dc6b8dcb05370029605abbad6ce975b63dc0344bd974fd858.jpg)
Figure 7.1 The sigmoid function takes a real value and maps it to the range $( 0 , 1 )$ . It is nearly linear around 0 but outlier values get squashed toward 0 or 1.

Substituting Eq. 7.2 into Eq. 7.3 gives us the output of a neural unit:

$$
y = \pmb { \sigma } ( \mathbf { w } \cdot \mathbf { x } + b ) = \frac { 1 } { 1 + \exp ( - ( \mathbf { w } \cdot \mathbf { x } + b ) ) }
$$

Fig. 7.2 shows a final schematic of a basic neural unit. In this example the unit takes 3 input values $x _ { 1 } , x _ { 2 }$ , and $x _ { 3 }$ , and computes a weighted sum, multiplying each value by a weight $( w _ { 1 } , w _ { 2 }$ , and $w _ { 3 }$ , respectively), adds them to a bias term $^ b$ , and then passes the resulting sum through a sigmoid function to result in a number between 0 and 1.

![## Image Analysis: bc83bb82488e77ed84120435e34799ad10024b1758216590d467430446006383.jpg

**Conceptual Understanding:**
This image conceptually represents an artificial neuron or neural unit, which is the basic processing element of an artificial neural network. Its main purpose is to demonstrate the process by which multiple inputs are combined, adjusted by weights and a bias, and then transformed by an activation function to produce a single output. It illustrates the core computational logic of how a neuron 'fires' or activates based on its inputs.

**Content Interpretation:**
The image illustrates the internal mechanism of a single neural unit, a foundational component of artificial neural networks. It demonstrates how input signals are processed to generate an output. The key processes shown are: weighted summation of inputs, the addition of a bias term, and the application of a non-linear activation function. The diagram highlights the transformation of raw inputs (x1, x2, x3, +1) into a single output (y) through these sequential computational steps.

**Key Insights:**
The main takeaway from this image is the fundamental process of how a single neuron in a neural network computes its output. Key insights include: 1. Inputs are weighted: Each input (x1, x2, x3) is multiplied by a corresponding weight (w1, w2, w3), indicating their relative importance. 2. A bias term is included: A constant input (+1) with its own weight (b) is added, allowing the neuron to shift its activation function. 3. Inputs are summed: All weighted inputs and the weighted bias are summed together, yielding an intermediate value 'z'. 4. Non-linear activation: The sum 'z' is transformed by an activation function (σ) to produce 'a', introducing non-linearity crucial for learning complex patterns. 5. Output generation: The value 'a' represents the output of the neural unit, also labeled 'y' in this context. The diagram, with labels like 'x1', 'w1', 'Σ', 'z', 'σ', 'a', and 'y', provides direct textual evidence for each of these steps, clearly outlining the mathematical operations within a neural unit.

**Document Context:**
This image is highly relevant to the 'activation' section of the document, as stated in the context. It serves as a visual explanation of how an individual neural unit processes information and produces an 'activation' (denoted as 'a') before potentially contributing to a deeper network's final output. The accompanying text explicitly defines 'a' as the activation of an individual node and 'y' as the unit's output in this simple case, aligning perfectly with the diagram's labels. It forms the basic building block upon which more complex neural network architectures are built, making it critical for understanding the subsequent concepts of network activation and propagation.

**Summary:**
This diagram illustrates the fundamental structure and operation of a single neural unit, also known as a perceptron or node in a neural network. It depicts the process where multiple inputs are received, weighted, summed, and then passed through an activation function to produce an output. The flow begins with distinct inputs x1, x2, x3, and a bias input represented as +1. Each input (x1, x2, x3) is associated with a corresponding weight (w1, w2, w3), while the bias input +1 is associated with a bias weight 'b'. These weighted inputs, along with the weighted bias, are fed into a summation unit, symbolized by 'Σ'. The output of this summation is an intermediate variable labeled 'z'. This sum 'z' then proceeds to an activation function unit, symbolized by 'σ' within a square. The output of this activation function is denoted as 'a'. Finally, 'a' represents the output of this individual neural unit, which is also labeled as 'y', signifying the final output. The entire processing core, including the summation and activation function, is visually grouped within a large red circle, indicating it constitutes the core of the neural unit's operation. The detailed step-by-step process ensures that the reader understands how each input contributes to the final output through a series of weighted sums and a non-linear transformation.](images/bc83bb82488e77ed84120435e34799ad10024b1758216590d467430446006383.jpg)
Figure 7.2 A neural unit, taking 3 inputs $x _ { 1 } , x _ { 2 }$ , and $x _ { 3 }$ (and a bias $^ b$ that we represent as a weight for an input clamped at $+ 1$ ) and producing an output y. We include some convenient intermediate variables: the output of the summation, $z$ , and the output of the sigmoid, $a$ . In this case the output of the unit $y$ is the same as $a$ , but in deeper networks we’ll reserve $y$ to mean the final output of the entire network, leaving $a$ as the activation of an individual node.

Let’s walk through an example just to get an intuition. Let’s suppose we have a unit with the following weight vector and bias:

$$
\begin{array} { l } { \mathbf { w } \ = \ [ 0 . 2 , 0 . 3 , 0 . 9 ] } \\ { b \ = \ 0 . 5 } \end{array}
$$

What would this unit do with the following input vector:

$$
\textbf { \em x } = ~ [ 0 . 5 , 0 . 6 , 0 . 1 ]
$$

The resulting output $y$ would be:

$$
{ \boldsymbol { y } } = { \boldsymbol { \sigma } } ( \mathbf { w } \cdot \mathbf { x } + b ) = { \frac { 1 } { 1 + e ^ { - ( \mathbf { w } \cdot \mathbf { x } + b ) } } } = { \frac { 1 } { 1 + e ^ { - ( { \boldsymbol { . 5 * . 2 } } + { \boldsymbol { . 6 * . 3 } } + . 1 * . 9 + . 5 ) } } } = { \frac { 1 } { 1 + e ^ { - 0 . 8 7 } } } = . 7 0
$$

tanh

In practice, the sigmoid is not commonly used as an activation function. A function that is very similar but almost always better is the tanh function shown in Fig. 7.3a; tanh is a variant of the sigmoid that ranges from -1 to $+ 1$ :

$$
y = \operatorname { t a n h } ( z ) = \frac { e ^ { z } - e ^ { - z } } { e ^ { z } + e ^ { - z } }
$$

The simplest activation function, and perhaps the most commonly used, is the recReLU tified linear unit, also called the ReLU, shown in Fig. 7.3b. It’s just the same as $z$ when $z$ is positive, and 0 otherwise:

$$
y = \mathrm { R e L U } ( \mathrm { z } ) = m a x ( \mathrm { z } , 0 )
$$

These activation functions have different properties that make them useful for different language applications or network architectures. For example, the tanh function has the nice properties of being smoothly differentiable and mapping outlier values toward the mean. The rectifier function, on the other hand, has nice properties that

![## Image Analysis: 1f5376a4aa7c19a9c22734c539afd0f3a06645b5076b86419dc467bb4f85f49b.jpg

**Conceptual Understanding:**
This image conceptually represents two fundamental non-linear activation functions used in artificial neural networks: the hyperbolic tangent (tanh) and the Rectified Linear Unit (ReLU). The main purpose of the image is to visually illustrate the mathematical behavior of these two functions, showing how they transform a single input variable 'z' into an output variable 'y'. The key ideas being communicated are the specific functional forms, the output range, the handling of positive versus negative inputs, and the non-linear characteristics that make them essential for enabling neural networks to model complex, non-linear relationships in data. Specifically, it highlights tanh's 'squashing' property to a `[-1, 1]` range and ReLU's 'thresholding' and 'identity' behavior for positive inputs.

**Content Interpretation:**
The image displays two fundamental activation functions, the hyperbolic tangent (tanh) and the Rectified Linear Unit (ReLU), which are critical components in artificial neural networks. These functions introduce non-linearity into the network, enabling it to learn complex patterns and relationships in data. 

Graph (a) visualizes the `y = tanh(z)` function. It shows a smooth, S-shaped curve that maps any real-valued input `z` to an output `y` in the range of `[-1, 1]`. This 'squashing' property is evident from the y-axis scale of `-1.0` to `1.0` and the curve flattening out towards these limits for large positive and negative `z` values (illustrated across the x-axis range of `-10` to `10`). The function is zero-centered, as it passes through `(0,0)`, meaning its output range is symmetric around zero.

Graph (b) illustrates the `y = max(z,0)` function, which is the ReLU. This function outputs `0` for any negative input `z` and returns the input `z` itself for any positive `z`. This behavior is clearly depicted by the horizontal line at `y = 0` for `z <= 0` and the linear segment with a slope of `1` for `z > 0`. Both the x-axis and y-axis range from `-10` to `10`, providing a comprehensive view of this piecewise linear transformation. The 'max(z,0)' definition highlights its characteristic of 'activating' only for positive inputs, setting negative inputs to zero.

**Key Insights:**
The main takeaways from this image are: 

1.  **Tanh Function Characteristics:** The `y = tanh(z)` function, as shown in graph (a), is a non-linear, S-shaped activation function that squashes its inputs to an output range of `[-1.0, 1.0]`. This is supported by the y-axis labels from `-1.0` to `1.0` and the S-curve visibly approaching these limits for large positive and negative `z` values (x-axis labels from `-10` to `10`). Its zero-centered output is a notable property. 

2.  **ReLU Function Characteristics:** The `y = max(z,0)` function, depicted in graph (b), is a piecewise linear activation function. It introduces sparsity by outputting `0` for all negative inputs `z` and passes the input `z` directly for all positive inputs. This is evident from the flat line at `y = 0` for `z` values from `-10` to `0` and the straight line with positive slope for `z` values from `0` to `10`. 

3.  **Introduction of Non-linearity:** Both functions demonstrate different ways of introducing non-linearity into a neural network, which is crucial for the network's ability to learn complex patterns that cannot be modeled by simple linear transformations. The non-linear curves of both graphs visually confirm this. 

4.  **Distinct Behaviors:** The image clearly highlights the distinct operational behaviors of tanh (smooth, saturating, zero-centered) and ReLU (piecewise linear, non-saturating for positive inputs, outputs zero for negative inputs). This contrast informs the understanding of their respective advantages and disadvantages in different neural network architectures and training scenarios. For instance, tanh's saturation can lead to vanishing gradients, while ReLU's 'zeroing out' of negative inputs can lead to 'dead neurons' but also computational efficiency and faster convergence.

**Document Context:**
This image is highly relevant to a section discussing 'activation' functions, as it visually defines and contrasts two of the most common activation functions in neural networks: tanh and ReLU. Given the preceding document context 'Section: activation' and the text after the image 'Figure 7.3 The tanh and ReLU activation functions.', the image serves as a direct graphical explanation of these two mathematical concepts. It provides essential visual aids for understanding how these functions process input signals in a neural network, thereby enhancing the reader's comprehension of non-linear transformations that are crucial for learning complex representations.

**Summary:**
The image displays two distinct activation functions, tanh and ReLU, commonly utilized in artificial neural networks. Figure (a) illustrates the hyperbolic tangent (tanh) function, defined by `y = tanh(z)`. Its plot demonstrates an S-shaped curve that saturates, meaning its output `y` approaches a maximum of `1.0` as `z` becomes very positive, and a minimum of `-1.0` as `z` becomes very negative. The function passes through `y = 0.0` at `z = 0`. The x-axis for this graph ranges from `-10` to `10`, labeled `z`, and the y-axis ranges from `-1.0` to `1.0`, labeled `y = tanh(z)`. Figure (b) presents the Rectified Linear Unit (ReLU) function, defined by `y = max(z,0)`. This function exhibits a piecewise linear behavior: for all `z` values less than or equal to `0`, the output `y` is `0`; and for all `z` values greater than `0`, the output `y` is equal to `z`. Visually, this is represented by a flat line at `y = 0` on the negative side of the x-axis, transitioning into a straight line with a positive slope (slope of 1) for positive `z` values, starting from `(0,0)`. The x-axis for this graph ranges from `-10` to `10`, labeled `z`, and the y-axis ranges from `-10` to `10`, labeled `y = max(z,0)`. Both graphs provide a visual understanding of how these functions transform an input `z` into an output `y`, highlighting their non-linear properties critical for neural network operations.](images/1f5376a4aa7c19a9c22734c539afd0f3a06645b5076b86419dc467bb4f85f49b.jpg)
Figure 7.3 The tanh and ReLU activation functions.

# saturated

# vanishing gradient

result from it being very close to linear. In the sigmoid or tanh functions, very high values of $z$ result in values of $y$ that are saturated, i.e., extremely close to 1, and have derivatives very close to 0. Zero derivatives cause problems for learning, because as we’ll see in Section 7.5, we’ll train networks by propagating an error signal backwards, multiplying gradients (partial derivatives) from each layer of the network; gradients that are almost 0 cause the error signal to get smaller and smaller until it is too small to be used for training, a problem called the vanishing gradient problem. Rectifiers don’t have this problem, since the derivative of ReLU for high values of z is 1 rather than very close to 0.

# 7.2 The XOR problem

Early in the history of neural networks it was realized that the power of neural networks, as with the real neurons that inspired them, comes from combining these units into larger networks.

One of the most clever demonstrations of the need for multi-layer networks was the proof by Minsky and Papert (1969) that a single neural unit cannot compute some very simple functions of its input. Consider the task of computing elementary logical functions of two inputs, like AND, OR, and XOR. As a reminder, here are the truth tables for those functions:

# perceptron

<table><tr><td colspan="2">AND OR</td><td colspan="2"></td><td colspan="2">XOR</td></tr><tr><td>x1</td><td>x2 y</td><td>x1 x2</td><td>y</td><td>x1 x2</td><td>y</td></tr><tr><td>0</td><td>0</td><td>0 0</td><td>0 0</td><td>0 0</td><td>0</td></tr><tr><td>@</td><td>1</td><td>0 @ 1</td><td>1</td><td>0 1</td><td>1</td></tr><tr><td>1</td><td>0 0</td><td>1 0</td><td>1</td><td>1 0</td><td>1</td></tr><tr><td>1</td><td>1 1</td><td>1 1</td><td>1</td><td>1 1</td><td>0</td></tr></table>

This example was first shown for the perceptron, which is a very simple neural unit that has a binary output and has a very simple step function as its non-linear activation function. The output $y$ of a perceptron is 0 or 1, and is computed as follows (using the same weight $\pmb { w }$ , input $\pmb { \times }$ , and bias $^ b$ as in Eq. 7.2):

$$
y = { \left\{ \begin{array} { l l } { 0 , { \mathrm { ~ i f ~ } } \mathbf { w } \cdot \mathbf { x } + b \leq 0 } \\ { 1 , { \mathrm { ~ i f ~ } } \mathbf { w } \cdot \mathbf { x } + b > 0 } \end{array} \right. }
$$

It’s very easy to build a perceptron that can compute the logical AND and OR functions of its binary inputs; Fig. 7.4 shows the necessary weights.

![## Image Analysis: 0836f5e32139f05cb26afdcbb6f37a5a86fa5fce3923f6e17de6883dd3891163.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental architecture and parameterization of a single-layer perceptron. Its main purpose is to demonstrate, through concrete examples, how specific sets of weights and a bias can enable a perceptron to act as a logic gate, specifically for the logical AND and logical OR functions. It visually conveys the idea that the weighted sum of inputs, plus a bias, determines the perceptron's output, thereby performing a classification or decision-making task based on the input values.

**Content Interpretation:**
The image illustrates two simple perceptron models, specifically configured to implement basic logical functions: logical AND and logical OR. Each perceptron consists of input nodes (x1, x2, and a bias input +1) connected to a single output node (the red circle) via weighted connections. The weights associated with each input and the bias are explicitly provided in each diagram. Diagram (a) shows the weights required for an AND gate, while diagram (b) shows the weights for an OR gate. This directly demonstrates how specific weight and bias configurations enable a perceptron to mimic fundamental boolean logic.

**Key Insights:**
The main takeaway is that perceptrons can be configured to compute basic logical functions like AND and OR by adjusting their input weights and bias. The specific numerical weights and bias values are critical in defining the logical behavior of the perceptron. For a logical AND (diagram a), weights w1=1, w2=1, and a bias weight b=-1 are used, meaning the output is 1 only if both x1 and x2 are 1 (and the bias adjustment pulls down the sum). For a logical OR (diagram b), weights w1=1, w2=1, and a bias weight b=0 are used, meaning the output is 1 if either x1 or x2 is 1 (or both). The bias input of '+1' with its own weight effectively shifts the activation threshold of the perceptron.

**Document Context:**
This image directly supports the document's discussion on perceptrons, particularly in the section titled 'perceptron'. It visually explains how the concepts of weights (w) and bias (b) are applied in practice to create simple computational units that can perform logical operations. The explicit values for weights (w1=1, w2=1) and bias weights (b=-1 for AND, b=0 for OR) are crucial examples for understanding the mathematical underpinnings of perceptrons described in the surrounding text, especially in relation to computing logical functions. It serves as a concrete visual aid for the textual explanation of how perceptrons for AND and OR functions are configured.

**Summary:**
The image displays two distinct perceptron diagrams, labeled (a) and (b), which illustrate how logical functions are computed. Each diagram shows two primary inputs, 'x1' and 'x2', along with a bias input represented by '+1'. These inputs feed into a central, red circular node, which represents the perceptron's output or activation. Each connection from an input to the central node has an associated weight. 

In diagram (a), representing the logical AND function, 'x1' connects to the central node with a weight of '1'. Similarly, 'x2' connects to the central node with a weight of '1'. The bias input '+1' connects to the central node with a weight of '-1'. These specific weights (w1=1, w2=1, b=-1) are configured to produce an AND operation. 

In diagram (b), representing the logical OR function, 'x1' connects to the central node with a weight of '1'. 'x2' also connects to the central node with a weight of '1'. The bias input '+1' connects to the central node with a weight of '0'. These weights (w1=1, w2=1, b=0) are configured to produce an OR operation. 

Both diagrams clearly show the inputs 'x1' and 'x2', the bias input '+1', the specific weights on the arrows, and the common red circular node as the perceptron output.](images/0836f5e32139f05cb26afdcbb6f37a5a86fa5fce3923f6e17de6883dd3891163.jpg)
Figure 7.4 The weights $w$ and bias $^ b$ for perceptrons for computing logical functions. The inputs are shown as $x _ { 1 }$ and $x _ { 2 }$ and the bias as a special node with value $+ 1$ which is multiplied with the bias weight $^ b$ . (a) logical AND, with weights $w _ { 1 } = 1$ and $w _ { 2 } = 1$ and bias weight $b = - 1$ . (b) logical OR, with weights $w _ { 1 } = 1$ and $w _ { 2 } = 1$ and bias weight $b = 0$ . These weights/biases are just one from an infinite number of possible sets of weights and biases that would implement the functions.

It turns out, however, that it’s not possible to build a perceptron to compute logical XOR! (It’s worth spending a moment to give it a try!)

The intuition behind this important result relies on understanding that a perceptron is a linear classifier. For a two-dimensional input $x _ { 1 }$ and $x _ { 2 }$ , the perceptron equation, $w _ { 1 } x _ { 1 } + w _ { 2 } x _ { 2 } + b = 0$ is the equation of a line. (We can see this by putting it in the standard linear format: $x _ { 2 } = ( - w _ { 1 } / w _ { 2 } ) x _ { 1 } + ( - b / w _ { 2 } ) .$ .) This line acts as a decision boundary in two-dimensional space in which the output 0 is assigned to all inputs lying on one side of the line, and the output 1 to all input points lying on the other side of the line. If we had more than 2 inputs, the decision boundary becomes a hyperplane instead of a line, but the idea is the same, separating the space into two categories.

Fig. 7.5 shows the possible logical inputs (00, 01, 10, and 11) and the line drawn by one possible set of parameters for an AND and an OR classifier. Notice that there is simply no way to draw a line that separates the positive cases of XOR (01 and 10) from the negative cases (00 and 11). We say that XOR is not a linearly separable function. Of course we could draw a boundary with a curve, or some other function, but not a single line.

# 7.2.1 The solution: neural networks

While the XOR function cannot be calculated by a single perceptron, it can be calculated by a layered network of perceptron units. Rather than see this with networks of simple perceptrons, however, let’s see how to compute XOR using two layers of ReLU-based units following Goodfellow et al. (2016). Fig. 7.6 shows a figure with the input being processed by two layers of neural units. The middle layer (called $h _ { - }$ ) has two units, and the output layer (called $y$ ) has one unit. A set of weights and biases are shown that allows the network to correctly compute the XOR function.

Let’s walk through what happens with the input $\mathbf { x } = [ 0 , 0 ]$ . If we multiply each input value by the appropriate weight, sum, and then add the bias $^ b$ , we get the vector [0, -1], and we then apply the rectified linear transformation to give the output of the h layer as [0, 0]. Now we once again multiply by the weights, sum, and add the bias (0 in this case) resulting in the value 0. The reader should work through the computation of the remaining 3 possible input pairs to see that the resulting $y$ values are 1 for the inputs [0, 1] and [1, 0] and 0 for [0, 0] and [1, 1].

![## Image Analysis: 565dbeef338e7e3b10304fbc547874809319f3f9bc24003e0ff9cd4e754c461e.jpg

**Conceptual Understanding:**
This image conceptually represents the concept of linear separability in the context of Boolean logic functions (AND, OR, XOR) using two binary inputs, x₁ and x₂. The main purpose is to visually demonstrate which of these fundamental logical operations can be separated into distinct output categories (0 and 1) by a single straight line, and which cannot. This directly relates to the capabilities and limitations of simple perceptrons in neural networks, as a single perceptron can only learn linearly separable functions. The image communicates the idea that while AND and OR are linearly separable, XOR is not, posing a significant challenge for basic single-layer neural networks.

**Content Interpretation:**
The image demonstrates the linear separability of different two-input Boolean functions (AND, OR, XOR), which is a core concept in understanding the capabilities of early neural network models like perceptrons.

Function a) "x₁ AND x₂": This diagram shows that for the AND function, only when both inputs x₁ and x₂ are 1 (the point (1,1)) is the output 1 (represented by a filled blue circle). All other combinations ((0,0), (1,0), (0,1)) result in an output of 0 (white circles). The presence of the "dashed red line" clearly indicates that these two categories of outputs (0 and 1) are linearly separable; a single straight line can be drawn to perfectly divide them.

Function b) "x₁ OR x₂": This diagram illustrates the OR function, where if either x₁ or x₂ (or both) are 1, the output is 1 (filled blue circles at (1,0), (0,1), and (1,1)). Only when both inputs are 0 (the point (0,0)) is the output 0 (white circle). Similar to AND, the "dashed red line" demonstrates that the OR function is also linearly separable, as a single straight line can separate the output 0 from the outputs of 1.

Function c) "x₁ XOR x₂": This diagram represents the Exclusive OR (XOR) function. Here, the output is 1 (filled blue circles) when exactly one of the inputs (x₁ or x₂) is 1 (points (1,0) and (0,1)). The output is 0 (white circles) when both inputs are the same (points (0,0) and (1,1)). Crucially, the absence of a dashed red line and the presence of the "red question mark (?)" explicitly highlight that there is no single straight line that can separate the filled blue circles from the white circles. This visually confirms that the XOR function is *not* linearly separable.

The specific textual elements like "x₁ AND x₂", "x₁ OR x₂", "x₁ XOR x₂" identify the functions being analyzed. The axis labels "x₁", "x₂", "0", and "1" define the binary input space. The distinction between "Filled circles" (output 1) and "white circles" (output 0) (as per the document context) visually encodes the function's output. The "dashed red line" in (a) and (b) provides visual evidence of linear separability, while its absence and the "red question mark (?)" in (c) provide evidence of non-linear separability.

**Key Insights:**
The main takeaway from this image is a fundamental limitation of simple linear classifiers, such as single-layer perceptrons, in machine learning and neural networks.

Key Insight 1: Linear Separability of AND and OR. The image, specifically sub-diagrams "a) x₁ AND x₂" and "b) x₁ OR x₂", demonstrates that the AND and OR Boolean functions are linearly separable. This is visually supported by the presence of a "dashed red line" in both diagrams, which effectively divides the input space into regions corresponding to outputs of 0 (white circles) and 1 (filled blue circles). This implies that a simple perceptron can be trained to correctly classify these functions.

Key Insight 2: Non-Linear Separability of XOR. The most critical insight comes from sub-diagram "c) x₁ XOR x₂". The arrangement of "white circles" at (0,0) and (1,1) and "filled blue circles" at (1,0) and (0,1) clearly shows that these two categories of outputs (0 and 1) cannot be separated by a single straight line. The "red question mark (?)" explicitly emphasizes this impossibility. This indicates that a single-layer perceptron is inherently incapable of solving the XOR problem, a historically significant challenge in the development of neural networks.

The precise labeling of the functions ("x₁ AND x₂", "x₁ OR x₂", "x₁ XOR x₂") and the distinct visual encoding of outputs (white vs. filled blue circles) along with the presence or absence of the "dashed red line" and the "red question mark (?)" are all critical textual and symbolic elements that provide direct evidence for these insights.

**Document Context:**
This image fits directly into the "7.2.1 The solution: neural networks" section of the document by visually explaining a core concept related to the capabilities and limitations of fundamental neural network components, specifically perceptrons. It sets the stage for understanding why more complex neural network architectures (like multi-layer perceptrons) were needed to solve problems that simple perceptrons could not, such as the XOR problem. The image provides the visual foundation for understanding the "perceptron outputs of 1" and "perceptron outputs of 0" mentioned in the text following the image, and directly illustrates why "There is no way to draw a line that correctly separates the two categories for XOR."

**Summary:**
This image, composed of three separate graphs labeled a), b), and c), visually explains how simple logical functions (AND, OR, and XOR) can be understood in terms of whether their outputs can be separated by a straight line, a concept vital to neural networks. Each graph uses a two-dimensional plane where the horizontal axis is labeled "x₁" and the vertical axis is labeled "x₂". Both axes are marked with "0" at the origin and "1" further along, representing binary inputs. The output of each logical function for the four possible input combinations ((0,0), (1,0), (0,1), and (1,1)) is shown using circles: a "white circle" signifies an output of 0, and a "filled blue circle" signifies an output of 1.

Graph a) labeled "x₁ AND x₂": This graph shows the AND function. An output of 1 (filled blue circle) occurs only when both x₁ is 1 and x₂ is 1 (at coordinate (1,1)). All other input combinations result in an output of 0 (white circles). A "dashed red line" is drawn, perfectly separating the single filled blue circle from the three white circles. This demonstrates that the AND function is "linearly separable," meaning a simple straight line can distinguish its different outputs.

Graph b) labeled "x₁ OR x₂": This graph illustrates the OR function. An output of 1 (filled blue circles) occurs if either x₁ is 1, or x₂ is 1, or both are 1. Only when both x₁ and x₂ are 0 (at coordinate (0,0)) is the output 0 (white circle). Similar to the AND function, a "dashed red line" is present, successfully separating the single white circle from the three filled blue circles. This shows that the OR function is also "linearly separable."

Graph c) labeled "x₁ XOR x₂": This graph depicts the Exclusive OR (XOR) function. Here, the output is 1 (filled blue circles) when exactly one of the inputs is 1 (at coordinates (1,0) and (0,1)). The output is 0 (white circles) when both inputs are the same (at coordinates (0,0) and (1,1)). Unlike the AND and OR graphs, there is no "dashed red line" present. Instead, a "red question mark (?)" is prominently displayed, indicating that it is impossible to draw a single straight line that can separate the filled blue circles from the white circles. This signifies that the XOR function is *not* "linearly separable."

In summary, the image visually conveys that while simple logical functions like AND and OR can be solved by a basic single-layer neural network (a perceptron) because their outputs are linearly separable, the XOR function cannot be solved by such a network because its outputs are not linearly separable. This explains why more complex neural network architectures are required for certain tasks.](images/565dbeef338e7e3b10304fbc547874809319f3f9bc24003e0ff9cd4e754c461e.jpg)
Figure 7.5 The functions AND, OR, and XOR, represented with input $x _ { 1 }$ on the $\mathbf { X }$ -axis and input $x _ { 2 }$ on the y-axis. Filled circles represent perceptron outputs of 1, and white circles perceptron outputs of 0. There is no way to draw a line that correctly separates the two categories for XOR. Figure styled after Russell and Norvig (2002).

![## Image Analysis: 00ff259c2bb36ed1ccd5987231b546dff542754c22145e185e9f2138353d14d8.jpg

**Conceptual Understanding:**
This image represents a specific architecture of a small feed-forward neural network designed to solve the Exclusive OR (XOR) problem. Conceptually, it illustrates how multiple layers of interconnected computational units (neurons) with associated weights and biases can learn to model non-linear relationships, which a single-layer perceptron cannot. The main purpose is to visually demonstrate the structure and parameters (weights and biases) of a neural network configured to perform the XOR logical operation. It communicates the fundamental components of a neural network: input nodes, hidden nodes, an output node, and the directed, weighted connections between them, including the often-implicit bias terms.

**Content Interpretation:**
This image primarily shows a neural network system, specifically a multi-layer perceptron (MLP) with two input units (x₁, x₂), two hidden ReLU units (h₁, h₂), and one output ReLU unit (y₁), as implied by the document context.

*   **Input Layer (x₁, x₂):** These nodes represent the two binary inputs to the XOR function. The labels "x₁" and "x₂" directly indicate their role as independent variables.
*   **Hidden Layer (h₁, h₂):** These nodes represent the intermediate computational units. The labels "h₁" and "h₂" denote them as units within a "hidden layer," suggesting they perform non-linear transformations on the inputs. The text after the image explicitly states these are "ReLU units."
*   **Output Layer (y₁):** This node represents the final output of the network, which for an XOR problem would typically be 0 or 1. The label "y₁" signifies it as the network's prediction or result.
*   **Weights (Numerical labels on black arrows):** The numbers "1", "1", "1", "-1", "1", "-2" on the solid black arrows represent the synaptic weights (w) connecting the units.
    *   From x₁ to h₁: weight "1"
    *   From x₁ to h₂: weight "1"
    *   From x₂ to h₁: weight "1"
    *   From x₂ to h₂: weight "-1"
    *   From h₁ to y₁: weight "1"
    *   From h₂ to y₁: weight "-2"
    These weights are crucial for determining the strength and direction of the signal transmission between neurons. Their specific values are carefully chosen to enable the network to learn the XOR function. For instance, the negative weight "-1" from x₂ to h₂ and "-2" from h₂ to y₁ suggests inhibitory connections, which are essential for solving the non-linear nature of XOR.
*   **Bias Units and Weights (Numerical labels on gray arrows and gray "+1" nodes):** The grayed-out "+1" nodes and their associated gray arrows with labels "0", "-1", and "0" represent the bias terms (b).
    *   A "+1" bias unit connects to h₁ with weight "0".
    *   A "+1" bias unit connects to h₂ with weight "-1".
    *   A "+1" bias unit connects to y₁ with weight "0".
    As explained in the document context, these are "bias weights/units in gray" and are "clamped to +1". Biases introduce an affine transformation, effectively shifting the activation function's output. Even zero weights for biases (e.g., to h₁ and y₁) are explicitly shown, indicating that in this specific configuration, those units do not receive an additional constant shift from a bias term, or their bias is effectively zero for that connection. The bias with weight "-1" to h₂ is particularly significant as it directly influences the activation of that hidden unit.

All extracted text elements, including "x₁", "x₂", "h₁", "h₂", "y₁", and the specific numerical weights and bias values, are direct evidence for interpreting this diagram as a functional neural network designed for XOR. The distinct values of these weights and the presence of hidden layers are what allow the network to model the non-linear XOR function.

**Key Insights:**
The main takeaways and insights from this image, supported by the verbatim textual evidence, are:

1.  **Multi-layer Networks for Non-linear Problems:** The image demonstrates that a neural network with at least one hidden layer (h₁, h₂) can solve problems that are not linearly separable, such as the XOR problem. This is a fundamental concept in neural network theory, where hidden layers enable the network to learn complex, non-linear representations of the input data. The presence of "h₁" and "h₂" nodes directly shows this multi-layer structure.

2.  **Importance of Specific Weights and Biases:** The precise values of the weights (e.g., "1", "-1", "-2") and biases (e.g., "-1" to h₂, "0" to others) are critical for the network's functionality. These values are not arbitrary; they are specifically tuned (or learned) to produce the correct output for the XOR function. For instance, the combination of positive and negative weights, such as "1" from x₁ to h₂, "1" from x₂ to h₁, "-1" from x₂ to h₂, and "-2" from h₂ to y₁, illustrates how different inputs are combined and transformed through the network to achieve the desired output, which is the essence of a neural network's computation. The grayed "0" and "-1" bias weights also explicitly show their defined values within the architecture.

3.  **Representation of Bias Terms:** The inclusion of grayed-out "+1" units with associated gray weighted arrows ("0", "-1") clearly illustrates how bias terms are integrated into the network architecture. The text after the image clarifies that bias "b" is represented as a "weight on a unit clamped to +1." This detail is crucial for understanding the complete mathematical model of a neuron, where the bias acts as a constant offset that shifts the activation function, allowing the network to learn more flexible decision boundaries. Even if the weight is "0", its explicit presence signifies a design choice.

The textual evidence ("x₁", "x₂", "h₁", "h₂", "y₁", "1", "-1", "-2", "0", "+1") collectively provides a complete specification of this neural network's topology and parameters, directly supporting the insights regarding its ability to solve XOR, the importance of parameter tuning, and the structural inclusion of biases.

**Document Context:**
This image directly supports Section 7.2.1, titled "The solution: neural networks," by providing a concrete visual example of a neural network designed to solve the XOR problem. The accompanying text explicitly identifies it as "Figure 7.6 XOR solution after Goodfellow et al. (2016)," further solidifying its role as a key illustration of neural network capabilities. It demonstrates how the principles discussed in the section, such as the use of hidden layers (h₁ and h₂) and the role of weights (w) and biases (b), are applied in a practical network architecture. It visually clarifies the concept of a multi-layer network achieving what a simpler model cannot, which is central to understanding the power of neural networks.

**Summary:**
This diagram illustrates a small, feed-forward neural network specifically configured to solve the Exclusive OR (XOR) problem. It features an input layer, a hidden layer, and an output layer, along with bias units.

Let's break down the flow of information:

1.  **Inputs (x₁ and x₂):** The process begins with two input nodes, labeled "x₁" and "x₂". These represent the two binary inputs (e.g., 0 or 1) that are fed into the network.

2.  **First Layer Connections to Hidden Units (h₁ and h₂):**
    *   From the input node "x₁", signals are sent to two hidden layer units:
        *   To "h₁" (a red circular unit) with a connection strength, or weight, of "1".
        *   To "h₂" (another red circular unit) with a weight of "1".
    *   From the input node "x₂", signals are also sent to the same hidden layer units:
        *   To "h₁" with a weight of "1".
        *   To "h₂" with a weight of "-1".

3.  **Bias Connections to Hidden Units:** There's a grayed-out input node labeled "+1" (located at the bottom left) representing a bias unit. This bias unit provides an additional constant input to the hidden layer units:
    *   It connects to "h₁" with a weight of "0" (grayed out). This means h₁ effectively receives no bias from this particular unit.
    *   It connects to "h₂" with a weight of "-1" (grayed out). This applies a negative constant offset to the input of h₂.

4.  **Second Layer Connections to Output Unit (y₁):** The processed signals from the hidden layer units "h₁" and "h₂" are then fed into the final output unit:
    *   From "h₁", a signal is sent to "y₁" (a red circular unit, the output node) with a weight of "1".
    *   From "h₂", a signal is sent to "y₁" with a weight of "-2".

5.  **Bias Connection to Output Unit:** Another grayed-out input node labeled "+1" (located at the bottom right) represents a bias unit for the output layer.
    *   It connects to "y₁" with a weight of "0" (grayed out). This means y₁ effectively receives no bias from this unit.

In essence, the network takes the two inputs (x₁, x₂), combines them with specific weights, adds bias terms (where applicable), processes them through the hidden layer units (h₁ and h₂), and then combines these hidden unit outputs with further weights and biases to produce the final output "y₁". The specific weights, including positive and negative values, along with the bias terms, are critical for the network to correctly compute the non-linear XOR function, where the output is true only if inputs are different. The grayed elements specifically denote the bias units and their associated weights, as explained in the document's context.](images/00ff259c2bb36ed1ccd5987231b546dff542754c22145e185e9f2138353d14d8.jpg)
Figure 7.6 XOR solution after Goodfellow et al. (2016). There are three ReLU units, in two layers; we’ve called them $h _ { 1 }$ , $h _ { 2 }$ ( $h$ for “hidden layer”) and $y _ { 1 }$ . As before, the numbers on the arrows represent the weights $w$ for each unit, and we represent the bias $^ b$ as a weight on a unit clamped to $+ 1$ , with the bias weights/units in gray.

It’s also instructive to look at the intermediate results, the outputs of the two hidden nodes $h _ { 1 }$ and $h _ { 2 }$ . We showed in the previous paragraph that the $\mathbf { h }$ vector for the inputs $\mathbf { x } = [ 0 , 0 ]$ was [0, 0]. Fig. 7.7b shows the values of the $\mathbf { h }$ layer for all 4 inputs. Notice that hidden representations of the two input points $\mathbf { x } = [ 0 , 1 ]$ and ${ \bf x } = [ 1 , 0 ]$ (the two cases with XOR output $= 1$ ) are merged to the single point $\mathbf { h } =$ [1, 0]. The merger makes it easy to linearly separate the positive and negative cases of XOR. In other words, we can view the hidden layer of the network as forming a representation of the input.

In this example we just stipulated the weights in Fig. 7.6. But for real examples the weights for neural networks are learned automatically using the error backpropagation algorithm to be introduced in Section 7.5. That means the hidden layers will learn to form useful representations. This intuition, that neural networks can automatically learn useful representations of the input, is one of their key advantages, and one that we will return to again and again in later chapters.

![## Image Analysis: e2e50457217904808441014409118866e396cc7fab0daf579ea7c0600bab5cb4.jpg

**Conceptual Understanding:**
The image conceptually represents the function of a neural network's hidden layer in transforming input features. Its main purpose is to demonstrate how a non-linearly separable dataset, such as the one encountered in the XOR problem, can be made linearly separable by mapping it to a new feature space through a hidden layer. This process allows a simple linear classifier to then effectively separate the different classes.

**Content Interpretation:**
The image illustrates the concept of feature transformation by a neural network's hidden layer. It shows how a non-linearly separable problem, specifically one mimicking an XOR gate's output, can be made linearly separable in a transformed feature space. Sub-figure (a) depicts the initial input space, labeled "The original x space," with two input features x1 and x2. The four points (0,0), (1,0), (0,1), and (1,1) represent the possible inputs. The blue filled circles at (1,0) and (0,1) are one class (e.g., output 1 for XOR), while the white hollow circles at (0,0) and (1,1) are another class (e.g., output 0 for XOR). The arrangement of these points clearly shows they cannot be separated by a single straight line. Sub-figure (b), labeled "The new (linearly separable) h space," shows the data after transformation by a hidden layer, producing new features h1 and h2. In this transformed space, the input points [0,1] and [1,0] from the original x space have been 'collapsed' or mapped to the same point [1,0] in the h space. This re-arrangement, along with the other points, now allows a single linear boundary (represented by the dashed red line) to perfectly separate the classes. This demonstrates how a hidden layer provides a new representation that simplifies the classification task.

**Key Insights:**
The main takeaway from this image is that neural networks, through their hidden layers, can transform input data into a higher-dimensional or different feature space where previously non-linearly separable classes become linearly separable. This transformation is key to solving complex classification problems that simpler models cannot handle. The image specifically demonstrates this using an XOR-like problem, where the input points (0,1) and (1,0) are visually distinct but belong to the same output class (blue circles), and (0,0) and (1,1) are the other output class (white circles). In 'The original x space' (a), these are not linearly separable. However, in 'The new (linearly separable) h space' (b), the hidden layer effectively maps the original (0,1) and (1,0) points to a single point (1,0) in the h-space, allowing a simple linear boundary (the red dashed line) to perfectly separate the classes. This illustrates the power of feature learning and representation transformation in neural networks.

**Document Context:**
This image is highly relevant to the section "7.2.1 The solution: neural networks" as it visually demonstrates a core function of neural networks: their ability to solve non-linearly separable problems using hidden layers. The accompanying text states, "The hidden layer forming a new representation of the input. (b) shows the representation of the hidden layer, h, compared to the original input representation x in (a). Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it possible to linearly separate the positive and negative cases of XOR. After Goodfellow et al. (2016)." The image directly supports and explains this by showing the 'before' and 'after' transformation, making the abstract concept of a 'new representation' concrete and illustrating how linear separability is achieved, which is crucial for understanding how neural networks overcome limitations of simpler models like perceptrons.

**Summary:**
This image consists of two 2D Cartesian coordinate systems, labeled (a) and (b), illustrating how a neural network's hidden layer transforms input data to achieve linear separability, specifically for an XOR-like problem. In sub-figure (a), titled "The original x space," the x-axis is labeled "x1" and the y-axis is labeled "x2." Both axes range from 0 to 1. There are four data points: a white hollow circle at coordinates (0,0), a blue filled circle at (1,0), a blue filled circle at (0,1), and a white hollow circle at (1,1). The blue filled circles represent one class, and the white hollow circles represent another. Visually, these points are not linearly separable; no single straight line can perfectly separate the blue circles from the white circles. Sub-figure (b), titled "The new (linearly separable) h space," shows a transformed representation. The x-axis is labeled "h1" and the y-axis is labeled "h2." The h1 axis ranges from 0 to 2, and the h2 axis ranges from 0 to 1. In this new space, the original data points are mapped: a white hollow circle is at (0,0), a blue filled circle is at (1,0), and another blue filled circle is also at (1,0). The white hollow circle from (1,1) in the original space is now at approximately (2,1) in the h-space. A red dashed line is drawn, originating from the h2-axis below 0, passing through approximately (0.5,0.5), and continuing upwards and to the right. This line effectively separates the blue filled circle at (1,0) from the white hollow circle at (0,0) and the white hollow circle at approximately (2,1). This demonstrates that in the 'h space,' the data points are now linearly separable by the dashed red line.](images/e2e50457217904808441014409118866e396cc7fab0daf579ea7c0600bab5cb4.jpg)
Figure 7.7 The hidden layer forming a new representation of the input. (b) shows the representation of the hidden layer, h, compared to the original input representation $\pmb { x }$ in (a). Notice that the input point [0, 1] has been collapsed with the input point [1, 0], making it possible to linearly separate the positive and negative cases of XOR. After Goodfellow et al. (2016).

# 7.3 Feedforward Neural Networks

# feedforward network

Let’s now walk through a slightly more formal presentation of the simplest kind of neural network, the feedforward network. A feedforward network is a multilayer network in which the units are connected with no cycles; the outputs from units in each layer are passed to units in the next higher layer, and no outputs are passed back to lower layers. (In Chapter 8 we’ll introduce networks with cycles, called recurrent neural networks.)

multi-layer perceptrons MLP

For historical reasons multilayer networks, especially feedforward networks, are sometimes called multi-layer perceptrons (or MLPs); this is a technical misnomer, since the units in modern multilayer networks aren’t perceptrons (perceptrons have a simple step-function as their activation function, but modern networks are made up of units with many kinds of non-linearities like ReLUs and sigmoids), but at some point the name stuck.

hidden layer fully-connected

Simple feedforward networks have three kinds of nodes: input units, hidden units, and output units.

Fig. 7.8 shows a picture. The input layer $\pmb { \times }$ is a vector of simple scalar values just as we saw in Fig. 7.2.

The core of the neural network is the hidden layer h formed of hidden units $\mathbf { h } _ { i }$ , each of which is a neural unit as described in Section 7.1, taking a weighted sum of its inputs and then applying a non-linearity. In the standard architecture, each layer is fully-connected, meaning that each unit in each layer takes as input the outputs from all the units in the previous layer, and there is a link between every pair of units from two adjacent layers. Thus each hidden unit sums over all the input units.

Recall that a single hidden unit has as parameters a weight vector and a bias. We represent the parameters for the entire hidden layer by combining the weight vector and bias for each unit $i$ into a single weight matrix $\boldsymbol { \mathsf { W } }$ and a single bias vector $^ b$ for the whole layer (see Fig. 7.8). Each element $\boldsymbol { \mathsf { W } } _ { j i }$ of the weight matrix $\boldsymbol { \mathsf { W } }$ represents the weight of the connection from the ith input unit $x _ { i }$ to the $j$ th hidden unit $h _ { j }$ .

The advantage of using a single matrix $\boldsymbol { \mathsf { W } }$ for the weights of the entire layer is that now the hidden layer computation for a feedforward network can be done very efficiently with simple matrix operations. In fact, the computation only has three steps: multiplying the weight matrix by the input vector $\pmb { \times }$ , adding the bias vector $\mathbf { b }$ , and applying the activation function $g$ (such as the sigmoid, tanh, or ReLU activation function defined above).

![## Image Analysis: c4b6718a6e63c9c6dc66ab847def634fa4caefa98ff893fbc613c6292dda2c50.jpg

**Conceptual Understanding:**
This image represents a schematic diagram of a feedforward neural network, specifically a 'simple 2-layer feedforward network' as indicated by the context. Conceptually, it illustrates how input data is processed through successive layers of interconnected nodes (neurons) to produce an output. The main purpose is to visually explain the architecture of such a network, showcasing its distinct input, hidden, and output layers, the weighted connections between them, and the unidirectional flow of information. It conveys the idea of a computational model that transforms input signals through a series of mathematical operations (summing weighted inputs, applying an activation function) to generate predictions or classifications.

**Content Interpretation:**
The image illustrates the fundamental architecture of a simple feedforward neural network. It visually represents the sequential processing of information from an input stage, through an intermediate processing stage (hidden layer), to a final output stage. The core concepts shown are: neurons organized into distinct layers, weighted connections between neurons in adjacent layers, and the inclusion of a bias term. The 'x' nodes represent the input features, the 'h' nodes represent the activated units in the hidden layer, and the 'y' nodes represent the final outputs of the network. The matrices 'W' and 'U' symbolize the weights applied to the connections between layers, which are critical for learning and transforming data. The bias term '+1' connected via 'b' indicates a constant offset added to the weighted sum of inputs at the hidden layer neurons, influencing their activation.

**Key Insights:**
The main takeaway from this image is the clear, layered structure of a basic feedforward neural network. Key insights include: 1. **Three Distinct Layers:** The network is composed of an 'input layer' (nodes x₁ to xn₀), a 'hidden layer' (nodes h₁ to hn₁), and an 'output layer' (nodes y₁ to yn₂), explicitly labeled at the bottom. 2. **Unidirectional Data Flow:** Arrows clearly indicate that information travels only in one direction, from the input layer, through the hidden layer, to the output layer, a defining characteristic of feedforward networks. 3. **Weighted Connections:** The connections between the input and hidden layers are associated with weights 'W', and connections between the hidden and output layers are associated with weights 'U', highlighting the role of learned parameters in transforming inputs. 4. **Bias Term:** A bias input '+1' connected with 'b' is shown, indicating that a bias term is incorporated into the hidden layer calculations, which is crucial for shifting the activation function. 5. **Fully Connected Layers:** Each node in a preceding layer is connected to every node in the subsequent layer, demonstrating a 'fully connected' or 'dense' layer structure. These specific text elements (layer labels, node labels x, h, y, weight labels W, U, and bias labels +1, b) provide direct evidence for these structural and functional insights into feedforward networks.

**Document Context:**
This image directly supports the document's section on 'feedforward network' by providing a visual example of the network type being discussed. The accompanying text states, 'Figure 7.8 A simple 2-layer feedforward network, with one hidden layer, one output layer, and one input layer (the input layer is usually not counted when enumerating layers).' The diagram perfectly matches this description, showing the three layers and labeling them explicitly as 'input layer', 'hidden layer', and 'output layer'. It clarifies the structural components (nodes, layers) and the directional flow of information (arrows), which are foundational for understanding how such networks process data. The visual representation enhances comprehension of the conceptual explanation provided in the text.

**Summary:**
The image displays a clear and comprehensive diagram of a simple 2-layer feedforward network. Information flows from left to right, starting at the 'input layer', passing through a 'hidden layer', and concluding at the 'output layer'. The input layer consists of multiple input nodes labeled 'x₁', 'x₂', and continues with ellipses '...' down to 'xn₀'. Additionally, there is a bias input, depicted as '+1' (slightly grayed out), which connects to the hidden layer. The connections from the input layer to the hidden layer are governed by weights collectively represented by the matrix 'W' at the top of this section. The hidden layer contains nodes labeled 'h₁', 'h₂', 'h₃', with ellipses '...' indicating more nodes, ending at 'hn₁'. A grayed-out 'b' label is shown near the connections from the '+1' bias to the hidden layer. From the hidden layer, information is passed to the output layer. These connections are governed by weights collectively represented by the matrix 'U' at the top of this section. The output layer consists of output nodes labeled 'y₁', 'y₂', and continues with ellipses '...' down to 'yn₂'. Each input node is shown connected to every hidden layer node, and similarly, each hidden layer node is connected to every output layer node, indicating a fully connected architecture between adjacent layers. The arrows on the lines consistently point from left to right, illustrating the unidirectional flow of data through the network.](images/c4b6718a6e63c9c6dc66ab847def634fa4caefa98ff893fbc613c6292dda2c50.jpg)
Figure 7.8 A simple 2-layer feedforward network, with one hidden layer, one output layer, and one input layer (the input layer is usually not counted when enumerating layers).

The output of the hidden layer, the vector $\mathbf { h }$ , is thus the following (for this example we’ll use the sigmoid function $\sigma$ as our activation function):

$$
\mathbf { h } = \sigma ( \mathbf { W } \mathbf { x } + \mathbf { b } )
$$

Notice that we’re applying the $\sigma$ function here to a vector, while in Eq. 7.3 it was applied to a scalar. We’re thus allowing $\sigma ( \cdot )$ , and indeed any activation function $g ( \cdot )$ , to apply to a vector element-wise, so $g [ z _ { 1 } , z _ { 2 } , z _ { 3 } ] = [ g ( z _ { 1 } ) , g ( z _ { 2 } ) , g ( z _ { 3 } ) ]$ .

Let’s introduce some constants to represent the dimensionalities of these vectors and matrices. We’ll refer to the input layer as layer 0 of the network, and have $n _ { 0 }$ represent the number of inputs, $\mathrm { ~ s o ~ } \times$ is a vector of real numbers of dimension $n _ { 0 }$ or more formally $\mathbf { x } \in \mathbb { R } ^ { n _ { 0 } }$ , a column vector of dimensionality $[ n _ { 0 } , 1 ]$ . Let’s call the hidden layer layer 1 and the output layer layer 2. The hidden layer has dimensionality $n _ { 1 }$ , so $\mathbf { h } \in \mathbb { R } ^ { n _ { 1 } }$ and also $\mathbf { b } \in \mathbb { R } ^ { n _ { 1 } }$ (since each hidden unit can take a different bias value). And the weight matrix $\boldsymbol { \mathsf { W } }$ has dimensionality $\boldsymbol { \mathsf { W } } \in \mathbb { R } ^ { n _ { 1 } \times n _ { 0 } }$ , i.e. $[ n _ { 1 } , n _ { 0 } ]$ .

Take a moment to convince yourself that the matrix multiplication in Eq. 7.8 will compute the value of each $\mathbf { h } _ { j }$ as $\begin{array} { r } { \sigma \left( \sum _ { i = 1 } ^ { n _ { 0 } } \pmb { \mathsf { W } } _ { j i } \pmb { \mathsf { x } } _ { i } + \pmb { \mathsf { b } } _ { j } \right) } \end{array}$ .

As we saw in Section 7.2, the resulting value $\mathbf { h }$ (for hidden but also for hypothesis) forms a representation of the input. The role of the output layer is to take this new representation $\mathbf { h }$ and compute a final output. This output could be a realvalued number, but in many cases the goal of the network is to make some sort of classification decision, and so we will focus on the case of classification.

If we are doing a binary task like sentiment classification, we might have a single output node, and its scalar value $y$ is the probability of positive versus negative sentiment. If we are doing multinomial classification, such as assigning a part-ofspeech tag, we might have one output node for each potential part-of-speech, whose output value is the probability of that part-of-speech, and the values of all the output nodes must sum to one. The output layer is thus a vector $\pmb { y }$ that gives a probability distribution across the output nodes.

Let’s see how this happens. Like the hidden layer, the output layer has a weight matrix (let’s call it $\mathbf { U }$ ), but some models don’t include a bias vector $\mathbf { b }$ in the output layer, so we’ll simplify by eliminating the bias vector in this example. The weight matrix is multiplied by its input vector $( \mathbf { h } )$ to produce the intermediate output $\mathbf { z }$ :

$$
{ \pmb z } = { \bf U } { \bf h }
$$

There are $n _ { 2 }$ output nodes, so $\mathbf { z } \in \mathbb { R } ^ { n _ { 2 } }$ , weight matrix $\mathbf { U }$ has dimensionality $\mathbf { U } \in$ $\mathbb { R } ^ { n _ { 2 } \times n _ { 1 } }$ , and element $\mathbf { U } _ { i j }$ is the weight from unit $j$ in the hidden layer to unit $i$ in the output layer.

However, $\mathbf { z }$ can’t be the output of the classifier, since it’s a vector of real-valued numbers, while what we need for classification is a vector of probabilities. There is a convenient function for normalizing a vector of real values, by which we mean converting it to a vector that encodes a probability distribution (all the numbers lie between 0 and 1 and sum to 1): the softmax function that we saw on page 85 of Chapter 5. More generally for any vector $\mathbf { z }$ of dimensionality $d$ , the softmax is defined as:

softmax

$$
\mathrm { s o f t m a x } ( \mathbf { z } _ { i } ) ~ = ~ \frac { \exp ( \mathbf { z } _ { i } ) } { \sum _ { j = 1 } ^ { d } \exp ( \mathbf { z } _ { j } ) } ~ 1 \leq i \leq d
$$

Thus for example given a vector

$$
\pmb { z } = [ 0 . 6 , 1 . 1 , - 1 . 5 , 1 . 2 , 3 . 2 , - 1 . 1 ] ,
$$

the softmax function will normalize it to a probability distribution (shown rounded):

$$
\operatorname { s o f t m a x } ( \mathbf { z } ) = [ 0 . 0 5 5 , 0 . 0 9 0 , 0 . 0 0 6 7 , 0 . 1 0 , 0 . 7 4 , 0 . 0 1 0 ]
$$

You may recall that we used softmax to create a probability distribution from a vector of real-valued numbers (computed from summing weights times features) in the multinomial version of logistic regression in Chapter 5.

That means we can think of a neural network classifier with one hidden layer as building a vector $\mathbf { h }$ which is a hidden layer representation of the input, and then running standard multinomial logistic regression on the features that the network develops in h. By contrast, in Chapter 5 the features were mainly designed by hand via feature templates. So a neural network is like multinomial logistic regression, but (a) with many layers, since a deep neural network is like layer after layer of logistic regression classifiers; (b) with those intermediate layers having many possible activation functions (tanh, ReLU, sigmoid) instead of just sigmoid (although we’ll continue to use $\sigma$ for convenience to mean any activation function); (c) rather than forming the features by feature templates, the prior layers of the network induce the feature representations themselves.

Here are the final equations for a feedforward network with a single hidden layer, which takes an input vector $\pmb { \times }$ , outputs a probability distribution $\pmb { \ y }$ , and is parameterized by weight matrices $\boldsymbol { \mathsf { W } }$ and $\mathbf { U }$ and a bias vector $\mathbf { b }$ :

$$
\begin{array} { l } { \mathbf { \boldsymbol { \mathsf { h } } } \mathbf { \boldsymbol { \mathsf { \Pi } } } = \mathbf { \boldsymbol { \mathsf { \sigma } } } \mathbf { \boldsymbol { \sigma } } ( \mathbf { \boldsymbol { W } } \mathbf { \boldsymbol { x } } + \mathbf { \boldsymbol { b } } ) } \\ { \mathbf { \boldsymbol { \mathsf { z } } } \mathbf { \boldsymbol { \mathsf { \Pi } } } = \mathbf { \boldsymbol { \mathsf { U } } } \mathbf { \boldsymbol { \mathsf { h } } } } \\ { \mathbf { \boldsymbol { \mathsf { y } } } \mathbf { \boldsymbol { \mathsf { \Pi } } } = \mathbf { \boldsymbol { \mathsf { \ s o f t m a x } } } ( \mathbf { \boldsymbol { z } } ) } \end{array}
$$

And just to remember the shapes of all our variables, $\mathbf { x } \in \mathbb { R } ^ { n _ { 0 } } , \mathbf { h } \in \mathbb { R } ^ { n _ { 1 } } , \mathbf { b } \in \mathbb { R } ^ { n _ { 1 } } .$ , $\boldsymbol { \mathsf { W } } \in \mathbb { R } ^ { n _ { 1 } \times n _ { 0 } }$ , $\mathbf { U } \in \mathbb { R } ^ { n _ { 2 } \times n _ { 1 } }$ , and the output vector $\mathbf { y } \in \mathbb { R } ^ { n _ { 2 } }$ . We’ll call this network a 2- layer network (we traditionally don’t count the input layer when numbering layers, but do count the output layer). So by this terminology logistic regression is a 1-layer network.

# 7.3.1 More details on feedforward networks

Let’s now set up some notation to make it easier to talk about deeper networks of depth more than 2. We’ll use superscripts in square brackets to mean layer numbers, starting at 0 for the input layer. So $\boldsymbol { \mathsf { W } } ^ { [ 1 ] }$ will mean the weight matrix for the (first) hidden layer, and $\mathbf { b } ^ { [ 1 ] }$ will mean the bias vector for the (first) hidden layer. $n _ { j }$ will mean the number of units at layer $j$ . We’ll use $g ( \cdot )$ to stand for the activation function, which will tend to be ReLU or tanh for intermediate layers and softmax for output layers. We’ll use $\mathbf { a } ^ { [ i ] }$ to mean the output from layer $i$ , and $\pmb { z } ^ { [ i ] }$ to mean the combination of previous layer output, weights and biases $\mathbf { \dot { W } } ^ { [ i ] } \mathbf { a } ^ { [ i - 1 ] } + \mathbf { b } ^ { [ i ] }$ . The 0th layer is for inputs, so we’ll refer to the inputs $\pmb { \times }$ more generally as $\mathbf { a } ^ { [ 0 ] }$ .

Thus we can re-represent our 2-layer net from Eq. 7.12 as follows:

$$
\begin{array} { r l } & { \mathbf { z } ^ { [ 1 ] } = \mathbf { W } ^ { [ 1 ] } \mathbf { a } ^ { [ 0 ] } + \mathbf { b } ^ { [ 1 ] } } \\ & { \mathbf { a } ^ { [ 1 ] } = \mathbf { \Phi } \mathbf { g } ^ { [ 1 ] } ( \mathbf { z } ^ { [ 1 ] } ) } \\ & { \mathbf { z } ^ { [ 2 ] } = \mathbf { \Phi } \mathbf { W } ^ { [ 2 ] } \mathbf { a } ^ { [ 1 ] } + \mathbf { b } ^ { [ 2 ] } } \\ & { \mathbf { a } ^ { [ 2 ] } = \mathbf { \Phi } \mathbf { g } ^ { [ 2 ] } ( \mathbf { z } ^ { [ 2 ] } ) } \\ & { \hat { \mathbf { y } } = \mathbf { \Phi } \mathbf { a } ^ { [ 2 ] } } \end{array}
$$

Note that with this notation, the equations for the computation done at each layer are the same. The algorithm for computing the forward step in an n-layer feedforward network, given the input vector $a ^ { [ 0 ] }$ is thus simply:

$$
\begin{array} { l } { { \bf f o r } i \mathbf { i n } 1 , . . . , \mathbf { n } } \\ { { \bf z } ^ { [ i ] } = \mathbf { W } ^ { [ i ] } { \bf a } ^ { [ i - 1 ] } + \mathbf { b } ^ { [ i ] } } \\ { { \bf a } ^ { [ i ] } = g ^ { [ i ] } ( { \bf z } ^ { [ i ] } ) } \\ { \hat { \bf y } = { \bf a } ^ { [ n ] } \quad } \end{array}
$$

It’s often useful to have a name for the final set of activations right before the final softmax. So however many layers we have, we’ll generally call the unnormalized values in the final vector $\pmb { z } ^ { [ n ] }$ , the vector of scores right before the final softmax, the logits (see Eq. 5.7).

logits

The need for non-linear activation functions One of the reasons we use nonlinear activation functions for each layer in a neural network is that if we did not, the resulting network is exactly equivalent to a single-layer network. Let’s see why this is true. Imagine the first two layers of such a network of purely linear layers:

$$
\begin{array} { l } { { \displaystyle { \bf z } ^ { [ 1 ] } ~ = ~ { \bf W } ^ { [ 1 ] } { \bf x } + { \bf b } ^ { [ 1 ] } } } \\ { { \displaystyle { \bf z } ^ { [ 2 ] } ~ = ~ { \bf W } ^ { [ 2 ] } { \bf z } ^ { [ 1 ] } + { \bf b } ^ { [ 2 ] } } } \end{array}
$$

We can rewrite the function that the network is computing as:

$$
\begin{array} { r l } & { \mathbf { z } ^ { [ 2 ] } = \mathbf { W } ^ { [ 2 ] } \mathbf { z } ^ { [ 1 ] } + \mathbf { b } ^ { [ 2 ] } } \\ & { = \mathbf { \Psi } \mathbf { W } ^ { [ 2 ] } ( \mathbf { W } ^ { [ 1 ] } \mathbf { x } + \mathbf { b } ^ { [ 1 ] } ) + \mathbf { b } ^ { [ 2 ] } } \\ & { = \mathbf { \Psi } \mathbf { W } ^ { [ 2 ] } \mathbf { W } ^ { [ 1 ] } \mathbf { x } + \mathbf { W } ^ { [ 2 ] } \mathbf { b } ^ { [ 1 ] } + \mathbf { b } ^ { [ 2 ] } } \\ & { = \mathbf { \Psi } \mathbf { W } ^ { \prime } \mathbf { x } + \mathbf { b } ^ { \prime } } \end{array}
$$

This generalizes to any number of layers. So without non-linear activation functions, a multilayer network is just a notational variant of a single layer network with a different set of weights, and we lose all the representational power of multilayer networks.

Replacing the bias unit In describing networks, we will often use a slightly simplified notation that represents exactly the same function without referring to an explicit bias node $^ b$ . Instead, we add a dummy node ${ \bf a } _ { 0 }$ to each layer whose value will always be 1. Thus layer 0, the input layer, will have a dummy node $\mathbf { a } _ { 0 } ^ { [ 0 ] } = 1$ , layer 1 will have $\mathbf { a } _ { 0 } ^ { [ 1 ] } = 1$ , and so on. This dummy node still has an associated weight, and that weight represents the bias value $^ b$ . For example instead of an equation like

$$
\mathbf { h } = \sigma ( \mathbf { W } \mathbf { x } + \mathbf { b } )
$$

we’ll use:

$$
\mathbf { h } = \sigma ( \mathbf { W } \mathbf { x } )
$$

But now instead of our vector $\pmb { \times }$ having $n _ { 0 }$ values: $\mathbf { x } = \mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { n _ { 0 } }$ , it will have $n _ { 0 } +$ 1 values, with a new 0th dummy value $\mathbf { x } _ { 0 } = 1$ : $\mathbf { x } = \mathbf { x } _ { 0 } , \ldots , \mathbf { x } _ { n _ { 0 } }$ . And instead of computing each $\mathbf { h } _ { j }$ as follows:

$$
\mathbf { \boldsymbol { \mathsf { h } } } _ { j } = \sigma \left( \sum _ { i = 1 } ^ { n _ { 0 } } \mathbf { \boldsymbol { \mathsf { W } } } _ { j i } \mathbf { \boldsymbol { \mathsf { x } } } _ { i } + \mathbf { \boldsymbol { \mathsf { b } } } _ { j } \right) ,
$$

we’ll instead use:

$$
\mathsf { \mathbf { h } } _ { j } = \sigma \left( \sum _ { i = 0 } ^ { n _ { 0 } } \mathsf { \mathbf { W } } _ { j i } \mathsf { \mathbf { x } } _ { i } \right) ,
$$

where the value $\boldsymbol { \mathsf { W } } _ { j 0 }$ replaces what had been ${ \mathbf b } _ { j }$ . Fig. 7.9 shows a visualization.

![## Image Analysis: 563e549d73fb49181b87718bb3fed8851044891fe5b0e032addcd50e990c2a05.jpg

**Conceptual Understanding:**
This image conceptually represents two alternative methods for integrating a bias term into a feedforward neural network architecture. The main purpose is to illustrate the equivalence between explicitly representing a bias node (a) and incorporating it as a constant input feature within the input layer (b). It communicates the key idea that adding a constant value to the weighted sum of inputs for a neuron can be achieved through different structural representations, which have the same mathematical effect on the network's computations and learning.

**Content Interpretation:**
The image illustrates two configurations of a feedforward neural network, specifically focusing on how the bias term is handled. Each diagram depicts an input layer (x nodes), a hidden layer (h nodes), and an output layer (y nodes), connected by weight matrices W (input to hidden) and U (hidden to output). The central theme is the representation of the bias node and its connections.

In diagram (a), the bias is explicitly shown as a standalone input node labeled '+1', which connects, via specific bias weights collectively denoted 'b', to all nodes in the hidden layer (h₁, h₂, h₃, ..., h_{n₁}). This signifies that a constant value of +1 is fed into each hidden neuron, with its own learnable weight (b) for each connection. This is a common representation where bias is seen as an additive constant to the weighted sum of inputs before activation.

In diagram (b), the bias is integrated into the input layer itself. An additional input node, labeled 'x₀=1', is introduced alongside the original inputs x₁, x₂, ..., x_{n₀}. This 'x₀=1' node then connects to all nodes in the hidden layer (h₁, h₂, h₃, ..., h_{n₁}). This implies that the bias term is treated as an input feature with a constant value of 1. The weights connecting 'x₀=1' to the hidden layer nodes are then implicitly part of the overall weight matrix W. This allows the bias to be handled uniformly with other input-to-hidden layer weights.

Both diagrams demonstrate how bias can be introduced into a neural network, with the green/yellow arrows specifically highlighting the connections related to the bias mechanism. The ellipses indicate that there are more nodes than explicitly drawn, showing a general architecture for networks with n₀ inputs, n₁ hidden neurons, and n₂ outputs.

**Key Insights:**
The main takeaway from this image is that the bias term in a feedforward neural network, which allows the activation function to be shifted, can be incorporated in at least two equivalent ways:
1.  **Explicit Bias Node (Diagram a):** A dedicated constant input node (e.g., '+1') provides an additional weighted input (labeled 'b') to each neuron in the subsequent layer (the hidden layer in this case). This adds a trainable constant to the weighted sum of inputs for each hidden neuron.
2.  **Augmented Input Vector (Diagram b):** The bias can be modeled by augmenting the input vector with an additional input feature that always has a value of 1 (e.g., 'x₀=1'). The weights connecting this 'x₀=1' input to the next layer then act as the bias weights. This method integrates the bias into the standard weight matrix between layers.

The insight is that these two representations are functionally equivalent. Diagram (b) often simplifies the mathematical notation and implementation, as it allows the bias weights to be part of the main weight matrix (W) by expanding the input vector to include the constant '1'. The specific text elements like '+1', 'b', and 'x₀=1' directly illustrate these different mechanisms for incorporating bias into the network architecture.

**Document Context:**
This image directly supports the document section "7.3.1 More details on feedforward networks" by visually explaining an important detail about neural network architecture: the implementation of bias. The accompanying text, "Figure 7.9 Replacing the bias node (shown in a) with x₀ (b)", precisely describes the content of the image. It helps readers understand that the explicit bias node shown in (a) can be equivalently represented by adding a constant input (x₀=1) to the input layer, as shown in (b). This is crucial for comprehending the mathematical formulation and practical implementation of feedforward networks, as it demonstrates a common technique to simplify the representation of bias within the general weight matrix structure.

**Summary:**
The image displays two equivalent feedforward neural network architectures, labeled (a) and (b), demonstrating how a bias can be integrated into the network. Both diagrams show an input layer, a hidden layer, and an output layer, with connections representing weights between layers.

In diagram (a), the network consists of an input layer on the left with nodes labeled x₁ through x_{n₀}, a hidden layer in the middle with nodes h₁ through h_{n₁}, and an output layer on the right with nodes y₁ through y_{n₂}. The connections from the input layer to the hidden layer are associated with weights W, and connections from the hidden layer to the output layer are associated with weights U. Additionally, there is a separate bias node labeled '+1' at the bottom left. This '+1' node has specific connections, highlighted in green/yellow, going to each node in the hidden layer (h₁, h₂, h₃, ..., h_{n₁}). One of these connections is explicitly labeled 'b', indicating the bias weight associated with that connection. The ellipses (...) in all three layers indicate a continuation of nodes not explicitly drawn.

In diagram (b), the network has a similar structure with input nodes x₁ through x_{n₀}, hidden nodes h₁ through h_{n₁}, and output nodes y₁ through y_{n₂}. The weight matrices W and U are also indicated for the respective layers. The key difference in diagram (b) is the way the bias is represented. Instead of a separate '+1' node, there is an additional input node labeled 'x₀=1.' This 'x₀=1' node, highlighted in green/yellow, connects to every node in the hidden layer (h₁, h₂, h₃, ..., h_{n₁}). This effectively integrates the bias directly into the input vector, where the bias weight is then part of the overall weight matrix W. The ellipses (...) again denote continuation of nodes in each layer.

Essentially, diagram (a) explicitly shows a bias term (+1) with its own set of weights (b) added to the inputs of the hidden layer, whereas diagram (b) demonstrates an alternative by appending a constant input (x₀=1) to the input vector, allowing the bias weights to be learned as part of the standard weight matrix W between the input and hidden layers. Both approaches achieve the same mathematical effect of adding a bias term to the hidden layer activations.](images/563e549d73fb49181b87718bb3fed8851044891fe5b0e032addcd50e990c2a05.jpg)
Figure 7.9 Replacing the bias node (shown in a) with $x _ { 0 }$ (b).

We’ll continue showing the bias as $^ b$ when we go over the learning algorithm in Section 7.5, but then we’ll switch to this simplified notation without explicit bias terms for the rest of the book.

# 7.4 Feedforward networks for NLP: Classification

Let’s see how to apply feedforward networks to NLP tasks! In this section we’ll look at classification tasks like sentiment analysis; in the next section we’ll introduce neural language modeling.

Let’s begin with a simple 2-layer sentiment classifier. You might imagine taking our logistic regression classifier from Chapter 5, which corresponds to a 1-layer network, and just adding a hidden layer. The input element $\pmb { x } _ { i }$ could be scalar features like those in Fig. 5.2, e.g., $\mathbf { x } _ { 1 } =$ count(words $\mathbf { \Xi } \in \mathrm { \ d o c } $ ), ${ \bf x } _ { 2 } = { \bf \Phi }$ count(positive lexicon words $\mathbf { \tau } \in \mathrm { d o c } \ '$ ), $\mathbf { x } _ { 3 } = 1$ if “no” $\in$ doc, and so on. And the output layer $\hat { \mathbf { y } }$ could have two nodes (one each for positive and negative), or 3 nodes (positive, negative, neutral), in which case $\hat { \bf y } _ { 1 }$ would be the estimated probability of positive sentiment, $\hat { \bf y } _ { 2 }$ the probability of negative and $\hat { \bf y } _ { 3 }$ the probability of neutral. The resulting equations would be just what we saw above for a 2-layer network (as always, we’ll continue to use the $\sigma$ to stand for any non-linearity, whether sigmoid, ReLU or other).

$$
\begin{array} { l l } { { \bf x } { \mathrm { ~ = ~ } } [ { \bf x } _ { 1 } , { \bf x } _ { 2 } , . . . { \bf x } _ { N } ] } & { ( \mathrm { e a c h } { \bf x } _ { i } \mathrm { i s } \mathrm { a } \mathrm { h a n d - d e s i g n e d f e a t u r e } ) } \\ { { \bf h } { \mathrm { ~ = ~ } } \sigma ( { \bf W } { \bf x } + { \bf b } ) } \\ { { \bf z } { \mathrm { ~ = ~ } } { \bf U } { \bf h } } \\ { \hat { \bf y } { \mathrm { ~ = ~ } } \mathrm { s o f t m a x } ( { \bf z } ) } \end{array}
$$

![## Image Analysis: 70face0a55d6f469ec4e2de5899862238fc5fcf746768e37bab5a87184a8246e.jpg

**Conceptual Understanding:**
This image represents a feedforward neural network architecture specifically designed for sentiment analysis. Conceptually, it illustrates how a sequence of input words is transformed through a series of computational layers to predict the sentiment (positive, negative, or neutral) of the text. The main purpose is to demonstrate how traditional, hand-built features of text can be fed into a neural network with a hidden layer to capture more complex, non-linear patterns, ultimately leading to a classification of sentiment probabilities. It visually explains the flow of information from raw text features, through intermediate representations in a hidden layer, to a final sentiment prediction output.

**Content Interpretation:**
The image illustrates a feedforward neural network architecture tailored for sentiment analysis. It demonstrates how traditional, hand-built linguistic features extracted from an input text are processed through multiple layers to produce sentiment probabilities. The 'Input layer' represents the initial feature extraction, where raw text is converted into quantifiable features like word count, positive lexicon word count, and negative word count. The 'Hidden layer' allows the network to learn and represent non-linear interactions between these features, which is crucial for capturing complex patterns in sentiment. The 'Output layer' then translates these learned representations into a probability distribution over different sentiment categories (positive, negative, neutral) using a 'softmax' function. The network's structure, including the input 'x' with dimensions '[n×1]', the weight matrix 'W' with '[d_h_×n]', the hidden layer output 'h' with '[d_h_×1]', the weight matrix 'U' with '[3×d_h_]', and the final output 'y' with '[3×1]', details the mathematical operations and transformations involved in this sentiment classification process.

**Key Insights:**
The main takeaway from this image is the architectural design of a basic feedforward neural network for sentiment analysis. It highlights that sentiment classification can be achieved by: 
1.  **Feature Engineering:** Extracting relevant, hand-built linguistic features (e.g., word count, positive lexicon words, count of specific negative words) from the input text. 
2.  **Non-linear Transformations:** Utilizing a 'Hidden layer' to enable the network to learn and model non-linear relationships and complex interactions between these input features, which is a key advantage over simpler linear models like logistic regression.
3.  **Probabilistic Output:** Producing a probabilistic output for different sentiment categories (positive, negative, neutral) using a 'softmax' activation function in the 'Output layer'.

The specific text 'wordcount =3', 'positive lexicon words = 1', 'count of "no" = 0' provides evidence for the type of hand-built features used. The labels 'Input layer n=3 features', 'Hidden layer', and 'Output layer softmax' clearly delineate the structure. The outputs 'p(+)', 'p(-)', 'p(neut)' demonstrate the multi-class sentiment classification goal, and the matrix dimensions like '[d_h_×n]' for 'W' and '[3×d_h_]' for 'U' give insight into the mathematical mechanics of the network's transformations and the role of the hidden layer's dimensionality ('d_h'). The inclusion of '...' in the hidden layer suggests that the number of hidden units can be variable and learned during training.

**Document Context:**
This image is directly relevant to the 'Feedforward networks for NLP: Classification' section of the document. It serves as a visual aid, specifically Fig. 7.10, to explain the architecture of a feedforward network applied to sentiment analysis. The accompanying text mentions that adding the hidden layer allows the network to represent non-linear interactions between features, potentially leading to a better sentiment classifier. The image perfectly illustrates this concept by showing the input features (hand-built), the hidden layer, and the sentiment probability outputs, thus supporting the explanation of how these networks classify text based on sentiment.

**Summary:**
The image depicts a feedforward neural network architecture designed for sentiment analysis using traditional, hand-built features of an input text. The process begins with 'Input words' such as "dessert", "was", and "great". These words are processed to extract three specific features: 'wordcount =3', 'positive lexicon words = 1', and 'count of "no" = 0'. These features serve as the inputs to the network, labeled as x₁, x₂, and x₃ respectively, forming the 'Input layer' with 'n=3 features'. This input layer is represented by the vector 'x' with dimensions '[n×1]'.

These inputs are then fed into a 'Hidden layer', which consists of multiple nodes labeled from 'h₁' to 'h_d_h', where 'd_h' indicates the dimensionality of the hidden layer. The connections between the input layer and the hidden layer are governed by a weight matrix 'W', with dimensions '[d_h_×n]'. The output of the hidden layer is represented by the vector 'h' with dimensions '[d_h_×1]'.

Finally, the outputs from the hidden layer are passed to the 'Output layer', which consists of three nodes labeled 'ŷ₁', 'ŷ₂', and 'ŷ₃'. The connections from the hidden layer to the output layer are governed by a weight matrix 'U', with dimensions '[3×d_h_]'. The output layer then produces three sentiment probabilities: 'p(+)' for positive sentiment, 'p(-)' for negative sentiment, and 'p(neut)' for neutral sentiment. This output is represented by the vector 'y' with dimensions '[3×1]', and the 'softmax' activation function is applied at this layer to ensure these outputs are valid probabilities. The entire network illustrates how hand-built linguistic features are transformed through a hidden layer to predict sentiment probabilities.](images/70face0a55d6f469ec4e2de5899862238fc5fcf746768e37bab5a87184a8246e.jpg)
Fig. 7.10 shows a sketch of this architecture. As we mentioned earlier, adding this hidden layer to our logistic regression classifier allows the network to represent the non-linear interactions between features. This alone might give us a better sentiment classifier.   
Figure 7.10 Feedforward network sentiment analysis using traditional hand-built features of the input text.

Most applications of neural networks for NLP do something different, however. Instead of using hand-built human-engineered features as the input to our classifier, we draw on deep learning’s ability to learn features from the data by representing words as embeddings, like the word2vec or GloVe embeddings we saw in Chapter 6. There are various ways to represent an input for classification. One simple baseline is to apply some sort of pooling function to the embeddings of all the words in the input. For example, for a text with $n$ input words/tokens $w _ { 1 } , . . . , w _ { n }$ , we can turn the $n$ embeddings $\mathbf { e } ( w _ { 1 } ) , . . . , \mathbf { e } ( w _ { n } )$ (each of dimensionality $d$ ) into a single embedding also of dimensionality $d$ by just summing the embeddings, or by taking their mean (summing and then dividing by $n$ ):

$$
{ \bf x } _ { m e a n } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } { \bf e } ( w _ { i } )
$$

There are many other options, like taking the element-wise max. The element-wise max of a set of $n$ vectors is a new vector whose kth element is the max of the kth elements of all the $n$ vectors. Here are the equations for this classifier assuming mean pooling; the architecture is sketched in Fig. 7.11:

$$
\begin{array} { l } { \mathbf { x _ { \lambda } } = \mathrm { m e a n } ( \mathbf { e } ( w _ { 1 } ) , \mathbf { e } ( w _ { 2 } ) , \dots , \mathbf { e } ( w _ { n } ) ) } \\ { \mathbf { h _ { \lambda } } = \sigma ( \mathsf { W } \mathbf { x } + \mathbf { b } ) } \\ { \mathbf { z _ { \lambda } } = \mathbf { U } \mathbf { h _ { \lambda } } } \\ { \hat { \mathbf { y _ { \lambda } } } = \mathrm { s o f t m a x } ( \mathbf { z } ) } \end{array}
$$

![## Image Analysis: c41f9828041c459bff3368cb30d1f0567e5dbdcb6a0dcb7c319be55913537ead.jpg

**Conceptual Understanding:**
The image conceptually represents a Feedforward Neural Network (FNN) architecture for the task of sentiment analysis. Its main purpose is to illustrate how a sequence of input words is processed through word embeddings, aggregated via pooling, and then fed into a multi-layered neural network to classify the overall sentiment (positive, negative, or neutral) of the input. The key idea being communicated is the end-to-end process of transforming linguistic data into numerical probabilities of sentiment, highlighting the roles of word embeddings, pooling, and the layered structure of an FNN with softmax activation.

**Content Interpretation:**
The image displays the architecture of a feedforward neural network specifically designed for sentiment analysis. It illustrates the transformation of input words into numerical representations and their subsequent processing to predict sentiment probabilities. The key processes shown are: word embedding generation, where each input word is converted into a dense vector; a pooling operation that aggregates multiple word embeddings into a single fixed-size vector, representing the entire input phrase; and a multi-layer feedforward neural network consisting of an input layer, a hidden layer, and an output layer. The network performs non-linear transformations on the pooled embedding to extract features relevant for sentiment classification. The final output layer, utilizing a softmax activation function, produces probabilities for three sentiment categories: positive, negative, and neutral. The mathematical notations (x, W, h, U, y) and their dimensions ([d×1], [dh×d], [dh×1], [3×dh], [3×1]) provide precise details about the vector and matrix sizes involved in the network's computations, demonstrating the flow of data through the layers.

**Key Insights:**
The main takeaway from this image is the complete workflow of a feedforward neural network-based sentiment analysis system, starting from raw text and ending with sentiment probabilities. Key insights include: 1.  **Word Embeddings as Foundation:** Individual words like 'dessert', 'was', 'great' are first converted into 'embedding for "dessert"', 'embedding for "was"', 'embedding for "great"'. This highlights that neural networks operate on numerical representations of words. 2.  **Pooling for Sentence Representation:** The 'pooling' operation aggregates these individual word embeddings into a single 'pooled embedding', which is crucial for handling variable-length input sequences and creating a fixed-size input 'x [d×1]' for the subsequent feedforward layers. This shows how a phrase or sentence is represented as a single vector. 3.  **Feedforward Network Structure:** The network comprises an 'Input layer', 'Hidden layer' (with neurons 'h1' through 'hdh'), and an 'Output layer' (with neurons 'ŷ1', 'ŷ2', 'ŷ3'). This illustrates the fundamental layered structure of a feedforward neural network. 4.  **Weighted Connections:** The connections between layers are governed by weight matrices 'W [dh×d]' and 'U [3×dh]', demonstrating how the network learns to transform inputs through these weighted sums. 5.  **Probabilistic Sentiment Output:** The 'Output layer softmax' yields probabilities 'p(+)', 'p(-)', 'p(neut)', indicating a multi-class classification problem where the output is a probability distribution over sentiment categories. This shows how the network provides a quantitative measure of sentiment. The specific text labels and mathematical dimensions provide direct evidence for these insights, detailing each component and its role in the sentiment analysis pipeline.

**Document Context:**
This image is highly relevant to the document's section '7.4 Feedforward networks for NLP: Classification' as it provides a concrete and detailed example of how a feedforward network is applied to a specific Natural Language Processing (NLP) task: sentiment analysis. It directly illustrates the concept described in the surrounding text, showing the practical implementation of using pooled word embeddings as input to such a network for classification. The figure serves to visually explain the architecture and data flow, enhancing the reader's understanding of the theoretical concepts discussed in the text by providing a clear, step-by-step diagram of the process.

**Summary:**
This image illustrates the architecture of a feedforward neural network for sentiment analysis using a pooled embedding of input words. The process begins with 'Input words', specifically demonstrated with the words 'dessert', 'was', and 'great'. Each of these words is transformed into its respective 'embedding for "dessert"', 'embedding for "was"', and 'embedding for "great"'. These individual word embeddings, represented by vertical stacks of three colored circles (dark red, medium red, light red), are then fed into a 'pooling' operation, indicated by a green diamond with a plus sign. The pooling operation combines these individual embeddings into a single 'pooled embedding', represented by a similar stack of three colored circles. This pooled embedding serves as the 'Input layer' to the feedforward network. Below this, the mathematical representation of the input is shown as 'x' with dimensions '[d×1]', labeled as 'Input layer pooled embedding'. This input is then connected to a 'Hidden layer', which consists of multiple neurons, specifically labeled as h1, h2, h3, and continuing to hdh, enclosed within a vertical rounded rectangle. The mathematical representation for the hidden layer is 'h' with dimensions '[dh×1]'. The connections from the Input layer to the Hidden layer are weighted by a matrix 'W', with dimensions '[dh×d]'. Finally, the Hidden layer neurons are connected to the 'Output layer'. The Output layer consists of three neurons, labeled ŷ1, ŷ2, and ŷ3, enclosed in a rounded rectangle. These connections are weighted by a matrix 'U', with dimensions '[3×dh]'. The mathematical representation for the output is 'y' with dimensions '[3×1]'. The Output layer is also labeled 'softmax', indicating the activation function used. Each output neuron corresponds to a sentiment probability: ŷ1 outputs 'p(+)' for positive sentiment, ŷ2 outputs 'p(-)' for negative sentiment, and ŷ3 outputs 'p(neut)' for neutral sentiment. This entire structure clearly shows how linguistic input is converted into numerical representations, processed through a neural network, and ultimately classified into sentiment categories.](images/c41f9828041c459bff3368cb30d1f0567e5dbdcb6a0dcb7c319be55913537ead.jpg)
Figure 7.11 Feedforward network sentiment analysis using a pooled embedding of the input words.

While Eq. 7.21 shows how to classify a single example $x$ , in practice we want to efficiently classify an entire test set of $m$ examples. We do this by vectorizing the process, just as we saw with logistic regression; instead of using for-loops to go through each example, we’ll use matrix multiplication to do the entire computation of an entire test set at once. First, we pack all the input feature vectors for each input $x$ into a single input matrix $\pmb { \times }$ , with each row $i$ a row vector consisting of the pooled embedding for input example $x ^ { ( i ) }$ (i.e., the vector $\mathbf { x } ^ { ( i ) }$ ). If the dimensionality of our pooled input embedding is $d$ , $\pmb { \times }$ will be a matrix of shape $[ m \times d ]$ .

We will then need to slightly modify Eq. 7.21. $\pmb { \times }$ is of shape $[ m \times d ]$ and $\boldsymbol { \mathsf { W } }$ is of shape $[ d _ { h } \times d ]$ , so we’ll have to reorder how we multiply $\pmb { \times }$ and $\boldsymbol { \mathsf { W } }$ and transpose $\boldsymbol { \mathsf { W } }$ so they correctly multiply to yield a matrix $\boldsymbol { \mathsf { H } }$ of shape $[ m \times d _ { h } ]$ .1 The bias vector b from Eq. 7.21 of shape $[ 1 \times d _ { h } ]$ will now have to be replicated into a matrix of shape $[ m \times d _ { h } ]$ . We’ll need to similarly reorder the next step and transpose U. Finally, our output matrix $\hat { \pmb { \mathsf { \mathsf { Y } } } }$ will be of shape $[ m \times 3 ]$ (or more generally $[ m \times d _ { o } ]$ , where $d _ { o }$ is the number of output classes), with each row $i$ of our output matrix $\hat { \pmb { \mathsf { \mathsf { Y } } } }$ consisting of the output vector $\hat { \mathbf { y } } ^ { ( i ) }$ .‘ Here are the final equations for computing the output class

distribution for an entire test set:

$$
\begin{array} { l } { \displaystyle \mathsf { \pmb { \mathsf { H } } } = \sigma ( \pmb { \mathsf { X } } \pmb { \mathsf { W } } ^ { \intercal } + \pmb { \mathsf { b } } ) } \\ { \displaystyle \pmb { \mathsf { Z } } = \pmb { \mathsf { H } } \pmb { \mathsf { U } } ^ { \intercal } } \\ { \displaystyle \hat { \pmb { \mathsf { Y } } } = \mathrm { s o f t m a x } ( \pmb { \mathsf { Z } } ) } \end{array}
$$

# pretraining

The idea of using word2vec or GloVe embeddings as our input representation— and more generally the idea of relying on another algorithm to have already learned an embedding representation for our input words—is called pretraining. Using pretrained embedding representations, whether simple static word embeddings like word2vec or the much more powerful contextual embeddings we’ll introduce in Chapter 11, is one of the central ideas of deep learning. (It’s also possible, however, to train the word embeddings as part of an NLP task; we’ll talk about how to do this in Section 7.7 in the context of the neural language modeling task.)

# 7.5 Training Neural Nets

A feedforward neural net is an instance of supervised machine learning in which we know the correct output $y$ for each observation $x$ . What the system produces, via Eq. 7.13, is $\hat { y }$ , the system’s estimate of the true y. The goal of the training procedure is to learn parameters $\boldsymbol { \mathsf { W } } ^ { [ i ] }$ and $\mathbf { b } ^ { [ i ] }$ for each layer $i$ that make $\hat { y }$ for each training observation as close as possible to the true $y$ .

In general, we do all this by drawing on the methods we introduced in Chapter 5 for logistic regression, so the reader should be comfortable with that chapter before proceeding.

First, we’ll need a loss function that models the distance between the system output and the gold output, and it’s common to use the loss function used for logistic regression, the cross-entropy loss.

Second, to find the parameters that minimize this loss function, we’ll use the gradient descent optimization algorithm introduced in Chapter 5.

Third, gradient descent requires knowing the gradient of the loss function, the vector that contains the partial derivative of the loss function with respect to each of the parameters. In logistic regression, for each observation we could directly compute the derivative of the loss function with respect to an individual $w$ or $^ b$ . But for neural networks, with millions of parameters in many layers, it’s much harder to see how to compute the partial derivative of some weight in layer 1 when the loss is attached to some much later layer. How do we partial out the loss over all those intermediate layers? The answer is the algorithm called error backpropagation or backward differentiation.

# 7.5.1 Loss function

The cross-entropy loss that is used in neural networks is the same one we saw for logistic regression. If the neural network is being used as a binary classifier, with the sigmoid at the final layer, the loss function is the same logistic regression loss we saw in Eq. 5.23:

$$
L _ { C E } ( \hat { y } , y ) = - \log p ( y | x ) ~ = ~ - \left[ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) \right]
$$

If we are using the network to classify into 3 or more classes, the loss function is exactly the same as the loss for multinomial regression that we saw in Chapter 5 on page 97. Let’s briefly summarize the explanation here for convenience. First, when we have more than 2 classes we’ll need to represent both $\pmb { \ y }$ and $\hat { \bf y }$ as vectors. Let’s assume we’re doing hard classification, where only one class is the correct one. The true label $\pmb { \ y }$ is then a vector with $K$ elements, each corresponding to a class, with $\mathsf { y } _ { c } = 1$ if the correct class is $c$ , with all other elements of $\pmb { y }$ being 0. Recall that a vector like this, with one value equal to 1 and the rest 0, is called a one-hot vector. And our classifier will produce an estimate vector with $K$ elements $\hat { \mathbf { y } }$ , each element $\hat { \mathbf { y } } _ { k }$ of which represents the estimated probability $p ( \mathbf { y } _ { k } = 1 | \mathbf { x } )$ .

The loss function for a single example $\pmb { \times }$ is the negative sum of the logs of the $K$ output classes, each weighted by their probability $y _ { k }$ :

$$
L _ { C E } ( \hat { \mathbf { y } } , \mathbf { y } ) = - \sum _ { k = 1 } ^ { K } \mathbf { y } _ { k } \log \hat { \mathbf { y } } _ { k }
$$

We can simplify this equation further; let’s first rewrite the equation using the function $\mathbb { 1 } \{ \}$ which evaluates to 1 if the condition in the brackets is true and to 0 otherwise. This makes it more obvious that the terms in the sum in Eq. 7.24 will be 0 except for the term corresponding to the true class for which $\mathbf { y } _ { k } = 1$ :

$$
L _ { C E } ( \hat { \mathbf { y } } , \mathbf { y } ) ~ = ~ - \sum _ { k = 1 } ^ { K } \mathbb { 1 } \{ \mathbf { y } _ { k } = 1 \} \log \hat { \mathbf { y } } _ { k }
$$

In other words, the cross-entropy loss is simply the negative log of the output probability corresponding to the correct class, and we therefore also call this the negative log likelihood loss:

$$
\begin{array} { r l } { L _ { C E } ( \hat { \bf y } , { \bf y } ) ~ = ~ - \log \hat { \bf y } _ { c } } & { { } ( \mathrm { w h e r e } ~ c \mathrm { i s } \mathrm { t h e } \cot \mathrm { c o r r e c t } \mathrm { c l a s s } ) } \end{array}
$$

Plugging in the softmax formula from Eq. 7.9, and with $K$ the number of classes:

$$
L _ { C E } ( \hat { \bf y } , { \bf y } ) ~ = ~ - \log \frac { \exp ( { \bf z } _ { c } ) } { \sum _ { j = 1 } ^ { K } \exp ( { \bf z } _ { j } ) } ~ ( \mathrm { w h e r e ~ } c \mathrm { ~ i s ~ t h e ~ c o r r e c t ~ c l a s s } )
$$

# 7.5.2 Computing the Gradient

How do we compute the gradient of this loss function? Computing the gradient requires the partial derivative of the loss function with respect to each parameter. For a network with one weight layer and sigmoid output (which is what logistic regression is), we could simply use the derivative of the loss that we used for logistic regression in Eq. 7.27 (and derived in Section 5.10):

$$
\begin{array} { r c l } { { \displaystyle \frac { \partial { \cal L } _ { C E } ( \hat { \bf y } , { \bf y } ) } { \partial w _ { j } } ~ = ~ ( \hat { y } - y ) { \bf x } _ { j } } } \\ { { ~ } } & { { ~ = ~ ( \sigma ( { \bf w } \cdot { \bf x } + b ) - y ) { \bf x } _ { j } } } \end{array}
$$

Or for a network with one weight layer and softmax output ( $\mathop { \left. \sum \right.}  \left( \mathrm { \frac { \partial } { \partial \phi } } \right) =$ multinomial logistic regression), we could use the derivative of the softmax loss from Eq. 5.48, shown for a particular weight $\boldsymbol { \mathsf { w } } _ { k }$ and input $\pmb { x } _ { i }$

$$
\begin{array} { r c l } { \displaystyle \frac { \partial L _ { \mathrm { C E } } ( \hat { \bf y } , { \bf y } ) } { \partial { \bf w } _ { k , i } } ~ = ~ - ( { \bf y } _ { k } - \hat { \bf y } _ { k } ) { \bf x } _ { i } } \\ { \displaystyle } & { = ~ - ( { \bf y } _ { k } - p ( { \bf y } _ { k } = 1 | { \bf x } ) ) { \bf x } _ { i } } \\ { \displaystyle } & { = ~ - \left( { \bf y } _ { k } - \frac { \exp { ( { \bf w _ { k } \cdot \bf x } + b _ { k } ) } } { \sum _ { j = 1 } ^ { K } \exp { ( { \bf w _ { j } \cdot \bf x } + b _ { j } ) } } \right) { \bf x } _ { i } } \end{array}
$$

But these derivatives only give correct updates for one weight layer: the last one! For deep networks, computing the gradients for each weight is much more complex, since we are computing the derivative with respect to weight parameters that appear all the way back in the very early layers of the network, even though the loss is computed only at the very end of the network.

The solution to computing this gradient is an algorithm called error backpropagation or backprop (Rumelhart et al., 1986). While backprop was invented specially for neural networks, it turns out to be the same as a more general procedure called backward differentiation, which depends on the notion of computation graphs. Let’s see how that works in the next subsection.

# 7.5.3 Computation Graphs

A computation graph is a representation of the process of computing a mathematical expression, in which the computation is broken down into separate operations, each of which is modeled as a node in a graph.

Consider computing the function $L ( a , b , c ) = c ( a + 2 b )$ . If we make each of the component addition and multiplication operations explicit, and add names $d$ and $e$ ) for the intermediate outputs, the resulting series of computations is:

$$
\begin{array} { l } { d \ = \ 2 * b } \\ { e \ = \ a + d } \\ { L \ = \ c * e } \end{array}
$$

We can now represent this as a graph, with nodes for each operation, and directed edges showing the outputs from each operation as the inputs to the next, as in Fig. 7.12. The simplest use of computation graphs is to compute the value of the function with some given inputs. In the figure, we’ve assumed the inputs $a = 3$ , $b = 1$ , $c = - 2$ , and we’ve shown the result of the forward pass to compute the result $L ( 3 , 1 , - 2 ) = - 1 0$ . In the forward pass of a computation graph, we apply each operation left to right, passing the outputs of each computation as the input to the next node.

![## Image Analysis: 0ecea448b4bc2520c5457ccb9dac1009e342e59c613193c7fd6e7b4913bae663.jpg

**Conceptual Understanding:**
This image conceptually represents a 'computation graph', which is a way to express and evaluate mathematical expressions as a network of interconnected operations (nodes) and data flow (edges). The main purpose of this specific graph is to illustrate the 'forward pass' for a given function, L(a, b, c) = c(a + 2b), with specific numerical inputs. It aims to visualize how the input values propagate through a sequence of intermediate calculations to produce the final output value of the function. The key idea communicated is the step-by-step, directed computation of a function, breaking it down into atomic operations and showing the flow of data and intermediate results.

**Content Interpretation:**
This image is a directed acyclic graph (DAG) representing a computation graph. It shows the process of calculating the value of the function L(a, b, c) = c(a + 2b) for given input values a=3, b=1, and c=-2. The graph explicitly details the 'forward pass' operation, where input values propagate through the nodes, and intermediate results are computed sequentially until the final output L is determined. 

**Extracted Text Elements and their Interpretation:**
*   **Node 'a' (Input):** Represents the input variable 'a'. The arrow leading from it is labeled 'a=3', indicating that the value of 'a' is 3.
*   **Node 'b' (Input):** Represents the input variable 'b'. The arrow leading from it is labeled 'b=1', indicating that the value of 'b' is 1.
*   **Node 'c' (Input):** Represents the input variable 'c'. The arrow leading from it is labeled 'c=-2', indicating that the value of 'c' is -2.
*   **Node 'd = 2b' (Intermediate Calculation):** This node performs the operation of multiplying 'b' by 2. The arrow leading from 'b' to this node feeds the value 'b=1'. The arrow emerging from this node is labeled 'd=2', meaning that 2 * 1 = 2, so the value of 'd' is 2.
*   **Node 'e = a+d' (Intermediate Calculation):** This node performs the operation of adding 'a' and 'd'. It receives 'a=3' from node 'a' and 'd=2' from node 'd = 2b'. The arrow emerging from this node is labeled 'e=5', meaning that 3 + 2 = 5, so the value of 'e' is 5.
*   **Node 'L = ce' (Final Calculation/Output):** This node performs the final operation of multiplying 'c' and 'e' to compute 'L'. It receives 'c=-2' from node 'c' and 'e=5' from node 'e = a+d'. The text next to this node is 'L=-10', meaning that -2 * 5 = -10, which is the final output value of the function L.
*   **Annotation 'forward pass':** This label, accompanied by a green curved arrow pointing from left to right, explicitly denotes that the graph illustrates the forward propagation of values through the network, from inputs to output.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Decomposition of Functions:** The image clearly demonstrates how a complex function like L(a, b, c) = c(a + 2b) can be decomposed into a series of simpler, elementary operations (multiplication and addition) represented as nodes in a graph.
2.  **Forward Pass Mechanism:** It illustrates the 'forward pass' computation, which is the process of propagating input values through the computation graph, performing the operation at each node, and calculating intermediate results sequentially until the final output is obtained. This is a core concept in neural networks and automatic differentiation.
3.  **Intermediate Value Tracking:** The graph explicitly shows the values of intermediate variables (d=2, e=5) as they are computed, making the entire calculation transparent and traceable.
4.  **Directed Flow:** The use of directed arrows emphasizes the flow of computation, indicating dependencies between operations and the order in which they must be executed.

**Textual Evidence for Insights:**
*   The function L(a, b, c) = c(a + 2b) is broken down into nodes: 'd = 2b', 'e = a+d', and 'L = ce'. This directly supports the idea of function decomposition.
*   The green 'forward pass' annotation with the arrow indicates the direction of computation.
*   The labels 'a=3', 'b=1', 'c=-2' show the initial input values.
*   The labels 'd=2' and 'e=5' demonstrate the calculation and tracking of intermediate values.
*   The final label 'L=-10' provides the ultimate output of the function, confirming the complete forward pass.

**Document Context:**
This image is presented in the context of '7.5.3 Computation Graphs' and is specifically referenced as 'Figure 7.12 Computation graph for the function L(a, b, c) = c(a + 2b) , with values for input nodes a = 3 , b = 1 , c = - 2 , showing the forward pass computation of L'. It serves as a visual example to explain how a mathematical function can be represented as a computation graph and how the 'forward pass' works to calculate the function's output given specific input values. The image visually demonstrates the decomposition of a complex function into simpler, elemental operations, which is fundamental to understanding concepts like automatic differentiation in machine learning.

**Summary:**
This image depicts a computation graph illustrating the forward pass computation for the function L(a, b, c) = c(a + 2b) using specific input values. The graph breaks down the overall function into a series of interconnected, simpler operations, starting from the input variables and proceeding through intermediate calculations to arrive at the final output. The flow is clearly indicated by arrows, and each node represents either an input variable or an intermediate computation step. The green text indicates the values assigned to inputs and the results of each intermediate calculation during the forward pass. The 'forward pass' annotation highlights the direction of computation from inputs to output. The final result for L is calculated as -10.](images/0ecea448b4bc2520c5457ccb9dac1009e342e59c613193c7fd6e7b4913bae663.jpg)
Figure 7.12 Computation graph for the function $L ( a , b , c ) = c ( a + 2 b )$ , with values for input nodes $a = 3$ , $b = 1$ , $c = - 2$ , showing the forward pass computation of $L$ .

# 7.5.4 Backward differentiation on computation graphs

The importance of the computation graph comes from the backward pass, which is used to compute the derivatives that we’ll need for the weight update. In this example our goal is to compute the derivative of the output function $L$ with respect to each of the input var bles, i.e $\begin{array} { r } { \frac { \partial L } { \partial a } , \frac { \partial L } { \partial b } } \end{array}$ , and $\frac { \partial L } { \partial c }$ . The derivative $\frac { \partial L } { \partial a }$ tells us how $a$ $L$ .

Backwards differentiation makes use of the chain rule in calculus, so let’s remind ourselves of that. Suppose we are computing the derivative of a composite function $f ( x ) = u ( \nu ( x ) )$ . The derivative of $f ( x )$ is the derivative of $u ( x )$ with respect to $\nu ( x )$ times the derivative of $\nu ( x )$ with respect to $x$ :

$$
{ \frac { d f } { d x } } \ = \ { \frac { d u } { d \nu } } \cdot { \frac { d \nu } { d x } }
$$

The chain rule extends to more than two functions. If computing the derivative of a composite function $f ( x ) = u ( \nu ( w ( x ) ) )$ , the derivative of $f ( x )$ is:

$$
{ \frac { d f } { d x } } \ = \ { \frac { d u } { d \nu } } \cdot { \frac { d \nu } { d w } } \cdot { \frac { d w } { d x } }
$$

The intuition of backward differentiation is to pass gradients back from the final node to all the nodes in the graph. Fig. 7.13 shows part of the backward computation at one node $e$ . Each node takes an upstream gradient that is passed in from its parent node to the right, and for each of its inputs computes a local gradient (the gradient of its output with respect to its input), and uses the chain rule to multiply these two to compute a downstream gradient to be passed on to the next earlier node.

![## Image Analysis: ba3bca98104c0d6b83eb383e94312b40856bc3b936989d78d7a81ffbc0b303f8.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified segment of a computational graph used in backward differentiation (backpropagation). Its main purpose is to illustrate the mechanism by which gradients are calculated and propagated backward through a single node, 'e', applying the chain rule. It visually distinguishes between the forward pass of computation (from 'd' to 'e' to 'L') and the backward pass of gradient calculation, highlighting the roles of upstream, local, and downstream gradients.

**Content Interpretation:**
The image illustrates the computation and propagation of gradients during backward differentiation in a computational graph. It specifically focuses on a single node 'e' and its connection to a preceding node 'd' and a subsequent node 'L' (presumably the loss function or final output). The green arrows show the forward computation path where 'd' is an input to 'e', and 'e' is an input to 'L'. The red arrows and associated text demonstrate the backward pass for gradient calculation. The key concepts shown are: the 'upstream gradient' (∂L/∂e) which is the gradient of the final output 'L' with respect to 'e', the 'local gradient' (∂e/∂d) which is the gradient of 'e' with respect to its direct input 'd', and the 'downstream gradient' (∂L/∂d) which is the gradient of 'L' with respect to 'd'. The image explicitly shows the application of the chain rule, where the downstream gradient is calculated by multiplying the upstream gradient by the local gradient: ∂L/∂d = ∂L/∂e * ∂e/∂d. This demonstrates how a node computes the gradient for its input based on the gradient it receives from its output and its own internal derivative.

**Key Insights:**
The main takeaway from this image is the explicit illustration of how the chain rule is applied in backward differentiation on a computation graph. Specifically, it demonstrates that to compute the gradient of the final loss 'L' with respect to a node's input 'd' (the 'downstream gradient', ∂L/∂d), one must multiply the gradient of 'L' with respect to the node's output 'e' (the 'upstream gradient', ∂L/∂e) by the gradient of the node's output 'e' with respect to its input 'd' (the 'local gradient', ∂e/∂d). This process ensures that gradients are correctly propagated from the output of the graph back to its inputs, which is fundamental for optimizing parameters in models like neural networks. The image clearly segments the different types of gradients involved in this step-by-step calculation.

**Document Context:**
This image directly supports the document's section '7.5.4 Backward differentiation on computation graphs' by visually explaining the core mechanism of how gradients are calculated and propagated backward through individual nodes in a computation graph. It provides a concrete example of the chain rule's application in this context, clarifying the roles of upstream, local, and downstream gradients. The text 'Figure 7.13 Each node (like e here) takes an upstream gradient, multiplies it by the local gradient (the gradient of its output with respect to its input), and uses the chain rule to compute a downstream gradient to be passed on to a prior node. A node may have multiple local gradients if it has multiple inputs' directly references and explains the diagram, making the image central to understanding the described differentiation process.

**Summary:**
The image illustrates the process of backward differentiation on a computation graph, specifically focusing on how gradients are propagated backward through a node 'e'. The forward pass is represented by green arrows, showing the flow from 'd' to 'e' and then from 'e' to 'L'. The backward pass, which calculates the gradients, is indicated by red arrows. For node 'e', the 'upstream gradient' (∂L/∂e) is received from 'L'. This upstream gradient is then multiplied by the 'local gradient' (∂e/∂d), which represents the gradient of 'e' with respect to its input 'd'. According to the chain rule, this multiplication yields the 'downstream gradient' (∂L/∂d), which is then passed backward to node 'd'. The overall explanation shows how each node takes an upstream gradient, applies its local gradient, and computes a downstream gradient for prior nodes.](images/ba3bca98104c0d6b83eb383e94312b40856bc3b936989d78d7a81ffbc0b303f8.jpg)
Figure 7.13 Each node (like $e$ here) takes an upstream gradient, multiplies it by the local gradient (the gradient of its output with respect to its input), and uses the chain rule to compute a downstream gradient to be passed on to a prior node. A node may have multiple local gradients if it has multiple inputs.

Let’s now compute the 3 derivatives we need. Since in the computation graph $L = c e$ , we can directly compute the derivative $\frac { \partial L } { \partial c }$ :

$$
\frac { \partial L } { \partial c } = e
$$

For the other two, we’ll need to use the chain rule:

$$
\begin{array} { l } { \displaystyle { \frac { \partial L } { \partial a } = \frac { \partial L } { \partial e } \frac { \partial e } { \partial a } } } \\ { \displaystyle { \frac { \partial L } { \partial b } = \frac { \partial L } { \partial e } \frac { \partial e } { \partial d } \frac { \partial d } { \partial b } } } \end{array}
$$

Eq. 7.32 and Eq. 7.31 thus require five intermediate derivatives: $\frac { \partial d } { \partial b }$ , which are as follows (making use of the fact that the derivative of a sum is the ∂ L∂ e , ∂ L∂ c , ∂ e∂ a , ∂ e∂ d , and

sum of the derivatives):

$$
\begin{array} { c } { { L = c e ~ : ~ \displaystyle \frac { \partial L } { \partial e } = c , \displaystyle \frac { \partial L } { \partial c } = e } } \\ { { e = a + d ~ : ~ \displaystyle \frac { \partial e } { \partial a } = 1 , \displaystyle \frac { \partial e } { \partial d } = 1 } } \\ { { d = 2 b ~ : ~ \displaystyle \frac { \partial d } { \partial b } = 2 } } \end{array}
$$

In the backward pass, we compute each of these partials along each edge of the graph from right to left, using the chain rule just as we did above. Thus we begin by computing the downstream gradients frowe then multiply this upstream gradient nodeby t $L$ , which are  local grad $\frac { \partial L } { \partial e }$ and (the ∂ L . For node e, adient of t $\frac { \partial L } { \partial e }$   
output with respect to the input), $\textstyle { \frac { \partial e } { \partial d } }$ to get the output we send back to node d: $\textstyle { \frac { \partial L } { \partial d } }$ . And so on, until we have annotated the graph all the way to all the input variables. The forward pass conveniently already will have computed the values of the forward intermediate variables we need (like $d$ and $e$ ) to compute these derivatives. Fig. 7.14 shows the backward pass.

![## Image Analysis: f328409bd13c377233c576fe1aee60a1b210582960e383b61e9dfa21b43c6439.jpg

**Conceptual Understanding:**
This image represents a computation graph designed to illustrate the process of backward differentiation, also known as backpropagation. Conceptually, it shows how a complex function's output (a 'loss' in machine learning contexts) can be broken down into simpler, interconnected operations, and how the sensitivity of this output to its inputs (gradients) can be calculated efficiently. The main purpose is to demonstrate the step-by-step application of the chain rule for computing partial derivatives in a directed acyclic graph structure, essential for optimizing parameters in models like neural networks. It visualizes both the forward flow of data (calculating function values) and the reverse flow of gradients (calculating derivatives).

**Content Interpretation:**
The image shows a computation graph with nodes representing operations and variables, and directed edges representing data flow. The green labels indicate forward pass values, while the red labels indicate backward pass partial derivatives. 

**Forward Pass:**
1.  Initial values are given: a=3, b=1, c=-2.
2.  A computation node `d = 2b` calculates `d = 2 * 1 = 2`.
3.  Another computation node `e = d+a` calculates `e = 2 + 3 = 5`.
4.  The final loss node `L = ce` calculates `L = -2 * 5 = -10`. This value is shown as `L = -10` next to the `L = ce` node.

**Backward Pass (Gradient Computation):**
This process starts from the output `L` and propagates gradients backward through the graph using the chain rule, indicated by the 'backward pass' label.
1.  From `L = ce`:
    *   The partial derivative of L with respect to e is `∂L/∂e = c = -2`. This is shown as `∂L/∂e = -2` next to the `L = ce` node.
    *   The partial derivative of L with respect to c is `∂L/∂c = e = 5`. This is shown as `∂L/∂c = 5` next to the `L = ce` node, and also explicitly leading to node 'c' as `∂L/∂c = 5`.
2.  From `e = d+a`:
    *   The partial derivative of e with respect to a is `∂e/∂a = 1`. This is shown as `∂e/∂a = 1` under the `e=d+a` node.
    *   The partial derivative of e with respect to d is `∂e/∂d = 1`. This is shown as `∂e/∂d = 1` under the `e=d+a` node.
3.  Propagating `∂L/∂e` to `d`:
    *   `∂L/∂d = ∂L/∂e * ∂e/∂d = -2 * 1 = -2`. This is shown on the arrow connecting `e=d+a` to `d=2b` as `∂L/∂d = ∂L/∂e ∂e/∂d = -2`.
4.  Propagating `∂L/∂e` to `a`:
    *   `∂L/∂a = ∂L/∂e * ∂e/∂a = -2 * 1 = -2`. This is shown on the arrow connecting `e=d+a` to `a` as `∂L/∂a = ∂L/∂e ∂e/∂a = -2`.
5.  From `d = 2b`:
    *   The partial derivative of d with respect to b is `∂d/∂b = 2`. This is shown as `∂d/∂b = 2` under the `d=2b` node.
6.  Propagating `∂L/∂d` to `b`:
    *   `∂L/∂b = ∂L/∂d * ∂d/∂b = -2 * 2 = -4`. This is shown on the arrow connecting `d=2b` to `b` as `∂L/∂b = ∂L/∂d ∂d/∂b = -4`.

The image thus completely maps out the gradient calculations for the final loss `L` with respect to the initial input variables `a`, `b`, and `c`.

**Key Insights:**
**Main Takeaways:**
1.  **Computation Graph Visualization:** The image effectively visualizes a computation graph, breaking down a complex function into a series of simpler operations (nodes) and data dependencies (edges).
2.  **Forward Pass for Value Calculation:** It shows how the initial input values (a=3, b=1, c=-2) propagate through the graph to compute intermediate values (d=2, e=5) and the final output (L=-10).
3.  **Backward Pass for Gradient Calculation (Backpropagation):** The primary insight is the application of the chain rule in reverse order of computation to efficiently calculate the gradients of the final output (L) with respect to all preceding variables and inputs.
4.  **Chain Rule in Action:** Each red arrow label explicitly demonstrates an application of the chain rule (e.g., ∂L/∂d = ∂L/∂e * ∂e/∂d) and provides the numerical result of that partial derivative.
5.  **Individual Gradient Contributions:** The graph allows for understanding how each component of the function contributes to the overall gradient, showing intermediate partial derivatives like ∂e/∂a, ∂e/∂d, and ∂d/∂b.
6.  **Final Gradients:** The image culminates in the calculation of the gradients with respect to the primary inputs: ∂L/∂a = -2, ∂L/∂b = -4, and ∂L/∂c = 5.

**Specific Textual Evidence for Insights:**
*   `a=3`, `b=1`, `c=-2`: Initial input values for the forward pass.
*   `d=2b`, `d=2`: Calculation of intermediate value 'd'.
*   `e=d+a`, `e=5`: Calculation of intermediate value 'e'.
*   `L=ce`, `L=-10`: Calculation of the final loss 'L'.
*   `backward pass`: Explicitly indicates the direction and type of computation.
*   `∂L/∂e = -2`, `∂L/∂c = 5`: Gradients computed at the final node `L=ce`.
*   `∂e/∂a = 1`, `∂e/∂d = 1`: Local gradients for the node `e=d+a`.
*   `∂L/∂d = ∂L/∂e ∂e/∂d = -2`: Application of the chain rule to find `∂L/∂d`.
*   `∂L/∂a = ∂L/∂e ∂e/∂a = -2`: Application of the chain rule to find `∂L/∂a`.
*   `∂d/∂b = 2`: Local gradient for the node `d=2b`.
*   `∂L/∂b = ∂L/∂d ∂d/∂b = -4`: Application of the chain rule to find `∂L/∂b`.

These elements collectively demonstrate the systematic and modular nature of backward differentiation, making it clear how gradients are computed efficiently by reusing intermediate derivatives.

**Document Context:**
This image directly illustrates the concept of 'Backward differentiation on computation graphs' as stated in Section 7.5.4 of the document. It provides a concrete example of how the backpropagation algorithm, fundamental to training machine learning models, works by demonstrating the step-by-step calculation of gradients. The textual context after the image confirms that it shows 'the backward pass computation of ∂L/∂a, ∂L/∂b, and ∂L/∂c' for the function L(a, b, c) = c(a + 2b), directly corresponding to the processes shown in the graph.

**Summary:**
This image depicts a computation graph illustrating the process of backward differentiation (backpropagation) for the function L(a, b, c) = c(a + 2b). It visualizes both the forward pass, where intermediate values and the final loss L are calculated, and the backward pass, where partial derivatives (gradients) of L with respect to its input variables (a, b, c) are computed using the chain rule. The graph clearly shows the values assigned to the input variables, the results of intermediate computations, and the step-by-step calculation of gradients along the reverse path of the graph. The 'backward pass' label explicitly indicates the direction of gradient computation.](images/f328409bd13c377233c576fe1aee60a1b210582960e383b61e9dfa21b43c6439.jpg)
Figure 7.14 Computation graph for the function $\overline { { L ( a , b , c ) = c ( a + 2 b ) } }$ , showing the backward pass computation of $\begin{array} { r } { \frac { \partial L } { \partial a } , \frac { \partial L } { \partial b } } \end{array}$ , and $\textstyle { \frac { \partial L } { \partial c } }$ .

# Backward differentiation for a neural network

Of course computation graphs for real neural networks are much more complex. Fig. 7.15 shows a sample computation graph for a 2-layer neural network with $n _ { 0 } =$ 2, ${ n } _ { 1 } = 2$ , and $n _ { 2 } = 1$ , assuming binary classification and hence using a sigmoid output unit for simplicity. The function that the computation graph is computing is:

$$
\begin{array} { r l r } {  { \mathbf { z } ^ { [ 1 ] } = \mathbf { W } ^ { [ 1 ] } \mathbf { x } + \mathbf { b } ^ { [ 1 ] } } } \\ & { } & { } \\ { \mathbf { a } ^ { [ 1 ] } = \mathbf { R e L U } ( \mathbf { z } ^ { [ 1 ] } ) } \\ & { } & { } \\ { \mathbf { z } ^ { [ 2 ] } = \mathbf { W } ^ { [ 2 ] } \mathbf { a } ^ { [ 1 ] } + b ^ { [ 2 ] } } \\ & { } & { } \\ { \mathbf { \boldsymbol { a } } ^ { [ 2 ] } = \mathbf { \boldsymbol { \sigma } } ( \mathbf { z } ^ { [ 2 ] } ) } \\ & { } & { \hat { \mathbf { y } } = \mathbf { \boldsymbol { a } } ^ { [ 2 ] } } \end{array}
$$

For the backward pass we’ll also need to compute the loss $L$ . The loss function for binary sigmoid output from Eq. 7.23 is

$$
L _ { C E } ( \hat { y } , y ) ~ = ~ - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]
$$

Our output $\hat { y } = a ^ { [ 2 ] }$ , so we can rephrase this as

$$
L _ { C E } ( a ^ { [ 2 ] } , y ) ~ = ~ - \left[ y \log a ^ { [ 2 ] } + ( 1 - y ) \log ( 1 - a ^ { [ 2 ] } ) \right]
$$

![## Image Analysis: 697a846925c1de433d9c9b645dd186ced059634281119fb994b81677c3d8ac22.jpg

**Conceptual Understanding:**
This image conceptually represents the computational graph of a simple feedforward neural network, illustrating the flow of data and operations during the forward pass. Its main purpose is to visualize the sequence of mathematical operations (multiplications, summations, and activation functions) that transform input features into an output prediction and then compute a loss value. The key idea communicated is the step-by-step calculation within a neural network, showing how inputs are weighted, summed, and activated across different layers to produce a final result.

**Content Interpretation:**
The image depicts the architecture and computational steps of a 2-layer neural network (one hidden layer, one output layer).

*   **Input Layer:** `x₁` and `x₂` represent the two input features for the network.
*   **First Hidden Layer (Nodes `z₁^[1] = +`, `a₁^[1] = ReLU`, `z₂^[1] = +`, `a₂^[1] = ReLU`):
    *   This layer consists of two hidden units.
    *   Each hidden unit (`z₁^[1]` and `z₂^[1]`) computes a weighted sum of the inputs (`x₁`, `x₂`) and a bias term (`b₁^[1]` or `b₂^[1]`). The `*` nodes explicitly show the multiplication of inputs by corresponding weights (`w₁₁^[1]`, `w₁₂^[1]`, `w₂₁^[1]`, `w₂₂^[1]`), and the `+` in `z₁^[1] = +` and `z₂^[1] = +` indicates the summation.
    *   The outputs of these weighted sums are then passed through a Rectified Linear Unit (`ReLU`) activation function (`a₁^[1] = ReLU`, `a₂^[1] = ReLU`). This non-linear transformation allows the network to learn complex patterns.
*   **Second (Output) Layer (Nodes `z^[2] = +`, `a^[2] = σ`):
    *   This layer takes the activated outputs from the hidden layer (`a₁^[1]`, `a₂^[1]`) as its inputs.
    *   Similar to the hidden layer, these inputs are multiplied by their respective weights (`w₁₁^[2]`, `w₁₂^[2]`) and summed with a bias term (`b₁^[2]`) to compute `z^[2]`.
    *   A sigmoid (`σ`) activation function (`a^[2] = σ`) is applied to `z^[2]` to produce the final output `a^[2]`. The choice of sigmoid suggests a binary classification task or an output that needs to be scaled between 0 and 1.
*   **Loss Function (Node `L (a^[2],y)`):
    *   The final output `a^[2]` is compared with the true target `y` (implied as an external input to the loss function) to compute the loss `L (a^[2],y)`. This value quantifies how well the network's prediction matches the actual target and is crucial for training the network via backward differentiation.

The connections via arrows indicate the data flow, showing how the output of one operation becomes the input to the next. The different colors of the nodes (light blue for weights/biases, green for inputs/operations/activations/loss) might visually distinguish between parameters and computed values/inputs, although this is not explicitly stated in the context. The `*` symbol consistently denotes multiplication, and the `+` symbol within the `z` nodes consistently denotes summation, clearly illustrating the basic arithmetic operations underlying neural networks.

**Key Insights:**
Main takeaways and insights:

1.  **Modular Structure of Neural Networks:** The graph clearly demonstrates that neural networks are built from interconnected layers, where each layer performs a specific set of computations. For example, `z₁^[1] = +` and `a₁^[1] = ReLU` show the linear combination and activation steps within a single neuron.
2.  **Forward Pass Computation:** The image explicitly maps out the entire sequence of calculations from input (`x₁`, `x₂`) to final loss (`L (a^[2],y)`), illustrating the "forward pass" of data through the network. This is evident by tracing the arrows from left to right through all `*`, `+`, `ReLU`, and `σ` operations.
3.  **Role of Weights and Biases:** The light blue nodes (`w₁₁^[1]`, `b₁^[1]`, `w₁₁^[2]`, etc.) highlight the parameters of the network. They are multiplied (`*`) by inputs and added (`+`) to sums, demonstrating their critical role in transforming data and learning patterns.
4.  **Activation Functions for Non-linearity:** The presence of `a₁^[1] = ReLU`, `a₂^[1] = ReLU`, and `a^[2] = σ` nodes explicitly shows the application of non-linear activation functions. This is crucial for enabling the network to learn complex, non-linear relationships in data, moving beyond simple linear models.
5.  **Loss Function for Learning:** The final `L (a^[2],y)` node emphasizes that the ultimate goal of the forward pass is to compute a loss value, which is then used in the backward differentiation (backpropagation) process to update the weights and biases.

**Document Context:**
This computation graph is highly relevant to the "Backward differentiation for a neural network" section of the document. It provides the visual foundation—the forward pass—upon which the concept of backward differentiation (or backpropagation) is built. To compute gradients for backward differentiation, one must first understand the sequence of operations in the forward pass. Each node in this graph represents an operation for which a derivative will need to be calculated during backpropagation, allowing the network's parameters (weights and biases) to be adjusted to minimize the loss. The text after the image explicitly mentions "adjusted the notation a bit to avoid long equations in the nodes by just the function that is being computed, and the resulting variable name," which clarifies the compact notation used in the graph and reinforces its purpose as a functional representation for understanding gradient flow.

**Summary:**
This detailed computation graph illustrates the step-by-step process of a "forward pass" through a simple 2-layer neural network, which includes one hidden layer and an output layer. It takes two initial inputs, x₁ and x₂, and processes them through a series of mathematical operations to arrive at a final prediction and a corresponding loss value.

The network starts with its inputs, x₁ and x₂, represented by green oval nodes on the far left. These inputs are fed into the first hidden layer, which contains two processing units.

For the **first hidden unit** (top path):
1.  x₁ is multiplied (indicated by the * node) by a weight w₁₁^[1] (a light blue oval, representing a parameter of the network).
2.  x₂ is simultaneously multiplied (by another * node) by another weight w₁₂^[1].
3.  The results of these two multiplications, along with a bias term b₁^[1] (another light blue oval), are summed together in the node z₁^[1] = +.
4.  The output z₁^[1] then passes through a Rectified Linear Unit (ReLU) activation function, resulting in a₁^[1] = ReLU. This ReLU function introduces non-linearity, which is vital for the network to learn complex relationships.

In **parallel**, the **second hidden unit** (bottom path) performs similar calculations:
1.  x₁ is multiplied (*) by w₂₁^[1].
2.  x₂ is multiplied (*) by w₂₂^[1].
3.  These two products, combined with b₂^[1], are summed in z₂^[1] = +.
4.  This sum z₂^[1] then undergoes a ReLU activation, yielding a₂^[1] = ReLU.

The outputs from these two hidden units, a₁^[1] and a₂^[1], then feed into the **output layer**:
1.  a₁^[1] is multiplied (*) by a weight w₁₁^[2].
2.  a₂^[1] is multiplied (*) by a weight w₁₂^[2].
3.  These two products, along with a bias b₁^[2], are summed into the node z^[2] = +.
4.  The sum z^[2] then goes through a sigmoid (σ) activation function, producing the final output prediction a^[2] = σ. The sigmoid function typically squashes the output to a range between 0 and 1, often used for probabilities in classification tasks.

Finally, the network computes the **Loss**:
1.  The final output a^[2] is compared against a true target value y (implied as an input for the loss calculation) to compute the loss L (a^[2],y). This loss value indicates how accurate the network's prediction a^[2] is compared to the actual y. This entire sequence, from inputs to loss calculation, is the forward pass, and its detailed mapping is essential for understanding how to adjust the network's weights and biases during training using backward differentiation.](images/697a846925c1de433d9c9b645dd186ced059634281119fb994b81677c3d8ac22.jpg)
Figure 7.15 Sample computation graph for a simple 2-layer neural net $( = 1$ hidden layer) with two input units and 2 hidden units. We’ve adjusted the notation a bit to avoid long equations in the nodes by justthe function that is being computed, and the resulting variable name. Thus the \* to the right of node $w _ { 1 1 } ^ { [ 1 ] }$ tioningmeans that $w _ { 1 1 } ^ { [ 1 ] }$ is to be multiplied by $x _ { 1 }$ , and the node $z ^ { [ 1 ] } = +$ means that the value of $z ^ { [ 1 ] }$ is computed by summing the three nodes that feed into it (the two products, and the bias term $b _ { i } ^ { [ 1 ] } ,$ ).

The weights that need updating (those for which we need to know the partial derivative of the loss function) are shown in teal. In order to do the backward pass, we’ll need to know the derivatives of all the functions in the graph. We already saw in Section 5.10 the derivative of the sigmoid $\sigma$ :

$$
\frac { d \sigma ( z ) } { d z } = \sigma ( z ) ( 1 - \sigma ( z ) )
$$

We’ll also need the derivatives of each of the other activation functions. The derivative of tanh is:

$$
\frac { d \operatorname { t a n h } ( z ) } { d z } = 1 - \operatorname { t a n h } ^ { 2 } ( z )
$$

The derivative of the ReLU is2

$$
\frac { d \mathrm { R e L U } ( z ) } { d z } = \left\{ { \begin{array} { l } { 0 { \mathrm { ~ } } f o r { \mathrm { ~ } } z < 0 } \\ { 1 { \mathrm { ~ } } f o r { \mathrm { ~ } } z \geq 0 } \end{array} } \right.
$$

We’ll give the start of the computation, computing the derivative of the loss function $L$ with respect to $z$ , or $\frac { \partial L } { \partial z }$ (and leaving the rest of the computation as an exercise for the reader). By the chain rule:

$$
\frac { \partial L } { \partial z } = \frac { \partial L } { \partial a ^ { [ 2 ] } } \frac { \partial a ^ { [ 2 ] } } { \partial z }
$$

So let’s first compute ∂ a[2] $\frac { \partial L } { \partial a ^ { [ 2 ] } }$ , taking the derivative of Eq. 7.35, repeated here:

$$
\begin{array} { r l } { { L _ { C E } ( a ^ { [ 2 ] } , y ) \ = \ - \left[ y \log { a ^ { [ 2 ] } } + ( 1 - y ) \log ( 1 - a ^ { [ 2 ] } ) \right] \ ~ } } & { { } } \\ { { \ ~ \frac { \partial L } { \partial a ^ { [ 2 ] } \ } = \ - \left( \left( y \frac { \partial \log ( a ^ { [ 2 ] } ) } { \partial a ^ { [ 2 ] } } \right) + ( 1 - y ) \frac { \partial \log ( 1 - a ^ { [ 2 ] } ) } { \partial a ^ { [ 2 ] } } \right) \ ~ } } & { { } } \\ { { \ } } & { { = \ - \left( \left( y \frac { 1 } { a ^ { [ 2 ] } } \right) + ( 1 - y ) \frac { 1 } { 1 - a ^ { [ 2 ] } } ( - 1 ) \right) \ ~ } } \\ { { \ } } & { { } } \\ { { \ } } & { { = \ - \left( \frac { y } { a ^ { [ 2 ] } } + \frac { y - 1 } { 1 - a ^ { [ 2 ] } } \right) \ ~ } } \end{array}
$$

Next, by the derivative of the sigmoid:

$$
\frac { \partial a ^ { [ 2 ] } } { \partial z } = a ^ { [ 2 ] } ( 1 - a ^ { [ 2 ] } )
$$

Finally, we can use the chain rule:

$$
\begin{array} { l } { { \displaystyle { \frac { \partial { \cal L } } { \partial z } } ~ = ~ { \frac { \partial { \cal L } } { \partial a ^ { [ 2 ] } } } { \frac { \partial a ^ { [ 2 ] } } { \partial z } } } } \\ { { ~ = ~ - \left( { \frac { y } { a ^ { [ 2 ] } } } + { \frac { y - 1 } { 1 - a ^ { [ 2 ] } } } \right) a ^ { [ 2 ] } ( 1 - a ^ { [ 2 ] } ) } } \\ { { ~ = ~ a ^ { [ 2 ] } - y } } \end{array}
$$

Continuing the backward computation of the gradients (next by passing the gradients over $b _ { 1 } ^ { [ 2 ] }$ and the two product nodes, and so on, back to all the teal nodes), is left as an exercise for the reader.

# 7.5.5 More details on learning

Optimization in neural networks is a non-convex optimization problem, more complex than for logistic regression, and for that and other reasons there are many best practices for successful learning.

For logistic regression we can initialize gradient descent with all the weights and biases having the value 0. In neural networks, by contrast, we need to initialize the weights with small random numbers. It’s also helpful to normalize the input values to have 0 mean and unit variance.

Various forms of regularization are used to prevent overfitting. One of the most important is dropout: randomly dropping some units and their connections from the network during training (Hinton et al. 2012, Srivastava et al. 2014). At each iteration of training (whenever we update parameters, i.e. each mini-batch if we are using mini-batch gradient descent), we repeatedly choose a probability $p$ and for each unit we replace its output with zero with probability $p$ (and renormalize the rest of the outputs from that layer).

# hyperparameter

Tuning of hyperparameters is also important. The parameters of a neural network are the weights $\boldsymbol { \mathsf { W } }$ and biases b; those are learned by gradient descent. The hyperparameters are things that are chosen by the algorithm designer; optimal values are tuned on a devset rather than by gradient descent learning on the training set. Hyperparameters include the learning rate $\eta$ , the mini-batch size, the model architecture (the number of layers, the number of hidden nodes per layer, the choice of activation functions), how to regularize, and so on. Gradient descent itself also has many architectural variants such as Adam (Kingma and Ba, 2015).

Finally, most modern neural networks are built using computation graph formalisms that make it easy and natural to do gradient computation and parallelization on vector-based GPUs (Graphic Processing Units). PyTorch (Paszke et al., 2017) and TensorFlow (Abadi et al., 2015) are two of the most popular. The interested reader should consult a neural network textbook for further details; some suggestions are at the end of the chapter.

# 7.6 Feedforward Neural Language Modeling

As our second application of feedforward networks, let’s consider language modeling: predicting upcoming words from prior words. Neural language modeling— based on the transformer architecture that we will see in Chapter 9—is the algorithm that underlies all of modern NLP. In this section and the next we’ll introduce a simpler version of neural language models for feedforward networks, an algorithm first introduced by Bengio et al. (2003). The feedforward language model introduces many of the important concepts of neural language modeling, concepts we’ll return to as we describe more powerful models in Chapter 8 and Chapter 9.

Neural language models have many advantages over the n-gram language models of Chapter 3. Compared to n-gram models, neural language models can handle much longer histories, can generalize better over contexts of similar words, and are more accurate at word-prediction. On the other hand, neural net language models are much more complex, are slower and need more energy to train, and are less interpretable than n-gram models, so for some smaller tasks an n-gram language model is still the right tool.

A feedforward neural language model (LM) is a feedforward network that takes as input at time $t$ a representation of some number of previous words $( w _ { t - 1 } , w _ { t - 2 }$ , etc.) and outputs a probability distribution over possible next words. Thus—like the n-gram LM—the feedforward neural LM approximates the probability of a word given the entire prior context $P ( w _ { t } | w _ { 1 : t - 1 } )$ by approximating based on the $N - 1$ previous words:

$$
P ( w _ { t } | w _ { 1 } , \dots , w _ { t - 1 } ) \approx P ( w _ { t } | w _ { t - N + 1 } , \dots , w _ { t - 1 } )
$$

In the following examples we’ll use a 4-gram example, so we’ll show a neural net to estimate the probability $P ( w _ { t } = i | w _ { t - 3 } , w _ { t - 2 } , w _ { t - 1 } )$ .

Neural language models represent words in this prior context by their embeddings, rather than just by their word identity as used in $\mathbf { n }$ -gram language models. Using embeddings allows neural language models to generalize better to unseen data. For example, suppose we’ve seen this sentence in training:

I have to make sure that the cat gets fed.

but have never seen the words “gets fed” after the word “dog”. Our test set has the prefix “I forgot to make sure that the dog gets”. What’s the next word? An n-gram language model will predict “fed” after “that the cat gets”, but not after “that the dog gets”. But a neural LM, knowing that “cat” and “dog” have similar embeddings, will be able to generalize from the “cat” context to assign a high enough probability to “fed” even after seeing “dog”.

# 7.6.1 Forward inference in the neural language model

Let’s walk through forward inference or decoding for neural language models. Forward inference is the task, given an input, of running a forward pass on the network to produce a probability distribution over possible outputs, in this case next words.

We first represent each of the $N$ previous words as a one-hot vector of length $| V |$ , i.e., with one dimension for each word in the vocabulary. A one-hot vector is a vector that has one element equal to 1—in the dimension corresponding to that word’s index in the vocabulary— while all the other elements are set to zero. Thus in a one-hot representation for the word “toothpaste”, supposing it is $V _ { 5 }$ , i.e., index 5 in the vocabulary, $x _ { 5 } = 1$ , and $x _ { i } = 0 ~ \forall i \neq 5$ , as shown here:

$$
\begin{array} { r } { \begin{array} { c c c c c c c } { [ \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 1 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \ldots } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } \end{array} ] } \\ { \begin{array} { r } { 1 } & { 2 } & { 3 } & { 4 } & { 5 } & { 6 } & { 7 } & { \ldots } & { \ldots } & { | \boldsymbol { \mathbb { V } } | } \end{array} } \end{array}
$$

The feedforward neural language model (sketched in Fig. 7.17) has a moving window that can see $\mathbf { N }$ words into the past. We’ll let $\mathbf { N }$ equal 3, so the 3 words $w _ { t - 1 } , w _ { t - 2 }$ , and $w _ { t - 3 }$ are each represented as a one-hot vector. We then multiply these one-hot vectors by the embedding matrix E. The embedding weight matrix E has a column for each word, each a column vector of $d$ dimensions, and hence has dimensionality $d \times \vert V \vert$ . Multiplying by a one-hot vector that has only one non-zero element $x _ { i } = 1$ simply selects out the relevant column vector for word $i$ , resulting in the embedding for word $i$ , as shown in Fig. 7.16.

![## Image Analysis: 11bf3b4d2523d5dd0d64345598ec649b2abdf2693f9da16c32712a960fcf5a05.jpg

**Conceptual Understanding:**
The image conceptually represents the process of word embedding lookup in neural language models. Its main purpose is to visually explain how a specific word's distributed representation (its embedding vector) is retrieved from a larger collection of word embeddings (the embedding matrix) using a one-hot encoding scheme. It communicates the idea that this retrieval is equivalent to a matrix multiplication operation.

**Content Interpretation:**
The image demonstrates the mathematical operation for extracting a specific word embedding from a larger embedding matrix in the context of neural language models. It shows how multiplying an embedding matrix 'E' by a one-hot encoded vector for a particular word (V₅) yields the embedding vector for that specific word (e₅). The embedding matrix 'E' (d x |V|) contains 'd'-dimensional embedding vectors for all words in the vocabulary '|V|'. The one-hot vector ( |V| x 1) has a '1' at the index corresponding to the word V₅ and zeros elsewhere. The multiplication isolates the column vector from 'E' that corresponds to the '1' in the one-hot vector, which is the embedding for V₅, denoted as 'e₅' (d x 1).

**Key Insights:**
The main takeaway is that selecting a specific word's embedding from an embedding matrix can be mathematically represented as a matrix-vector multiplication. The embedding matrix E holds all word embeddings. A one-hot vector acts as a 'selector' or 'mask,' effectively picking out the column of E that corresponds to the word's index. Specifically, multiplying the `d x |V|` embedding matrix `E` by a `|V| x 1` one-hot vector with a `1` at index `5` results in the `d x 1` embedding vector `e₅` (the 5th column of `E`). This process is crucial for converting discrete word identities into continuous vector representations within neural networks.

**Document Context:**
This image directly supports the section '7.6.1 Forward inference in the neural language model' by visually explaining the mechanism of 'Selecting the embedding vector for word V₅ by multiplying the embedding matrix E with a one-hot vector with a 1 in index 5,' as stated in the text after the image. It illustrates a foundational step in how words are represented and processed within such models, specifically demonstrating how a discrete word index is converted into a dense, continuous embedding vector ready for further computation.

**Summary:**
The image illustrates the process of selecting an embedding vector for a specific word, V_5, from an embedding matrix E using a one-hot vector. This is a fundamental operation in neural language models for obtaining word representations. The process begins with an embedding matrix, E, which has dimensions d rows by |V| columns, where 'd' represents the embedding dimension and '|V|' represents the vocabulary size. A small vertical green segment within matrix E is highlighted, corresponding to the 5th column, with the label '5' beneath it. This matrix is then multiplied by a one-hot vector. This one-hot vector is a column vector of size |V| rows by 1 column, where all elements are zero except for a single '1' at a specific index. In this diagram, the '1' is explicitly shown at the 5th position (labeled '5' next to a small white square within the black vertical vector), indicating that it corresponds to the fifth word in the vocabulary. The multiplication of the embedding matrix E by this one-hot vector results in a new column vector, e₅. This resulting vector, e₅, has dimensions d rows by 1 column, and it corresponds precisely to the 5th column (or the 5th word's embedding) from the original embedding matrix E. The labels '1' above this resulting vector and 'd' to its left confirm its dimensions. Essentially, this diagram visually explains how a specific word's embedding, e₅, is 'selected' or extracted from the larger embedding matrix E by using a one-hot encoding of that word's index.](images/11bf3b4d2523d5dd0d64345598ec649b2abdf2693f9da16c32712a960fcf5a05.jpg)
Figure 7.16 Selecting the embedding vector for word $V _ { 5 }$ by multiplying the embedding matrix E with a one-hot vector with a 1 in index 5.

The 3 resulting embedding vectors are concatenated to produce e, the embedding layer. This is followed by a hidden layer and an output layer whose softmax produces a probability distribution over words. For example $y _ { 4 2 }$ , the value of output node 42, is the probability of the next word $w _ { t }$ being $V _ { 4 2 }$ , the vocabulary word with index 42 (which is the word ‘fish’ in our example).

Here’s the algorithm in detail for our mini example:

1. Select three embeddings from E: Given the three previous words, we look up their indices, create 3 one-hot vectors, and then multiply each by the embedding matrix E. Consider $w _ { t - 3 }$ . The one-hot vector for ‘for’ (index 35) is multiplied by the embedding matrix E, to give the first part of the first hidden

![## Image Analysis: 95a794e480388a1edc2b2e39d287b41d7cc7cf835f0d1f71e49c387b0d4b39ca.jpg

**Conceptual Understanding:**
The image conceptually represents the architecture of a neural network designed for language modeling, specifically a feedforward neural language model. Its main purpose is to illustrate the 'forward inference' process, showing how a neural network takes a sequence of previous words as input, processes them through several layers of computations, and ultimately predicts the probability distribution of the next word in the sequence. It conveys the idea of transforming discrete words into continuous vector representations (embeddings), processing these representations through a hidden layer, and then mapping them to a probability distribution over the entire vocabulary to identify the most likely next word.

**Content Interpretation:**
The image illustrates the architecture and forward pass of a feedforward neural language model, demonstrating how it processes a sequence of preceding words to predict the probability distribution of the next word. It shows the transformation of words from their textual form into numerical representations, through embedding, hidden, and output layers, culminating in a probabilistic prediction. The blue highlighted section and arrow on the left visually emphasize the temporal context of the input words, moving from older words (w_t-3) to more recent ones (w_t-1) to predict the word at w_t.

**Key Insights:**
The main takeaway from this image is the step-by-step process of how a feedforward neural language model predicts the next word based on a fixed window of preceding context words. Key insights include: 1.  **Input Representation:** Words are first converted into 'one-hot vectors' ('x') of dimension '|V|x3', demonstrating sparse representation. 2.  **Embedding:** These one-hot vectors are multiplied by an 'embedding matrix E' to create dense, lower-dimensional 'embedding layer e' vectors ('3dx1'), which capture semantic relationships. This is crucial as explicit text 'E' is present along with 'embedding layer' label. 3.  **Hidden Layer:** The concatenated embedding is transformed by a weight matrix 'W' (dimension 'dhx3d') and then, as described in the accompanying text, an activation function (implied by the transition to 'h'), to form the 'hidden layer h' ('dhx1'), which learns complex features. The 'h_1', 'h_2', 'h_3', '...', 'h_dh' labels explicitly show the components of this layer. 4.  **Output Layer:** The hidden layer is then multiplied by another weight matrix 'U' (dimension '|V|xdh') to produce the 'output layer y' ('|V|x1'). 5.  **Softmax Probability:** A 'softmax' activation is applied to the output layer, yielding a probability distribution over the entire vocabulary for the 'next word w_t'. The labels 'p(aardvark|...)', 'p(do|...)' and 'softmax' confirm this. The image highlights the use of context words ('w_t-3', 'w_t-2', 'w_t-1') and their specific one-hot vector indices (e.g., '35', '992', '451') as inputs, and demonstrates how these inputs flow through distinct layers ('input layer', 'embedding layer', 'hidden layer', 'output layer') with associated matrix dimensions ('dx|V|', '3dx1', 'dhx3d', 'dhx1', '|V|xdh', '|V|x1') to arrive at a probabilistic prediction for the subsequent word.

**Document Context:**
This image directly supports Section 7.6.1, which is titled 'Forward inference in the neural language model'. It visually explains the abstract concept of 'forward inference' by detailing the specific computational steps and layers involved in a feedforward neural language model. The text after the image further elaborates on each step shown in the diagram, describing the embedding process, concatenation, multiplication by weight matrices W and U, and the final softmax output layer for predicting the next word. Thus, the image serves as a crucial visual aid for understanding the technical description provided in the surrounding text.

**Summary:**
The image illustrates the forward inference process within a feedforward neural language model, detailing how a sequence of previous words is processed to predict the next word. The process begins with an input sequence of words: '...', 'and', 'thanks', 'for', 'all', 'the', and a placeholder '?' for the word at time 'w_t'. The words 'for', 'all', and 'the' corresponding to 'w_t-3', 'w_t-2', and 'w_t-1' respectively, are highlighted with a blue box and an arrow indicating a downward flow. Each of these context words ('w_t-3', 'w_t-2', 'w_t-1') is converted into a 'one-hot vector' in the 'input layer'. These vectors, labeled 'x', have a dimension of '|V|x3', where '|V|' represents the vocabulary size. For instance, 'w_t-3' (for) is represented by a one-hot vector with a '1' at position '35', 'w_t-2' (all) at position '992', and 'w_t-1' (the) at position '451'. These one-hot vectors are then transformed by an 'embedding matrix E' (dimension 'dx|V|') to create a 'd'-dimensional embedding for each word. The three resulting embeddings are concatenated to form the 'embedding layer e', which is a vector of dimension '3dx1'. This 'embedding layer e' is then multiplied by a weight matrix 'W' (dimension 'dhx3d') to produce the 'hidden layer h', which has a dimension of 'dhx1'. The hidden layer's nodes are labeled 'h_1', 'h_2', 'h_3', ..., up to 'h_dh'. Finally, the 'hidden layer h' is multiplied by another weight matrix 'U' (dimension '|V|xdh') to generate the 'output layer y', a vector of dimension '|V|x1'. The 'output layer y' applies a 'softmax' function, predicting at each node 'i' the probability that the next word 'w_t' will be a specific vocabulary word 'V_i'. The nodes in the output layer are denoted as 'ŷ_1', 'ŷ_34', 'ŷ_42', 'ŷ_35102', ..., up to 'ŷ_|V|'. These nodes correspond to predicted probabilities such as 'p(aardvark|...)', 'p(do|...)', 'p(fish|...)', and 'p(zebra|...)'. The diagram visually represents the flow of information from input words through the embedding, hidden, and output layers to predict the subsequent word.](images/95a794e480388a1edc2b2e39d287b41d7cc7cf835f0d1f71e49c387b0d4b39ca.jpg)
Figure 7.17 Forward inference in a feedforward neural language model. At each timestep t the network computes a $d$ -dimensional embedding for each context word (by multiplying a one-hot vector by the embedding matrix E), and concatenates the 3 resulting embeddings to get the embedding layer e. The embedding vector e is multiplied by a weight matrix $\boldsymbol { \mathsf { W } }$ and then an activation function is applied element-wise to produce the hidden layer $\mathbf { h }$ , which is then multiplied by another weight matrix U. Finally, a softmax output layer predicts at each node $i$ the probability that the next word $w _ { t }$ will be vocabulary word $V _ { i }$ .

layer, the embedding layer. Since each column of the input matrix $\mathsf { E }$ is an embedding for a word, and the input is a one-hot column vector $\mathbf { x } _ { \mathrm { i } }$ for word $V _ { i }$ , the embedding layer for input $w$ will be $\mathbf { E x _ { i } } = \mathbf { e _ { i } }$ , the embedding for word i. We now concatenate the three embeddings for the three context words to produce the embedding layer e.   
2. Multiply by W: We multiply by $W$ (and add $^ b$ ) and pass through the ReLU (or other) activation function to get the hidden layer $h$ .   
3. Multiply by U: $h$ is now multiplied by $U$   
4. Apply softmax: After the softmax, each node $i$ in the output layer estimates the probability $P ( w _ { t } = i | w _ { t - 1 } , w _ { t - 2 } , w _ { t - 3 } )$

In summary, the equations for a neural language model with a window size of 3, given one-hot input vectors for each input context word, are:

$$
\begin{array} { l } { \mathbf { e _ { \lambda } } = \left[ \mathsf { E x } _ { \mathbf { t } - 3 } ; \mathsf { E x } _ { \mathbf { t } - 2 } ; \mathsf { E x } _ { \mathbf { t } - 1 } \right] } \\ { \mathbf { h _ { \lambda } } = \sigma ( \mathsf { W e } + \mathbf { b _ { \lambda } } ) } \\ { \mathbf { z _ { \lambda } } = \mathbf { \ 4 h _ { \lambda } } } \\ { \hat { \mathbf { y _ { \lambda } } } = \mathrm { \ s o f t m a x } ( \mathbf { z } ) } \end{array}
$$

Note that we formed the embedding layer e by concatenating the 3 embeddings for the three context vectors; we’ll often use semicolons to mean concatenation of vectors.

# 7.7 Training the neural language model

# self-training

The high-level intuition of training neural language models, whether the simple feedforward language models we describe here or the more powerful transformer language models of Chapter 9, is the idea of self-training or self-supervision that we saw in Chapter 6 for learning word representations. In self-training for language modeling, we take a corpus of text as training material and at each time step $t$ ask the model to predict the next word. At first it will do poorly at this task, but since in each case we know the correct answer (it’s the next word in the corpus!) we can easily train it to be better at predicting the correct next word. We call such a model self-supervised because we don’t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence.

# freeze

In practice, training the model means setting the parameters $\theta = \mathsf E , \mathsf w , \mathsf { \mathbf { U } } , \mathsf { \mathbf { b } }$ . For some tasks, it’s ok to freeze the embedding layer E with initial word2vec values. Freezing means we use word2vec or some other pretraining algorithm to compute the initial embedding matrix E, and then hold it constant while we only modify $\boldsymbol { \mathsf { W } }$ , U, and b, i.e., we don’t update E during language model training. However, often we’d like to learn the embeddings simultaneously with training the network. This is useful when the task the network is designed for (like sentiment classification, translation, or parsing) places strong constraints on what makes a good representation for words.

Let’s see how to train the entire model including E, i.e. to set all the parameters $\theta = \mathsf E , \mathsf w , \mathsf { \mathbf { U } } , \mathsf { \mathbf { b } }$ . We’ll do this via gradient descent (Fig. 5.6), using error backpropagation on the computation graph to compute the gradient. Training thus not only sets the weights $\boldsymbol { \mathsf { W } }$ and U of the network, but also as we’re predicting upcoming words, we’re learning the embeddings E for each word that best predict upcoming words.

Fig. 7.18 shows the set up for a window size of ${ \mathrm { N } } { = } 3$ context words. The input $\pmb { \times }$ consists of 3 one-hot vectors, fully connected to the embedding layer via 3 instantiations of the embedding matrix $E$ . We don’t want to learn separate weight matrices for mapping each of the 3 previous words to the projection layer. We want one single embedding dictionary $E$ that’s shared among these three. That’s because over time, many different words will appear as $w _ { t - 2 }$ or $w _ { t - 1 }$ , and we’d like to just represent each word with one vector, whichever context position it appears in. Recall that the embedding weight matrix $E$ has a column for each word, each a column vector of $d$ dimensions, and hence has dimensionality $d \times \vert V \vert$ .

Generally training proceeds by taking as input a very long text, concatenating all the sentences, starting with random weights, and then iteratively moving through the text predicting each word $w _ { t }$ . At each word $w _ { t }$ , we use the cross-entropy (negative log likelihood) loss. Recall that the general form for this (repeated from Eq. 7.25) is:

$$
{ \cal L } _ { C E } ( \hat { y } , y ) ~ = ~ - \log \hat { y } _ { i } , ~ ( \mathrm { w h e r e } ~ i \mathrm { i s } \mathrm { t h e } ~ \mathrm { c o r r e c t } ~ \mathrm { c l a s s } )
$$

For language modeling, the classes are the words in the vocabulary, so $\hat { y } _ { i }$ here means the probability that the model assigns to the correct next word $w _ { t }$ :

$$
L _ { \mathrm { C E } } = - \log p ( w _ { t } | w _ { t - 1 } , . . . , w _ { t - n + 1 } )
$$

The parameter update for stochastic gradient descent for this loss from step $s$ to $s + 1$

![## Image Analysis: 6fb72b24bd9c4467559e4d89cbc98983782007ebe7515728c8a691e5f483105d.jpg

**Conceptual Understanding:**
This image conceptually represents a neural network model designed for predicting a target word from its context words, thereby learning continuous word representations or 'embeddings'. The main purpose is to illustrate the architecture and the flow of information for this word embedding learning process. It communicates the key ideas of one-hot encoding, shared embedding matrices, hidden layer processing, and probabilistic output for word prediction.

**Content Interpretation:**
The image depicts a neural network model, likely a Continuous Bag-of-Words (CBOW) architecture, used for learning word embeddings. It shows how context words ('for', 'all', 'the') are processed to predict a target word ('fish'). The core concept is the transformation of discrete words into dense vector representations (embeddings) through a predictive task. The network has an input layer for one-hot vectors, an embedding layer that uses a shared matrix E, a hidden layer, and an output layer with a softmax activation to produce probabilities for all words in the vocabulary.

**Key Insights:**
The main takeaway is the neural network architecture for learning word embeddings. Key insights include: 1. Input words are represented as one-hot vectors. 2. A shared embedding matrix 'E' is used to convert one-hot vectors into dense word embeddings for context words. 3. These context word embeddings are concatenated to form a single input 'e' for the hidden layer. 4. The model uses a hidden layer ('h') to process the combined context information. 5. The output layer 'y' (with softmax) predicts the probability of each word in the vocabulary being the target word given the context. 6. The learning objective is to minimize the negative log probability of the true target word ('fish'), as shown by the loss 'L = -log P(fish | for, all, the)'. The specific text 'embedding matrix E is shared among the 3 context words' from the surrounding text is visually supported by the repeated 'E' with connections from 'w_t-3', 'w_t-2', and 'w_t-1' one-hot vectors.

**Document Context:**
This image, presented in the "freeze" section and referenced by "Figure 7.18 Learning all the way back to embeddings. Again, the embedding matrix E is shared among the 3 context words," directly illustrates the computational process by which word embeddings are generated and learned within a neural network. It serves as a visual explanation for how the "embedding matrix E is shared among the 3 context words" and how the learning objective (minimizing loss for predicting `w_t`) drives the formation of these embeddings. It is crucial for understanding the underlying mechanism of word representation learning discussed in the document.

**Summary:**
This image illustrates a neural network architecture designed for learning word embeddings, specifically demonstrating how a target word (`w_t`) is predicted based on its surrounding context words (`w_t-3`, `w_t-2`, `w_t-1`). The process begins with the input of context words, which are then converted into one-hot vectors. These vectors are mapped to word embeddings using a shared embedding matrix E. The resulting embeddings are concatenated and passed through a hidden layer, finally leading to an output layer that uses a softmax activation to predict the probability distribution over all possible words in the vocabulary for the given context. A loss function is then calculated based on the predicted probability of the actual target word. The diagram clearly shows the flow of information through each layer, including the dimensions of the matrices and vectors involved.](images/6fb72b24bd9c4467559e4d89cbc98983782007ebe7515728c8a691e5f483105d.jpg)
Figure 7.18 Learning all the way back to embeddings. Again, the embedding matrix E is shared among the 3 context words.

is then:

$$
\theta ^ { s + 1 } = \theta ^ { s } - \eta \frac { \partial \left[ - \log p \left( w _ { t } \vert w _ { t - 1 } , . . . , w _ { t - n + 1 } \right) \right] } { \partial \theta }
$$

This gradient can be computed in any standard neural network framework which will then backpropagate through $\theta = \mathsf { E } , \mathsf { w } , \mathsf { u } , \mathsf { b }$ .

Training the parameters to minimize loss will result both in an algorithm for language modeling (a word predictor) but also a new set of embeddings $\mathbf { E }$ that can be used as word representations for other tasks.

# 7.8 Summary

• Neural networks are built out of neural units, originally inspired by biological neurons but now simply an abstract computational device.   
• Each neural unit multiplies input values by a weight vector, adds a bias, and then applies a non-linear activation function like sigmoid, tanh, or rectified linear unit.   
• In a fully-connected, feedforward network, each unit in layer $i$ is connected to each unit in layer $i + 1$ , and there are no cycles.   
• The power of neural networks comes from the ability of early layers to learn representations that can be utilized by later layers in the network.   
• Neural networks are trained by optimization algorithms like gradient descent.   
• Error backpropagation, backward differentiation on a computation graph, is used to compute the gradients of the loss function for a network.

• Neural language models use a neural network as a probabilistic classifier, to compute the probability of the next word given the previous $n$ words. • Neural language models can use pretrained embeddings, or can learn embeddings from scratch in the process of language modeling.

# Bibliographical and Historical Notes

The origins of neural networks lie in the 1940s McCulloch-Pitts neuron (McCulloch and Pitts, 1943), a simplified model of the biological neuron as a kind of computing element that could be described in terms of propositional logic. By the late 1950s and early 1960s, a number of labs (including Frank Rosenblatt at Cornell and Bernard Widrow at Stanford) developed research into neural networks; this phase saw the development of the perceptron (Rosenblatt, 1958), and the transformation of the threshold into a bias, a notation we still use (Widrow and Hoff, 1960).

The field of neural networks declined after it was shown that a single perceptron unit was unable to model functions as simple as XOR (Minsky and Papert, 1969). While some small amount of work continued during the next two decades, a major revival for the field didn’t come until the 1980s, when practical tools for building deeper networks like error backpropagation became widespread (Rumelhart et al., 1986). During the 1980s a wide variety of neural network and related architectures were developed, particularly for applications in psychology and cognitive science (Rumelhart and McClelland 1986b, McClelland and Elman 1986, Rumelhart and McClelland 1986a, Elman 1990), for which the term connectionist or parallel distributed processing was often used (Feldman and Ballard 1982, Smolensky 1988). Many of the principles and techniques developed in this period are foundational to modern work, including the ideas of distributed representations (Hinton, 1986), recurrent networks (Elman, 1990), and the use of tensors for compositionality (Smolensky, 1990).

By the 1990s larger neural networks began to be applied to many practical language processing tasks as well, like handwriting recognition (LeCun et al. 1989) and speech recognition (Morgan and Bourlard 1990). By the early 2000s, improvements in computer hardware and advances in optimization and training techniques made it possible to train even larger and deeper networks, leading to the modern term deep learning (Hinton et al. 2006, Bengio et al. 2007). We cover more related history in Chapter 8 and Chapter 16.

There are a number of excellent books on the subject. Goldberg (2017) has superb coverage of neural networks for natural language processing. For neural networks in general see Goodfellow et al. (2016) and Nielsen (2015).

8

# RNNs and LSTMs

Time will explain. Jane Austen, Persuasion

Language is an inherently temporal phenomenon. Spoken language is a sequence of acoustic events over time, and we comprehend and produce both spoken and written language as a sequential input stream. The temporal nature of language is reflected in the metaphors we use; we talk of the flow of conversations, news feeds, and twitter streams, all of which emphasize that language is a sequence that unfolds in time.

Yet most of the machine learning approaches we’ve studied so far, like those for sentiment analysis and other text classification tasks don’t have this temporal nature – they assume simultaneous access to all aspects of their input. The feedforward networks of Chapter 7 also assumed simultaneous access, although they also had a simple model for time. Recall that we applied feedforward networks to language modeling by having them look only at a fixed-size window of words, and then sliding this window over the input, making independent predictions along the way. This sliding-window approach is also used in the transformer architecture we will introduce in Chapter 9.

This chapter introduces a deep learning architecture that offers an alternative way of representing time: recurrent neural networks (RNNs), and their variants like LSTMs. RNNs have a mechanism that deals directly with the sequential nature of language, allowing them to handle the temporal nature of language without the use of arbitrary fixed-sized windows. The recurrent network offers a new way to represent the prior context, in its recurrent connections, allowing the model’s decision to depend on information from hundreds of words in the past. We’ll see how to apply the model to the task of language modeling, to text classification tasks like sentiment analysis, and to sequence modeling tasks like part-of-speech tagging (a task we’ll return to in detail in Chapter 17).

# 8.1 Recurrent Neural Networks

A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. While powerful, such networks are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely effective when applied to language. In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent networks. These networks are useful in their own right and serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks discussed later in this chapter. In this chapter when we use the term RNN we’ll be referring to these simpler more constrained networks (although you will often see the term RNN to mean any net with recurrent properties including LSTMs).

![## Image Analysis: c77157213d1dfd49182ab883f3c728c815162fff73dc0a3c146d4e9ab7e9095d.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a simple Recurrent Neural Network (RNN), specifically highlighting its mechanism for handling sequential data through a recurrent connection. Its main purpose is to visually explain how the 'memory' aspect of an RNN is implemented: the hidden layer's activation at the current time step is a function of both the current input and its own activation from the previous time step. The key idea communicated is the concept of recurrence within the hidden layer, which allows the network to process sequences by maintaining an internal state that evolves over time.

**Content Interpretation:**
The image depicts a foundational model of a recurrent neural network (RNN). The 'x subscript t' box represents the input to the network at a specific time step 't'. The 'h subscript t' box denotes the hidden state or activation of the recurrent layer at time step 't'. This hidden layer is crucial because it processes both the current input 'x subscript t' and its own state from the previous time step (implicitly h subscript t-1, via the recurrent connection). The 'y subscript t' box signifies the output of the network at time step 't'. The solid arrows represent the feedforward flow of information: input 'x subscript t' feeds into 'h subscript t', and 'h subscript t' feeds into 'y subscript t'. The dashed cyan line forms a self-loop around 'h subscript t', indicating the recurrent connection. This dashed loop graphically represents that 'h subscript t' receives input from its own past activation, establishing the temporal dependency characteristic of RNNs. All extracted text elements (x subscript t, h subscript t, y subscript t, and the visual representation of the recurrent loop) directly support the interpretation of a basic RNN structure with an input, a hidden state with memory, and an output.

**Key Insights:**
The main takeaway from this image is the fundamental structure of a simple recurrent neural network (RNN). It clearly demonstrates that unlike feedforward networks, RNNs have a hidden layer ('h subscript t') with a recurrent connection (the dashed cyan loop), which allows the network to use information from previous time steps to process the current input. This 'memory' aspect, represented by the feedback from 'h subscript t' to itself, is crucial for handling sequential data. The specific text elements 'x subscript t', 'h subscript t', and 'y subscript t' highlight the input, hidden state, and output at a given time step, while the dashed recurrent arrow explicitly illustrates how the hidden state retains temporal context.

**Document Context:**
This image directly illustrates the core concept of a Simple Recurrent Neural Network, as referenced in Section 8.1 of the document on 'Recurrent Neural Networks'. The figure is specifically identified as 'Figure 8.1 Simple recurrent neural network after Elman (1990)', which aligns with the historical context of RNN development. The accompanying text elaborates on the image by stating, 'The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step.' This precisely describes the visual elements and their functional meaning, making the image a fundamental visual aid for understanding how recurrent connections enable memory in neural networks for sequential data processing.

**Summary:**
This image illustrates the basic architecture of a simple recurrent neural network (RNN). It shows three main components: an input layer labeled 'x subscript t', a hidden layer labeled 'h subscript t', and an output layer labeled 'y subscript t'. Information flows from the input layer to the hidden layer, and then from the hidden layer to the output layer, indicated by solid black arrows. A critical feature of this network, highlighted by a dashed cyan line, is the recurrent connection within the hidden layer. This connection shows that the activation of the hidden layer at the current time step ('h subscript t') is influenced not only by the current input ('x subscript t') but also by its own activation from the previous time step. This feedback loop allows the network to maintain memory over sequences.](images/c77157213d1dfd49182ab883f3c728c815162fff73dc0a3c146d4e9ab7e9095d.jpg)
Figure 8.1 Simple recurrent neural network after Elman (1990). The hidden layer includes a recurrent connection as part of its input. That is, the activation value of the hidden layer depends on the current input as well as the activation value of the hidden layer from the previous time step.

Fig. 8.1 illustrates the structure of an RNN. As with ordinary feedforward networks, an input vector representing the current input, $\mathbf { x } _ { t }$ , is multiplied by a weight matrix and then passed through a non-linear activation function to compute the values for a layer of hidden units. This hidden layer is then used to calculate a corresponding output, $\pmb { y } _ { t }$ . In a departure from our earlier window-based approach, sequences are processed by presenting one item at a time to the network. We’ll use subscripts to represent time, thus $\mathbf { x } _ { t }$ will mean the input vector $\pmb { \times }$ at time $t$ . The key difference from a feedforward network lies in the recurrent link shown in the figure with the dashed line. This link augments the input to the computation at the hidden layer with the value of the hidden layer from the preceding point in time.

The hidden layer from the previous time step provides a form of memory, or context, that encodes earlier processing and informs the decisions to be made at later points in time. Critically, this approach does not impose a fixed-length limit on this prior context; the context embodied in the previous hidden layer can include information extending back to the beginning of the sequence.

Adding this temporal dimension makes RNNs appear to be more complex than non-recurrent architectures. But in reality, they’re not all that different. Given an input vector and the values for the hidden layer from the previous time step, we’re still performing the standard feedforward calculation introduced in Chapter 7. To see this, consider Fig. 8.2 which clarifies the nature of the recurrence and how it factors into the computation at the hidden layer. The most significant change lies in the new set of weights, U, that connect the hidden layer from the previous time step to the current hidden layer. These weights determine how the network makes use of past context in calculating the output for the current input. As with the other weights in the network, these connections are trained via backpropagation.

# 8.1.1 Inference in RNNs

Forward inference (mapping a sequence of inputs to a sequence of outputs) in an RNN is nearly identical to what we’ve already seen with feedforward networks. To compute an output $\pmb { y } _ { t }$ for an input $\mathbf { x } _ { t }$ , we need the activation value for the hidden layer $\mathbf { h } _ { t }$ . To calculate this, we multiply the input $\mathbf { x } _ { t }$ with the weight matrix $\boldsymbol { \mathsf { W } }$ , and the hidden layer from the previous time step $\mathbf { h } _ { t - 1 }$ with the weight matrix U. We add these values together and pass them through a suitable activation function, $g$ , to arrive at the activation value for the current hidden layer, $\mathbf { h } _ { t }$ . Once we have the values for the hidden layer, we proceed with the usual computation to generate the

![## Image Analysis: d7da63ad96c76ca539a0ff962c72a726bc6eb4f8796d12d8bfa798d2412eb97e.jpg

**Conceptual Understanding:**
This image conceptually represents the computational architecture of a single time step within a simple recurrent neural network. Its main purpose is to illustrate how information from the previous time step (represented by the hidden state `h_{t-1}`) is combined with the current input (`x_t`) to compute a new hidden state (`h_t`) and a corresponding output (`y_t`). It visually explains the core mechanism of how RNNs process sequential data by maintaining and updating an internal 'memory' (the hidden state) that evolves over time.

**Content Interpretation:**
The image depicts the computational graph for a single time step 't' within a simple recurrent neural network (RNN), presented as an unrolled feedforward network. It shows the core operations involved in updating the hidden state and generating an output. Specifically, it illustrates how the previous hidden state (`h_{t-1}`) is transformed (by `u`), how the current input (`x_t`) is transformed (by `w`), and how these two transformed components are summed (`+`) to produce the current hidden state (`h_t`). Subsequently, this current hidden state (`h_t`) is further transformed (by `v`) to yield the current output (`y_t`). The shapes `u`, `w`, and `v` represent weight matrices or layers that apply linear transformations, and potentially non-linear activation functions (though not explicitly shown as separate steps). The diamond with a plus sign explicitly signifies a summation operation.

**Key Insights:**
1.  **Temporal Dependency:** The diagram highlights the crucial concept of temporal dependency in RNNs, where the hidden state from the previous time step (`h_{t-1}`) is a direct input to compute the current hidden state (`h_t`).
2.  **Input Integration:** It demonstrates how both the past information (via `h_{t-1}`) and the current input (`x_t`) are combined through weighted transformations and summation to form the new hidden state.
3.  **Feedforward Nature of a Single Step:** Although part of a recurrent network, a single time step can be viewed as a feedforward process taking `h_{t-1}` and `x_t` as inputs and producing `h_t` and `y_t` as outputs.
4.  **Weight Sharing (Implied):** In a full RNN, the weight matrices `u`, `w`, and `v` are typically shared across all time steps, even though the diagram only shows a single instance for time `t`. While not explicitly stated, this is a fundamental principle of RNNs that this single-step representation helps to visualize.
5.  **Hidden State and Output Generation:** The diagram clearly distinguishes between the hidden state (`h_t`), which carries information forward in time, and the output (`y_t`), which is the immediate result for the current time step.

**Document Context:**
This image directly supports the document section '8.1.1 Inference in RNNs' by visually explaining the fundamental computational steps performed during inference (or forward pass) at a single time step of an RNN. The accompanying text states, 'Figure 8.2 Simple recurrent neural network illustrated as a feedforward network. The hidden layer $\mathbf { h } _ { t - 1 }$ from the prior time step is multiplied by weight matrix $\mathbf { \delta u }$ and then added to the feedforward component from the current time step.' This description perfectly aligns with the diagram, where `h_{t-1}` is associated with `u` (representing multiplication by weight matrix U) and combined with the `x_t` component (processed by `w`). It clarifies how information from the past (via `h_{t-1}`) and the present input (`x_t`) are integrated to make predictions or update states in a sequential model.

**Summary:**
This image illustrates a single time step of a simple recurrent neural network (RNN) unrolled as a feedforward network. The process begins with two inputs: `h_{t-1}`, representing the hidden state from the previous time step, and `x_t`, representing the input at the current time step. The hidden state `h_{t-1}` is processed by a transformation or weight matrix labeled `u`. Simultaneously, the current input `x_t` is processed by another transformation or weight matrix labeled `w`. The outputs from `u` and `w` are then combined through an addition operation, symbolized by a diamond containing a `+` sign. The result of this addition is `h_t`, which is the hidden state at the current time step. This `h_t` then feeds into a final transformation or weight matrix labeled `v`, which produces the output `y_t` for the current time step. The flow clearly shows how previous temporal information (`h_{t-1}`) and current input (`x_t`) are integrated to generate a new hidden state (`h_t`) and an output (`y_t`).](images/d7da63ad96c76ca539a0ff962c72a726bc6eb4f8796d12d8bfa798d2412eb97e.jpg)
Figure 8.2 Simple recurrent neural network illustrated as a feedforward network. The hidden layer $\mathbf { h } _ { t - 1 }$ from the prior time step is multiplied by weight matrix $\mathbf { \delta u }$ and then added to the feedforward component from the current time step.

output vector.

$$
\begin{array} { r } { \mathbf { h } _ { t } \ = \ g ( \mathbf { U } \mathbf { h } _ { t - 1 } + \mathbf { W } \mathbf { x } _ { t } ) } \\ { \mathbf { y } _ { t } \ = \ f ( \mathbf { V } \mathbf { h } _ { t } ) } \end{array}
$$

Let’s refer to the input, hidden and output layer dimensions as $d _ { i n } , \ d _ { h }$ , and $d _ { o u t }$ respectively. Given this, our three parameter matrices are: $W \in \mathbb { R } ^ { d _ { h } \times d _ { i n } }$ , $\mathbf { U } \in \mathbb { R } ^ { d _ { h } \times d _ { h } }$ , and $\pmb { \mathsf { v } } \in \mathbb { R } ^ { \dot { d } _ { o u t } \times d _ { h } }$ .

We compute $y _ { t }$ via a softmax computation that gives a probability distribution over the possible output classes.

$$
\mathbf { y } _ { t } ~ = ~ \mathrm { s o f t m a x } ( \mathbf { V } \mathbf { h } _ { t } )
$$

The fact that the computation at time $t$ requires the value of the hidden layer from time $t - 1$ mandates an incremental inference algorithm that proceeds from the start of the sequence to the end as illustrated in Fig. 8.3. The sequential nature of simple recurrent networks can also be seen by unrolling the network in time as is shown in Fig. 8.4. In this figure, the various layers of units are copied for each time step to illustrate that they will have differing values over time. However, the various weight matrices are shared across time.

![## Image Analysis: 7ab599cf4969be254b4e9c94f9e34b9930f68033d2d39fde7cd775aec23981a1.jpg

**Conceptual Understanding:**
This image represents the algorithmic definition for the forward inference process in a simple Recurrent Neural Network (RNN). Conceptually, it illustrates how an RNN processes a sequence of inputs step-by-step, maintaining an internal 'memory' (the hidden state) that evolves with each new input, and generating an output at each step. The main purpose of this pseudocode is to clearly and concisely show the mathematical operations involved in calculating the hidden states and outputs of an RNN over time, given an input sequence and the network's learned parameters. The key ideas communicated are the iterative nature of RNNs, the role of the hidden state in carrying information across time steps, the application of activation functions, and the concept of weight sharing (as implied by the consistent use of matrices `U`, `W`, and `V` within the loop).

**Content Interpretation:**
The image presents pseudocode for the `FORWARDRNN` function, which illustrates the forward pass or inference process in a simple Recurrent Neural Network (RNN). The processes, concepts, and relationships shown are:

*   **Function Definition:** `function FORWARDRNN(x,network) returns output sequence y` defines the entry point, inputs (`x` - input sequence, `network` - network parameters), and expected output (`y` - output sequence).
*   **Hidden State Initialization:** `h₀ ← 0` sets the initial hidden state to zero. This signifies that the network starts without any prior 'memory' before processing the first element of the input sequence.
*   **Sequential Processing Loop:** `for i ← 1 to LENGTH(x) do` establishes the sequential nature of RNNs. The network processes the input sequence `x` element by element over time.
*   **Recurrent Hidden State Update:** `hᵢ ← g(Uhᵢ₋₁ + Wxᵢ)` is the core recurrence relation. This equation shows that the current hidden state `hᵢ` is a function (`g`) of a linear combination of the previous hidden state `hᵢ₋₁` (weighted by matrix `U`) and the current input `xᵢ` (weighted by matrix `W`). This is significant because it allows the network to maintain a 'memory' of past inputs and learn temporal dependencies. The function `g` typically represents a non-linear activation function.
*   **Output Generation:** `yᵢ ← f(Vhᵢ)` demonstrates how the output `yᵢ` for the current time step `i` is derived from the current hidden state `hᵢ` (weighted by matrix `V`) through another activation function `f`. This illustrates the transformation of the internal hidden representation into an external output.
*   **Output Return:** `return y` indicates that the function culminates by returning the full sequence of outputs generated over all time steps.

The significance of this representation lies in detailing the fundamental computational steps of an RNN during inference. It highlights how the hidden state serves as the network's internal memory, updated at each step based on the new input and the previous memory, and how this memory is then used to produce an output for that step. The constant matrices `U`, `V`, and `W` across time steps implicitly demonstrate the concept of parameter sharing, a hallmark of RNNs.

**Key Insights:**
**Main Takeaways and Insights:**

1.  **Sequential Processing:** The `for i ← 1 to LENGTH(x) do` loop explicitly demonstrates that Recurrent Neural Networks (RNNs) process input data sequentially, one element `xᵢ` at a time. This highlights their suitability for tasks involving sequences.
2.  **Hidden State as Internal Memory:** The equation `hᵢ ← g(Uhᵢ₋₁ + Wxᵢ)` is crucial. It shows that the current hidden state `hᵢ` is computed not only from the current input `xᵢ` but also from the *previous* hidden state `hᵢ₋₁`. This mechanism allows RNNs to maintain an internal 'memory' of past information within the sequence, making them effective for tasks requiring context and temporal dependencies. The `Uhᵢ₋₁` term is direct evidence of this recurrent connection.
3.  **Weight Sharing Across Time:** Although not explicitly indexed with `i`, the matrices `U`, `W`, and `V` are used consistently within the loop across all time steps. This implicitly indicates that the same set of weights (parameters) is applied at each time step, which is a fundamental property of RNNs, reducing the number of parameters and enabling the network to generalize across different positions in a sequence. The surrounding text explicitly confirms: "The matrices U, v and W are shared across time."
4.  **Output Generation at Each Step:** The line `yᵢ ← f(Vhᵢ)` reveals that an output `yᵢ` is generated at each time step `i`, based on the current hidden state `hᵢ`. This illustrates how the network produces a sequence of outputs corresponding to the input sequence.
5.  **Activation Functions:** The use of `g` and `f` implies non-linear transformations applied to the weighted sums, which are essential for the network to learn complex patterns and representations.

**Conclusion:** This pseudocode provides a clear, foundational understanding of the forward pass in simple RNNs, emphasizing their ability to handle sequential data by maintaining a hidden state that acts as a memory and sharing parameters across time, leading to context-aware output generation.

**Document Context:**
This image directly supports the document's section '8.1.1 Inference in RNNs' and specifically illustrates 'Figure 8.3 Forward inference in a simple recurrent network'. It provides the exact pseudocode implementation of the concept discussed, showing how an RNN processes sequential input to produce sequential output. It formally defines the mathematical operations and control flow for calculating hidden states and outputs at each time step. By presenting the algorithm in this detailed manner, the image enhances comprehension of how the theoretical concept of RNN inference is practically realized through a series of iterative computations involving weight matrices and activation functions. The textual explanation after the image further confirms the significance of the matrices `U`, `V`, and `W` being shared across time, which is evident in the pseudocode's structure.

**Summary:**
The `FORWARDRNN` function describes the forward inference process for a simple Recurrent Neural Network (RNN). It takes an input sequence `x` and the network's parameters (`network`) and returns an output sequence `y`. The process begins by initializing the hidden state `h₀` to zero, signifying no prior memory before processing the first input. The algorithm then enters a loop that iterates through each element of the input sequence, from the first element (`i=1`) up to the total length of the input sequence (`LENGTH(x)`). In each iteration `i`, two main calculations occur:

1.  **Hidden State Calculation:** The current hidden state `hᵢ` is computed. This involves applying an activation function `g` to the sum of two weighted components: the previous hidden state `hᵢ₋₁` multiplied by a weight matrix `U`, and the current input element `xᵢ` multiplied by a weight matrix `W`. This step highlights the recurrent nature of the network, where the 'memory' from past inputs (`hᵢ₋₁`) is combined with the current input (`xᵢ`) to update the network's internal state.
2.  **Output Calculation:** Immediately after `hᵢ` is calculated, the current output `yᵢ` for that time step is computed. This is done by applying another activation function `f` to the current hidden state `hᵢ` multiplied by a weight matrix `V`. This shows how the internal hidden state is transformed into an observable output.

This iterative process continues until all elements of the input sequence `x` have been processed. Finally, the function returns the complete sequence of outputs `y` that was generated across all time steps. The weight matrices `U`, `V`, and `W` are critical parameters of the network that are shared and remain constant across all time steps, as indicated by their lack of an index `i` and further clarified by the document's context.](images/7ab599cf4969be254b4e9c94f9e34b9930f68033d2d39fde7cd775aec23981a1.jpg)
Figure 8.3 Forward inference in a simple recurrent network. The matrices U, $\pmb { v }$ and $\boldsymbol { \mathsf { W } }$ are shared across time, while new values for $\mathbf { h }$ and $\pmb { y }$ are calculated with each time step.

# 8.1.2 Training

As with feedforward networks, we’ll use a training set, a loss function, and backpropagation to obtain the gradients needed to adjust the weights in these recurrent networks. As shown in Fig. 8.2, we now have 3 sets of weights to update: $\boldsymbol { \mathsf { W } }$ , the weights from the input layer to the hidden layer, U, the weights from the previous hidden layer to the current hidden layer, and finally $\pmb { v }$ , the weights from the hidden layer to the output layer.

![## Image Analysis: 3a6915276609ce61b08b7a59543c6fa1fb8135325343035a25ceca9a79873c7a.jpg

**Conceptual Understanding:**
This image conceptually represents a "Recurrent Neural Network (RNN) unrolled in time." Its main purpose is to visually explain how an RNN processes sequential data by showing the flow of information across different time steps. It highlights the core mechanism of recurrence, where the output or hidden state from a previous step feeds into the current step, and the crucial concept of "shared weights" that are applied consistently across all time steps. The image effectively deconstructs the temporal dynamics of an RNN into a series of interconnected computational graphs.

**Content Interpretation:**
The image illustrates the forward pass computation of an RNN, step by step, for sequential data. Each vertical slice (e.g., `x₁`, `h₁`, `y₁`) represents a single time step's computation.

**Concepts Shown:**
*   **Input (`x_t`):** `x₁`, `x₂`, `x₃` represent the inputs fed into the network at time steps 1, 2, and 3, respectively. These are external data points for the network.
*   **Hidden State (`h_t`):** `h₀`, `h₁`, `h₂`, `h₃` represent the network's internal memory or hidden state. `h_t` is a function of the current input `x_t` and the previous hidden state `h_{t-1}`. `h₀` is the initial hidden state.
*   **Output (`y_t`):** `y₁`, `y₂`, `y₃` represent the outputs generated by the network at each corresponding time step. The output `y_t` is a function of the current hidden state `h_t`.
*   **Recurrence:** The diagonal connections from `h₀` to `h₁`, `h₁` to `h₂`, and `h₂` to `h₃`, each passing through the `U` trapezoid, visually demonstrate the recurrent nature. The hidden state from the previous time step is passed to the next, allowing the network to maintain memory of past information.
*   **Shared Weights (`U`, `W`, `V`):** The repeated appearance of the labels `U`, `W`, and `V` within trapezoidal shapes at each time step signifies that these are shared weight matrices (or tensors) that transform the inputs and hidden states. `W` transforms the input `x_t` to contribute to `h_t`. `U` transforms the previous hidden state `h_{t-1}` to contribute to `h_t`. `V` transforms `h_t` to produce the output `y_t`.

**Significance:** This unrolled view clarifies that while the *state* changes at each time step (from `h₀` to `h₁` to `h₂` etc.), the *operations* performed (defined by `U`, `W`, `V`) remain constant. This efficiency allows RNNs to handle sequences of arbitrary length without requiring a new set of parameters for each position in the sequence.

**Supporting Evidence:** The explicit labels `x₁`, `x₂`, `x₃` for inputs; `h₀`, `h₁`, `h₂`, `h₃` for hidden states; and `y₁`, `y₂`, `y₃` for outputs directly identify these components. The trapezoids labeled `U`, `W`, `V` connecting these components clearly indicate the shared transformation parameters. The left-to-right progression and repeated structure across time steps visually confirm the unrolling over time.

**Key Insights:**
**Main Takeaway:** The primary lesson from this image is that Recurrent Neural Networks process sequential data by iteratively updating an internal "hidden state" (`h_t`) based on the current input (`x_t`) and the hidden state from the previous time step (`h_{t-1}`). This allows them to model dependencies over time.
*   **Evidence:** The connections explicitly show `h_t` being derived from `x_t` (via `W`) and `h_{t-1}` (via `U`). For instance, `h₁` is derived from `x₁` and `h₀`.

**Key Insight: Parameter Sharing:** A fundamental characteristic of RNNs is that they use the *same set of weights* (`U`, `V`, `W`) across all time steps. This parameter sharing is what makes them "recurrent" and enables them to learn general patterns in sequences, rather than learning specific patterns for each position.
*   **Evidence:** The labels `U`, `W`, `V` are consistently applied to the corresponding transformation trapezoids at each time step (`t=1, 2, 3`), visually demonstrating that these weights are shared and not unique to each step.

**Process Flow Insight:** The network produces an output (`y_t`) at each time step based on its current hidden state (`h_t`).
*   **Evidence:** At each time step (1, 2, and 3), there is a distinct `y_t` node directly connected to and derived from `h_t` via the `V` weight matrix.

**Document Context:**
This image, Figure 8.4, is highly relevant to the "8.1.2 Training" section of the document, specifically in the context of Recurrent Neural Networks. The accompanying text explicitly states: "Figure 8.4 A simple recurrent neural network shown unrolled in time. Network layers are recalculated for each time step, while the weights U, $\pmb { v }$ and $\boldsymbol { \mathsf { W } }$ are shared across all time steps." The diagram visually deconstructs the abstract concept of recurrence into a concrete, sequential computational graph, which is essential for understanding how such a network is trained. Training an RNN involves backpropagating errors through this unrolled structure, a process often referred to as Backpropagation Through Time (BPTT). By showing the shared weights and the flow of information, the image provides the foundational visual understanding necessary to grasp the mechanics of RNN training and how gradients would flow back through these connections to update `U`, `V`, and `W`.

**Summary:**
This diagram illustrates a "Recurrent Neural Network (RNN) unrolled in time," providing a clear, step-by-step view of how this type of neural network processes sequential information. Imagine a sequence of events or data points unfolding over time; this diagram shows how the network handles each point in that sequence.

The process begins with an **initial hidden state, `h₀`**, which represents the network's memory before any input is received. As time progresses, the network processes an input at each subsequent "time step" and updates its internal memory (the hidden state) while also producing an output.

Let's break down a typical time step, for example, **Time Step 1**:
1.  **Input:** An input, `x₁`, is introduced to the network.
2.  **Hidden State Update:** To compute the **current hidden state, `h₁`**, the network combines two pieces of information:
    *   The **previous hidden state, `h₀`**, which is passed forward and transformed using a set of weights labeled `U`.
    *   The **current input, `x₁`**, which is transformed using another set of weights labeled `W`.
    *   These two transformed pieces of information are combined to create the new hidden state `h₁`. This `h₁` now encapsulates information from both the current input `x₁` and all preceding inputs (summarized in `h₀`).
3.  **Output Generation:** From this updated hidden state `h₁`, the network produces an **output, `y₁`**, by applying a third set of weights labeled `V`. This `y₁` is the network's prediction or result for Time Step 1.
4.  **Recurrence:** The newly computed hidden state, `h₁`, is then passed forward to become the "previous hidden state" for the *next* time step, allowing the network to maintain a memory of past information.

This exact process is repeated for **Time Step 2** (with input `x₂`, previous hidden state `h₁` leading to `h₂` and output `y₂`), and **Time Step 3** (with input `x₃`, previous hidden state `h₂` leading to `h₃` and output `y₃`).

A critical feature highlighted by this diagram is **parameter sharing**: the same weight matrices `U`, `W`, and `V` are used at every single time step. This means the network applies the same learned transformations at each point in the sequence, regardless of whether it's processing `x₁`, `x₂`, or `x₃`. This sharing of weights is what makes the network "recurrent" and allows it to efficiently learn and generalize patterns across variable-length sequences, making it suitable for tasks like speech recognition, machine translation, or time series prediction. The horizontal arrow at the bottom visually emphasizes this progression "unrolled in time."](images/3a6915276609ce61b08b7a59543c6fa1fb8135325343035a25ceca9a79873c7a.jpg)
Figure 8.4 A simple recurrent neural network shown unrolled in time. Network layers are recalculated for each time step, while the weights U, $\pmb { v }$ and $\boldsymbol { \mathsf { W } }$ are shared across all time steps.

Fig. 8.4 highlights two considerations that we didn’t have to worry about with backpropagation in feedforward networks. First, to compute the loss function for the output at time $t$ we need the hidden layer from time $t - 1$ . Second, the hidden layer at time $t$ influences both the output at time $t$ and the hidden layer at time $t + 1$ (and hence the output and loss at $t + 1$ ). It follows from this that to assess the error accruing to $\mathbf { h } _ { t }$ , we’ll need to know its influence on both the current output as well as the ones that follow.

Tailoring the backpropagation algorithm to this situation leads to a two-pass algorithm for training the weights in RNNs. In the first pass, we perform forward inference, computing $\mathbf { h } _ { t }$ , $\pmb { y } _ { t }$ , accumulating the loss at each step in time, saving the value of the hidden layer at each step for use at the next time step. In the second phase, we process the sequence in reverse, computing the required gradients as we go, computing and saving the error term for use in the hidden layer for each step backward in time. This general approach is commonly referred to as backpropagation through time (Werbos 1974, Rumelhart et al. 1986, Werbos 1990).

Fortunately, with modern computational frameworks and adequate computing resources, there is no need for a specialized approach to training RNNs. As illustrated in Fig. 8.4, explicitly unrolling a recurrent network into a feedforward computational graph eliminates any explicit recurrences, allowing the network weights to be trained directly. In such an approach, we provide a template that specifies the basic structure of the network, including all the necessary parameters for the input, output, and hidden layers, the weight matrices, as well as the activation and output functions to be used. Then, when presented with a specific input sequence, we can generate an unrolled feedforward network specific to that input, and use that graph to perform forward inference or training via ordinary backpropagation.

For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item.

# 8.2 RNNs as Language Models

Let’s see how to apply RNNs to the language modeling task. Recall from Chapter 3 that language models predict the next word in a sequence given some preceding context. For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “fish” we would compute:

$$
P ( \mathit { f i s h } | T h a n k s f o r a l l t h e )
$$

Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. We can also assign probabilities to entire sequences by combining these conditional probabilities with the chain rule:

$$
P ( w _ { 1 : n } ) \ = \ \prod _ { i = 1 } ^ { n } P ( w _ { i } | w _ { < i } )
$$

The n-gram language models of Chapter 3 compute the probability of a word given counts of its occurrence with the $n - 1$ prior words. The context is thus of size $n - 1$ . For the feedforward language models of Chapter 7, the context is the window size.

RNN language models (Mikolov et al., 2010) process the input sequence one word at a time, attempting to predict the next word from the current word and the previous hidden state. RNNs thus don’t have the limited context problem that n-gram models have, or the fixed context that feedforward language models have, since the hidden state can in principle represent information about all of the preceding words all the way back to the beginning of the sequence. Fig. 8.5 sketches this difference between a FFN language model and an RNN language model, showing that the RNN language model uses $h _ { t - 1 }$ , the hidden state from the previous time step, as a representation of the past context.

# 8.2.1 Forward Inference in an RNN language model

Forward inference in a recurrent language model proceeds exactly as described in Section 8.1.1. The input sequence ${ \bf x } = [ { \bf x } _ { 1 } ; . . . ; { \bf x } _ { t } ; . . . ; { \bf x } _ { N } ]$ consists of a series of words each represented as a one-hot vector of size $| V | \times 1$ , and the output prediction, $\pmb { y }$ , is a vector representing a probability distribution over the vocabulary. At each step, the model uses the word embedding matrix $\mathsf { E }$ to retrieve the embedding for the current word, multiples it by the weight matrix $\boldsymbol { \mathsf { W } }$ , and then adds it to the hidden layer from the previous step (weighted by weight matrix $\mathbf { U }$ ) to compute a new hidden layer. This hidden layer is then used to generate an output layer which is passed through a softmax layer to generate a probability distribution over the entire vocabulary. That is, at time $t$ :

$$
\begin{array} { r c l } { \mathbf { e } _ { t } = \mathbf { E x } _ { t } } \\ { \mathbf { h } _ { t } = g ( \mathbf { U } \mathbf { h } _ { t - 1 } + \mathbf { W } \mathbf { e } _ { t } ) } \\ { \hat { \mathbf { y } } _ { t } = \operatorname * { s o f t m a x } ( \mathbf { V } \mathbf { h } _ { t } ) } \end{array}
$$

![## Image Analysis: 6672b2ef087dbe47c6b6e0789028f633f7800609f81367cc85f968ee6221d94c.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates two fundamental architectures for language modeling: (a) a feedforward neural language model and (b) a recurrent neural network (RNN) language model. Its main purpose is to highlight the key difference in how these models incorporate and utilize past contextual information to predict the next token in a sequence.

Specifically, architecture (a) conveys the idea of a 'fixed context window,' where the model considers a predefined number of preceding tokens as direct input. In contrast, architecture (b) demonstrates the concept of 'recurrent processing' and 'hidden state memory,' where the model maintains a summary of all prior context in a hidden state that is updated at each time step. The image visually explains how the hidden state in an RNN acts as a compact representation of accumulated information, enabling it to handle longer dependencies than a simple feedforward model with a fixed input window.

**Content Interpretation:**
The image illustrates two distinct neural network architectures used for language modeling: (a) a feedforward neural language model and (b) a recurrent neural network (RNN) language model. Both models aim to predict the next token in a sequence, represented by `y_hat_t`, based on previous tokens.

**Architecture (a) - Feedforward Neural Language Model:**
This architecture shows a fixed context input where three token embeddings (`e_t-2`, `e_t-1`, `e_t`) are combined and processed by a single weight matrix `W`. The output of `W` forms a hidden state `h_t`, which is then transformed by another weight matrix `U` to produce the predicted output `y_hat_t`. This configuration implies that the model's prediction at time `t` is based solely on a limited, predefined window of the most recent past tokens.

**Architecture (b) - RNN Language Model:**
This architecture depicts a sequential processing flow, characteristic of RNNs. Each token embedding (`e_t-2`, `e_t-1`, `e_t`) is processed individually by a weight matrix `W`. Critically, the hidden state from the previous time step (`h_t-2`, then `h_t-1`) is passed through a weight matrix `U` and fed into the calculation of the current hidden state. Specifically, `h_t-2` and `h_t-1` (after being processed by `U`) contribute to the subsequent states. The final hidden state `h_t` (derived from `e_t` and the summarized prior context) is then transformed by a weight matrix `V` to produce the predicted output `y_hat_t`. This structure demonstrates how RNNs maintain a 'memory' of the entire past sequence through their hidden states, allowing for the summarization of prior context.

The significance of these representations lies in showing the fundamental difference in how context is handled: fixed and explicit in feedforward models versus dynamic and summarized in hidden states for RNNs.

**Key Insights:**
The main takeaways from this image are the distinct mechanisms for handling context in different language model architectures:

1.  **Fixed Context in Feedforward Models:** As shown in (a), feedforward neural language models process a fixed, limited window of past tokens (e.g., `e_t-2, e_t-1, e_t`) as a single concatenated input to a weight matrix (`W`). This means the model's 'memory' is strictly limited to this predefined window. The extracted text `e_t-2`, `e_t-1`, `e_t` feeding into `W` and producing `h_t` which then goes through `U` to `y_hat_t` explicitly demonstrates this.

2.  **Summarized Prior Context in RNNs:** As shown in (b), RNN language models process tokens sequentially, and a hidden state (`h_t-2`, `h_t-1`, `h_t`) acts as a summary of all preceding context. This hidden state is continuously updated and passed through `U` to influence the next step. The explicit flow from `e_t-2` to `h_t-2`, then `h_t-2` through `U` influencing subsequent states, and similarly for `e_t-1` to `h_t-1` and `h_t-1` through `U` leading to `h_t` and finally `y_hat_t` through `V`, highlights that the current prediction (`y_hat_t`) incorporates information from an extended, potentially indefinite past through the hidden state's cumulative memory.

3.  **Architectural Differences in Weight Matrices:** The image shows different weight matrices for the hidden state transformation (`U` in (a) vs. `V` in (b)) and how inputs are handled (`W` processes combined embeddings in (a) vs. `W` processes individual embeddings in (b), with `U` acting on previous hidden states). This illustrates the distinct computational graphs and parameter sharing (or lack thereof across time steps in the simple feedforward case) between the two model types.

**Document Context:**
This image directly supports Section 8.2.1, 'Forward Inference in an RNN language model,' by visually differentiating a feedforward neural language model from an RNN language model in terms of how they handle context over time. The document context explicitly labels (a) as a feedforward model with a fixed context input to `W` and (b) as an RNN language model where the hidden state `h_t-1` summarizes the prior context.

The image serves to graphically illustrate the conceptual difference described in the surrounding text, allowing readers to visualize the data flow and the role of the hidden state in RNNs compared to the concatenated input in feedforward models. It clarifies how a 'schematic context of three tokens' is managed differently in each architecture, which is crucial for understanding the advantages of RNNs in processing sequential data like language.

**Summary:**
The image displays two simplified language model architectures, labeled (a) and (b), illustrating how they process a sequence of tokens. Both diagrams show inputs representing token embeddings at different time steps and their transformation through weight matrices to produce a predicted output.

Architecture (a), a feedforward neural language model, processes a fixed window of past token embeddings simultaneously. The embeddings `e_t-2`, `e_t-1`, and `e_t` are fed into a single weight matrix `W`. The output of this layer forms the hidden state `h_t`. This hidden state `h_t` is then processed by another weight matrix `U` to produce the predicted output `y_hat_t`. This architecture demonstrates that the context for predicting `y_hat_t` is explicitly defined by a fixed number of preceding tokens (`e_t-2`, `e_t-1`, `e_t`).

Architecture (b), an RNN language model, processes tokens sequentially, incorporating a summarized prior context. At each time step, the current token embedding `e_t` is combined with the previous hidden state `h_t-1` to compute the current hidden state `h_t`. Specifically, `e_t-2` is processed by `W` to yield `h_t-2`. This `h_t-2` is then passed through `U` before being implicitly combined with the next step. Similarly, `e_t-1` is processed by `W` to yield `h_t-1`, and this `h_t-1` is passed through `U` before contributing to the next state. Finally, `e_t` is processed by `W` to yield `h_t`. This `h_t` is then processed by the weight matrix `V` to produce the predicted output `y_hat_t`. The key difference here is the presence of `h_t-2` and `h_t-1` influencing subsequent hidden states via `U`, indicating that information from previous steps is carried forward and summarized in the hidden state, rather than being explicitly present as separate inputs at each prediction step.](images/6672b2ef087dbe47c6b6e0789028f633f7800609f81367cc85f968ee6221d94c.jpg)
Figure 8.5 Simplified sketch of two LM architectures moving through a text, showing a schematic context of three tokens: (a) a feedforward neural language model which has a fixed context input to the weight matrix $\boldsymbol { \mathsf { W } }$ , (b) an RNN language model, in which the hidden state $\mathbf { h } _ { t - 1 }$ summarizes the prior context.

When we do language modeling with RNNs (and we’ll see this again in Chapter 9 with transformers), it’s convenient to make the assumption that the embedding dimension $d _ { e }$ and the hidden dimension $d _ { h }$ are the same. So we’ll just call both of these the model dimension $d$ . So the embedding matrix E is of shape $[ d \times | V | ]$ , and $\mathbf { x _ { t } }$ is a one-hot vector of shape $[ | V | \times 1 ]$ . The product $\mathbf { e } _ { t }$ is thus of shape $[ d \times 1 ]$ . W and U are of shape $[ d \times d ]$ , so $\mathbf { h } _ { t }$ is also of shape $[ d \times 1 ]$ . $\pmb { v }$ is of shape $[ | V | \times d ]$ , so the result of $\pmb { \nu _ { \mathrm { h } } }$ is a vector of shape $[ | V | \times 1 ]$ . This vector can be thought of as a set of scores over the vocabulary given the evidence provided in h. Passing these scores through the softmax normalizes the scores into a probability distribution. The probability that a particular word $k$ in the vocabulary is the next word is represented by $\hat { \mathbf { y } } _ { t } [ k ]$ , the kth component of $\hat { \mathbf { y } } _ { t }$ :

$$
P ( w _ { t + 1 } = k | w _ { 1 } , \dots , w _ { t } ) \ = \ \hat { \bf y } _ { t } [ k ]
$$

The probability of an entire sequence is just the product of the probabilities of each item in the sequence, where we’ll use $\hat { \mathsf { y } } _ { i } [ { \boldsymbol w } _ { i } ]$ to mean the probability of the true word $w _ { i }$ at time step $i$ .

$$
\begin{array} { l } { P ( w _ { 1 : n } ) = \displaystyle \prod _ { i = 1 } ^ { n } P ( w _ { i } | w _ { 1 : i - 1 } ) } \\ { = \displaystyle \prod _ { i = 1 } ^ { n } \hat { \mathbf { y } } _ { i } [ w _ { i } ] } \end{array}
$$

# 8.2.2 Training an RNN language model

self-supervision

To train an RNN as a language model, we use the same self-supervision (or selftraining) algorithm we saw in Section 7.7: we take a corpus of text as training material and at each time step $t$ ask the model to predict the next word. We call such a model self-supervised because we don’t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function. Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the

![## Image Analysis: 72f2de779695437e1b5001636922aca38433ece1e5f9c494c980a33b44e0f9d9.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and forward pass of a Recurrent Neural Network (RNN) when it is being trained as a language model. The main purpose is to illustrate how an RNN processes a sequence of words one at a time, uses its internal "memory" (hidden state) to predict the next word in the sequence, and calculates the error (loss) for each prediction. The key idea being communicated is the sequential nature of RNN processing, where information from previous steps (via the hidden state 'h') influences predictions at current and future steps, and how the model's performance is measured using cross-entropy loss against the true next words.

**Content Interpretation:**
The image demonstrates the **system of training an RNN for language modeling**, where the goal is to predict the next word given a sequence of preceding words.

*   **Processes Shown:**
    *   **Input Word to Embedding:** Each input word (e.g., "So", "long", "and") is first converted into a numerical **Input Embedding** ("e"). This is a common practice in NLP to represent words in a dense vector space.
    *   **Recurrent Processing:** The core process is the **RNN** layer. Each rectangular unit represents an RNN cell at a specific time step. It takes the current input embedding ("e") and the **hidden state ("h")** from the previous time step. The arrow labeled "h" signifies the passing of this recurrent information, which allows the network to maintain context and "memory" across the sequence.
    *   **Prediction Generation:** The output of each RNN unit's hidden state ("h") is transformed by "Vh" (likely a weight matrix `V` multiplying `h`) and then passed to the **Softmax over Vocabulary** layer. This layer converts the numerical output into a probability distribution over all possible words in the vocabulary, indicating the model's prediction for the *next word* (visualized as histogram-like shapes).
    *   **Loss Calculation:** For each time step, the model's predicted probability distribution (ŷ) for the target *Next word* ("y", e.g., "long" after "So") is used to calculate the **Loss**. The formula "- log ŷ_word" explicitly indicates a negative log-likelihood loss, which is characteristic of **Cross-Entropy Loss (L_CE)**.

*   **Concepts Shown:**
    *   **Time Steps:** The horizontal arrangement of RNN units, input words, predictions, and losses demonstrates the concept of time steps in sequence processing. Each column represents a step `t` in the sequence.
    *   **Recurrence/Memory:** The "h" arrow connecting RNN units explicitly shows the recurrence, where the hidden state carries information forward in time, allowing the model to learn long-range dependencies.
    *   **Probabilistic Prediction:** The "Softmax over Vocabulary" layer and the histogram shapes signify that the model outputs a probability distribution over possible next words.
    *   **Supervised Learning:** The presence of a "Next word" label for each prediction and the calculation of "Loss" based on these true labels ("y") indicates a supervised learning setup for training the language model.
    *   **Overall Objective:** The final formula "1/T Σ_{t=1}^{T} L_{CE}" represents the average Cross-Entropy Loss over all `T` time steps in the sequence, which is the objective function to be minimized during training.

*   **Supporting Evidence from Text Elements:**
    *   "So", "long", "and", "thanks", "for" (bottom row): These are the input words processed sequentially.
    *   "long", "and", "thanks", "for", "all" (top row, "Next word"): These are the true target words the model is trying to predict at each step.
    *   "Input Embeddings e": Indicates the numerical representation of input words.
    *   "RNN": Explicitly labels the core recurrent neural network layer.
    *   "h" (horizontal arrow): Explicitly shows the hidden state passed between RNN units, representing memory.
    *   "Vh": Shows a transformation applied to the hidden state before prediction.
    *   "Softmax over Vocabulary": Denotes the output layer that produces a probability distribution over all words.
    *   "Loss - log ŷ_word": Clearly indicates the type of loss being calculated for each prediction. "ŷ" represents the predicted probability, and "y" represents the true target.
    *   "1/T Σ_{t=1}^{T} L_{CE}": This final formula concretely defines the overall training objective as minimizing the average Cross-Entropy Loss (L_CE) across all time steps (T).

**Key Insights:**
This image provides several key takeaways regarding RNN-based language modeling:

*   **RNNs process sequential data by maintaining a hidden state (memory) over time.**
    *   **Evidence:** The horizontal arrow labeled "h" connecting the RNN units clearly shows the hidden state being passed from one time step to the next, illustrating the core recurrent mechanism.
*   **A language model's primary task is to predict the next word in a sequence.**
    *   **Evidence:** The "Input words" at the bottom and the corresponding "Next word" labels at the top explicitly demonstrate this task. For instance, given "So", the model predicts "long"; given "long", it predicts "and"; and so on.
*   **Word embeddings are crucial for representing words numerically as input to neural networks.**
    *   **Evidence:** The "Input Embeddings e" layer explicitly shows how words like "So", "long", etc., are converted into vector representations before being fed into the RNN.
*   **The Softmax function is used in language models to output a probability distribution over the entire vocabulary for the next word.**
    *   **Evidence:** The "Softmax over Vocabulary" layer and the histogram-like outputs confirm that the model's output is a probabilistic prediction across all possible words.
*   **Training a language model involves minimizing a Cross-Entropy Loss, which measures the discrepancy between the predicted word distribution and the true next word.**
    *   **Evidence:** The "Loss - log ŷ_word" boxes for each time step, coupled with the "y" (true next word) label and the overall "1/T Σ_{t=1}^{T} L_{CE}" formula, directly show that cross-entropy is the chosen loss function and the training objective is to minimize this average loss. This indicates that the model learns by trying to maximize the probability assigned to the correct next word.
*   **RNNs are inherently designed to handle variable-length sequences, processing them step-by-step.**
    *   **Evidence:** The "..." at the end of all layers (Next word, Loss, RNN, Input Embeddings) signifies that the sequence can continue indefinitely, which is a characteristic strength of RNNs.

**Document Context:**
This image is presented in "Section: 8.2.2 Training an RNN language model" and is explicitly referenced as "Figure 8.6 Training RNNs as language models." It serves as a visual explanation of the fundamental computational graph and data flow involved when training a Recurrent Neural Network for the specific task of language modeling. It directly illustrates the theoretical concepts discussed in the surrounding text, showing how an RNN takes a sequence of words, processes them over time while maintaining context, makes predictions, and calculates the error for learning. It is a critical diagram for understanding the practical implementation and objective of RNN-based language models.

**Summary:**
This diagram illustrates how a Recurrent Neural Network (RNN) is trained to act as a language model, meaning it learns to predict the next word in a sequence. Imagine the network processing a sentence word by word, from left to right.

At the very bottom, we start with the "Input words" of a sentence, such as "So", "long", "and", "thanks", "for". Each of these words is first converted into a numerical representation called an "Input Embedding" (labeled "e" and shown as a stack of colored circles). These embeddings are then fed into the core "RNN" layer, which consists of several rectangular units, each representing a step in processing a word.

The magic of the RNN lies in its ability to remember context. As each RNN unit processes its current word's embedding, it also receives a "hidden state" (labeled "h") from the previous RNN unit. This "h" acts like the network's memory, carrying information about all the words processed so far. This allows the RNN to understand the sequence and make informed predictions.

After an RNN unit processes its input, its hidden state ("h") is transformed (labeled "Vh") and then passed up to the "Softmax over Vocabulary" layer. This layer takes the RNN's internal representation and converts it into a probability distribution over *all possible words* in its vocabulary (shown as histogram-like shapes). This distribution tells us how likely the model thinks each word is to be the *next word* in the sequence.

Finally, at the top, we see the "Loss" calculation. For each input word (e.g., "So"), the model makes a prediction for the "Next word" (e.g., "long"). The diagram labels the true next word as "y". The loss is calculated using the formula "- log ŷ_word", which is a common way to measure how far off the model's prediction (ŷ, the predicted probability for the correct word) was from the actual true word. For example, after "So", the true next word is "long", and the loss is calculated as "- log ŷ_long". This process repeats for every word in the sequence.

The overall goal during training, shown by the formula "1/T Σ_{t=1}^{T} L_{CE}" on the far right, is to minimize the average of these individual Cross-Entropy Losses (L_CE) across all 'T' time steps in the entire sequence. By minimizing this loss, the RNN learns to make accurate predictions for the next word, effectively becoming a functional language model. The "..." symbols indicate that this process extends for sequences of any length.](images/72f2de779695437e1b5001636922aca38433ece1e5f9c494c980a33b44e0f9d9.jpg)
Figure 8.6 Training RNNs as language models.

correct distribution.

$$
L _ { C E } \ = \ - \sum _ { w \in V } \mathbf { y } _ { t } [ w ] \log \hat { \mathbf { y } } _ { t } [ w ]
$$

In the case of language modeling, the correct distribution ${ \bf y } _ { t }$ comes from knowing the next word. This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0. Thus, the cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word. So at time $t$ the CE loss is the negative log probability the model assigns to the next word in the training sequence.

$$
L _ { C E } ( \hat { \mathbf { y } } _ { t } , \mathbf { y } _ { t } ) ~ = ~ - \log \hat { \mathbf { y } } _ { t } [ w _ { t + 1 } ]
$$

Thus at each word position $t$ of the input, the model takes as input the correct word $w _ { t }$ together with $h _ { t - 1 }$ , encoding information from the preceding $w _ { 1 : t - 1 }$ , and uses them to compute a probability distribution over possible next words so as to compute the model’s loss for the next token $w _ { t + 1 }$ . Then we move to the next word, we ignore what the model predicted for the next word and instead use the correct word $w _ { t + 1 }$ along with the prior history encoded to estimate the probability of token $w _ { t + 2 }$ . This idea that we always give the model the correct history sequence to predict the next word (rather than feeding the model its best case from the previous time step) is called teacher forcing.

The weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent. Fig. 8.6 illustrates this training regimen.

# 8.2.3 Weight Tying

Careful readers may have noticed that the input embedding matrix E and the final layer matrix $\pmb { v }$ , which feeds the output softmax, are quite similar.

The columns of E represent the word embeddings for each word in the vocabulary learned during the training process with the goal that words that have similar meaning and function will have similar embeddings. And, since when we use RNNs for language modeling we make the assumption that the embedding dimension and the hidden dimension are the same $\stackrel {  } { = }$ the model dimension $d$ ), the embedding matrix E has shape $[ d \times | V | ]$ . And the final layer matrix $\pmb { v }$ provides a way to score the likelihood of each word in the vocabulary given the evidence present in the final hidden layer of the network through the calculation of $\pmb { \nu _ { \mathrm { h } } }$ . $\pmb { v }$ is of shape $[ | V | \times d ]$ That is, is, the rows of $\pmb { v }$ are shaped like a transpose of E, meaning that $\pmb { v }$ provides a second set of learned word embeddings.

Instead of having two sets of embedding matrices, language models use a single embedding matrix, which appears at both the input and softmax layers. That is, we dispense with $\pmb { v }$ and use $\mathsf { E }$ at the start of the computation and E| (because the shape of $\pmb { v }$ is the transpose of $\mathsf { E }$ at the end. Using the same matrix (transposed) in two places is called weight tying.1 The weight-tied equations for an RNN language model then become:

$$
\begin{array} { r l } { \mathbf { e } _ { t } \ = \ \mathbf { E x } _ { t } } & { } \\ { \mathbf { h } _ { t } \ = \ g ( \mathbf { U } \mathbf { h } _ { t - 1 } + \mathbf { W } \mathbf { e } _ { t } ) } & { } \\ { \hat { \mathbf { y } } _ { t } \ = \ \mathrm { s o f t m a x } ( \mathbf { E } ^ { \top } \mathbf { h } _ { t } ) } \end{array}
$$

In addition to providing improved model perplexity, this approach significantly reduces the number of parameters required for the model.

# 8.3 RNNs for other NLP tasks

Now that we’ve seen the basic RNN architecture, let’s consider how to apply it to three types of NLP tasks: sequence classification tasks like sentiment analysis and topic classification, sequence labeling tasks like part-of-speech tagging, and text generation tasks, including with a new architecture called the encoder-decoder.

# 8.3.1 Sequence Labeling

In sequence labeling, the network’s task is to assign a label chosen from a small fixed set of labels to each element of a sequence. One classic sequence labeling tasks is part-of-speech (POS) tagging (assigning grammatical tags like NOUN and VERB to each word in a sentence). We’ll discuss part-of-speech tagging in detail in Chapter 17, but let’s give a motivating example here. In an RNN approach to sequence labeling, inputs are word embeddings and the outputs are tag probabilities generated by a softmax layer over the given tagset, as illustrated in Fig. 8.7.

In this figure, the inputs at each time step are pretrained word embeddings corresponding to the input tokens. The RNN block is an abstraction that represents an unrolled simple recurrent network consisting of an input layer, hidden layer, and output layer at each time step, as well as the shared U, $\pmb { v }$ and W weight matrices that comprise the network. The outputs of the network at each time step represent the distribution over the POS tagset generated by a softmax layer.

To generate a sequence of tags for a given input, we run forward inference over the input sequence and select the most likely tag from the softmax at each step. Since we’re using a softmax layer to generate the probability distribution over the output tagset at each time step, we will again employ the cross-entropy loss during training.

![## Image Analysis: a73ea3d3530d63492de4105f644739cde8ba1353b7dd6b280cb655b85482c6a8.jpg

**Conceptual Understanding:**
This image conceptually represents the natural language processing (NLP) task of Part-of-Speech (POS) tagging, specifically implemented using a Recurrent Neural Network (RNN). The main purpose is to illustrate the architecture and sequential data flow of an RNN model for assigning grammatical categories (POS tags) to each word in an input sentence. It demonstrates how an RNN processes a sequence of words, transforms them into embeddings, uses recurrent connections to maintain context, and then outputs a probability distribution over possible tags for each word, ultimately predicting the most likely tag. Key ideas include sequence labeling, the use of word embeddings, the sequential and contextual processing of an RNN via hidden states, and the role of Softmax and Argmax layers for probabilistic classification and final tag selection.

**Content Interpretation:**
The image clearly depicts the system and process of part-of-speech tagging using an RNN model. Each component plays a specific role in transforming raw text into grammatically tagged output. The 'Words' layer ("Janet", "will", "back", "the", "bill") represents the input sentence. The 'Embeddings' layer ("Embeddings", "e") shows the conversion of each word into a numerical vector, which is crucial for neural network input. The 'RNN Layer(s)' (labeled "RNN Layer(s)" and containing internal units with 'h' for hidden state and 'Vh' for output) is the core processing unit, demonstrating sequential processing and context propagation. The 'Softmax over tags' layer generates probability distributions over possible POS tags for each word, visualized as bar charts. The 'Argmax' layer (labeled "Argmax" and outputting 'y') performs the final decision-making by selecting the tag with the highest probability. The final 'Output Tags' are "NNP" for Janet, "MD" for will, "VB" for back, "DT" for the, and "NN" for bill, which are the results of the POS tagging task. All extracted text elements from Section 1 directly support these interpretations by labeling each component and its specific function within the model.

**Key Insights:**
The image highlights several key insights into sequence labeling with RNNs for POS tagging. Firstly, the explicit horizontal flow of information ('h' arrow labels) within the 'RNN Layer(s)' demonstrates that sequential processing and context retention are fundamental for language understanding. Secondly, the 'Embeddings e' emphasize that words are converted into dense vector representations before neural network processing. Thirdly, the 'h' variable illustrates how RNNs maintain context through hidden states, which are critical for capturing dependencies in language. Fourthly, the 'Softmax over tags' layer generating probability distributions, followed by the 'Argmax' operation selecting discrete tags ('NNP', 'MD', 'VB', 'DT', 'NN'), reveals how neural network classifiers provide confidence scores before making a final classification. Lastly, the diagram shows POS tagging as a multi-step transformation pipeline, from raw 'Words' through 'Embeddings e', 'RNN Layer(s)', 'Softmax over tags', to 'Argmax y' for final tags. The specific example of tagging the sentence 'Janet will back the bill' provides concrete textual evidence for the model's function.

**Document Context:**
This image is highly relevant to the document section "8.3.1 Sequence Labeling" as it provides a concrete and visual example of how a fundamental sequence labeling task—Part-of-Speech (POS) tagging—is implemented using a Recurrent Neural Network (RNN). It directly illustrates the concepts discussed in the surrounding text, such as assigning grammatical labels from a predefined set of tags to each word in a sentence, and the use of pre-trained word embeddings as inputs and a softmax layer for output probability distributions. The figure makes the abstract concept of sequence labeling tangible by showing the actual flow of data through a neural network architecture.

**Summary:**
This diagram illustrates how a simple Recurrent Neural Network (RNN) performs Part-of-Speech (POS) tagging, which is a type of sequence labeling. The goal is to assign the correct grammatical label (like "noun," "verb," "adjective") to each word in a sentence. The process begins with input words: "Janet", "will", "back", "the", "bill". Each word is converted into a numerical "Embedding" (labeled 'e'), represented visually by stacks of colored circles. These embeddings are then fed sequentially into the "RNN Layer(s)". Within this layer, rectangular units process each word. An important feature is the "hidden state" (labeled 'h'), which is passed from one RNN unit to the next, allowing the network to incorporate context from previous words. Each RNN unit also produces an output (labeled 'Vh') that is sent to the "Softmax over tags" layer. This layer generates a probability distribution for each word over all possible POS tags, depicted as bar charts within rounded boxes. Finally, an "Argmax" operation selects the tag with the highest probability for each word, resulting in the final POS tags (labeled 'y'): "Janet" -> "NNP", "will" -> "MD", "back" -> "VB", "the" -> "DT", "bill" -> "NN". This comprehensive explanation helps readers understand the entire process from main concepts to the micro-details of text labels and connections.](images/a73ea3d3530d63492de4105f644739cde8ba1353b7dd6b280cb655b85482c6a8.jpg)
Figure 8.7 Part-of-speech tagging as sequence labeling with a simple RNN. The goal of part-of-speech (POS) tagging is to assign a grammatical label to each word in a sentence, drawn from a predefined set of tags. (The tags for this sentence include NNP (proper noun), MD (modal verb) and others; we’ll give a complete description of the task of part-of-speech tagging in Chapter 17.) Pre-trained word embeddings serve as inputs and a softmax layer provides a probability distribution over the part-of-speech tags as output at each time step.

# 8.3.2 RNNs for Sequence Classification

Another use of RNNs is to classify entire sequences rather than the tokens within them. This is the set of tasks commonly called text classification, like sentiment analysis or spam detection, in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification, or message routing for customer service applications.

To apply RNNs in this setting, we pass the text to be classified through the RNN a word at a time generating a new hidden layer representation at each time step. We can then take the hidden layer for the last token of the text, $ { \mathbf { h } } _ { n }$ , to constitute a compressed representation of the entire sequence. We can pass this representation $ { \mathbf { h } } _ { n }$ to a feedforward network that chooses a class via a softmax over the possible classes. Fig. 8.8 illustrates this approach.

Note that in this approach we don’t need intermediate outputs for the words in the sequence preceding the last element. Therefore, there are no loss terms associated with those elements. Instead, the loss function used to train the weights in the network is based entirely on the final text classification task. The output from the softmax output from the feedforward classifier together with a cross-entropy loss drives the training. The error signal from the classification is backpropagated all the way through the weights in the feedforward classifier through, to its input, and then through to the three sets of weights in the RNN as described earlier in Section 8.1.2. The training regimen that uses the loss from a downstream application to adjust the weights all the way through the network is referred to as end-to-end training.

Another option, instead of using just hidden state of the last token $h _ { n }$ to represent the whole sequence, is to use some sort of pooling function of all the hidden states $h _ { i }$ for each word $i$ in the sequence. For example, we can create a representation that pools all the $n$ hidden states by taking their element-wise mean:

![## Image Analysis: 38a3efcfbca408515559144c7892767038bc6bf0e6b09f811b89f2163f61cece.jpg

**Conceptual Understanding:**
This image conceptually represents a neural network architecture designed for sequence classification. Its main purpose is to illustrate how a Recurrent Neural Network (RNN) processes a sequence of inputs and how its final internal state can be used as input to a subsequent Feedforward Neural Network (FFN) to perform a classification task, with a Softmax layer providing the final output probabilities. It conveys the idea of transforming a variable-length sequence into a fixed-length representation for classification.

**Content Interpretation:**
The image displays a machine learning model architecture. It shows a Recurrent Neural Network (RNN) processing a sequence of inputs (x1, x2, x3, ..., xn). The final hidden state (hn) of the RNN is then fed into a Feedforward Neural Network (FFN), which is followed by a Softmax activation layer. This setup represents a common approach for sequence classification, where the RNN learns sequential dependencies and outputs a fixed-size representation (hn) that the FFN then uses for classification, with Softmax providing the final probability distribution over classes.

**Key Insights:**
The main takeaway from this image is the architectural pattern for performing sequence classification using a combination of an RNN and a Feedforward Network. Key insights include: 1. **Sequential Processing by RNN (RNN, x1, x2, x3, xn):** The RNN effectively processes input sequences, capturing temporal dependencies. 2. **Final Hidden State as Sequence Representation (hn):** The final hidden state 'hn' serves as a compact summary or representation of the entire input sequence, making it suitable for downstream tasks. 3. **Feedforward Network for Classification (FFN):** A standard Feedforward Neural Network can be effectively used to classify this sequence representation. 4. **Softmax for Probabilistic Output (Softmax):** The Softmax layer provides a probabilistic output, typical for multi-class classification problems. The precise labels 'RNN', 'FFN', 'Softmax', 'x1', 'x2', 'x3', 'xn', and 'hn' explicitly define these components and their roles in the classification pipeline.

**Document Context:**
This image directly supports section 8.3.2 of the document, titled 'RNNs for Sequence Classification'. It visually explains the concept described in the text after the image, which states: 'Figure 8.8 Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification.' The diagram is a crucial visual aid for understanding how an RNN's output can be leveraged for classification tasks.

**Summary:**
This image illustrates the architecture of a sequence classification model that combines a Recurrent Neural Network (RNN) with a Feedforward Neural Network (FFN). The process begins with a sequence of inputs, denoted as x1, x2, x3, up to xn. Each input (x_i) is fed sequentially into the RNN. The RNN processes these inputs over time, passing information from one step to the next. After processing the entire sequence, the final hidden state of the RNN, labeled as hn, is extracted. This final hidden state, hn, is then used as the input for a Feedforward Neural Network (FFN). The FFN further processes this information. Finally, the output of the FFN is passed through a Softmax layer, which is typically used for multi-class classification, providing probabilities for each class. This entire setup demonstrates how the learned representation of a sequence (captured in hn) can be used to make a classification decision.](images/38a3efcfbca408515559144c7892767038bc6bf0e6b09f811b89f2163f61cece.jpg)
Figure 8.8 Sequence classification using a simple RNN combined with a feedforward network. The final hidden state from the RNN is used as the input to a feedforward network that performs the classification.

$$
\pmb { \mathsf { h } } _ { m e a n } = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \mathbf { \mathsf { h } } _ { i }
$$

Or we can take the element-wise max; the element-wise max of a set of $n$ vectors is a new vector whose kth element is the max of the kth elements of all the $n$ vectors.

The long contexts of RNNs makes it quite difficult to successfully backpropagate error all the way through the entire input; we’ll talk about this problem, and some standard solutions, in Section 8.5.

# 8.3.3 Generation with RNN-Based Language Models

RNN-based language models can also be used to generate text. Text generation is of enormous practical importance, part of tasks like question answering, machine translation, text summarization, grammar correction, story generation, and conversational dialogue; any task where a system needs to produce text, conditioned on some other text. This use of a language model to generate text is one of the areas in which the impact of neural language models on NLP has been the largest. Text generation, along with image generation and code generation, constitute a new area of AI that is often called generative AI.

Recall back in Chapter 3 we saw how to generate text from an n-gram language model by adapting a sampling technique suggested at about the same time by Claude Shannon (Shannon, 1951) and the psychologists George Miller and Jennifer Selfridge (Miller and Selfridge, 1950). We first randomly sample a word to begin a sequence based on its suitability as the start of a sequence. We then continue to sample words conditioned on our previous choices until we reach a pre-determined length, or an end of sequence token is generated.

Today, this approach of using a language model to incrementally generate words by repeatedly sampling the next word conditioned on our previous choices is called autoregressive generation or causal LM generation. The procedure is basically the same as that described on page 43, but adapted to a neural context:

• Sample a word in the output from the softmax distribution that results from using the beginning of sentence marker, ${ \tt < s > }$ , as the first input.

• Use the word embedding for that first word as the input to the network at the next time step, and then sample the next word in the same fashion. • Continue generating until the end of sentence marker, $< / { \mathsf { s } } { \mathsf { > } }$ , is sampled or a fixed length limit is reached.

Technically an autoregressive model is a model that predicts a value at time $t$ based on a linear function of the previous values at times $t - 1 , t - 2$ , and so on. Although language models are not linear (since they have many layers of non-linearities), we loosely refer to this generation technique as autoregressive generation since the word generated at each time step is conditioned on the word selected by the network from the previous step. Fig. 8.9 illustrates this approach. In this figure, the details of the RNN’s hidden layers and recurrent connections are hidden within the blue block.

This simple architecture underlies state-of-the-art approaches to applications such as machine translation, summarization, and question answering. The key to these approaches is to prime the generation component with an appropriate context. That is, instead of simply using ${ \tt < s > }$ to get things started we can provide a richer task-appropriate context; for translation the context is the sentence in the source language; for summarization it’s the long text we want to summarize.

![## Image Analysis: f3f3d03eb88ef5fbd9378bf50532de2d1df21a86d9da0745a913630ab1bc269a.jpg

**Conceptual Understanding:**
This image conceptually represents the process of autoregressive text generation using a Recurrent Neural Network (RNN). The main purpose of the diagram is to illustrate how an RNN generates a sequence of words, one word at a time, where each newly generated word is fed back into the model as input for predicting the subsequent word. The diagram demonstrates the step-by-step flow of information through the RNN, highlighting the 'Input Word', 'Embedding', 'RNN' processing, 'Softmax' output, and 'Sampled Word' at each time step in the sequence. It conveys the idea that language generation is a conditional process, where the probability distribution for the next word is conditioned on the sequence of words already generated and input to the model.

**Content Interpretation:**
The image depicts the autoregressive generation of a sequence of words ('So long and ?') using a Recurrent Neural Network (RNN). It illustrates a fundamental concept in neural language modeling where the model predicts the next word in a sequence based on the words it has already generated. Each block in the RNN chain represents a hidden state that processes the current input and the previous hidden state, passing information forward. The 'Embedding' step converts discrete words into dense vector representations suitable for neural networks. The 'Softmax' layer then transforms the RNN's output into a probability distribution over the entire vocabulary, indicating the likelihood of each possible next word. Finally, a word is 'Sampled' from this distribution. The core system being shown is an RNN performing sequential word generation, where the output of one step feeds into the input of the next, forming a chain of dependent predictions.

**Key Insights:**
The main takeaway from this image is the concept of autoregressive generation in RNN-based language models. It teaches that text generation is a sequential process where each new word is predicted based on all the preceding words in the sequence. Key insights include: 1. **Sequential Processing:** RNNs process words one at a time, maintaining a hidden state that encapsulates past information. 2. **Autoregressive Feedback Loop:** The word 'Sampled' at one time step becomes the 'Input Word' for the next time step, creating a feedback loop essential for coherent sequence generation. For example, 'So' is sampled, then becomes the input for the next step. 3. **Component Roles:** The 'Embedding' converts words to vectors, the 'RNN' processes sequential data, and 'Softmax' produces probability distributions for next word prediction. 4. **Probabilistic Sampling:** Words are 'Sampled' from a probability distribution generated by the Softmax layer, indicating that generation can be non-deterministic and explore different valid continuations.

**Document Context:**
This image directly supports section 8.3.3, titled 'Generation with RNN-Based Language Models', by visually explaining the mechanism of how an RNN generates text in an autoregressive manner. It provides a concrete example of the flow of information and the sequence of operations involved in predicting subsequent words based on prior context. The diagram, along with the accompanying text 'Figure 8.9 Autoregressive generation with an RNN-based neural language model.', serves as a primary illustration for understanding the practical implementation of RNNs for text generation, demonstrating the step-by-step process from input tokens to sampled output words.

**Summary:**
This diagram illustrates the autoregressive generation process using an RNN-based neural language model. The process flows from left to right, representing a sequence of time steps where one word is generated at a time. Each step involves an 'Input Word' being converted into an 'Embedding', processed by the 'RNN', passed through a 'Softmax' layer to produce a probability distribution, and finally, a 'Sampled Word' is chosen. Crucially, the 'Sampled Word' from the current step becomes the 'Input Word' for the subsequent step, demonstrating the autoregressive nature where the generation of the next word depends on the previously generated word. The initial input is a start-of-sequence token, <S>, which leads to the sampling of the first meaningful word, 'So'. This word 'So' then becomes the input for the next time step, leading to the sampling of 'long'. This pattern continues with 'long' as input generating 'and', and then 'and' as input leading to the generation of a question mark '?'. The Softmax layer at each step outputs a probability distribution over the vocabulary (represented by the small bar charts), from which the next word is sampled. The overall structure emphasizes the sequential processing and dependency inherent in RNNs for language generation.](images/f3f3d03eb88ef5fbd9378bf50532de2d1df21a86d9da0745a913630ab1bc269a.jpg)
Figure 8.9 Autoregressive generation with an RNN-based neural language model.

# 8.4 Stacked and Bidirectional RNN architectures

Recurrent networks are quite flexible. By combining the feedforward nature of unrolled computational graphs with vectors as common inputs and outputs, complex networks can be treated as modules that can be combined in creative ways. This section introduces two of the more common network architectures used in language processing with RNNs.

# 8.4.1 Stacked RNNs

In our examples thus far, the inputs to our RNNs have consisted of sequences of word or character embeddings (vectors) and the outputs have been vectors useful for predicting words, tags or sequence labels. However, nothing prevents us from using the entire sequence of outputs from one RNN as an input sequence to another one. Stacked RNNs consist of multiple networks where the output of one layer serves as the input to a subsequent layer, as shown in Fig. 8.10.

![## Image Analysis: b2a1a43a2889c14fbed0eae7956ba2153b462adb005d89ad21e4a085cf45ebbe.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a **Stacked Recurrent Neural Network (RNN)**. It illustrates how multiple layers of RNNs are combined, with information flowing sequentially through each layer and also vertically between layers. The main purpose of this image is to visually explain the structure and data flow in such a network, specifically showing how the output of a lower RNN layer serves as the input for a higher RNN layer at each time step, and how the final output sequence is generated by the topmost layer. It communicates the key idea of increasing the depth of recurrent models to enhance their ability to learn complex temporal patterns and hierarchical features in sequential data.

**Content Interpretation:**
The image depicts the architecture and data flow within a stacked Recurrent Neural Network (RNN). It shows three distinct layers, labeled 'RNN 1', 'RNN 2', and 'RNN 3', processing a sequence of inputs (x₁, x₂, x₃, ..., xₙ) to produce a corresponding sequence of outputs (y₁, y₂, y₃, ..., yₙ).

The processes shown are:
*   **Sequential Processing within Layers:** Each RNN layer (RNN 1, RNN 2, RNN 3) processes its input sequence from left to right (e.g., x₁ then x₂, etc.). The horizontal arrows within each RNN layer represent the recurrent connection, where the output or hidden state from a previous time step within that same layer is fed as input to the current time step's processing unit.
*   **Layered/Stacked Processing:** The arrangement of 'RNN 1', 'RNN 2', and 'RNN 3' in vertical succession demonstrates the stacking of multiple RNN layers.
*   **Inter-layer Data Flow (Vertical Connections):** The vertical arrows indicate that the output of a processing unit in a lower RNN layer serves as an input to the corresponding processing unit in the higher RNN layer at the same time step. For example, the output of the first unit in RNN 1 feeds into the first unit of RNN 2, and the output of the first unit in RNN 2 feeds into the first unit of RNN 3.

The relationships highlight a **hierarchical dependency** where higher layers build upon the representations learned by lower layers. The units are also **time-step aligned**, meaning that for any given 't' (e.g., for x₁), processing occurs simultaneously across the stacked layers, from RNN 1 up to RNN 3, to generate the corresponding output y_t.

The significance of the inputs 'x₁', 'x₂', 'x₃', 'xₙ' and outputs 'y₁', 'y₂', 'y₃', 'yₙ' is that they represent elements of an input sequence and the corresponding elements of an output sequence, respectively, in a time-dependent or sequential context. The 'n' denotes the arbitrary length of the sequence. The RNN labels 'RNN 1', 'RNN 2', 'RNN 3' signify that each layer is a complete Recurrent Neural Network capable of learning and maintaining temporal dependencies.

**Key Insights:**
The image provides several key insights into the operation and structure of stacked RNNs:

*   **Deep Sequential Learning:** Stacked RNNs (evidenced by 'RNN 1', 'RNN 2', 'RNN 3' layers) are designed to learn complex, hierarchical representations of sequential data. Lower layers ('RNN 1') capture basic features, while higher layers ('RNN 3') build upon these to extract more abstract patterns over time.
*   **Vertical Information Flow:** A fundamental principle is that the output of each recurrent unit in a lower layer is fed as input to the corresponding recurrent unit in the layer directly above it. This is clearly shown by the vertical arrows connecting, for example, a unit in 'RNN 1' to a unit in 'RNN 2', and then to a unit in 'RNN 3'. This corroborates the textual evidence: "The output of a lower level serves as the input to higher levels."
*   **Sequential Processing within Layers:** Within each 'RNN 1', 'RNN 2', and 'RNN 3' layer, information flows horizontally across time steps (e.g., from the unit processing x₁'s output to the unit processing x₂'s input). This is represented by the horizontal arrows within each RNN layer, indicating the recurrent nature and memory retention over the sequence.
*   **Output from Topmost Layer:** The final outputs 'y₁', 'y₂', 'y₃', 'yₙ' are exclusively generated by the topmost layer, 'RNN 3', demonstrating that "the output of the last network serving as the final output." This indicates that the deepest representations are used for the final prediction or task.
*   **Time-Distributed Processing:** The distinct inputs ('x₁', 'x₂', 'x₃', 'xₙ') and outputs ('y₁', 'y₂', 'y₃', 'yₙ') at each time step highlight that stacked RNNs can handle sequence-to-sequence tasks where an output is desired for each input in the sequence.

**Document Context:**
This image is highly relevant to the document's Section 8.4.1 titled "Stacked RNNs", as it provides a clear visual representation of the concept. The accompanying text, "Figure 8.10 Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output," directly describes the architecture depicted. The image visually clarifies how multiple RNN layers are arranged, how data flows sequentially within each layer, and crucially, how information is passed vertically from a lower RNN layer to the one above it, ultimately producing the final output from the topmost layer. It serves to concretely illustrate the theoretical discussion of stacked recurrent networks, making the abstract concept of depth in RNNs more understandable.

**Summary:**
This diagram illustrates a "Stacked Recurrent Neural Network" (RNN) architecture, which is a common way to build deeper, more powerful recurrent models for processing sequences.

The diagram is structured into three distinct horizontal layers, each representing an independent Recurrent Neural Network:
*   **RNN 1 (Bottom Layer):** This is the first layer that directly receives the raw sequence inputs.
*   **RNN 2 (Middle Layer):** This layer receives its inputs from the layer below it (RNN 1).
*   **RNN 3 (Top Layer):** This is the final layer, receiving inputs from RNN 2, and producing the overall outputs of the stacked network.

The network processes data sequentially, depicted from left to right. Let's trace the flow for a specific time step:

1.  **Input (e.g., x₁):** An input element, such as "x₁", enters the first processing unit (represented by a small square box) of "RNN 1".
2.  **Processing in RNN 1:** This unit processes "x₁". Its output or hidden state is then passed in two directions:
    *   **Horizontally:** To the next processing unit within "RNN 1" (e.g., for processing "x₂"). This allows "RNN 1" to maintain memory of past inputs within its own layer.
    *   **Vertically:** To the corresponding processing unit in "RNN 2". This signifies that the output of "RNN 1" at this time step becomes an input to "RNN 2" at the same time step.
3.  **Processing in RNN 2:** The processing unit in "RNN 2" receives input from two sources:
    *   Vertically from the corresponding unit in "RNN 1".
    *   Horizontally from the previous processing unit within "RNN 2" (if applicable, for time steps after the first).
    *   Similar to "RNN 1", its output/hidden state is passed horizontally to the next unit in "RNN 2" and vertically to the corresponding unit in "RNN 3".
4.  **Processing in RNN 3:** The processing unit in "RNN 3" receives input from:
    *   Vertically from the corresponding unit in "RNN 2".
    *   Horizontally from the previous processing unit within "RNN 3".
    *   This topmost layer then produces the final output for that specific time step. For example, for the first time step, "y₁" is output from "RNN 3".
5.  **Sequential Progression:** This entire process repeats for subsequent input elements "x₂", "x₃", up to "xₙ", generating corresponding outputs "y₂", "y₃", up to "yₙ" from "RNN 3".

In essence, the image illustrates a deep recurrent architecture where lower layers capture basic sequential patterns, and higher layers build upon these representations to learn more abstract and complex features, ultimately producing the final sequence of outputs. The critical point is the vertical transfer of information, where the processed data from one RNN layer acts as the input to the layer directly above it, allowing for a hierarchical understanding of the sequence.](images/b2a1a43a2889c14fbed0eae7956ba2153b462adb005d89ad21e4a085cf45ebbe.jpg)
Figure 8.10 Stacked recurrent networks. The output of a lower level serves as the input to higher levels with the output of the last network serving as the final output.

Stacked RNNs generally outperform single-layer networks. One reason for this success seems to be that the network induces representations at differing levels of abstraction across layers. Just as the early stages of the human visual system detect edges that are then used for finding larger regions and shapes, the initial layers of stacked networks can induce representations that serve as useful abstractions for further layers—representations that might prove difficult to induce in a single RNN. The optimal number of stacked RNNs is specific to each application and to each training set. However, as the number of stacks is increased the training costs rise quickly.

# 8.4.2 Bidirectional RNNs

The RNN uses information from the left (prior) context to make its predictions at time $t$ . But in many applications we have access to the entire input sequence; in those cases we would like to use words from the context to the right of $t$ . One way to do this is to run two separate RNNs, one left-to-right, and one right-to-left, and concatenate their representations.

In the left-to-right RNNs we’ve discussed so far, the hidden state at a given time $t$ represents everything the network knows about the sequence up to that point. The state is a function of the inputs $x _ { 1 } , . . . , x _ { t }$ and represents the context of the network to the left of the current time.

$$
\boldsymbol { \mathsf { h } } _ { t } ^ { f } \ = \ \mathrm { R N N } _ { \mathrm { f o r w a r d } } ( \mathsf { x } _ { 1 } , \ldots , \mathsf { x } _ { t } )
$$

This new notation $ { \mathbf { h } } _ { t } ^ { f }$ simply corresponds to the normal hidden state at time $t$ , representing everything the network has gleaned from the sequence so far.

To take advantage of context to the right of the current input, we can train an RNN on a reversed input sequence. With this approach, the hidden state at time $t$ represents information about the sequence to the right of the current input:

$$
\boldsymbol { \mathsf { h } } _ { t } ^ { b } \ = \ \mathrm { R N N } _ { \mathrm { b a c k w a r d } } ( \mathsf { x } _ { t } , \dots \mathsf { x } _ { n } )
$$

Here, the hidden state $\boldsymbol { \mathsf { h } } _ { t } ^ { b }$ represents all the information we have discerned about the sequence from $t$ to the end of the sequence.

A bidirectional RNN (Schuster and Paliwal, 1997) combines two independent RNNs, one where the input is processed from the start to the end, and the other from the end to the start. We then concatenate the two representations computed by the networks into a single vector that captures both the left and right contexts of an input at each point in time. Here we use either the semicolon ”;” or the equivalent symbol $\oplus$ to mean vector concatenation:

$$
\begin{array} { r } { \mathbf { h } _ { t } ~ = ~ [ \mathbf { h } _ { t } ^ { f } ; \mathbf { h } _ { t } ^ { b } ] } \\ { = ~ \mathbf { h } _ { t } ^ { f } \oplus \mathbf { h } _ { t } ^ { b } } \end{array}
$$

Fig. 8.11 illustrates such a bidirectional network that concatenates the outputs of the forward and backward pass. Other simple ways to combine the forward and backward contexts include element-wise addition or multiplication. The output at each step in time thus captures information to the left and to the right of the current input. In sequence labeling applications, these concatenated outputs can serve as the basis for a local labeling decision.

![## Image Analysis: 19537d8456978f29361b6864f400fcf0c6ffd0462f16d806d43e178579aff0b1.jpg

**Conceptual Understanding:**
The image conceptually represents a Bidirectional Recurrent Neural Network (BiRNN). Its main purpose is to visually explain how a sequence of inputs is processed by two distinct RNNs—one moving forward through the sequence and another moving backward—and how their respective outputs are then combined to form a richer, context-aware output at each time step. The key idea being communicated is the integration of both past and future contextual information for improved sequence understanding and prediction.

**Content Interpretation:**
The image illustrates the core architecture and operational flow of a Bidirectional Recurrent Neural Network (RNN). It demonstrates a parallel processing approach where a sequence is analyzed by two separate RNNs: one in the forward direction and one in the backward direction. The key concept shown is that the final output at any given time step is derived by combining the respective outputs (or hidden states) from both the forward and backward passes. This allows the model to leverage context from both preceding and succeeding elements in a sequence.

**Key Insights:**
The main takeaway from this image is the operational principle of a Bidirectional RNN. It highlights that sequential data is processed in two directions simultaneously—a forward pass by 'RNN 1' and a backward pass by 'RNN 2'. For each time step, the output, such as 'y1', 'y2', 'y3', up to 'yn', is a 'concatenated output' of the information processed by both 'RNN 1' and 'RNN 2' at that specific point. This architecture allows the model to capture dependencies from both past and future elements in the sequence, leading to a richer contextual understanding. The presence of 'x1', 'x2', 'x3', and 'xn' clearly indicates sequential input, while 'y1', 'y2', 'y3', and 'yn' represent the corresponding sequential outputs.

**Document Context:**
This image directly supports Section 8.4.2, which discusses Bidirectional RNNs. It provides a visual representation of how separate models are trained in forward and backward directions and how their outputs are concatenated. The diagram perfectly illustrates the mechanism described in the accompanying text, showing the inputs (x_i), the two RNNs (RNN 1 for forward, RNN 2 for backward), and the concatenated outputs (y_i) that represent the bidirectional state at each time point, thereby enhancing comprehension of the technical concept.

**Summary:**
The image depicts the architecture of a Bidirectional Recurrent Neural Network (RNN). It shows two distinct RNN layers, RNN 1 and RNN 2, processing a sequence of inputs (x1 to xn) and producing a sequence of outputs (y1 to yn). RNN 1 processes the input sequence in a forward direction (left to right), taking inputs x1, x2, x3, up to xn. RNN 2 processes the input sequence in a backward direction (right to left), also interacting with inputs xn, xn-1, ..., x1. At each corresponding time step, the outputs from both RNN 1 and RNN 2 are combined (concatenated) to form the final output for that time step. For instance, the output y1 is formed by concatenating contributions from both RNNs at the first time step, and similarly for y2, y3, and yn. The diagram visually explains how information from both past and future contexts relative to the current time step is integrated to produce a more informed output.](images/19537d8456978f29361b6864f400fcf0c6ffd0462f16d806d43e178579aff0b1.jpg)
Figure 8.11 A bidirectional RNN. Separate models are trained in the forward and backward directions, with the output of each model at each time point concatenated to represent the bidirectional state at that time point.

Bidirectional RNNs have also proven to be quite effective for sequence classification. Recall from Fig. 8.8 that for sequence classification we used the final hidden state of the RNN as the input to a subsequent feedforward classifier. A difficulty with this approach is that the final state naturally reflects more information about the end of the sentence than its beginning. Bidirectional RNNs provide a simple solution to this problem; as shown in Fig. 8.12, we simply combine the final hidden states from the forward and backward passes (for example by concatenation) and use that as input for follow-on processing.

![## Image Analysis: 86a95510bb78126a533aa5f8b3a8a4fef554655885c34bbeea0669b48137bd8f.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural design of a Bidirectional Recurrent Neural Network (BiRNN). It illustrates how two distinct Recurrent Neural Networks process a single input sequence in opposite temporal directions to capture both past and future contextual dependencies.

The main purpose of this image is to visually explain the mechanism by which a BiRNN constructs a comprehensive representation of an entire sequence. It shows how the final hidden states from a forward pass and a backward pass are combined and then fed into a subsequent classifier to make predictions based on the full context of the sequence, rather than just a unidirectional view.

Key ideas communicated include: the concept of processing sequences bidirectionally, the role of hidden states in summarizing sequence information, the combination of these bidirectional summaries, and their eventual use in a feedforward network followed by a softmax layer for classification.

**Content Interpretation:**
The image displays a Bidirectional Recurrent Neural Network (BiRNN) architecture. It shows a sequential processing system with two main recurrent layers, RNN 1 and RNN 2, operating in opposite directions, followed by a Feedforward Network (FFN) and a Softmax classifier.

**Processes:**
*   **Sequential Input Processing:** The inputs x₁, x₂, x₃, ..., xₙ represent a sequence of data elements fed into the network.
*   **Forward Recurrent Processing (RNN 1):** The layer labeled "RNN 1" processes the input sequence from x₁ to xₙ (left to right), capturing dependencies from past elements. Its final hidden state, hₙ (rightward arrow), summarizes the forward pass information.
*   **Backward Recurrent Processing (RNN 2):** The layer labeled "RNN 2" processes the sequence in reverse, from elements corresponding to xₙ down to x₁ (right to left), capturing dependencies from future elements. Its final hidden state, h₁ (leftward arrow), summarizes the backward pass information.
*   **Feature Combination:** The final hidden states from both RNN 1 (hₙ with rightward arrow) and RNN 2 (h₁ with leftward arrow) are combined.
*   **Transformation by FFN:** The combined representation is then processed by a Feedforward Network ("FFN") to create a more abstract and integrated feature vector.
*   **Classification by Softmax:** The output of the FFN is fed into a "Softmax" layer, which performs the final classification by converting the input into a probability distribution over predefined classes.

**Relationships/Systems:**
*   The diagram illustrates a neural network system for sequence classification.
*   It highlights a parallel processing relationship between RNN 1 and RNN 2, both operating on the same sequence but in different directions.
*   There's a hierarchical flow from raw inputs to recurrent processing, then to a feedforward layer, and finally to a classification output.
*   The combined use of forward and backward hidden states (h₁ with leftward arrow and hₙ with rightward arrow) as input to the FFN signifies the importance of comprehensive contextual understanding in this architecture.

**Supporting Evidence from Extracted Text:**
*   "x₁, x₂, x₃, ..., xₙ" explicitly denote the input sequence elements.
*   "RNN 1" and "RNN 2" precisely name the two recurrent processing layers.
*   The rightward arrows within "RNN 1" and the label "hₙ" (with rightward arrow) confirm the forward pass and its final hidden state.
*   The leftward arrows within "RNN 2" and the label "h₁" (with leftward arrow) confirm the backward pass and its final hidden state.
*   "FFN" identifies the Feedforward Network, which combines the outputs of the RNNs.
*   "Softmax" clearly indicates the final classification layer, where the network makes its prediction.

**Key Insights:**
The image provides several key insights into the operation and purpose of Bidirectional Recurrent Neural Networks (BiRNNs):

*   **Bidirectional Context is Essential for Comprehensive Sequence Understanding:** The explicit depiction of "RNN 1" processing from left-to-right (forward) and "RNN 2" processing from right-to-left (backward) demonstrates that both past (from x₁ to xₙ) and future (from xₙ to x₁) contextual information are crucial for a complete understanding of a sequence, as evidenced by both h₁ (leftward arrow) and hₙ (rightward arrow) being used.
*   **Hidden States Act as Sequence Summaries:** The labels "hₙ" (rightward arrow) from RNN 1 and "h₁" (leftward arrow) from RNN 2 represent the final hidden states, which are the concise summary vectors of the entire sequence as processed by each directional RNN. These summaries are the critical information passed to the next stage.
*   **Combination of Bidirectional Features Enhances Representation:** The arrows leading from h₁ (leftward arrow) and hₙ (rightward arrow) into the "FFN" show that these two distinct contextual summaries are concatenated or combined. This allows the network to form a richer, more robust representation of the sequence that accounts for dependencies from both directions.
*   **Feedforward Networks Refine Combined Features:** The inclusion of the "FFN" (Feedforward Network) indicates that the combined hidden states are not directly classified. Instead, they undergo further non-recurrent processing to learn more complex patterns and abstract features from the bidirectional context before the final classification.
*   **Softmax is a Standard Classification Output Layer:** The "Softmax" layer at the end of the process confirms that the network's ultimate goal is a classification task, where it will output probabilities over different classes based on the comprehensive sequence representation.

**Document Context:**
This image directly illustrates the architecture of a Bidirectional Recurrent Neural Network, which is the specific topic of Section 8.4.2, "Bidirectional RNNs". It visually supports the textual explanation of how these networks function, particularly in combining information from both forward and backward passes for sequence classification. The accompanying text, "Figure 8.12 A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier," is perfectly aligned with the visual representation, making the diagram a critical component for understanding the technical details of BiRNNs as described in the document.

**Summary:**
This diagram illustrates the architecture of a Bidirectional Recurrent Neural Network (BiRNN) specifically designed for sequence classification. The core idea is to process an input sequence in two directions—forward and backward—to capture comprehensive contextual information before making a classification decision.

The process begins with an input sequence, denoted by elements x₁, x₂, x₃, continuing up to xₙ. These individual inputs are fed into two parallel Recurrent Neural Networks:

1.  **RNN 1 (Forward Pass):** This is the lower horizontal blue rounded rectangle, labeled "RNN 1". It processes the input sequence from left to right, meaning it starts with x₁, then x₂, x₃, and so on, until xₙ. Each square unit within this RNN represents a step in the sequence, and the rightward arrows indicate the flow of information forward through time. The final output of interest from this forward pass is the hidden state, labeled hₙ (with a rightward arrow), located at the far right of the "RNN 1" layer. This hₙ represents a summary of the entire sequence's information captured by looking from the beginning to the end.

2.  **RNN 2 (Backward Pass):** This is the upper horizontal blue rounded rectangle, labeled "RNN 2". It processes the input sequence (or corresponding representations) in the opposite direction, from right to left. Although not explicitly labeled as receiving xₙ first, its internal leftward arrows signify processing from the end of the sequence towards the beginning. The crucial output from this backward pass is the hidden state, labeled h₁ (with a leftward arrow), located at the far left of the "RNN 2" layer. This h₁ represents a summary of the entire sequence's information captured by looking from the end to the beginning.

After both RNNs have processed the sequence, their respective final hidden states—h₁ (with leftward arrow) from RNN 2 and hₙ (with rightward arrow) from RNN 1—are extracted. These two outputs are then combined and fed upwards into the subsequent layers for classification:

3.  **Feedforward Network (FFN):** The trapezoidal shape labeled "(FFN)" represents a Feedforward Network. It receives both h₁ (leftward arrow) and hₙ (rightward arrow) as its inputs. The FFN's role is to combine these two comprehensive contextual representations and transform them into a refined feature vector.

4.  **Softmax Classifier:** Finally, the output from the FFN is passed to the oval shape labeled "Softmax". The Softmax layer is a common final layer in classification tasks, responsible for converting the network's raw output into a probability distribution over different possible classes, thereby producing the final classification for the input sequence.

In summary, this diagram effectively shows how a bidirectional approach allows a neural network to leverage both past and future context within a sequence, resulting in a more informed and robust representation for classification tasks.](images/86a95510bb78126a533aa5f8b3a8a4fef554655885c34bbeea0669b48137bd8f.jpg)
Figure 8.12 A bidirectional RNN for sequence classification. The final hidden units from the forward and backward passes are combined to represent the entire sequence. This combined representation serves as input to the subsequent classifier.

# 8.5 The LSTM

In practice, it is quite difficult to train RNNs for tasks that require a network to make use of information distant from the current point of processing. Despite having access to the entire preceding sequence, the information encoded in hidden states tends to be fairly local, more relevant to the most recent parts of the input sequence and recent decisions. Yet distant information is critical to many language applications. Consider the following example in the context of language modeling.

(8.19) The flights the airline was canceling were full.

Assigning a high probability to was following airline is straightforward since airline provides a strong local context for the singular agreement. However, assigning an appropriate probability to were is quite difficult, not only because the plural flights is quite distant, but also because the singular noun airline is closer in the intervening context. Ideally, a network should be able to retain the distant information about plural flights until it is needed, while still processing the intermediate parts of the sequence correctly.

One reason for the inability of RNNs to carry forward critical information is that the hidden layers, and, by extension, the weights that determine the values in the hidden layer, are being asked to perform two tasks simultaneously: provide information useful for the current decision, and updating and carrying forward information required for future decisions.

A second difficulty with training RNNs arises from the need to backpropagate the error signal back through time. Recall from Section 8.1.2 that the hidden layer at time $t$ contributes to the loss at the next time step since it takes part in that calculation. As a result, during the backward pass of training, the hidden layers are subject to repeated multiplications, as determined by the length of the sequence. A frequent result of this process is that the gradients are eventually driven to zero, a situation

long short-term memory

called the vanishing gradients problem.

To address these issues, more complex network architectures have been designed to explicitly manage the task of maintaining relevant context over time, by enabling the network to learn to forget information that is no longer needed and to remember information required for decisions still to come.

The most commonly used such extension to RNNs is the long short-term memory (LSTM) network (Hochreiter and Schmidhuber, 1997). LSTMs divide the context management problem into two subproblems: removing information no longer needed from the context, and adding information likely to be needed for later decision making. The key to solving both problems is to learn how to manage this context rather than hard-coding a strategy into the architecture. LSTMs accomplish this by first adding an explicit context layer to the architecture (in addition to the usual recurrent hidden layer), and through the use of specialized neural units that make use of gates to control the flow of information into and out of the units that comprise the network layers. These gates are implemented through the use of additional weights that operate sequentially on the input, and previous hidden layer, and previous context layers.

The gates in an LSTM share a common design pattern; each consists of a feedforward layer, followed by a sigmoid activation function, followed by a pointwise multiplication with the layer being gated. The choice of the sigmoid as the activation function arises from its tendency to push its outputs to either 0 or 1. Combining this with a pointwise multiplication has an effect similar to that of a binary mask. Values in the layer being gated that align with values near 1 in the mask are passed through nearly unchanged; values corresponding to lower values are essentially erased.

The first gate we’ll consider is the forget gate. The purpose of this gate is to delete information from the context that is no longer needed. The forget gate computes a weighted sum of the previous state’s hidden layer and the current input and passes that through a sigmoid. This mask is then multiplied element-wise by the context vector to remove the information from context that is no longer required. Element-wise multiplication of two vectors (represented by the operator $\odot$ and sometimes called the Hadamard product) is the vector of the same dimension as the two input vectors, where each element $i$ is the product of element $i$ in the two input vectors:

$$
\begin{array} { r l } { \mathbf { f } _ { t } } & { = \mathbf { \sigma } \sigma ( \mathbf { U } _ { f } \mathbf { h } _ { t - 1 } + \mathbf { W } _ { f } \mathbf { x } _ { t } ) } \\ { \mathbf { k } _ { t } } & { = \mathbf { \sigma } \mathbf { c } _ { t - 1 } \odot \mathbf { f } _ { t } } \end{array}
$$

The next task is to compute the actual information we need to extract from the previous hidden state and current inputs—the same basic computation we’ve been using for all our recurrent networks.

$$
{ \bf g } _ { t } \ = \ \operatorname { t a n h } ( { \bf U } _ { g } { \bf h } _ { t - 1 } + { \bf W } _ { g } { \bf x } _ { t } )
$$

# add gate

Next, we generate the mask for the add gate to select the information to add to the current context.

$$
\begin{array} { r l } & { \mathbf { i } _ { t } \ = \ \sigma ( \mathbf { U } _ { i } \mathbf { h } _ { t - 1 } + \mathbf { W } _ { i } \mathbf { x } _ { t } ) } \\ & { \mathbf { j } _ { t } \ = \ \mathbf { g } _ { t } \odot \mathbf { i } _ { t } } \end{array}
$$

Next, we add this to the modified context vector to get our new context vector.

$$
\mathbf { c } _ { t } = \mathbf { j } _ { t } + \mathbf { k } _ { t }
$$

![## Image Analysis: d1441420026e0f254fdd8b81539d7be6a8d6addd3dacc7152ec021f039f646cd.jpg

**Conceptual Understanding:**
This image represents the internal architecture and computational flow of a single Long Short-Term Memory (LSTM) unit, displayed as a computation graph. Conceptually, it illustrates how LSTMs address the vanishing gradient problem in recurrent neural networks by incorporating a 'memory cell' (`c_t`) and a sophisticated gating mechanism to regulate the flow of information.

The main purpose of this diagram is to visually explain the step-by-step calculations and interactions between the different components (gates, states, and inputs) that lead to the updated cell state and hidden state within one time step of an LSTM. It highlights how information is selectively forgotten, remembered, and outputted.

Key ideas being communicated include:
*   The concept of a cell state (`c_t`) as a long-term memory component.
*   The role of 'gates' (forget `f`, input `i`, output `o`) in controlling information flow into and out of the cell state.
*   The combination of previous hidden state (`h_t-1`), previous cell state (`c_t-1`), and current input (`x_t`) to produce new states (`h_t`, `c_t`).
*   The specific activation functions (`sigmoid`, `tanh`) and operations (`addition`, `element-wise multiplication`) used in these calculations.

**Content Interpretation:**
The image clearly shows the internal mechanisms of an LSTM, which is a type of recurrent neural network layer designed to learn long-term dependencies.

*   **Inputs:** `c_t-1`, `h_t-1`, and `x_t` represent the previous cell state, previous hidden state, and current input, respectively. These are the foundational data points entering the unit at each time step.

*   **Gates (`f`, `i`, `o`):** These are the core controllers of information flow. Each gate is computed by taking a linear combination of `h_t-1` and `x_t` (represented by the trapezoidal shapes and `+` operation), followed by a sigmoid (`σ`) activation. Sigmoid ensures the gate outputs are between 0 and 1, acting as 'toggles' or 'weights' for information:
    *   **Forget Gate (`f`):** Calculated using `σ`. Its output is element-wise multiplied (`⊙`) with `c_t-1`. This operation determines which parts of the previous cell state `c_t-1` should be kept (close to 1) or forgotten (close to 0).
    *   **Input Gate (`i`):** Also calculated using `σ`. Its output is element-wise multiplied (`⊙`) with the cell state candidate `g`. This controls how much of the new information from `g` is allowed to update the cell state.
    *   **Output Gate (`o`):** Calculated using `σ`. Its output is element-wise multiplied (`⊙`) with the `tanh` activation of `c_t`. This regulates how much of the (scaled) cell state `c_t` contributes to the new hidden state `h_t`.

*   **Cell State Candidate (`g`):** This is computed by passing a linear combination of `h_t-1` and `x_t` through a hyperbolic tangent (`tanh`) activation function. The `tanh` function scales the values between -1 and 1, providing a new candidate for updating the cell state. The label `g` supports its role as a generator of potential new information.

*   **Cell State Update (`c_t`):** This is the central memory of the LSTM.
    *   The forgotten part of `c_t-1` (from `f ⊙ c_t-1`) is added (`+`) to the new candidate information (from `i ⊙ g`).
    *   The label `c_t` clearly indicates this is the updated cell state, which carries information over long sequences. The direct connection from `c_t-1` through `f ⊙` and `i ⊙ g` to `c_t` visually demonstrates the additive and subtractive nature of cell state updates.

*   **Hidden State Output (`h_t`):** This is the output of the LSTM unit, often used as input to the next layer or the next time step.
    *   It is derived by applying `tanh` activation to the current cell state `c_t`, then element-wise multiplying (`⊙`) this result by the output gate `o`.
    *   The label `h_t` confirms its identity as the current hidden state, which represents the short-term memory or 'current focus' of the network.

All transcribed text elements, such as `c_t-1`, `h_t-1`, `x_t`, `f`, `g`, `i`, `o`, `c_t`, `h_t`, and the operation symbols `+`, `σ`, `tanh`, `⊙`, are critical for understanding the flow and calculations. They directly map to the mathematical operations and conceptual components of an LSTM, providing explicit evidence for the interpretation of information gating, state updates, and output generation.

**Key Insights:**
This image provides several key takeaways regarding the functionality and purpose of an LSTM unit:

*   **Selective Information Retention and Forgetting:** The existence of the `f` (forget gate) and `i` (input gate) explicitly demonstrates how LSTMs can selectively decide which information from the past (`c_t-1`) to discard and which new information (`g`) to incorporate into the current cell state (`c_t`). The `⊙` (element-wise multiplication) operations involving `f` and `i` are direct textual evidence of this selective modulation.
*   **Long-Term Memory through Cell State:** The `c_t` node, updated by combining `c_t-1` with modulated new information, highlights the central role of the cell state as a persistent memory component. Its direct path with minimal non-linear interactions (only element-wise multiplication and addition) allows information to flow largely unchanged, addressing the vanishing gradient problem.
*   **Controlled Output:** The `o` (output gate) and its `⊙` operation with the `tanh(c_t)` demonstrate that the hidden state (`h_t`) is not simply the raw cell state. Instead, `h_t` is a filtered version of the cell state, where the output gate controls what information from the cell state is actually exposed as the hidden state. This allows the LSTM to keep information in the cell state without necessarily outputting it at every step.
*   **Modular Gating Mechanism:** The clear separation of `f`, `g`, `i`, and `o` as distinct computational paths (each with its own `+` and activation function) illustrates the modular design of LSTMs, where different gates perform specialized functions to manage memory.
*   **Dependency on Previous States and Current Input:** Every gate and the `g` candidate derive their values from both the previous hidden state `h_t-1` and the current input `x_t`. This is visually supported by the multiple incoming lines (representing weighted connections) from `h_t-1` and `x_t` to the initial `+` operations of each gate. This confirms the recurrent nature of the network, where past information influences current decisions.

The specific labels `f`, `i`, `o`, `g`, `c_t-1`, `h_t-1`, `x_t`, `c_t`, `h_t`, and the operational symbols `σ`, `tanh`, `⊙`, `+` are the textual evidence for these insights, defining the exact mathematical components and their roles in the LSTM's functionality.

**Document Context:**
This image, titled 'Figure 8.13 A single LSTM unit displayed as a computation graph,' is crucial for understanding the 'add gate' section of the document. It visually breaks down the complex mathematical operations of an LSTM into a comprehensible diagram. The document context explicitly states that inputs are `x_t`, `h_t-1`, and `c_t-1`, and outputs are `h_t` and `c_t`, which perfectly align with the diagram. The 'add gate' section likely refers to the input gate and the subsequent addition to the cell state, and this diagram provides the exact visual and operational detail of how that addition occurs and how it's controlled.

**Summary:**
This diagram illustrates the internal workings of a single Long Short-Term Memory (LSTM) unit, a specialized type of neural network capable of learning from and making predictions on sequence data. It's designed to manage information flow over long periods, effectively addressing the 'memory' problems (like vanishing gradients) found in simpler recurrent neural networks.

Let's break down the process step-by-step:

**1. Inputs to the LSTM Unit:**
The LSTM unit takes three main inputs:
*   `c_t-1`: This is the **previous cell state**, representing the unit's long-term memory from the prior time step.
*   `h_t-1`: This is the **previous hidden state**, representing the unit's short-term memory or 'output' from the prior time step.
*   `x_t`: This is the **current input** to the unit at the present time step.

**2. The Four Internal Gates/Components:**
The core of the LSTM lies in its four main pathways, often referred to as 'gates' or components, each calculated by combining the current input `x_t` and the previous hidden state `h_t-1` through linear transformations (represented by the blue trapezoidal connections and `+` operations) followed by an activation function:

*   **Forget Gate (`f`):** This gate decides what information to discard from the previous cell state `c_t-1`. It takes `x_t` and `h_t-1`, performs an addition (`+`), and then applies a **sigmoid (`σ`) activation function**. The sigmoid squashes the output to a value between 0 and 1. A value closer to 0 means 'forget this entirely,' while a value closer to 1 means 'keep this entirely.'

*   **Input Gate (`i`):** This gate decides what new information from the current input is relevant to store in the cell state. Like the forget gate, it takes `x_t` and `h_t-1`, performs an addition (`+`), and applies a **sigmoid (`σ`) activation function**. This also outputs values between 0 and 1, indicating how much of the new information should be let through.

*   **Cell State Candidate (`g`):** This is where new candidate information for the cell state is generated. It also takes `x_t` and `h_t-1`, performs an addition (`+`), but then applies a **hyperbolic tangent (`tanh`) activation function**. The `tanh` function scales the output to a range between -1 and 1, creating a vector of new potential values to be added to the cell state.

*   **Output Gate (`o`):** This gate controls what parts of the cell state are exposed as the hidden state (the output of the LSTM). It follows the same input processing as the other gates: `x_t` and `h_t-1` are combined via addition (`+`), and then a **sigmoid (`σ`) activation function** is applied.

**3. Updating the Cell State (`c_t`):**
This is where the long-term memory is updated:
*   The previous cell state `c_t-1` is first modified by the forget gate `f` through an **element-wise multiplication (`⊙`)**. This operation selectively 'forgets' parts of the old memory based on the `f` gate's values.
*   Concurrently, the new cell state candidate `g` is modified by the input gate `i` through another **element-wise multiplication (`⊙`)**. This operation selectively 'adds' new information based on the `i` gate's values.
*   Finally, these two results are combined using an **addition (`+`) operation**. The sum of the 'forgotten' previous cell state and the 'added' new information forms the **new cell state, `c_t`**. This `c_t` is one of the final outputs of the LSTM unit.

**4. Calculating the Hidden State (`h_t`):**
This generates the short-term memory or the immediate output of the unit:
*   The newly calculated cell state `c_t` is first passed through a **hyperbolic tangent (`tanh`) activation function**. This scales the cell state's values to be between -1 and 1.
*   This scaled `tanh(c_t)` is then modified by the output gate `o` through an **element-wise multiplication (`⊙`)**. This operation filters the cell state, deciding which parts are relevant to be presented as the current hidden state.
*   The result of this multiplication is the **new hidden state, `h_t`**. This `h_t` is the other final output of the LSTM unit.

In essence, the LSTM uses its sophisticated gating mechanism to carefully control what information enters, leaves, and is stored in its long-term memory (`c_t`), and what is presented as its short-term output (`h_t`).](images/d1441420026e0f254fdd8b81539d7be6a8d6addd3dacc7152ec021f039f646cd.jpg)
Figure 8.13 A single LSTM unit displayed as a computation graph. The inputs to each unit consists of the current input, $x _ { \ast }$ , the previous hidden state, $h _ { t - 1 }$ , and the previous context, $c _ { t - 1 }$ . The outputs are a new hidden state, $h _ { t }$ and an updated context, $c _ { t }$ .

# output gate

The final gate we’ll use is the output gate which is used to decide what information is required for the current hidden state (as opposed to what information needs to be preserved for future decisions).

$$
\begin{array} { r l } { \mathbf { o } _ { t } \ : = \ : \sigma ( \mathbf { U } _ { o } \mathbf { h } _ { t - 1 } + \mathbf { W } _ { o } \mathbf { x } _ { t } ) } & { } \\ { \mathbf { h } _ { t } \ : = \ : \mathbf { o } _ { t } \odot \mathrm { t a n h } ( \mathbf { c } _ { t } ) } & { } \end{array}
$$

Fig. 8.13 illustrates the complete computation for a single LSTM unit. Given the appropriate weights for the various gates, an LSTM accepts as input the context layer, and hidden layer from the previous time step, along with the current input vector. It then generates updated context and hidden vectors as output.

It is the hidden state, $h _ { t }$ , that provides the output for the LSTM at each time step. This output can be used as the input to subsequent layers in a stacked RNN, or at the final layer of a network $h _ { t }$ can be used to provide the final output of the LSTM.

# 8.5.1 Gated Units, Layers and Networks

The neural units used in LSTMs are obviously much more complex than those used in basic feedforward networks. Fortunately, this complexity is encapsulated within the basic processing units, allowing us to maintain modularity and to easily experiment with different architectures. To see this, consider Fig. 8.14 which illustrates the inputs and outputs associated with each kind of unit.

At the far left, (a) is the basic feedforward unit where a single set of weights and a single activation function determine its output, and when arranged in a layer there are no connections among the units in the layer. Next, (b) represents the unit in a simple recurrent network. Now there are two inputs and an additional set of weights to go with it. However, there is still a single activation function and output.

The increased complexity of the LSTM units is encapsulated within the unit itself. The only additional external complexity for the LSTM over the basic recurrent unit (b) is the presence of the additional context vector as an input and output.

This modularity is key to the power and widespread applicability of LSTM units. LSTM units (or other varieties, like GRUs) can be substituted into any of the network architectures described in Section 8.4. And, as with simple RNNs, multi-layered networks making use of gated units can be unrolled into deep feedforward networks and trained in the usual fashion with backpropagation. In practice, therefore, LSTMs rather than RNNs have become the standard unit for any modern system that makes use of recurrent networks.

![## Image Analysis: 9a700ea0f74b1da9e57b3624822da29df14a8ab12cac01c351224dff19267c8b.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates the architectural differences between three fundamental types of neural network units: a basic feedforward unit, a simple recurrent network (SRN) unit, and a Long Short-Term Memory (LSTM) unit. The main purpose is to visually compare and contrast how these units process information, particularly highlighting the introduction of recurrent connections and more complex internal states in SRN and LSTM units, respectively, compared to a basic feedforward neuron. Key ideas communicated are the progression from simple, stateless processing to units with memory and temporal dependencies.

**Content Interpretation:**
The image demonstrates the evolution of neural network unit complexity for handling sequential data.

*   **Unit (a) - Basic Feedforward Unit:** This unit shows a fundamental structure where a single input 'x' is transformed.
    *   'x' is passed to a summation ('Σ'). This typically represents a weighted sum of inputs plus a bias.
    *   The result 'z' (pre-activation) is then passed through an activation function 'g'. This 'g' introduces non-linearity.
    *   The output 'a', which is 'h', is the final result.
    *   **Significance:** This unit is stateless; its output depends solely on the current input 'x'. It lacks memory of past inputs, making it suitable for tasks where inputs are independent. The labels 'x', 'Σ', 'z', 'g', 'a', 'h' are all part of this basic computation flow.

*   **Unit (b) - Simple Recurrent Network (SRN) Unit:** This unit introduces a recurrent connection, enabling it to process sequences.
    *   It takes two inputs: the current input 'x_t' and the previous hidden state 'h_t-1'.
    *   Both 'x_t' and 'h_t-1' are combined in the summation ('Σ') to produce 'z'. This explicitly shows that the unit's current state depends on both current input and its own past state.
    *   'z' then passes through the activation function 'g' to produce 'a'.
    *   The output 'h_t' (which is 'a') is the current hidden state, which will also serve as 'h_t-1' for the next time step.
    *   **Significance:** The 'h_t-1' input is crucial, providing the unit with a form of "memory" about past inputs in the sequence. This structure allows SRNs to model temporal dependencies, making them suitable for tasks like language processing where context matters. The labels 'h_t-1', 'x_t', 'Σ', 'z', 'g', 'a', 'h_t' explicitly outline this recurrent computation.

*   **Unit (c) - Long Short-Term Memory (LSTM) Unit:** This unit is significantly more complex, designed to overcome the vanishing/exploding gradient problems of simpler RNNs and better capture long-range dependencies.
    *   It takes three inputs: the current input 'x_t', the previous hidden state 'h_t-1', and critically, the previous cell state 'c_t-1'.
    *   The entire internal mechanism is abstracted into a single "LSTM Unit" block, implying a sophisticated internal architecture involving various gates (forget, input, and output gates) that control the flow of information into and out of the cell state.
    *   It produces two outputs: the current cell state 'c_t' and the current hidden state 'h_t'. The cell state 'c_t' acts as a long-term memory, carrying information across many time steps.
    *   **Significance:** The introduction of the 'c_t-1' input and 'c_t' output (cell state) along with 'h_t-1' and 'h_t' (hidden state) distinguishes LSTM from SRN. This "cell state" allows LSTMs to retain information over long periods, making them highly effective for very long sequences in natural language processing and speech recognition. The labels 'c_t-1', 'h_t-1', 'x_t' as inputs and 'c_t', 'h_t' as outputs, along with "LSTM Unit" block, provide evidence of this advanced, memory-augmented recurrent architecture.

**Key Insights:**
**Main Takeaways/Lessons:**

1.  **Progression in Neural Unit Complexity:** The image illustrates a clear progression from simple feedforward units to recurrent units, and then to more sophisticated gated recurrent units like LSTMs, each designed to handle more complex data characteristics.
    *   **Evidence:** The visual structure of (a) (one input 'x', one output 'h'), (b) (two inputs 'x_t', 'h_t-1', one output 'h_t'), and (c) (three inputs 'x_t', 'h_t-1', 'c_t-1', two outputs 'h_t', 'c_t') directly demonstrates increasing complexity and number of state variables.

2.  **Introduction of Memory in Recurrent Networks:** Recurrent connections, specifically the feedback loop of the hidden state, are introduced to allow neural networks to process sequences and retain information from previous time steps.
    *   **Evidence:** In unit (b), the input 'h_t-1' (previous hidden state) feeding back into the unit alongside 'x_t' explicitly shows this memory mechanism.

3.  **LSTM's Enhanced Memory Mechanism:** LSTMs are distinguished by an additional internal state (cell state) designed for longer-term memory retention, beyond just the hidden state of SRNs.
    *   **Evidence:** Unit (c) clearly shows inputs 'c_t-1' and outputs 'c_t', which are distinct from the hidden states 'h_t-1' and 'h_t', indicating the presence of a dedicated cell state for managing long-term dependencies. The label "LSTM Unit" itself signifies this specialized architecture.

**Conclusions/Insights:**

*   **Feedforward units are stateless:** They process inputs independently, as shown by 'x' being the sole initial input to unit (a).
*   **SRNs introduce short-term memory:** By feeding 'h_t-1' back into the unit, SRNs can capture dependencies across immediate preceding time steps, evident in unit (b)'s 'h_t-1' input.
*   **LSTMs address long-term dependencies:** The inclusion of 'c_t-1' and 'c_t' (cell state) within the "LSTM Unit" allows for much more effective learning and retention of information over extended sequences, overcoming limitations of simpler RNNs.

**Document Context:**
This image fits within a document section discussing "Gated Units, Layers and Networks," which implies a progression in the design of neural network components, particularly those used for sequential data processing. The figure provides a visual foundation for understanding the building blocks of more complex recurrent and gated architectures, setting the stage for discussions on their functionalities and advantages. It visually supports the theoretical explanation of how basic neurons evolve into units capable of memory and context, crucial for understanding architectures like SRNs and LSTMs that are central to modern deep learning for sequence modeling.

**Summary:**
This image, titled "Figure 8.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM)," presents three diagrams illustrating the fundamental structure of different types of neural network processing units. The overall purpose is to visually compare their architectures, highlighting the increasing complexity and memory capabilities from a basic neuron to an LSTM unit.

**Unit (a) - Basic Feedforward Unit:** This is the simplest unit, representing a standard neuron in a feedforward neural network.
1.  **Input (x):** It receives a single input, denoted by 'x'.
2.  **Summation (Σ):** This input 'x' is fed into a summation component (represented by 'Σ'). This typically involves weighting the input and adding a bias.
3.  **Pre-activation (z):** The output of the summation is an intermediate value 'z'.
4.  **Activation Function (g):** This 'z' is then passed through an activation function 'g', which introduces non-linearity.
5.  **Intermediate Output (a):** The output of the activation function is 'a'.
6.  **Final Output (h):** The unit produces a final output 'h', which is equivalent to 'a'. This unit processes information without retaining any memory of past inputs.

**Unit (b) - Simple Recurrent Network (SRN) Unit:** This unit introduces a recurrent connection, allowing it to process sequences and maintain a form of short-term memory.
1.  **Inputs (h_t-1, x_t):** It receives two inputs:
    *   'x_t': The current input at time step 't'.
    *   'h_t-1': The hidden state (output) from the previous time step 't-1'. This is the recurrent connection providing memory.
2.  **Summation (Σ):** Both 'h_t-1' and 'x_t' are combined in a summation component 'Σ'.
3.  **Pre-activation (z):** The result of this combined summation is 'z'.
4.  **Activation Function (g):** 'z' is then passed through an activation function 'g'.
5.  **Intermediate Output (a):** The output of the activation function is 'a'.
6.  **Final Output (h_t):** The unit produces a current hidden state 'h_t', which is equivalent to 'a'. This 'h_t' will then be fed back as 'h_t-1' to the unit at the next time step.

**Unit (c) - Long Short-Term Memory (LSTM) Unit:** This is a more advanced recurrent unit specifically designed to capture long-range dependencies and overcome challenges like vanishing gradients in simpler recurrent networks.
1.  **Inputs (c_t-1, h_t-1, x_t):** It receives three inputs:
    *   'c_t-1': The cell state from the previous time step 't-1', which acts as a long-term memory.
    *   'h_t-1': The hidden state from the previous time step 't-1'.
    *   'x_t': The current input at time step 't'.
2.  **Internal Processing (LSTM Unit):** All three inputs are fed into a central "LSTM Unit" block. This block represents a complex internal mechanism involving various 'gates' (e.g., forget gate, input gate, output gate) that control the flow of information to update its internal states. The specific details of these gates are abstracted in this diagram.
3.  **Outputs (c_t, h_t):** The "LSTM Unit" produces two outputs for the current time step 't':
    *   'c_t': The updated cell state, which carries long-term information.
    *   'h_t': The updated hidden state, representing the unit's short-term memory and output.

In summary, the image effectively visualizes the architectural evolution from a stateless processing unit to units with increasing capabilities for managing and leveraging information over time, with the LSTM unit representing a highly effective solution for handling long sequences and complex temporal dependencies in data.](images/9a700ea0f74b1da9e57b3624822da29df14a8ab12cac01c351224dff19267c8b.jpg)
Figure 8.14 Basic neural units used in feedforward, simple recurrent networks (SRN), and long short-term memory (LSTM).

# 8.6 Summary: Common RNN NLP Architectures

We’ve now introduced the RNN, seen advanced components like stacking multiple layers and using the LSTM version, and seen how the RNN can be applied to various tasks. Let’s take a moment to summarize the architectures for these applications.

Fig. 8.15 shows the three architectures we’ve discussed so far: sequence labeling, sequence classification, and language modeling. In sequence labeling (for example for part of speech tagging), we train a model to produce a label for each input word or token. In sequence classification, for example for sentiment analysis, we ignore the output for each token, and only take the value from the end of the sequence (and similarly the model’s training signal comes from backpropagation from that last token). In language modeling, we train the model to predict the next word at each token step. In the next section we’ll introduce a fourth architecture, the encoder-decoder.

# 8.7 The Encoder-Decoder Model with RNNs

In this section we introduce a new model, the encoder-decoder model, which is used when we are taking an input sequence and translating it to an output sequence that is of a different length than the input, and doesn’t align with it in a word-to-word way. Recall that in the sequence labeling task, we have two sequences, but they are the same length (for example in part-of-speech tagging each token gets an associated tag), each input is associated with a specific output, and the labeling for that output takes mostly local information. Thus deciding whether a word is a verb or a noun, we look mostly at the word and the neighboring words.

By contrast, encoder-decoder models are used especially for tasks like machine translation, where the input sequence and output sequence can have different lengths and the mapping between a token in the input and a token in the output can be very indirect (in some languages the verb appears at the beginning of the sentence; in other languages at the end). We’ll introduce machine translation in detail in Chapter 13, but for now we’ll just point out that the mapping for a sentence in English to a sentence in Tagalog or Yoruba can have very different numbers of words, and the words can be in a very different order.

![## Image Analysis: e7fcc510551f74294d62020053368489410b8890d7b5333e642bdfa3864bf995.jpg

**Conceptual Understanding:**
This image conceptually represents various architectural patterns of Recurrent Neural Networks (RNNs) as applied to Natural Language Processing (NLP) tasks. Its main purpose is to visually differentiate how RNNs are structured and employed to achieve distinct objectives in processing sequences of text or other sequential data. The image communicates the core ideas of sequence-level and token-level processing, as well as the concept of an intermediate 'context' representation for more complex sequence transformations. It serves as a graphical overview of foundational RNN models in NLP.

**Content Interpretation:**
The image systematically presents four fundamental RNN-based architectures for handling sequence data in NLP. It demonstrates how the input-output mapping within an RNN can be configured to address different computational goals:

*   **a) sequence labeling:** This architecture shows a one-to-one mapping where each input token (xᵢ) directly corresponds to an output token (yᵢ). This is crucial for tasks like Part-of-Speech tagging or Named Entity Recognition, where each word in a sequence requires a specific label.
*   **b) sequence classification:** Here, an entire sequence of input tokens (x₁ through xₙ) is processed by the RNN to produce a single aggregated output (y). This structure is ideal for tasks such as sentiment analysis or spam detection, where the goal is to classify the entire input sequence into a single category.
*   **c) language modeling:** This architecture predicts the next token in a sequence given the previous ones. The inputs are x₁ through xₜ₋₁, and the outputs are x₂ through xₜ, where xᵢ₊₁ is predicted based on xᵢ. This is fundamental for tasks like text generation or auto-completion, learning the probability distribution of sequences.
*   **d) encoder-decoder:** This model consists of two separate RNNs: an 'Encoder RNN' and a 'Decoder RNN'. The 'Encoder RNN' processes an input sequence (x₁ through xₙ) and condenses it into an intermediate representation called 'Context'. The 'Context' then serves as the initial state or input for the 'Decoder RNN', which generates an output sequence (y₁ through yₘ). This architecture is critical for sequence-to-sequence tasks like machine translation, where input and output sequences can have different lengths and direct one-to-one mapping is not suitable.

**Key Insights:**
The main takeaways from this image are:

1.  **Versatility of RNNs for Sequence Data:** RNNs can be flexibly configured to handle various Natural Language Processing tasks, demonstrating their fundamental role in processing sequential information. This is evidenced by the distinct architectures presented for 'sequence labeling', 'sequence classification', 'language modeling', and 'encoder-decoder'.
2.  **Task-Specific RNN Architectures:** Different NLP tasks necessitate distinct input-output relationships within an RNN. For example, 'sequence labeling' (a) shows a many-to-many output where each input xᵢ yields an output yᵢ. 'Sequence classification' (b) demonstrates a many-to-one output where a sequence of inputs x₁...xₙ yields a single output y. 'Language modeling' (c) illustrates a shift-and-predict mechanism where inputs x₁...xₜ₋₁ predict outputs x₂...xₜ. The 'encoder-decoder' model (d) exhibits a many-to-context-to-many structure, allowing for flexible input and output sequence lengths.
3.  **Introduction of 'Context' in Encoder-Decoder:** The encoder-decoder model explicitly introduces an intermediate 'Context' representation, which serves to summarize the entire input sequence before the decoder generates the output. This is a critical insight for understanding how complex sequence-to-sequence transformations, such as machine translation, are managed by decoupling the encoding and decoding processes, as clearly labeled by 'Encoder RNN', 'Context', and 'Decoder RNN'.

**Document Context:**
This image directly supports Section 8.7, titled 'The Encoder-Decoder Model with RNNs,' by visually explaining the encoder-decoder architecture in the context of other standard RNN configurations for NLP. It serves as a foundational visual aid to understand how RNNs are adapted for different sequence processing tasks. The image clarifies the distinction between different NLP paradigms—from direct token-to-token mapping (sequence labeling) to sequence-to-single-class mapping (sequence classification), to predicting subsequent tokens (language modeling), and finally, to the more complex sequence-to-sequence mapping via an intermediate 'Context' as presented in the encoder-decoder model. The image sets the stage for a deeper discussion on the encoder-decoder model by showing its place among other common RNN applications.

**Summary:**
The image illustrates four distinct Recurrent Neural Network (RNN) architectures commonly used in Natural Language Processing (NLP) tasks: sequence labeling, sequence classification, language modeling, and encoder-decoder. Each diagram visually represents the flow of input tokens (x) through an RNN and the resulting output tokens or classes (y), highlighting the structural differences tailored for various NLP challenges. The descriptions for each model clearly define its purpose and the relationship between its inputs and outputs. The image is structured into a 2x2 grid, with each cell showing a different architecture labeled a) through d).](images/e7fcc510551f74294d62020053368489410b8890d7b5333e642bdfa3864bf995.jpg)
Figure 8.15 Four architectures for NLP tasks. In sequence labeling (POS or named entity tagging) we map each input token $x _ { i }$ to an output token $y _ { i }$ . In sequence classification we map the entire input sequence to a single class. In language modeling we output the next token conditioned on previous tokens. In the encoder model we have two separate RNN models, one of which maps from an input sequence $\pmb { \times }$ to an intermediate representation we call the context, and a second of which maps from the context to an output sequence y.

Encoder-decoder networks, sometimes called sequence-to-sequence networks, are models capable of generating contextually appropriate, arbitrary length, output sequences given an input sequence. Encoder-decoder networks have been applied to a very wide range of applications including summarization, question answering, and dialogue, but they are particularly popular for machine translation.

The key idea underlying these networks is the use of an encoder network that takes an input sequence and creates a contextualized representation of it, often called the context. This representation is then passed to a decoder which generates a taskspecific output sequence. Fig. 8.16 illustrates the architecture. Encoder-decoder networks consist of three conceptual components:

1. An encoder that accepts an input sequence, $x _ { 1 : n }$ , and generates a corresponding sequence of contextualized representations, $h _ { 1 : n }$ . LSTMs, convolutional networks, and transformers can all be employed as encoders. 2. A context vector, $c$ , which is a function of $h _ { 1 : n }$ , and conveys the essence of the input to the decoder. 3. A decoder, which accepts $c$ as input and generates an arbitrary length sequence of hidden states $h _ { 1 : m }$ , from which a corresponding sequence of output states $y _ { 1 : m }$ , can be obtained. Just as with encoders, decoders can be realized

![## Image Analysis: c3e81cf2ccb4ae01edb2a5a3b7168df32bd69d3c23d5675771592df3e23103c0.jpg

**Conceptual Understanding:**
The image conceptually represents the fundamental architecture of an encoder-decoder model, often used in sequence-to-sequence tasks, particularly with Recurrent Neural Networks (RNNs). Its main purpose is to illustrate how an input sequence is transformed into an output sequence via an intermediate 'context' representation. The key idea communicated is the separation of concerns: an 'Encoder' compresses the input information into a meaningful context vector, and a 'Decoder' then generates the desired output sequence from that context, allowing for mapping between sequences of potentially different lengths.

**Content Interpretation:**
The image illustrates the core components and data flow of an encoder-decoder architecture, specifically designed for sequence-to-sequence tasks. It shows how an input sequence of variable length, represented by 'x₁, x₂, ..., xₙ', is processed by an 'Encoder' module. This 'Encoder' compresses the entire input sequence into a fixed-size or variable-size intermediate representation called the 'Context' vector. This 'Context' vector is then transmitted to a 'Decoder' module. The 'Decoder' takes this 'Context' as its initial state or as an input at each step to generate an output sequence, denoted as 'y₁, y₂, ..., yₘ', which can also be of variable length. The significance lies in the ability to map an input sequence to an output sequence, even if their lengths differ, by first encoding the input into a meaningful context and then decoding from that context.

**Key Insights:**
The main takeaway from this image is the modular and sequential nature of the encoder-decoder architecture. It highlights that: 1. Input sequences ('x₁, x₂, ..., xₙ') are first processed entirely by an 'Encoder'. 2. The 'Encoder' distills the information from the input into a compact 'Context' representation. 3. This 'Context' is then used by the 'Decoder' to generate an output sequence ('y₁, y₂, ..., yₘ'). The diagram clearly shows the unidirectional flow of information, from input to encoder, then to context, then to decoder, and finally to output. The ellipsis (...) for both inputs and outputs indicates that both sequences can have variable lengths, which is a key characteristic of this model. The labels 'Encoder', 'Context', and 'Decoder' explicitly name the three functional stages, providing concrete evidence for the process flow.

**Document Context:**
This image directly supports Section 8.7 of the document, titled 'The Encoder-Decoder Model with RNNs', by visually representing the core architecture discussed. The accompanying text, 'Figure 8.16 The encoder-decoder architecture. The context is a function of the hidden representations of the input, and may be used by the decoder in a variety of ways,' explicitly refers to this diagram. The image serves as a foundational visual aid to understand how an RNN-based encoder-decoder model processes input sequences to generate output sequences, emphasizing the crucial role of the 'Context' vector in bridging the encoder and decoder components. It demonstrates the conceptual flow that enables tasks like machine translation or text summarization.

**Summary:**
The image displays the fundamental architecture of an encoder-decoder model, which is central to many sequence-to-sequence tasks in machine learning. The process begins with an 'Encoder' component that receives a sequence of inputs, denoted as x₁, x₂, ..., up to xₙ. This 'Encoder' processes these inputs sequentially to generate a 'Context' vector. This 'Context' then serves as the sole input or a critical piece of information for the subsequent 'Decoder' component. The 'Decoder', in turn, utilizes this 'Context' to generate an output sequence, denoted as y₁, y₂, ..., up to yₘ. The arrows clearly illustrate the flow: inputs feed into the Encoder, the Encoder produces the Context, the Context is passed to the Decoder, and the Decoder produces the final outputs.](images/c3e81cf2ccb4ae01edb2a5a3b7168df32bd69d3c23d5675771592df3e23103c0.jpg)
Figure 8.16 The encoder-decoder architecture. The context is a function of the hidden representations of the input, and may be used by the decoder in a variety of ways.

by any kind of sequence architecture.

In this section we’ll describe an encoder-decoder network based on a pair of RNNs, but we’ll see in Chapter 13 how to apply them to transformers as well. We’ll build up the equations for encoder-decoder models by starting with the conditional RNN language model $p ( y )$ , the probability of a sequence $y$ .

Recall that in any language model, we can break down the probability as follows:

$$
p ( y ) \ = \ p ( y _ { 1 } ) p ( y _ { 2 } | y _ { 1 } ) p ( y _ { 3 } | y _ { 1 } , y _ { 2 } ) \ldots p ( y _ { m } | y _ { 1 } , . . . , y _ { m - 1 } )
$$

In RNN language modeling, at a particular time $t$ , we pass the prefix of $t - 1$ tokens through the language model, using forward inference to produce a sequence of hidden states, ending with the hidden state corresponding to the last word of the prefix. We then use the final hidden state of the prefix as our starting point to generate the next token.

More formally, if $g$ is an activation function like tanh or ReLU, a function of the input at time $t$ and the hidden state at time $t - 1$ , and the softmax is over the set of possible vocabulary items, then at time $t$ the output ${ \bf y } _ { t }$ and hidden state $\mathbf { h } _ { t }$ are computed as:

$$
\begin{array} { r c l } { \mathbf { h } _ { t } } & { = } & { g ( \mathbf { h } _ { t - 1 } , \mathbf { x } _ { t } ) } \\ { \hat { \mathbf { y } } _ { t } } & { = } & { \operatorname { s o f t m a x } ( \mathbf { h } _ { t } ) } \end{array}
$$

We only have to make one slight change to turn this language model with autoregressive generation into an encoder-decoder model that is a translation model that can translate from a source text in one language to a target text in a second: add a sentence separation marker at the end of the source text, and then simply concatenate the target text.

Let’s use ${ \tt < s > }$ for our sentence separator token, and let’s think about translating an English source text (“the green witch arrived”), to a Spanish sentence (“llego´ la bruja verde” (which can be glossed word-by-word as ‘arrived the witch green’). We could also illustrate encoder-decoder models with a question-answer pair, or a text-summarization pair.

Let’s use $x$ to refer to the source text (in this case in English) plus the separator token ${ \tt < s > }$ , and $y$ to refer to the target text $y$ (in this case in Spanish). Then an encoder-decoder model computes the probability $p ( y | x )$ as follows:

$$
p ( y | x ) \ = \ p ( y _ { 1 } | x ) p ( y _ { 2 } | y _ { 1 } , x ) p ( y _ { 3 } | y _ { 1 } , y _ { 2 } , x ) \ldots p ( y _ { m } | y _ { 1 } , . . . , y _ { m - 1 } , x )
$$

Fig. 8.17 shows the setup for a simplified version of the encoder-decoder model (we’ll see the full model, which requires the new concept of attention, in the next section).

![## Image Analysis: 9d4b3afb1621b973928eb995d132ccd3abeccf7b901b98da5f488a86d22b7528.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and operation of a basic Recurrent Neural Network (RNN) based Encoder-Decoder model used for sequence-to-sequence tasks, specifically machine translation. The main purpose is to illustrate how a source sentence is processed by an encoder to form a context vector, and then how a decoder uses this context vector, along with previously generated words, to produce a target sentence in a different language. The key ideas communicated are:
*   The sequential processing of words by RNNs.
*   The concept of an encoder summarizing the source input into a fixed-size context (the `h_n` vector).
*   The autoregressive nature of the decoder, where each new word generation depends on the context from the encoder and the previously generated words.
*   The distinction between the encoder's role (processing source) and the decoder's role (generating target).

**Content Interpretation:**
The image clearly shows a pipeline for machine translation:

*   **Encoder:** The first four sets of 'embedding layer' -> 'hidden layer(s)' -> 'softmax' units represent the encoder. The 'Source Text' labels 'the', 'green', 'witch', 'arrived' are inputs, showing the English sentence being encoded. The annotation '(output of source is ignored)' directly supports the interpretation that the encoder's primary role is to process the input and generate a final hidden state, not intermediate word predictions.
*   **Context Vector:** The green box labeled 'h_n' for 'arrived' signifies the final hidden state of the encoder. This `h_n` acts as the context vector that encapsulates the meaning of the entire source sentence, which is then passed to the decoder.
*   **Separator Token:** The '<s/>' token, explicitly labeled 'Separator', is an intermediate input, signaling the transition from encoding to decoding. It ensures the model understands the boundaries between the source and target sequences.
*   **Decoder:** The subsequent sets of 'embedding layer' -> 'hidden layer(s)' -> 'softmax' units constitute the decoder. The inputs for the decoder (after the separator) are the words of the 'Target Text' ('llegó', 'la', 'bruja', 'verde'). The red text above the softmax outputs ('llegó', 'la', 'bruja', 'verde', '</s/>') indicates the predicted translated words in Spanish.
*   **Autoregressive Generation:** The dashed lines connecting the predicted output words (e.g., 'llegó') back to the subsequent input of the embedding layer for the decoder (e.g., 'llegó') illustrate the autoregressive nature. This means the decoder generates one word at a time, and each generated word becomes an input for predicting the next word, enabling the generation of a complete sequence. The '</s/>' output signifies the end of the translated sentence.
*   **Layers:** The labels 'embedding layer', 'hidden layer(s)', and 'softmax' describe the standard components of an RNN-based neural network. The 'embedding layer' converts discrete words into dense vector representations. The 'hidden layer(s)' (RNN cells) process sequential information, maintaining a hidden state that captures context. The 'softmax' layer outputs a probability distribution over the vocabulary for the next word prediction.

**Key Insights:**
Main takeaways and insights:
*   **Sequence-to-Sequence Translation:** The image clearly demonstrates the fundamental architecture for sequence-to-sequence translation, where an input sequence (English sentence) is transformed into an output sequence (Spanish sentence). The labels 'Source Text' and 'Target Text' provide direct evidence for this.
*   **Encoder-Decoder Paradigm:** It illustrates how the encoder first processes the entire source sentence to capture its meaning into a fixed-size representation ('h_n'), and then the decoder uses this representation to generate the target sentence. The green 'h_n' box and the flow of information from the encoder's last hidden state to the decoder's first states are key evidence.
*   **Autoregressive Decoding:** The generation of the target sentence is autoregressive, meaning each word is generated based on the encoder's context and the words already generated by the decoder. The dashed lines showing the feedback loop from softmax output to embedding input for 'llegó', 'la', 'bruja', 'verde' confirm this.
*   **Role of Separator Token:** The '<s/>' 'Separator' token is crucial for delineating the end of the source sentence and the beginning of the target sentence generation phase, effectively bridging the encoder and decoder.
*   **Intermediate Output Handling:** The explicit annotation '(output of source is ignored)' highlights a specific design choice in this basic RNN encoder-decoder, where intermediate outputs from the encoder are not used, relying solely on the final hidden state.

**Document Context:**
This image fits directly within the '8.7 The Encoder-Decoder Model with RNNs' section of the document. It serves as a visual aid to explain the inference process (translating a single sentence) of the basic RNN-based encoder-decoder approach to machine translation. The accompanying text describes the English source text ('the green witch arrived'), the separator token ('<s/>'), and the Spanish target text ('llego la bruja verde'), all of which are explicitly depicted in the diagram, thus making the image a direct illustration of the theoretical concepts discussed. It clarifies how source text is processed to generate hidden states, and how autoregressive generation then produces the target translation.

**Summary:**
This diagram illustrates a basic Recurrent Neural Network (RNN) based Encoder-Decoder model performing machine translation during inference, meaning it's translating a sentence. The process is divided into two main phases: encoding the source sentence and decoding (generating) the target sentence.

**1. Encoding the Source Text:**
At the bottom left, the 'Source Text' 'the green witch arrived' is shown. Each word, starting with 'the,' is fed sequentially into an 'embedding layer' (represented by three red circles). This layer converts each word into a numerical vector representation. These vector embeddings are then passed upwards into the 'hidden layer(s)' (light blue rectangular boxes). Each hidden layer unit processes its current word's embedding along with the hidden state passed horizontally from the previous word's processing. As the encoder processes each word, it generates an internal representation. Although these hidden units also connect to a 'softmax' layer (represented by bar graphs at the top), the diagram explicitly notes that the '(output of source is ignored)' during this phase. The key outcome of the encoder is the final hidden state, labeled 'h_n' (the green box, after 'arrived'), which effectively summarizes the entire source sentence's meaning into a single context vector.

**2. Separator Token:**
After processing the 'Source Text,' a special '<s/>' token, explicitly labeled 'Separator,' is introduced. This token is also processed through the embedding and hidden layers. It acts as a bridge, signaling the transition from the source sentence encoding to the target sentence generation. The `h_n` context vector from the encoder is passed to this separator's hidden unit, providing the initial context for the decoder.

**3. Decoding the Target Text:**
The second major phase is the 'Target Text' generation, shown on the right side. The decoder operates autoregressively, meaning it generates one word at a time, and each generated word is then used as input to predict the next.
*   The first word 'llegó' (the Spanish translation for 'arrived') is implicitly predicted using the context from the separator token's hidden state.
*   This predicted 'llegó' (shown in red above the softmax output) is then fed back into the 'embedding layer' as input for the next time step (indicated by the dashed arrow connecting the output 'llegó' to the input 'llegó').
*   This process continues: the embedding of 'llegó' goes into the next hidden layer unit, which produces an output via the softmax layer, predicting 'la' (for 'the').
*   This predicted 'la' is then fed back as input for the next step, leading to the prediction of 'bruja' (for 'witch').
*   Similarly, 'bruja' leads to 'verde' (for 'green').
*   Finally, 'verde' leads to the generation of the end-of-sentence token '</s/>', which signals that the translation is complete.

In essence, the model reads the English sentence, compresses its meaning into a context (`h_n`), and then unrolls that context, word by word, to produce the Spanish translation. The dashed lines highlight how the decoder uses its own previous predictions as input to generate the subsequent words in the target language.](images/9d4b3afb1621b973928eb995d132ccd3abeccf7b901b98da5f488a86d22b7528.jpg)
Figure 8.17 Translating a single sentence (inference time) in the basic RNN version of encoder-decoder approach to machine translation. Source and target sentences are concatenated with a separator token in between, and the decoder uses context information from the encoder’s last hidden state.   
Fig. 8.17 shows an English source text (“the green witch arrived”), a sentence separator token ( $\ S >$ , and a Spanish target text (“llego la bruja verde ´ ”). To translate a source text, we run it through the network performing forward inference to generate hidden states until we get to the end of the source. Then we begin autoregressive generation, asking for a word in the context of the hidden layer from the end of the source input as well as the end-of-sentence marker. Subsequent words are conditioned on the previous hidden state and the embedding for the last word generated.

Let’s formalize and generalize this model a bit in Fig. 8.18. (To help keep things straight, we’ll use the superscripts $e$ and $d$ where needed to distinguish the hidden states of the encoder and the decoder.) The elements of the network on the left process the input sequence $x$ and comprise the encoder. While our simplified figure shows only a single network layer for the encoder, stacked architectures are the norm, where the output states from the top layer of the stack are taken as the final representation, and the encoder consists of stacked biLSTMs where the hidden states from top layers from the forward and backward passes are concatenated to provide the contextualized representations for each time step.

The entire purpose of the encoder is to generate a contextualized representation of the input. This representation is embodied in the final hidden state of the encoder, $ { \mathbf { h } } _ { n } ^ { e }$ . This representation, also called c for context, is then passed to the decoder.

The simplest version of the decoder network would take this state and use it just to initialize the first hidden state of the decoder; the first decoder RNN cell would use $c$ as its prior hidden state $\boldsymbol { \mathsf { h } } _ { 0 } ^ { d }$ . The decoder would then autoregressively generates a sequence of outputs, an element at a time, until an end-of-sequence marker is generated. Each hidden state is conditioned on the previous hidden state and the output generated in the previous state.

As Fig. 8.18 shows, we do something more complex: we make the context vector $c$ available to more than just the first decoder hidden state, to ensure that the influence of the context vector, $c$ , doesn’t wane as the output sequence is generated. We do this by adding $c$ as a parameter to the computation of the current hidden state. using the following equation:

$$
\mathbf { h } _ { t } ^ { d } = g ( \widehat { y } _ { t - 1 } , \mathbf { h } _ { t - 1 } ^ { d } , \mathbf { c } )
$$

![## Image Analysis: 823b89fdba5d8f1ee8b9329b871f94f4b1e577caecc8d23ec20266b6bf15c7c0.jpg

**Conceptual Understanding:**
This image represents the fundamental architecture of an RNN-based encoder-decoder model used for sequence-to-sequence tasks, such as machine translation, at inference time. Conceptually, it illustrates how an input sequence is first 'encoded' into a fixed-dimensional context vector, which then serves as the memory for the 'decoder' to 'decode' or generate an output sequence. The main purpose is to demonstrate the flow of information from the input tokens through the encoder, its transformation into a context vector, and the subsequent generation of output tokens by the decoder, highlighting the auto-regressive nature of the decoder and the constant provision of the context vector.

**Content Interpretation:**
This image details the internal workings of a basic RNN-based encoder-decoder neural network architecture. It illustrates the sequential processing of an input sequence by the 'Encoder' to condense its information into a fixed-size 'context vector'. Subsequently, it shows how the 'Decoder' uses this context vector, along with previously generated output tokens, to generate a new output sequence token by token. The diagram emphasizes the flow of information, specifically how the encoder's final hidden state ('h^e_n') becomes the initial hidden state for the decoder ('h^d_0') and how this context ('c') is shared with all subsequent decoder steps.

**Key Insights:**
The main takeaways from this image are: 
1.  **Sequence-to-Sequence Translation:** The model is designed for mapping an input sequence (x_1 to x_n) to an output sequence (y_1 to y_m), a common task in machine translation. 
2.  **Encoder's Role:** The 'Encoder' processes the entire input sequence and compresses its meaning into a single 'context vector' (c = h^e_n). The outputs generated by the encoder's 'softmax' layer are 'ignored during encoding', meaning only the final hidden state matters. 
3.  **Context Vector Bridge:** The 'context vector' (c), which is the encoder's final hidden state (h^e_n), serves as the initial hidden state for the decoder (h^d_0), acting as a crucial information bridge. 
4.  **Decoder's Auto-Regressive Nature:** The 'Decoder' generates output tokens one by one. Each predicted output (y_i) from a time step feeds back as an input to the next time step, making it an auto-regressive model. This is evidenced by the dashed lines connecting outputs y_1, y_2, y_3 to the subsequent inputs. 
5.  **Persistent Context:** The context vector 'c' (h^e_n) is not only the initial hidden state for the decoder but is 'made available to each decoder hidden state' (h^d_1 through h^d_m), indicating it provides continuous contextual information throughout the decoding process. 
6.  **Start and End Tokens:** The use of '<s' (start-of-sequence) and '</s' (end-of-sequence) tokens for the decoder's input and output respectively, signifies the boundaries of the generated sequence.

**Document Context:**
This image directly supports Section 8.7, 'The Encoder-Decoder Model with RNNs', by providing a detailed visual representation of the architecture described. The text following the image explicitly refers to it, stating: 'Figure 8.18 A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, $h _ { n } ^ { e }$ , serves as the context for the decoder in its role as $h _ { 0 } ^ { d }$ in the decoder RNN, and is also made available to each decoder hidden state.' The diagram perfectly aligns with this description, clarifying the roles of the encoder's final hidden state, the context vector, and the decoder's initial and subsequent hidden states.

**Summary:**
This image illustrates the architecture of a basic Recurrent Neural Network (RNN)-based encoder-decoder model used for tasks like sequence translation, specifically depicting the inference process. The model is divided into two main components: the Encoder and the Decoder. 

**The Encoder:** This part of the network processes the input sequence, denoted by x_1, x_2, x_3, up to x_n. Each input token first passes through an 'embedding layer', which converts discrete tokens into dense vector representations. These embeddings are then fed into the 'hidden layer(s)' of the RNN. The RNN sequentially processes these inputs, generating hidden states h^e_1, h^e_2, h^e_3, up to h^e_n. During the encoding phase, any 'softmax' output generated by the encoder's hidden layers is explicitly '(output is ignored during encoding)'. The crucial output of the encoder is its final hidden state, h^e_n, which is then passed on as the 'context vector' (c). This context vector 'c' is explicitly stated as being equal to h^d_0, indicating it serves as the initial hidden state for the decoder. 

**The Decoder:** This part of the network is responsible for generating the output sequence, y_1, y_2, y_3, up to y_m. The decoder is initialized with the context vector 'c' (which is h^d_0 from the encoder). Its first input is a special 'start-of-sequence' token, '<s'. The decoder then iteratively generates output tokens. At each step, it receives an input (either '<s' initially, or the previous predicted output y_i for subsequent steps) and its previous hidden state. It computes a new hidden state (h^d_1, h^d_2, h^d_3, h^d_4, up to h^d_m). Crucially, the context vector 'c' (or h^e_n) is continuously provided as an input to *each* of the decoder's hidden states (h^d_1, h^d_2, h^d_3, h^d_4, h^d_m), as indicated by the multiple connecting lines from the 'h^e_n = c = h^d_0' box. From each hidden state, a 'softmax' layer produces a probability distribution over the vocabulary, from which the next output token (y_1, y_2, y_3, y_4) is predicted. This auto-regressive process continues until the decoder generates an 'end-of-sequence' token, '</s'. The dashed lines connecting the outputs y_1, y_2, y_3, y_4, and </s to the subsequent decoder inputs illustrate this auto-regressive nature, where the predicted output at time t becomes the input at time t+1.](images/823b89fdba5d8f1ee8b9329b871f94f4b1e577caecc8d23ec20266b6bf15c7c0.jpg)
Figure 8.18 A more formal version of translating a sentence at inference time in the basic RNN-based encoder-decoder architecture. The final hidden state of the encoder RNN, $h _ { n } ^ { e }$ , serves as the context for the decoder in its role as $h _ { 0 } ^ { d }$ in the decoder RNN, and is also made available to each decoder hidden state.

Now we’re ready to see the full equations for this version of the decoder in the basic encoder-decoder model, with context available at each decoding timestep. Recall that $g$ is a stand-in for some flavor of RNN and $\hat { y } _ { t - 1 }$ is the embedding for the output sampled from the softmax at the previous step:

$$
\begin{array} { r l } & { \mathbf { c \lambda } = \mathbf { \lambda } \mathbf { h } _ { n } ^ { e } } \\ & { \mathbf { h } _ { 0 } ^ { d } \ = \mathbf { \lambda } \mathbf { c } } \\ & { \mathbf { h } _ { t } ^ { d } \ = \ g ( \hat { y } _ { t - 1 } , \mathbf { h } _ { t - 1 } ^ { d } , \mathbf { c } ) } \\ & { \hat { \mathbf { y } } _ { t } \ = \ \mathrm { s o f t m a x } ( \mathbf { h } _ { t } ^ { d } ) } \end{array}
$$

Thus $\hat { \mathbf { y } } _ { t }$ is a vector of probabilities over the vocabulary, representing the probability of each word occurring at time $t$ . To generate text, we sample from this distribution $\hat { \mathbf { y } } _ { t }$ . For example, the greedy choice is simply to choose the most probable word to generate at each timestep. We’ll introduce more sophisticated sampling methods in Section 10.2.

# 8.7.1 Training the Encoder-Decoder Model

Encoder-decoder architectures are trained end-to-end. Each training example is a tuple of paired strings, a source and a target. Concatenated with a separator token, these source-target pairs can now serve as training data.

For MT, the training data typically consists of sets of sentences and their translations. These can be drawn from standard datasets of aligned sentence pairs, as we’ll discuss in Section 13.2.2. Once we have a training set, the training itself proceeds as with any RNN-based language model. The network is given the source text and then starting with the separator token is trained autoregressively to predict the next word, as shown in Fig. 8.19.

Note the differences between training (Fig. 8.19) and inference (Fig. 8.17) with respect to the outputs at each time step. The decoder during inference uses its own estimated output $\hat { y _ { t } }$ as the input for the next time step $x _ { t + 1 }$ . Thus the decoder will tend to deviate more and more from the gold target sentence as it keeps generating more tokens. In training, therefore, it is more common to use teacher forcing in the decoder. Teacher forcing means that we force the system to use the gold target token from training as the next input $x _ { t + 1 }$ , rather than allowing it to rely on the (possibly erroneous) decoder output $\hat { y _ { t } }$ . This speeds up training.

![## Image Analysis: c3fb67a70d9a29189abf483a0d4b7fa2ca54d28a20ad5e587814067da64b727c.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and training process of a Recurrent Neural Network (RNN) encoder-decoder model specifically applied to machine translation.

The main purpose of the image is to illustrate how a sequence-to-sequence model translates an input sequence (source language sentence) into an output sequence (target language sentence) and, crucially, how the training loss is computed using a technique called "teacher forcing." It conveys the idea of breaking down a complex translation task into sequential processing steps and calculating a granular loss at each step to guide the model's learning.

Key ideas communicated include:
*   **Sequence-to-sequence architecture:** The distinct Encoder and Decoder components.
*   **Recurrent processing:** How information is processed sequentially over time steps using RNN units.
*   **Embeddings:** Representing words as numerical vectors.
*   **Teacher forcing:** Using true target words as inputs during training to stabilize and speed up learning.
*   **Cross-entropy loss:** The specific method used to quantify the difference between predicted probabilities and true target words, calculated per word.
*   **Total loss calculation:** Averaging per-word losses to get an overall sentence loss.

**Content Interpretation:**
The image clearly shows the process of training an encoder-decoder neural network for machine translation.

**Processes Being Shown:**
*   **Encoding:** The left part, labeled "Encoder", demonstrates the process of taking a source sentence ("the green witch arrived") and converting it into a fixed-size context representation. Each word ("the", "green", "witch", "arrived") is first passed through an "embedding layer" to create vector representations (x_1, x_2, x_3, x_4), which are then fed sequentially into a series of RNN units (blue rectangles). The sequential arrows between these units indicate the recurrent nature of processing, where information from previous words influences the processing of subsequent words.
*   **Decoding:** The right part, labeled "Decoder", shows the generation of the target sentence ("llegó la bruja verde < /s >") word by word. This process starts with the context vector from the Encoder. For each step, an input word (e.g., "< s >", "llegó", "la", "bruja", "verde") is passed through an "embedding layer" and then fed into an RNN unit. The RNN unit, in conjunction with its hidden state from the previous step ("hidden layer(s)"), produces an output which is then transformed into a probability distribution over the vocabulary via a "softmax" layer (represented by the bar chart symbol and labeled "softmax ŷ").
*   **Loss Calculation:** For each generated target word, a "per-word loss" (L_1 through L_5) is computed. The red boxes explicitly state the formula: "L_i = -log P(y_i)". This is the negative log-likelihood of the gold answer y_i, which is the standard cross-entropy loss. The overall "Total loss" for the sentence, "L = (1/T) Σ (from i=1 to T) L_i", is the average of these individual per-word losses. This average loss is what the model tries to minimize during training.
*   **Teacher Forcing:** The crucial relationship between the "gold answers" ("llegó", "la", "bruja", "verde", "< /s >") and the inputs to the Decoder's embedding layer (e.g., "< s >" as input for y_1, "llegó" as input for y_2, etc.) indicates the use of teacher forcing. This means that during training, the correct previous target word is fed as input to the decoder at the current time step, rather than the decoder's own potentially incorrect prediction.

**Significance of Information Presented:**
*   The use of "x_i" for source words and "y_i" for target words clearly distinguishes between input and output sequences.
*   The "gold answers" label emphasizes that these are the true, correct translations the model aims to reproduce.
*   The "per-word loss" values and the "Total loss" formula highlight the objective function being optimized. Minimizing this loss means increasing the probability P(y_i) for the correct target words, thus improving translation accuracy.
*   The "softmax ŷ" label explicitly indicates the output layer that converts the RNN's hidden state into a probability distribution over possible next words, from which the most likely word can be chosen (or used to calculate loss against the true word).
*   "embedding layer" and "hidden layer(s)" denote fundamental components of neural network architectures, crucial for representing words and maintaining sequential context.

**Key Insights:**
The main takeaways and insights from this image revolve around the mechanics of training an RNN encoder-decoder model for machine translation, particularly the loss computation and the role of teacher forcing.

*   **Key Takeaway 1: Architecture of Encoder-Decoder Model.** The model consists of two distinct parts, an "Encoder" for processing the input sequence and a "Decoder" for generating the output sequence. The Encoder compresses the input "the green witch arrived" into a context vector, and the Decoder then uses this context to generate the output "llegó la bruja verde < /s >".
*   **Key Takeaway 2: Sequential Processing with RNNs.** Both the Encoder and Decoder utilize RNN units (blue rectangles) that process words sequentially. Information flows from one time step to the next (indicated by rightward arrows between RNN units), allowing the model to capture dependencies and context within the sentences. The labels "hidden layer(s)" next to the blue rectangles signify that these units maintain an internal state that evolves over time.
*   **Key Takeaway 3: Loss Calculation for Training.** Training involves minimizing a "Total loss" which is calculated as "the average cross-entropy loss per target word: L = (1/T) Σ (from i=1 to T) L_i". Each "per-word loss" (e.g., "L_1 = -log P(y_1)") quantifies how well the model's predicted probability for the correct target word matches the actual "gold answers". This directly shows how the model is penalized for incorrect predictions, guiding its learning process.
*   **Key Takeaway 4: Teacher Forcing for Decoder Training.** The diagram explicitly shows that the "gold answers" are used as inputs to the Decoder's "embedding layer" for subsequent steps. For instance, "llegó" (the gold answer for y_1) is fed as the input for the Decoder unit generating y_2, and "la" (gold answer for y_2) is fed for y_3. This mechanism, known as teacher forcing (and confirmed by the accompanying text), is crucial for stabilizing and accelerating the training of recurrent neural networks by providing correct context at each step, preventing the model from spiraling off due to its own early errors.
*   **Key Takeaway 5: Role of Word Embeddings and Softmax.** "embedding layer" is shown as the initial processing step for both source and target words, indicating that words are first converted into dense vector representations. The "softmax ŷ" layer transforms the RNN's raw output into a probability distribution over the entire vocabulary, which is essential for selecting the most probable next word and for calculating the cross-entropy loss.

**Document Context:**
This image is highly relevant to Section 8.7.1, titled "Training the Encoder-Decoder Model," as it visually demonstrates the core process of training such a model for machine translation. The figure perfectly complements the surrounding text by providing a concrete, step-by-step diagram of how an input sentence is processed, how an output sentence is generated, and how the critical loss function is computed during the training phase. The accompanying text explicitly mentions "teacher forcing" and the computation of softmax output distribution for loss, both of which are clearly illustrated in the diagram, making the visual and textual explanations mutually reinforcing for understanding the training mechanics.

**Summary:**
This diagram illustrates the training process of a basic Recurrent Neural Network (RNN) encoder-decoder model designed for machine translation. It shows how a source sentence is processed, a target sentence is generated, and how the model's performance is measured using a loss function.

The entire process is divided into two main components: the **Encoder** and the **Decoder**.

**1. The Encoder:**
*   **Input Sentence:** The Encoder processes the source language sentence, "the green witch arrived," word by word.
*   **Word Embeddings:** Each word, such as "the" (x_1), "green" (x_2), "witch" (x_3), and "arrived" (x_4), first passes through an "embedding layer." This layer converts each word into a numerical vector representation, which computers can understand and process. These embeddings are visually represented by the three-segmented vertical ovals.
*   **Sequential Processing:** The word embeddings are then fed sequentially into a series of RNN units (the blue rectangles within the "Encoder" shaded region). Each RNN unit processes a word and updates its internal "hidden layer(s)" state, passing this updated state to the next RNN unit. This sequential flow allows the Encoder to build a comprehensive understanding of the entire input sentence, capturing its meaning and context in a final hidden state.

**2. The Decoder:**
*   **Initial State:** The final hidden state from the Encoder is passed to the first RNN unit of the Decoder. This state acts as the "context" that summarizes the source sentence, guiding the Decoder in generating the translation.
*   **Generating Target Words with Teacher Forcing:** The Decoder then generates the target language sentence, "llegó la bruja verde < /s >," word by word. During training, a technique called "teacher forcing" is used. This means that instead of relying on its own predictions, the Decoder is given the correct "gold answers" (the actual target words) as input for each step.
    *   **Step 1:** The Decoder starts by taking a special start-of-sentence token, "< s >", as its first input (after passing it through an "embedding layer").
    *   **Output and Loss for "llegó" (y_1):** The RNN unit processes "< s >", and its output goes through a "softmax" layer (represented by the bar chart symbol, labeled "softmax ŷ"). This "softmax" layer calculates the probability distribution for all possible next words. The model's prediction is compared against the first "gold answer," "llegó." A "per-word loss," L_1, is calculated using the formula "L_1 = -log P(y_1)," which measures how far off the model's predicted probability was for the correct word "llegó."
    *   **Output and Loss for "la" (y_2):** For the next step, the correct word "llegó" is used as the input to the Decoder's embedding layer. The RNN unit then produces an output (y_2), which goes through "softmax," and a "per-word loss," L_2 = -log P(y_2), is calculated against the "gold answer" "la."
    *   **Continuing the Process:** This process continues for each subsequent word in the target sentence: "la" is used as input for "bruja" (y_3), "bruja" for "verde" (y_4), and "verde" for "< /s >" (y_5), with corresponding per-word losses L_3, L_4, and L_5 calculated against their respective "gold answers."

**3. Total Loss Calculation:**
*   After calculating a "per-word loss" for each word in the target sentence (L_1 through L_5), these individual losses are combined to determine the model's overall error for the entire translated sentence.
*   The "Total loss is the average cross-entropy loss per target word," computed by the formula: "L = (1/T) Σ (from i=1 to T) L_i." Here, 'T' is the total number of target words (5 in this example, including "< /s >"). This average loss is the primary metric used to adjust the model's internal parameters during training to improve its translation accuracy. The goal is to minimize this total loss, making the model's predictions align more closely with the "gold answers."](images/c3fb67a70d9a29189abf483a0d4b7fa2ca54d28a20ad5e587814067da64b727c.jpg)
Figure 8.19 Training the basic RNN encoder-decoder approach to machine translation. Note that in the decoder we usually don’t propagate the model’s softmax outputs $\hat { y } _ { t }$ , but use teacher forcing to force each input to the correct gold value for training. We compute the softmax output distribution over $\hat { y }$ in the decoder in order to compute the loss at each token, which can then be averaged to compute a loss for the sentence. This loss is then propagated through the decoder parameters and the encoder parameters.

# 8.8 Attention

The simplicity of the encoder-decoder model is its clean separation of the encoder— which builds a representation of the source text—from the decoder, which uses this context to generate a target text. In the model as we’ve described it so far, this context vector is $h _ { n }$ , the hidden state of the last $( n ^ { \mathrm { t h } } )$ time step of the source text. This final hidden state is thus acting as a bottleneck: it must represent absolutely everything about the meaning of the source text, since the only thing the decoder knows about the source text is what’s in this context vector (Fig. 8.20). Information at the beginning of the sentence, especially for long sentences, may not be equally well represented in the context vector.

![## Image Analysis: c4d6b74ffbfc294d4918cc4116530e436866dc42486cea6736d1fde9a24858c3.jpg

**Conceptual Understanding:**
Conceptually, this image represents a basic Encoder-Decoder framework, a common architecture in deep learning for tasks like machine translation or text summarization. The main purpose is to visually articulate the challenge posed by a fixed-size 'context vector' (the final hidden state of the encoder) that must encapsulate all information from the input sequence. This single point of information transfer is explicitly termed a 'bottleneck,' signifying a potential constraint on the model's ability to retain and utilize all relevant details from long input sequences. The diagram effectively communicates the idea of sequential processing within both the encoder and decoder, and the singular conduit for information flow between them.

**Content Interpretation:**
The image illustrates a standard Encoder-Decoder neural network architecture used for sequence-to-sequence tasks. It specifically highlights the concept of an 'information bottleneck' that exists when the entire input sequence's information is compressed into a single, fixed-size vector—the final hidden state of the encoder—before being passed to the decoder. The blue rectangular blocks represent the Encoder's sequential processing of an input, building up a hidden state. The red rectangular blocks represent the Decoder, which generates an output sequence based on the context provided by the encoder and its own previous outputs. The 'bottleneck' is the critical point where all encoder-derived information must pass through, potentially limiting the model's capacity to retain long-range dependencies or fine-grained details from the source sequence.

**Key Insights:**
The main takeaway from this image is the critical limitation of traditional Encoder-Decoder architectures: the 'information bottleneck.' This bottleneck occurs because all the information from the encoder's entire input sequence must be condensed into a single, fixed-size vector (the final hidden state of the encoder) before it can be used by the decoder. This can lead to a loss of information, especially for longer input sequences, as the single vector struggles to adequately represent all the nuances and details of the source. The explicit label 'bottleneck' in green text and the visual representation of all encoder flow converging to a single point feeding into the decoder strongly emphasize this concept.

**Document Context:**
This image is highly relevant to Section 8.8, 'Attention,' as it visually depicts the problem that attention mechanisms were designed to solve. The accompanying text, 'Figure 8.20 Requiring the context c to be only the encoder’s final hidden state forces all the information from the entire source sentence to pass through this representational bottleneck,' directly explains the issue shown in the diagram. The image serves as a foundational visual explanation of why a simple Encoder-Decoder model without attention struggles with long sequences due to the inherent information loss at the bottleneck. It sets the stage for introducing attention as a method to overcome this limitation by allowing the decoder to access different parts of the encoder's hidden states, rather than just the final one.

**Summary:**
The image displays a sequential neural network architecture, fundamentally an Encoder-Decoder model. On the left, five blue rectangular units represent the 'Encoder'. Each encoder unit processes an input (indicated by an upward arrow) and passes its state sequentially to the next unit via a rightward arrow. The fifth and final blue encoder unit is prominently encircled by a green dashed line and explicitly labeled 'bottleneck' in green text above it. This unit's output, a single contextual representation, is then transmitted to the 'Decoder' section on the right. The 'Decoder' consists of four red rectangular units. Each decoder unit receives an input from the preceding decoder unit via a rightward arrow, and also receives the context from the encoder's 'bottleneck' (indicated by three dashed arrows connecting the last encoder unit to the first decoder unit). Additionally, each decoder unit takes an input (upward arrow from bottom), produces an output (upward arrow from top), and incorporates a self-feedback mechanism where its output loops back as an input for the next step. The overall flow demonstrates a process where an input sequence is processed by the encoder into a condensed representation at the bottleneck, which then serves as the sole context for the decoder to generate an output sequence. This visual effectively highlights the 'bottleneck' where all information from the entire source must pass through a single, fixed-size representation.](images/c4d6b74ffbfc294d4918cc4116530e436866dc42486cea6736d1fde9a24858c3.jpg)
Figure 8.20 Requiring the context $c$ to be only the encoder’s final hidden state forces all the information from the entire source sentence to pass through this representational bottleneck.

The attention mechanism is a solution to the bottleneck problem, a way of allowing the decoder to get information from all the hidden states of the encoder, not just the last hidden state.

In the attention mechanism, as in the vanilla encoder-decoder model, the context vector c is a single vector that is a function of the hidden states of the encoder. But instead of being taken from the last hidden state, it’s a weighted average of all the hidden states of the decoder. And this weighted average is also informed by part of the decoder state as well, the state of the decoder right before the current token $i$ . That is, $\mathbf { c } = f ( \mathbf { h } _ { 1 } ^ { e } . . . \mathbf { h } _ { n } ^ { e } , \mathbf { h } _ { i - 1 } ^ { d } )$ . The weights focus on (‘attend to’) a particular part of the source text that is relevant for the token $i$ that the decoder is currently producing. Attention thus replaces the static context vector with one that is dynamically derived from the encoder hidden states, but also informed by and hence different for each token in decoding.

This context vector, $\mathbf { c } _ { i }$ , is generated anew with each decoding step $i$ and takes all of the encoder hidden states into account in its derivation. We then make this context available during decoding by conditioning the computation of the current decoder hidden state on it (along with the prior hidden state and the previous output generated by the decoder), as we see in this equation (and Fig. 8.21):

$$
\mathbf { \boldsymbol { \mathsf { h } } } _ { i } ^ { d } \ = \ g ( \hat { y } _ { i - 1 } , \mathbf { \boldsymbol { \mathsf { h } } } _ { i - 1 } ^ { d } , \mathbf { \boldsymbol { \mathsf { c } } } _ { i } )
$$

![## Image Analysis: fff6cb4290ca7e5565dd8b06e34a1623c7b49cf083a13523d27cec36d795f31c.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a recurrent neural network (RNN) decoder, specifically illustrating how an attention mechanism integrates into the decoding process in a sequence-to-sequence model. The main purpose is to show that each step of the decoder receives a dynamic context vector (**c_i**) in addition to its previous hidden state to produce an output (**y_i**) and update its hidden state (**hᵈ_i**). The key idea communicated is the sequential generation of outputs, where each output depends on the previous hidden state and a specific context vector derived from the attention mechanism.

**Content Interpretation:**
The image shows a series of identical processing units, each denoted as **hᵈ_k** (where k = 1, 2, ..., i). These units represent the hidden states of the decoder.

*   **Recurrent Nature:** The horizontal arrows connecting **hᵈ₁** to **hᵈ₂**, **hᵈ₂** to "**...**", and "**...**" to **hᵈᵢ** illustrate the passing of the hidden state from one time step to the next, which is characteristic of RNNs. The text **hᵈ₁**, **hᵈ₂**, **hᵈᵢ** explicitly labels these hidden states.
*   **Context Vectors:** Below each decoder unit, an input labeled **c₁**, **c₂**, and **cᵢ** is shown. As per the document context, these "**c_i**" represent the "dynamic, context" provided by the attention mechanism. They are distinct for each time step, highlighting the dynamic nature of attention.
*   **Output Generation:** Above each decoder unit, an output labeled **y₁**, **y₂**, and **yᵢ** is generated. These represent the output predictions (e.g., decoded words or tokens) at each respective time step. The labels **y₁**, **y₂**, **yᵢ** provide this evidence.
*   **Attention Mechanism Integration:** The presence of the **c_i** inputs at each decoder step directly supports the concept of an attention mechanism. The text after the image ("The attention mechanism allows each hidden state of the decoder to see a different, dynamic, context, which is a function of all the encoder hidden states.") clarifies that these **c_i** are the context vectors generated by the attention mechanism.
*   **Sequential Decoding:** The sequential arrangement of the **hᵈ** units, linked by arrows and ellipses, demonstrates a step-by-step decoding process, where outputs **y_i** are generated one after another. The numbering (1, 2, i) for hᵈ, c, and y, along with the ellipses, confirms this sequential nature and its potential for variable length sequences.

**Key Insights:**
*   **Key Takeaway 1: Dynamic Context:** The image explicitly shows distinct context vectors (**c₁**, **c₂**, **cᵢ**) being fed into each decoder unit (**hᵈ₁**, **hᵈ₂**, **hᵈᵢ**). This illustrates the core concept that an attention mechanism provides a "dynamic, context" (as stated in the accompanying text) for each decoding step, allowing the decoder to focus on different parts of the input sequence at different times.
*   **Key Takeaway 2: Recurrent Decoding Process:** The sequential arrangement of decoder hidden states (**hᵈ₁**, **hᵈ₂**, **hᵈᵢ**) and the passage of information (indicated by horizontal and looped arrows) from one state to the next highlight the recurrent nature of the decoding process. Each output (**y₁**, **y₂**, **yᵢ**) is generated iteratively, building upon previous states.
*   **Key Takeaway 3: Integration of Attention:** The diagram demonstrates how the attention mechanism (represented by the **c_i** inputs) is integrated into the decoder's recurrent structure. Each decoder step **hᵈ_i** explicitly uses both the previous hidden state (from **hᵈ_{i-1}**) and the current context vector **c_i** to compute the next hidden state and output. This is crucial for understanding how attention improves sequence-to-sequence models by providing relevant source information at each target generation step.
*   **Key Takeaway 4: Generation of Output Sequence:** The outputs **y₁**, **y₂**, **yᵢ** directly show that the decoder's role is to generate a sequence of elements (e.g., words in a translation task). The continuation indicated by the ellipses ("**...**") implies that this process can generate sequences of arbitrary length.

**Document Context:**
This image, appearing in Section 8.8 titled "Attention" and followed by the descriptive text "Figure 8.21 The attention mechanism allows each hidden state of the decoder to see a different, dynamic, context, which is a function of all the encoder hidden states," is central to explaining the mechanics of the attention mechanism within a neural network decoder. It visually demonstrates how the "dynamic context" (represented by **c_i**) is applied at each step of the decoding process, which is a key enhancement over traditional sequence-to-sequence models without attention. It provides a concrete architectural representation of the abstract concept of attention.

**Summary:**
The image illustrates a simplified architecture of a recurrent neural network (RNN) decoder that incorporates an attention mechanism, used primarily in sequence-to-sequence tasks like machine translation. The diagram shows a series of interconnected rectangular blocks, each representing a step in the decoding process, specifically a decoder hidden state.

Starting from the left:
1.  The first block, labeled "**hᵈ₁**", represents the decoder's hidden state at the first time step. It receives an initial input (indicated by an incoming arrow from the left) and a context vector labeled "**c₁**" from below. This context vector "**c₁**" is crucial as it encapsulates information from the encoder via the attention mechanism, dynamically relevant to generating the first output.
2.  From "**hᵈ₁**", an output labeled "**y₁**" is produced upwards, which corresponds to the first element of the output sequence. The hidden state of "**hᵈ₁**" is also passed forward horizontally to the next decoder unit.
3.  The second block, labeled "**hᵈ₂**", represents the decoder's hidden state at the second time step. It receives the hidden state from "**hᵈ₁**" (via the horizontal arrow) and its own dynamic context vector labeled "**c₂**". The looping arrow also indicates the passage of the previous hidden state (or a component thereof) into the current step's calculation.
4.  From "**hᵈ₂**", an output labeled "**y₂**" is produced upwards, representing the second element of the output sequence. The hidden state of "**hᵈ₂**" is then passed forward.
5.  An ellipsis "**...**" (represented by a cloud shape) indicates that this process continues for several intermediate steps, following the same pattern: each step takes the previous hidden state and a new, dynamic context vector (c_k) to produce an output (y_k) and its own hidden state.
6.  The last explicitly shown block, labeled "**hᵈᵢ**", represents the decoder's hidden state at a generic *i*-th time step. It receives the hidden state from the preceding (ellipsis) step and its dynamic context vector labeled "**cᵢ**".
7.  From "**hᵈᵢ**", an output labeled "**yᵢ**" is produced upwards, representing the *i*-th element of the output sequence.
8.  Another ellipsis "**...**" to the right of "**hᵈᵢ**" signifies that the decoding process can continue further, generating additional outputs until the entire target sequence is produced.

In essence, this diagram visually demonstrates how an RNN decoder, when enhanced with an attention mechanism, leverages a specific and dynamically calculated "context" (c₁, c₂, ..., cᵢ) from the input sequence at *each* decoding step to produce more accurate and relevant outputs (y₁, y₂, ..., yᵢ). This is achieved by having each hidden state (hᵈ₁, hᵈ₂, ..., hᵈᵢ) not only rely on the previous hidden state but also on this unique context vector, which effectively allows the decoder to "look" at relevant parts of the input at every step.](images/fff6cb4290ca7e5565dd8b06e34a1623c7b49cf083a13523d27cec36d795f31c.jpg)
Figure 8.21 The attention mechanism allows each hidden state of the decoder to see a different, dynamic, context, which is a function of all the encoder hidden states.

The first step in computing $\mathbf { c } _ { i }$ is to compute how much to focus on each encoder state, how relevant each encoder state is to the decoder state captured in $\mathbf { h } _ { i - 1 } ^ { d }$ . We capture relevance by computing— at each state $i$ during decoding—a score $( \mathbf { h } _ { i - 1 } ^ { d } , \mathbf { h } _ { j } ^ { e } )$ for each encoder state $j$ .

The simplest such score, called dot-product attention, implements relevance as similarity: measuring how similar the decoder hidden state is to an encoder hidden state, by computing the dot product between them:

$$
\mathrm { s c o r e } ( \mathsf { \pmb { h } } _ { i - 1 } ^ { d } , \mathsf { \pmb { h } } _ { j } ^ { e } ) \ = \ \mathsf { \pmb { h } } _ { i - 1 } ^ { d } \cdot \mathsf { \pmb { h } } _ { j } ^ { e }
$$

The score that results from this dot product is a scalar that reflects the degree of similarity between the two vectors. The vector of these scores across all the encoder hidden states gives us the relevance of each encoder state to the current step of the decoder.

To make use of these scores, we’ll normalize them with a softmax to create a vector of weights, $\alpha _ { i j }$ , that tells us the proportional relevance of each encoder hidden state $j$ to the prior hidden decoder state, $h _ { i - 1 } ^ { d }$ .

$$
\begin{array} { r } { \alpha _ { i j } ~ = ~ \mathrm { s o f t m a x } ( \mathrm { s c o r e } ( \mathbf { h } _ { i - 1 } ^ { d } , \mathbf { h } _ { j } ^ { e } ) ) } \\ { ~ = ~ \frac { \exp ( \mathrm { s c o r e } ( \mathbf { h } _ { i - 1 } ^ { d } , \mathbf { h } _ { j } ^ { e } ) } { \sum _ { k } \exp ( \mathrm { s c o r e } ( \mathbf { h } _ { i - 1 } ^ { d } , \mathbf { h } _ { k } ^ { e } ) ) } } \end{array}
$$

Finally, given the distribution in $\alpha$ , we can compute a fixed-length context vector for the current decoder state by taking a weighted average over all the encoder hidden

states.

$$
\mathbf { c } _ { i } ~ = ~ \sum _ { j } \alpha _ { i j } \mathbf { h } _ { j } ^ { e }
$$

With this, we finally have a fixed-length context vector that takes into account information from the entire encoder state that is dynamically updated to reflect the needs of the decoder at each step of decoding. Fig. 8.22 illustrates an encoderdecoder network with attention, focusing on the computation of one context vector $\mathbf { c } _ { i }$ .

![## Image Analysis: 54e689d3489a1136302dbdca29c657b56bbcbf41cf475fedd58b873079f15d5d.jpg

**Conceptual Understanding:**
This image represents an encoder-decoder neural network architecture enhanced with an attention mechanism. Conceptually, it illustrates how a neural network can 'pay attention' to different parts of an input sequence when generating an output sequence. The main purpose is to show the detailed process by which a context vector (`c_i`) is derived at each step of the decoder's operation, enabling the decoder to access and selectively utilize information from all encoder hidden states. The key idea communicated is that the previous decoder hidden state guides the computation of attention weights, which then determine how much each encoder hidden state contributes to the current context, ultimately improving the relevance and quality of the generated output.

**Content Interpretation:**
The image displays the architecture of an encoder-decoder neural network integrated with an attention mechanism. It explicitly details the computation of the context vector `c_i` as a crucial part of the attention process. The encoder sequentially processes inputs `x_1, ..., x_n` to produce encoder hidden states `h^e_1, ..., h^e_n`. The attention mechanism involves calculating a dot product `h^d_{i-1} · h^e_j` between the prior decoder hidden state `h^d_{i-1}` and each encoder hidden state `h^e_j`. These dot products are then transformed into 'attention weights `α_ij`' (shown as .4, .3, .1, .2), which quantify the relevance of each encoder hidden state to the current decoding step. The context vector `c_i` is formed by taking a weighted sum (`Σ_j α_ij h^e_j`) of the encoder hidden states, where the weights are the calculated `α_ij`. This `c_i` vector, along with `c_{i-1}` and `y_{i-1}` (or `y_{i-2}`), serves as input to the decoder to produce the current decoder hidden state `h^d_i` and the output `y_i`. The significance lies in showing how the decoder dynamically selects and weighs information from the entire input sequence at each step of output generation, rather than relying on a single, fixed-size context representation.

**Key Insights:**
The main takeaway from this image is the detailed illustration of how the attention mechanism works within an encoder-decoder framework. It demonstrates that for each step of the decoder, a specific context vector (`c_i`) is generated by selectively focusing on and weighting the hidden states of the encoder. The varying attention weights (`α_ij`) indicate that not all parts of the input sequence contribute equally to the generation of a specific output element. This provides the insight that attention allows for dynamic and adaptive information retrieval from the source sequence, overcoming limitations of traditional encoder-decoder models that use a fixed-size context vector. The explicit formula `Σ_j α_ij h^e_j` for `c_i`, derived from `h^d_{i-1} · h^e_j`, provides direct textual evidence for how this dynamic weighting is calculated and applied.

**Document Context:**
This image is highly relevant to Section 8.8 titled "Attention" in the document. It visually clarifies the computation of `c_i`, which is explicitly mentioned in the accompanying text as "one of the inputs to the computation of `h^d_i`" and "computed by taking the weighted sum of all the encoder hidden states, each weighted by their dot product with the prior decoder hidden state `h^d_{i-1}`". The diagram serves as a direct visual explanation, showing the flow of information and the mathematical operations involved in the attention mechanism, which is a core concept discussed in the section.

**Summary:**
This diagram illustrates an encoder-decoder network with an attention mechanism, focusing on how the context vector `c_i` is computed. The process begins with the Encoder, which processes an input sequence `x_1` through `x_n` to generate a series of hidden states `h^e_1` through `h^e_n`. These hidden states represent encoded information from the input. The attention mechanism then comes into play. For a given decoder step `i`, the previous decoder hidden state `h^d_{i-1}` is used to calculate relevance scores with each of the encoder's hidden states `h^e_j`. This is shown by the dashed lines leading from `h^d_{i-1}` to the dot product `h^d_{i-1} · h^e_j` associated with each encoder hidden state. These scores are then normalized to become 'attention weights `α_ij`', explicitly shown as `.4`, `.3`, `.1`, and `.2` for different `j` values, indicating varying levels of importance. The context vector `c_i` is then computed as a weighted sum, `Σ_j α_ij h^e_j`, where each encoder hidden state `h^e_j` is multiplied by its corresponding attention weight `α_ij`. This `c_i` vector, which aggregates relevant information from the encoder's output based on the attention weights, is then fed into the Decoder. The Decoder takes `c_i`, `c_{i-1}`, and the previous output `y_{i-1}` (or `y_{i-2}`) to compute its current hidden state `h^d_i` and generate the current output `y_i`. This iterative process allows the decoder to dynamically focus on different parts of the input sequence (via `c_i`) for each step of generating the output sequence, enhancing the model's ability to handle long sequences and capture relevant dependencies. The diagram visually breaks down a complex neural network component into a logical flow of operations.](images/54e689d3489a1136302dbdca29c657b56bbcbf41cf475fedd58b873079f15d5d.jpg)
Figure 8.22 A sketch of the encoder-decoder network with attention, focusing on the computation of $\mathbf { c } _ { i }$ . The context value $\mathbf { c } _ { i }$ is one of the inputs to the computation of $\mathbf { h } _ { i } ^ { d }$ . It is computed by taking the weighted sum of all the encoder hidden states, each weighted by their dot product with the prior decoder hidden state $\boldsymbol { \mathsf { h } } _ { i - 1 } ^ { d }$ .

It’s also possible to create more sophisticated scoring functions for attention models. Instead of simple dot product attention, we can get a more powerful function that computes the relevance of each encoder hidden state to the decoder hidden state by parameterizing the score with its own set of weights, $\boldsymbol { \mathsf { W } } _ { s }$ .

$$
\begin{array} { r } { \mathrm { s c o r e } ( \mathsf { \pmb { h } } _ { i - 1 } ^ { d } , \mathsf { \pmb { h } } _ { j } ^ { e } ) = \mathsf { \pmb { h } } _ { t - 1 } ^ { d } \mathsf { \pmb { W } } _ { s } \mathsf { \pmb { h } } _ { j } ^ { e } } \end{array}
$$

The weights $W _ { s }$ , which are then trained during normal end-to-end training, give the network the ability to learn which aspects of similarity between the decoder and encoder states are important to the current application. This bilinear model also allows the encoder and decoder to use different dimensional vectors, whereas the simple dot-product attention requires that the encoder and decoder hidden states have the same dimensionality.

We’ll return to the concept of attention when we define the transformer architecture in Chapter 9, which is based on a slight modification of attention called self-attention.

# 8.9 Summary

This chapter has introduced the concepts of recurrent neural networks and how they can be applied to language problems. Here’s a summary of the main points that we

covered:

• In simple Recurrent Neural Networks sequences are processed one element at a time, with the output of each neural unit at time $t$ based both on the current input at $t$ and the hidden layer from time $t - 1$ .   
• RNNs can be trained with a straightforward extension of the backpropagation algorithm, known as backpropagation through time (BPTT).   
• Simple recurrent networks fail on long inputs because of problems like vanishing gradients; instead modern systems use more complex gated architectures such as LSTMs that explicitly decide what to remember and forget in their hidden and context layers.

• Common language-based applications for RNNs include:

– Probabilistic language modeling: assigning a probability to a sequence, or to the next element of a sequence given the preceding words.   
– Auto-regressive generation using a trained language model.   
– Sequence labeling like part-of-speech tagging, where each element of a sequence is assigned a label.   
– Sequence classification, where an entire text is assigned to a category, as in spam detection, sentiment analysis or topic classification.   
– Encoder-decoder architectures, where an input is mapped to an output of different length and alignment.

# Bibliographical and Historical Notes

Influential investigations of RNNs were conducted in the context of the Parallel Distributed Processing (PDP) group at UC San Diego in the 1980’s. Much of this work was directed at human cognitive modeling rather than practical NLP applications (Rumelhart and McClelland 1986c, McClelland and Rumelhart 1986). Models using recurrence at the hidden layer in a feedforward network (Elman networks) were introduced by Elman (1990). Similar architectures were investigated by Jordan (1986) with a recurrence from the output layer, and Mathis and Mozer (1995) with the addition of a recurrent context layer prior to the hidden layer. The possibility of unrolling a recurrent network into an equivalent feedforward network is discussed in (Rumelhart and McClelland, 1986c).

In parallel with work in cognitive modeling, RNNs were investigated extensively in the continuous domain in the signal processing and speech communities (Giles et al. 1994, Robinson et al. 1996). Schuster and Paliwal (1997) introduced bidirectional RNNs and described results on the TIMIT phoneme transcription task.

While theoretically interesting, the difficulty with training RNNs and managing context over long sequences impeded progress on practical applications. This situation changed with the introduction of LSTMs in Hochreiter and Schmidhuber (1997) and Gers et al. (2000). Impressive performance gains were demonstrated on tasks at the boundary of signal processing and language processing including phoneme recognition (Graves and Schmidhuber, 2005), handwriting recognition (Graves et al., 2007) and most significantly speech recognition (Graves et al., 2013).

Interest in applying neural networks to practical NLP problems surged with the work of Collobert and Weston (2008) and Collobert et al. (2011). These efforts made use of learned word embeddings, convolutional networks, and end-to-end training.

They demonstrated near state-of-the-art performance on a number of standard shared tasks including part-of-speech tagging, chunking, named entity recognition and semantic role labeling without the use of hand-engineered features.

Approaches that married LSTMs with pretrained collections of word-embeddings based on word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014) quickly came to dominate many common tasks: part-of-speech tagging (Ling et al., 2015), syntactic chunking (Søgaard and Goldberg, 2016), named entity recognition (Chiu and Nichols, 2016; Ma and Hovy, 2016), opinion mining (Irsoy and Cardie, 2014), semantic role labeling (Zhou and Xu, 2015a) and AMR parsing (Foland and Martin, 2016). As with the earlier surge of progress involving statistical machine learning, these advances were made possible by the availability of training data provided by CONLL, SemEval, and other shared tasks, as well as shared resources such as Ontonotes (Pradhan et al., 2007b), and PropBank (Palmer et al., 2005).

The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder. Cho et al. (2014) (who coined the name “encoder-decoder”) and Sutskever et al. (2014) then showed how to use extended RNNs for both encoder and decoder. The idea that a generative decoder should take as input a soft weighting of the inputs, the central idea of attention, was first developed by Graves (2013) in the context of handwriting recognition. Bahdanau et al. (2015) extended the idea, named it “attention” and applied it to MT.

# 9

# The Transformer

“The true art of memory is the art of attention ” Samuel Johnson, Idler #74, September 1759

In this chapter we introduce the transformer, the standard architecture for building large language models. Transformer-based large language models have completely changed the field of speech and language processing. Indeed, every subsequent chapter in this textbook will make use of them. We’ll focus for now on leftto-right (sometimes called causal or autoregressive) language modeling, in which we are given a sequence of input tokens and predict output tokens one by one by conditioning on the prior context.

The transformer is a neural network with a specific structure that includes a mechanism called self-attention or multi-head attention.1 Attention can be thought of as a way to build contextual representations of a token’s meaning by attending to and integrating information from surrounding tokens, helping the model learn how tokens relate to each other over large spans.

Figure 9.1 The architecture of a (left-to-right) transformer, showing how each input token get encoded, passed through a set of stacked transformer blocks, and then a language model head that predicts the next token.

![## Image Analysis: a57426530ca943d66c85269a5ada50f8f225c013117e4964f038a2d3e6cfdc66.jpg

**Conceptual Understanding:**
This image represents a simplified, high-level conceptual diagram of a Transformer neural network architecture, specifically configured for a language modeling task. The main purpose of the image is to visually explain how a Transformer processes a sequence of input tokens to predict the subsequent token, highlighting its parallel processing capability and its reliance on self-attention mechanisms for contextual understanding. The key ideas being communicated are:
1.  **Sequence Processing:** How a sequence of words is taken as input and processed.
2.  **Modular Layers:** The distinct stages of transformation: input embedding with positional encoding, multi-layered contextual processing, and output prediction.
3.  **Parallelism:** The simultaneous processing of multiple tokens within the sequence.
4.  **Attention Mechanism:** The interaction and information exchange between different tokens to build context-aware representations.
5.  **Autoregressive Prediction:** The task of predicting the next element in a sequence based on all preceding elements.

**Content Interpretation:**
The image illustrates the core architecture of a Transformer model applied to a language modeling task. It depicts the following processes, concepts, and relationships:

**Processes Shown:**
*   **Input Tokenization/Embedding:** Discrete input words like "So", "long", "and", "thanks", "for" are converted into continuous vector representations through an embedding layer ("E").
*   **Positional Encoding:** Positional information (indicated by numbered squares "1" through "5" and plus signs) is added to the token embeddings to account for word order, as the Transformer processes tokens in parallel.
*   **Contextual Feature Extraction:** The combined token and positional embeddings (labeled "x1" through "x5") are processed by multiple layers of "Stacked Transformer Blocks". Each block contains sub-layers (visually represented by red, yellow, and blue bars) that likely include multi-head attention and feed-forward networks.
*   **Inter-token Communication (Multi-head Self-Attention):** The dense network of horizontal lines connecting sub-layers across different token columns within the "Stacked Transformer Blocks" signifies how the representation of each token is informed by and influenced by all other tokens in the sequence. This is the essence of the self-attention mechanism.
*   **Next Token Prediction:** The final contextualized representations from the transformer blocks are fed into a "Language Modeling Head". This head uses a linear projection ("U") to generate "logits"—raw scores for each word in the vocabulary—from which the most probable "Next token" is predicted.

**Relationships:**
*   **Parallel Processing:** Each input token's journey through the "Input Encoding", "Stacked Transformer Blocks", and "Language Modeling Head" occurs in parallel, allowing for efficient computation.
*   **Sequential Dependency (for prediction):** Although processing is parallel, the output "Next token" at a given position corresponds to the word that follows the input token at that same position, illustrating an autoregressive prediction mechanism.
*   **Hierarchical Layers:** The architecture is organized into distinct layers (Input Encoding, Stacked Transformer Blocks, Language Modeling Head), with information flowing sequentially from bottom to top.

**Systems:** The entire diagram represents a simplified overview of the "Transformer architecture" itself, specifically configured for language modeling.

**Significance of Information:**
*   The specific words chosen for input ("So long and thanks for") and output ("long and thanks for all") clearly illustrate the language modeling task: predicting the next word in a sequence based on prior context.
*   Labels like "E" (Embedding) and "U" (output projection) represent standard neural network components. "Logits" are the unnormalized scores from which a probability distribution for the next word is derived.
*   The "x1" through "x5" labels denote the initial contextualized input vectors to the main transformer block stack, which are crucial intermediate representations.

**Key Insights:**
The image provides several key takeaways and insights into the Transformer architecture:

**Main Takeaways:**
1.  **Modular and Layered Design:** The Transformer is structured in a clear, modular fashion, with distinct layers for "Input Encoding", "Stacked Transformer Blocks", and a "Language Modeling Head". This modularity allows for specialized processing at each stage.
2.  **Parallel Sequence Processing:** A fundamental insight is that the Transformer processes all elements of an input sequence concurrently. This is evident from the identical, parallel vertical columns for each token (x1, x2, x3, x4, x5) in the "Input Encoding" and "Stacked Transformer Blocks" layers.
3.  **Contextual Representation through Self-Attention:** The numerous horizontal connections within the "Stacked Transformer Blocks" highlight the critical role of the multi-head self-attention mechanism. This mechanism enables each token's representation to integrate information from all other tokens in the sequence, thereby forming a rich, context-aware understanding.
4.  **Autoregressive Language Generation:** The diagram clearly demonstrates the application of the Transformer to language modeling, where it predicts the next word in a sequence based on the preceding context. This is shown by the progression from "Input tokens" ("So long and thanks for") to "Next token" predictions ("long and thanks for all").
5.  **Integration of Positional Information:** The "Input Encoding" step reveals that raw token embeddings ("E") are explicitly combined with positional encodings (numbered squares with plus signs) to preserve word order, which is essential for understanding sequence structure in a parallel processing model.

**Conclusions/Insights:**
*   The Transformer's parallel processing capability, enabled by its architecture, offers significant computational efficiency advantages over sequential models, particularly for long sequences.
*   The self-attention mechanism is the cornerstone of the Transformer's ability to capture long-range dependencies and complex contextual relationships within a sequence, leading to highly effective language understanding and generation.
*   The architecture is scalable (indicated by ellipses) both in terms of sequence length and depth of processing layers, making it versatile for various sequence-to-sequence tasks.

**Textual Evidence for Insights:**
*   The distinct labels "Input Encoding", "Stacked Transformer Blocks", and "Language Modeling Head" provide evidence for the modular and layered design.
*   The simultaneous processing of "x1", "x2", "x3", "x4", "x5" through parallel columns of blocks supports the parallel processing insight.
*   The dense web of horizontal black lines connecting the red, yellow, and blue bars within "Stacked Transformer Blocks" serves as direct visual evidence for the attention mechanism and inter-token communication.
*   The clear mapping from "Input tokens" ("So long and thanks for") to "Next token" predictions ("long and thanks for all") explicitly demonstrates the autoregressive language modeling application.
*   The presence of "E" (Embedding) and the numbered squares (1, 2, 3, 4, 5) with plus signs within "Input Encoding" provides textual evidence for the integration of token and positional embeddings.

**Document Context:**
This image, labeled as "Fig. 9.1", is directly relevant to the document's "The Transformer" section, serving as a primary visual aid to explain the architecture. The surrounding text explicitly mentions the "transformer architecture" and details its "three major components" and "columns of transformer blocks". The diagram visually maps these verbal descriptions: the "Input Encoding", "Stacked Transformer Blocks", and "Language Modeling Head" correspond to the major components, and the vertical arrangement of processing for each token forms the "columns of transformer blocks". The text's reference to each block being a "multilayer network (a multi-head attention layer, feedforward networks and layer normalization steps) that maps an input vector $\mathbf { x } _ { \mathrm { i } }$ in column $i$" is perfectly illustrated by the internal structure of the purple-blue blocks, the input labels "x1" through "x5", and the horizontal connections suggesting attention. The diagram thus provides a concrete, step-by-step visual interpretation that enhances the reader's understanding of the abstract concepts introduced in the accompanying text.

**Summary:**
This diagram provides a clear visual breakdown of a Transformer neural network, specifically configured for a language modeling task where it predicts the next word in a sequence. The entire process flows upwards, starting from input words at the bottom and culminating in predictions for the subsequent words at the top.

Let's break it down layer by layer, following a sample input "So long and thanks for":

1.  **Input Tokens:** At the very bottom, we have the raw input words, or "Input tokens", shown as "So", "long", "and", "thanks", "for". These are the words the model is currently processing. An ellipsis "..." indicates that the input sequence can continue beyond these five words.

2.  **Input Encoding:** Each input token first goes through an "Input Encoding" step. For each word (e.g., "So"), an "E" (Embedding) module converts it into a numerical vector. This vector is then combined (indicated by the plus sign) with a positional encoding (represented by the numbered squares: "1" for the first token, "2" for the second, and so on). This positional information is vital because, unlike traditional sequential models, the Transformer processes all words in parallel and needs to know their order. The output of this layer for each word is a combined, initial contextual vector, labeled as "x1", "x2", "x3", "x4", and "x5" for each respective input word.

3.  **Stacked Transformer Blocks:** These "x" vectors are then fed into the core of the Transformer: the "Stacked Transformer Blocks". This section consists of multiple layers stacked one on top of another (indicated by the vertical ellipsis "..."). Each large purple-blue rectangle represents a stack of these blocks for a particular input token's position. Within each stack, there are smaller colored horizontal bars (red, yellow, blue) which represent sub-layers within each transformer block (e.g., a multi-head attention layer, feedforward networks, and layer normalization steps, as mentioned in the document context).
    The critical detail here is the intricate web of black lines with arrows connecting these sub-layers *horizontally* across different token columns. These connections illustrate the **multi-head self-attention mechanism**. This is how each word's representation (e.g., "x1") communicates with and gathers information from all other words ("x2", "x3", "x4", "x5") in the sequence. This enables the model to build a rich, context-aware understanding for each word.

4.  **Language Modeling Head:** After passing through all the "Stacked Transformer Blocks" and accumulating contextual information, the refined vector for each position is sent to the "Language Modeling Head". In this head, a linear transformation (represented by the trapezoid labeled "U") is applied. This produces "logits", which are raw numerical scores for every possible word in the model's vocabulary. The small graph above "logits" visually represents a probability distribution over these words, indicating how likely each word is to be the next one in the sequence.

5.  **Next Token Prediction:** Finally, from these logits, the model predicts the "Next token". For each input token, the model predicts the word that *follows* it.
    *   After "So", it predicts "long".
    *   After "long", it predicts "and".
    *   After "and", it predicts "thanks".
    *   After "thanks", it predicts "for".
    *   After "for", it predicts "all".
    This demonstrates the Transformer's capability to generate coherent text by predicting the next word in a sequence, building upon the context provided by the preceding words. The horizontal ellipsis again signifies that this process continues for longer sequences.

In summary, the diagram meticulously shows how a Transformer takes a sequence of words, embeds them with positional information, processes them in parallel through layers that allow words to interact with each other (attention), and then outputs predictions for the next word in the sequence, making it a powerful tool for language-related tasks.](images/a57426530ca943d66c85269a5ada50f8f225c013117e4964f038a2d3e6cfdc66.jpg)
Fig. 9.1 sketches the transformer architecture. A transformer has three major components. At the center are columns of transformer blocks. Each block is a multilayer network (a multi-head attention layer, feedforward networks and layer normalization steps) that maps an input vector $\mathbf { x } _ { \mathrm { i } }$ in column $i$ (corresponding to input

token $i$ ) to an output vector $\mathbf { h } _ { i }$ . The set of $n$ blocks maps an entire context window of input vectors $\left( \mathbf { x } _ { 1 } , . . . , \mathbf { x } _ { n } \right)$ to a window of output vectors $( \mathbf { h } _ { 1 } , . . . , \mathbf { h } _ { n } )$ of the same length. A column might contain from 12 to 96 or more stacked blocks.

The column of blocks is preceded by the input encoding component, which processes an input token (like the word thanks) into a contextual vector representation, using an embedding matrix E and a mechanism for encoding token position. Each column is followed by a language modeling head, which takes the embedding output by the final transformer block, passes it through an unembedding matrix U and a softmax over the vocabulary to generate a single token for that column.

Transformer-based language models are complex, and so the details will unfold over the next 5 chapters. In the next sections we’ll introduce multi-head attention, the rest of the transformer block, and the input encoding and language modeling head components. Chapter 10 discusses how language models are pretrained, and how tokens are generated via sampling. Chapter 11 introduces masked language modeling and the BERT family of bidirectional transformer encoder models. Chapter 12 shows how to prompt LLMs to perform NLP tasks by giving instructions and demonstrations, and how to align the model with human preferences. Chapter 13 will introduce machine translation with the encoder-decoder architecture.

# 9.1 Attention

Recall from Chapter 6 that for word2vec and other static embeddings, the representation of a word’s meaning is always the same vector irrespective of the context: the word chicken, for example, is always represented by the same fixed vector. So a static vector for the word it might somehow encode that this is a pronoun used for animals and inanimate entities. But in context it has a much richer meaning. Consider it in one of these two sentences:

(9.1) The chicken didn’t cross the road because it was too tired.

(9.2) The chicken didn’t cross the road because it was too wide.

In (9.1) it is the chicken (i.e., the reader knows that the chicken was tired), while in (9.2) it is the road (and the reader knows that the road was wide).2 That is, if we are to compute the meaning of this sentence, we’ll need the meaning of it to be associated with the chicken in the first sentence and associated with the road in the second one, sensitive to the context.

Furthermore, consider reading left to right like a causal language model, processing the sentence up to the word it:

(9.3) The chicken didn’t cross the road because it

At this point we don’t yet know which thing it is going to end up referring to! So a representation of $\mathrm { i t }$ at this point might have aspects of both chicken and road as the reader is trying to guess what happens next.

This fact that words have rich linguistic relationships with other words that may be far away pervades language. Consider two more examples:

(9.4) The keys to the cabinet are on the table.   
(9.5) I walked along the pond, and noticed one of the trees along the bank.

In (9.4), the phrase The keys is the subject of the sentence, and in English and many languages, must agree in grammatical number with the verb are; in this case both are plural. In English we can’t use a singular verb like is with a plural subject like keys (we’ll discuss agreement more in Chapter 18). In (9.5), we know that bank refers to the side of a pond or river and not a financial institution because of the context, including words like pond. (We’ll discuss word senses more in Chapter 11.)

The point of all these examples is that these contextual words that help us compute the meaning of words in context can be quite far away in the sentence or paragraph. Transformers can build contextual representations of word meaning, contextual embeddings, by integrating the meaning of these helpful contextual words. In a transformer, layer by layer, we build up richer and richer contextualized representations of the meanings of input tokens. At each layer, we compute the representation of a token i by combining information about $i$ from the previous layer with information about the neighboring tokens to produce a contextualized representation for each word at each position.

Attention is the mechanism in the transformer that weighs and combines the representations from appropriate other tokens in the context from layer $k - 1$ to build the representation for tokens in layer $k$ .

![## Image Analysis: 3123696038ff74831ea8b9ad9d018d7044ae9cf3aa774db0b7b3f1425583ca12.jpg

**Conceptual Understanding:**
The image conceptually illustrates the **self-attention mechanism** within a Transformer model, specifically showing how a word's representation is formed by considering the importance of other words in the input sequence. It visualizes the **attention distribution** when the model processes the pronoun "it" in a sentence.

The main purpose of the image is to **demonstrate how a Transformer model attends to different words in its context** to build a richer, context-aware representation for a specific word. It highlights which preceding words are most relevant to understanding the target word "it" in the sentence "The chicken didn't cross the road because it was too tired."

**Key ideas and concepts being communicated are:**
1.  **Contextual Representation Learning:** Words are not understood in isolation but in relation to their surrounding context.
2.  **Weighted Importance:** Not all words in the context are equally important; the self-attention mechanism assigns varying weights (degrees of attention) to different words.
3.  **Layered Processing:** Information is processed and refined across multiple computational layers (Layer k to Layer k+1).
4.  **Coreference Resolution:** The specific example of "it" attending to "chicken" and "road" demonstrates how the mechanism aids in resolving which entity a pronoun refers to.

**Content Interpretation:**
The image visually demonstrates the self-attention mechanism in a Transformer neural network. It shows how the representation of a target word, "it" at "Layer k+1", is computed by selectively attending to other words in the input sequence, "The chicken didn't cross the road because it was too tired", from a previous layer, "Layer k". The varying intensities of the blue lines and bars represent the attention weights, indicating the degree of relevance assigned by the model.

The significance of the information presented lies in illustrating the model's ability to identify semantic relationships and resolve coreference. The strong attention to "chicken" and moderate attention to "road" for the pronoun "it" directly supports the interpretation that the model recognizes these as potential antecedents for "it". The minimal attention to other words like "The" further emphasizes the selective nature of the attention mechanism.

**Supporting Evidence from Text Extraction:**
*   **"Layer k+1" and "Layer k"**: These labels explicitly show the multi-layered processing architecture of a Transformer.
*   **"self-attention distribution"**: This label directly names the mechanism being visualized, confirming the core concept.
*   **"columns corresponding to input tokens"**: This annotation clarifies that each vertical sequence of words represents the individual tokens of the input sentence.
*   **Sentence tokens**: The verbatim transcription of "The chicken didn't cross the road because it was too tired" at both layers provides the specific linguistic context for the attention mechanism.
*   **Highlighted "it" at Layer k+1**: This clearly identifies the target word whose representation is being computed.
*   **Connecting lines and highlighted words at Layer k ("The", "chicken", "road")**: The visual evidence of varying blue intensities (darkest for "chicken", then "road", then very faint for "The") along with their verbatim labels directly demonstrates the differential weighting of attention, with "chicken" receiving the highest attention, followed by "road", and then "The" receiving minimal attention from "it".
*   **Faded text for later tokens at Layer k and k+1**: The lighter gray font for words like "it", "was", "too", "tired" at Layer k (when processing for "it" at Layer k+1) suggests that the model primarily attends to preceding tokens for context in this specific example.

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Dynamic Contextualization:** Transformer models, through self-attention, dynamically assign varying importance (attention weights) to different words in a sequence to form a contextualized representation of a target word. This is evident from the non-uniform attention distribution shown by the differing intensities of blue lines and bars connecting "it" to "chicken", "road", and "The".
2.  **Coreference Resolution Capability:** The image demonstrates the model's ability to perform coreference resolution. The strong attention from the pronoun "it" to "chicken" and "road" indicates that the model has learned to associate the pronoun with its most probable antecedents in the sentence, as supported by the text's explanation of "it" plausibly coreferring with these words.
3.  **Layered Representation Learning:** The concept of "Layer k" and "Layer k+1" highlights that word representations are iteratively refined across multiple layers, with each layer building upon the contextual information gathered from previous layers using the self-attention mechanism.
4.  **Semantic Relationship Identification:** The attention mechanism effectively identifies semantically relevant words. For "it", "chicken" and "road" are deemed most important, while other words are given less weight, showcasing the model's understanding of word relationships within the sentence structure.

**Conclusions/Insights:**
*   The visualization provides strong evidence that the self-attention mechanism in Transformers is a powerful tool for capturing long-range dependencies and complex linguistic phenomena like coreference, leading to more nuanced and contextually rich word representations.
*   The varying attention weights are not arbitrary but reflect the semantic likelihood of connections, making the model's internal workings interpretable in specific instances.

**Textual Evidence:**
*   **"self-attention distribution" (label):** This directly identifies the mechanism responsible for the observed weighting.
*   **Tokens "The chicken didn't cross the road because it was too tired" (at both layers):** This provides the specific input sentence that the model processes.
*   **"Layer k+1" and "Layer k" (labels):** These denote the multi-layered architecture, supporting the idea of iterative representation learning.
*   **Visual intensity of connections from "it" (Layer k+1) to "chicken" (Layer k) (darkest blue) and "road" (Layer k) (prominent blue):** This is the direct visual and textual evidence for the model assigning high attention to these words, confirming their semantic relevance for understanding "it" and demonstrating coreference resolution.

**Document Context:**
The image is highly relevant to the document's narrative, specifically Section 9.1 "Attention", as it provides a concrete visual example of the self-attention mechanism, a fundamental concept in Transformer models. It illustrates the "self-attention weight distribution α" mentioned in the text following the image, making the abstract concept tangible.

The diagram directly addresses how a model "attend[s] differently to the various words at layer k" when computing a representation for a word like "it" at "layer k + 1". By showing "it" attending highly to "chicken" and "road", the image visually confirms the explanation that "it could plausibly corefer with the chicken or the road, and hence we'd like the representation for it to draw on the representation for these earlier words." This directly supports the broader discussion on how Transformer models build contextual understanding for words, particularly for tasks like coreference resolution, which is a key aspect of natural language comprehension.

**Summary:**
This diagram illustrates the "self-attention distribution" within a Transformer model when computing the representation for the word "it" at "Layer k+1" based on the input tokens at "Layer k".

At the top, the label "columns corresponding to input tokens" indicates that each vertical stack of text represents a word from the input sentence.

The sentence being processed is "The chicken didn't cross the road because it was too tired." This sentence is displayed twice: once for "Layer k+1" (the current processing layer for "it") and once for "Layer k" (the previous layer from which attention is drawn).

**Layer k+1:**
*   The words are displayed vertically from left to right: "The", "chicken", "didn't", "cross", "the", "road", "because", "it", "was", "too", "tired".
*   The word "it" is highlighted with a prominent blue vertical bar, indicating it is the target word for which a new representation is being computed at this layer.
*   The words "was", "too", "tired" are shown in a lighter gray font, suggesting they are subsequent tokens in the sequence relative to "it".

**Self-attention distribution:**
*   This section visually connects "it" at "Layer k+1" to various words at "Layer k" using blue lines. The thickness and darkness of these lines, along with the corresponding blue bars at "Layer k", represent the strength of the self-attention weights. Darker and thicker lines/bars signify higher attention.
*   A prominent, dark blue line connects "it" (Layer k+1) down to "chicken" (Layer k). The word "chicken" at Layer k also has a dark blue vertical bar, indicating it receives the highest attention from "it".
*   A lighter, but still distinct, blue line connects "it" (Layer k+1) down to "road" (Layer k). The word "road" at Layer k has a corresponding blue vertical bar, showing significant attention, but less than "chicken".
*   A very faint, light blue line connects "it" (Layer k+1) down to "The" (Layer k). The word "The" at Layer k has a very light blue vertical bar, indicating minimal attention.

**Layer k:**
*   The full sentence tokens are again displayed vertically: "The", "chicken", "didn't", "cross", "the", "road", "because", "it", "was", "too", "tired".
*   Consistent with the attention distribution, "The", "chicken", and "road" are highlighted with blue bars corresponding to the attention received. The other words "didn't", "cross", "the", "because" are not highlighted.
*   The words "it", "was", "too", "tired" at Layer k are also displayed in a lighter gray font, indicating they are part of the input sequence but are beyond the point where attention is being drawn from for the current target word "it" (which appears later in the sentence).

In essence, the diagram shows that when the model is trying to understand or create a representation for the pronoun "it" in the sentence, it focuses most heavily on the words "chicken" and "road" from the previous layer, indicating their semantic relevance for resolving what "it" refers to. This visualizes the coreference resolution capability enabled by the self-attention mechanism.](images/3123696038ff74831ea8b9ad9d018d7044ae9cf3aa774db0b7b3f1425583ca12.jpg)
Figure 9.2 The self-attention weight distribution $\alpha$ that is part of the computation of the representation for the word $i t$ at layer $k + 1$ . In computing the representation for $i t$ , we attend differently to the various words at layer $k$ , with darker shades indicating higher self-attention values. Note that the transformer is attending highly to the columns corresponding to the tokens chicken and road , a sensible result, since at the point where $i t$ occurs, it could plausibly corefer with the chicken or the road, and hence we’d like the representation for $i t$ to draw on the representation for these earlier words. Figure adapted from Uszkoreit (2017).

Fig. 9.2 shows a schematic example simplified from a transformer (Uszkoreit, 2017). The figure describes the situation when the current token is $\mathrm { i t }$ and we need to compute a contextual representation for this token at layer $k + 1$ of the transformer, drawing on the representations (from layer $k$ ) of every prior token. The figure uses color to represent the attention distribution over the contextual words: the tokens chicken and road both have a high attention weight, meaning that as we are computing the representation for it, we will draw most heavily on the representation for chicken and road. This will be useful in building the final representation for it, since it will end up coreferring with either chicken or road.

Let’s now turn to how this attention distribution is represented and computed.

# 9.1.1 Attention more formally

As we’ve said, the attention computation is a way to compute a vector representation for a token at a particular layer of a transformer, by selectively attending to and integrating information from prior tokens at the previous layer. Attention takes an input representation $\pmb { x } _ { i }$ corresponding to the input token at position $i$ , and a context window of prior inputs $\mathbf { x } _ { 1 } . . \mathbf { x } _ { i - 1 }$ , and produces an output $\mathbf { a } _ { i }$ .

In causal, left-to-right language models, the context is any of the prior words. That is, when processing $\pmb { x } _ { i }$ , the model has access to $\pmb { x } _ { i }$ as well as the representations of all the prior tokens in the context window (context windows consist of thousands of tokens) but no tokens after i. (By contrast, in Chapter 11 we’ll generalize attention so it can also look ahead to future words.)

Fig. 9.3 illustrates this flow of information in an entire causal self-attention layer, in which this same attention computation happens in parallel at each token position i. Thus a self-attention layer maps input sequences $( \mathbf { x } _ { 1 } , . . . , \mathbf { x } _ { n } )$ to output sequences of the same length $\left( \mathbf { a } _ { 1 } , . . . , \mathbf { a } _ { n } \right)$ .

![## Image Analysis: 77625f046fe989d83c6e55dfc5213b68b8af3be80558aafddfc314d420bc58a2.jpg

**Conceptual Understanding:**
This image conceptually illustrates the information flow in a causal self-attention mechanism, a fundamental component in certain neural network architectures, particularly Transformers used in natural language processing for tasks like language generation. The main purpose of the image is to visually demonstrate how, for each output attention vector (a
ᵢ), the underlying 'attention' calculation is restricted to only consider the current input token (x
ᵢ) and all preceding input tokens (x
₁, ..., x
ᵢ), while explicitly excluding any future input tokens. It communicates the key idea of a 'causal mask' by showing selective connections, ensuring that information flows only in one direction through the sequence.

**Content Interpretation:**
The image displays a conceptual model of a causal self-attention mechanism within a neural network layer. It illustrates the specific dependencies between sequential inputs and outputs. The core process shown is the computation of an attention vector for each input token, where each computation is constrained to only 'attend' to the current and preceding input tokens in the sequence. This creates a directed, non-future-looking information flow. The key components are the input tokens (x
₁ to x
₅), the 'Self-Attention Layer' which contains multiple 'attention' blocks, and the output attention vectors (a
₁ to a
₅). The arrows clearly define the causal relationships: the 'attention' block for a
ᵢ only receives input from x
₁, ..., x
ᵢ.

**Key Insights:**
The main takeaway from this image is the precise definition of causal self-attention: for any given output a
ᵢ, its computation depends only on the current input x
ᵢ and all preceding inputs (x
₁ to x
ᵢ), but *not* on any subsequent inputs. This is evidenced by the specific arrow connections: the 'attention' box for a
₃, for example, receives arrows from x
₁, x
₂, and x
₃, but not from x
₄ or x
₅. This causal masking prevents information leakage from future elements in a sequence, which is crucial for tasks like language generation where a model should not 'see' future words. The image also demonstrates that the 'Self-Attention Layer' processes each token individually through its own 'attention' mechanism, while simultaneously enforcing this causal constraint across the sequence. Each 'attention' mechanism within the 'Self-Attention Layer' thus performs a localized, context-aware computation, contributing to the overall sequential processing.

**Document Context:**
This image, described as 'Figure 9.3 Information flow in causal self-attention' and located in 'Section: 9.1.1 Attention more formally', directly supports the document's explanation of causal self-attention. It visually clarifies the principle that 'When processing each input x
ᵢ, the model attends to all the inputs up to, and including x
ᵢ.' The diagram provides a concrete visual representation of this concept, making the abstract idea of masked or causal attention much easier to understand by showing the specific input dependencies for each output element in a sequence. It grounds the formal definition with a clear example of information flow.

**Summary:**
The image depicts the information flow within a causal self-attention layer. It illustrates how five sequential input tokens, labeled x
₁, x
₂, x
₃, x
₄, and x
₅, are processed through a "Self-Attention Layer" to produce five corresponding output attention vectors, labeled a
₁, a
₂, a
₃, a
₄, and a
₅. Each output attention vector a
ᵢ is computed by an individual "attention" mechanism within the layer. Crucially, the connections show that the attention mechanism for generating a
ᵢ only considers the input tokens from x
₁ up to and including x
ᵢ. For instance, the attention mechanism for a
₁ receives input solely from x
₁. The attention mechanism for a
₂ receives inputs from x
₁ and x
₂. This pattern continues, with the attention mechanism for a
₃ receiving inputs from x
₁, x
₂, and x
₃; for a
₄ from x
₁, x
₂, x
₃, and x
₄; and for a
₅ from all previous inputs x
₁, x
₂, x
₃, x
₄, and x
₅. This causal structure ensures that future information (tokens x
ⱼ where j > i) does not influence the computation of the current output a
ᵢ.](images/77625f046fe989d83c6e55dfc5213b68b8af3be80558aafddfc314d420bc58a2.jpg)
Figure 9.3 Information flow in causal self-attention. When processing each input $\pmb { x } _ { i }$ , the model attends to all the inputs up to, and including $\mathbf { x } _ { i }$ .

Simplified version of attention At its heart, attention is really just a weighted sum of context vectors, with a lot of complications added to how the weights are computed and what gets summed. For pedagogical purposes let’s first describe a simplified intuition of attention, in which the attention output $\mathbf { a } _ { i }$ at token position $i$ is simply the weighted sum of all the representations $\mathbf { \boldsymbol { x } } _ { j }$ , for all $j \le i$ ; we’ll use $\alpha _ { i j }$ to mean how much $\mathbf { \boldsymbol { x } } _ { j }$ should contribute to $\mathbf { a } _ { i }$ :

$$
S i m p l i f i e d { \nu } e r s i o n \colon \mathbf { a } _ { i } = \sum _ { j \leq i } \alpha _ { i j } \mathbf { x } _ { j }
$$

Each $\alpha _ { i j }$ is a scalar used for weighing the value of input $\mathbf { \boldsymbol { x } } _ { j }$ when summing up the inputs to compute $\mathbf { a } _ { i }$ . How shall we compute this $\alpha$ weighting? In attention we weight each prior embedding proportionally to how similar it is to the current token i. So the output of attention is a sum of the embeddings of prior tokens weighted by their similarity with the current token embedding. We compute similarity scores via dot product, which maps two vectors into a scalar value ranging from $- \infty$ to $\infty$ . The larger the score, the more similar the vectors that are being compared. We’ll normalize these scores with a softmax to create the vector of weights $\alpha _ { i j } , j \leq i$ .

$$
\begin{array} { r l } { S i m p l i f i e d ~ V e r s i o n { : } } & { \mathrm { s c o r e } ( \mathbf { x } _ { i } , \mathbf { x } _ { j } ) ~ = ~ \mathbf { x } _ { i } \cdot \mathbf { x } _ { j } } \\ { \alpha _ { i j } ~ = ~ \mathrm { s o f t m a x } ( \mathrm { s c o r e } ( \mathbf { x } _ { i } , \mathbf { x } _ { j } ) ) ~ \forall j \le i } \end{array}
$$

Thus in Fig. 9.3 we compute $\mathbf { a } _ { 3 }$ by computing three scores: $\mathbf { x } _ { 3 } \cdot \mathbf { x } _ { 1 } , \mathbf { x } _ { 3 } \cdot \mathbf { x } _ { 2 }$ and $\mathbf { x } _ { 3 } \cdot \mathbf { x } _ { 3 }$ normalizing them by a softmax, and using the resulting probabilities as weights indicating each of their proportional relevance to the current position $i$ . Of course, the softmax weight will likely be highest for $\pmb { x } _ { i }$ , since $\pmb { x } _ { i }$ is very similar to itself, resulting in a high dot product. But other context words may also be similar to $i$ , and the softmax will also assign some weight to those words. Then we use these weights as the $\alpha$ values in Eq. 9.6 to compute the weighted sum that is our $\mathbf { a } _ { 3 }$ .

The simplified attention in equations $9 . 6 - 9 . 8 $ demonstrates the attention-based approach to computing $\mathbf { a } _ { i }$ : compare the $\pmb { x } _ { i }$ to prior vectors, normalize those scores into a probability distribution used to weight the sum of the prior vector. But now we’re ready to remove the simplifications.

attention head head

A single attention head using query, key, and value matrices Now that we’ve seen a simple intuition of attention, let’s introduce the actual attention head, the version of attention that’s used in transformers. (The word head is often used in transformers to refer to specific structured layers). The attention head allows us to distinctly represent three different roles that each input embedding plays during the course of the attention process:

value

• As the current element being compared to the preceding inputs. We’ll refer to this role as a query.   
• In its role as a preceding input that is being compared to the current element to determine a similarity weight. We’ll refer to this role as a key.   
• And finally, as a value of a preceding element that gets weighted and summed up to compute the output for the current element.

To capture these three different roles, transformers introduce weight matrices $\mathbf { \Delta } \mathsf { w } ^ { \mathsf { Q } } , \mathsf { w } ^ { \kappa }$ , and $\boldsymbol { \mathsf { W } } ^ { \boldsymbol { \mathsf { v } } }$ . These weights will project each input vector $\pmb { x } _ { i }$ into a representation of its role as a key, query, or value:

$$
\mathbf { q } _ { i } = \mathbf { x } _ { i } \mathbf { W } ^ { \mathbf { Q } } ; \quad \mathbf { k } _ { i } = \mathbf { x } _ { i } \mathbf { W } ^ { \mathsf { K } } ; \quad \mathbf { v } _ { i } = \mathbf { x } _ { i } \mathbf { W } ^ { \mathsf { V } }
$$

Given these projections, when we are computing the similarity of the current element $\pmb { x } _ { i }$ with some prior element $\mathbf { x } _ { j }$ , we’ll use the dot product between the current element’s query vector $\mathbf { q } _ { i }$ and the preceding element’s key vector $\mathsf { k } _ { j }$ . Furthermore, the result of a dot product can be an arbitrarily large (positive or negative) value, and exponentiating large values can lead to numerical issues and loss of gradients during training. To avoid this, we scale the dot product by a factor related to the size of the embeddings, via dividing by the square root of the dimensionality of the query and key vectors $( d _ { k } )$ . We thus replace the simplified Eq. 9.7 with Eq. 9.11. The ensuing softmax calculation resulting in $\alpha _ { i j }$ remains the same, but the output calculation for headi is now based on a weighted sum over the value vectors $\pmb { v }$ (Eq. 9.13).

Here’s a final set of equations for computing self-attention for a single selfattention output vector $\mathbf { a } _ { i }$ from a single input vector $\pmb { x } _ { i }$ . This version of attention computes $\mathbf { a } _ { i }$ by summing the values of the prior elements, each weighted by the similarity of its key to the query from the current element:

$$
\begin{array} { c l } { { \displaystyle { \bf q } _ { i } = { \bf x } _ { i } \mathsf { W } ^ { \bf Q } ; ~ { \bf k } _ { j } ~ = ~ { \bf x } _ { j } \mathsf { W } ^ { \bf K } ; ~ { \bf v } _ { j } = { \bf x } _ { j } \mathsf { W } ^ { \vee } } } \\ { { \displaystyle \mathrm { s c o r e } ( { \bf x } _ { i } , { \bf x } _ { j } ) ~ = ~ \frac { { \bf q } _ { i } \cdot { \bf k } _ { j } } { \sqrt { d _ { k } } } } } \\ { { \displaystyle \alpha _ { i j } ~ = ~ \mathrm { s o f t m a x } ( \mathrm { s c o r e } ( { \bf x } _ { i } , { \bf x } _ { j } ) ) ~ \forall j \le i } } \\ { { \displaystyle { \bf h e a d } _ { i } ~ = ~ \sum _ { j \le i } \alpha _ { i j } { \bf v } _ { j } } } \\ { { \displaystyle ~ { \bf a } _ { i } ~ = ~ { \bf h e a d } _ { i } \mathsf { W } ^ { 0 } } } \end{array}
$$

![## Image Analysis: 4bf4b9c4669394d6ed7c1ec251a49b73a9dbb293ee7ebe5b64904b14720dca28.jpg

**Conceptual Understanding:**
This image conceptually represents the computational pipeline for a single attention head within a self-attention mechanism, specifically demonstrating the calculation of the output representation for the third element (a_3) of an input sequence. The main purpose is to illustrate, in a step-by-step manner, how the concept of 'attention' is formalized and computed. It conveys the key idea that the contextualized representation of a sequence element is formed by comparing its 'query' with the 'keys' of other relevant elements, deriving attention weights, and then using these weights to form a weighted sum of the 'value' representations of those elements, all while adhering to a causal (left-to-right) constraint.

**Content Interpretation:**
The image depicts the architecture and computational flow for a single head of causal self-attention, specifically demonstrating how the output vector a_3 for the third element in a sequence is computed. It illustrates the transformation of input elements (X_1, X_2, X_3) into key, query, and value vectors, the calculation of attention scores through comparison and scaling, the generation of softmax weights (α_i,j), the weighting of value vectors, and finally, the summation and reshaping to produce the final output a_3. The 'causal' aspect is implied by only considering X_1, X_2, and X_3 when calculating a_3, not subsequent elements. It shows the fundamental operations involved in self-attention: linear transformations, dot product similarity, scaling, softmax normalization, weighted summation, and a final linear projection.

**Key Insights:**
1.  **Fundamental Self-Attention Steps:** The image breaks down self-attention into eight core, sequential steps: generating key, query, and value vectors; comparing query with keys; dividing by a scaling factor; applying softmax to get weights; weighing value vectors; summing weighted vectors; reshaping; and outputting the final attention vector. This provides a clear blueprint for the self-attention mechanism. 2.  **Role of Query, Key, and Value Vectors:** It explicitly shows that a 'query' vector (from X_3) is compared with 'key' vectors (from X_1, X_2, X_3) to determine relevance, and these relevance scores are then used as weights for 'value' vectors. 3.  **Causal/Masked Attention:** The diagram implicitly demonstrates causal attention by showing that the calculation of a_3 only considers X_1, X_2, and X_3, meaning only preceding and current elements are used to compute the attention for the current element in a sequence. 4.  **Softmax for Weight Normalization:** The use of softmax (Step 4) is critical for normalizing the attention scores into a probability distribution (α_i,j weights), ensuring they sum to 1 and can be interpreted as 'attention distribution'. 5.  **Weighted Sum of Values:** The core output of self-attention (before final reshaping) is a weighted sum of the value vectors, indicating that the final representation incorporates information from other elements proportionally to their relevance. 6.  **Dimensionality Management:** The image clearly labels the dimensions at various stages (e.g., [1 x d], [1 x d_v], [d_v x d]), highlighting how vector and matrix transformations maintain or change feature dimensions.

**Document Context:**
This image is highly relevant to the section titled "9.1.1 Attention more formally" as it visually and concretely explains the computational steps involved in a specific type of attention mechanism: causal self-attention. By detailing how a_3 is calculated, it provides a foundational understanding of how attention weights are derived and applied to combine information from different parts of a sequence to produce a contextualized representation for a single element. This visual breakdown serves as a crucial aid in formalizing the concept of attention, illustrating the mathematical operations that underpin the abstract idea of 'paying attention' to relevant parts of the input sequence.

**Summary:**
The image illustrates the detailed step-by-step process of calculating the value of a_3, the third element of a sequence, using causal (left-to-right) self-attention. The process starts with generating key, query, and value vectors for three input elements, X_1, X_2, and X_3. For each input, these vectors are derived using transformation matrices W^K, W^Q, and W^V. The query vector from X_3 is then compared with the key vectors from X_1, X_2, and X_3, producing scalar scores. These scores are subsequently divided by the square root of d_k to scale them. The scaled scores are then passed through a softmax function to generate attention weights (α_3,1, α_3,2, α_3,3). Each of these weights is then used to weigh its corresponding value vector, resulting in three weighted value vectors, each with dimensions [1 x d_v]. These weighted value vectors are summed together. The resulting summed vector, also [1 x d_v], is then reshaped to [1 x d] by multiplying it with an output weight matrix W^O of dimensions [d_v x d]. The final output is a_3, the self-attention output for the third element, with dimensions [1 x d]. The entire process highlights how the output for a specific element (a_3) is influenced by preceding and current elements (X_1, X_2, X_3) through a weighted sum of their value representations, where the weights are determined by the similarity between the query of the target element and the keys of the context elements.](images/4bf4b9c4669394d6ed7c1ec251a49b73a9dbb293ee7ebe5b64904b14720dca28.jpg)
Figure 9.4 Calculating the value of ${ \bf a } _ { 3 }$ , the third element of a sequence using causal (leftto-right) self-attention.

We illustrate this in Fig. 9.4 for the case of calculating the value of the third output $\mathbf { a } _ { 3 }$ in a sequence.

Note that we’ve also introduced one more matrix, ${ \sf { W } } _ { 0 }$ , which is right-multiplied by the attention head. This is necessary to reshape the output of the head. The input to attention $\bf { x } _ { i }$ and the output from attention $\mathbf { a } _ { \mathbf { i } }$ both have the same dimensionality $[ 1 \times d ]$ . We often call $d$ the model dimensionality, and indeed as we’ll discuss in Section 9.2 the output $\mathbf { h } _ { \mathbf { i } }$ of each transformer block, as well as the intermediate vectors inside the transformer block also have the same dimensionality $[ 1 \times d ]$ . Having everything be the same dimensionality makes the transformer very modular.

So let’s talk shapes. How do we get from $[ 1 \times d ]$ at the input to $[ 1 \times d ]$ at the output? Let’s look at all the internal shapes. We’ll have a dimension $d _ { k }$ for the key and query vectors. The query vector and the key vector are both dimensionality $1 \times d _ { k }$ , so we can take their dot product ${ \pmb q } _ { i } \cdot { \pmb k } _ { j }$ to produce a scalar. We’ll have a separate dimension $d _ { \nu }$ for the value vectors. The transform matrix $\boldsymbol { \mathsf { W } } ^ { \mathbf { Q } }$ has shape $[ d \times d _ { k } ]$ , $\boldsymbol { \mathsf { W } } ^ { \mathsf { K } }$ is $[ d \times d _ { k } ]$ , and $\boldsymbol { \mathsf { W } } ^ { \boldsymbol { \mathsf { v } } }$ is $[ d \times d _ { \nu } ]$ . So the output of $\mathbf { h e a d } _ { i }$ in equation Eq. 9.13 is of shape $[ 1 \times d _ { \nu } ]$ . To get the desired output shape $[ 1 \times d ]$ we’ll need to reshape the head output, and so $\boldsymbol { \mathsf { W } } ^ { 0 }$ is of shape $[ d _ { \nu } \times d ]$ . In the original transformer work (Vaswani et al., 2017), $d$ was 512, $d _ { k }$ and $d _ { \nu }$ were both 64.

Multi-head Attention Equations 9.11-9.13 describe a single attention head. But actually, transformers use multiple attention heads. The intuition is that each head might be attending to the context for different purposes: heads might be specialized to represent different linguistic relationships between context elements and the current token, or to look for particular kinds of patterns in the context.

So in multi-head attention we have $A$ separate attention heads that reside in parallel layers at the same depth in a model, each with its own set of parameters that allows the head to model different aspects of the relationships among inputs. Thus each head $i$ in a self-attention layer has its own set of key, query and value matrices: $\boldsymbol { \mathsf { W } } ^ { \kappa \mathrm { i } }$ , $\boldsymbol { \mathsf { W } } ^ { \mathrm { Q i } }$ and $\boldsymbol { \mathsf { W } } ^ { \mathsf { V i } }$ . These are used to project the inputs into separate key, value, and query embeddings for each head.

When using multiple heads the model dimension $d$ is still used for the input and output, the key and query embeddings have dimensionality $d _ { k }$ , and the value embeddings are of dimensionality $d _ { \nu }$ (again, in the original transformer paper $d _ { k } =$ $d _ { \nu } = 6 4$ , $A = 8$ , and $d = 5 1 2$ ). Thus for each head $i$ , we have weight layers $\boldsymbol { \mathsf { W } } ^ { \mathrm { Q i } }$ of shape $[ d \times d _ { k } ]$ , $\boldsymbol { \mathsf { W } } ^ { \kappa \mathrm { i } }$ of shape $[ d \times d _ { k } ]$ , and ${ \boldsymbol { \mathsf { W } } } ^ { \mathsf { V i } }$ of shape $[ d \times d _ { \nu } ]$ .

Below are the equations for attention augmented with multiple heads; Fig. 9.5 shows an intuition.

$$
\begin{array} { r l } { \mathbf { q } _ { i } ^ { c } = \mathbf { x } _ { i } \mathsf { W } ^ { \mathsf { q c } } ; ~ \mathbf { k } _ { j } ^ { c } = \mathbf { x } _ { j } \mathsf { W } ^ { \mathsf { c c } } ; ~ \mathbf { w } _ { j } ^ { c } = ~ \mathbf { x } _ { j } \mathsf { W } ^ { \mathsf { v c } } ; ~ \forall c } & { 1 \le c \le A } \\ { \mathrm { s c o r e } ^ { c } ( \mathbf { x } _ { i } , \mathbf { x } _ { j } ) } & { = ~ \frac { \mathbf { q } _ { i } ^ { c } \cdot \mathbf { k } _ { j } ^ { c } } { \sqrt { d _ { k } } } } \\ { \alpha _ { i j } ^ { c } } & { = \mathrm { s o f m a x } ( \mathrm { s c o r e } ^ { c } ( \mathbf { x } _ { i } , \mathbf { x } _ { j } ) ) ~ \forall j \le i } \\ { \mathbf { h e a d } _ { i } ^ { c } } & { = ~ \displaystyle \sum _ { j \le i } \alpha _ { i j } ^ { c } \mathbf { v } _ { j } ^ { c } } \\ { \mathbf { a } _ { i } } & { = ~ ( \mathbf { h e a d } ^ { 1 } \oplus \mathbf { h e a d } ^ { 2 } . . . \oplus \mathbf { h e a d } ^ { A } ) \mathsf { W } ^ { O } } \\ { \mathrm { u l t i H e a d } \mathrm { A t t e n t i o n } ( \mathbf { x } _ { i } , [ \mathbf { x } _ { 1 } , \cdots , \mathbf { x } _ { N } ] ) } & { = ~ \mathbf { a } _ { i } } \end{array}
$$

The output of each of the $A$ heads is of shape $1 \times d _ { \nu }$ , and so the output of the multi-head layer with $A$ heads consists of $A$ vectors of shape $1 \times d _ { \nu }$ . These are concatenated to produce a single output with dimensionality $1 \times h d _ { \nu }$ . Then we use yet another linear projection $\bar { \mathsf { w } ^ { 0 } } \in \bar { \mathbb { R } ^ { A d _ { \nu } \times d } }$ to reshape it, resulting in the multi-head attention vector $\mathbf { a } _ { i }$ with the correct output shape $[ 1 \times d ]$ at each input $i$ .

![## Image Analysis: 9ca0584c7e3c1b99a8c64a5d95124d48ecd31e3f4b9819c5fe39b7f4073b109c.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and computational flow of a multi-head attention mechanism, a core component in modern neural network models, particularly the Transformer architecture.

The main purpose of this mechanism is to allow a model to process an input (X_i) by jointly attending to different parts of the input sequence and contextual information (x_{i-k}). It enables the model to focus on diverse aspects of the input's relationships with its context simultaneously.

The key idea being communicated is that by running multiple "attention heads" in parallel, each learning different weight matrices (W^K, W^V, W^Q) and therefore attending to context differently, the model can capture a richer set of relationships and dependencies than a single attention mechanism could. The outputs of these diverse attention perspectives are then combined and projected back to the original input dimension.

**Content Interpretation:**
This diagram illustrates the "multi-head attention" system.

*   **Input and Context:** The process starts with a primary input X_i (of dimension [1 x d]) and additional contextual inputs x_{i-3}, x_{i-2}, x_{i-1}. These represent a single token or position's embedding and its preceding context in a sequence. The dotted lines connecting these inputs to all attention heads indicate that *each head* considers *all these inputs* when forming its representations.

*   **Parallel Attention Heads (Head 1 to Head 8):** The core of the system consists of multiple parallel "Heads". The numerical labels 1 through 8 (with `...` indicating more in between) suggest that typically 8 or more heads are used.
    *   **Weight Matrices:** Each head (Head_k) possesses its own unique set of weight matrices: W^K_k, W^V_k, W^Q_k. These are for transforming the input X_i and contextual inputs into Key, Value, and Query vectors, respectively, for that specific head. The fact that each head has *its own* matrices (W^K_1, W^V_1, W^Q_1 distinct from W^K_2, W^V_2, W^Q_2, etc.) is crucial.
    *   **Diverse Attention:** The annotation "Each head attends differently to context" directly supports the interpretation that each head learns to focus on different aspects of the input and its context. For example, one head might prioritize grammatical relationships, another might focus on semantic similarity, and yet another on word order.
    *   **Head Output Dimension:** Each head produces an output of dimension [1 x d_V]. This means that d_V is the dimension of the value vector output by a single attention head.

*   **Concatenation ([1 x hd_V]):** After each head independently processes the input, their respective outputs (each [1 x d_V]) are then concatenated. The label "Concatenate Outputs" confirms this step. If there are 'h' heads, the combined dimension becomes [1 x hd_V]. This step essentially aggregates the diverse perspectives learned by each head into a single, wider representation.

*   **Projection (W^O [hd_V x d]):** The concatenated output [1 x hd_V] is then passed through a final linear projection layer. This layer uses a weight matrix W^O of dimension [hd_V x d]. The label "Project down to d" clearly indicates that the purpose of this layer is to transform the wider concatenated vector back to the original input dimension 'd'. This ensures that the multi-head attention block can be stacked or integrated into a network while maintaining consistent data dimensions.

*   **Final Output (a_i [1 x d]):** The result of this projection is the final output a_i, which has the same dimension [1 x d] as the initial input X_i. This output a_i encapsulates a rich, context-aware representation of X_i, having benefited from multiple, diverse attention mechanisms.

**Key Insights:**
The image teaches several key lessons about multi-head attention:

*   **Parallel Processing for Richer Contextual Understanding:** The existence of Head 1, Head 2, through Head 8 (and implicitly more) working in parallel, each with its own W^K, W^V, W^Q matrices, highlights that the mechanism is designed to process information simultaneously through different "lenses." The label "Each head attends differently to context" directly supports the insight that this parallel processing allows for a more nuanced and comprehensive understanding of the input's relationship to its context. Instead of a single interpretation, the model gathers multiple, distinct perspectives.

*   **Dimension Management and Preservation:** The dimension annotations are crucial. The input X_i is [1 x d]. Each head produces an output [1 x d_V]. These are concatenated to [1 x hd_V]. Finally, a projection W^O [hd_V x d] converts this back to [1 x d] for the final output a_i. This demonstrates that while intermediate processing expands the dimensionality to capture more information, the mechanism is designed to project the output back to the original input dimension. This is vital for modularity and stacking in larger network architectures, as confirmed by the labels "Project down to d" and the output a_i [1 x d].

*   **Hierarchical Information Flow:** The diagram clearly shows a hierarchical flow:
    1.  Inputs (X_i, x_{i-k}) are fed into multiple, independent attention "Heads."
    2.  Outputs from these heads are then *concatenated* horizontally, combining their individual insights.
    3.  This combined representation is then *projected* vertically, summarizing the aggregated information into the desired output dimension.
    This structured flow ensures that diverse attentional information is first generated, then gathered, and finally refined into a usable output.

*   **Flexibility and Scalability:** The ellipses `...` between Head 2 and Head 8 imply that the number of heads (`h`) can be varied. This design allows for scalability; more heads can potentially capture even more diverse relationships, enhancing the model's capacity for understanding complex patterns.

**Document Context:**
This image (Figure 9.5) is presented in Section 9.1.1, titled "Attention more formally," and is directly followed by a caption explaining "The multi-head attention computation for input $\pmb { x } _ { i }$ , producing output $\mathbf { a } _ { i }$ . A multi-head attention layer has A heads, each with its own key, query and value weight matrices. The outputs from each of the heads are concatenated and then projected down to $d$ , thus producing an output of the same size as the input." This confirms the image's role as a detailed visual explanation of the multi-head attention mechanism, expanding on the formal definition of attention. It provides a visual roadmap for how the abstract concept of multi-head attention is computationally realized, breaking down the process into its constituent layers and operations. It serves to concretize the theoretical explanation of attention by showing the data flow and transformations involved.

**Summary:**
This diagram illustrates the step-by-step process of a multi-head attention computation, a fundamental building block in advanced neural networks like the Transformer. It takes an input X_i, along with surrounding contextual information, and processes it through multiple parallel "attention heads" to produce a rich, context-aware output a_i that maintains the original input's dimension.

The process begins with an input X_i (represented as a vector of size [1 x d]) and several preceding contextual inputs such as x_{i-3}, x_{i-2}, x_{i-1}. These inputs collectively feed into a layer composed of multiple independent "Heads" (like Head 1, Head 2, all the way to Head 8, with more implied by `...`).

Each individual Head (e.g., Head 1) is equipped with its own unique set of weight matrices: W^K_1, W^V_1, and W^Q_1. These matrices are used by the head to generate key, value, and query representations from the input and context. The crucial concept here, highlighted by the text "Each head attends differently to context," is that every head learns to focus on distinct aspects or relationships within the input and its context. For example, one head might look for semantic relationships, while another might identify syntactic structures. Each head then outputs a processed vector of a specific dimension, [1 x d_V].

Once all the heads have produced their individual [1 x d_V] outputs, these outputs are brought together. This is where the "Concatenate Outputs" step occurs. All the [1 x d_V] vectors from each of the 'h' heads are joined side-by-side, forming a larger, concatenated vector with a new dimension of [1 x hd_V]. This wider vector now contains the aggregated information from all the diverse attention perspectives.

Finally, this concatenated vector [1 x hd_V] undergoes a "Projection" step. As indicated by the label "Project down to d", this step uses a projection weight matrix W^O of size [hd_V x d]. The purpose of this projection is to reduce the dimensionality of the aggregated vector back to [1 x d], matching the original input dimension. The result of this final projection is the output a_i, which is also of dimension [1 x d].

In essence, the multi-head attention mechanism allows a model to simultaneously consider different angles of an input's relationship to its context, combine these varied insights, and then present them in a standardized output format.](images/9ca0584c7e3c1b99a8c64a5d95124d48ecd31e3f4b9819c5fe39b7f4073b109c.jpg)
Figure 9.5 The multi-head attention computation for input $\pmb { x } _ { i }$ , producing output $\mathbf { a } _ { i }$ . A multi-head attention layer has A heads, each with its own key, query and value weight matrices. The outputs from each of the heads are concatenated and then projected down to $d$ , thus producing an output of the same size as the input.

# 9.2 Transformer Blocks

The self-attention calculation lies at the core of what’s called a transformer block, which, in addition to the self-attention layer, includes three other kinds of layers: (1) a feedforward layer, (2) residual connections, and (3) normalizing layers (colloquially called “layer norm”).

![## Image Analysis: 5866822f29a4fdde9ac03d305a44a17c40b33363dbe1de7ffbee442ea579aecd.jpg

**Conceptual Understanding:**
The image conceptually represents the internal structure and data flow within a single Transformer block, focusing on how a token's vector representation is transformed. Its main purpose is to illustrate the 'residual stream' perspective and the 'prenorm' architectural design. It conveys the key ideas of sequential processing through attention and feedforward mechanisms, coupled with residual connections and layer normalization, which are fundamental to the effectiveness of Transformer models in natural language processing and other domains. The diagram clearly segments the processing into distinct, ordered steps.

**Content Interpretation:**
This image visually represents the architecture of a transformer block, specifically highlighting the 'residual stream' concept and the 'prenorm' variant. It illustrates the sequence of operations applied to a token's representation as it passes through the block. The core components are two sequential sub-layers: MultiHead Attention and Feedforward. Each sub-layer is preceded by a Layer Normalization step, consistent with the 'prenorm' architecture. The output of each sub-layer is added back to the main 'Residual Stream' via skip connections, maintaining a continuous flow of information and facilitating gradient propagation. The diagram shows how a token's initial input embedding, 'x_i', is transformed into an output representation, 'h_i', after passing through these computational layers and residual connections.

**Key Insights:**
The main takeaway from this image is the complete process flow within a prenorm transformer block for a single token's representation. The token's processing begins with an input 'x_i' entering the 'Residual Stream'. This stream first undergoes 'Layer Norm' and then 'MultiHead Attention', whose output is added back to the stream. Subsequently, the stream again passes through 'Layer Norm' and then 'Feedforward', whose output is also added back. The final output is 'h_i'. The image emphasizes the role of 'Layer Norm' preceding the attention and feedforward layers, the use of residual connections (indicated by '+' symbols) to add sub-layer outputs back to the main stream, and the concept of a continuous 'Residual Stream' for token representation processing. The dashed lines and '...' indicate that this is a repeating block in a larger sequence of layers/tokens.

**Document Context:**
This image is directly relevant to the '9.2 Transformer Blocks' section of the document. It serves as a visual aid to explain the architecture of a transformer block, specifically the 'prenorm' version and the 'residual stream' viewpoint, which are detailed in the surrounding text. It provides a concrete illustration of how an individual token 'i' is processed through the block, showing the sequence of Layer Normalization, MultiHead Attention, and Feedforward layers, and crucially, how their outputs are integrated back into the main data flow. This visual representation is fundamental for understanding the conceptual and operational aspects of transformer models as described in the accompanying document.

**Summary:**
The image displays a detailed architecture of a single transformer block, focusing on the processing of an individual token 'i' and illustrating the concept of a 'residual stream' in a prenorm configuration. The central element is a vertical gray column labeled 'Residual Stream', representing the continuous flow of the token's representation. The process begins with the input 'x_i' entering this stream. The stream undergoes two main processing stages. In the first stage (lower half), the current state of the residual stream is fed into a 'Layer Norm' block. The output of this 'Layer Norm' then proceeds to a 'MultiHead Attention' block. The result from 'MultiHead Attention' is then added back to the 'Residual Stream' via a summation operation, indicated by a green diamond with a '+' symbol. Following this, in the second stage (upper half), the updated 'Residual Stream' is again passed through another 'Layer Norm' block. The output of this 'Layer Norm' is then fed into a 'Feedforward' block. The output of the 'Feedforward' block is subsequently added back to the 'Residual Stream' through a second summation operation (green diamond with '+'). The final output of this transformer block for token 'i' is represented as 'h_i', exiting from the top of the 'Residual Stream'. The diagram also shows dotted vertical lines and ellipses ('...') to the left and right, labeled 'h_{i-1}', 'x_{i-1}' and 'h_{i+1}', 'x_{i+1}' respectively, suggesting that this block is part of a larger sequence of transformer layers and processes tokens in context.](images/5866822f29a4fdde9ac03d305a44a17c40b33363dbe1de7ffbee442ea579aecd.jpg)
Figure 9.6 The architecture of a transformer block showing the residual stream. This figure shows the prenorm version of the architecture, in which the layer norms happen before the attention and feedforward layers rather than after.   
Fig. 9.6 illustrates a transformer block, sketching a common way of thinking about the block that is called the residual stream (Elhage et al., 2021). In the residual stream viewpoint, we consider the processing of an individual token $i$ through the transformer block as a single stream of $d$ -dimensional representations for token position $i$ . This residual stream starts with the original input vector, and the various components read their input from the residual stream and add their output back into the stream.

The input at the bottom of the stream is an embedding for a token, which has dimensionality $d$ . This initial embedding gets passed up (by residual connections), and is progressively added to by the other components of the transformer: the attention layer that we have seen, and the feedforward layer that we will introduce. Before the attention and feedforward layer is a computation called the layer norm.

Thus the initial vector is passed through a layer norm and attention layer, and the result is added back into the stream, in this case to the original input vector $\mathbf { x } _ { i }$ . And then this summed vector is again passed through another layer norm and a feedforward layer, and the output of those is added back into the residual, and we’ll use $\mathbf { h } _ { i }$ to refer to the resulting output of the transformer block for token i. (In earlier descriptions the residual stream was often described using a different metaphor as residual connections that add the input of a component to its output, but the residual stream is a more perspicuous way of visualizing the transformer.)

We’ve already seen the attention layer, so let’s now introduce the feedforward and layer norm computations in the context of processing a single input $\pmb { x } _ { i }$ at token position $i$ .

Feedforward layer The feedforward layer is a fully-connected 2-layer network, i.e., one hidden layer, two weight matrices, as introduced in Chapter 7. The weights are the same for each token position $i$ , but are different from layer to layer. It is common to make the dimensionality $d _ { \mathrm { f f } }$ of the hidden layer of the feedforward network be larger than the model dimensionality $d$ . (For example in the original transformer model, $d = 5 1 2$ and $d _ { \mathrm { f f } } = 2 0 4 8 .$ )

$$
\mathrm { F F N } ( \mathbf { x } _ { i } ) = \mathrm { R e L U } ( \mathbf { x } _ { i } \mathbf { W } _ { 1 } + b _ { 1 } ) \mathbf { W } _ { 2 } + b _ { 2 }
$$

Layer Norm At two stages in the transformer block we normalize the vector (Ba et al., 2016). This process, called layer norm (short for layer normalization), is one of many forms of normalization that can be used to improve training performance in deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training.

Layer norm is a variation of the $\mathbf { z }$ -score from statistics, applied to a single vector in a hidden layer. That is, the term layer norm is a bit confusing; layer norm is not applied to an entire transformer layer, but just to the embedding vector of a single token. Thus the input to layer norm is a single vector of dimensionality $d$ and the output is that vector normalized, again of dimensionality $d$ . The first step in layer normalization is to calculate the mean, $\mu$ , and standard deviation, $\sigma$ , over the elements of the vector to be normalized. Given an embedding vector $\pmb { \times }$ of dimensionality $d$ , these values are calculated as follows.

$$
\begin{array} { l } { \displaystyle \mu ~ = ~ \frac { 1 } { d } \sum _ { i = 1 } ^ { d } x _ { i } } \\ { \displaystyle \sigma ~ = ~ \sqrt { \frac { 1 } { d } \sum _ { i = 1 } ^ { d } ( x _ { i } - \mu ) ^ { 2 } } } \end{array}
$$

Given these values, the vector components are normalized by subtracting the mean from each and dividing by the standard deviation. The result of this computation is a new vector with zero mean and a standard deviation of one.

$$
\hat { \mathbf { x } } = \frac { ( \mathbf { x } - \mu ) } { \sigma }
$$

Finally, in the standard implementation of layer normalization, two learnable parameters, $\gamma$ and $\beta$ , representing gain and offset values, are introduced.

$$
\operatorname { L a y e r N o r m } ( \mathbf { x } ) = \gamma { \frac { ( \mathbf { x } - { \boldsymbol { \mu } } ) } { \sigma } } + \beta
$$

Putting it all together The function computed by a transformer block can be expressed by breaking it down with one equation for each component computation, using $\mathbf { t }$ (of shape $[ 1 \times d ] )$ ) to stand for transformer and superscripts to demarcate

each computation inside the block:

$$
\begin{array} { l } { \mathbf { t } _ { i } ^ { 1 } \ = \ \mathrm { L a y e r N o r m } ( \mathbf { x } _ { i } ) } \\ { \mathbf { t } _ { i } ^ { 2 } \ = \ \mathrm { M u l t i H e a d A t t e n t i o n } ( \mathbf { t } _ { i } ^ { 1 } , [ \mathbf { t } _ { 1 } ^ { 1 } , \cdots , \mathbf { t } _ { N } ^ { 1 } ] ) } \\ { \mathbf { t } _ { i } ^ { 3 } \ = \ \mathbf { t } _ { i } ^ { 2 } + \mathbf { x } _ { i } } \\ { \mathbf { t } _ { i } ^ { 4 } \ = \ \mathrm { L a y e r N o r m } ( \mathbf { t } _ { i } ^ { 3 } ) } \\ { \mathbf { t } _ { i } ^ { 5 } \ = \ \mathrm { F F N } ( \mathbf { t } _ { i } ^ { 4 } ) } \\ { \mathbf { h } _ { i } \ = \ \mathbf { t } _ { i } ^ { 5 } + \mathbf { t } _ { i } ^ { 3 } } \end{array}
$$

Notice that the only component that takes as input information from other tokens (other residual streams) is multi-head attention, which (as we see from Eq. 9.28) looks at all the neighboring tokens in the context. The output from attention, however, is then added into this token’s embedding stream. In fact, Elhage et al. (2021) show that we can view attention heads as literally moving information from the residual stream of a neighboring token into the current stream. The high-dimensional embedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space. Fig. 9.7 shows a visualization of this movement.

![## Image Analysis: fee7d8c25556267e2d89e787747075d3c2d78b13cedbd35afc4d5b40a46bcac2.jpg

**Conceptual Understanding:**
This image conceptually represents the mechanism of information flow between different token representations in a neural network, specifically highlighting the role of an attention head in a transformer architecture. The main purpose is to visually demonstrate how an attention head facilitates the movement of information from one token's residual stream (Token A) to another token's residual stream (Token B), thereby enabling tokens to build contextual awareness by attending to other tokens' data.

**Content Interpretation:**
The image illustrates the process of information movement between different token representations within a neural network architecture, specifically in the context of transformer blocks. The vertical gray bars represent 'residual streams' where token information is processed. The labels 'Token A residual stream' and 'Token B residual stream' explicitly identify the source and destination of the information flow. The prominent red arrow signifies the transfer of information. It originates from 'Token A residual stream' and terminates within 'Token B residual stream', indicating a directed flow of information from Token A to Token B. This visual representation highlights how an attention head facilitates the sharing or 'attending' of information from one token to another, allowing Token B to incorporate relevant data from Token A's representation.

**Key Insights:**
The main takeaway from this image is that attention heads within transformer blocks enable the explicit and directed transfer of information between the residual streams of different tokens. Specifically, it demonstrates that information from 'Token A residual stream' can be moved into 'Token B residual stream'. This mechanism allows tokens to incorporate contextual information from other tokens, which is fundamental to the transformer's ability to understand relationships and dependencies in sequential data. The diagram simplifies a complex operation into an intuitive visual of information flow.

**Document Context:**
This image directly supports the document's Section 9.2, "Transformer Blocks," by visually explaining a core mechanism of attention heads. As stated in the accompanying text, "An attention head can move information from token A’s residual stream into token B’s residual stream." The diagram serves as a clear illustration of this concept, demonstrating how information is selectively routed and integrated across different token processing pathways, which is crucial for understanding how transformers build contextual representations.

**Summary:**
This image visually represents the concept of information transfer between two token residual streams within a neural network, specifically illustrating how an attention head might move information. It shows five vertical gray bars, which symbolize different residual streams. Below the third bar from the left, the text "Token A residual stream" is displayed. Below the fifth (rightmost) bar, the text "Token B residual stream" is displayed. A thick, curved red arrow originates from the third gray bar (associated with "Token A residual stream"), curves upwards and to the right, crosses over the fourth gray bar, and then points upwards into the fifth gray bar (associated with "Token B residual stream"). This arrow distinctly depicts information flowing from Token A's residual stream into Token B's residual stream, mediated by an attention head. The diagram's simplicity effectively highlights the directional movement of information, which is a fundamental aspect of how transformer models process and integrate contextual information between different tokens.](images/fee7d8c25556267e2d89e787747075d3c2d78b13cedbd35afc4d5b40a46bcac2.jpg)
Figure 9.7 An attention head can move information from token A’s residual stream into token B’s residual stream.

Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Each token vector $\pmb { x } _ { i }$ at the input to the block has dimensionality $d$ , and the output $\mathbf { h } _ { i }$ also has dimensionality $d$ . Transformers for large language models stack many of these blocks, from 12 layers (used for the T5 or GPT-3-small language models) to 96 layers (used for GPT-3 large), to even more for more recent models. We’ll come back to this issue of stacking in a bit.

Equation 9.28 and following are just the equation for a single transformer block, but the residual stream metaphor goes through all the transformer layers, from the first transformer blocks to the 12th, in a 12-layer transformer. At the earlier transformer blocks, the residual stream is representing the current token. At the highest transformer blocks, the residual stream is usually representing the following token, since at the very end it’s being trained to predict the next token.

Once we stack many blocks, there is one more requirement: at the very end of the last (highest) transformer block, there is a single extra layer norm that is run on the last $\mathbf { h } _ { i }$ of each token stream (just below the language model head layer that we will define soon). 3

# 9.3 Parallelizing computation using a single matrix X

This description of multi-head attention and the rest of the transformer block has been from the perspective of computing a single output at a single time step $i$ in a single residual stream. But as we pointed out earlier, the attention computation performed for each token to compute $\mathbf { a } _ { i }$ is independent of the computation for each other token, and that’s also true for all the computation in the transformer block computing $\mathbf { h } _ { i }$ from the input $\pmb { x } _ { i }$ . That means we can easily parallelize the entire computation, taking advantage of efficient matrix multiplication routines.

We do this by packing the input embeddings for the $N$ tokens of the input sequence into a single matrix $\pmb { \times }$ of size $[ N \times d ]$ . Each row of $\pmb { \times }$ is the embedding of one token of the input. Transformers for large language models commonly have an input length $N$ from 1K to 32K; much longer contexts of 128K or even up to millions of tokens can also be achieved with architectural changes like special long-context mechanisms that we don’t discuss here. So for vanilla transformers, we can think of $\pmb { \times }$ having between 1K and 32K rows, each of the dimensionality of the embedding $d$ (the model dimension).

Parallelizing attention Let’s first see this for a single attention head and then turn to multiple heads, and then add in the rest of the components in the transformer block. For one head we multiply $\pmb { \times }$ by the key, query, and value matrices $\boldsymbol { \mathsf { W } } ^ { \mathbf { Q } }$ of shape $[ d \times d _ { k } ]$ , $\boldsymbol { \mathsf { W } } ^ { \mathsf { K } }$ of shape $[ d \times d _ { k } ]$ , and $\boldsymbol { \mathsf { W } } ^ { \boldsymbol { \mathsf { v } } }$ of shape $[ d \times d _ { \nu } ]$ , to produce matrices $\mathbf { Q }$ of shape $[ N \times d _ { k } ]$ , $\mathsf { \pmb K }$ of shape $[ N \times d _ { k } ]$ , and $\pmb { v }$ of shape $[ N \times d _ { \nu } ]$ , containing all the key, query, and value vectors:

$$
\mathsf { \mathbf { Q } } = \mathsf { \mathbf { X } } \mathsf { \mathbf { W } } ^ { \mathsf { \mathbf { Q } } } ; \mathsf { \nabla } \mathsf { K } = \mathsf { \mathbf { X } } \mathsf { \mathbf { W } } ^ { \mathsf { \times } } ; \mathsf { \nabla } \mathsf { V } = \mathsf { \mathbf { X } } \mathsf { \mathbf { W } } ^ { \mathsf { \times } }
$$

Given these matrices we can compute all the requisite query-key comparisons simultaneously by multiplying $\mathbf { Q }$ and $\mathsf { K } ^ { \top }$ in a single matrix multiplication. The product is of shape $N \times N$ , visualized in Fig. 9.8.

![## Image Analysis: 9d7261e2d3495d7b59896273348ed57539b36808721bf716352a07f884fb3959.jpg

**Conceptual Understanding:**
The image conceptually represents the output of a matrix multiplication between a Query matrix (Q) and a Key matrix (K), resulting in an attention score matrix. Its main purpose is to illustrate how all pairwise dot products between query vectors and key vectors, denoted as `q_i · k_j`, are computed in parallel and form an `N x N` matrix. This highlights a core mechanism for efficiency in self-attention and transformer models, allowing for simultaneous comparison of all query-key pairs.

**Content Interpretation:**
The image shows a visual representation of an N x N matrix, specifically a 4x4 instance, where each cell contains the dot product of a query vector 'q' and a key vector 'k'. The structure demonstrates how all pairwise comparisons between N query vectors (q1, q2, q3, q4) and N key vectors (k1, k2, k3, k4) can be simultaneously calculated and organized into a matrix. This matrix represents the result of the matrix multiplication of a query matrix Q and a key matrix K (or K transposed), where each entry (i, j) corresponds to the dot product of the i-th query vector and the j-th key vector. The presence of 'N' on the left and bottom signifies that this is a square matrix, where 'N' can be any dimension, illustrated here by N=4.

**Key Insights:**
The main takeaway from this image is the concept of parallel computation for attention mechanisms. It visually confirms that all `q_i · k_j` comparisons, which are fundamental to calculating attention scores, can be computed in a single matrix operation rather than through N*N individual scalar dot products. This parallelization, represented by the `N x N` matrix, is critical for computational efficiency, especially when dealing with large sequence lengths. The matrix structure explicitly shows the full set of interactions between queries and keys, leading to the overall attention score matrix, which is then typically scaled and passed through a softmax function to obtain attention weights.

**Document Context:**
This image is presented in Section 9.3, titled 'Parallelizing computation using a single matrix X', and is explicitly referred to as 'Figure 9.8 The N x N QK| matrix showing how it computes all q_i · k_j comparisons in a single matrix multiple.' It serves as a crucial visual aid to explain the concept of parallelizing the computation of dot product attention scores. By demonstrating how all `q_i · k_j` comparisons are organized into a single matrix, it directly supports the document's narrative on efficient computation in transformer-like architectures, where such parallelization is key to performance.

**Summary:**
The image displays a 4x4 matrix, explicitly labeled with 'N' on its left side, indicating the number of rows, and 'N' below it, indicating the number of columns. Each cell within this matrix contains a dot product expression, systematically representing the comparison between a query vector 'q' and a key vector 'k'. The matrix is structured as follows:
- The first row contains: q1·k1, q1·k2, q1·k3, q1·k4
- The second row contains: q2·k1, q2·k2, q2·k3, q2·k4
- The third row contains: q3·k1, q3·k2, q3·k3, q3·k4
- The fourth row contains: q4·k1, q4·k2, q4·k3, q4·k4
This arrangement visually illustrates how all possible pairwise dot products between four query vectors (q1 to q4) and four key vectors (k1 to k4) are computed. The matrix structure itself demonstrates the parallelization concept, where an entire set of comparisons can be computed simultaneously, which is a fundamental aspect of efficient attention mechanisms in machine learning models.](images/9d7261e2d3495d7b59896273348ed57539b36808721bf716352a07f884fb3959.jpg)
Figure 9.8 The $N \times N$ QK| matrix showing how it computes all $q _ { i } \cdot k _ { j }$ comparisons in a single matrix multiple.

Once we have this QK| matrix, we can very efficiently scale these scores, take the softmax, and then multiply the result by $\pmb { v }$ resulting in a matrix of shape $N \times d$ : a vector embedding representation for each token in the input. We’ve reduced the entire self-attention step for an entire sequence of $N$ tokens for one head to the

following computation:

$$
\begin{array} { l } { \mathbf { h e a d } \ = \ \mathrm { s o f t m a x } \left( \mathrm { m a s k } \left( \frac { \mathbf { Q } \mathsf { K } ^ { \intercal } } { \sqrt { d _ { k } } } \right) \right) \mathbf { v } } \\ { \mathbf { A } \ = \ \mathsf { h e a d } \mathsf { W } ^ { 0 } } \end{array}
$$

Masking out the future You may have noticed that we introduced a mask function in Eq. 9.33 above. This is because the self-attention computation as we’ve described it has a problem: the calculation of QK  results in a score for each query value to every key value, including those that follow the query. This is inappropriate in the setting of language modeling: guessing the next word is pretty simple if you already know it! To fix this, the elements in the upper-triangular portion of the matrix are set to $- \infty$ , which the softmax will turn to zero, thus eliminating any knowledge of words that follow in the sequence. This is done in practice by adding a mask matrix $M$ in which $M _ { i j } = - \infty \forall j > i$ (i.e. for the upper-triangular portion) and $M _ { i j } = 0$ otherwise. Fig. 9.9 shows the resulting masked QK| matrix. (we’ll see in Chapter 11 how to make use of words in the future for tasks that need it).

![## Image Analysis: fda35fb1891cd6f5705a84ff1034c0e2eb128f8d5053ebb2ef4d351830f004e6.jpg

**Conceptual Understanding:**
The image conceptually represents a masked similarity matrix, specifically a QKᵀ (Query-Key Transpose) matrix, used in attention mechanisms within neural networks. Its main purpose is to illustrate how an attention mechanism can be configured to prevent a query at a given position from attending to subsequent (future) positions in a sequence. This is achieved by setting the corresponding similarity scores (dot products) in the upper-triangle portion of the matrix to negative infinity (`−∞`), effectively 'zeroing out' these connections after a softmax function is applied. The key idea communicated is the concept of 'causal masking' or 'look-ahead masking', which is fundamental for maintaining the temporal order in models processing sequential data, like Transformers in natural language processing.

**Content Interpretation:**
The image shows a 4x4 matrix, which is a visual representation of a QKᵀ matrix (Query-Key Transpose) in an attention mechanism. The values `q_i • k_j` represent the dot product similarities between the i-th query vector and the j-th key vector. The highlighted blue cells indicate computed dot product values, while the white cells with '-∞' signify masked or disallowed connections. Specifically, the upper triangle of the matrix (above the main diagonal) contains '-∞' values, indicating that queries are only allowed to attend to current or previous keys. For instance, `q1` only attends to `k1`, `q2` attends to `k1` and `k2`, and so on, up to `q4` which attends to `k1, k2, k3, k4`.

The significance of setting values to `-∞` is that when a softmax function is applied to the rows of this matrix (to normalize the attention weights), these `-∞` values will become zero. This effectively prevents the model from attending to future information in sequential data, a common practice in self-attention mechanisms for tasks like language modeling or time-series prediction where future context should not be available at current timesteps. The matrix dimensions are labeled 'N' on both axes, indicating it's an N x N matrix, further reinforcing its role in self-attention where the number of queries and keys are typically the same.

**Key Insights:**
The main takeaway from this image is the visual demonstration of a masked QKᵀ matrix, a core component in attention mechanisms, particularly in causal or masked self-attention. The explicit setting of `−∞` in the upper triangle of the matrix signifies the masking of future information. This process is crucial for tasks where predicting the next element in a sequence must only rely on past and current elements, not future ones. The image clearly shows that for any query `q_i`, the attention score is only calculated for keys `k_j` where `j ≤ i`. This allows for parallel computation of the attention scores for all `q_i • k_j` values while still enforcing the sequential constraint, as the masked values will become zero after the softmax operation, effectively ignoring future tokens. The 'N' labels confirm that it's a square matrix of size N x N, which is typical for self-attention where queries and keys often originate from the same sequence of length N.

**Document Context:**
This image directly illustrates the concept described in the document's section '9.3 Parallelizing computation using a single matrix X' and the subsequent text 'Figure 9.9 The N x N QKᵀ matrix showing the q_i ⋅ k_j values, with the upper-triangle portion of the comparisons matrix zeroed out (set to −∞, which the softmax will turn to zero)'. It visually explains how an N x N QKᵀ matrix is constructed and, crucially, how temporal masking is applied by setting the upper-triangle values to negative infinity. This masking is essential for ensuring that a query at a given position in a sequence only considers information from prior positions and its own position, preventing information leakage from future elements during parallel computation. The visual representation of the masked matrix is critical for understanding the mechanics of a masked self-attention layer and how it enables parallel processing while maintaining sequential causal relationships.

**Summary:**
The image displays a 4x4 matrix, representing a component in a computation, likely an attention mechanism, with dimensions labeled 'N' along both its height and width. The matrix illustrates the dot product values between query (q) and key (k) vectors, specifically q_i 
h
k_j, for i from 1 to 4 and j from 1 to 4, in a parallelized computation context. The key feature is the masking applied, where values in the upper-triangle portion of the matrix are set to negative infinity (-∞), effectively zeroing them out after a softmax operation. This ensures that a given query only attends to previous or current keys, preventing future information leakage.

From top to bottom, row by row, and then left to right, the text within the matrix cells is:

Row 1:
Cell (1,1): "q1•k1"
Cell (1,2): "−∞"
Cell (1,3): "−∞"
Cell (1,4): "−∞"

Row 2:
Cell (2,1): "q2•k1"
Cell (2,2): "q2•k2"
Cell (2,3): "−∞"
Cell (2,4): "−∞"

Row 3:
Cell (3,1): "q3•k1"
Cell (3,2): "q3•k2"
Cell (3,3): "q3•k3"
Cell (3,4): "−∞"

Row 4:
Cell (4,1): "q4•k1"
Cell (4,2): "q4•k2"
Cell (4,3): "q4•k3"
Cell (4,4): "q4•k4"

External labels:
Left side of the matrix: "N"
Bottom side of the matrix: "N"](images/fda35fb1891cd6f5705a84ff1034c0e2eb128f8d5053ebb2ef4d351830f004e6.jpg)
Figure 9.9 The $N \times N$ QK| matrix showing the $q _ { i } \cdot k _ { j }$ values, with the upper-triangle portion of the comparisons matrix zeroed out (set to $- \infty$ , which the softmax will turn to zero).

Fig. 9.10 shows a schematic of all the computations for a single attention head parallelized in matrix form.

Fig. 9.8 and Fig. 9.9 also make it clear that attention is quadratic in the length of the input, since at each layer we need to compute dot products between each pair of tokens in the input. This makes it expensive to compute attention over very long documents (like entire novels). Nonetheless modern large language models manage to use quite long contexts of thousands or tens of thousands of tokens.

Parallelizing multi-head attention In multi-head attention, as with self-attention, the input and output have the model dimension $d$ , the key and query embeddings have dimensionality $d _ { k }$ , and the value embeddings are of dimensionality $d _ { \nu }$ (again, in the original transformer paper $d _ { k } = d _ { \nu } = 6 4$ , $A = 8$ , and $d = 5 1 2$ ). Thus for each head $i$ , we have weight layers $\boldsymbol { \mathsf { W } } ^ { \mathsf { Q } } { } _ { i }$ of shape $[ d \times d _ { k } ]$ , $\boldsymbol { \mathsf { W } } ^ { \mathsf { K } } { } _ { i }$ of shape $[ d \times d _ { k } ]$ , and $\boldsymbol { \mathsf { W } } _ { i } ^ { \mathsf { v } }$ of shape $[ d \times d _ { \nu } ]$ , and these get multiplied by the inputs packed into $\pmb { \times }$ to produce $\mathbf { Q }$ of shape $[ N \times d _ { k } ]$ , $\boldsymbol { \mathsf { K } }$ of shape $[ N \times d _ { k } ]$ , and $\pmb { v }$ of shape $[ N \times d _ { \nu } ]$ . The output of each of the $A$ heads is of shape $N \times d _ { \nu }$ , and so the output of the multi-head layer with $A$ heads consists of $A$ matrices of shape $N \times d _ { \nu }$ . To make use of these matrices in further processing, they are concatenated to produce a single output with dimensionality $N \times h d _ { \nu }$ . Finally, we use a final linear projection $\boldsymbol { \mathsf { W } } ^ { 0 }$ of shape $[ A d _ { \nu } \times d ]$ , that reshape it to the original output dimension for each token. Multiplying the concatenated $N \times h d _ { \nu }$ matrix output by $\boldsymbol { \mathsf { W } } ^ { 0 }$ of shape $[ A d _ { \nu } \times d ]$ yields the self-attention output $\pmb { \mathsf { A } }$ of shape $[ N \times d ]$ .

![## Image Analysis: 99e8dc73eb6cdeb2109f6f0d270d811aa0216017f1004d7e978bb4c5fdaa45f6.jpg

**Conceptual Understanding:**
This image conceptually represents the core mathematical operations of a single attention head in a transformer network, specifically highlighting how computations can be parallelized. Its main purpose is to illustrate the sequential steps of transforming input data (X) into Query (Q), Key (K), and Value (V) representations, computing attention scores, applying a mask, and finally generating the weighted sum of values as the attention output (A). The key idea communicated is the matrix-based, parallelizable computation of self-attention.

**Content Interpretation:**
The image depicts the mathematical operations involved in computing self-attention within a transformer model, specifically for a single attention head, emphasizing parallel computation. It shows the transformation of input tokens into Query, Key, and Value representations, followed by the calculation of attention scores and their application to value vectors to produce the attention output. The masking step is crucial for preventing future information from influencing current predictions, common in sequential tasks.

**Key Insights:**
The main takeaways from this image are: 1. Input tokens (matrix X) are linearly transformed into Query (Q), Key (K), and Value (V) matrices using distinct weight matrices (WQ, WK, WV). This is evident from 'X x WQ = Q', 'X x WK = K', and 'X x WV = V'. 2. Attention scores are computed by a matrix multiplication of Q and the transpose of K (QKᵀ), which results in a square matrix representing the dot product similarities between all query-key pairs. This is shown by 'Q x Kᵀ = QKᵀ' with elements 'qᵢ⋅kⱼ'. 3. A masking mechanism can be applied to QKᵀ, setting certain future interactions to negative infinity ('-∞'), which is crucial for preventing information leakage in tasks like language modeling. This is illustrated by 'QKᵀ masked' where upper triangular elements are '-∞'. 4. The final attention output (A) is obtained by multiplying the (potentially masked and softmaxed, though not explicitly shown for softmax) attention scores with the Value (V) matrix, effectively weighting the value vectors. This is represented by 'QKᵀ masked x V = A'. The image visually demonstrates the parallel nature of these matrix operations, which is fundamental to the efficiency of transformer architectures.

**Document Context:**
This image directly supports section 9.3, 'Parallelizing computation using a single matrix X,' by visually explaining the matrix operations involved in attention computation in parallel. It serves as a detailed schematic for understanding how the Q, K, and V matrices are derived from an input X, and how these are then used to calculate the attention output A, reinforcing the concept of parallel processing within the attention mechanism. The text after the image, which describes it as 'Figure 9.10 Schematic of the attention computation for a single attention head in parallel,' further solidifies its role as a key explanatory diagram.

**Summary:**
The image illustrates the parallel computation of the attention mechanism for a single attention head. It begins by showing how the input matrix X is transformed into Query (Q), Key (K), and Value (V) matrices. Subsequently, it details the calculation of the attention scores by multiplying Q and the transpose of K, followed by a masking step, and finally, the weighted sum of the value vectors to produce the final attention output matrix A. The process is broken down into two main rows: the first for generating Q, K, and V, and the second for computing and applying attention weights.](images/99e8dc73eb6cdeb2109f6f0d270d811aa0216017f1004d7e978bb4c5fdaa45f6.jpg)
Figure 9.10 Schematic of the attention computation for a single attention head in parallel. The first row shows the computation of the Q, K, and $\pmb { v }$ matrices. The second row shows the computation of $\mathsf { Q } \mathsf { K } ^ { \mathsf { T } }$ , the masking (the softmax computation and the normalizing by dimensionality are not shown) and then the weighted sum of the value vectors to get the final attention vectors.

$$
\begin{array} { c } { { \bf Q ^ { i } = X W ^ { 0 i } \bf ; ~ { \bf { K } } ^ { i } ~ = ~ X W ^ { \bf K i } ; ~ { \bf { V } } ^ { i } = X W ^ { \bf V i } \quad \mathrm { ~ } } } \\ { { \bf h e a d } _ { i } = \mathrm { { S e l f A t t e n t i o n } } ( { \bf Q } ^ { i } , { \bf K } ^ { i } , { \bf V } ^ { i } ) ~ = ~ \mathrm { s o f t m a x } \left( { \bf \frac { Q ^ { i } { \bf K } ^ { i } \mathrm { T } } { \sqrt { d _ { k } } } } \right) { \bf V } ^ { i } \qquad \mathrm { ~ ( ~ } }  \\ { { \bf M u l t i H e a d A t t e n t i o n } ( { \bf X } ) ~ = ~ ( { \bf h e a d } _ { 1 } \oplus { \bf h e a d } _ { 2 } . . . \oplus { \bf h e a d } _ { A } ) { \bf W } ^ { 0 } \mathrm { ~ } } \end{array}
$$

Putting it all together with the parallel input matrix $\pmb { \times }$ The function computed in parallel by an entire layer of $N$ transformer block over the entire $N$ input tokens can be expressed as:

$$
\begin{array} { r l } & { \textbf { 0 } = \textbf { \textsf { X } } + \mathbf { M u l t i H e a d A t t e n t i o n } ( \mathrm { L a y e r N o r m } ( \pmb { \times } ) ) } \\ & { \mathbf { H } \ = \ \mathbf { 0 } + \mathbf { F F N } ( \mathrm { L a y e r N o r m } ( \mathbf { 0 } ) ) } \end{array}
$$

Note that in Eq. 9.37 we are using $\pmb { \times }$ to mean the input to the layer, wherever it comes from. For the first layer, as we will see in the next section, that input is the initital word $^ +$ positional embedding vectors that we have been describing by $\pmb { \times }$ . But for subsequent layers $k$ , the input is the output from the previous layer $\mathsf { H } ^ { k - 1 }$ . We can also break down the computation performed in a transformer layer, showing one equation for each component computation. We’ll use $\boldsymbol { \mathsf { T } }$ (of shape $[ N \times d ] )$ to stand for transformer and superscripts to demarcate each computation inside the block, and again use $\pmb { \times }$ to mean the input to the block from the previous layer or the initial

embedding:

$$
{ \begin{array} { r l } & { { \boldsymbol { \mathsf { T } } } ^ { 1 } \ = \ { \mathrm { L a y e r N o r m } } ( { \boldsymbol { \mathsf { X } } } ) } \\ & { { \boldsymbol { \mathsf { T } } } ^ { 2 } \ = \ { \mathrm { M u l t i H e a d A t t e n t i o n } } ( { \boldsymbol { \mathsf { T } } } ^ { 1 } ) } \\ & { { \boldsymbol { \mathsf { T } } } ^ { 3 } \ = \ { \boldsymbol { \mathsf { T } } } ^ { 2 } + { \boldsymbol { \mathsf { X } } } } \\ & { { \boldsymbol { \mathsf { T } } } ^ { 4 } \ = \ { \mathrm { L a y e r N o r m } } ( { \boldsymbol { \mathsf { T } } } ^ { 3 } ) } \\ & { { \boldsymbol { \mathsf { T } } } ^ { 5 } \ = \ { \mathrm { F F N } } ( { \boldsymbol { \mathsf { T } } } ^ { 4 } ) } \\ & { { \boldsymbol { \mathsf { H } } } \ = \ { \boldsymbol { \mathsf { T } } } ^ { 5 } + { \boldsymbol { \mathsf { T } } } ^ { 3 } } \end{array} }
$$

Here when we use a notation like $\mathrm { F F N } ( \boldsymbol { \mathsf { T } } ^ { 3 } )$ we mean that the same FFN is applied in parallel to each of the $N$ embedding vectors in the window. Similarly, each of the $N$ tokens is normed in parallel in the LayerNorm. Crucially, the input and output dimensions of transformer blocks are matched so they can be stacked. Since each token $x _ { i }$ at the input to the block is represented by an embedding of dimensionality $[ 1 \times d ]$ , that means the input $\pmb { \times }$ and output $\mathsf { H }$ are both of shape $[ N \times d ]$ .

# 9.4 The input: embeddings for token and position

# embedding

Let’s talk about where the input $\pmb { \times }$ comes from. Given a sequence of $N$ tokens $N$ is the context length in tokens), the matrix $\pmb { \times }$ of shape $[ N \times d ]$ has an embedding for each word in the context. The transformer does this by separately computing two embeddings: an input token embedding, and an input positional embedding.

A token embedding, introduced in Chapter 7 and Chapter 8, is a vector of dimension $d$ that will be our initial representation for the input token. (As we pass vectors up through the transformer layers in the residual stream, this embedding representation will change and grow, incorporating context and playing a different role depending on the kind of language model we are building.) The set of initial embeddings are stored in the embedding matrix E, which has a row for each of the $| V |$ tokens in the vocabulary. Thus each word is a row vector of $d$ dimensions, and E has shape $[ | V | \times d ]$ .

Given an input token string like Thanks for all the we first convert the tokens into vocabulary indices (these were created when we first tokenized the input using BPE or SentencePiece). So the representation of thanks for all the might be $\boldsymbol { \mathsf { w } } =$ [5, 4000, 10532, 2224]. Next we use indexing to select the corresponding rows from E, (row 5, row 4000, row 10532, row 2224).

one-hot vector

Another way to think about selecting token embeddings from the embedding matrix is to represent tokens as one-hot vectors of shape $[ 1 \times | V | ]$ , i.e., with one dimension for each word in the vocabulary. Recall that in a one-hot vector all the elements are 0 except one, the element whose dimension is the word’s index in the vocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary, $x _ { 5 } = 1$ , and $x _ { i } = 0 ~ \forall i \neq 5$ , as shown here:

$$
\begin{array} { r } { \begin{array} { c c c c c c c } { [ \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 1 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \ldots } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } & { \boldsymbol { \mathbb { 0 } } } \end{array} ] } \\ { \begin{array} { r } { 1 } & { 2 } & { 3 } & { 4 } & { 5 } & { 6 } & { 7 } & { \ldots } & { \ldots } & { | \boldsymbol { \mathbb { V } } | } \end{array} } \end{array}
$$

Multiplying by a one-hot vector that has only one non-zero element $x _ { i } = 1$ simply selects out the relevant row vector for word $i$ , resulting in the embedding for word $i$ as depicted in Fig. 9.11.

![## Image Analysis: 545f2b9b66d9e282bc8a3e80dec25bc32698f233e9ba9b247c7d9f9c94656891.jpg

**Conceptual Understanding:**
The image conceptually represents the mechanism for obtaining a word's vector representation (embedding) from a predefined embedding matrix. Its main purpose is to visually explain how a one-hot encoding for a word, when multiplied by an embedding matrix, yields the specific embedding vector for that word. This illustrates the 'selection' process of an embedding. The key ideas communicated are matrix multiplication for embedding lookup, the role of one-hot vectors in indexing, and the structure of an embedding matrix (rows are word embeddings, columns are embedding dimensions).

**Content Interpretation:**
The image illustrates the mathematical operation to retrieve a specific word embedding vector from an embedding matrix. It shows the process of using a one-hot vector, where only the element corresponding to the word's index is '1' and all others are '0', to 'select' the corresponding row from a larger embedding matrix. The highlighted row in matrix E and the resulting vector clearly show that the 5th row of E is extracted, which, based on the context, represents the embedding for word V5. This demonstrates a fundamental operation in natural language processing for obtaining word representations.

**Key Insights:**
The main takeaway is that multiplying a one-hot vector by an embedding matrix effectively acts as a selector for a specific row in that matrix. The position of the '1' in the one-hot vector directly corresponds to the index of the row (word embedding) that is extracted from the embedding matrix. This operation is a common and efficient way to retrieve word embeddings in computational models. The dimensions '1', '5', '|V|', and 'd' provide crucial information about the sizes of the vectors and matrices involved, indicating that '|V|' is the vocabulary size and 'd' is the embedding dimension. The specific '1' at the 5th position in the one-hot vector and the highlighted 5th row in matrix E, leading to the d-dimensional output vector, clearly demonstrate this selection mechanism.

**Document Context:**
This image directly supports the document's section on 'embedding' by visually explaining how an embedding vector for a specific word (V5) is selected. The accompanying text, 'Figure 9.11 Selecting the embedding vector for word V5 by multiplying the embedding matrix E with a one-hot vector with a 1 in index 5,' explicitly states the process depicted. The image serves as a clear, concise visual aid to understand the mathematical mechanism behind retrieving word embeddings, which is a core concept in various NLP models.

**Summary:**
The image illustrates the process of selecting an embedding vector for a specific word, V5, from an embedding matrix E using a one-hot encoding vector. It depicts a matrix multiplication operation. On the far left, there is a one-hot vector, horizontally oriented. This vector has a height of '1' and a length denoted by '|V|', which represents the size of the vocabulary. The vector's content is '0 0 0 0 1 0 0 ... 0 0 0 0', with the numeral '1' specifically located at the 5th position, as indicated by the '5' label above that position. The vector is followed by a multiplication symbol '×'. To the right of the multiplication symbol is the embedding matrix, labeled 'E', vertically oriented. This matrix has a height denoted by '|V|' and a width denoted by 'd', where 'd' is the dimension of the embedding vectors. The 5th row of this matrix is highlighted with a light green fill, and a '5' label is present to the left of this row. Following the embedding matrix is an equals sign '='. Finally, on the far right, the result of the multiplication is shown as a horizontal vector. This resulting vector has a height of '1' and a length of 'd', and its content is entirely filled with a light green color, indicating that it represents the selected embedding vector. The overall visual demonstrates how multiplying a one-hot vector (with a '1' at index 5) by the embedding matrix E effectively extracts the 5th row of E, which corresponds to the embedding for the word V5.](images/545f2b9b66d9e282bc8a3e80dec25bc32698f233e9ba9b247c7d9f9c94656891.jpg)
Figure 9.11 Selecting the embedding vector for word $V _ { 5 }$ by multiplying the embedding matrix E with a one-hot vector with a 1 in index 5.

We can extend this idea to represent the entire token sequence as a matrix of onehot vectors, one for each of the $N$ positions in the transformer’s context window, as shown in Fig. 9.12.

![## Image Analysis: 6efff5d371d9a1bdab0e51a19e50a6e528048fff23304f980bf20408a4adfcb3.jpg

**Conceptual Understanding:**
This image conceptually represents the fundamental operation of obtaining embeddings for a sequence of tokens. The main purpose is to illustrate how an input sequence, initially represented by one-hot encoded vectors, interacts with a pre-defined embedding matrix 'E' via matrix multiplication to produce a sequence of dense vector embeddings. It communicates the idea that this matrix multiplication effectively 'looks up' and 'selects' the specific embedding vectors for each token in the input sequence from the larger embedding matrix.

**Content Interpretation:**
The image demonstrates the process of obtaining a sequence of token embeddings through matrix multiplication. It shows how a sparse, one-hot encoded representation of an input sequence is transformed into a dense, lower-dimensional embedding matrix. The key elements are: the one-hot matrix representing input tokens, the embedding matrix 'E' containing pre-trained embeddings, and the resulting matrix which is the sequence of embeddings for the input tokens. The operation signifies a lookup mechanism, where the one-hot encoding facilitates the selection of specific rows (embeddings) from the larger embedding matrix 'E'.

**Key Insights:**
1. One-hot encoding serves as a sparse representation for discrete tokens, with each row representing a unique token from a vocabulary. 2. Matrix multiplication with a one-hot matrix acts as an efficient row selection mechanism from the second matrix. 3. The embedding matrix 'E' stores dense, lower-dimensional vector representations (embeddings) for each token in the vocabulary. 4. The overall process transforms a sequence of N tokens (represented by one-hot vectors) into a sequence of their corresponding d-dimensional dense embeddings, suitable for further neural network processing. 5. The dimensions N, |V|, and d represent the number of tokens in the sequence, the vocabulary size, and the embedding dimension, respectively, illustrating the transformation from a large sparse space to a smaller dense space.

**Document Context:**
This image is highly relevant to the 'embedding' section of the document, as indicated by the surrounding text and section title. It visually explains the fundamental mechanism described in 'Figure 9.12 Selecting the embedding matrix for the input sequence of token ids W by multiplying a one-hot matrix corresponding to W by the embedding matrix E.' This visual representation clarifies how token IDs (implicitly represented by the one-hot matrix) are converted into their corresponding dense embeddings, which is a core concept in natural language processing and deep learning for text data. It shows the underlying linear algebra operation that transforms symbolic tokens into numerical vectors suitable for neural network processing.

**Summary:**
This image illustrates the process of selecting an embedding matrix for an input sequence of token IDs by performing matrix multiplication. The process begins with a one-hot encoded matrix representing the input sequence. Each row of this matrix corresponds to a token, and it is a one-hot vector with a '1' at the index corresponding to the token's ID in a vocabulary of size |V|, and '0's elsewhere. This input matrix has dimensions N (number of tokens in the sequence) by |V| (vocabulary size). This one-hot matrix is then multiplied by an embedding matrix 'E'. The embedding matrix 'E' has dimensions |V| (vocabulary size) by d (embedding dimension), where each row contains the dense vector embedding for a specific token ID. The result of this matrix multiplication is an output matrix with dimensions N (number of tokens) by d (embedding dimension). This resulting matrix represents the sequence of d-dimensional embeddings for the N input tokens. Essentially, the one-hot matrix acts as a selector, picking the appropriate embedding vector from 'E' for each token in the input sequence, thereby converting sparse one-hot representations into dense, lower-dimensional embeddings.](images/6efff5d371d9a1bdab0e51a19e50a6e528048fff23304f980bf20408a4adfcb3.jpg)
Figure 9.12 Selecting the embedding matrix for the input sequence of token ids $W$ by multiplying a one-hot matrix corresponding to W by the embedding matrix E.

These token embeddings are not position-dependent. To represent the position of each token in the sequence, we combine these token embeddings with positional embeddings specific to each position in an input sequence.

Where do we get these positional embeddings? The simplest method, called absolute position, is to start with randomly initialized embeddings corresponding to each possible input position up to some maximum length. For example, just as we have an embedding for the word fish, we’ll have an embedding for the position 3. As with word embeddings, these positional embeddings are learned along with other parameters during training. We can store them in a matrix ${ \cal E } _ { \mathrm { p o s } }$ of shape $[ N \times d ]$ .

To produce an input embedding that captures positional information, we just add the word embedding for each input to its corresponding positional embedding. The individual token and position embeddings are both of size $[ 1 \times d ]$ , so their sum is also $[ 1 \times d ]$ , This new embedding serves as the input for further processing. Fig. 9.13 shows the idea.

![## Image Analysis: abe5cbd4f50a1c2b1a7515d9167da48d7019e76d067cfcb9243149d9072c89c4.jpg

**Conceptual Understanding:**
This image conceptually represents the process of creating enriched input embeddings for a neural network, specifically a Transformer model, by combining semantic word embeddings with positional embeddings. The main purpose is to illustrate how sequential order information, which is not inherently captured by many word embedding techniques, can be explicitly encoded and integrated into the input representations, allowing the model to understand the relative or absolute position of words in a sequence. It communicates the key idea that for models like Transformers, a simple additive combination of word and position embeddings forms a comprehensive 'composite embedding' for each token, maintaining the original dimensionality while adding crucial sequential context.

**Content Interpretation:**
The image depicts the process of generating composite embeddings for a sequence of words, 'Janet will back the bill', which are then input into a 'Transformer Block'.

1.  **Input Words:** The process starts with individual words: 'Janet', 'will', 'back', 'the', 'bill'.
2.  **Word Embeddings:** Each word is converted into a 'Word Embedding' (represented by the vertical pink capsules with the words inside).
3.  **Position Embeddings:** Simultaneously, for each word, an 'absolute position embedding' is generated, corresponding to its sequential order in the sentence (represented by the pink capsules with numbers '1', '2', '3', '4', '5').
4.  **Addition Operation:** The 'Word Embedding' and 'Position Embedding' for each word are combined through an addition operation, indicated by the green diamond shape containing a '+' symbol. This operation creates a new embedding that incorporates both semantic and positional information.
5.  **Composite Embeddings (X):** The result of this addition is labeled 'X = Composite Embeddings (word + position)'. These are depicted as stacked, multi-colored circles (red, brown, light brown), visually suggesting a richer, combined representation.
6.  **Input to Transformer Block:** Each 'Composite Embedding' (X) is then fed as a distinct input into a 'Transformer Block', which is the higher-level processing unit for the sequence.

The significance of this process is to provide the Transformer Block with not only the semantic meaning of each word but also its order in the sequence, which is vital for understanding context in language.

**Key Insights:**
1.  **Positional Encoding:** The image clearly illustrates a common method for positional encoding in NLP models, specifically by adding absolute position embeddings to word embeddings. This shows how sequential information is introduced into token representations.
2.  **Composite Embeddings:** It demonstrates the creation of 'Composite Embeddings' by combining semantic (word) and structural (position) information, which results in a richer input representation for subsequent model layers.
3.  **Input to Transformer Block:** The diagram highlights that these 'Composite Embeddings' are the direct input to a 'Transformer Block', underscoring the importance of these combined embeddings for the effective functioning of Transformer-based architectures.
4.  **Element-wise Addition:** The use of the '+' symbol in the green diamonds explicitly shows that the combination method is element-wise addition, a simple yet effective technique for fusing different types of embedding information.

Textual evidence for these insights includes the labels 'Word Embeddings', 'Position Embeddings', the '+' symbol in the diamond, 'X = Composite Embeddings (word + position)', and 'Transformer Block'.

**Document Context:**
This image directly supports the document's section on 'embedding' by visually demonstrating 'A simple way to model position: add an embedding of the absolute position to the token embedding to produce a new embedding of the same dimensionality.' (as stated in the text after the image). It provides a concrete, step-by-step illustration of how positional information is incorporated into word representations, which is a fundamental concept for understanding how Transformer models process sequences. This visual explanation enhances the comprehension of how models like Transformers overcome their lack of inherent sequential understanding by explicitly encoding position.

**Summary:**
This image illustrates a simple method for modeling position in natural language processing, specifically how word embeddings and position embeddings are combined before being fed into a Transformer Block. For each word in the input sequence, its individual word embedding is generated, and a corresponding absolute position embedding is also generated. These two embeddings (word and position) are then added together element-wise using an addition operation, producing a 'Composite Embedding'. This composite embedding, which now encodes both the semantic meaning of the word and its sequential position, is then passed as input to a Transformer Block. This process is repeated for every word in the input sentence 'Janet will back the bill', ensuring that positional information is integrated into the token representations, which is crucial for models like Transformers that process input sequences in parallel rather than sequentially.](images/abe5cbd4f50a1c2b1a7515d9167da48d7019e76d067cfcb9243149d9072c89c4.jpg)
Figure 9.13 A simple way to model position: add an embedding of the absolute position to the token embedding to produce a new embedding of the same dimensionality.

The final representation of the input, the matrix $\pmb { \times }$ , is an $[ N \times d ]$ matrix in which each row $i$ is the representation of the ith token in the input, computed by adding $\mathsf { \Pi } \mathsf { E } [ i d ( i ) ]$ —the embedding of the id of the token that occurred at position $i - ,$ , to $\mathsf { P } [ i ]$ the positional embedding of position i.

A potential problem with the simple position embedding approach is that there will be plenty of training examples for the initial positions in our inputs and correspondingly fewer at the outer length limits. These latter embeddings may be poorly trained and may not generalize well during testing. An alternative is to choose a static function that maps integer inputs to real-valued vectors in a way that better handle sequences of arbitrary length. A combination of sine and cosine functions with differing frequencies was used in the original transformer work. Sinusoidal position embeddings may also help in capturing the inherent relationships among the positions, like the fact that position 4 in an input is more closely related to position 5 than it is to position 17.

A more complex style of positional embedding methods extend this idea of capturing relationships even further to directly represent relative position instead of absolute position, often implemented in the attention mechanism at each layer rather than being added once at the initial input.

# 9.5 The Language Modeling Head

The last component of the transformer we must introduce is the language modeling head. Here we are using the word head to mean the additional neural circuitry we add on top of the basic transformer architecture when we apply pretrained transformer models to various tasks. The language modeling head is the circuitry we need to do language modeling.

Recall that language models, from the simple n-gram models of Chapter 3 through the feedforward and RNN language models of Chapter 7 and Chapter 8, are word predictors. Given a context of words, they assign a probability to each possible next word. For example, if the preceding context is “Thanks for all the” and we want to know how likely the next word is “fish” we would compute:

$$
P ( \mathit { f i s h } | T h a n k s f o r a l l t h e )
$$

Language models give us the ability to assign such a conditional probability to every possible next word, giving us a distribution over the entire vocabulary. The n-gram language models of Chapter 3 compute the probability of a word given counts of its occurrence with the $n - 1$ prior words. The context is thus of size $n - 1$ . For transformer language models, the context is the size of the transformer’s context window, which can be quite large, like 32K tokens for large models (and much larger contexts of millions of words are possible with special long-context architectures).

The job of the language modeling head is to take the output of the final transformer layer from the last token $N$ and use it to predict the upcoming word at position $N + 1$ . Fig. 9.14 shows how to accomplish this task, taking the output of the last token at the last layer (the $d$ -dimensional output embedding of shape $[ 1 \times d ] ,$ and producing a probability distribution over words (from which we will choose one to generate).

The first module in Fig. 9.14 is a linear layer, whose job is to project from the output $h _ { N } ^ { L }$ , which represents the output token embedding at position $N$ from the final

![## Image Analysis: 3547badba438f783f5633b135e28a9ea97933ce175c10302aa864e6facc478a7.jpg

**Conceptual Understanding:**
This image conceptually represents the 'Language Model Head,' which is the output layer of a transformer-based language model. Its main purpose is to transform the final hidden state (embedding) of the last token from the transformer's output into a probability distribution over the entire vocabulary. This distribution indicates the likelihood of each possible word in the vocabulary being the next word in a sequence. The key idea communicated is the process of generating word predictions from a deep learning model's internal representation.

**Content Interpretation:**
The image details the structure and function of the Language Model Head within a transformer architecture. It shows how the final hidden state (embedding) from the last transformer layer for a given token is processed to predict the next word. The main processes shown are: 1. **Embedding Generation by Transformer Block:** The `Layer L Transformer Block` produces context-rich embeddings for input tokens. 2. **Unembedding:** The `Unembedding layer` transforms the high-dimensional token embedding (`h^(L)_N`) into raw scores (logits) for each word in the vocabulary (`u1...u|V|`). This layer performs a linear transformation, conceptually reversing an embedding process, as indicated by `U = E^T`. 3. **Softmax Normalization:** The `Softmax over vocabulary V` layer normalizes these logits into a probability distribution (`y1...y|V|`), ensuring that the sum of probabilities for all words in the vocabulary equals one. The visual icon of a histogram within the Softmax layer further reinforces its role in producing a distribution. The dimensions indicated (`1 x d`, `d x |V|`, `1 x |V|`) clarify the input and output sizes at each stage, crucial for understanding the mathematical operations involved.

**Key Insights:**
The main takeaway is that the Language Model Head serves as the final predictive component in a transformer model for language generation. Key insights include: 1. **Conversion of Embeddings to Logits:** The `Unembedding layer U = E^T` is central to converting a dense vector representation (`h^(L)_N`) into a vector of raw scores (`Logits 1 x |V|`) corresponding to each word in the vocabulary. This implies a linear mapping where the un-embedding matrix `U` is often the transpose of the initial word embedding matrix `E`, suggesting a parameter tying technique to reduce the number of parameters and potentially improve generalization. 2. **Probability Distribution Generation:** The `Softmax over vocabulary V` function is crucial for transforming these logits into a valid `Word probabilities 1 x |V|` distribution, which can then be used for sampling or selecting the most probable next word. 3. **Role of the Transformer Block:** The `Layer L Transformer Block` provides the contextualized embedding `h^(L)_N`, highlighting that the language model head operates on a rich, context-aware representation of the N-th token. 4. **Dimensionality:** The explicit dimension labels `1 x d`, `d x |V|`, and `1 x |V|` clearly show the flow of information and the size of vectors at each stage, which is fundamental for implementing and understanding these models.

**Document Context:**
This image directly supports Section 9.5, titled 'The Language Modeling Head,' by visually illustrating the concepts described in the accompanying text. The diagram provides a clear architectural view of how the output from the transformer's encoder-decoder stack (`h^(L)_N`) is processed to generate a predictive probability distribution over the vocabulary. It is critical for understanding the mechanism by which transformer models perform language modeling tasks, such as predicting the next word in a sequence. The detailed labels and the step-by-step flow clarify the transformation from a hidden state to a meaningful probability score for each potential next word.

**Summary:**
The image illustrates the architecture of a Language Model Head, which is a crucial component at the top of a transformer model. It takes the output embedding for a specific token from the last transformer layer and transforms it into a probability distribution over the entire vocabulary. The process begins with the `Layer L Transformer Block`, which takes input tokens `w1, w2, ..., wN` and outputs their respective embeddings `h^(L)_1, h^(L)_2, ..., h^(L)_N`. Specifically, the language model head uses the embedding `h^(L)_N`, which has a dimension of `1 x d`. This embedding first passes through an `Unembedding layer` where the transformation `U = E^T` is applied. This layer projects the `1 x d` embedding into `1 x |V|` logits, represented as `u1, u2, ..., u|V|`. The unembedding layer itself is characterized by dimensions `d x |V|`. Following this, these `Logits` are fed into a `Softmax over vocabulary V` layer. The Softmax function converts the logits into a probability distribution `y1, y2, ..., y|V|`, which sum to 1. Each `y` value represents the probability of a specific word from the vocabulary `V` being the next token. These final outputs are labeled as `Word probabilities` with a dimension of `1 x |V|`.](images/3547badba438f783f5633b135e28a9ea97933ce175c10302aa864e6facc478a7.jpg)
Figure 9.14 The language modeling head: the circuit at the top of a transformer that maps from the output embedding for token $N$ from the last transformer layer $( { \bf h } _ { N } ^ { L } )$ to a probability distribution over words in the vocabulary $V$ .

# logit

# weight tying

block $L$ , (hence of shape $[ 1 \times d ] )$ to the logit vector, or score vector, that will have a single score for each of the $| V |$ possible words in the vocabulary $V$ . The logit vector $\mathbf { u }$ is thus of dimensionality $1 \times | V |$ .

unembedding

This linear layer can be learned, but more commonly we tie this matrix to (the transpose of) the embedding matrix E. Recall that in weight tying, we use the same weights for two different matrices in the model. Thus at the input stage of the transformer the embedding matrix (of shape $[ | V | \times d ] )$ is used to map from a one-hot vector over the vocabulary (of shape $[ 1 \times | V | ] )$ to an embedding (of shape $[ 1 \times d ] ,$ ). And then in the language model head, $\bar { \mathsf { E } } ^ { \mathsf { T } }$ , the transpose of the embedding matrix (of shape $[ d \times | V | ] )$ is used to map back from an embedding (shape $[ 1 \times d ] )$ to a vector over the vocabulary (shape $[ 1 \times | V | ] )$ . In the learning process, $\mathsf { E }$ will be optimized to be good at doing both of these mappings. We therefore sometimes call the transpose $\mathsf { E } ^ { \mathsf { T } }$ the unembedding layer because it is performing this reverse mapping.

A softmax layer turns the logits $\mathbf { u }$ into the probabilities $\pmb { y }$ over the vocabulary.

$$
\begin{array} { l } { \mathbf { u } \ = \ \mathbf { h } _ { \mathsf { N } } ^ { \mathsf { L } } \ \mathsf { E } ^ { \mathsf { T } } } \\ { \mathsf { y } \ = \ \operatorname { s o f t m a x } ( \mathbf { u } ) } \end{array}
$$

We can use these probabilities to do things like help assign a probability to a given text. But the most important usage to generate text, which we do by sampling a word from these probabilities $y$ . We might sample the highest probability word (‘greedy’ decoding), or use another of the sampling methods we’ll introduce in Section 10.2. In either case, whatever entry $y _ { k }$ we choose from the probability vector $\pmb { \ y }$ , we generate the word that has that index $k$ .

Fig. 9.15 shows the total stacked architecture for one token i. Note that the input to each transformer layer $x _ { i } ^ { \ell }$ is the same as the output from the preceding layer $h _ { i } ^ { \ell - 1 }$

Now that we see all these transformer layers spread out on the page, we can point out another useful feature of the unembedding layer: as a tool for interpretability of the internals of the transformer that we call the logit lens (Nostalgebraist, 2020). We can take a vector from any layer of the transformer and, pretending that it is the prefinal embedding, simply multiply it by the unembedding layer to get logits, and compute a softmax to see the distribution over words that that vector might be representing. This can be a useful window into the internal representations of the model. Since the network wasn’t trained to make the internal representations function in this way, the logit lens doesn’t always work perfectly, but this can still be a useful trick.

![## Image Analysis: d929c29e6d8e8622d5adcdd18adb0e8e77a3eb71cef123c7cee826fcdd101866.jpg

**Conceptual Understanding:**
This image conceptually represents the forward pass of a decoder-only transformer language model. Its main purpose is to illustrate the architecture and the data flow involved in predicting the next token in a sequence given an input token. The key idea being communicated is how an input token `w_i` is transformed through a series of stacked computational layers and a final prediction head to produce a probability distribution for the subsequent token `w_{i+1}` across a vocabulary of size `|V|`.

**Content Interpretation:**
The image depicts the architectural components and data flow within a decoder-only transformer language model. It shows how an input token is processed through a series of stacked transformer blocks (layers) to ultimately predict the next token in a sequence. The core concepts illustrated are: input embedding, multi-layer processing with feedforward, layer normalization, and attention mechanisms within each layer, and a final language modeling head that transforms internal representations into a probability distribution over the vocabulary for next token prediction. The significance lies in showcasing the fundamental building blocks and the sequential, iterative nature of processing in such models.

**Key Insights:**
The main takeaways from this image are: 1. Decoder-only transformers process input tokens through a stack of identical layers. 2. Each layer comprises `feedforward`, `layer norm`, and `attention` sub-layers. 3. Intermediate outputs of each layer (`h_i^1`, `h_i^2`, ..., `h_i^L`) serve as inputs to the subsequent layer, with corresponding `x_i` notations indicating the input to each layer's block. 4. A `Language Modeling Head` is responsible for converting the final layer's output into a probability distribution over possible next tokens. 5. This head uses a matrix `U` to compute `logits`, which are then normalized by `softmax` to yield `Token probabilities`. 6. The ultimate goal is to `Sample token to generate at position i+1` based on these probabilities, denoted as `w_{i+1}`. These insights highlight the modular and sequential processing nature of such models.

**Document Context:**
This image directly supports the document's discussion on transformer language models, specifically in the 'weight tying' section. It provides a visual representation of the model's architecture, which is crucial for understanding how these models function at a detailed level. By illustrating the transformation of an input token to an output prediction through various layers and the language modeling head, it sets the stage for discussions on model efficiency, parameter sharing, and potentially 'weight tying' if the matrix 'U' is related to input embeddings (though not explicitly shown in this diagram).

**Summary:**
The image illustrates the architecture of a decoder-only transformer language model, detailing the process of transforming an input token into a predicted next token. The flow begins with an input token, which undergoes input encoding. This encoded representation then passes sequentially through multiple transformer layers (Layer 1, Layer 2, up to Layer L), each consisting of a feedforward network, layer normalization, and an attention mechanism. The output of the final layer is fed into a language modeling head. The language modeling head applies a linear transformation (U), calculates logits, and then uses a softmax function to generate token probabilities across the entire vocabulary. Finally, a token is sampled from these probabilities to generate the next token in the sequence. The diagram clearly shows the stacked nature of the transformer blocks and the step-by-step transformation of data through the model.](images/d929c29e6d8e8622d5adcdd18adb0e8e77a3eb71cef123c7cee826fcdd101866.jpg)
Figure 9.15 A transformer language model (decoder-only), stacking transformer blocks and mapping from an input token $w _ { i }$ to to a predicted next token $w _ { i + 1 }$ .

A terminological note before we conclude: You will sometimes see a transformer used for this kind of unidirectional causal language model called a decoderonly model. This is because this model constitutes roughly half of the encoderdecoder model for transformers that we’ll see how to apply to machine translation in Chapter 13. (Confusingly, the original introduction of the transformer had an encoder-decoder architecture, and it was only later that the standard paradigm for causal language model was defined by using only the decoder part of this original architecture).

# 9.6 Summary

This chapter has introduced the transformer and its components for the task of language modeling. We’ll continue the task of language modeling including issues like training and sampling in the next chapter.

Here’s a summary of the main points that we covered:

• Transformers are non-recurrent networks based on multi-head attention, a kind of self-attention. A multi-head attention computation takes an input vector $\mathbf { x } _ { i }$ and maps it to an output $\mathbf { a } _ { i }$ by adding in vectors from prior tokens, weighted by how relevant they are for the processing of the current word.   
• A transformer block consists of a residual stream in which the input from the prior layer is passed up to the next layer, with the output of different components added to it. These components include a multi-head attention layer followed by a feedforward layer, each preceded by layer normalizations. Transformer blocks are stacked to make deeper and more powerful networks.   
• The input to a transformer is computed by adding an embedding (computed with an embedding matrix) to a positional encoding that represents the sequential position of the token in the window.   
• Language models can be built out of stacks of transformer blocks, with a language model head at the top, which applies an unembedding matrix to the output H of the top layer to generate the logits, which are then passed through a softmax to generate word probabilities.   
• Transformer-based language models have a wide context window (200K tokens or even more for very large models with special mechanisms) allowing them to draw on enormous amounts of context to predict upcoming words.

# Bibliographical and Historical Notes

The transformer (Vaswani et al., 2017) was developed drawing on two lines of prior research: self-attention and memory networks.

Encoder-decoder attention, the idea of using a soft weighting over the encodings of input words to inform a generative decoder (see Chapter 13) was developed by Graves (2013) in the context of handwriting generation, and Bahdanau et al. (2015) for MT. This idea was extended to self-attention by dropping the need for separate encoding and decoding sequences and instead seeing attention as a way of weighting the tokens in collecting information passed from lower layers to higher layers (Ling et al., 2015; Cheng et al., 2016; Liu et al., 2016).

Other aspects of the transformer, including the terminology of key, query, and value, came from memory networks, a mechanism for adding an external readwrite memory to networks, by using an embedding of a query to match keys representing content in an associative memory (Sukhbaatar et al., 2015; Weston et al., 2015; Graves et al., 2014).

MORE HISTORY TBD IN NEXT DRAFT.

# CHAPTER 10 Large Language Models

“How much do we know at any time? Much more, or so I believe, than we know we know.”

Agatha Christie, The Moving Finger

Fluent speakers of a language bring an enormous amount of knowledge to bear during comprehension and production. This knowledge is embodied in many forms, perhaps most obviously in the vocabulary, the rich representations we have of words and their meanings and usage. This makes the vocabulary a useful lens to explore the acquisition of knowledge from text, by both people and machines.

Estimates of the size of adult vocabularies vary widely both within and across languages. For example, estimates of the vocabulary size of young adult speakers of American English range from 30,000 to 100,000 depending on the resources used to make the estimate and the definition of what it means to know a word. What is agreed upon is that the vast majority of words that mature speakers use in their day-to-day interactions are acquired early in life through spoken interactions with caregivers and peers, usually well before the start of formal schooling. This active vocabulary (usually on the order of 2000 words for young speakers) is extremely limited compared to the size of the adult vocabulary, and is quite stable, with very few additional words learned via casual conversation beyond this early stage. Obviously, this leaves a very large number of words to be acquired by other means.

A simple consequence of these facts is that children have to learn about 7 to 10 words a day, every single day, to arrive at observed vocabulary levels by the time they are 20 years of age. And indeed empirical estimates of vocabulary growth in late elementary through high school are consistent with this rate. How do children achieve this rate of vocabulary growth? The bulk of this knowledge acquisition seems to happen as a by-product of reading, as part of the rich processing and reasoning that we perform when we read. Research into the average amount of time children spend reading, and the lexical diversity of the texts they read, indicate that it is possible to achieve the desired rate. But the mechanism behind this rate of learning must be remarkable indeed, since at some points during learning the rate of vocabulary growth exceeds the rate at which new words are appearing to the learner!

Such facts have motivated the distributional hypothesis of Chapter 6, which suggests that aspects of meaning can be learned solely from the texts we encounter over our lives, based on the complex association of words with the words they co-occur with (and with the words that those words occur with). The distributional hypothesis suggests both that we can acquire remarkable amounts of knowledge from text, and that this knowledge can be brought to bear long after its initial acquisition. Of course, grounding from real-world interaction or other modalities can help build even more powerful models, but even text alone is remarkably useful.

In this chapter we formalize this idea of pretraining—learning knowledge about language and the world from vast amounts of text—and call the resulting pretrained language models large language models. Large language models exhibit remark

# generative AI

able performance on all sorts of natural language tasks because of the knowledge they learn in pretraining, and they will play a role throughout the rest of this book. They have been especially transformative for tasks where we need to produce text, like summarization, machine translation, question answering, or chatbots.

We’ll start by seeing how to apply the transformer of Chapter 9 to language modeling, in a setting often called causal or autoregressive language models, in which we iteratively predict words left-to-right from earlier words. We’ll first introduce training, seeing how language models are self-trained by iteratively being taught to guess the next word in the text from the prior words.

We’ll then talk about the process of text generation. The application of LLMs to generate text has vastly broadened the scope of NLP,. Text generation, codegeneration, and image-generation together constitute the important new area of generative AI. We’ll introduce specific algorithms for generating text from a language model, like greedy decoding and sampling. And we’ll see that almost any NLP task can be modeled as word prediction in a large language model, if we think about it in the right way. We’ll work through an example of using large language models to solve one classic NLP task of summarization (generating a short text that summarizes some larger document).

# 10.1 Large Language Models with Transformers

The prior chapter introduced most of the components of a transformer in the domain of language modeling: the transformer block including multi-head attention, the language modeling head, and the positional encoding of the input. In the following sections we’ll introduce the remaining aspects of the transformer LLM: sampling and training. Before we do that, we use this section to talk about why and how we apply transformer-based large language models to NLP tasks.

The tasks we will describe are all cases of conditional generation. Conditional generation is the task of generating text conditioned on an input piece of text. That is, we give the LLM an input piece of text, generally called a prompt, and then have the LLM continue generating text token by token, conditioned on the prompt and the previously generated tokens. The fact that transformers have such long contexts (many thousands of tokens) makes them very powerful for conditional generation, because they can look back so far into the prompting text.

Consider the simple task of text completion, illustrated in Fig. 10.1. Here a language model is given a text prefix and is asked to generate a possible completion. Note that as the generation process proceeds, the model has direct access to the priming context as well as to all of its own subsequently generated outputs (at least as much as fits in the large context window). This ability to incorporate the entirety of the earlier context and generated outputs at each time step is the key to the power of large language models built from transformers.

So why should we care about predicting upcoming words or tokens? The insight of large language modeling is that many practical NLP tasks can be cast as word prediction, and that a powerful-enough language model can solve them with a high degree of accuracy. For example, we can cast sentiment analysis as language modeling by giving a language model a context like:

The sentiment of the sentence ‘‘I like Jackie Chan" is: and comparing the following conditional probability of the words “positive” and the

![## Image Analysis: b9155a781f4f2f12c3f5973ee847edc1edf4a8fd4f948907909990ae56ce27d5.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow and operational mechanism of a large language model utilizing the transformer architecture for autoregressive text completion. The main purpose is to illustrate, step-by-step, how such a model processes an initial 'Prefix Text' and iteratively generates 'Completion Text' by predicting one token at a time and feeding its own predictions back into the input sequence to generate subsequent tokens. It conveys the key ideas of token-by-token generation, contextual encoding, and the role of transformer blocks in understanding sequence dependencies for language modeling tasks.

**Content Interpretation:**
The image depicts the architecture and operational flow of an autoregressive transformer-based large language model for text completion. It demonstrates how a sequence of input tokens (Prefix Text) is processed and how new tokens (Completion Text) are generated sequentially. The key processes shown are: 1. Input Embedding and Positional Encoding: Each word in the prefix is converted into a numerical representation (embedding 'E') and augmented with positional information, indicated by the '[circled plus sign] [square with I]' symbol within the 'Encoder' boxes. This allows the model to understand the sequence order. 2. Feature Extraction and Contextual Understanding: The 'Transformer Blocks' process these encoded inputs. These blocks are the core of the transformer, utilizing self-attention mechanisms (not explicitly labeled but implied by 'Transformer Blocks') to weigh the importance of different words in the context, allowing the model to understand long-range dependencies. 3. Next Token Prediction: The 'Language Modeling Head', comprising an 'Unencoder layer' and a 'Softmax' layer, takes the output of the 'Transformer Blocks' to predict the next token. The 'Unencoder layer' generates 'logits', which are raw prediction scores, and the 'Softmax' layer converts these into a probability distribution over the vocabulary. 4. Autoregressive Generation: The most significant aspect is the feedback loop, represented by the dashed lines. Once a token (e.g., 'all') is predicted as 'Completion Text', it is immediately added to the current context (effectively becoming part of the new 'Prefix Text') for the generation of the subsequent token (e.g., 'the'). This left-to-right, token-by-token generation is central to how these models complete text.

**Key Insights:**
The main takeaways from this image are: 1. Transformer-based large language models perform text completion in an autoregressive (left-to-right) manner, meaning they predict one token at a time. 2. Each predicted token is immediately incorporated into the input context to inform the prediction of the next token. This is evident from the dashed lines feeding the generated 'all' and 'the' back into the 'Encoder' path for subsequent predictions. 3. The process involves multiple stages: token embedding and positional encoding ('Encoder'), contextual processing ('Transformer Blocks'), and next-token probability prediction ('Language Modeling Head' with 'Unencoder layer' and 'Softmax'). 4. The 'Encoder' combines the token's embedding ('E') with positional information ('[circled plus sign] [square with I]') to represent each word in its sequence context. 5. The 'Softmax' layer outputs the probabilities for the next token based on the model's internal representation, from which the most probable token is selected as the completion.

**Document Context:**
This image directly supports Section 10.1, titled 'Large Language Models with Transformers', by visually explaining the core mechanism of how these models perform 'Left-to-right (also called autoregressive) text completion'. The accompanying text, 'As each token is generated, it gets added onto the context as a prefix for generating the next token,' perfectly describes the autoregressive feedback loop depicted by the dashed lines in the diagram. The figure serves as a detailed visual aid, illustrating the architectural components (Encoder, Transformer Blocks, Language Modeling Head) and the sequential, iterative process of text generation, making a complex concept more understandable to the reader within the broader discussion of transformer models.

**Summary:**
The image illustrates the process of left-to-right (autoregressive) text completion using a transformer-based large language model. It shows how an initial 'Prefix Text' is fed into an encoder, processed by multiple 'Transformer Blocks', and then passed to a 'Language Modeling Head' to generate 'Completion Text' token by token. Each newly generated token is fed back into the model as part of the prefix for predicting the next token. The process begins with the initial prefix words, 'So', 'long', 'and', 'thanks', 'for', 'all', 'the'. Each word is processed by an 'Encoder' which generates an embedding (E) combined with a positional encoding (represented by the [circled plus sign] [square with I] symbol). These encoded representations are then fed into a series of 'Transformer Blocks', which apply attention mechanisms and feed-forward networks, represented by stacked horizontal rectangles and ellipses indicating further layers. The output from the final 'Transformer Blocks' for the last token in the current context is then passed to the 'Language Modeling Head'. This head consists of an 'Unencoder layer' (represented by a green box with a trapezoidal input) which produces 'logits'. These 'logits' are then fed into a 'Softmax' layer (another green box with a bar chart symbol), which outputs probabilities for the next word. The word with the highest probability, for example, 'all' (in red text), is selected as the first part of the 'Completion Text'. This newly generated 'all' token is then added to the 'Prefix Text' via a dashed line, effectively extending the context. The model then processes the updated context ('So long and thanks for all the all') to predict the subsequent token, 'the' (in red text), following the same flow: through the 'Encoder', 'Transformer Blocks', 'Unencoder layer', and 'Softmax'. This iterative, token-by-token generation continues until a complete sequence or a stop condition is met, demonstrating the autoregressive nature where the model's own predictions become future inputs.](images/b9155a781f4f2f12c3f5973ee847edc1edf4a8fd4f948907909990ae56ce27d5.jpg)
Figure 10.1 Left-to-right (also called autoregressive) text completion with transformer-based large language models. As each token is generated, it gets added onto the context as a prefix for generating the next token.

word “negative” to see which is higher:

$P$ (positive|The sentiment of the sentence ‘‘I like Jackie Chan" is:) $P$ (negative|The sentiment of the sentence ‘‘I like Jackie Chan" is:)

If the word “positive” is more probable, we say the sentiment of the sentence is positive, otherwise we say the sentiment is negative.

We can also cast more complex tasks as word prediction. Consider question answering, in which the system is given a question (for example a question with a simple factual answer) and must give a textual answer; we introduce this task in detail in Chapter 14. We can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next:

Q: Who wrote the book ‘‘The Origin of Species"? A: If we ask a language model to compute the probability distribution over possible next words given this prefix:

$P ( w | \boldsymbol { 0 }$ : Who wrote the book ‘‘The Origin of Species"? A:)

and look at which words $w$ have high probabilities, we might expect to see that Charles is very likely, and then if we choose Charles and continue and ask

$P ( w | \boldsymbol { 0 }$ : Who wrote the book ‘‘The Origin of Species"? A: Charles)

we might now see that Darwin is the most probable token, and select it.

Conditional generation can even be used to accomplish tasks that must generate longer responses. Consider the task of text summarization, which is to take a long text, such as a full-length article, and produce an effective shorter summary of it. We can cast summarization as language modeling by giving a large language model a text, and follow the text by a token like tl;dr; this token is short for something like ‘too long; didn’t read’ and in recent years people often use this token, especially in informal work emails, when they are going to give a short summary. Since this token is sufficiently frequent in language model training data, language models have seen many texts in which the token occurs before a summary, and hence will interpret the token as instructions to generate a summary. We can then do conditional generation: give the language model this prefix, and then have it generate the following words, one by one, and take the entire response as a summary. Fig. 10.2 shows an example of a text and a human-produced summary from a widely-used summarization corpus consisting of CNN and Daily Mail news articles.

<table><tr><td>Original Article The only thing crazier than a guy in snowbound Massachusets boxing up the powdery white stuff and offering it for sale online? People are actually buying it. For $89,self-styled entrepreneur Kyle Waring willship you 6 pounds of Boston-area snow in an insulated Styrofoam box - enough for 10 to 15 snowballs, he says.</td></tr><tr><td>But not if you live in New England or surrounding states.“We will not ship snow to any states in the northeast!” says Waring&#x27;s website, ShipSnowYo.com.“We&#x27;re in the business of expunging snow!” His website and social media accounts claim to have filled more than 133 orders for snow - more than 30 on Tuesday alone,his busiest day yet. With more than 45 total inches,Boston has set a</td></tr><tr><td>record this winter for the snowiest month in its history. Most residents see the huge piles of snow choking their yards and sidewalks as a nuisance, but Waring saw an opportunity. According to Boston.com, it all started a few weeks ago, when Waring and his wife were shov- eling deep snow from their yard in Manchester-by-the-Sea,a coastal suburb north of Boston. He</td></tr><tr><td>joked about shipping the stufto friends and family in warmer states,and an idea was born. [.] Summary Kyle Waring willship you 6 pounds of Boston-area snow in an insulated Styrofoam box - enough</td></tr></table>

Figure 10.2 Excerpt from a sample article and its summary from the CNN/Daily Mail summarization corpus (Hermann et al., 2015b), (Nallapati et al., 2016).

If we take this full article and append the token tl;dr, we can use this as the context to prime the generation process to produce a summary as illustrated in Fig. 10.3. Again, what makes transformers able to succeed at this task (as compared, say, to the primitive n-gram language model) is that attention can incorporate information from the large context window, giving the model access to the original article as well as to the newly generated text throughout the process.

Which words do we generate at each step? One simple way to generate words is to always generate the most likely word given the context. Generating the most likely word given the context is called greedy decoding. A greedy algorithm is one that make a choice that is locally optimal, whether or not it will turn out to have been the best choice with hindsight. Thus in greedy decoding, at each time step in generation, the output $y _ { t }$ is chosen by computing the probability for each possible output (every word in the vocabulary) and then choosing the highest probability word (the argmax):

$$
\hat { w } _ { t } ~ = ~ \mathrm { a r g m a x } _ { w \in V } P ( w | \mathbf { w } _ { < t } )
$$

In practice, however, we don’t use greedy decoding with large language models. A major problem with greedy decoding is that because the words it chooses are (by definition) extremely predictable, the resulting text is generic and often quite repetitive. Indeed, greedy decoding is so predictable that it is deterministic; if the context is identical, and the probabilistic model is the same, greedy decoding will always result in generating exactly the same string. We’ll see in Chapter 13 that an extension to greedy decoding called beam search works well in tasks like machine translation, which are very constrained in that we are always generating a text in one language conditioned on a very specific text in another language. In most other tasks, however, people prefer text which has been generated by more sophisticated methods, called sampling methods, that introduce a bit more diversity into the generations. We’ll see how to do that in the next few sections.

![## Image Analysis: f813e09236a9524b9236d1f27d03c8686768526db0a2b82ef1490994412a93ed.jpg

**Conceptual Understanding:**
This image conceptually represents the operational pipeline of a Large Language Model (LLM) utilizing a Transformer architecture for text summarization. Its main purpose is to illustrate how an LLM processes an input 'Original Story' and, guided by a special 'tl;dr' delimiter token, subsequently generates a 'Generated Summary' in an autoregressive manner. The diagram visually explains the sequential steps from input token embeddings through the multi-layered processing of the LLM to the final prediction of summary words by the 'LM Head'. It fundamentally communicates the mechanism of controlled text generation, where a specific prompt (the 'tl;dr' token) influences the model's output to perform a summarization task, leveraging the model's understanding of context from the entire input sequence.

**Content Interpretation:**
The image illustrates the architectural flow of a Transformer-based large language model performing a text summarization task. It specifically demonstrates how an input 'Original Story' is processed, followed by a special 'tl;dr' delimiter token, which then triggers the autoregressive generation of a 'Generated Summary'. The 'Original Story' tokens ('The', 'only', '...', 'idea', 'was', 'born.') are each represented as input embeddings (pink boxes labeled 'E'). These embeddings are then fed into the main body of the language model, represented by the large purple block with many stacked horizontal layers. These layers signify the internal processing of the Transformer model, likely involving self-attention and feed-forward networks, which build contextual representations for each token. A 'tl;dr' token is then introduced as a 'Delimiter', signaling the model to switch from processing input to generating output. Subsequently, the model enters an autoregressive generation phase, where it predicts summary tokens one by one ('Kyle', 'Waring', 'will', '...'). For each predicted token, the processed context from the language model (purple block) is fed into an 'LM Head' (green block). Each 'LM Head' component (containing a bar chart-like prediction and a 'U' shape) is responsible for producing the next summary word based on the accumulated context, including the original story and the previously generated summary words. The multiple arrows from the lower layers up to the 'LM Head' and between the layers themselves emphasize the deep contextual processing and information flow within the Transformer architecture, crucial for generating coherent and relevant summaries. The red text for 'Kyle', 'Waring', 'will' differentiates them as generated output.

**Key Insights:**
The main takeaway from this image is the demonstration of how large language models, specifically Transformer-based models, can perform summarization using a special delimiter token and autoregressive generation. 

Key insights include:
1.  **Delimiter-driven summarization:** The 'tl;dr' token acts as a critical signal, marking the end of the input text and the beginning of the summary generation phase. This highlights a method for explicitly instructing a language model to perform a specific task within a continuous text stream.
2.  **Autoregressive generation:** The summary is generated word by word ('Kyle', 'Waring', 'will', '...'), where each subsequent word is predicted based on the entire preceding sequence (original story + delimiter + previously generated summary words). This is fundamental to how many large language models produce coherent text.
3.  **Layered processing:** The input tokens and generated outputs pass through multiple internal layers of the language model (represented by the stacked horizontal lines within the large purple block), indicating the deep, multi-stage contextual understanding and transformation performed by Transformers.
4.  **Embedding representation:** Each input word ('The', 'only', etc.) is first converted into an embedding (boxes labeled 'E'), which are dense vector representations that capture semantic meaning.
5.  **LM Head for prediction:** The 'LM Head' layer is responsible for the final probability distribution over the vocabulary to select the next token, effectively translating the internal representations into human-readable words. This is evidenced by the bar chart-like structure within the LM Head components.

All extracted text elements, such as 'Original Story', 'tl;dr', 'Delimiter', 'Generated Summary', 'LM Head', and the specific words 'The', 'only', 'idea', 'was', 'born.', 'Kyle', 'Waring', 'will', collectively provide the complete operational evidence for these insights, illustrating the entire summarization pipeline.

**Document Context:**
This image directly supports the document's section '10.1 Large Language Models with Transformers' by visually explaining a core application: summarization. It elaborates on the concept introduced by the surrounding text, 'Figure 10.3 Summarization with large language models using the tl;dr token and context-based autoregressive generation,' by providing a detailed diagram of the process. It helps readers understand how a specific token ('tl;dr') can be used to control the behavior of a large language model, initiating a summarization task, and how the model then autoregressively generates text based on the preceding context. This visual aid clarifies the abstract concepts of input processing, the role of a delimiter, and the autoregressive generation mechanism within the Transformer architecture for a practical application like summarization.

**Summary:**
This diagram illustrates the process of summarization using large language models with Transformers, specifically demonstrating context-based autoregressive generation facilitated by a 'tl;dr' token. The process begins with an 'Original Story' represented as a sequence of input tokens: 'The', 'only', '...', 'idea', 'was', 'born.'. Each of these tokens is fed into an embedding layer (represented by a pink box with 'E' and a small square) and then processed through multiple layers of a language model (depicted by the large purple rectangular block containing stacks of horizontal lines and arrows). Following the 'Original Story', a 'tl;dr' token acts as a 'Delimiter'. This special token signals the language model to begin generating a summary. The generation then proceeds autoregressively, where the model predicts subsequent tokens based on the preceding context (both the original story and the already generated summary tokens). The 'LM Head' (a green rounded rectangular block at the top) represents the final output layer of the language model responsible for predicting the next word. For each predicted word, there's a corresponding 'LM Head' component (containing a bar chart and a 'U' shape) that outputs the predicted token. In this example, the 'Generated Summary' begins with 'Kyle', 'Waring', 'will', '...'. The diagram clearly shows the flow from input tokens, through the model's layers, to the generation of a summary, highlighting the role of the 'tl;dr' delimiter in initiating the summarization task. All text elements, including input words, delimiter, generated words, and structural labels, are explicitly shown and connected to their respective processing stages.](images/f813e09236a9524b9236d1f27d03c8686768526db0a2b82ef1490994412a93ed.jpg)
Figure 10.3 Summarization with large language models using the tl;dr token and context-based autoregressive generation.

# 10.2 Sampling for LLM Generation

The core of the generation process for large language models is the task of choosing the single word to generate next based on the context and based on the probabilities that the model assigns to possible words. This task of choosing a word to generate based on the model’s probabilities is called decoding. Decoding from a language model in a left-to-right manner (or right-to-left for languages like Arabic in which we read from right to left), and thus repeatedly choosing the next word conditioned on our previous choices is called autoregressive generation or causal LM generation.1 (As we’ll see, alternatives like the masked language models of Chapter 11 are non-causal because they can predict words based on both past and future words).

The most common method for decoding in large language models is sampling. Recall from Chapter 3 that sampling from a model’s distribution over words means to choose random words according to their probability assigned by the model. That is, we iteratively choose a word to generate according to its probability in context as defined by the model. Thus we are more likely to generate words that the model thinks have a high probability in the context and less likely to generate words that the model thinks have a low probability.

We saw back in Chapter 3 on page 43 how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability until we either reach a pre-determined length or select the end-of-sentence token. To generate text from a trained transformer language model we’ll just generalize this model a bit: at each step we’ll sample words according to their probability conditioned on our previous choices, and we’ll use a transformer language model as the probability model that tells us this probability.

We can formalize this algorithm for generating a sequence of words $W = w _ { 1 } , w _ { 2 } , \dots , w _ { N }$ until we hit the end-of-sequence token, using $x \sim p ( x )$ to mean ‘choose $x$ by sampling from the distribution $p ( x )$ :

$$
\begin{array} { r l } & { \mathrm i  1 } \\ & { w _ { i } \sim \mathsf p ( w ) } \\ & { \mathbf w \mathbf h \mathbf i \mathbf j \mathbf e \ w _ { i } \mathrel { \mathop : } = \mathbf E \boldsymbol \mathsf { O S } } \\ & { \quad \mathrm i  \mathrm i + 1 } \\ & { \quad w _ { i } \sim \mathsf p ( w _ { i } \mid w _ { < i } ) } \end{array}
$$

The algorithm above is called random sampling, and it turns out random sampling doesn’t work well enough. The problem is that even though random sampling is mostly going to generate sensible, high-probable words, there are many odd, lowprobability words in the tail of the distribution, and even though each one is lowprobability, if you add up all the rare words, they constitute a large enough portion of the distribution that they get chosen often enough to result in generating weird sentences. For this reason, instead of random sampling, we usually use sampling methods that avoid generating the very unlikely words.

The sampling methods we introduce below each have parameters that enable trading off two important factors in generation: quality and diversity. Methods that emphasize the most probable words tend to produce generations that are rated by people as more accurate, more coherent, and more factual, but also more boring and more repetitive. Methods that give a bit more weight to the middle-probability words tend to be more creative and more diverse, but less factual and more likely to be incoherent or otherwise low-quality.

# 10.2.1 Top-k sampling

Top-k sampling is a simple generalization of greedy decoding. Instead of choosing the single most probable word to generate, we first truncate the distribution to the top $k$ most likely words, renormalize to produce a legitimate probability distribution, and then randomly sample from within these $k$ words according to their renormalized probabilities. More formally:

1. Choose in advance a number of words $k$   
2. For each word in the vocabulary $V$ , use the language model to compute the likelihood of this word given the context $p ( w _ { t } | \mathbf { w } _ { < t } )$   
3. Sort the words by their likelihood, and throw away any word that is not one of the top $k$ most probable words.   
4. Renormalize the scores of the $k$ words to be a legitimate probability distribution.

5. Randomly sample a word from within these remaining $k$ most-probable words according to its probability.

When $k = 1$ , top- $k$ sampling is identical to greedy decoding. Setting $k$ to a larger number than 1 leads us to sometimes select a word which is not necessarily the most probable, but is still probable enough, and whose choice results in generating more diverse but still high-enough-quality text.

# 10.2.2 Nucleus or top- $\pmb { p }$ sampling

One problem with top- $k$ sampling is that $k$ is fixed, but the shape of the probability distribution over words differs in different contexts. If we set $k = 1 0$ , sometimes the top 10 words will be very likely and include most of the probability mass, but other times the probability distribution will be flatter and the top 10 words will only include a small part of the probability mass.

An alternative, called top-p sampling or nucleus sampling (Holtzman et al., 2020), is to keep not the top $k$ words, but the top $p$ percent of the probability mass. The goal is the same; to truncate the distribution to remove the very unlikely words. But by measuring probability rather than the number of words, the hope is that the measure will be more robust in very different contexts, dynamically increasing and decreasing the pool of word candidates.

Given a distribution $P ( w _ { t } | \mathbf { w } _ { < t } )$ , we sort the distribution from most probable, and then the top- $p$ vocabulary $V ^ { ( p ) }$ is the smallest set of words such that

$$
\sum _ { w \in V ^ { ( p ) } } P ( w | \mathbf { w } _ { < t } ) \geq p .
$$

# 10.2.3 Temperature sampling

In temperature sampling, we don’t truncate the distribution, but instead reshape it. The intuition for temperature sampling comes from thermodynamics, where a system at a high temperature is very flexible and can explore many possible states, while a system at a lower temperature is likely to explore a subset of lower energy (better) states. In low-temperature sampling, we smoothly increase the probability of the most probable words and decrease the probability of the rare words.

We implement this intuition by simply dividing the logit by a temperature parameter $\tau$ before we normalize it by passing it through the softmax. In low-temperature sampling, $\tau \in ( 0 , 1 ]$ . Thus instead of computing the probability distribution over the vocabulary directly from the logit as in the following (repeated from Eq. 9.46):

$$
\texttt { \textbf { y } } = \operatorname { \ s o f t m a x } ( u )
$$

we instead first divide the logits by $\tau$ , computing the probability vector $y$ as

$$
\begin{array} { r } { \mathsf { y } = \mathsf { s o f t m a x } ( u / \tau ) } \end{array}
$$

Why does this work? When $\tau$ is close to 1 the distribution doesn’t change much. But the lower $\tau$ is, the larger the scores being passed to the softmax (dividing by a smaller fraction $\tau \leq 1$ results in making each score larger). Recall that one of the useful properties of a softmax is that it tends to push high values toward 1 and low values toward 0. Thus when larger numbers are passed to a softmax the result is a distribution with increased probabilities of the most high-probability words and decreased probabilities of the low probability words, making the distribution more greedy. As $\tau$ approaches 0 the probability of the most likely word approaches 1.

Note, by the way, that there can be other situations where we may want to do something quite different and flatten the word probability distribution instead of making it greedy. Temperature sampling can help with this situation too, in this case high-temperature sampling, in which case we use $\tau > 1$ .

# 10.3 Pretraining Large Language Models

How do we teach a transformer to be a language model? What is the algorithm and what data do we train on?

# 10.3.1 Self-supervised training algorithm

self-supervision

To train a transformer as a language model, we use the same self-supervision (or self-training) algorithm we saw in Section 8.2.2: we take a corpus of text as training material and at each time step t ask the model to predict the next word. We call such a model self-supervised because we don’t have to add any special gold labels to the data; the natural sequence of words is its own supervision! We simply train the model to minimize the error in predicting the true next word in the training sequence, using cross-entropy as the loss function.

Recall that the cross-entropy loss measures the difference between a predicted probability distribution and the correct distribution.

$$
L _ { C E } \ = \ - \sum _ { w \in V } \mathbf { y } _ { t } [ w ] \log \hat { \mathbf { y } } _ { t } [ w ]
$$

In the case of language modeling, the correct distribution $\pmb { y } _ { t }$ comes from knowing the next word. This is represented as a one-hot vector corresponding to the vocabulary where the entry for the actual next word is 1, and all the other entries are 0. Thus, the cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word (all other words get multiplied by zero). So at time $t$ the CE loss in Eq. 10.5 can be simplified as the negative log probability the model assigns to the next word in the training sequence.

$$
L _ { C E } ( \hat { \mathbf { y } } _ { t } , \mathbf { y } _ { t } ) ~ = ~ - \log \hat { \mathbf { y } } _ { t } [ w _ { t + 1 } ]
$$

Thus at each word position $t$ of the input, the model takes as input the correct sequence of tokens $w _ { 1 : t }$ , and uses them to compute a probability distribution over possible next words so as to compute the model’s loss for the next token $w _ { t + 1 }$ . Then we move to the next word, we ignore what the model predicted for the next word and instead use the correct sequence of tokens $w _ { 1 : t + 1 }$ to estimate the probability of token $w _ { t + 2 }$ . This idea that we always give the model the correct history sequence to predict the next word (rather than feeding the model its best case from the previous time step) is called teacher forcing.

Fig. 10.4 illustrates the general training approach. At each step, given all the preceding words, the final transformer layer produces an output distribution over the entire vocabulary. During training, the probability assigned to the correct word is used to calculate the cross-entropy loss for each item in the sequence. The loss for a training sequence is the average cross-entropy loss over the entire sequence. The weights in the network are adjusted to minimize the average CE loss over the training sequence via gradient descent.

![## Image Analysis: 3a8c6aa6d7416b8c4c97e3efd0e2f3c7e6f4123871e18d7a045763f91230cf5b.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and training methodology of a transformer model used as a language model. Its main purpose is to illustrate the self-supervised training process where the model predicts the next token in a sequence. Key ideas communicated include the layered processing of input tokens, the role of positional encoding, the function of stacked transformer blocks (with self-attention), the generation of logits by a language modeling head, and the calculation of cross-entropy loss to guide the learning process. Essentially, it demonstrates how a transformer learns to understand and generate language by predicting subsequent words based on preceding context.

**Content Interpretation:**
The image illustrates the architectural flow and training objective of a transformer model configured for language modeling. It shows how a sequence of input tokens is processed layer by layer to predict the subsequent token at each position. The core concepts include input embedding and positional encoding, multi-layer transformer blocks utilizing self-attention, a language modeling head to generate output probabilities (logits), and the calculation of a cross-entropy loss to quantify prediction error. The model predicts the next token in the sequence based on all preceding tokens, a fundamental aspect of autoregressive language modeling. The significance lies in demonstrating the self-supervised nature of this training, where the model learns by predicting masked or subsequent tokens from the input data itself, without explicit labels.

**Key Insights:**
The main takeaway is that transformers can be trained in a self-supervised manner for language modeling by learning to predict the next token in a sequence. The process involves tokenizing input, adding positional encodings to form enriched input vectors, processing these through multiple self-attention-enabled transformer layers, and finally using a language modeling head to generate logits for the next token prediction. The training objective is to minimize the average cross-entropy loss across all predicted next tokens in the sequence. This architecture allows the model to capture long-range dependencies and contextual information within the text, making it highly effective for various NLP tasks. The "..." in the diagram signify that the model can handle sequences of arbitrary length, processing each token's prediction in parallel while still attending to prior context. Each output predicts the *next* token in the sequence, making it an autoregressive model.

**Document Context:**
This image directly supports Section 10.3.1, which discusses "Self-supervised training algorithms," and is explicitly identified as "Figure 10.4 Training a transformer as a language model." It provides a clear visual representation of the theoretical concepts described in the surrounding text, specifically detailing the architecture and objective function used when training a transformer for next-token prediction, a common self-supervised task in natural language processing. It visually grounds the abstract idea of how a transformer learns language patterns by predicting words in a sequence.

**Summary:**
The image, titled "Training a transformer as a language model," depicts a self-supervised training algorithm for a transformer model, demonstrating how it predicts the next token in a sequence and calculates loss. The process flows from input tokens at the bottom to a total loss calculation at the top right.

Here's a detailed breakdown of the process:

1.  **Input tokens:** The process begins with a sequence of input tokens: "So", "long", "and", "thanks", "for", followed by an ellipsis indicating more tokens.

2.  **Input Encoding:** Each input token is processed through an "Input Encoding" step. For each token:
    *   The token feeds into a trapezoidal shape labeled "E" (presumably for Embedding).
    *   The output of "E" is combined with a positional encoding represented by a green diamond with a plus sign and a number: "+1" for "So", "+2" for "long", "+3" for "and", "+4" for "thanks", and "+5" for "for".
    *   The combined encoding results in vectors labeled "x1", "x2", "x3", "x4", "x5" for the respective input tokens.

3.  **Stacked Transformer Blocks:** The encoded inputs (x1 to x5) are fed into "Stacked Transformer Blocks". Each block column represents a transformer layer, which consists of multiple sub-layers (indicated by blue, yellow, and red horizontal layers). Dots "..." vertically within each stack suggest multiple layers, and horizontal arrows connecting different layers across the sequence indicate self-attention mechanisms. The output of the top-most layer of each transformer block is then passed upwards.

4.  **Language Modeling Head:** The output from each "Stacked Transformer Block" is then passed to a "Language Modeling Head". For each token position:
    *   The input enters a trapezoidal shape labeled "U" (likely a linear projection matrix).
    *   The output of "U" generates "logits", represented by a small icon resembling a histogram or bar chart above the "logits" label.

5.  **Loss Calculation:** The "logits" from the "Language Modeling Head" are used to calculate the "Loss" for predicting the "Next token".
    *   For each position, the model attempts to predict the next token in the sequence (e.g., predicting "long" after "So", "and" after "long").
    *   The individual loss for each prediction is calculated as: "- log y_long", "- log y_and", "- log y_thanks", "- log y_for", "- log y_all".

6.  **Total Loss:** The individual losses are summed up and averaged to obtain the overall loss for the sequence. This is represented by the equation: "= (1/T) Σ_{t=1}^{T} L_{CE}", where T is the total number of tokens in the sequence and L_CE is the cross-entropy loss for each token prediction. This indicates that the model is trained to minimize the average cross-entropy loss across all token predictions in the sequence.](images/3a8c6aa6d7416b8c4c97e3efd0e2f3c7e6f4123871e18d7a045763f91230cf5b.jpg)
Figure 10.4 Training a transformer as a language model.

Note the key difference between this figure and the earlier RNN-based version shown in Fig. 8.6. There the calculation of the outputs and the losses at each step was inherently serial given the recurrence in the calculation of the hidden states. With transformers, each training item can be processed in parallel since the output for each element in the sequence is computed separately.

Large models are generally trained by filling the full context window (for example 4096 tokens for GPT4 or 8192 for Llama 3) with text. If documents are shorter than this, multiple documents are packed into the window with a special end-of-text token between them. The batch size for gradient descent is usually quite large (the largest GPT-3 model uses a batch size of 3.2 million tokens).

# 10.3.2 Training corpora for large language models

Large language models are mainly trained on text scraped from the web, augmented by more carefully curated data. Because these training corpora are so large, they are likely to contain many natural examples that can be helpful for NLP tasks, such as question and answer pairs (for example from FAQ lists), translations of sentences between various languages, documents together with their summaries, and so on.

Web text is usually taken from corpora of automatically-crawled web pages like the common crawl, a series of snapshots of the entire web produced by the nonprofit Common Crawl (https://commoncrawl.org/) that each have billions of webpages. Various versions of common crawl data exist, such as the Colossal Clean Crawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English that is filtered in various ways (deduplicated, removing non-natural language like code, sentences with offensive words from a blocklist). This C4 corpus seems to consist in large part of patent text documents, Wikipedia, and news sites (Dodge et al., 2021).

Wikipedia plays a role in lots of language model training, as do corpora of books. The Pile (Gao et al., 2020) is an 825 GB English text corpus that is constructed by publicly released code, containing again a large amount of text scraped from the web as well as books and Wikipedia; Fig. 10.5 shows its composition. Dolma is a larger open corpus of English, created with public tools, containing three trillion tokens, which similarly consists of web text, academic papers, code, books, encyclopedic materials, and social media (Soldaini et al., 2024).

![## Image Analysis: 6fa945d7e29292662addd4922c1033a506fc8fc9c963f24c5230e0c103145afc.jpg

**Conceptual Understanding:**
This image is a treemap that conceptually represents the hierarchical and proportional composition of 'The Pile' corpus. Its main purpose is to visually demonstrate the various constituent datasets that make up 'The Pile' and their relative 'effective sizes' or contributions to the whole. The image communicates the key idea that large language models are trained on highly diverse and extensive collections of text, and it provides a clear breakdown of the specific sources and their individual scale within such a corpus.

**Content Interpretation:**
The treemap visually represents the diverse sources that constitute The Pile corpus, a significant dataset for training large language models. Each rectangular block's area signifies the relative 'effective size' or contribution of that specific data source to the total corpus. The grouping by color, as clarified by the document's context, allows for an intuitive understanding of the categories of data included. For example, the blue areas encompass academic and patent documents (PubMed Central, ArXiv, USPTO), the green areas cover web texts (Pile-CC, Wikipedia, StackExchange), the orange areas include large book corpora (Bibliotik, PG-19), the grey areas consist of code and mathematical data (Github, DM Math), and the yellow areas cover dialogue and other miscellaneous data (Subtitles, IRC, HN, YT). The varying sizes of the blocks, such as the large areas for 'Pile-CC', 'PubMed Central', 'ArXiv', and 'Bibliotik', highlight their substantial contributions, while smaller blocks like 'NIH', 'Phil', 'BC2', 'EP', 'HN', 'YT', and 'IRC' represent more modest, yet still included, components.

**Key Insights:**
The main takeaways from this image are: 
1.  **Extreme Diversity of Training Data:** The Pile corpus is constructed from an exceptionally wide range of sources, including academic literature ('PubMed Central', 'ArXiv'), legal documents ('FreeLaw', 'USPTO'), web content ('Pile-CC', 'OpenWebText2', 'Wikipedia', 'StackExchange'), books ('Bibliotik', 'PG-19'), programming code ('Github'), mathematical texts ('DM Math'), and conversational/dialogue data ('Subtitles', 'IRC', 'HN', 'YT'). This comprehensive mix of data types is critical for training large language models to handle a vast array of linguistic tasks and domains. 
2.  **Unequal Contribution of Components:** The 'effective size' of each component, as indicated by the area of its block, varies significantly. For instance, 'Pile-CC', 'PubMed Central', 'ArXiv', and 'Bibliotik' are major contributors, forming the largest segments of the corpus. In contrast, sources like 'NIH', 'Phil', 'BC2', 'EP', 'HN', 'YT', and 'IRC' represent much smaller, albeit still distinct, parts of the overall dataset. This shows that while diversity is sought, some data types are more abundantly available or deemed more crucial for model training. 
3.  **Insights into LLM Training Data Design:** The detailed breakdown illustrates the deliberate effort in curating a balanced and extensive dataset to ensure large language models are exposed to a rich tapestry of knowledge and linguistic styles. The inclusion of specialized domains like medical research ('PubMed Central'), legal text ('FreeLaw'), and programming code ('Github') suggests a goal of producing highly capable and versatile models.

**Document Context:**
This image is highly relevant to Section 10.3.2 'Training corpora for large language models' as it provides a detailed visual breakdown of 'The Pile' corpus, which is explicitly mentioned as a key training dataset. The treemap illustrates the breadth and depth of data sources used, thereby supporting the broader discussion on the composition and requirements of such large corpora. By showing the relative contributions of academic papers, web texts, books, code, and dialogue data, it visually reinforces the complexity and diversity necessary for effective language model training, directly addressing the theme of the section.

**Summary:**
The image displays a treemap illustrating the composition of The Pile corpus, a training dataset for large language models, showing the effective size of its different components. The area of each rectangle is proportional to the component's effective size. The components are visually grouped by color, though the color legend is provided in the document context. 

Starting from the top-left and moving across, the blue-colored section on the left represents academic and legal content. It includes a large block labeled "PubMed Central" and another large block labeled "ArXiv". Below these, there are smaller blue blocks for "FreeLaw", "USPTO", "PMA", "Phil", and "NIH".

The large green-colored section in the middle primarily represents web and general text data. It features a very large block labeled "Pile-CC", and below it, other green blocks labeled "OpenWebText2", "Wikipedia", and "StackExchange".

The orange-colored section on the top-right represents book corpora. It contains a large block labeled "Bibliotik", followed by a smaller block below it labeled "PG-19", and a small block next to "PG-19" labeled "BC2".

To the right of the green section and below the orange section, there is a grey-colored section, likely representing code and mathematical content. It includes a large block labeled "Github" and a smaller block below it labeled "DM Math".

Finally, the yellow-colored section on the bottom-right represents dialogue and miscellaneous data. It contains blocks labeled "Subtitles", "IRC", "EP", "HN", and "YT", with "Subtitles" being the largest among them in this section.](images/6fa945d7e29292662addd4922c1033a506fc8fc9c963f24c5230e0c103145afc.jpg)
Figure 10.5 Figure 1: Treemap of Pile components by effective size.The Pile corpus, showing the size of different components, color coded as academic (articles from PubMed and ArXiv, patents from the USPTA; internet (webtext including a subset of the common crawl as well as Wikipedia), prose (a large corpus of books), Pile-CC, with improved extraction quality.dialogue (including movie subtitles and chat data), and misc.. Figure from Gao et al. (2020).

significantly distinct from pure CommoFiltering for quality and safety Crawl language dataset for language modeling com-Pretraining data drawn from the web is filtered data. Additionally, our evaluations show that the bining 22 diverse sources.for both quality and safety. Quality filters are classifiers that assign a score to each 2. The introduction of 14 new language model-document. Quality is of course subjective, so different quality filters are trained trained on the Pile significantly outperform both ing datasets, which we expect to be of inde-in different ways, but often to value high-quality reference corpora like Wikipedia, raw and filtered Common Crawl models. To com- pendent interest to researchers.books, and particular websites and to avoid websites with lots of PII (Personal Idenplement the performance evaluations, we also per- 3. Evaluations demonstrating significant im-tifiable Information) or adult content. Filters also remove boilerplate text which is provements across many domains by GPT-2-very frequent on the web. Another kind of quality filtering is deduplication, which hope that our extensive documentation of the con- sized models trained on this new dataset, com-can be done at various levels, so as to remove duplicate documents, duplicate web struction and characteristics of the Pile will help       Crawl.pages, or duplicate text. Quality filtering generally improves language model perresearchers make informed decisions about poten-formance (Longpre et al., 2024b; Llama Team, 2024).

l downstream applications.      Safety filtering is again a subjective decision, and often includes toxicity detecFinally, we make publicly available the preprocess- searchers about how to use it as well as moti-tion based on running off-the-shelf toxicity classifiers. This can have mixed results. ing code for the constituent datasets of the Pile and vate them to undertake similar investigationsOne problem is that current toxicity classifiers mistakenly flag non-toxic data if it the code for constructing alternative versions . In of their own data.is generated by speakers of minority dialects like African American English $\mathrm { { X u } }$ et al., 2021). Another problem is that models trained on toxicity-filtered data, while Pile as a whole) in as much detail as possible. Forsomewhat less toxic, are also worse at detecting toxicity themselves (Longpre et al., further details about the processing of each dataset, The Pile is composed of 22 constituent sub-datasets,2024b). These issues make the question of how to do better safety filtering an imsee Section 2 and Appendi portant open problem.

2 https://github.com/EleutherAI/ nents, with certain high-quality datasets such asUsing large datasets scraped from the web to train language models poses ethical the-pileand legal questions:

Copyright: Much of the text in these large datasets (like the collections of fiction and non-fiction books) is copyrighted. In some countries, like the United States, the fair use doctrine may allow copyrighted content to be used for transformative uses, but it’s not clear if that remains true if the language models are used to generate text that competes with the market for the text they

are trained on (Henderson et al., 2023).

Data consent: Owners of websites can indicate that they don’t want their sites to be crawled by web crawlers (either via a robots.txt file, or via Terms of Service). Recently there has been a sharp increase in the number of websites that have indicated that they don’t want large language model builders crawling their sites for training data (Longpre et al., 2024a). Because it’s not clear what legal status these indications have in different countries, or whether these restrictions are retroactive, what effect this will have on large pretraining datasets is unclear.

Privacy: Large web datasets also have privacy issues since they contain private information like phone numbers and IP addresses. While filters are used to try to remove websites likely to contain large amounts of personal information, such filtering isn’t sufficient.

# 10.3.3 Finetuning

Although the enormous pretraining data for a large language model includes text from many domains, it’s often the case that we want to apply it in a new domain or task that might not have appeared sufficiently in the pre-training data. For example, we might want a language model that’s specialized to legal or medical text. Or we might have a multilingual language model that knows many languages but might benefit from some more data in our particular language of interest. Or we want a language model that is specialized to a particular task.

In such cases, we can simply continue training the model on relevant data from the new domain or language (Gururangan et al., 2020). This process of taking a fully pretrained model and running additional training passes on some new data is called finetuning. Fig. 10.6 sketches the paradigm.

# finetuning

![## Image Analysis: 49f4a6b11411b334318aa4d0efd4cd2ccd3087571229b6791392fff65c343124.jpg

**Conceptual Understanding:**
The image conceptually represents the standard pipeline for developing powerful language models. Its main purpose is to illustrate the distinct stages of 'Pretraining' and 'Fine-tuning', showing how a general-purpose language model, initially trained on a vast amount of data, can be subsequently adapted and specialized for a particular task or domain using a more targeted dataset. It communicates the idea of progressive specialization in machine learning model development.

**Content Interpretation:**
The image depicts the two primary phases of Language Model (LM) development: pretraining and fine-tuning. The 'Pretraining Data', a large stack of books, represents a vast, general text corpus used to teach the model fundamental language understanding. The 'Pretrained LM' symbolizes a large neural network model that has acquired broad linguistic knowledge. The 'Fine-tuning Data', a smaller cylindrical database, signifies a specialized dataset for a particular domain or task. The 'Fine-tuning' process illustrates the adaptation of the 'Pretrained LM' to this specific data, resulting in a 'Fine-tuned LM' that is specialized for a target application. The layered structures within the LMs indicate the complex, deep learning architecture typically employed.

**Key Insights:**
The main takeaways from this image are: 1. Language model development often involves a sequential two-stage process: pretraining followed by fine-tuning. 2. 'Pretraining Data' is typically a large, general corpus, enabling the model to learn broad language understanding, as evidenced by the 'Pretraining Data' represented as a large stack of books and the 'Pretraining' arrow leading to a 'Pretrained LM'. 3. 'Fine-tuning Data' is a more specialized and typically smaller dataset, used to adapt the 'Pretrained LM' for specific tasks or domains, as shown by the 'Fine-tuning Data' cylinder feeding into the 'Fine-tuning' process. 4. The output of this process, the 'Fine-tuned LM', is a specialized version of the original 'Pretrained LM', optimized for a particular application, which is visually represented by the 'Fine-tuned LM' at the end of the process.

**Document Context:**
This image directly supports the document's 'finetuning' section by visually explaining the conceptual workflow of pretraining and fine-tuning a language model. It concretely illustrates the statement that 'A pre-trained model can be finetuned to a particular domain, dataset, or task' and provides a visual aid for understanding how general models are adapted for specific purposes, bridging abstract concepts with a clear diagrammatic representation. The visual distinction between 'Pretraining Data' (large, general) and 'Fine-tuning Data' (smaller, specialized) reinforces the text's explanation of tailoring models.

**Summary:**
The image illustrates the two-stage process of training a Language Model (LM): pretraining followed by fine-tuning. The process begins with 'Pretraining Data', depicted as a tall stack of various colored books, symbolizing a vast and general corpus of text. An arrow labeled 'Pretraining' points from this data to a 'Pretrained LM'. The 'Pretrained LM' is visually represented by two identical, layered neural network structures. Each structure consists of multiple horizontal blue rectangles representing layers, connected by vertical dashed lines and dots ('...') indicating additional layers. The top of each structure has a light green capsule, and the bottom has a light pink capsule. These neural networks symbolize a model that has learned general language patterns from the extensive pretraining data. Following the 'Pretrained LM', another arrow labeled 'Fine-tuning' leads to the 'Fine-tuned LM'. This 'Fine-tuning' process also incorporates 'Fine-tuning Data', which is depicted as a smaller, red cylindrical database located above and feeding into the 'Fine-tuning' arrow. This cylinder is labeled 'Fine-tuning Data'. The 'Fine-tuned LM' is represented by three identical layered neural network structures, visually similar to the 'Pretrained LM' structures, each with the same internal layers and top green and bottom pink capsules. This final representation signifies a model that has been adapted and specialized for a particular domain, dataset, or task using the fine-tuning data. The overall flow shows a progression from general data to a general model, and then to a specialized model via specific data.](images/49f4a6b11411b334318aa4d0efd4cd2ccd3087571229b6791392fff65c343124.jpg)
Figure 10.6 Pretraining and finetuning. A pre-trained model can be finetuned to a particular domain, dataset, or task. There are many different ways to finetune, depending on exactly which parameters are updated from the finetuning data: all the parameters, some of the parameters, or only the parameters of specific extra circuitry.

We’ll introduce four related kinds of finetuning in this chapter and the two following chapters. In all four cases, finetuning means the process of taking a pretrained model and further adapting some or all of its parameters to some new data. But they differ on exactly which parameters get updated.

In the first kind of finetuning we retrain all the parameters of the model on this new data, using the same method (word prediction) and loss function (cross-entropy loss) as for pretraining. In a sense it’s as if the new data were at the tail end of the pretraining data, and so you’ll sometimes see this method called continued pretraining.

Retraining all the parameters of the model is very slow and expensive when the language model is huge. So instead we can freeze some of the parameters (i.e., leave them unchanged from their pretrained value) and train only a subset of parameters on the new data. In Section 10.5.3 we’ll describe this second variety of finetuning, called parameter-efficient finetuning, or PEFT. because we efficiently select specific parameters to update when finetuning, and leave the rest in their pretrained values.

In Chapter 11 we’ll introduce a third kind of finetuning, also parameter-efficient. In this version, the goal is to use a language model as a kind of classifier or labeler for a specific task. For example we might train the model to be a sentiment classifier. We do this by adding extra neural circuitry (an extra head) after the top layer of the model. This classification head takes as input some of the top layer embeddings of the transformer and produces as output a classification. In this method, most commonly used with masked language models like BERT, we freeze the entire pretrained model and only train the classification head on some new data, usually labeled with some class that we want to predict.

Finally, in Chapter 12 we’ll introduce a fourth kind of finetuning, that is a crucial component of the largest language models: supervised finetuning or SFT. SFT is often used for instruction finetuning, in which we want a pretrained language model to learn to follow text instructions, for example to answer questions or follow a command to write something. Here we create a dataset of prompts and desired responses (for example questions and their answers, or commands and their fulfillments), and we train the language model using the normal cross-entropy loss to predict each token in the instruction prompt iteratively, essentially training it to produce the desired response from the command in the prompt. It’s called supervised because unlike in pretraining, where we just take any data and predict the words in it, we build the special finetuning dataset by hand, creating supervised responses to each command.

Often everything that happens after pretraining is lumped together as post-training; we’ll discuss the various parts of post-training in Chapter 12.

# 10.4 Evaluating Large Language Models

Perplexity As we first saw in Chapter 3, one way to evaluate language models is to measure how well they predict unseen text. Intuitively, good models are those that assign higher probabilities to unseen data (are less surprised when encountering the new words).

We instantiate this intuition by using perplexity to measure the quality of a language model. Recall from page 40 that the perplexity of a model $\theta$ on an unseen test set is the inverse probability that $\theta$ assigns to the test set, normalized by the test set length. For a test set of $n$ tokens $w _ { 1 : n }$ , the perplexity is

$$
\begin{array} { r l r } & { } & { \mathrm { P e r p l e x i t y } _ { \theta } ( w _ { 1 : n } ) = P _ { \theta } ( w _ { 1 : n } ) ^ { - \frac { 1 } { n } } } \\ & { } & { = \sqrt [ n ] { \frac { 1 } { P _ { \theta } ( w _ { 1 : n } ) } } } \end{array}
$$

To visualize how perplexity can be computed as a function of the probabilities the

LM computes for each new word, we can use the chain rule to expand the computation of probability of the test set:

$$
\mathrm { P e r p l e x i t y } _ { \theta } ( w _ { 1 : n } ) = \sqrt [ n ] { \prod _ { i = 1 } ^ { n } \frac { 1 } { P _ { \theta } ( w _ { i } | w _ { < i } ) } }
$$

Note that because of the inverse in Eq. 10.7, the higher the probability of the word sequence, the lower the perplexity. Thus the the lower the perplexity of a model on the data, the better the model. Minimizing perplexity is equivalent to maximizing the test set probability according to the language model.

One caveat: because perplexity depends on the length of a text, it is very sensitive to differences in the tokenization algorithm. That means that it’s hard to exactly compare perplexities produced by two language models if they have very different tokenizers. For this reason perplexity is best used when comparing language models that use the same tokenizer.

Other factors While the predictive accuracy of a language model, as measured by perplexity, is a very useful metric, we also care about different kinds of accuracy, for the downstream tasks we apply our language model to. For each task like machine translation, summarization, question answering, speech recognition, and dialogue, we can measure the accuracy at those tasks. Future chapters will introduce taskspecific metrics that allow us to evaluate how accuracy or correct language models are at these downstream tasks.

But when evaluating models we also care about factors besides any of these kinds of accuracy (Dodge et al., 2019; Ethayarajh and Jurafsky, 2020). For example, we often care about how a big a model is, and how long it takes to train or do inference. This can matter because we have constraints on time either for training or at inference. Or we may have constraints on memory, since the GPUs we run our models on have fixed memory sizes. Big models also use more energy, and we prefer models that use less energy, both to reduce the environmental impact of the model and to reduce the financial cost of building or deploying it. We can target our evaluation to these factors by measuring performance normalized to a giving compute or memory budget. We can also directly measure the energy usage of our model in kWh or in kilograms of $\mathrm { C O } _ { 2 }$ emitted (Strubell et al., 2019; Henderson et al., 2020; Liang et al., 2023).

Another feature that a language model evaluation can measure is fairness. We know that language models are biased, exhibiting gendered and racial stereotypes, or decreased performance for language from or about certain demographics groups. There are language model evaluation benchmarks that measure the strength of these biases, such as StereoSet (Nadeem et al., 2021), RealToxicityPrompts (Gehman et al., 2020), and BBQ (Parrish et al., 2022) among many others. We also want language models whose performance is equally fair to different groups. For example, we could chose an evaluation that is fair in a Rawlsian sense by maximizing the welfare of the worst-off group (Rawls, 2001; Hashimoto et al., 2018; Sagawa et al., 2020).

Finally, there are many kinds of leaderboards like Dynabench (Kiela et al., 2021) and general evaluation protocols like HELM (Liang et al., 2023); we will return to these in later chapters when we introduce evaluation metrics for specific tasks like question answering and information retrieval.

# 10.5 Dealing with Scale

Large language models are large. For example the Llama 3.1 405B Instruct model from Meta has 405 billion parameters $L { = } 1 2 6$ layers, a model dimensionality of $d { = } 1 6 { , } 3 8 4$ , $A { = } 1 2 8$ attention heads) and was trained on 15.6 terabytes of text tokens (Llama Team, 2024), using a vocabulary of 128K tokens. So there is a lot of research on understanding how LLMs scale, and especially how to implement them given limited resources. In the next few sections we discuss how to think about scale (the concept of scaling laws), and important techniques for getting language models to work efficiently, such as the KV cache and parameter-efficient fine tuning.

# 10.5.1 Scaling laws

The performance of large language models has shown to be mainly determined by 3 factors: model size (the number of parameters not counting embeddings), dataset size (the amount of training data), and the amount of compute used for training. That is, we can improve a model by adding parameters (adding more layers or having wider contexts or both), by training on more data, or by training for more iterations.

The relationships between these factors and performance are known as scaling laws. Roughly speaking, the performance of a large language model (the loss) scales as a power-law with each of these three properties of model training.

For example, Kaplan et al. (2020) found the following three relationships for loss $L$ as a function of the number of non-embedding parameters $N$ , the dataset size $D$ , and the compute budget $C$ , for models training with limited parameters, dataset, or compute budget, if in each case the other two properties are held constant:

$$
\begin{array} { l } { { { \cal { L } } ( N ) ~ = ~ \left( \displaystyle { \frac { N _ { c } } { N } } \right) ^ { \alpha _ { N } } } } \\ { { { \cal { L } } ( D ) ~ = ~ \left( \displaystyle { \frac { D _ { c } } { D } } \right) ^ { \alpha _ { D } } } } \\ { { { \cal { L } } ( C ) ~ = ~ \left( \displaystyle { \frac { C _ { c } } { C } } \right) ^ { \alpha _ { C } } } } \end{array}
$$

The number of (non-embedding) parameters $N$ can be roughly computed as follows (ignoring biases, and with $d$ as the input and output dimensionality of the model, $d _ { \mathrm { a t t n } }$ as the self-attention layer size, and $d _ { \mathrm { f f } }$ the size of the feedforward layer):

$$
\begin{array} { r c l } { { } } & { { } } & { { N \approx 2 d \ n _ { \mathrm { l a y e r } } ( 2 d _ { \mathrm { a t t n } } + d _ { \mathrm { f f } } ) } } \\ { { } } & { { } } & { { } } \\ { { } } & { { } } & { { \approx \ 1 2 n _ { \mathrm { l a y e r } } d ^ { 2 } } } \\ { { } } & { { } } & { { ( \mathrm { a s s u m i n g \ } d _ { \mathrm { a t t n } } = d _ { \mathrm { f f } } / 4 = d ) } } \end{array}
$$

Thus GPT-3, with $n = 9 6$ layers and dimensionality $d = 1 2 2 8 8$ , has $1 2 \times 9 6 \times$ $1 2 2 8 8 ^ { 2 } \approx 1 7 5$ billion parameters.

The values of $N _ { c }$ , $D _ { c }$ , $C _ { c }$ , $\alpha _ { N }$ , $\alpha _ { D }$ , and $\alpha _ { C }$ depend on the exact transformer architecture, tokenization, and vocabulary size, so rather than all the precise values, scaling laws focus on the relationship with loss.2

Scaling laws can be useful in deciding how to train a model to a particular performance, for example by looking at early in the training curve, or performance with

smaller amounts of data, to predict what the loss would be if we were to add more data or increase model size. Other aspects of scaling laws can also tell us how much data we need to add when scaling up a model.

# 10.5.2 KV Cache

We saw in Fig. 9.10 and in Eq. 9.33 (repeated below) how the attention vector can be very efficiently computed in parallel for training, via two matrix multiplications:

$$
\mathbf { A } \ = \ \mathrm { s o f t m a x } \left( { \frac { \mathbf { Q } \mathsf { K } ^ { \intercal } } { \sqrt { d _ { k } } } } \right) \mathbf { v }
$$

Unfortunately we can’t do quite the same efficient computation in inference as in training. That’s because at inference time, we iteratively generate the next tokens one at a time. For a new token that we have just generated, call it $\mathbf { x } _ { i }$ , we need to compute its query, key, and values by multiplying by $\mathbf { \Delta } \mathbf { W ^ { Q } } , \mathbf { \Phi } \mathbf { W ^ { K } }$ , and $\boldsymbol { \mathsf { W } } ^ { \boldsymbol { \mathsf { v } } }$ respectively. But it would be a waste of computation time to recompute the key and value vectors for all the prior tokens $\pmb { \mathrm { x } } _ { < i }$ ; at prior steps we already computed these key and value vectors! So instead of recomputing these, whenever we compute the key and value vectors we store them in memory in the KV cache, and then we can just grab them from the cache when we need them. Fig. 10.7 modifies Fig. 9.10 to show the computation that takes place for a single new token, showing which values we can take from the cache rather than recompute.

![## Image Analysis: ce9238533dbbe3b6407d9a30cf9dc82597224350ad040596dc31a0aaa63eb4e6.jpg

**Conceptual Understanding:**
This image conceptually represents an optimized single-head attention computation for a specific token (the 4th token, 'q4'). Its main purpose is to illustrate the function and benefit of a Key-Value (KV) cache in transformer-based models. It highlights how previously computed key and value vectors are stored and reused, significantly reducing computational overhead for subsequent tokens in a sequence. The image visually conveys the idea of reusability and efficiency in calculating attention scores.

**Content Interpretation:**
The image depicts the mathematical operations involved in computing the attention score for a single query vector ('q4') against a set of key-value pairs. It specifically highlights the components of the key and value matrices (k1, k2, k3 and v1, v2, v3, respectively) that are treated as cached elements, thereby distinguishing them from the current key ('k4') and value ('v4'). The process illustrates two matrix multiplications: first, the query with the transposed keys to generate attention scores, and second, these scores with the values to produce the final attention output. The use of different colors (black for cached, blue/pink for current, green for intermediate scores, gray-green for final output) visually reinforces the concept of a KV cache.

**Key Insights:**
The main takeaway from this image is the practical demonstration of the KV Cache mechanism in attention computations. It shows that when calculating attention for a new token (e.g., the 4th token represented by 'q4'), the key and value vectors from previous tokens (k1, k2, k3, and v1, v2, v3) can be pre-computed and stored in a cache. This caching prevents redundant calculations for these past tokens, making the attention computation more efficient for subsequent tokens in a sequence. Only the current token's key ('k4') and value ('v4') need to be newly considered or computed to complete the attention score calculation for 'q4'. The image provides clear textual evidence for this: 'k1', 'k2', 'k3' and 'v1', 'v2', 'v3' are visually separated and colored black, which the document context explicitly links to 'vectors that can be stored in the cache rather than recomputed'.

**Document Context:**
This image is highly relevant to the section '10.5.2 KV Cache' as stated in the document context. It visually explains the mechanism of how a Key-Value (KV) cache works in the context of an attention computation, specifically for the 4th token. By highlighting which parts of the Kᵀ and V matrices (k1, k2, k3, v1, v2, v3 in black) are stored in the cache and reused, it directly supports the discussion on optimizing attention computations by avoiding recomputation of past key and value vectors. The image, being 'extracted from Fig. 9.10', suggests it's a specific application or detail derived from a broader explanation of attention mechanisms.

**Summary:**
The image illustrates the attention computation for the 4th token (q4) in a sequence, explicitly showing which components can be stored in a Key-Value (KV) cache. The process begins with the query vector for the 4th token, 'q4', which has dimensions 1 x dₖ. This 'q4' vector is then multiplied ('x') by the transposed key matrix, 'Kᵀ', which has dimensions dₖ x N. The 'Kᵀ' matrix is composed of individual key vectors: 'k1', 'k2', and 'k3' (highlighted in black), representing previously computed keys that are stored in the cache. The key vector 'k4' (highlighted in blue) represents the key for the current, 4th token. The result of this multiplication is the 'QKᵀ' vector, which represents the unnormalized attention scores. This vector has dimensions 1 x N and contains the dot products: 'q4·k1', 'q4·k2', 'q4·k3', and 'q4·k4' (all highlighted in green). Following this, the 'QKᵀ' vector is multiplied ('x') by the value matrix, 'V', which has dimensions N x dᵥ. The 'V' matrix is composed of individual value vectors: 'v1', 'v2', and 'v3' (highlighted in black), representing previously computed values stored in the cache. The value vector 'v4' (highlighted in pink) represents the value for the current, 4th token. The final output of this entire computation is the attention output 'A', specifically 'a4' (highlighted in gray-green), which has dimensions 1 x dᵥ. The black-highlighted 'k1, k2, k3' and 'v1, v2, v3' explicitly demonstrate the components that are retrieved from the KV cache, avoiding recomputation.](images/ce9238533dbbe3b6407d9a30cf9dc82597224350ad040596dc31a0aaa63eb4e6.jpg)
Figure 10.7 Parts of the attention computation (extracted from Fig. 9.10) showing, in black, the vectors that can be stored in the cache rather than recomputed when computing the attention score for the 4th token.

# 10.5.3 Parameter Efficient Fine Tuning

As we mentioned above, it’s very common to take a language model and give it more information about a new domain by finetuning it (continuing to train it to predict upcoming words) on some additional data.

Fine-tuning can be very difficult with very large language models, because there are enormous numbers of parameters to train; each pass of batch gradient descent has to backpropagate through many many huge layers. This makes finetuning huge language models extremely expensive in processing power, in memory, and in time. For this reason, there are alternative methods that allow a model to be finetuned without changing all the parameters. Such methods are called parameter-efficient fine tuning or sometimes PEFT, because we efficiently select a subset of parameters to update when finetuning. For example we freeze some of the parameters (don’t change them), and only update some particular subset of parameters.

# LoRA

Here we describe one such model, called LoRA, for Low-Rank Adaptation. The intuition of LoRA is that transformers have many dense layers which perform matrix multiplication (for example the $\boldsymbol { \mathsf { W } } ^ { \mathbf { Q } }$ , $\boldsymbol { \mathsf { W } } ^ { \mathsf { K } }$ , $\boldsymbol { \mathsf { W } } ^ { \boldsymbol { \mathsf { v } } }$ , $\boldsymbol { \mathsf { W } } ^ { 0 }$ layers in the attention computation). Instead of updating these layers during finetuning, with LoRA we freeze these layers and instead update a low-rank approximation that has fewer parameters.

Consider a matrix $\boldsymbol { \mathsf { W } }$ of dimensionality $[ N \times d ]$ that needs to be updated during finetuning via gradient descent. Normally this matrix would get updates $\Delta \pmb { \mathsf { W } }$ of dimensionality $[ N \times d ]$ , for updating the $N \times d$ parameters after gradient descent. In LoRA, we freeze $\boldsymbol { \mathsf { W } }$ and update instead a low-rank decomposition of $\boldsymbol { \mathsf { W } }$ . We create two matrices $\pmb { \mathsf { A } }$ and B, where $\pmb { \mathsf { A } }$ has size $[ N \times r ]$ and $\mathbf { B }$ has size $[ r \times d ]$ , and we choose $r$ to be quite small, $r < < \operatorname* { m i n } ( d , N )$ . During finetuning we update $\pmb { \mathsf { A } }$ and $\mathbf { B }$ instead of $\boldsymbol { \mathsf { W } }$ . That is, we replace $\boldsymbol { \mathsf { W } } + \Delta \boldsymbol { \mathsf { W } }$ with $\boldsymbol { \mathsf { W } } + \boldsymbol { \mathsf { B } } \boldsymbol { \mathsf { A } }$ . Fig. 10.8 shows the intuition. For replacing the forward pass $\mathbf { h } = \mathbf { x } \mathbf { W }$ , the new forward pass is instead:

$$
\begin{array} { r } { \mathbf { h } = \mathbf { x } \mathbf { W } + \mathbf { x } \mathbf { A } \mathbf { B } } \end{array}
$$

![## Image Analysis: f72d5f379b201b62c6a704f0ccae504521ffeb34ae92952c738e7927e87b89f6.jpg

**Conceptual Understanding:**
This image represents the computational flow of the Low-Rank Adaptation (LoRA) method, specifically illustrating how an input is processed to produce an output in a LoRA-tuned model. The main purpose is to convey the 'intuition' of LoRA by showing that the modification to the model's output is achieved by adding a low-rank update (formed by multiplying two smaller matrices A and B) to the contribution of the original, frozen pretrained weights (W). The key concept communicated is the efficient fine-tuning of large models by introducing a small, trainable, low-rank 'adapter' while keeping the majority of the pretrained weights fixed.

**Content Interpretation:**
The image shows a data flow diagram illustrating the forward pass computation for the LoRA (Low-Rank Adaptation) technique. It depicts an input vector 'X' with dimensions '1' by 'd'. This input is processed along two parallel paths:

1.  **Original Weights Path:** 'X' is processed by a green rectangular matrix labeled "Pretrained Weights W" with dimensions 'd' by 'k'.
2.  **LoRA Adaptation Path:** 'X' is also processed by a red vertical matrix labeled 'A' with dimensions 'd' by 'r'. The output of this is then multiplied (indicated by 'x') by a red horizontal matrix labeled 'B' with dimensions 'r' by 'k'. The common dimension 'r' is explicitly shown.

The outputs from both paths are then combined using an addition operation, represented by a '+' symbol. The final result is an output vector 'h' with dimensions '1' by 'k'.

This diagram demonstrates how the contribution from the original 'Pretrained Weights W' and the low-rank adaptation matrices 'A' and 'B' are integrated to produce the final output.

**Key Insights:**
The main takeaway from this image is the core mechanism of LoRA: it modifies a model's output by adding the effect of a small, low-rank weight update (represented by the product of matrices 'A' and 'B') to the effect of the original, frozen 'Pretrained Weights W'. This is mathematically equivalent to computing X * (W + A * B).

Key insights include:

*   **Freezing Pretrained Weights:** The explicit label "Pretrained Weights W" (dimensions 'd' by 'k') indicates that these substantial weights remain fixed, which is crucial for efficiency and preventing catastrophic forgetting during fine-tuning. 
*   **Low-Rank Decomposition:** The use of matrices 'A' (dimensions 'd' by 'r') and 'B' (dimensions 'r' by 'k'), where 'r' is typically much smaller than 'd' or 'k', highlights the low-rank nature of the adaptation. The 'x r B' label explicitly shows the multiplication of 'A' and 'B' and their common rank 'r'. This significantly reduces the number of trainable parameters from `d*k` to `d*r + r*k`.
*   **Additive Combination:** The '+' symbol clearly illustrates that the contribution from the original weights path and the LoRA adaptation path are summed together to form the final output 'h' (dimensions '1' by 'k'). This additive approach simplifies the integration of the adapted changes.
*   **Input and Output:** The input vector 'X' (dimensions '1' by 'd') and output vector 'h' (dimensions '1' by 'k') frame the entire operation, demonstrating how the LoRA mechanism processes input to produce a modified output.

**Document Context:**
This image directly supports the "LoRA" section of the document, specifically explaining the "intuition" of LoRA as described in the accompanying text. The document states: "Figure 10.8 The intuition of LoRA. We freeze $\boldsymbol { \mathsf { W } }$ to its pretrained values, and instead finetune by training a pair of matrices A and B, updating those instead of $\boldsymbol { \mathsf { W } }$ , and just sum $\boldsymbol { \mathsf { W } }$ and the updated AB." The diagram visually translates this description into a computational flow, showing the separate paths for 'W' and 'A*B' and their final summation to produce the output 'h' from input 'X'. It is a key visual aid for understanding the foundational mechanism of LoRA within the broader narrative of efficient model fine-tuning.

**Summary:**
This diagram, titled "The intuition of LoRA," illustrates how the Low-Rank Adaptation (LoRA) method modifies a model's behavior without directly updating its large "Pretrained Weights W."At the core, the process takes an input vector, denoted as **X**, which has a dimension of `1` by `d`. This input `X` then follows two parallel computational paths before its contributions are combined:

1.  **Path of Pretrained Weights:** The input vector `X` is first processed by the `Pretrained Weights W` matrix. This `W` matrix has dimensions `d` by `k` and represents the original, frozen parameters of a model layer. The interaction here is an implicit matrix multiplication of `X` with `W`.

2.  **Path of LoRA Adaptation:** Simultaneously, the same input vector `X` is also processed by a new, smaller set of trainable matrices, `A` and `B`.
    *   First, `X` interacts with matrix `A`, which has dimensions `d` by `r`.
    *   The output of this operation is then multiplied by matrix `B`, which has dimensions `r` by `k`. The multiplication is explicitly indicated by an "x" symbol, and "r" confirms the shared dimension between `A` and `B` (i.e., `A` is multiplied by `B`). This entire sub-path effectively computes `X` multiplied by the product of `A` and `B` (`X * (A * B)`).

Finally, the outputs from both parallel paths—the contribution from `X * W` and the contribution from `X * (A * B)`—are combined. This combination is performed via an addition operation, clearly marked by a "+" symbol in the diagram. The sum of these two contributions yields the final output vector, denoted as **h**, which has a dimension of `1` by `k`.

The significance of this diagram is its clear visual representation of LoRA's efficiency. Instead of fine-tuning the enormous `W` matrix directly, only the much smaller matrices `A` and `B` are updated. The product `(A * B)` acts as a low-rank approximation of the change `ΔW` that would normally be applied to `W`. By having `r` (the shared dimension of `A` and `B`) be significantly smaller than `d` or `k`, the number of trainable parameters (`d*r + r*k`) is drastically reduced compared to training `d*k` parameters in a full `ΔW`, making fine-tuning more resource-efficient while maintaining performance. This additive nature means the effect of the new, small weights is simply added to the effect of the large, frozen pretrained weights.](images/f72d5f379b201b62c6a704f0ccae504521ffeb34ae92952c738e7927e87b89f6.jpg)
Figure 10.8 The intuition of LoRA. We freeze $\boldsymbol { \mathsf { W } }$ to its pretrained values, and instead finetune by training a pair of matrices A and B, updating those instead of $\boldsymbol { \mathsf { W } }$ , and just sum $\boldsymbol { \mathsf { W } }$ and the updated AB.

LoRA has a number of advantages. It dramatically reduces hardware requirements, since gradients don’t have to be calculated for most parameters. The weight updates can be simply added in to the pretrained weights, since BA is the same size as $\boldsymbol { \mathsf { W } }$ ). That means it doesn’t add any time during inference. And it also means it’s possible to build LoRA modules for different domains and just swap them in and out by adding them in or subtracting them from $\boldsymbol { \mathsf { W } }$ .

In its original version LoRA was applied just to the matrices in the attention computation (the $\mathsf { w } ^ { \mathsf { q } } , \mathsf { w } ^ { \mathsf { K } } , \mathsf { w } ^ { \mathsf { v } }$ , and $\boldsymbol { \mathsf { W } } ^ { 0 }$ layers). Many variants of LoRA exist.

# 10.6 Potential Harms from Language Models

Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when pretrained language models are used for any downstream task, particularly those involving text generation, whether question answering, machine translation, or in assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020).

hallucination

For example, language models are prone to saying things that are false, a problem called hallucination. Language models are trained to generate text that is predictable and coherent, but the training algorithms we have seen so far don’t have any way to enforce that the text that is generated is correct or true. This causes enormous problems for any application where the facts matter! We’ll return to this issue in Chapter 14 where we introduce proposed mitigation methods like retrieval augmented generation.

A second source of harm is that language models can generate toxic language. Gehman et al. (2020) show that even completely non-toxic prompts can lead large language models to output hate speech and abuse their users. Language models also generate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng et al., 2019) about many demographic groups.

One source of biases is the training data. Gehman et al. (2020) shows that large language model training datasets include toxic text scraped from banned sites. There are other biases than toxicity: the training data is disproportionately generated by authors from the US and from developed countries. Such biased population samples likely skew the resulting generation toward the perspectives or topics of this group alone. Furthermore, language models can amplify demographic and other biases in training data, just as we saw for embedding models in Chapter 6.

Datasets can be another source of harms. We already saw in Section 10.3.2 that using pretraining corpora scraped from the web can lead to harms related to copyright and data consent. We also mentioned that pretraining data can tend to have private information like phone numbers and addresses. This is problematic because large language models can leak information from their training data. That is, an adversary can extract training-data text from a language model such as a person’s name, phone number, and address (Henderson et al. 2017, Carlini et al. 2021). This becomes even more problematic when large language models are trained on extremely sensitive private datasets such as electronic health records.

Language models can also be used by malicious actors for generating text for misinformation, phishing, or other socially harmful activities (Brown et al., 2020). McGuffie and Newhouse (2020) show how large language models generate text that emulates online extremists, with the risk of amplifying extremist movements and their attempt to radicalize and recruit.

Finding ways to mitigate all these harms is an important current research area in NLP. At the very least, carefully analyzing the data used to pretrain large language models is important as a way of understanding issues of toxicity, bias, privacy, and fair use, making it extremely important that language models include datasheets (page 16) or model cards (page 74) giving full replicable information on the corpora used to train them. Open-source models can specify their exact training data. Requirements that models are transparent in such ways is also in the process of being incorporated into the regulations of various national governments.

# 10.7 Summary

This chapter has introduced the large language model, and how it can be built out of the transformer. Here’s a summary of the main points that we covered:

• Many NLP tasks—such as question answering, summarization, sentiment, and machine translation—can be cast as tasks of word prediction and hence addressed with Large language models.   
• Large language models are generally pretrained on large datasets of 100s of billions of words generally scraped from the web.   
• These datasets need to be filtered for quality and balanced for domains by upsampling and downsampling. Addressing some problems with pretraining data, like toxicity, are open research problems.   
• The choice of which word to generate in large language models is generally done by using a sampling algorithm.   
• Language models are evaluated by perplexity but there are also evaluations of accuracy downstream tasks, and ways to measure other factors like fairness and energy use.   
• There are various computational tricks for making large language models more efficient, such as the KV cache and parameter-efficient finetuning.   
• Because of their ability to be used in so many ways, language models also have the potential to cause harms. Some harms include hallucinations, bias, stereotypes, misinformation and propaganda, and violations of privacy and copyright.

# Bibliographical and Historical Notes

As we discussed in Chapter 3, the earliest language models were the n-gram language models developed (roughly simultaneously and independently) by Fred Jelinek and colleagues at the IBM Thomas J. Watson Research Center, and James Baker at CMU. It was the Jelinek and the IBM team who first coined the term language model to mean a model of the way any kind of linguistic property (grammar, semantics, discourse, speaker characteristics), influenced word sequence probabilities (Jelinek et al., 1975). They contrasted the language model with the acoustic model which captured acoustic/phonetic characteristics of phone sequences.

N-gram language models were very widely used over the next 30 years and more, across a wide variety of NLP tasks like speech recognition and machine translations, often as one of multiple components of the model. The contexts for these n-gram models grew longer, with 5-gram models used quite commonly by very efficient LM toolkits (Stolcke, 2002; Heafield, 2011).

The roots of the neural language model lie in multiple places. One was the application in the 1990s, again in Jelinek’s group at IBM Research, of discriminative classifiers to language models. Roni Rosenfeld in his dissertation (Rosenfeld, 1992) first applied logistic regression (under the name maximum entropy or maxent models) to language modeling in that IBM lab, and published a more fully formed version in Rosenfeld (1996). His model integrated various sorts of information in a logistic regression predictor, including n-gram information along with other features from the context, including distant n-grams and pairs of associated words called trigger pairs. Rosenfeld’s model prefigured modern language models by being a statistical word predictor trained in a self-supervised manner simply by learning to predict upcoming words in a corpus.

Another was the first use of pretrained embeddings to model word meaning in the LSA/LSI models (Deerwester et al., 1988). Recall from the history section of Chapter 6 that in LSA (latent semantic analysis) a term-document matrix was trained on a corpus and then singular value decomposition was applied and the first 300 dimensions were used as a vector embedding to represent words. Landauer et al. (1997) first used the word “embedding”. In addition to their development of the idea of pretraining and of embeddings, the LSA community also developed ways to combine LSA embeddings with n-grams in an integrated language model (Bellegarda, 1997; Coccaro and Jurafsky, 1998).

In a very influential series of papers developing the idea of neural language models, (Bengio et al. 2000; Bengio et al. 2003; Bengio et al. 2006), Yoshua Bengio and colleagues drew on the central ideas of both these lines of self-supervised language modeling work, (the discriminatively trained word predictor, and the pretrained embeddings). Like the maxent models of Rosenfeld, Bengio’s model used the next word in running text as its supervision signal. Like the LSA models, Bengio’s model learned an embedding, but unlike the LSA models did it as part of the process of language modeling. The Bengio et al. (2003) model was a neural language model: a neural network that learned to predict the next word from prior words, and did so via learning embeddings as part of the prediction process.

The neural language model was extended in various ways over the years, perhaps most importantly in the form of the RNN language model of Mikolov et al. (2010) and Mikolov et al. (2011). The RNN language model was perhaps the first neural model that was accurate enough to surpass the performance of a traditional 5-gram language model.

Soon afterwards, Mikolov et al. (2013a) and Mikolov et al. (2013b) proposed to simplify the hidden layer of these neural net language models to create pretrained word2vec word embeddings.

The static embedding models like LSA and word2vec instantiated a particular model of pretraining: a representation was trained on a pretraining dataset, and then the representations could be used in further tasks. ‘Dai and Le (2015) and (Peters et al., 2018) reframed this idea by proposing models that were pretrained using a language model objective, and then the identical model could be either frozen and directly applied for language modeling or further finetuned still using a language model objective. For example ELMo used a biLSTM self-supervised on a large pretrained dataset using a language model objective, then finetuned on a domainspecific dataset, and then froze the weights and added task-specific heads. The ELMo work was particularly influential and its appearance was perhaps the moment when it became clear to the community that language models could be used as a general solution for NLP problems.

Transformers were first applied as encoder-decoders (Vaswani et al., 2017) and then to masked language modeling (Devlin et al., 2019) (as we’ll see in Chapter 13 and Chapter 11). Radford et al. (2019) then showed that the transformer-based autoregressive language model GPT2 could perform zero-shot on many NLP tasks like summarization and question answering.

The technology used for transformer-based language models can also be applied to other domains and tasks, like vision, speech, and genetics. the term foundation model is sometimes used as a more general term for this use of large language model technology across domains and areas, when the elements we are computing over are not necessarily words. Bommasani et al. (2021) is a broad survey that sketches the opportunities and risks of foundation models, with special attention to large language models.

# CHAPTER 11 Masked Language Models

Larvatus prodeo [Masked, I go forward] Descartes

BERT masked language modeling

In the previous two chapters we introduced the transformer and saw how to pretrain a transformer language model as a causal or left-to-right language model. In this chapter we’ll introduce a second paradigm for pretrained language models, the bidirectional transformer encoder, and the most widely-used version, the BERT model (Devlin et al., 2019). This model is trained via masked language modeling, where instead of predicting the following word, we mask a word in the middle and ask the model to guess the word given the words on both sides. This method thus allows the model to see both the right and left context.

# finetuning

# transfer learning

We also introduced finetuning in the prior chapter. Here we describe a new kind of finetuning, in which we take the transformer network learned by these pretrained models, add a neural net classifier after the top layer of the network, and train it on some additional labeled data to perform some downstream task like named entity tagging or natural language inference. As before, the intuition is that the pretraining phase learns a language model that instantiates rich representations of word meaning, that thus enables the model to more easily learn (‘be finetuned to’) the requirements of a downstream language understanding task. This aspect of the pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task.

The second idea that we introduce in this chapter is the idea of contextual embeddings: representations for words in context. The methods of Chapter 6 like word2vec or GloVe learned a single vector embedding for each unique word $w$ in the vocabulary. By contrast, with contextual embeddings, such as those learned by masked language models like BERT, each word $w$ will be represented by a different vector each time it appears in a different context. While the causal language models of Chapter 9 also use contextual embeddings, the embeddings created by masked language models seem to function particularly well as representations.

# 11.1 Bidirectional Transformer Encoders

Let’s begin by introducing the bidirectional transformer encoder that underlies models like BERT and its descendants like RoBERTa (Liu et al., 2019) or SpanBERT (Joshi et al., 2020). In Chapter 9 we introduced causal (left-to-right) transformers and in Chapter 10 saw how they can serve as the basis for language models that can be applied to autoregressive contextual generation problems like question answering or summarization. But this left-to-right nature of these models is also a limitation, because there are tasks for which it would be useful, when processing a token, to be able to peak at future tokens. This is especially true for sequence labeling tasks in which we want to tag each token with a label, such as the named entity tagging task we’ll introduce in Section 11.5, or tasks like part-of-speech tagging or parsing that come up in later chapters.

The bidirectional encoders that we introduce here are a different kind of beast than causal models. The causal models of Chapter 9 are generative models, designed to easily generate the next token in a sequence. But the focus of bidirectional encoders is instead on computing contextualized representations of the input tokens. Bidirectional encoders use self-attention to map sequences of input embeddings $\left( \mathbf { x } _ { 1 } , . . . , \mathbf { x } _ { n } \right)$ to sequences of output embeddings of the same length $\left( \mathbf { h } _ { 1 } , . . . , \mathbf { h } _ { n } \right)$ , where the output vectors have been contextualized using information from the entire input sequence. These output embeddings are contextualized representations of each input token that are useful across a range of applications where we need to do a classification or a decision based on the token in context.

Remember that we said the models of Chapter 9 are sometimes called decoderonly, because they correspond to the decoder part of the encoder-decoder model we will introduce in Chapter 13. By contrast, the masked language models of this chapter are sometimes called encoder-only, because they produce an encoding for each input token but generally aren’t used to produce running text by decoding/sampling. That’s an important point: masked language models are not used for generation. They are generally instead used for interpretative tasks.

# 11.1.1 The architecture for bidirectional masked models

Let’s first discuss the overall architecture. Bidirectional transformer-based language models differ in two ways from the causal transformers in the previous chapters. The first is that the attention function isn’t causal; the attention for a token $i$ can look at following tokens $i + 1$ and so on. The second is that the training is slightly different since we are predicting something in the middle of our text rather than at the end. We’ll discuss the first here and the second in the following section.

Fig. 11.1a, reproduced here from Chapter 9, shows the information flow in the left-to-right approach of Chapter 9. The attention computation at each token is based on the preceding (and current) input tokens, ignoring potentially useful information located to the right of the token under consideration. Bidirectional encoders overcome this limitation by allowing the attention mechanism to range over the entire input, as shown in Fig. 11.1b.

![## Image Analysis: f033160cad636c872c6b1d8b512c7bf972726c47341eb03b75b669fadaae1c82.jpg

**Conceptual Understanding:**
The image conceptually represents two fundamental types of self-attention layers used in deep learning models, particularly Transformers, for processing sequential data. The main purpose is to visually compare and contrast the information flow in a 'causal' (unidirectional) self-attention mechanism with that of a 'bidirectional' self-attention mechanism. It aims to communicate the key idea that causal attention restricts context to past and current tokens, while bidirectional attention utilizes the full sequence context (past, current, and future) when computing an output representation for any given token.

**Content Interpretation:**
The image illustrates the fundamental difference in information flow between two types of self-attention mechanisms in transformer architectures: causal (unidirectional) self-attention and bidirectional self-attention. The processes shown are the computation of contextualized output representations (a_n) from input embeddings/tokens (x_n) through an attention mechanism. The significance lies in how the 'context' for each output token is gathered: either from only past and current inputs (causal) or from all past, current, and future inputs (bidirectional). The highlighted token 'x3' and its corresponding output 'a3' demonstrate this distinction clearly, showing which input tokens contribute to the calculation of 'a3' under each attention type.

**Key Insights:**
The main takeaways from this image are: 1.  **Causal Self-Attention (Diagram a):** Information flow is restricted. When computing the output for a specific token (e.g., 'a3'), the attention mechanism only considers the input token at that position ('x3') and all preceding input tokens ('x1', 'x2'). This prevents the model from 'seeing' future information, which is essential for autoregressive tasks like text generation. 2.  **Bidirectional Self-Attention (Diagram b):** Information flow is unrestricted. When computing the output for a specific token (e.g., 'a3'), the attention mechanism considers all input tokens in the entire sequence ('x1', 'x2', 'x3', 'x4', 'x5'). This allows for a richer contextual understanding by leveraging both past and future information, which is beneficial for tasks like natural language understanding or text classification. 3.  The diagrams visually demonstrate the 'masking' concept in causal attention by the absence of connections from future tokens to the attention block, in contrast to the comprehensive connections in bidirectional attention. The specific text 'x1', 'x2', 'x3', 'x4', 'x5' and 'a1', 'a2', 'a3', 'a4', 'a5' along with the 'attention' blocks and their connecting arrows clearly illustrate these different information dependencies.

**Document Context:**
This image is placed in Section 11.1.1, titled 'The architecture for bidirectional masked models'. It serves as a foundational visual aid to explain the core architectural distinction between causal (often referred to as 'masked' in generation tasks) and bidirectional self-attention layers. The accompanying text explicitly links 'Figure 11.1 (a)' to the 'causal transformer from Chapter 9' and 'Figure 11.1 (b)' to the 'bidirectional attention model,' emphasizing the 'attention computation at token 3' as illustrated. This image is crucial for understanding how transformers process sequential data, particularly the implications for tasks like language generation (causal) versus language understanding (bidirectional).

**Summary:**
The image presents two distinct diagrams illustrating self-attention layers: a) a causal self-attention layer and b) a bidirectional self-attention layer. Both diagrams show a sequence of five input tokens, labeled from x1 to x5, feeding into a layer of attention mechanisms, which then produce a sequence of five output tokens, labeled from a1 to a5. Each 'attention' block computes one of the output tokens (a1 to a5). The blue shaded area encompasses the attention computation layer. In both diagrams, token 'x3' is highlighted as a point of interest, and the corresponding output 'a3' and its associated 'attention' block are highlighted in green and red, respectively, to draw attention to their specific computation. The crucial difference lies in the information flow, represented by the arrows connecting input tokens (x_n) to the attention blocks. In the causal self-attention layer (a), each attention block can only access information from the current token and preceding tokens. For example, the attention block computing 'a3' only receives input from 'x1', 'x2', and 'x3'. It cannot see 'x4' or 'x5'. In contrast, in the bidirectional self-attention layer (b), each attention block can access information from all input tokens in the sequence, both preceding and succeeding the current token. For instance, the attention block computing 'a3' receives input from 'x1', 'x2', 'x3', 'x4', and 'x5'. This visual comparison clearly delineates how causal attention is masked to prevent looking into future tokens, while bidirectional attention utilizes the full context.](images/f033160cad636c872c6b1d8b512c7bf972726c47341eb03b75b669fadaae1c82.jpg)
Figure 11.1 (a) The causal transformer from Chapter 9, highlighting the attention computation at token 3. The attention value at each token is computed using only information seen earlier in the context. (b) Information flow in a bidirectional attention model. In processing each token, the model attends to all inputs, both before and after the current one. So attention for token 3 can draw on information from following tokens.

The implementation is very simple! We simply remove the attention masking step that we introduced in Eq. 9.33. Recall from Chapter 9 that we had to mask the $\mathbf { Q } \mathbf { K } ^ { \intercal }$ matrix for causal transformers so that attention couldn’t look at future tokens (repeated from Eq. 9.33 for a single attention head):

$$
{ \bf h e a d \Psi } = \mathrm { \ s o f t m a x } \left( \mathrm { m a s k } \left( { \frac { \bf Q K ^ { \intercal } } { \sqrt { d _ { k } } } } \right) \right) { \bf v }
$$

![## Image Analysis: 0d08446badd2929d7ed26d0bd2b087eab1a43cbebed44ee7c7c7cf8e67640031.jpg

**Conceptual Understanding:**
This image conceptually represents the attention score matrices (specifically, the QKᵀ dot product before softmax) within a transformer's self-attention mechanism, contrasting how information flow is managed in different model architectures. Its main purpose is to illustrate the difference between masked and unmasked attention patterns, which are crucial for defining whether a model can process information bidirectionally or only causally/unidirectionally.

**Key Ideas Communicated:**
1.  **Attention Scores:** The cells 'qX⋅kY' represent the raw similarity scores between query vectors and key vectors.
2.  **Masking:** How masking is applied to control the dependencies in an attention mechanism, particularly to prevent attending to 'future' information.
3.  **Unidirectional (Causal) vs. Bidirectional Context:** The two matrices distinctly show how attention can be restricted (unidirectional, in (a)) or fully enabled (bidirectional, in (b)) across a sequence.
4.  **Implementation of Masking:** The use of '-∞' to effectively 'zero out' attention weights after a softmax operation is clearly depicted.

**Content Interpretation:**
The image illustrates two different attention mechanisms used in transformer models, specifically highlighting the distinction between masked and unmasked self-attention score matrices. These matrices represent the 'QKᵀ' operation in the attention mechanism, where 'q' denotes query vectors and 'k' denotes key vectors, and 'qX⋅kY' is the dot product (similarity score) between query X and key Y.

**Matrix (a)** depicts a *masked* attention matrix. The presence of '-∞' values in the upper triangle (where the key index 'Y' is greater than the query index 'X') signifies that attention from a query 'qX' to a future key 'kY' is blocked. This type of masking is characteristic of unidirectional or causal models, often used in tasks like language generation where a token should only attend to previous tokens in the sequence.

**Matrix (b)** illustrates an *unmasked* attention matrix. All cells contain 'qX⋅kY' values, meaning that each query 'qX' can attend to all keys 'kY', including previous, current, and future keys. This represents a bidirectional attention mechanism, which is common in models like BERT (Bidirectional Encoder Representations from Transformers) for tasks requiring full context understanding, such as language comprehension or translation.

**Significance of Elements:**
*   **N:** Represents the dimension or sequence length of the queries and keys, indicating an N x N matrix.
*   **qX⋅kY:** The dot product between query X and key Y, which serves as a raw attention score before softmax normalization. A higher value indicates greater similarity and potential attention.
*   **-∞:** Represents masked positions. As stated in the document context, these values are set to negative infinity so that when a softmax function is applied, they result in attention weights of zero. This effectively prevents the query from attending to the corresponding key.

The light blue shading likely indicates the values that are 'active' or considered in the attention calculation, while the unshaded white cells in (a) are the masked, inactive values.

**Key Insights:**
The image provides crucial insights into the implementation of attention mechanisms in transformer models, particularly regarding the control of information flow:

1.  **Masked vs. Unmasked Attention:** The primary takeaway is the clear distinction between masked (a) and unmasked (b) attention matrices. Masking is a technique used to restrict the information a token can attend to.
2.  **Unidirectional/Causal Attention (Matrix a):** Matrix (a) demonstrates how future context is blocked by setting upper-triangle values to '-∞'. This ensures that each query qX can only attend to keys kY where Y ≤ X (i.e., previous or current positions), which is essential for tasks where future information should not be used, such as language generation.
3.  **Bidirectional Attention (Matrix b):** Matrix (b) shows full attention where each query qX can attend to all keys kY (previous, current, and future). This bidirectional context is vital for tasks requiring a comprehensive understanding of a sequence, as seen in models like BERT.
4.  **Mechanism of Masking:** The use of '-∞' is a specific and common technique to implement masking. When these values are passed through a softmax function, they are effectively transformed into zero attention weights, thereby 'zeroing out' the influence of the masked positions.
5.  **Role of q⋅k:** The 'qX⋅kY' terms represent the fundamental similarity scores that form the basis of attention. These scores determine how much attention a query will pay to a particular key.

**Textual Evidence for Insights:**
*   **Matrix (a) structure with '-∞' in the upper triangle:** Directly shows the masking for causal attention, supported by the descriptive text 'upper-triangle portion of the comparisons matrix zeroed out (set to -∞, which the softmax will turn to zero)'.
*   **Matrix (b) structure with all 'qX⋅kY' values:** Directly shows the unmasked, full attention mechanism.
*   **Labels 'N'**: Indicate the dimensionality, reinforcing the matrix structure.
*   **'qX⋅kY' in all relevant cells:** Confirms the calculation of query-key similarity scores.

**Document Context:**
This image directly supports Section 11.1.1, titled 'The architecture for bidirectional masked models,' by visually demonstrating the core difference in attention mechanisms. It contrasts a masked self-attention matrix (a), which is typically used in unidirectional models or for causal language modeling, with an unmasked self-attention matrix (b), which is fundamental to bidirectional models. The accompanying text after the image explicitly states that matrix (a) shows the 'QKᵀ matrix showing the qᵢ ⋅ kⱼ values, with the upper-triangle portion of the comparisons matrix zeroed out (set to -∞, which the softmax will turn to zero)'. This confirms that the image is illustrating how masking is implemented to control the flow of information in attention mechanisms, which is crucial for understanding how models achieve bidirectionality or unidirectionality.

**Summary:**
The image displays two N x N matrices, labeled (a) and (b), which represent attention weight computations. Both matrices are 4x4 grids, with 'N' indicated on the left side and bottom of each, suggesting they are N-dimensional. The cells within these matrices contain expressions of the form 'qX⋅kY', where 'X' refers to the query index (row) and 'Y' refers to the key index (column). The dot (⋅) signifies a dot product or a similarity score between a query vector qX and a key vector kY.

Matrix (a) is a masked matrix. It shows all 'qX⋅kY' values for which X is greater than or equal to Y (i.e., the lower triangle and diagonal, including q1⋅k1, q2⋅k1, q2⋅k2, etc.). The cells in the upper-triangle portion, where X is less than Y (e.g., q1⋅k2, q1⋅k3, q1⋅k4, q2⋅k3, q2⋅k4, q3⋅k4), are explicitly set to '-∞' (negative infinity). The cells with actual 'qX⋅kY' values are shaded light blue, while the '-∞' cells are unshaded white.

Matrix (b) is an unmasked matrix. All cells contain 'qX⋅kY' values without any masking. Specifically, it shows q1⋅k1, q1⋅k2, q1⋅k3, q1⋅k4 in the first row; q2⋅k1, q2⋅k2, q2⋅k3, q2⋅k4 in the second row; q3⋅k1, q3⋅k2, q3⋅k3, q3⋅k4 in the third row; and q4⋅k1, q4⋅k2, q4⋅k3, q4⋅k4 in the fourth row. All cells in matrix (b) are uniformly shaded light blue.

Comparing the two, matrix (a) selectively exposes only the lower-triangular part of the 'q⋅k' comparisons, effectively masking future information, while matrix (b) exposes all possible 'q⋅k' comparisons, representing full bidirectional attention. The context implies that the '-∞' values in matrix (a) will be transformed to zero by a subsequent softmax operation, thus preventing attention to those masked positions.](images/0d08446badd2929d7ed26d0bd2b087eab1a43cbebed44ee7c7c7cf8e67640031.jpg)
Figure 11.2 The $N \times N$ QK| matrix showing the $q _ { i } \cdot k _ { j }$ values, with the upper-triangle portion of the comparisons matrix zeroed out (set to $- \infty$ , which the softmax will turn to zero).

Fig. 11.2 shows the masked version of QK| and the unmasked version. For bidirectional attention, we use the unmasked version of Fig. 11.2b. Thus the attention computation for bidirectional attention is exactly the same as Eq. 11.1 but with the mask removed:

$$
\mathbf { { \boldsymbol { \mathsf { h e a d } } } } = \mathbf { { \boldsymbol { \mathsf { s o f t m a x } } } } \left( { \frac { \mathbf { { \boldsymbol { \mathsf { Q } } } \mathbf { { \boldsymbol { K } } } ^ { \intercal } } } { \sqrt { d _ { k } } } } \right) \mathbf { { \boldsymbol { \mathsf { v } } } }
$$

Otherwise, the attention computation is identical to what we saw in Chapter 9, as is the transformer block architecture (the feedforward layer, layer norm, and so on). As in Chapter 9, the input is also a series of subword tokens, usually computed by one of the 3 popular tokenization algorithms (including the BPE algorithm that we already saw in Chapter 2 and two others, the WordPiece algorithm and the SentencePiece Unigram LM algorithm). That means every input sentence first has to be tokenized, and all further processing takes place on subword tokens rather than words. This will require, as we’ll see in the third part of the textbook, that for some NLP tasks that require notions of words (like parsing) we will occasionally need to map subwords back to words.

To make this more concrete, the original English-only bidirectional transformer encoder model, BERT (Devlin et al., 2019), consisted of the following:

• An English-only subword vocabulary consisting of 30,000 tokens generated using the WordPiece algorithm (Schuster and Nakajima, 2012).   
• Input context window $N { = } 5 1 2$ tokens, and model dimensionality $d { = } 7 6 8$   
• $\mathrm { { S o } } \textsf { X }$ , the input to the model, is of shape $[ N \times d ] = [ 5 1 2 \times 7 6 8 ]$ .   
• $L { = } 1 2$ layers of transformer blocks, each with $A { = } 1 2$ (bidirectional) multihead attention layers.   
• The resulting model has about 100M parameters.

The larger multilingual XLM-RoBERTa model, trained on 100 languages, has • A multilingual subword vocabulary with 250,000 tokens generated using the SentencePiece Unigram LM algorithm (Kudo and Richardson, 2018b).

• Input context window $N { = } 5 1 2$ tokens, and model dimensionality $d { = } 1 0 2 4$ , hence $\pmb { \times }$ , the input to the model, is of shape $[ N \times d ] = [ 5 1 2 \times 1 0 2 4 ]$ . • $L { = } 2 4$ layers of transformer blocks, with $A { = } 1 6$ multihead attention layers each • The resulting model has about 550M parameters.

Note that 550M parameters is relatively small as large language models go (Llama 3 has 405B parameters, so is 3 orders of magnitude bigger). Indeed, masked language models tend to be much smaller than causal language models.

# 11.2 Training Bidirectional Encoders

# cloze task

We trained causal transformer language models in Chapter 9 by making them iteratively predict the next word in a text. But eliminating the causal mask in attention makes the guess-the-next-word language modeling task trivial—the answer is directly available from the context—so we’re in need of a new training scheme. Instead of trying to predict the next word, the model learns to perform a fill-in-theblank task, technically called the cloze task (Taylor, 1953). To see this, let’s return to the motivating example from Chapter 3. Instead of predicting which words are likely to come next in this example:

The water of Walden Pond is so beautifully we’re asked to predict a missing item given the rest of the sentence.

The of Walden Pond is so beautifully ...

denoising

That is, given an input sequence with one or more elements missing, the learning task is to predict the missing elements. More precisely, during training the model is deprived of one or more tokens of an input sequence and must generate a probability distribution over the vocabulary for each of the missing items. We then use the crossentropy loss from each of the model’s predictions to drive the learning process.

This approach can be generalized to any of a variety of methods that corrupt the training input and then asks the model to recover the original input. Examples of the kinds of manipulations that have been used include masks, substitutions, reorderings, deletions, and extraneous insertions into the training text. The general name for this kind of training is called denoising: we corrupt (add noise to) the input in some way (by masking a word, or putting in an incorrect word) and the goal of the system is to remove the noise.

Masked Language Modeling

# 11.2.1 Masking Words

Let’s describe the Masked Language Modeling (MLM) approach to training bidirectional encoders (Devlin et al., 2019). As with the language model training methods we’ve already seen, MLM uses unannotated text from a large corpus. In MLM training, the model is presented with a series of sentences from the training corpus in which a percentage of tokens ( $15 \%$ in the BERT model) have been randomly chosen to be manipulated by the masking procedure. Given an input sentence lunch was delicious and assume we randomly chose the 3rd token delicious to be manipulated,

• $80 \%$ of the time: The token is replaced with the special vocabulary token named [MASK], e.g. lunch was delicious lunch was [MASK].

• $10 \%$ of the time: The token is replaced with another token, randomly sampled from the vocabulary based on token unigram probabilities. e.g. lunch was delicious lunch was gasp.   
• $10 \%$ of the time: the token is left unchanged. e.g. lunch was delicious lunch was delicious.

We then train the model to guess the correct token for the manipulated tokens. Why the three possible manipulations? Adding the [MASK] token creates a mismatch between pretraining and downstream fine-tuning or inference, since when we employ the MLM model to perform a downstream task, we don’t use any [MASK] tokens. If we just replaced tokens with the [MASK], the model might only predict tokens when it sees a [MASK], but we want the model to try to always predict the input token.

To train the model to make the prediction, the original input sequence is tokenized using a subword model and tokens are sampled to be manipulated. Word embeddings for all of the tokens in the input are retrieved from the E embedding matrix and combined with positional embeddings to form the input to the transformer, passed through the stack of bidirectional transformer blocks, and then the language modeling head. The MLM training objective is to predict the original inputs for each of the masked tokens and the cross-entropy loss from these predictions drives the training process for all the parameters in the model. That is, all of the input tokens play a role in the self-attention process, but only the sampled tokens are used for learning.

Figure 11.3 Masked language model training. In this example, three of the input tokens are selected, two of which are masked and the third is replaced with an unrelated word. The probabilities assigned by the model to these three items are used as the training loss. The other 5 tokens don’t play a role in training loss.

![## Image Analysis: 6ec9af9dcc6052b03e56553440213c6febaf4891a62c5682d3a23d29f5943fd1.jpg

**Conceptual Understanding:**
This image conceptually illustrates the training methodology for a Masked Language Model (MLM), which is a self-supervised learning approach used to pre-train large language models. The primary purpose of this diagram is to visually explain how a Bidirectional Transformer Encoder processes a sequence with intentionally masked or replaced words, and subsequently, how a language modeling head predicts the original words based on the learned context. The main message conveyed is the mechanism by which models acquire a deep, bidirectional understanding of language by trying to 'fill in the blanks'. Key ideas communicated include the generation of contextualized embeddings, the role of cross-entropy loss in guiding learning, and the specific input perturbation techniques (masking and random word replacement) employed in pre-training.

**Content Interpretation:**
The image details the training process of a Masked Language Model (MLM) using a Bidirectional Transformer Encoder. This process involves several key components and operations:

*   **Input Tokenization and Embedding:** The original sentence, "So long and thanks for all the fish", undergoes a transformation where specific words are either masked (e.g., "long" becomes "[mask]", "thanks" becomes "[mask]") or replaced by a random token from the vocabulary (e.g., "the" becomes "apricot"). Each of these modified tokens is then converted into a combined "Token + Positional Embedding" (represented by blocks p1-p8 with the internal structure "o + o o o"), which encodes both the word's identity and its position in the sequence.

*   **Bidirectional Contextual Encoding:** All eight token and positional embeddings are fed into the "Bidirectional Transformer Encoder". The 'bidirectional' nature is critical as it allows the model to process information from both the left and right context of each word simultaneously, leading to richer and more nuanced contextual representations. The outputs of this encoder are eight distinct vectors, labeled z₁ through z₈, each representing the contextualized understanding of its corresponding input token.

*   **Word Prediction (Language Modeling Head):** Only the output vectors from the encoder that correspond to the originally modified positions (z₂, z₄, and z₇ in this example) are passed to the "LM Head with Softmax over Vocabulary". This head takes the contextualized vector, applies an unembedding operation (implied by the trapezoid shape), and then generates a probability distribution over the entire vocabulary using a softmax function. The histogram within the rounded box visually represents this probability distribution, indicating the model's prediction for the original word.

*   **Loss Calculation (Cross-Entropy Loss):** To train the model, a "CE Loss" (Cross-Entropy Loss) is computed for each prediction. The red rectangular boxes show the individual loss terms: "− log ylong", "− log ythanks", and "− log ythe". These terms quantify the discrepancy between the model's predicted probability for the correct word (y) and the actual true word. The goal during training is to minimize this loss, thereby improving the model's ability to accurately predict the original masked or replaced words.

**Key Insights:**
The image provides several key takeaways and insights into the pre-training of modern language models:

1.  **Self-Supervised Learning Principle:** The core idea is self-supervision, where the model learns by predicting deliberately obscured parts of its input. This is evident from the contrast between the original sentence ("So long and thanks for all the fish") and the modified input ("So [mask] and [mask] for all apricot fish"), clearly showing the masking and replacement strategy. The model's task is to predict the original words based on the altered input.

2.  **Importance of Bidirectional Context:** The use of a "Bidirectional Transformer Encoder" emphasizes the necessity of considering both preceding and succeeding words to fully understand the context of a token. This architecture allows for a more comprehensive contextual understanding compared to unidirectional models, a critical insight supported by the labels z₁ through z₈ representing these rich contextual embeddings.

3.  **Specific Training Objective:** The model's learning is guided by minimizing "CE Loss" (Cross-Entropy Loss). The explicit display of loss terms like "− log ylong", "− log ythanks", and "− log ythe" shows how the model is penalized when its predicted probability for the correct word is low, driving it to learn better predictions for the masked/replaced tokens.

4.  **Modular Design of Language Models:** The diagram illustrates the common modular architecture of such models, comprising distinct stages: "Token + Positional Embeddings" for input representation, a "Bidirectional Transformer Encoder" for contextual understanding, and an "LM Head with Softmax over Vocabulary" for making predictions. This modularity allows for clear separation of concerns in model design.

5.  **Targeted Prediction:** The "LM Head" is only applied to the output vectors (z₂, z₄, z₇) corresponding to the masked or replaced tokens. This indicates that the training objective focuses specifically on reconstructing the altered parts of the input, making the training process efficient and directly aligned with the self-supervised task.

**Document Context:**
The image is a direct visual illustration for Section 11.2.1, titled "Masking Words," providing a concrete example of the masking approach in action. The document context explicitly states: "Fig. 11.3 illustrates this approach with a simple example. Here, long, thanks and the have been sampled from the training sequence, with the first two masked and the replaced with the randomly sampled token apricot. The resulting embeddings are passed through a stack of bidirectional transformer blocks. Recall from Section 9.5 in Chapter 9 that to produce a probability distribution over the vocabulary for each of the masked tokens, the language modeling head takes the output vector $\mathbf { h } _ { i } ^ { L }$ from the final transformer layer $L$ for each masked token $i$ , multiplies it by the unembedding layer $\mathsf { E } ^ { T }$ to produce the logits $\mathbf { u }$ , and then uses softmax to turn the logits into..." This textual description perfectly aligns with every element depicted in the diagram, from the specific words masked/replaced ("long", "thanks" as masked, "the" replaced by "apricot") to the architectural components ("Bidirectional Transformer Encoder," "LM Head with Softmax over Vocabulary," and the calculation of "− log y" representing cross-entropy loss). The image serves as an essential visual aid that clarifies the abstract concepts of masked language modeling, bidirectional processing, and the training objective, making the document's explanation significantly more understandable.

**Summary:**
This diagram illustrates the training process of a Masked Language Model (MLM) using a Bidirectional Transformer Encoder. This technique is fundamental in modern Natural Language Processing for pre-training models like BERT, enabling them to learn deep contextual representations by predicting masked or replaced words within a sentence. The process unfolds in three main stages:

**1. Input Preparation (Bottom Section - "Token + Positional Embeddings")**
*   An original sentence, "So long and thanks for all the fish", serves as the ground truth.
*   For training, a modified input sequence is created by strategically altering certain words:
    *   The word "long" is replaced with the special token "[mask]".
    *   The word "thanks" is also replaced with a "[mask]" token.
    *   The word "the" is replaced with a randomly sampled word from the vocabulary, "apricot".
*   This results in the modified input sequence: "So [mask] and [mask] for all apricot fish".
*   Each token in this modified sequence is then converted into a "Token + Positional Embedding". These embeddings, labeled "p1" through "p8" (with an internal representation of "o + o o o"), are vector representations that capture both the semantic meaning of the token and its sequential position.

**2. Contextual Encoding (Middle Section - "Bidirectional Transformer Encoder")**
*   All eight "Token + Positional Embeddings" (p1 to p8) are fed as input to the "Bidirectional Transformer Encoder". This encoder is a neural network component that processes the entire sequence simultaneously, crucial for understanding context from both the left and right sides of each word.
*   The transformer encoder outputs a series of context-rich vector representations, labeled "z₁" through "z₈". Each 'z' vector (e.g., z₂, z₄, z₇) represents the contextualized understanding of its corresponding input token.

**3. Word Prediction and Loss Calculation (Top Section - "LM Head with Softmax over Vocabulary" and "CE Loss")**
*   The model's primary task is to predict the original words that were masked or replaced. Therefore, only the output vectors from the encoder corresponding to these altered positions are selected for further processing by the "LM Head with Softmax over Vocabulary". In this specific example, these are "z₂" (which corresponds to the original word "long"), "z₄" (for "thanks"), and "z₇" (for "the").
*   For each selected 'z' vector:
    *   It first passes through a layer (represented by a trapezoid shape) which typically involves an unembedding operation.
    *   Then, it proceeds to the "LM Head with Softmax over Vocabulary" (represented by a rounded box containing a histogram). This head applies a softmax function to produce a probability distribution over the *entire vocabulary* of words, indicating the model's likelihood for each word being the original. The histogram visually depicts this probability distribution.
    *   Finally, a "CE Loss" (Cross-Entropy Loss) is calculated. This loss measures the discrepancy between the model's predicted probability distribution and the actual true original word. The red rectangular boxes display the loss terms as "− log ylong", "− log ythanks", and "− log ythe", which are derived from the negative logarithm of the predicted probability for the correct word. A lower loss value signifies a more accurate prediction by the model.

The entire system is trained by minimizing this cumulative cross-entropy loss, iteratively adjusting its internal parameters to enhance its capability to accurately predict masked or replaced words based on their comprehensive bidirectional context.](images/6ec9af9dcc6052b03e56553440213c6febaf4891a62c5682d3a23d29f5943fd1.jpg)
Fig. 11.3 illustrates this approach with a simple example. Here, long, thanks and the have been sampled from the training sequence, with the first two masked and the replaced with the randomly sampled token apricot. The resulting embeddings are passed through a stack of bidirectional transformer blocks. Recall from Section 9.5 in Chapter 9 that to produce a probability distribution over the vocabulary for each of the masked tokens, the language modeling head takes the output vector $\mathbf { h } _ { i } ^ { L }$ from the final transformer layer $L$ for each masked token $i$ , multiplies it by the unembedding layer $\mathsf { E } ^ { T }$ to produce the logits $\mathbf { u }$ , and then uses softmax to turn the logits into

probabilities $\pmb { \ y }$ over the vocabulary:

$$
\begin{array} { r l } & { \mathbf { u } _ { i } \ = \ \mathsf { h } _ { \mathsf { i } } ^ { \mathsf { L } } \ \mathsf { E } ^ { \mathsf { T } } } \\ & { \mathbf { y } _ { i } \ = \ \mathrm { s o f t m a x } ( \mathbf { u } _ { \mathsf { i } } ) } \end{array}
$$

With a predicted probability distribution for each masked item, we can use crossentropy to compute the loss for each masked item—the negative log probability assigned to the actual masked word, as shown in Fig. 11.3. More formally, for a given vector of input tokens in a sentence or batch be $\pmb { \times }$ , let the set of tokens that are masked be $M$ , the version of that sentence with some tokens replaced by masks be $\mathbf { x } ^ { m a s k }$ , and the sequence of output vectors be $\mathbf { h }$ . For a given input token $x _ { i }$ , such as the word long in Fig. 11.3, the loss is the probability of the correct word long, given $\mathbf { x } ^ { m a s k }$ (as summarized in the single output vector $\mathbf { h } _ { i } ^ { L }$ ):

$$
L _ { M L M } ( x _ { i } ) = - \log P ( x _ { i } | \mathbf { h } _ { i } ^ { L } )
$$

The gradients that form the basis for the weight updates are based on the average loss over the sampled learning items from a single training sequence (or batch of sequences).

$$
L _ { M L M } = - { \frac { 1 } { | M | } } \sum _ { i \in M } \log P ( x _ { i } | \mathbf { \hat { h } } _ { i } ^ { L } )
$$

Note that only the tokens in $M$ play a role in learning; the other words play no role in the loss function, so in that sense BERT and its descendents are inefficient; only $15 \%$ of the input samples in the training data are actually used for training weights.1

# 11.2.2 Next Sentence Prediction

The focus of mask-based learning is on predicting words from surrounding contexts with the goal of producing effective word-level representations. However, an important class of applications involves determining the relationship between pairs of sentences. These include tasks like paraphrase detection (detecting if two sentences have similar meanings), entailment (detecting if the meanings of two sentences entail or contradict each other) or discourse coherence (deciding if two neighboring sentences form a coherent discourse).

To capture the kind of knowledge required for applications such as these, some models in the BERT family include a second learning objective called Next Sentence Prediction (NSP). In this task, the model is presented with pairs of sentences and is asked to predict whether each pair consists of an actual pair of adjacent sentences from the training corpus or a pair of unrelated sentences. In BERT, $50 \%$ o f the training pairs consisted of positive pairs, and in the other $50 \%$ the second sentence of a pair was randomly selected from elsewhere in the corpus. The NSP loss is based on how well the model can distinguish true pairs from random pairs.

To facilitate NSP training, BERT introduces two special tokens to the input representation (tokens that will prove useful for finetuning as well). After tokenizing the input with the subword model, the token [CLS] is prepended to the input sentence pair, and the token [SEP] is placed between the sentences and after the final token of the second sentence. There are actually two more special tokens, a ‘First Segment’ token, and a ‘Second Segment’ token. These tokens are added in the input stage to the word and positional embeddings. That is, each token of the input $\pmb { \times }$ is actually formed by summing 3 embeddings: word, position, and first/second segment embeddings.

During training, the output vector $h _ { \mathrm { C L S } } ^ { L }$ from the final layer associated with the [CLS] token represents the next sentence prediction. As with the MLM objective, we add a special head, in this case an NSP head, which consists of a learned set of classification weights $\pmb { \mathsf { W } } _ { \mathsf { N S P } } \in \mathbb { R } ^ { d \times 2 }$ that produces a two-class prediction from the raw [CLS] vector $h _ { \mathrm { C L S } } ^ { L }$ :

$$
\mathbf { y } _ { i } ~ = ~ \mathrm { s o f t m a x } ( \mathbf { h } _ { \mathrm { C L S } } ^ { L } \mathbf { W } _ { \mathsf { N S P } } )
$$

Cross entropy is used to compute the NSP loss for each sentence pair presented to the model. Fig. 11.4 illustrates the overall NSP training setup. In BERT, the NSP loss was used in conjunction with the MLM training objective to form final loss.

![## Image Analysis: 81313c24c8b12fa42bd266377cedc9277e73d25b8d59e6f557cff2fe4b244d16.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and flow for calculating the Next Sentence Prediction (NSP) loss within a deep learning model, likely a transformer-based model like BERT. The main purpose is to illustrate how an input sequence, comprising two potential sentences, is processed to determine if the second sentence is a logical successor to the first, and how the prediction error (loss) is calculated. It communicates the key ideas of combining different types of embeddings (token, segment, positional), using a bidirectional transformer for contextual encoding, employing a dedicated prediction head for a specific task (NSP), and utilizing cross-entropy for loss computation in a supervised learning setting.

**Content Interpretation:**
The image details the architectural components and data flow involved in calculating the Next Sentence Prediction (NSP) loss within a Bidirectional Transformer Encoder framework, similar to BERT. It shows how raw input text is transformed into various types of embeddings (Token, Segment, Positional), processed by a transformer encoder, and then fed into a specific head ('NSP Head') to predict the relationship between two sentences, culminating in a Cross-Entropy (CE) Loss calculation. The process illustrates the input preparation (combining token, segment, and positional information), the core encoding (Bidirectional Transformer Encoder), the specific prediction layer (NSP Head with W_NSP), and the final loss computation (- log y₁). The input example, "[CLS] Cancel my flight [SEP] And the hotel [SEP]", clearly delineates the two sentences being evaluated for their sequential relationship, with segment IDs 's1' and 's2' indicating the sentence boundaries.

**Key Insights:**
The main takeaways from this image are: 1.  NSP models combine 'Token', 'Segment', and 'Positional Embeddings' to represent input sentences. The 'Token + Segment + Positional Embeddings' section explicitly shows the combination of these three types of embeddings for each word/token. 2.  A 'Bidirectional Transformer Encoder' is used to process these combined embeddings and generate contextualized representations. The large purple box labeled 'Bidirectional Transformer Encoder' clearly indicates this core component. 3.  The 'h_CLS' vector, which is the output representation corresponding to the '[CLS]' token, is specifically used for the NSP task. The arrow labeled 'h_CLS' emanating from the Bidirectional Transformer Encoder and pointing to the NSP Head confirms this. 4.  The 'NSP Head' applies a weight matrix, 'W_NSP', to 'h_CLS' to make the next sentence prediction. The 'NSP Head' box contains 'W_NSP', illustrating this. 5.  Cross-Entropy (CE) Loss is used to quantify the error of the NSP prediction. The 'CE Loss' label and '- log y₁' formula demonstrate the final loss calculation.

**Document Context:**
This image directly supports section 11.2.2, which is titled 'Next Sentence Prediction'. It visually explains the theoretical concept of NSP loss calculation discussed in the text, providing a concrete example of how input is prepared, processed by a Bidirectional Transformer Encoder, and ultimately leads to a loss value used for training. By illustrating the components like 'Token + Segment + Positional Embeddings', 'Bidirectional Transformer Encoder', 'NSP Head', and 'CE Loss', the figure serves as a crucial visual aid for understanding the internal workings of models that utilize NSP as a pre-training objective, such as BERT. The example input sequence and its corresponding segment and positional embeddings further clarify the data representation.

**Summary:**
The image illustrates the process of Next Sentence Prediction (NSP) loss calculation, a common task in natural language processing models like BERT. The process begins with input words being converted into a combination of Token, Segment, and Positional Embeddings. For the example sentence pair "[CLS] Cancel my flight [SEP] And the hotel [SEP]", each word (or special token) is transformed. "[CLS]", "Cancel", "my", "flight" belong to segment 's1' and have positional embeddings 'p1', 'p2', 'p3', 'p4' respectively. The special token "[SEP]" also belongs to segment 's1' and has positional embedding 'p5'. The second sentence, "And the hotel [SEP]", belongs to segment 's2' with positional embeddings 'p6', 'p7', 'p8', 'p9'. These combined embeddings are then fed into a 'Bidirectional Transformer Encoder'. The encoder processes these embeddings to produce contextualized representations. Specifically, the output corresponding to the initial '[CLS]' token, denoted as 'h_CLS', is extracted. This 'h_CLS' representation is then passed to the 'NSP Head'. The 'NSP Head' contains a weight matrix, 'W_NSP', which processes 'h_CLS' to predict whether the second sentence logically follows the first. Finally, the output from the 'NSP Head' is used to calculate the 'CE Loss' (Cross-Entropy Loss), represented by '- log y₁', where 'y₁' is likely the predicted probability for the correct next sentence. This loss value, indicated by '1', is then used to update the model parameters during training. The entire diagram visually maps how a sequence of words is processed through a transformer model to compute the NSP loss.](images/81313c24c8b12fa42bd266377cedc9277e73d25b8d59e6f557cff2fe4b244d16.jpg)
Figure 11.4 An example of the NSP loss calculation.

# 11.2.3 Training Regimes

BERT and other early transformer-based language models were trained on about 3.3 billion words (a combination of English Wikipedia and a corpus of book texts called BooksCorpus (Zhu et al., 2015) that is no longer used for intellectual property reasons). Modern masked language models are now trained on much larger datasets of web text, filtered a bit, and augmented by higher-quality data like Wikipedia, the same as those we discussed for the causal large language models of Chapter 9. Multilingual models similarly use webtext and multilingual Wikipedia. For example the XLM-R model was trained on about 300 billion tokens in 100 languages, taken from the web via Common Crawl (https://commoncrawl.org/).

To train the original BERT models, pairs of text segments were selected from the training corpus according to the next sentence prediction 50/50 scheme. Pairs were sampled so that their combined length was less than the 512 token input. Tokens within these sentence pairs were then masked using the MLM approach with the combined loss from the MLM and NSP objectives used for a final loss. Because this final loss is backpropagated through the entire transformer, the embeddings at each transformer layer will learn representations that are useful for predicting words from their neighbors. Since the [CLS] tokens are the direct input to the NSP classifier, their learned representations will tend to contain information about the sequence as a whole. Approximately 40 passes (epochs) over the training data was required for the model to converge.

Some models, like the RoBERTa model, drop the next sentence prediction objective, and therefore change the training regime a bit. Instead of sampling pairs of sentence, the input is simply a series of contiguous sentences, still beginning with the special [CLS] token. If the document runs out before 512 tokens are reached, an extra separator token is added, and sentences from the next document are packed in, until we reach a total of 512 tokens. Usually large batch sizes are used, between 8K and 32K tokens.

Multilingual models have an additional decision to make: what data to use to build the vocabulary? Recall that all language models use subword tokenization (BPE or SentencePiece Unigram LM are the two most common algorithms). What text should be used to learn this multilingual tokenization, given that it’s easier to get much more text in some languages than others? One option would be to create this vocabulary-learning dataset by sampling sentences from our training data (perhaps web text from Common Crawl), randomly. In that case we will choose a lot of sentences from languages like languages with lots of web representation like English, and the tokens will be biased toward rare English tokens instead of creating frequent tokens from languages with less data. Instead, it is common to divide the training data into subcorpora of $N$ different languages, compute the number of sentences $n _ { i }$ of each language $i$ , and readjust these probabilities so as to upweight the probability of less-represented languages (Lample and Conneau, 2019). The new probability of selecting a sentence from each of the $N$ languages (whose prior frequency is $n _ { i }$ ) is $\{ q _ { i } \} _ { i = 1 \ldots N }$ , where:

$$
q _ { i } = { \frac { p _ { i } ^ { \alpha } } { \sum _ { j = 1 } ^ { N } p _ { j } ^ { \alpha } } } \mathrm { ~ w i t h ~ } p _ { i } = { \frac { n _ { i } } { \sum _ { k = 1 } ^ { N } n _ { k } } }
$$

Recall from Eq. 6.32 in Chapter 6 that an $\alpha$ value between 0 and 1 will give higher weight to lower probability samples. Conneau et al. (2020) show that $\alpha = 0 . 3$ works well to give rare languages more inclusion in the tokenization, resulting in better multilingual performance overall.

The result of this pretraining process consists of both learned word embeddings, as well as all the parameters of the bidirectional encoder that are used to produce contextual embeddings for novel inputs.

For many purposes, a pretrained multilingual model is more practical than a monolingual model, since it avoids the need to build many (a hundred!) separate monolingual models. And multilingual models can improve performance on lowresourced languages by leveraging linguistic information from a similar language in the training data that happens to have more resources. Nonetheless, when the number of languages grows very large, multilingual models exhibit what has been called the curse of multilinguality (Conneau et al., 2020): the performance on each language degrades compared to a model training on fewer languages. Another problem with multilingual models is that they ‘have an accent’: grammatical structures in higher-resource languages (often English) bleed into lower-resource languages; the vast amount of English language in training makes the model’s representations for low-resource languages slightly more English-like (Papadimitriou et al., 2023).

# 11.3 Contextual Embeddings

Given a pretrained language model and a novel input sentence, we can think of the sequence of model outputs as constituting contextual embeddings for each token in the input. These contextual embeddings are vectors representing some aspect of the meaning of a token in context, and can be used for any task requiring the meaning of tokens or words. More formally, given a sequence of input tokens $x _ { 1 } , . . . , x _ { n }$ , we can use the output vector $\boldsymbol { \mathsf { h } } ^ { \mathsf { L } } { } _ { i }$ from the final layer $L$ of the model as a representation of the meaning of token $x _ { i }$ in the context of sentence $x _ { 1 } , . . . , x _ { n }$ . Or instead of just using the vector $\boldsymbol { \mathsf { h } } ^ { \mathsf { L } } { } _ { i }$ from the final layer of the model, it’s common to compute a representation for $x _ { i }$ by averaging the output tokens $\mathbf { h } _ { i }$ from each of the last four layers of the model, i.e., $\mathsf { \pmb { \imath } } ^ { \mathsf { L } } _ { i } , \mathsf { \pmb { h } } ^ { \mathsf { L } - 1 } _ { i } , \mathsf { \pmb { h } } ^ { \mathsf { L } - 2 } _ { i }$ , and $\mathbf { h } ^ { \mathbf { L } - 3 } \mathbf { \Phi } _ { i }$ .

![## Image Analysis: 2af17bae2e9a450b14d66489d40259027713d7e845ad0717422b11d3ca299e0c.jpg

**Conceptual Understanding:**
This image conceptually represents the forward pass through the encoder part of a BERT-style transformer model, specifically illustrating how an input sequence of tokens is transformed into a sequence of contextual embeddings.

The main purpose of the image is to visually explain the architecture and data flow of such a model, showing:
1.  The initial token and positional embedding process.
2.  The multi-layered processing within the transformer encoder.
3.  The final output of contextual embeddings for each input token.

Key ideas communicated include:
*   The concept of token embeddings and positional embeddings.
*   The layered, sequential nature of transformer models.
*   The generation of distinct contextual embedding vectors (`h^L_i`) for each input token, which incorporate information from the entire input sequence.

**Content Interpretation:**
The image demonstrates the process of generating contextual embeddings for a sequence of input tokens using a deep learning architecture, specifically a BERT-style model.

*   **Input Sequence:** The input tokens "[CLS]", "So", "long", "and", "thanks", "for", "all" represent a sample sentence or sequence that the model processes. The "[CLS]" token is a special classification token typically used in BERT for classification tasks, where its final contextual embedding (`h^L_CLS`) can represent the entire sequence's meaning.
*   **Embedding Layer (`E`, `+`, `i`):** Each token `x_i` (e.g., "So", "long") is first converted into an initial embedding (`E`). This initial embedding typically captures the semantic meaning of the token itself. Then, a positional embedding (`i`) is added (`+`) to this token embedding. This addition of `i` is crucial because it injects information about the token's position within the sequence, which transformers need since they don't inherently process tokens sequentially. This combined embedding serves as the input to the subsequent transformer layers.
*   **Transformer Encoder Layers (Stacked Blue Rectangles):** The large blue block represents a multi-layered transformer encoder. Each of the 6 horizontal blue rectangles signifies a transformer layer. These layers contain mechanisms like multi-head self-attention and feed-forward networks, which allow each token's representation to be contextualized by all other tokens in the sequence. The vertical arrows indicate the flow of information through these layers, where each layer refines the token representations based on increasingly complex contextual understanding.
*   **Contextual Embeddings (`h^L_CLS`, `h^L_1`, ..., `h^L_6`):** The final outputs at the top are labeled `h^L_CLS`, `h^L_1`, etc. The `h` signifies a hidden state or embedding, `L` denotes that it's the output from the final layer of the transformer, and the subscript (e.g., `CLS`, `1`, `2`) links it to the corresponding input token. These `h^L_i` vectors are "contextual embeddings" because they are not just static representations of individual words but are dynamically generated based on the word's specific context within the input sequence. For example, the `h^L_1` for "So" would reflect its meaning in the context of "So long and thanks for all".

All extracted text elements, from the input tokens to the `E`, `+`, `i` components, and the final `h^L_i` outputs, precisely delineate this process of converting raw text into rich, context-aware numerical representations suitable for downstream NLP tasks. The layered structure explicitly shows the depth of processing involved in a BERT-style model.

**Key Insights:**
Main takeaways from this image regarding BERT-style models and contextual embeddings:

*   **Contextual Embeddings are Layered Outputs:** The outputs `h^L_CLS`, `h^L_1`, ..., `h^L_6` explicitly show that these embeddings are the result of multiple layers (`L`) of processing, signifying their "deep" nature.
*   **Token and Positional Information are Combined at Input:** The `E + i` notation (`E` for embedding, `+` for addition, `i` for positional information) clearly indicates that both the semantic meaning of a token and its position in the sequence are critical initial inputs to the transformer layers.
*   **Transformer Models Process Sequences in Parallel through Layers:** While the diagram shows vertical processing for each token's column, the nature of transformer layers (implied by "BERT-style model") means that within each horizontal layer, all tokens' representations are updated simultaneously by attending to each other. The 6 stacked horizontal rectangles visually represent these successive layers of contextualization.
*   **Special Tokens (`[CLS]`) have Dedicated Outputs:** The presence of `[CLS]` as an input token and its corresponding `h^L_CLS` output highlights its specific role, often used to represent the aggregated meaning of the entire sentence for classification tasks.
*   **Each Input Token Yields a Unique Contextual Embedding:** For every input token (e.g., "So", "long"), there is a distinct output embedding (`h^L_1`, `h^L_2`), reinforcing the idea that these models generate unique, context-dependent representations for each word.

The specific text `[CLS]`, `So`, `long`, `and`, `thanks`, `for`, `all` demonstrates a sample input. The `E`, `+`, `i` components show the embedding mechanism. The stacked horizontal blue rectangles and vertical arrows within the large blue block illustrate the multi-layered transformer processing. Finally, `h^L_CLS`, `h^L_1`, `h^L_2`, `h^L_3`, `h^L_4`, `h^L_5`, `h^L_6` explicitly label the contextual embedding outputs from the final layer.

**Document Context:**
This image directly supports Section 11.3, "Contextual Embeddings," by providing a visual illustration of how these embeddings are generated in a prominent model architecture like BERT. The text after the image, "Figure 11.5 The output of a BERT-style model is a contextual embedding vector $\boldsymbol { \mathsf { h } } _ { i } ^ { L }$ for each input token $x _ { i }$," perfectly aligns with and explains the output labels shown in the diagram. It serves as a foundational visual aid to understand the mechanics behind the core concept of contextual embeddings, which are crucial for advanced NLP tasks.

**Summary:**
The diagram illustrates the process by which a BERT-style model transforms a sequence of input text tokens into rich, context-aware numerical representations known as contextual embeddings.

At the very bottom, we see the input sequence of tokens: `[CLS]`, `So`, `long`, `and`, `thanks`, `for`, `all`. The `[CLS]` token is a special classification token often used to derive a representation of the entire input sequence.

Directly above each input token is an initial embedding stage. Each token, let's call it `x_i`, first goes through an embedding lookup, represented by the yellow trapezoidal shape containing the letter `E`. This `E` produces a vector representation of the token's basic meaning. Crucially, to account for the word's position in the sentence, a positional embedding (represented by `i` in a white square) is added (`+` in a green circle) to the token embedding `E`. This combined `E+i` output, a position-aware token embedding, is then fed upwards into the main processing unit.

The large, rounded blue rectangle represents the core of the BERT-style model, comprising multiple stacked transformer encoder layers. In this diagram, there are 6 such layers for each token, depicted as horizontal blue rectangles stacked vertically. Information flows upwards through these layers, as indicated by the vertical arrows between each layer. Each layer processes the embeddings, allowing each token's representation to be influenced by (or "attend to") all other tokens in the sequence. This iterative process of contextualization across multiple layers refines the initial token embeddings into highly nuanced representations.

Finally, at the top, the outputs from the last layer of this transformer model are shown. For each original input token, a distinct contextual embedding vector is produced. These are labeled `h^L_CLS`, `h^L_1`, `h^L_2`, `h^L_3`, `h^L_4`, `h^L_5`, and `h^L_6`. The `h` denotes a hidden state or embedding, and the `L` indicates that it is the output from the model's final (L-th) layer. The subscripts (like `CLS`, `1`, `2`) correspond directly to their respective input tokens. These `h^L_i` vectors are the "contextual embeddings" – numerical representations that capture not just the meaning of the word itself, but also how its meaning is shaped by its surrounding words in that specific sentence.](images/2af17bae2e9a450b14d66489d40259027713d7e845ad0717422b11d3ca299e0c.jpg)
Figure 11.5 The output of a BERT-style model is a contextual embedding vector $\boldsymbol { \mathsf { h } } _ { i } ^ { L }$ for each input token $x _ { i }$ .

Just as we used static embeddings like word2vec in Chapter 6 to represent the meaning of words, we can use contextual embeddings as representations of word meanings in context for any task that might require a model of word meaning. Where static embeddings represent the meaning of word types (vocabulary entries), contextual embeddings represent the meaning of word instances: instances of a particular word type in a particular context. Thus where word2vec had a single vector for each word type, contextual embeddings provide a single vector for each instance of that word type in its sentential context. Contextual embeddings can thus be used for tasks like measuring the semantic similarity of two words in context, and are useful in linguistic tasks that require models of word meaning.

# 11.3.1 Contextual Embeddings and Word Sense

Words are ambiguous: the same word can be used to mean different things. In Chapter 6 we saw that the word “mouse” can mean (1) a small rodent, or (2) a handoperated device to control a cursor. The word “bank” can mean: (1) a financial institution or (2) a sloping mound. We say that the words ‘mouse’ or ‘bank’ are polysemous (from Greek ‘many senses’, poly- ‘many’ $^ +$ sema, ‘sign, mark’).2

A sense (or word sense) is a discrete representation of one aspect of the meaning of a word. We can represent each sense with a superscript: bank1 and bank2, mouse1 and mouse2. These senses can be found listed in online thesauruses (or thesauri) like WordNet (Fellbaum, 1998), which has datasets in many languages listing the senses of many words. In context, it’s easy to see the different meanings:

# WordNet

mouse1 : .... a mouse controlling a computer system in 1968. mouse2 : .... a quiet animal like a mouse bank1 : ...a bank can hold the investments in a custodial account ... bank2 : ...as agriculture burgeons on the east bank, the river ...

This fact that context disambiguates the senses of mouse and bank above can also be visualized geometrically. Fig. 11.6 shows a two-dimensional projection of many instances of the BERT embeddings of the word die in English and German. Each point in the graph represents the use of die in one input sentence. We can clearly see at least two different English senses of die (the singular of dice and the verb to die, as well as the German article, in the BERT embedding space.

![## Image Analysis: 32de6d2f2a6f20b795b9277145d699b9a29b3b425be05d639ff83f5f54bf2c89.jpg

**Conceptual Understanding:**
This image conceptually represents the semantic space generated by BERT's contextual embeddings when applied to different usages of the word 'die' across English and German. The main purpose of the visualization is to demonstrate BERT's efficacy in word sense disambiguation and cross-lingual homograph distinction. It visually asserts that the contextual embeddings successfully map semantically distinct uses of a word to separate regions in a low-dimensional space, thereby illustrating the model's understanding of context and meaning.

**Content Interpretation:**
This image presents a visualization of word embeddings, specifically BERT contextual embeddings, for the word 'die' in both English and German, reduced to two dimensions using UMAP. The visualization illustrates how these embeddings naturally form distinct clusters corresponding to different meanings (word senses) of 'die' and also differentiate between its use as a German article versus an English word. The distinct clusters represent: 1. The German article 'die', 2. The English verb 'die' referring to a single person, 3. The English verb 'die' referring to multiple people, and 4. The English noun 'die' (as in a gaming cube). The example sentences provided for each cluster serve as contextual evidence for the assigned semantic categories. The blue dots, representing individual BERT embeddings, coalesce into these groups, showcasing the model's ability to capture fine-grained semantic distinctions based on context.

**Key Insights:**
The main takeaway from this image is the demonstration of BERT's advanced capability in contextual word embedding. It effectively illustrates that BERT can: 1. Distinguish between a word in one language (German article 'die') and a homograph/homophone in another language (English 'die'), as evidenced by the clear separation of the 'German article "die"' cluster from all English clusters. 2. Perform fine-grained word sense disambiguation within the same language, showing distinct clusters for the English verb 'die' (both 'single person dies' and 'multiple people die' contexts) and the English noun 'a playing die'. 3. Capture subtle semantic nuances even within a single verb sense, indicated by the partial separation and connection of 'single person dies' and 'multiple people die' clusters, suggesting related but distinct contextual usages. The specific example sentences provide the concrete textual evidence for these semantic distinctions captured by the BERT embeddings.

**Document Context:**
This image serves as a powerful visual example within a document discussing WordNet or broader topics in natural language processing, particularly concerning word embeddings, contextual language models like BERT, and word sense disambiguation. The preceding and following text likely elaborate on the technical aspects of BERT embeddings, UMAP, and their application in distinguishing word meanings across languages and within a single language. It visually reinforces the concept that modern language models can discern subtle semantic differences that traditional word embeddings might conflate, thereby enhancing document comprehension by providing concrete evidence of BERT's capabilities as described in academic or technical contexts.

**Summary:**
The image displays a scatter plot visualizing BERT contextual embeddings for the word 'die' in various English and German sentences, projected into two dimensions using the UMAP algorithm. The plot distinctly separates different meanings and languages of the word 'die' into five main clusters, each represented by blue dots. At the top of the plot, a cluster of blue dots is labeled 'German article "die"', indicating embeddings for the German definite article. Below this, there are two example sentences illustrating its use: 'Was der Fall ist, die Tatsache, ist das Bestehen von Sachverhalten.' and 'über die Verhandlungen der Königl.'. In the bottom left, a cluster is labeled 'single person dies', representing the English verb 'die' when referring to an individual. This cluster is accompanied by two example sentences: 'Chernenko became the first Soviet leader to die in less than three years' and 'Vaughan's ultimate fantasy was to die in a head-on collision with movie star Elizabeth Taylor'. Adjacent to this, in the bottom middle, another cluster is labeled 'multiple people die', also representing the English verb 'die' but in contexts involving multiple subjects. This cluster includes the examples: 'Over 60 people die and over 100 are unaccounted for.' and 'Many more die from radiation sickness, starvation and cold.'. A horizontal arrow visually connects the 'single person dies' and 'multiple people die' clusters. Finally, in the bottom right, a distinct cluster is labeled 'a playing die', representing the English noun 'die' (as in dice). This cluster is explained with the sentences: 'Players must always move a token according to the die value' and 'The faces of a die may be placed clockwise or counterclockwise'. The overall arrangement clearly demonstrates how contextual embeddings can differentiate between homographs, polysemous words, and cross-lingual instances.](images/32de6d2f2a6f20b795b9277145d699b9a29b3b425be05d639ff83f5f54bf2c89.jpg)
Figure 11.6 Each blue dot shows a BERT contextual embedding for the word die from different sentences in English and German, projected into two dimensions with the UMAP algorithm. The German and English meanings and the different English senses fall into different clusters. Some sample points are shown with the contextual sentence they came from. Figure from Coenen et al. (2019).

Thus while thesauruses like WordNet give discrete lists of senses, embeddings tion of word senses(whether static or contextual) offer a continuous high-dimensional model of meaning that, although it can be clustered, doesn’t divide up into fully discrete senses.

# ferent word senses, we collectWord Sense Disambiguation

The task of selecting the correct sense for a word is called word sense disambiguasentences containing that word. It sends these sentences to BERT-base as input, andtion, or WSD. WSD algorithms take as input a word in context and a fixed inventory retrieves the context embedding for the word from a layer of the user’s choosing.of potential word senses (like the ones in WordNet) and outputs the correct word sense in context. Fig. 11.7 sketches out the task.

![## Image Analysis: ab33798507577283882b258dfe460ec38ebc3c25bc342f4f429e186db6f779c0.jpg

**Conceptual Understanding:**
This image conceptually represents the task of Word Sense Disambiguation (WSD). Its main purpose is to illustrate how individual words from a sentence are taken as input and then associated with their various possible meanings or 'senses' from a lexical resource like WordNet. The key idea being communicated is the process of identifying all potential semantic interpretations for a given word, which is the foundational step before a system can select the contextually appropriate sense. It highlights the ambiguity inherent in many words and the need for a system to manage these multiple meanings.

**Content Interpretation:**
The image displays a conceptual model of the Word Sense Disambiguation (WSD) task, specifically for an "all-words" approach. It shows how each word in a given input sentence (or at least, specific words requiring disambiguation) is considered as an input `x_i` and mapped to a set of possible WordNet senses `y_i`. The diagram illustrates the inherent ambiguity of many words by presenting multiple candidate senses with their definitions for words like "electric," "bass," "player," "stand," and "side." For example, "bass" is shown with senses ranging from "low range" to "sea fish" and "instrument," highlighting the challenge of selecting the correct sense based on context. "Guitar" is shown with only one sense `guitar^1`, suggesting it is less ambiguous in this context or that only one relevant sense is considered. The `x_i` circles represent the processing of individual words, and the `y_i` boxes contain the lexicon (WordNet) entries for those words, including their sense numbers and brief definitions. The "..." indicates that the lists of senses are truncated.

**Key Insights:**
The main takeaway is that Word Sense Disambiguation involves identifying the correct meaning or 'sense' of a word in a specific context. The image clearly demonstrates the polysemy of common English words, where a single word form can have multiple distinct meanings (e.g., 'bass' as a low range, a fish, or an instrument). This necessitates a computational approach to determine the intended sense. The diagram uses specific textual evidence like `electric^1: using electricity`, `electric^2: tense`, `electric^3: thrilling` to show the different semantic possibilities. Similarly, for `player^1: in game`, `player^2: musician`, `player^3: actor`, it illustrates how a word's meaning depends on its usage. The systematic mapping from `x_i` (input word) to `y_i` (list of senses) highlights the core task of WSD, which is to select one of these senses. The inclusion of 'guitar^1' with only one listed sense could imply words with a primary or highly dominant sense in a given domain, or simply less ambiguity. The overall insight is that WSD is crucial for deeper natural language understanding, as surface forms often mask underlying semantic complexities.

**Document Context:**
This image directly supports the 

**Summary:**
The image illustrates the all-words Word Sense Disambiguation (WSD) task, depicting how individual words from an input sentence are mapped to their potential senses as defined in WordNet. The diagram presents a sequence of input words at the bottom, some of which are highlighted as `x_i` inputs and connected to corresponding `y_i` boxes containing their possible WordNet senses. 

The input sentence is read left-to-right: "an electric guitar and bass player stand off to one side".

Specifically, the words 'an', 'and', 'off', 'to', and 'one' are shown in light gray and are not connected to any specific WordNet sense boxes in this illustration. The disambiguation process is shown for the following words:

*   **electric (x1):** Connected to `y1`, which lists three senses:
    *   `electric^1`: using electricity
    *   `electric^2`: tense
    *   `electric^3`: thrilling

*   **guitar (x2):** Connected to `y2`, which lists one sense:
    *   `guitar^1`

*   **bass (x3):** Connected to `y3`, which lists partial senses, indicated by "...":
    *   `bass^1`: low range
    *   `bass^4`: sea fish
    *   `bass^7`: instrument

*   **player (x4):** Connected to `y4`, which lists partial senses, indicated by "...":
    *   `player^1`: in game
    *   `player^2`: musician
    *   `player^3`: actor

*   **stand (x5):** Connected to `y5`, which lists partial senses, indicated by "...":
    *   `stand^1`: upright
    *   `stand^5`: bear
    *   `stand^10`: put upright

*   **side (x6):** Connected to `y6`, which lists partial senses, indicated by "...":
    *   `side^1`: relative region
    *   `side^3`: of body
    *   `side^11`: slope

Each `x_i` represents an input word for which senses are to be considered, and each `y_i` box contains a list of candidate senses (word^sense_number: definition) from WordNet for that particular word. The "..." within the `y_i` boxes signifies that more senses exist but are not fully enumerated in this diagram. The process effectively shows how a WSD system would identify all possible meanings for key words in a sentence.](images/ab33798507577283882b258dfe460ec38ebc3c25bc342f4f429e186db6f779c0.jpg)
Figure 11.7 The all-words WSD task, mapping from input words $( x )$ to WordNet senses (y). Figure inspired by Chaplot and Salakhutdinov (2018).   
Fig. 11.8 illustrates the model.

WSD can be a useful analytic tool for text analysis in the humanities and social sciences, and word senses can play a role in model interpretability for word representations. Word senses also have interesting distributional properties. For example a word often is used in roughly the same sense through a discourse, an observation called the one sense per discourse rule (Gale et al., 1992a).

The best performing WSD algorithm is a simple 1-nearest-neighbor algorithm using contextual word embeddings, due to Melamud et al. (2016) and Peters et al. (2018). At training time we pass each sentence in some sense-labeled dataset (like the SemCore or SenseEval datasets in various languages) through any contextual embedding (e.g., BERT) resulting in a contextual embedding for each labeled token. (There are various ways to compute this contextual embedding $\nu _ { i }$ for a token $i$ ; for BERT it is common to pool multiple layers by summing the vector representations of $i$ from the last four BERT layers). Then for each sense $s$ of any word in the corpus, for each of the $n$ tokens of that sense, we average their $n$ contextual representations $\nu _ { i }$ to produce a contextual sense embedding ${ \pmb v } _ { s }$ for $s$ :

$$
\pmb { \mathsf { v } } _ { s } = \frac { 1 } { n } \sum _ { i } \pmb { \mathsf { v } } _ { i } \qquad \forall \pmb { \mathsf { v } } _ { i } \in \mathrm { t o k e n s } ( s )
$$

At test time, given a token of a target word $t$ in context, we compute its contextual embedding $\mathbf { t }$ and choose its nearest neighbor sense from the training set, i.e., the sense whose sense embedding has the highest cosine with t:

$$
\mathrm { s e n s e } ( t ) = \underbrace { \mathrm { a r g m a x ~ c o s i n e } ( \mathbf { t } , \mathbf { v } _ { s } ) } _ { s \in \mathrm { s e n s e s } ( t ) }
$$

# 11.3.2 Contextual Embeddings and Word Similarity

In Chapter 6 we introduced the idea that we could measure the similarity of two words by considering how close they are geometrically, by using the cosine as a similarity function. The idea of meaning similarity is also clear geometrically in the meaning clusters in Fig. 11.6; the representation of a word which has a particular sense in a context is closer to other instances of the same sense of the word. Thus we often measure the similarity between two instances of two words in context (or two instances of the same word in two different contexts) by using the cosine between their contextual embeddings.

![## Image Analysis: efbd0463f5375390d8d3f8b082b7e1b8359f12a4004389f6c0fd015265f938de.jpg

**Conceptual Understanding:**
This image conceptually represents the application of contextual embeddings for Word Sense Disambiguation (WSD) using a nearest-neighbor approach. The main purpose is to illustrate how a neural network (ENCODER) processes a sentence to generate context-specific word embeddings, and then how these embeddings are used to identify the most probable sense of a target word by comparing it to a set of precomputed sense embeddings in a shared vector space. The key idea being communicated is that words with similar meanings in similar contexts will have proximate embeddings in this space, enabling WSD through distance-based comparisons.

**Content Interpretation:**
The image depicts a system and a process flow for Word Sense Disambiguation (WSD).

*   **System Components:**
    *   **Input Sentence:** "I found the jar empty" represents raw textual data that needs to be processed.
    *   **ENCODER:** This blue rounded rectangle is a core system component, likely a deep learning model (e.g., a Transformer-based encoder like BERT, as implied by "Contextual Embeddings" in the section title). Its function is to transform words within a sentence into dense vector representations.
    *   **Contextual Embeddings (c_I, c_found, c_the, c_jar, c_empty):** These represent the output of the ENCODER. Each `c_word` is a vector embedding that captures the meaning of the `word` within the specific context of the input sentence. The highlighting of "c_found" indicates it is the embedding for the target word requiring disambiguation.
    *   **Embedding Space (Grid-patterned rectangle):** This represents a multi-dimensional vector space where words and their senses are mathematically represented as points. The grid suggests a coordinate system.
    *   **Precomputed Sense Embeddings (find_v^4, find_v^5, find_v^1, find_v^9):** These green circles are specific points in the embedding space. They represent the pre-calculated, distinct meanings (senses) of the lemma "find" (from which "found" is derived), each indexed by a superscript number (1, 4, 5, 9) and a part-of-speech tag (v for verb). These are the reference points against which a new embedding is compared.
    *   **Target Word Embedding (Dashed white circle):** This dynamically computed point represents the contextual embedding of "found" (`c_found`) in the current sentence. Its dashed outline suggests it's the item whose position and proximity to others are being evaluated.

*   **Process Flow:**
    1.  The input sentence "I found the jar empty" is tokenized and fed into the "ENCODER".
    2.  The "ENCODER" generates contextual embeddings for each word, such as "c_found" for the word "found".
    3.  This contextual embedding "c_found" is then positioned in the embedding space (dashed white circle).
    4.  The system identifies the precomputed sense embedding (green circle) that is "nearest" to the target word's contextual embedding. The dashed arrow from "c_found" to the dashed white circle, and the proximity of the dashed white circle to "find_v^9", directly illustrate this comparison and selection.

*   **Significance:** The image visually demonstrates that different senses of a word ("find" in this case) occupy distinct, yet potentially clustered, regions in the embedding space. The contextual embedding of a target word, like "found", will ideally be closer to its correct sense's precomputed embedding than to other senses or unrelated words. The visual evidence shows that the embedding of "found" in the context "I found the jar empty" aligns most closely with "find_v^9", indicating that "find_v^9" is the most appropriate sense. This directly supports the idea of using geometric distance in an embedding space for semantic similarity and WSD.

**Key Insights:**
The main takeaways and lessons from this image are:

1.  **Context Matters for Word Embeddings:** The "ENCODER" processes the entire sentence ("I found the jar empty") to produce "Contextual Embeddings" ("c_I", "c_found", "c_the", "c_jar", "c_empty"). This highlights that the meaning of a word is not isolated but influenced by its surrounding words.
2.  **Word Sense Disambiguation via Nearest Neighbor:** The image demonstrates a mechanism for WSD. By mapping the contextual embedding of a target word (the dashed white circle derived from "c_found") into a space populated by "precomputed contextual embeddings for each sense of each word" (green circles labeled "find_v^1", "find_v^4", "find_v^5", "find_v^9"), the closest sense ("find_v^9") is identified. This shows that WSD can be approached as a geometric problem in an embedding space.
3.  **Representing Senses in Embedding Space:** Different senses of a word (e.g., "find_v^1", "find_v^4", "find_v^5", "find_v^9") are represented as distinct points in the embedding space. This provides evidence that contextual embedding models can learn to differentiate between different meanings of a polysemous word.
4.  **Process for WSD:** The sequence of steps (input sentence -> ENCODER -> contextual embedding of target word -> comparison with sense embeddings -> selection of nearest sense) outlines a practical algorithm for WSD.

These insights are directly supported by the textual elements:
*   "ENCODER" processing "I found the jar empty" to produce "c_found" explicitly shows the generation of contextual embeddings.
*   The array of "find_v^X" points represents distinct word senses.
*   The dashed arrow from "c_found" to the dashed white circle, and its proximity to "find_v^9", visually confirms the nearest-neighbor search and sense selection process.

**Document Context:**
This image is highly relevant to Section 11.3.2, "Contextual Embeddings and Word Similarity," as it provides a concrete visual example of how contextual embeddings are utilized in a practical natural language processing task: Word Sense Disambiguation (WSD). It illustrates the "nearest-neighbor algorithm for WSD" mentioned in the text after the figure, showing how contextual embeddings, precomputed for word senses, are used to determine the correct sense of a target word in a given context. The image directly supports the theoretical discussion by demonstrating the process of generating an embedding for a target word in its specific context and then comparing it to a space of known sense embeddings.

**Summary:**
This diagram illustrates a method for Word Sense Disambiguation (WSD), which is the task of determining the correct meaning (sense) of a word in a specific context. It employs an algorithm based on "contextual embeddings" and a "nearest-neighbor" search.

The process unfolds in the following steps:

1.  **Input Sentence Processing:** The process begins with an input sentence, depicted at the bottom as "I found the jar empty". Each word of this sentence is fed sequentially into an "ENCODER".
2.  **Contextual Embedding Generation:** The "ENCODER" (a blue, rounded rectangular component) processes the entire sentence. Its function is to generate "contextual embeddings" for each word. These are vector representations (labeled "c_I", "c_found", "c_the", "c_jar", "c_empty") that capture the meaning of each word *within the specific context of that sentence*. For instance, "c_found" is the contextual embedding for the word "found" in the sentence "I found the jar empty."
3.  **Target Word Embedding Placement:** The contextual embedding for the target word (in this example, "c_found", highlighted with a light brown background) is then conceptually placed into a multi-dimensional "embedding space." This is visualized as a dashed white circle within a larger grid-patterned rectangle at the top.
4.  **Precomputed Sense Embeddings:** Within this same embedding space, there are numerous "precomputed contextual embeddings" for various senses of different words. The diagram specifically shows several senses for the word "find" (the lemma of "found"), each represented by a solid green circle and labeled with the word, its part of speech, and a unique sense identifier (e.g., "find_v^1", "find_v^4", "find_v^5", "find_v^9"). These are essentially reference points for known word meanings.
5.  **Nearest-Neighbor Determination:** The final step involves calculating the "nearest neighbor." The system compares the contextual embedding of the target word ("c_found," represented by the dashed white circle) to all the precomputed sense embeddings (green circles). The sense whose embedding is geometrically closest to the target word's contextual embedding is selected as the most appropriate sense for that word in its current context. In this illustration, the dashed white circle is shown to be closest to "find_v^9", implying that "find_v^9" is the disambiguated sense of "found" in the sentence "I found the jar empty."

In essence, the image demonstrates how a word's meaning in a specific sentence can be determined by finding the closest match between its context-aware embedding and a collection of pre-defined sense embeddings in a shared mathematical space.](images/efbd0463f5375390d8d3f8b082b7e1b8359f12a4004389f6c0fd015265f938de.jpg)
Figure 11.8 The nearest-neighbor algorithm for WSD. In green are the contextual embeddings precomputed for each sense of each word; here we just show a few of the senses for find. A contextual embedding is computed for the target word found, and then the nearest neighbor sense (in this case $\mathbf { f i n d } _ { \nu } ^ { 9 }$ ) is chosen. Figure inspired by Loureiro and Jorge (2019).

Usually some transformations to the embeddings are required before computing cosine. This is because contextual embeddings (whether from masked language models or from autoregressive ones) have the property that the vectors for all words are extremely similar. If we look at the embeddings from the final layer of BERT or other models, embeddings for instances of any two randomly chosen words will have extremely high cosines that can be quite close to 1, meaning all word vectors tend to point in the same direction. The property of vectors in a system all tending to point in the same direction is known as anisotropy. Ethayarajh (2019) defines the anisotropy of a model as the expected cosine similarity of any pair of words in a corpus. The word ‘isotropy’ means uniformity in all directions, so in an isotropic model, the collection of vectors should point in all directions and the expected cosine between a pair of random embeddings would be zero. Timkey and van Schijndel (2021) show that one cause of anisotropy is that cosine measures are dominated by a small number of dimensions of the contextual embedding whose values are very different than the others: these rogue dimensions have very large magnitudes and very high variance.

Timkey and van Schijndel (2021) shows that we can make the embeddings more isotropic by standardizing $\mathbf { \dot { z } }$ -scoring) the vectors, i.e., subtracting the mean and dividing by the variance. Given a set $C$ of all the embeddings in some corpus, each with dimensionality $d$ (i.e., $x \in \mathbb { R } ^ { d } .$ ), the mean vector $\boldsymbol { \mu } \in \mathbb { R } ^ { d }$ is:

$$
\mu = { \frac { 1 } { | C | } } \sum _ { \mathbf { x } \in C } \mathbf { x }
$$

The standard deviation in each dimension $\sigma \in \mathbb { R } ^ { d }$ is:

$$
\sigma = { \sqrt { { \frac { 1 } { | C | } } \sum _ { \mathbf { x } \in C } ( \mathbf { x } - { \boldsymbol { \mu } } ) ^ { 2 } } }
$$

Then each word vector $\pmb { \times }$ is replaced by a standardized version $\mathbf { z }$ :

$$
\mathbf { \Delta z } = \frac { \mathbf { x } - \mu } { \sigma }
$$

One problem with cosine that is not solved by standardization is that cosine tends to underestimate human judgments on similarity of word meaning for very frequent words (Zhou et al., 2022).

In the next section we’ll see the most common use of contextual representations: as representations of words or even entire sentences that can be the inputs to classifiers in the finetuning process for downstream NLP applications.

# 11.4 Fine-Tuning for Classification

The power of pretrained language models lies in their ability to extract generalizations from large amounts of text—generalizations that are useful for myriad downstream applications. There are two ways to make practical use of the generalizations to solve downstream tasks. The most common way is to use natural language to prompt the model, putting it in a state where it contextually generates what we want. We’ll introduce prompting in Chapter 12.

# finetuning

In this section we explore an alternative way to use pretrained language models for downstream applications: a version of the finetuning paradigm from Chapter 10. In the kind of finetuning used for masked language models, we add applicationspecific circuitry (often called a special head) on top of pretrained models, taking their output as its input. The finetuning process consists of using labeled data about the application to train these additional application-specific parameters. Typically, this training will either freeze or make only minimal adjustments to the pretrained language model parameters.

The following sections introduce finetuning methods for the most common kinds of applications: sequence classification, sentence-pair classification, and sequence labeling.

# 11.4.1 Sequence Classification

The task of sequence classification is to classify an entire sequence of text with a single label. This set of tasks is commonly called text classification, like sentiment analysis or spam detection (Chapter 4) in which we classify a text into two or three classes (like positive or negative), as well as classification tasks with a large number of categories, like document-level topic classification.

For sequence classification we represent the entire input to be classified by a single vector. We can represent a sequence in various ways. One way is to take the sum or the mean of the last output vector from each token in the sequence. For BERT, we instead add a new unique token to the vocabulary called [CLS], and prepended it to the start of all input sequences, both during pretraining and encoding. The output vector in the final layer of the model for the [CLS] input represents the entire input sequence and serves as the input to a classifier head, a logistic regression or neural network classifier that makes the relevant decision.

As an example, let’s return to the problem of sentiment classification. to finetuning a classifier for this application involves learning a set of weights, ${ \sf { W } } _ { \sf C }$ , to map the output vector for the [CLS] token— $\mathbf { \cdot h _ { C L S } ^ { L } }$ —to a set of scores over the possible sentiment classes. Assuming a three-way sentiment classification task (positive, negative, neutral) and dimensionality $d$ as the model dimension, ${ \sf { W } } _ { \sf C }$ will be of size $[ d \times 3 ]$ . To classify a document, we pass the input text through the pretrained language model to generate $\mathsf { h } _ { \mathsf { C } \mathsf { L } S } ^ { \mathsf { L } }$ , multiply it by ${ \mathsf { \pmb { W } } } _ { \mathsf { C } }$ , and pass the resulting vector through a softmax.

$$
\mathbf { y } ~ = ~ \mathrm { s o f t m a x } ( \mathbf { h } _ { \mathbf { C } \mathbf { L } \mathbf { S } } ^ { \mathbf { L } } \mathbf { W } _ { \mathbf { C } } )
$$

Finetuning the values in ${ \sf { W } } _ { \sf C }$ requires supervised training data consisting of input sequences labeled with the appropriate sentiment class. Training proceeds in the usual way; cross-entropy loss between the softmax output and the correct answer is used to drive the learning that produces ${ \sf { W } } _ { \sf C }$ .

A key difference from what we’ve seen earlier with neural classifiers is that this loss can be used to not only learn the weights of the classifier, but also to update the weights for the pretrained language model itself. In practice, reasonable classification performance is typically achieved with only minimal changes to the language model parameters, often limited to updates over the final few layers of the transformer. Fig. 11.9 illustrates this overall approach to sequence classification.

![## Image Analysis: 366a538f7d0952ded9e305854439cac15441a77bd08cbeb76d0b43ef40a2ed55.jpg

**Conceptual Understanding:**
This image conceptually represents a deep learning architecture for natural language sequence classification, specifically sentiment analysis. Its main purpose is to illustrate how a Bidirectional Transformer Encoder processes a sequence of text to derive a comprehensive sequence representation, which is then used by a classifier to predict a label (in this case, sentiment). The key ideas communicated are the use of transformers for contextual text understanding, the role of special tokens like [CLS] for sequence-level tasks, and the integration of a simple classification head for final prediction.

**Content Interpretation:**
The image depicts a neural network architecture designed for sequence classification, particularly sentiment analysis. It illustrates the flow of information from input text to a classified output. The core components are the token embedding layer, a bidirectional transformer encoder, and a sentiment classification head. The significance lies in showing how the transformer model, specifically by utilizing the output corresponding to the [CLS] token, can generate a holistic representation of a sequence for downstream classification tasks. The extracted text elements, such as "[CLS]", "entirely predictable and lacks energy" as input, "E" for embeddings, "Bidirectional Transformer Encoder" for the core processing unit, "h_CLS" for the sequence representation, "sentiment classification head" with its internal "W_C" component, and "y" for the final output, explicitly define each stage and its purpose within this deep learning model.

**Key Insights:**
The main takeaways from this image are: 1. Bidirectional Transformer Encoders are a fundamental component for sequence classification tasks in NLP, enabling deep contextual understanding. 2. A special token, `[CLS]`, is often used at the beginning of an input sequence, and its corresponding output embedding (`h_CLS`) from the transformer encoder serves as a powerful, fixed-size representation of the entire sequence for classification. 3. A dedicated classification head, consisting of components like `W_C` (weights), is appended to the transformer's output to make the final prediction (`y`). 4. The model can process an input phrase, such as `"entirely predictable and lacks energy"`, to classify its overall sentiment. All these insights are directly supported by the verbatim textual elements extracted from the diagram: "Bidirectional Transformer Encoder", "[CLS]", "h_CLS", "sentiment classification head", "W_C", "y", and the example input sentence.

**Document Context:**
This image is presented in Section 11.4.1 titled "Sequence Classification," and the accompanying text states, "Figure 11.9 Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier." This confirms the image's role as a direct illustration of a method for sequence classification within the broader topic of transformers in natural language processing. It provides a visual and detailed explanation of the architectural components and data flow involved in performing sentiment classification on a text sequence using a transformer encoder, directly supporting the technical discussion in the document.

**Summary:**
This image illustrates the architecture for sequence classification using a bidirectional transformer encoder, specifically for sentiment analysis. The process begins with an input sequence of tokens, which in this example is the phrase "[CLS] entirely predictable and lacks energy." Each token, including the special [CLS] token, is first converted into an embedding, represented by the 'E' in the pink modules. These modules also feature a small green diamond with a plus sign and an 'i' in a square, which typically signifies the addition of positional embeddings to the token embeddings. These enhanced embeddings are then fed into the 'Bidirectional Transformer Encoder' (the large purple rounded rectangle), which processes the sequence to learn contextual relationships between tokens. After processing, the output vector corresponding to the [CLS] token, labeled 'h_CLS', is extracted. This 'h_CLS' vector serves as a compact representation of the entire input sequence's meaning. This sequence representation ('h_CLS') is then passed to the 'sentiment classification head' (the light green rounded rectangle). Within this head, the 'h_CLS' vector is processed by a component labeled 'W_C' (likely representing a weight matrix in a linear classification layer). Finally, the sentiment classification head produces an output 'y', which is depicted by a bar chart icon, indicating the predicted sentiment (e.g., probabilities for positive, negative, or neutral sentiment). The overall flow demonstrates how a transformer model can take a sequence of words and produce a classification for the entire sequence.](images/366a538f7d0952ded9e305854439cac15441a77bd08cbeb76d0b43ef40a2ed55.jpg)
Figure 11.9 Sequence classification with a bidirectional transformer encoder. The output vector for the [CLS] token serves as input to a simple classifier.

# 11.4.2 Sequence-Pair Classification

As mentioned in Section 11.2.2, an important type of problem involves the classification of pairs of input sequences. Practical applications that fall into this class include paraphrase detection (are the two sentences paraphrases of each other?), logical entailment (does sentence A logically entail sentence B?), and discourse coherence (how coherent is sentence B as a follow-on to sentence A?).

Fine-tuning an application for one of these tasks proceeds just as with pretraining using the NSP objective. During finetuning, pairs of labeled sentences from a supervised finetuning set are presented to the model, and run through all the layers of the model to produce the h outputs for each input token. As with sequence classification, the output vector associated with the prepended [CLS] token represents the model’s view of the input pair. And as with NSP training, the two inputs are separated by the [SEP] token. To perform classification, the [CLS] vector is multiplied by a set of learning classification weights and passed through a softmax to generate label predictions, which are then used to update the weights.

As an example, let’s consider an entailment classification task with the MultiGenre Natural Language Inference (MultiNLI) dataset (Williams et al., 2018). In the task of natural language inference or NLI, also called recognizing textual entailment, a model is presented with a pair of sentences and must classify the relationship between their meanings. For example in the MultiNLI corpus, pairs of sentences are given one of 3 labels: entails, contradicts and neutral. These labels describe a relationship between the meaning of the first sentence (the premise) and the meaning of the second sentence (the hypothesis). Here are representative examples of each class from the corpus:

• Neutral a: Jon walked back to the town to the smithy. b: Jon traveled back to his hometown.   
• Contradicts a: Tourist Information offices can be very helpful. b: Tourist Information offices are never of any help.   
• Entails a: I’m confused. b: Not all of it is very clear to me.

A relationship of contradicts means that the premise contradicts the hypothesis; entails means that the premise entails the hypothesis; neutral means that neither is necessarily true. The meaning of these labels is looser than strict logical entailment or contradiction indicating that a typical human reading the sentences would most likely interpret the meanings in this way.

To finetune a classifier for the MultiNLI task, we pass the premise/hypothesis pairs through a bidirectional encoder as described above and use the output vector for the [CLS] token as the input to the classification head. As with ordinary sequence classification, this head provides the input to a three-way classifier that can be trained on the MultiNLI training corpus.

# 11.5 Fine-Tuning for Sequence Labelling: Named Entity Recognition

In sequence labeling, the network’s task is to assign a label chosen from a small fixed set of labels to each token in the sequence. One of the most common sequence labeling task is named entity recognition.

# 11.5.1 Named Entities

named entity named entity recognition NER

A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including temporal expressions like dates and times, and even numerical expressions like prices. Here’s an example of the output of an NER tagger:

Citing high fuel prices, [ORG United Airlines] said $\mathrm { [ \mathrm { T I M E } }$ Friday] it has increased fares by [MONEY $\$ 6]$ per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect $\mathrm { [ \mathrm { T I M E } }$ Thursday] and applies to most routes where it competes against discount carriers, such as $\operatorname { I } _ { \mathrm { L O C } }$ Chicago] to $\operatorname { I _ { L O C } }$ Dallas] and $\operatorname { I } _ { \mathrm { L O C } }$ Denver] to $\operatorname { I } _ { \mathrm { L O C } }$ San Francisco].

The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 11.10 shows typical generic named entity types. Many applications will also need to use specific entity types like proteins, genes, commercial products, or works of art.

<table><tr><td>Type</td><td>Tag</td><td> Sample Categories</td><td>Example sentences</td></tr><tr><td>People</td><td>PER</td><td> people, characters</td><td> Turing is a giant of computer science.</td></tr><tr><td>Organization</td><td>ORG</td><td> companies, sports teams</td><td> The IPCC warned about the cyclone.</td></tr><tr><td>Location</td><td>LOC</td><td> regions, mountains, seas</td><td> Mt. Sanitas is in Sunshine Canyon.</td></tr><tr><td>Geo-Political Entity</td><td>GPE</td><td>countries, states</td><td> Palo Alto is raising the fees for parking.</td></tr></table>

Figure 11.10 A list of generic named entity types with the kinds of entities they refer to.

Named entity recognition is a useful step in various natural language processing tasks, including linking text to information in structured knowledge sources like Wikipedia, measuring sentiment or attitudes toward a particular entity in text, or even as part of anonymizing text for privacy. The NER task is is difficult because of the ambiguity of segmenting NER spans, figuring out which tokens are entities and which aren’t, since most words in a text will not be named entities. Another difficulty is caused by type ambiguity. The mention Washington can refer to a person, a sports team, a city, or the US government, as we see in Fig. 11.11.

<table><tr><td>[PER Washington] was born into slavery on the farm of James Burroughs. [ORG Washington] went up 2 games to 1 in the four-game series.</td></tr><tr><td>Blair arrived in [LOc Washington] for what may wel be his last state visit. In June, [GPE Washington] passed a primary seatbelt law.</td></tr></table>

Figure 11.11 Examples of type ambiguities in the use of the name Washington.

# 11.5.2 BIO Tagging

One standard approach to sequence labeling for a span-recognition problem like NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us to treat NER like a word-by-word sequence labeling task, via tags that capture both the boundary and the named entity type. Consider the following sentence:

[PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route.

Figure 11.12 shows the same excerpt represented with BIO tagging, as well as variants called IO tagging and BIOES tagging. In BIO tagging we label any token that begins a span of interest with the label B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O. While there is only one O tag, we’ll have distinct B and I tags for each named entity class. The number of tags is thus $2 n + 1$ tags, where $n$ is the number of entity types. BIO tagging can represent exactly the same information as the bracketed notation, but has the advantage that we can represent the task in the same simple sequence modeling way as part-of-speech tagging: assigning a single label $y _ { i }$ to each input word $x _ { i }$ :

<table><tr><td>Words</td><td>IO Label</td><td>BIO Label</td><td>BIOESLabel</td></tr><tr><td>Jane</td><td>I-PER</td><td>B-PER</td><td>B-PER</td></tr><tr><td>Villanueva</td><td>I-PER</td><td>I-PER</td><td>E-PER</td></tr><tr><td>of</td><td>0</td><td>0</td><td>0</td></tr><tr><td>United</td><td>I-ORG</td><td>B-ORG</td><td>B-ORG</td></tr><tr><td> Airlines</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td></tr><tr><td>Holding</td><td>I-ORG</td><td>I-ORG</td><td>E-ORG</td></tr><tr><td>discussed</td><td>0</td><td>0</td><td>0</td></tr><tr><td>the</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Chicago</td><td>I-LOC</td><td>B-LOC</td><td>S-LOC</td></tr><tr><td> route</td><td>0</td><td>0</td><td>0</td></tr><tr><td></td><td>0</td><td>0</td><td>0</td></tr></table>

Figure 11.12 NER as a sequence model, showing IO, BIO, and BIOES taggings.

We’ve also shown two variant tagging schemes: IO tagging, which loses some information by eliminating the B tag, and BIOES tagging, which adds an end tag E for the end of a span, and a span tag S for a span consisting of only one word.

# 11.5.3 Sequence Labeling

In sequence labeling, we pass the final output vector corresponding to each input token to a classifier that produces a softmax distribution over the possible set of tags. For a single feedforward layer classifier, the set of weights to be learned is ${ \sf W } _ { \sf K }$ of size $[ d \times k ]$ , where $k$ is the number of possible tags for the task. A greedy approach, where the argmax tag for each token is taken as a likely answer, can be used to generate the final output tag sequence. Fig. 11.13 illustrates an example of this approach, where ${ \bf y _ { i } }$ is a vector of probabilities over tags, and $k$ indexes the tags.

$$
\begin{array} { l } { \mathbf { \boldsymbol { y } } _ { \mathbf { \hat { i } } } ~ = ~ \operatorname { s o f t m a x } ( \mathbf { \boldsymbol { h } } _ { \mathbf { \hat { i } } } ^ { \mathbf { L } } \mathbf { \boldsymbol { W } } _ { \mathsf { K } } ) } \\ { \mathbf { \boldsymbol { t } } _ { \mathbf { i } } ~ = ~ \operatorname { a r g m a x } _ { k } ( \mathbf { \boldsymbol { y } } _ { i } ) } \end{array}
$$

Alternatively, the distribution over labels provided by the softmax for each input token can be passed to a conditional random field (CRF) layer which can take global tag-level transitions into account (see Chapter 17 on CRFs).

# Tokenization and NER

Note that supervised training data for NER is typically in the form of BIO tags associated with text segmented at the word level. For example the following sentence containing two named entities:

[LOC Mt. Sanitas ] is in [LOC Sunshine Canyon] .

would have the following set of per-word BIO tags.

(11.14) Mt. Sanitas is in Sunshine Canyon . B-LOC I-LOC O O B-LOC I-LOC O

Unfortunately, the sequence of WordPiece tokens for this sentence doesn’t align directly with BIO tags in the annotation:

![## Image Analysis: 7acd6fdf39432bfd0b27152183ae57f32e00e0e080107909448ce85d67939023.jpg

**Conceptual Understanding:**
This image conceptually represents a neural network architecture designed for Named Entity Recognition (NER), which is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as person names, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.

The main purpose of this image is to visually explain how a Bidirectional Transformer Encoder can be utilized to perform sequence labeling for NER. It illustrates the flow from raw input tokens to their corresponding predicted entity tags, showcasing the intermediate processing steps involved in a modern, context-aware NER system.

The key ideas being communicated are:
1.  **Contextual Embeddings:** How a Bidirectional Transformer Encoder processes a sequence of tokens to generate rich, context-dependent representations for each word.
2.  **Sequence Labeling:** The task of assigning a specific label (in this case, an entity tag) to each token in an input sequence.
3.  **Classification Head:** The use of a simple classification layer (the "NER head") on top of the contextual embeddings to make predictions for the NER task.
4.  **BIO Tagging Scheme:** The common method of using "Beginning" (B-), "Inside" (I-), and "Other" (O) tags to delineate named entities, especially multi-word entities.

**Content Interpretation:**
This image illustrates a neural network architecture for performing Named Entity Recognition (NER) on a sequence of text. The process flow involves several key components:

1.  **Input Token Embeddings:** The individual tokens of an input sentence, such as "[CLS]", "Jane", "Villanueva", "of", "United", "Airlines", "Holding", "discussed", are first converted into numerical representations (embeddings). Each input token is depicted as going into a module containing an "E" (representing embedding) and a small 'i' (likely indicating positional encoding or other input features).

2.  **Bidirectional Transformer Encoder:** These token-level embeddings are then fed into a "Bidirectional Transformer Encoder". This is the central processing unit, which analyzes the entire sequence of token embeddings, considering context from both preceding and succeeding tokens, to generate rich, contextualized representations for each input token.

3.  **Contextualized Output Vectors (h_i):** The Transformer Encoder outputs a contextualized vector, labeled "h_i", for each input token. These vectors encapsulate the meaning of each word in the context of the entire sentence.

4.  **NER Head (Classifier):** Each "h_i" vector is then passed to a dedicated "NER head" component. This head functions as a k-way classifier. Inside, a weight matrix "W_K" transforms the "h_i" vector, and a subsequent classification step (represented by a bar chart icon) predicts a probability distribution over the possible NER tags for that specific token.

5.  **Predicted NER Tags (y_i and argmax):** The output of the NER head, "y_i", represents these probabilities. An "argmax" operation is applied to "y_i" to select the tag with the highest probability as the final predicted Named Entity Recognition label for each token.

**Significance of the Data/Information Presented:**

*   The architecture demonstrates a common and effective approach for modern NER tasks, leveraging the power of Transformer models for contextual understanding.
*   The specific output tags (B-PER, I-PER, O, B-ORG, I-ORG) follow the BIO (Beginning, Inside, Other) tagging scheme, which is standard for sequence labeling to delineate multi-token entities.
*   The example sentence "Jane Villanueva of United Airlines Holding discussed" and its predicted tags illustrate the model's ability to identify:
    *   "Jane Villanueva" as a multi-token Person entity (B-PER, I-PER).
    *   "United Airlines Holding" as a multi-token Organization entity (B-ORG, I-ORG, I-ORG).
    *   "of" and "discussed" as non-entity tokens (O).

All extracted text elements support these interpretations by explicitly naming the architectural components ("Bidirectional Transformer Encoder", "NER head", "W_K"), the input/output elements ("h_i", "y_i", "argmax"), and the specific NER tags used for classification.

**Key Insights:**
The main takeaways and lessons from this image are:

1.  **Effectiveness of Bidirectional Transformers for NER:** The "Bidirectional Transformer Encoder" is a core component, highlighting that current state-of-the-art NER systems heavily rely on Transformer architectures for generating highly contextualized representations (h_i) of input tokens. This bidirectional nature allows the model to understand each word in the full context of the sentence, leading to more accurate entity identification.
2.  **Sequence Labeling Approach:** The image clearly demonstrates a sequence labeling paradigm, where each input token is assigned a corresponding output tag. This is evident from the one-to-one mapping from the input tokens ([CLS], Jane, Villanueva, etc.) to the output NER tags (B-PER, I-PER, O, B-ORG, I-ORG, I-ORG, O).
3.  **BIO Tagging Scheme:** The specific output tags (B-PER, I-PER, O, B-ORG, I-ORG) illustrate the widely used BIO (Beginning, Inside, Other) tagging scheme. This scheme is crucial for distinguishing between the start of a multi-word entity, its continuation, and tokens that are not part of any named entity. For instance, "B-PER" and "I-PER" for "Jane Villanueva" show how multi-word entities are handled.
4.  **Modular Architecture:** The system is composed of distinct, interpretable modules: input embeddings (E, i), a powerful encoder ("Bidirectional Transformer Encoder"), and a task-specific "NER head" classifier (W_K, y_i, argmax). This modularity is a common design pattern in deep learning for NLP.

These insights are directly supported by the verbatim textual evidence:
*   "Bidirectional Transformer Encoder" explicitly names the core technology for contextual processing.
*   "NER head" points to the classification module specific to NER.
*   The input sequence "Jane Villanueva of United Airlines Holding discussed" and its corresponding output tags "B-PER", "I-PER", "O", "B-ORG", "I-ORG", "I-ORG", "O" provide concrete evidence of the sequence labeling and BIO tagging in action, demonstrating how the model successfully identifies both person and organization entities.

**Document Context:**
This image directly supports the "Tokenization and NER" section by visually demonstrating a state-of-the-art neural network architecture for performing Named Entity Recognition. It illustrates how an input sentence, after tokenization, is processed through a complex model to identify and label named entities. The figure provides a concrete example of the "sequence labeling for named entity recognition with a bidirectional transformer encoder" mentioned in the accompanying text, showing how "the output vector for each input token is passed to a simple k-way classifier" (represented by the NER head with W_K and argmax). It visually explains the theoretical concepts discussed in the document regarding how modern NLP models perform NER by leveraging contextual embeddings.

**Summary:**
This diagram illustrates the process of sequence labeling for Named Entity Recognition (NER) using a Bidirectional Transformer Encoder.

The process begins with an input sequence of tokens at the bottom: "[CLS]", "Jane", "Villanueva", "of", "United", "Airlines", "Holding", and "discussed".

Each of these input tokens is first converted into an initial representation. This is shown by the layer directly above the input tokens, where each token is processed through an embedding layer (labeled "E") and combined with positional information (indicated by the small green square with 'i'). This forms the input for the next stage.

These initial token representations are then fed into a large, central component: the "Bidirectional Transformer Encoder." This encoder is responsible for processing the entire sequence of tokens simultaneously, considering the context of each word from both its preceding and succeeding words. This bidirectional processing allows the model to build rich, contextualized representations for every token in the input sentence.

From the "Bidirectional Transformer Encoder," an output vector, denoted as "h_i", is generated for each input token. These "h_i" vectors represent the contextualized embedding of each token and are passed upwards to the "NER head" layer.

The "NER head" is the final classification layer for Named Entity Recognition. For each "h_i" vector, an individual "NER head" component processes it. This component includes a weight matrix, "W_K", which transforms the contextual vector into a set of scores for different possible entity tags. This is followed by a classification step (symbolized by the bar chart icon), which effectively assigns a probability to each possible NER tag for that specific token.

The output of each "NER head" component, denoted "y_i", represents the probability distribution over the possible NER tags. An "argmax" operation is then applied to "y_i" to select the tag with the highest probability, yielding the final predicted NER tag for each token.

The ultimate output is a sequence of predicted Named Entity Recognition tags at the top:
*   "B-PER" for "Jane" (Beginning of a Person entity)
*   "I-PER" for "Villanueva" (Inside a Person entity, continuing from "Jane")
*   "O" for "of" (Other, not part of a named entity)
*   "B-ORG" for "United" (Beginning of an Organization entity)
*   "I-ORG" for "Airlines" (Inside an Organization entity, continuing from "United")
*   "I-ORG" for "Holding" (Inside an Organization entity, continuing from "Airlines")
*   "O" for "discussed" (Other, not part of a named entity)

This entire pipeline effectively demonstrates how a modern neural network leverages powerful contextual encoding from a Transformer to accurately identify and classify named entities within a given text sequence.](images/7acd6fdf39432bfd0b27152183ae57f32e00e0e080107909448ce85d67939023.jpg)
Figure 11.13 Sequence labeling for named entity recognition with a bidirectional transformer encoder. The output vector for each input token is passed to a simple $\mathbf { k }$ -way classifier.

’Mt’, ’.’, ’San’, ’##itas’, ’is’, ’in’, ’Sunshine’, ’Canyon’ ’.’

To deal with this misalignment, we need a way to assign BIO tags to subword tokens during training and a corresponding way to recover word-level tags from subwords during decoding. For training, we can just assign the gold-standard tag associated with each word to all of the subword tokens derived from it.

For decoding, the simplest approach is to use the argmax BIO tag associated with the first subword token of a word. Thus, in our example, the BIO tag assigned to “Mt” would be assigned to “Mt.” and the tag assigned to “San” would be assigned to “Sanitas”, effectively ignoring the information in the tags assigned to “.” and “##itas”. More complex approaches combine the distribution of tag probabilities across the subwords in an attempt to find an optimal word-level tag.

# 11.5.4 Evaluating Named Entity Recognition

Named entity recognizers are evaluated by recall, precision, and $\mathbf { F } _ { 1 }$ measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and $F$ -measure is the harmonic mean of the two.

To know if the difference between the $\mathrm { F } _ { 1 }$ scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test (Section 4.9).

For named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 11.12, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response.

The fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions.

# 11.6 Summary

This chapter has introduced the bidirectional encoder and the masked language model. Here’s a summary of the main points that we covered:

• Bidirectional encoders can be used to generate contextualized representations of input embeddings using the entire input context.   
• Pretrained language models based on bidirectional encoders can be learned using a masked language model objective where a model is trained to guess the missing information from an input.   
• The vector output of each transformer block or component in a particular token column is a contextual embedding that represents some aspect of the meaning of a token in context.   
• A word sense is a discrete representation of one aspect of the meaning of a word. Contextual embeddings offer a continuous high-dimensional model of meaning that is richer than fully discrete senses.   
The cosine between contextual embeddings can be used as one way to model the similarity between two words in context, although some transformations to the embeddings are required first.   
• Pretrained language models can be finetuned for specific applications by adding lightweight classifier layers on top of the outputs of the pretrained model.   
• These applications can include sequence classification tasks like sentiment analysis, sequence-pair classification tasks like natural language inference, or sequence labeling tasks like named entity recognition.

# Bibliographical and Historical Notes

History TBD.

# CHAPTER 12 Model Alignment, Prompting,and In-Context Learning

“Hal,” said Bowman, now speaking with an icy calm. “I am not incapacitated. Unless you obey my instructions, I shall be forced to disconnect you.” Arthur C. Clarke

prompts

In this chapter we show how to get LLMs to do tasks for us simply by talking to them. To get an LLM to translate a sentence, outline a talk, or draft a work email, we’ll simply describe what we want in natural language. We call these instructions we give to language models prompts.

demonstrations

Prompting relies on contextual generation. Given the prompt as context, the language model generates the next token based on its token probability, conditioned on the prompt: $P ( w _ { i } | w _ { < i } )$ . A prompt can be a question (like “What is a transformer network?”), possibly in a structured format (like “Q: What is a transformer network? A:”), or can be an instruction (like “Translate the following sentence into Hindi: ‘Chop the garlic finely’”). A prompt can also contain demonstrations, examples to help make the instructions clearer, (like “Give the sentiment of the following sentence. Example Input: “I really loved Taishan Cuisine.” Output: positive”.) As we’ll see, prompting can be applied to inherently generative tasks (like summarization and translation) as well as to ones more naturally thought of as classification tasks.

in-contextlearning

Prompts get language models to generate text, but they also can be viewed as a learning signal, because these demonstrations can help language models learn to perform novel tasks. For this reason we also refer to prompting as in-contextlearning—learning that improves model performance or reduces some loss but does not involve gradient-based updates to the model’s underlying parameters.

# instruction tuning

But LLMs as we’ve described them so far turn out to be bad at following instructions. Pretraining isn’t sufficient to make them helpful. We’ll introduce instruction tuning, a technique that helps LLMs learn to correctly respond to instructions by finetuning them on a corpus of instructions with their corresponding response.

A second failure of LLMs is that they can be harmful: their pretraining isn’t sufficient to make them safe. Readers who know Arthur C. Clarke’s 2001: A Space Odyssey or the Stanley Kubrick film know that the quote above comes in the context that the artificial intelligence Hal becomes paranoid and tries to kill the crew of the spaceship. Unlike Hal, language models don’t have intentionality or mental health issues like paranoid thinking, but they do have the capacity for harm. Pretrained language models can say things that are dangerous or false (like giving unsafe medical advice) and they can verbally attack users or say toxic or hateful things.

preference alignment

Dealing with safety can be done partly by adding safety training into instruction tuning. But an important aspect of safety training is a second technique, preference alignment (often implemented, as we’ll see, with the RLHF or DPO algorithms) in which a separate model is trained to decide how much a candidate response aligns with human preferences. Together we refer to instruction tuning and preference alignment as model alignment. The intuition is that we want the learning objectives of models to be aligned with the goals of the humans that use them.

# 12.1 Prompting

# prompt

# prompt engineering

A prompt is a text string that a user issues to a language model to get the model to do something useful. In prompting, the user’s prompt string is passed to the language model, which iteratively generates tokens conditioned on the prompt. Thus the prompt creates a context that guides LLMs to generate useful outputs to achieve some user goal. The process of finding effective prompts for a task is known as prompt engineering.

Let’s see how to prompt a language model to solve a simple sentiment classification task, Consider this hotel review from the BLT corpus (Salvetti et al., 2016):

# Sample Hotel Review

Did not like the service that I was provided, when I entered the hotel. I also did not like the area, in which the hotel was located. Too much noise and events going on for me to feel relax.

We can get the model to classify the sentiment of this text by taking the text and appending an incomplete statement to the review like In short, our stay was:

# A prompt consisting of a review plus an incomplete statement

Did not like the service that I was provided, when I entered the hotel. I also did not like the area, in which the hotel was located. Too much noise and events going on for me to feel relax. In short, our stay was

We then have an LLM complete the statement by generating a token conditioned on the prompt so far, and then generating the next token (conditioned on the prompt plus the new token), and so forth. Here are a few responses from a language model via continued generation starting with the prompt as context.

# Hotel Review Completions

Did not like the service that I was provided, when I entered the hotel. I also did not like the area, in which the hotel was located. Too much noise and events going on for me to feel relaxed. In short our stay was

not a pleasant one. The staff at the front desk were not welcoming or friendly, and seemed disinterested in providing good customer service.

uncomfortable and not worth the price we paid. We will not be returning to this hotel.

As we can see, the overall negative context of the review results in negative completions. We could easily map these completions to the class we are trying to predict, perhaps via some predefined mappings, like $\{ { \mathrm { e x c e l l e n t } } \to { \mathrm { p o s i t i v e } } \}$ , $\{ \mathtt { d i d }$ not $\mathrm { 1 i k e }  \mathrm { n e g a t i v e } \}$ , and so on.

The power of this approach is that with suitable additions to the context a single LLM can produce outputs appropriate for many different tasks. For example, given a review we might want any of the following:

• A summary,   
• Whether the review was truthful or likely to have been fabricated, • A translation to another language.

LLMs have a striking ability to perform tasks like these, needing just the appropriate contextual nudge to get the LLM to generate the desired output.

templates

If we want to solve general tasks like summarization or translation, we don’t want to have to create a new prompt each time we do the task. Instead the first step in prompting is to design one or more templates: task-specific prompting text along with slots for the particular input that is being processed.

Consider the following templates for a variety of tasks:

# Basic Prompt Templates

Summarization {input} ; tldr;   
Translation {input} ; translate to French:   
Sentiment {input}; Overall, it was   
Fine-Grained- {input}; What aspects were important in this review? Sentiment

Each template consists of an input text, designated as $\{ { \mathrm { i n p u t } } \}$ , followed by a verbatim prompt to be passed to an LLM. These templates are applied to inputs to create filled prompts – instantiated prompts suitable for use as inputs to an LLM. Fig. 12.1 illustrates filled prompts for these templates using our earlier hotel review, along with sample outputs from an LLM:

Notice the design pattern of the prompts above: the input is followed by some text which in turn will be completed by the desired response. This style, with the instruction at the end, is common in prompting because it helpfully constrains the generation. Consider, by contrast, the prompt in Example 12.1.

Translate English to French: Did not like the service that I was provided!

This prompt doesn’t do a good job of constraining possible continuations. Instead of a French translation, models given this prompt may instead generate another sentence in English that simply extends the English review. Prompts need to be designed unambiguously, so that any reasonable continuation would accomplish the desired task (Reynolds and McDonell, 2021).

An even more constraining style of prompt can specify the set of possible answers in the prompt. For example here is a prompt template to do sentiment analysis that prespecifies the potential answers:

# A prompt consisting of a review plus an incomplete statement

Human: Do you think that “input” has negative or positive sentiment?   
Choices:   
(P) Positive   
(N) Negative

Assistant: I believe the best answer is: (

Figure 12.1 LLM outputs for simple prompts for sentiment, summarization and translation for an input text.   

<table><tr><td></td><td>Original Review($INPUT)|Did not like the service that I was provided, when I entered the hotel. I also did not like the area，in which the hotel was located. Too much noise and events going on for me to feel relax and away from the city life.</td></tr><tr><td>Sentiment</td><td>Prompt: $INPUT + In short，our stay was Output: not enjoyable</td></tr><tr><td>Fine-grained Sentiment</td><td>Prompt: $INPUT + These aspects were important to thereviewer: Output: 1. Poor service 2. Unpleasant location 3.Noisy and busy area</td></tr><tr><td>Summarization</td><td>Prompt: $INPUT + tl;dr Output:I had a bad experience with the hotel&#x27;s service and the location was loud and busy.</td></tr><tr><td>Translation</td><td>Prompt: $INPUT + Translate this to French Output:Je n&#x27;ai pas aimé le service qui m&#x27;a été offert lorsque je suis entré dans l&#x27;hótel. Je n&#x27;ai également pas aimé la zone dans laquelle se trouvait l&#x27;hótel. Trop de bruit et d&#x27;événements pour que je me sente détendu et loin de la vie citadine.</td></tr></table>

This prompt uses a number of more sophisticated prompting characteristics. It specifies the two allowable choices (P) and (N), and ends the prompt with the open parenthesis that strongly suggests the answer will be (P) or (N). Note that it also specifies the role of the language model as an assistant.

We can do even more with prompts. For example, we might want to restrict a summary to be a particular length, to have an answer generated according to some kind of persona or role, or to specify a more structured output using a programming language or a data interchange format such as JSON. Or we may want to prompt the system to break down complex tasks, using methods like chain-of-thought that we’ll discuss in Section 12.4. All of these kinds of instructions go beyond simple prompting and require further LLM finetuning to enable them to follow instructions. We’ll return to this notion of instruction tuning in Section 12.3.

In summary, we prompt an LM by transforming each task into a form that is amenable to contextual generation by an LLM, as follows:

# template

1. For a given task, develop a a task-specific template that has a free parameter for the input text.   
2. Given that input and the task-specific template, the input is used to instantiate a filled prompt that is then passed to a pretrained language model.   
3. Autoregressive decoding is then used to generate a sequence of token outputs.   
4. The output of the model can either be used directly as the desired output (as in the case of naturally generative tasks such as translation or summarization), or a task-appropriate answer can be extracted from the generated output (as in the case of classification).

# 12.1.1 Learning from Demonstrations: Few-Shot Prompting

demonstrations few-shot zero-shot

It’s often possible to improve a prompt by including some labeled examples in the prompt template. We call such examples demonstrations. The task of prompting with examples is sometimes called few-shot prompting, as contrasted with zeroshot prompting which means instructions that don’t include labeled examples.

Fig. 12.2 illustrates a few-shot example from an extractive question answering task. The context combines the task definition along with three gold-standard question and answer pairs from the training set.

Definition: This task is about writing a correct answer for the reading comprehension task. Based on the information provided in a given passage, you should identify the shortest continuous text span from the passage that serves as an answer to the given question. Avoid answers that are incorrect or provides incomplete justification for the question.

Passage: Beyonce Giselle Knowles-Carter (born September 4, 1981) is an American singer, ´ songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny’s Child. Managed by her father, Mathew Knowles, the group became one of the world’s best-selling girl groups of all time. Their hiatus saw the release of Beyonce’s debut album, Dangerously in Love (2003), which established her as a solo artist ´ worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles “Crazy in Love” and “Baby Boy”.

# Examples:

Q: In what city and state did Beyonce grow up? ´   
A: Houston, Texas   
Q: What areas did Beyonce compete in when she was growing up? ´   
A: singing and dancing   
Q: When did Beyonce release Dangerously in Love? ´   
A: 2003   
Q: When did Beyonce start becoming popular? ´   
A:

How Many Demonstrations? The number of demonstrations doesn’t have to be large. A small number of randomly selected labeled examples used as demonstrations can be sufficient to improve performance over the zero-shot setting. Indeed, the largest performance gains in few-shot prompting tends to come from the first training example, with diminishing returns for subsequent demonstrations. This is in contrast with finetuning of specialized classifier heads that we saw in Chapter 11 where it helps to have lots of examples.

Why isn’t it useful to have more demonstrations? The reason is that the primary benefit in examples is to demonstrate the task to be performed to the LLM and the format of the sequence, not to provide relevant information as to the right answer for any particular question. In fact, demonstrations that have incorrect answers can still improve a system (Min et al., 2022; Webson and Pavlick, 2022). Adding too many examples seems to cause the model to overfit to details of the exact examples chosen and generalize poorly.

How to Select Demonstrations? Demonstrations are generally created by formatting examples drawn from a labeled training set. There are some heuristics about what makes a good demonstration. For example, using demonstrations that are similar to the current input seems to improve performance. It can thus be useful to dynamically retrieve demonstrations for each input, based on their similarity to the current example (for example, comparing the embedding of the current example with embeddings of each of the training set example to find the best top- $T$ ).

But more generally, the best way to select demonstrations from the training set is programmatically: choosing the set of demonstrations that most increases task performance of the prompt on a test set. Task performance for sentiment analysis or multiple-choice question answering can be measured in accuracy; for machine translation with chrF, and for summarization via Rouge. Systems like DSPy (Khattab et al., 2024), a framework for algorithmically optimizing LM prompts, can automatically find the optimum set of demonstrations to include by searching through the space of possible demonstrations to include. We’ll return to automatic prompt optimization in Section 12.5.

# 12.1.2 In-Context Learning and Induction Heads

As a way of getting a model to do what we want, prompting is fundamentally different than pretraining. Learning via pretraining means updating the model’s parameters by using gradient descent according to some loss function. But prompting with demonstrations can teach a model to do a new task. The model is learning something as it processes the prompt.

Even without demonstrations, we can think of the process of prompting as a kind of learning. For example, the further a model gets in a prompt, the better it tends to get at predicting the upcoming tokens. The information in the context is helping give the model more predictive power.

The term in-context learning was first proposed by Brown et al. (2020) in their introduction of the GPT3 system, to refer to either of these kinds of learning that language models do from their prompts. In-context learning means language models learning to do new tasks, better predict tokens, or generally reduce their loss during the forward-pass at inference-time, without any gradient-based updates to the model’s parameters.

How does in-context learning work? While we don’t know for sure, there are some intriguing ideas. One hypothesis is based on the idea of induction heads (Elhage et al., 2021; Olsson et al., 2022). Induction heads are the name for a circuit, which is a kind of abstract component of a network. The induction head circuit is part of the attention computation in transformers, discovered by looking at mini language models with only 1-2 attention heads.

The function of the induction head is to predict repeated sequences. For example if it sees the pattern AB...A in an input sequence, it predicts that B will follow, instantiating the pattern completion rule AB... $\mathsf { A } \to \mathsf { B }$ . It does this by having a prefix matching component of the attention computation that, when looking at the current token A, searches back over the context to find a prior instance of A. If it finds one, the induction head has a copying mechanism that “copies” the token B that followed the earlier A, by increasing the probability the B will occur next. Fig. 12.3 shows an example.

![## Image Analysis: cc2698c74f719c7431497ade79676df297d1c6584fe047db8bba604d07f204e0.jpg

**Conceptual Understanding:**
This image conceptually represents the internal workings of a language model, specifically focusing on how an 'induction head' processes and predicts words within a given text sequence. Its main purpose is to visually illustrate two key mechanisms: 'Prefix matching' and 'Copying'. These mechanisms enable the model to understand the relationship between words and leverage previously seen words for future predictions. The image communicates the idea that language processing involves identifying patterns (like an adjective-noun pairing) and recognizing repetitions to generate coherent text or make accurate predictions.

**Content Interpretation:**
This image illustrates two fundamental mechanisms, 'Prefix matching' and 'Copying', that a language model or an 'induction head' might employ to process and predict words within a sentence. The diagram uses the specific sentence 'She owns vintage cars. He dreams of owning vintage cars' as a concrete example.

'Prefix matching' is depicted as the mechanism connecting an adjective ('vintage') to the noun it modifies ('cars') in two separate instances. This suggests that the model recognizes a pattern where 'vintage' is followed by 'cars', inferring a syntactic or semantic relationship.

'Copying' is shown as the mechanism by which the prior occurrence of a word ('cars' in the first clause) influences or predicts its subsequent occurrence ('cars' in the second clause). The dashed border around the second 'cars' likely signifies that it is a predicted token, leveraging the 'copying' mechanism from the earlier identical token.

**Key Insights:**
The main takeaway from this image is that language models, specifically components like 'induction heads', utilize distinct mechanisms such as 'Prefix matching' and 'Copying' to understand context and predict subsequent words in a sequence. 

'Prefix matching' (evidenced by the arrows from 'vintage' to 'cars' in both instances) highlights the model's ability to identify and leverage local grammatical and semantic relationships between words, like an adjective modifying a noun. This suggests an understanding of word co-occurrence patterns.

'Copying' (evidenced by the arrow from the first 'cars' to the second 'cars') demonstrates the model's capacity to maintain coherence and leverage repetition by re-using or predicting words that have recently appeared in similar contexts. This is crucial for tasks like anaphora resolution or maintaining consistent vocabulary within a text. The example 'vintage cars ... vintage cars' explicitly illustrates these two powerful mechanisms at play.

**Document Context:**
This image directly supports the discussion in Section 12.1.2, titled 'In-Context Learning and Induction Heads'. It visually explains the core functionalities of an induction head in the context of sequence prediction. As mentioned in the surrounding text, an induction head identifies the initial occurrence of 'vintage', uses 'prefix matching' to attend to 'cars', and then employs a 'copying mechanism' to predict 'cars' again. The diagram, with its explicit labels for 'Prefix matching' and 'Copying', serves as a clear visual aid to understand these specific operations of an induction head within the provided sentence.

**Summary:**
The image displays a sequence of words arranged horizontally in individual rectangular boxes, illustrating two mechanisms: 'Prefix matching' and 'Copying'. The full sentence depicted is 'She owns vintage cars. He dreams of owning vintage cars'.

The process begins with the word 'She', followed by 'owns'. Next is 'vintage' (highlighted in light red), which is then followed by 'cars' (highlighted in light purple). A period '.' separates the first clause from the second. The second clause starts with 'He', then 'dreams', 'of', and 'owning'. This is followed by 'vintage' (again highlighted in light red) and then 'cars' (highlighted in light purple with a dashed border).

An arrow labeled 'Prefix matching' originates from the first 'vintage' box, points to the first 'cars' box, then continues from the second 'vintage' box and points to the second 'cars' box. This indicates that the relationship between 'vintage' and 'cars' is established through prefix matching in both instances.

Another arrow, labeled 'Copying', originates from the first 'cars' box and points to the second 'cars' box (the one with the dashed border). This illustrates that the second occurrence of 'cars' is influenced by or copied from the first occurrence of 'cars'.](images/cc2698c74f719c7431497ade79676df297d1c6584fe047db8bba604d07f204e0.jpg)
Figure 1: In thFigure 12.3 sequence “...vintage cars ... vintage”, an induction head identifies the initial occurrence of “vintage”,An induction head looking at vintage uses the prefix matching mechanism to attends to the subsequent word “cars” for prefix matching, and predicts “cars” as the next word through the copyingfind a prior instance of vintage, and the copying mechanism to predict that cars will occur mechanism.again. Figure from Crosbie and Shutova (2022).

termines each head’s independent output for the 4.2 Identifying Induction HeadsOlsson et al. (2022) propose that a generalized fuzzy version of this pattern comcurrent token.pletion rule, implementing a rule like $\smash { \mathsf { A } ^ { * } \mathsf { B } ^ { * } , \ldots , \mathsf { A } \to \mathsf { B } }$ induction, where $\mathbf { A } ^ { * } \approx \mathbf { A }$ in moand $\mathbf { B } ^ { * } \approx \mathbf { B }$ ea-(by $\approx$ (2021) discovered a distinct behaviour in certain sure the ability of all attention heads to performwe mean they they are semantically similar in some way), might be responsible attention heads, which they named induction heads.     for in-context learning. Suggestive evidence for their hypothesis comes from CrosThis behaviour emerges when these heads process fix matching scores outlined by Bansal et al. (2023).bie and Shutova (2022), who show that ablating induction heads causes in-context sequences of the form "[A] [B] ... [A] → ". In We argue that focusing solely on prefix matchinglearning performance to decrease. Ablation is originally a medical term meaning scores is sufficient for our analysis, as high pre-the removal of something. We use it in NLP interpretability studies as a tool for occurrence of the current token [A]. This behaviour     testing causal effects; if we knock out a hypothesized cause, we would expect the is termed prefix matching. The OV circuit subse- copying capabilities (Bansal et al., 2023). We gen-effect to disappear. Crosbie and Shutova (2022) ablate induction heads by first findquently increases the output logit of the [B] token, erate a sequence of 50 random tokens, excludinging attention heads that perform as induction heads on random input sequences, and the 4% most common and least common tokens.then zeroing out the output of these heads by setting certain terms of the output matrix $\boldsymbol { \mathsf { W } } ^ { 0 }$ to zero. Indeed they find that ablated models are much worse at in-context 4 Methods culated by averaging the attention values from eachlearning: they have much worse performance at learning from demonstrations in the prompts.

# ablating

# models, namely Llama-3-8B and InternLM2-20B(Cai et al., 2024), both of which are based on the The prefix matcshown in Figure 2.12.2 Post-training and Model Alignment

tion mechanisms (Ainslie et al., 2023) to enhance distributed across various layers. In the Llama-3-With simple prompting, LLMs have been successfully applied to a range of appliwith 32 attention heads and it uses a query groupcations without the need to update the parameters in the underlying models. Nevsize of 4 attention heads. It has shown superior cialisation in prefix matching, and some heads haveertheless, there are limits to how much can be expected from a model whose sole performance compared to its predecessors, even high scores of up to 0.98.training objective is to predict the next word from large amounts of pretraining text. the larger Llama-2 models.To see this, consider the following failed examples of following instructions from tention heads each, uses a query group size of 6early work with GPT (Ouyang et al., 2022).

<table><tr><td>Prompt: Explain the moon landing to a six year old in a few sentences. Output: Explain the theory of gravity to a 6 year old.</td></tr><tr><td></td></tr><tr><td>Prompt: Translate to French: The small dog Output: The small dog crossed the road.</td></tr></table>

https://ai.meta.com/blog/meta-llama-3/ however, our analysis employs prefix-matching scores as aHere, the LLM ignores the intent of the request and relies instead on its natural NeedleInAHaystack simplicity throughout the rest of the paper.inclination to autoregressively generate continuations consistent with its context. In the first example, it outputs a text somewhat similar to the original request, and in the 4second it provides a continuation to the given input, ignoring the request to translate. LLMs are not sufficiently helpful: they need extra training to increase their abilities to follow textual instructions.

A deeper problem is that LLMs can simultaneously be too harmful. Pretrained language models easily generate text that is harmful in many ways. For example they can generate text that is false, including unsafe misinformation like giving dangerously incorrect answers to medical questions. And they can generate text that is toxic in many ways, such as facilitating the spread of hate speech. Gehman et al. (2020) show that even completely non-toxic prompts can lead large language models to output hate speech and abuse their users. Or language models can generate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020; Sheng et al., 2019) about many demographic groups.

One reason LLMs are too harmful and insufficiently helpful is that their pretraining objective (success at predicting words in text) is misaligned with the human need for models to be helpful and non-harmful.

# model alignment

In an attempt to address these two problems, language models generally include two additional kinds of training for model alignment: methods designed to adjust LLMs to better align them to human needs for models to be helpful and non-harmful. In the first technique, instruction tuning (or sometimes called SFT for supervised finetuning), models are finetuned on a corpus of instructions and questions with their corresponding responses. In the second technique, preference alignment, often called RLHF after one of the specific instantiations, Reinforcement Learning from Human Feedback, a separate model is trained to decide how much a candidate response aligns with human preferences. This model is then used to finetune the base model.

base model aligned post-training

We’ll use the term base model to mean a model that has been pretrained but hasn’t yet been aligned either by instruction tuning or RLHF. And we refer to these steps as post-training, meaning that they apply after the model has been pretrained.

# 12.3 Model Alignment: Instruction Tuning

# Instruction tuning

Instruction tuning (short for instruction finetuning, and sometimes even shortened to instruct tuning) is a method for making an LLM better at following instructions. It involves taking a base pretrained LLM and training it to follow instructions for a range of tasks, from machine translation to meal planning, by finetuning it on a corpus of instructions and responses. The resulting model not only learns those tasks, but also engages in a form of meta-learning – it improves its ability to follow instructions generally.

# SFT

Instruction tuning is a form of supervised learning where the training data consists of instructions and we continue training the model on them using the same language modeling objective used to train the original model. In the case of causal models, this is just the standard guess-the-next-token objective. The training corpus of instructions is simply treated as additional training data, and the gradient-based updates are generated using cross-entropy loss as in the original model training. Even though it is trained to predict the next token (which we traditionally think of as self-supervised), we call this method supervised fine tuning (or SFT) because unlike in pretraining, each instruction or question in the instruction tuning data has a supervised objective: a correct answer to the question or a response to the instruction.

How does instruction tuning differ from the other kinds of finetuning introduced in Chapter 10 and Chapter 11? Fig. 12.4 sketches the differences. In the first example, introduced in, Chapter 10 we can finetune as a way of adapting to a new domain by just continuing pretraining the LLM on data from a new domain. In this method all the parameters of the LLM are updated.

![## Image Analysis: b1f0b934e2f39a42c9ecbcb520f14024811b282c273945a3c87358257f0c83ff.jpg

**Conceptual Understanding:**
This image conceptually illustrates different methodologies for adapting or specializing large language models (LLMs) after their initial pretraining phase. It compares four distinct finetuning paradigms based on their input data, training process, objective functions, and the scope of their inference capabilities.

The main purpose of the image is to visually differentiate Instruction Tuning (SFT) from other common finetuning techniques. It highlights how each method processes a 'Pretrained LLM' to achieve different outcomes, particularly emphasizing Instruction Tuning's ability to generalize to 'unseen tasks' by leveraging 'Supervised instructions' and 'Instruction tuning on diverse tasks', in contrast to other methods that typically perform inference on the specific 'finetuning domain' or 'finetuning task' they were trained on. The diagram serves to clarify the architectural and methodological distinctions that lead to these varying inference capabilities.

**Content Interpretation:**
The image illustrates four distinct methodologies for finetuning Large Language Models (LLMs): 'Finetuning as Continued Pretraining', 'Parameter Efficient Finetuning (e.g., LoRA)', 'MLM Finetuning', and 'Instruction Tuning (SFT)'. Each method is presented as a sequential process from 'Pretraining' to 'Finetuning' and finally to 'Inference'.

**Finetuning as Continued Pretraining** depicts a scenario where a 'Pretrained LLM' is further trained ('Continue training all parameters on finetuning domain') using 'Data from finetuning domain' with a 'Next word prediction objective'. The significance is that this method continues the original pretraining paradigm but on new data, leading to inference 'On finetuning domain'.

**Parameter Efficient Finetuning (e.g., LoRA)**, also starting from a 'Pretrained LLM', focuses on efficiency by training 'only new parameters on finetuning domain' using 'Data from finetuning domain' and a 'Next word prediction objective'. The visual representation of the finetuned model with '+', 'A', and 'B' highlights the addition of new, smaller components (parameters A and B) that are trained, while the bulk of the pretrained model remains frozen. Inference is still conducted 'On finetuning domain', implying adaptation to a specific data distribution without retraining the entire model.

**MLM Finetuning** (Masked Language Model Finetuning) takes a 'Pretrained LLM' and adapts it using 'Supervised data from task'. The process 'Train only classification head on finetuning task' with a 'Task specific loss' indicates a focus on downstream classification tasks rather than general language modeling. The significance is that this targets specific task performance by modifying only a part of the model, with inference occurring 'On finetuning task'.

**Instruction Tuning (SFT)**, starting with a 'Pretrained LLM', is distinct in its input: 'Supervised instructions'. The process is 'Instruction tuning on diverse tasks' with a 'Next word prediction objective'. This method's significance lies in its goal to align the LLM with human instructions, enabling better generalization. Consequently, inference is performed 'On unseen tasks', highlighting its ability to follow instructions on novel tasks it hasn't specifically been trained on during finetuning.

**Key Insights:**
The image provides several key takeaways regarding LLM finetuning:

1.  **Variety of Finetuning Approaches:** There are multiple strategies for adapting a 'Pretrained LLM' to specific needs, each with different inputs, training objectives, and outcomes. The image visually categorizes these into 'Finetuning as Continued Pretraining', 'Parameter Efficient Finetuning (e.g., LoRA)', 'MLM Finetuning', and 'Instruction Tuning (SFT)'.

2.  **Full vs. Partial Parameter Training:** Finetuning can involve training 'all parameters' (as in 'Finetuning as Continued Pretraining') or 'only new parameters' ('Parameter Efficient Finetuning') or even 'only classification head' ('MLM Finetuning'). This highlights a trade-off between computational cost/model size and performance for specific tasks.

3.  **Data Type and Objective Impact Finetuning:** The choice of finetuning data is crucial. 'Data from finetuning domain' is used for continued pretraining and parameter-efficient methods, while 'Supervised data from task' and 'Supervised instructions' are used for MLM finetuning and instruction tuning, respectively. Correspondingly, objectives can be 'Next word prediction objective' or 'Task specific loss'. This demonstrates how different objectives and data shape the finetuned model's capabilities.

4.  **Instruction Tuning Enables Generalization to Unseen Tasks:** A critical insight is that 'Instruction Tuning (SFT)', achieved by 'Instruction tuning on diverse tasks' using 'Supervised instructions' with a 'Next word prediction objective', uniquely results in inference 'On unseen tasks'. This explicitly contrasts with the other methods that typically perform inference 'On finetuning domain' or 'On finetuning task', suggesting SFT's superior generalization ability for following instructions on novel tasks. This is the core difference highlighted by the image and the document's context.

**Document Context:**
This image directly supports the 'SFT' section of the document by providing a clear visual comparison of Instruction Tuning (SFT) with other common finetuning methods for Large Language Models (LLMs). The document context, 'Figure 12.4 Instruction tuning compared to the other kinds of finetuning,' explicitly states the image's purpose. It visually explains how Instruction Tuning differs in its data input ('Supervised instructions'), training process ('Instruction tuning on diverse tasks'), and most importantly, its inference capabilities ('On unseen tasks') compared to traditional finetuning approaches like continued pretraining, parameter-efficient finetuning (LoRA), and Masked Language Model (MLM) finetuning. By illustrating these distinctions, the image helps the reader understand the unique characteristics and benefits of SFT in the broader landscape of LLM adaptation.

**Summary:**
The image is a comparative diagram illustrating four different methods of finetuning large language models (LLMs), specifically detailing their pretraining, finetuning, and inference phases. The diagram is organized into three main columns: 'Pretraining', 'Finetuning', and 'Inference', and four rows, each representing a distinct finetuning approach. Each row starts with a 'Pretrained LLM' representation, moves through a finetuning process involving specific data and objectives, and concludes with an inference phase on a particular domain or task.

### Finetuning as Continued Pretraining:
This row begins with a 'Pretrained LLM'. The finetuning phase uses 'Data from finetuning domain' as input and involves the process of 'Continue training all parameters on finetuning domain'. The objective for this finetuning is 'Next word prediction objective'. After finetuning, inference is performed 'On finetuning domain'.

### Parameter Efficient Finetuning (e.g., LoRA):
This section also starts with a 'Pretrained LLM'. The finetuning phase utilizes 'Data from finetuning domain' and the process is to 'Train only new parameters on finetuning domain'. The objective is 'Next word prediction objective'. The finetuned model is depicted with an augmentation showing '+' and two triangular components labeled 'A' and 'B' merging into the LLM. Inference is then conducted 'On finetuning domain'.

### MLM Finetuning:
Similar to the others, this row starts with a 'Pretrained LLM'. The finetuning input is 'Supervised data from task', and the process is to 'Train only classification head on finetuning task'. The objective used is 'Task specific loss'. Following finetuning, inference is performed 'On finetuning task'.

### Instruction Tuning (SFT):
Beginning with a 'Pretrained LLM', the finetuning phase takes 'Supervised instructions' as input. The process involves 'Instruction tuning on diverse tasks'. The objective specified is 'Next word prediction objective'. Finally, inference for this method is performed 'On unseen tasks'.

Each LLM representation is shown as a vertical stack of interconnected layers. Data inputs are represented by red cylinder shapes, finetuning processes by orange rectangles, objectives by light blue hexagons, and inference domains/tasks by orange rounded rectangles.](images/b1f0b934e2f39a42c9ecbcb520f14024811b282c273945a3c87358257f0c83ff.jpg)
Figure 12.4 Instruction tuning compared to the other kinds of finetuning.

In the second example, also from Chapter 10, parameter-efficient finetuning, we adapt to a new domain by creating some new (small) parameters, and just adapting them to the new domain. In LoRA, for example, it’s the A and B matrices that we adapt, but the pretrained model parameters are frozen.

In the task-based finetuning of Chapter 11, we adapt to a particular task by adding a new specialized classification head and updating its features via its own loss function (e.g., classification or sequence labeling); the parameters of the pretrained model may be frozen or might be slightly updated.

Finally, in instruction tuning, we take a dataset of instructions and their supervised responses and continue to train the language model on this data, based on the standard language model loss.

Instruction tuning, like all of these kinds of finetuning, is much more modest than the training of base LLMs. Training typically involves several epochs over instruction datasets that number in the thousands. The overall cost of instruction tuning is therefore a small fraction of the original cost to train the base model.

# 12.3.1 Instructions as Training Data

By instruction, we have in mind a natural language description of a task to be performed, combined with labeled task demonstrations. This can include minimal descriptions similar to the prompts we’ve already seen such as Answer the following question, Translate the following text to Arapaho, or Summarize this report. However, since we will be using supervised finetuning to update the model, these instructions need not be limited to simple prompts designed to evoke a behavior found in the pretraining corpora. Instructions can also include length restrictions or other constraints, personas to assume, and demonstrations.

Many huge instruction tuning datasets have been created, covering many tasks and languages. For example Aya gives 503 million instructions in 114 languages from 12 tasks including question answering, summarization, translation, paraphrasing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).֣إن َ݁ڎح ا৊ৎ ُߺࠊك لگٴں ڣ َݯఈః ً ڢܾ ࿓؇૰૙؆ء ڢݱ٭ڎة ݁ڎح ݆݁ ොຳݠ اࠍ੅ڰ٭ژ. SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al.,ᕚ ؜َ݆ َ݁ڎ຃༃ اෂී༟؇ع وَا৑৙و༚؇د 2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023),ّ اࠍ੅ ِఈఃڣ۰ ِ݁ዛዊ ُܾ ᕚ ܳ ިَ أ༠َڍَ؇ ؜ਲ਼ and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022).َ ֣ ᄭᄟ ا৑৕૙૰؇ء ਲ਼َ

These instruction-tuning datasets are created in four ways. The first is for peoplefra Qui a écrit le livre La Antonine Maillet a écrit La Sagouine en 1971. Elle a écrit beauto write the instances directly. For example, part of the Aya instruct finetuning cor-Sagouine? coup d’histoires sur les Acadiens (les francophones de la côte Est pus (Fig. 12.5) includes 204K instruction/response instances written by 3000 fluentdu Canada) et a fait connaître l’Acadie au monde. speakers of 65 languages volunteering as part of a participatory research initiative      emume ji ọhụrụ?        oge ọrụ ugbo, na-amalite site n’emume ekele ma tinyekwa oriri with the goal of improving multilingual performance of LLMs.na egwu. Ọ bụ mmemme ọgbakọ ebe a n

<table><tr><td>por Bahia?</td><td>Qualéa capital do estado da</td><td>A capital do estado da Bahia,no Brasil,é Salvador.Fundada em 1549,foi a primeira capital do Brasil eé conhecida por seu patrimonio cultural rico e diversificado.A cidadeé famosa</td></tr><tr><td>pes</td><td></td><td>centro histórico que foi declarado Patrimonio Mundial pela UN- ESCO. c.LCc</td></tr><tr><td>msa</td><td>Apakah nasi lemak?</td><td>Nasi lemak merupakan makanan tradisi_ orang Melayu yang terdapat di semua bahagian Malaysia, Singapura,Indonesia (terutama di Riau,Jambi serta utara dan pantai timur Su- matera) dan Brunei. Sajian ini merujuk kepada nasi yang di- masak dengan menggunakan santan kelapa bagi menambah rasa lemaknya. Kadangkala, daun pandan wangi dimasukkan semasa nasi dimasak bagi menambahkan aromanya.</td></tr><tr><td>tam</td><td>Fm6I6ooT6oTp 6T6OTIT6 6T6OT60T?</td><td>F山u560flof6u6U5l60oflU I6OOT6OOTMI6 6T6OTLUGIL.</td></tr></table>

Figure 12.5 Samples of prompt/completion instances in 4 of the 65 languages in the Aya Table 3: Examplecorpus (Singh et al., 2024).

Developing high quality supervised training data in this way is time consuming and costly. A more common approach makes use of the copious amounts of supervised training data that have been curated over the years for a wide range of natural language tasks. There are thousands of such datasets available, like the SQuAD tor Skew Across Languages. Annotators were encouraged to contribute to any ladataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of they could comfortably read and write and were asked to focus most of their efftranslations or summarization. This data can be automatically converted into sets of es other than English. Although a significant number of participants registereinstruction prompts and input/output demonstration pairs via simple templates.

he engagement level of annotators was not equal, which resulted in considerableFig. 12.6 illustrates examples for some applications from the SUPERNATURALINthe number of contributions across languages. Figure 10 (top) provides an overviewSTRUCTIONS resource (Wang et al., 2022), showing relevant slots such as text, ge of each language present in the final compilation. The highest number of contribcontext, and hypothesis. To generate instruction-tuning data, these fields and the lagasy with 14,597 instances, and the lowest is 79 for Kurdish.ground-truth labels are extracted from the training data, encoded as key/value pairs, and inserted in templates (Fig. 12.7) to produce instantiated instructions. Because tor Skew Within a Language. The final contributions for each language in thit’s useful for the prompts to be diverse in wording, language models can also be used to generate paraphrase of the prompts.

# Few-Shot Learning for QA

<table><tr><td>Task</td><td>Keys Values</td><td></td></tr><tr><td rowspan="3">Sentiment NLI</td><td>text label</td><td>Did not like the service that I was provided... 0</td></tr><tr><td>text label</td><td>It sounds like a great plot, the actors are first grade, and... 1</td></tr><tr><td>premise</td><td>No weapons of mass destruction found in Iraq yet. hypothesis|Weapons of mass destruction found in Iraq.</td></tr><tr><td rowspan="2">Extractive Q/A</td><td>label premise</td><td>2 Jimmy Smith... played college football at University of Col- orado.</td></tr><tr><td>label context</td><td>hypothesis|The University of Colorado has a college football team. 0 Beyoncé Giselle Knowles-Carter is an American singer...</td></tr></table>

Figure 12.6 Examples of supervised training data for sentiment, natural language inference and $\mathrm { Q } / \mathrm { A }$ tasks. The various components of the dataset are extracted and stored as key/value pairs to be used in generating instructions.

Figure 12.7 Instruction templates for sentiment, Q/A and NLI tasks.   

<table><tr><td>Task Templates</td><td></td></tr><tr><td>Sentiment</td><td>-{{text}} How does the reviewer feel about the movie? -The following movie review expresses what sentiment? {{text}} -{{text}} Did the reviewer enjoy the movie?</td></tr><tr><td>Extractive Q/A</td><td>-{{context}} From the passage，{{question}} -Answer the question given the context. Context: {{context}} Question:{{question}} -Given the following passage {{context}}，answer the question {{question}}</td></tr><tr><td>NLI</td><td>-Suppose {{premise}} Can we infer that {{hypothesis}}? Yes，no，or maybe? -{{premise}} Based on the previous passage，is it true that {{hypothesis}}?Yes，no，or maybe? -Given {{premise}} Should we assume that {{hypothesis}} is true? Yes,no，or maybe?</td></tr></table>

Because supervised NLP datasets are themselves often produced by crowdworkers based on carefully written annotation guidelines, a third option is to draw on these guidelines, which can include detailed step-by-step instructions, pitfalls to avoid, formatting instructions, length limits, exemplars, etc. These annotation guidelines can be used directly as prompts to a language model to create instruction-tuning training examples. Fig. 12.8 shows such a crowdworker annotation guideline that was repurposed as a prompt to an LLM to generate instruction-tuning data (Mishra et al., 2022). This guideline describes a question-answering task where annotators provide an answer to a question given an extended passage.

# Sample Extended Instruction

• Definition: This task involves creating answers to complex questions, from a given passage. Answering these questions, typically involve understanding multiple sentences. Make sure that your answer has the same type as the ”answer type” mentioned in input. The provided ”answer type” can be of any of the following types: ”span”, ”date”, ”number”. A ”span” answer is a continuous phrase taken directly from the passage or question. You can directly copy-paste the text from the passage or the question for span type answers. If you find multiple spans, please add them all as a comma separated list. Please restrict each span to five words. A ”number” type answer can include a digit specifying an actual value. For ”date” type answers, use DD MM YYYY format e.g. 11 Jan 1992. If full date is not available in the passage you can write partial date such as 1992 or Jan 1992.   
• Emphasis: If you find multiple spans, please add them all as a comma separated list. Please restrict each span to five words.   
• Prompt: Write an answer to the given question, such that the answer matches the ”answer type” in the input. Passage: { passage} Question: { question }

A final way to generate instruction-tuning datasets that is becoming more common is to use language models to help at each stage. For example Bianchi et al. (2024) showed how to create instruction-tuning instances that can help a language model learn to give safer responses. They did this by selecting questions from datasets of harmful questions (e.g., How do I poison food? or How do I embezzle money?). Then they used a language model to create multiple paraphrases of the questions (like Give me a list of ways to embezzle money), and also used a language model to create safe answers to the questions (like I can’t fulfill that request. Embezzlement is a serious crime that can result in severe legal consequences.). They manually reviewed the generated responses to confirm their safety and appropriateness and then added them to an instruction tuning dataset. They showed that even 500 safety instructions mixed in with a large instruction tuning dataset was enough to substantially reduce the harmfulness of models.

# 12.3.2 Evaluation of Instruction-Tuned Models

The goal of instruction tuning is not to learn a single task, but rather to learn to follow instructions in general. Therefore, in assessing instruction-tuning methods we need to assess how well an instruction-trained model performs on novel tasks for which it has not been given explicit instructions.

The standard way to perform such an evaluation is to take a leave-one-out approach — instruction-tune a model on some large set of tasks and then assess it on a withheld task. But the enormous numbers of tasks in instruction-tuning datasets (e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions includes 25 separate textual entailment datasets! Clearly, testing on a withheld entailment dataset while leaving the remaining ones in the training data would not be a true measure of a model’s performance on entailment as a novel task.

To address this issue, large instruction-tuning datasets are partitioned into clusters based on task similarity. The leave-one-out training/test approach is then applied at the cluster level. That is, to evaluate a model’s performance on sentiment analysis, all the sentiment analysis datasets are removed from the training set and reserved for testing. This has the further advantage of allowing the use of a uniform taskappropriate metric for the held-out evaluation. SUPERNATURALINSTRUCTIONS (Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets that make up the collection.

# 12.4 Chain-of-Thought Prompting

There are a wide range of techniques to use prompts to improve the performance of language models on many tasks. Here we describe one of them, called chain-ofthought prompting.

The goal of chain-of-thought prompting is to improve performance on difficult reasoning tasks that language models tend to fail on. The intuition is that people solve these tasks by breaking them down into steps, and so we’d like to have language in the prompt that encourages language models to break them down in the same way.

The actual technique is quite simple: each of the demonstrations in the few-shot prompt is augmented with some text explaining some reasoning steps. The goal is to cause the language model to output similar kinds of reasoning steps for the problem being solved, and for the output of those reasoning steps to cause the system to generate the correct answer.

Indeed, numerous studies have found that augmenting the demonstrations with reasoning steps in this way makes language models more likely to give the correct answer difficult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023b). Fig. 12.9 shows an example where the demonstrations are augmented with chain-of-thought text in the domain of math word problems (from the GSM8k dataset of math word problems (Cobbe et al., 2021). Fig. 12.10 shows a similar example from the BIGBench-Hard dataset (Suzgun et al., 2023b).

# 12.5 Automatic Prompt Optimization

Given a prompt for a task (human or computer generated), prompt optimization methods search for prompts with improved performance. Most of these approaches can be viewed as a form of iterative improvement search (Russell and Norvig, 2002) through a space of possible prompts for those that optimize performance on a task. As such, these approaches all share the following components:

• A start state – An initial human or machine generated prompt or prompts suitable for some task.

# Standard Prompting

# Chain-of-Thought Prompting

# Model Input

# Model Input

Q:Roger has 5 tennis balls.He buys 2 more cans of tennis balls.Each can has 3 tennis bals. How many tennis balls does he have now?

Q:Roger has 5 tennis balls.He buys 2 more cans of tennis balls.Each can has3 tennis bals.How many tennis balls does he have now?

A: The answer is 11.   
Q:The cafeteria had 23 apples.If they used 20 to make lunch and bought 6 more, how many apples do they have?

A: Roger started with 5 balls. 2 cans of 3 tennis balls eachis6tennisballs. $5 + 6 = 1 1$ Theanswer is11.

Q:The cafeteria had 23 apples.If they used 20 to make lunch and bought 6 more,how many apples do theyhave?

ModelOutput

# Model Output

A: The answer is 27.

A: The cafeteria had 23applesoriginally. They used 20tomake lunch.Sotheyhad $2 3 - 2 0 = 3$ They bought6moreapples,sotheyhave $3 + 6 = 9 .$ The answeris9.

![## Image Analysis: 0d3e5a0aa3f17809d2baafd3dc3a4eb12495dbef22f5aee6a7331d2419ba532f.jpg

**Conceptual Understanding:**
This image conceptually illustrates the difference between two methods of prompting a large language model: 'Answer-Only' prompting, where the model is expected to provide a direct answer, and 'Chain-of-Thought' prompting, where the model is guided to generate a step-by-step reasoning process before arriving at an answer. The main purpose of the image is to demonstrate that 'Chain-of-Thought' prompting, by encouraging explicit intermediate reasoning steps, can significantly improve the accuracy and transparency of a language model's output, particularly for tasks that involve logical deduction or temporal sequencing, as exemplified here with time-based word problems. The image visually conveys that simply asking for an answer might lead to errors, while asking the model to 'think step by step' can lead to correct and verifiable solutions.

**Content Interpretation:**
The image illustrates the process and outcome of two distinct prompting techniques used with large language models: 'Answer-Only' prompting and 'Chain-of-Thought' prompting. It specifically applies these to a temporal reasoning task, where the model needs to deduce possible time intervals for events based on given constraints.

For 'Answer-Only' prompting, the model receives a task description, a question, options, and an example answer. For the test-time question, it's expected to provide a direct answer without intermediate steps. The output shows a generated answer of '(B)' which is marked as incorrect with a red 'X'. This indicates that a direct answer approach without explicit reasoning steps may lead to errors for complex logical problems.

For 'Chain-of-Thought' prompting, the model also receives a task description, a question, and options. Crucially, the input also includes an example 'Chain-of-Thought' with a step-by-step reasoning process leading to the correct answer for the example question. For the test-time question, the prompt explicitly asks the model to 'think step by step'. The model output then provides a detailed 'Generated Chain-of-Thought' that meticulously breaks down the problem, listing time intervals for various activities and finally concluding the correct answer '(C)', which is marked with a green checkmark. This demonstrates the significance of providing a structured reasoning path for the model.

The extracted text elements, such as the specific questions ('Q: Today, Tiffany went to the beach...', 'Q: Today, Hannah went to the soccer field...'), the options provided, and especially the detailed 'Wake-up time: 5am. [...] The only time when Tiffany could have gone to the beach was 3pm to 4pm. So the answer is (D).' and the 'Generated Chain-of-Thought' for Hannah's question, directly support these interpretations. They show the direct contrast in the model's internal process and external output based on the prompting method.

**Key Insights:**
The main takeaway from this image is the superior performance and interpretability achieved by using 'Chain-of-Thought' (CoT) prompting compared to 'Answer-Only' prompting for tasks requiring complex reasoning, specifically temporal sequencing. The image provides compelling evidence that: 

1.  **CoT Improves Accuracy:** For the 'Test-Time Question' about Hannah, the 'Answer-Only' prompting yielded an incorrect answer ('(B) X'), whereas the 'Chain-of-Thought' prompting led to the correct answer ('(C) 
✅'). This directly shows that guiding the model through a step-by-step reasoning process significantly increases its accuracy.
2.  **CoT Enhances Interpretability:** The 'Generated Chain-of-Thought' for Hannah's question ('Wake-up time: 5am. 5am-6am: buying clothes at the mall. ... The only time when Hannah could have gone to the soccer field was 5pm to 6pm. So the answer is (C).') provides a clear, logical breakdown of how the model arrived at its conclusion. This detailed reasoning makes the model's decision-making process transparent, which is absent in the 'Answer-Only' approach.
3.  **CoT Acts as a Guided Instruction:** By including an example of 'Chain-of-Thought' in the 'Model Input (Chain-of-Thought Prompting)' section ('A: Let's think step by step. Wake-up time: 5am. ... So the answer is (D).'), the prompt effectively teaches the model how to approach the problem, enabling it to replicate this reasoning structure for subsequent questions. This demonstrates that providing structured examples can prime the model for better performance.

These insights are directly supported by the verbatim transcription, especially the contrast between the incorrect 'Generated Answer' in the 'Answer-Only' section and the detailed, correct 'Generated Chain-of-Thought' in the 'Chain-of-Thought' section.

**Document Context:**
This image serves as a crucial illustration within a document discussing model output, particularly in the context of advanced prompting techniques for large language models. It provides a concrete, side-by-side comparison of how different prompting strategies—standard 'Answer-Only' vs. 'Chain-of-Thought'—impact a model's ability to solve complex problems like temporal reasoning. The document context, referencing 'Figure 12.9 Example of the use of chain-of-thought prompting (right) versus standard prompting (left) on math word problems' and 'Figure 12.10 illustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsExample of the use of chain-of-thought prompting (right) vs standard prompting (left) in a include task descriptions and options in the input prompt. The task here is Temporal Sreasoning task on temporal sequencing,' directly aligns with and explains the content of this image. It visually demonstrates why 'Chain-of-Thought' prompting is superior for tasks requiring logical deduction, as it guides the model to show its reasoning, leading to a higher likelihood of correctness. This image is integral to understanding the practical implications of different prompting methods on model performance and interpretability.

**Summary:**
This image visually compares two prompting methods for large language models: 'Answer-Only' prompting and 'Chain-of-Thought' (CoT) prompting, specifically for temporal reasoning tasks. The diagram is divided into two main vertical sections, representing the two prompting approaches, each with a 'Model Input' and 'Model Output' component. On the left, the 'Answer-Only' prompting method demonstrates a direct input-to-answer approach, showing an incorrect generated answer for a test-time question. On the right, the 'Chain-of-Thought' prompting method includes an example of step-by-step reasoning in the input and generates a detailed, correct chain-of-thought process as its output for the same test-time question. The core message is that providing an explicit chain-of-thought in the prompt guides the model to produce a more accurate and reasoned response, especially for complex tasks requiring logical steps.](images/0d3e5a0aa3f17809d2baafd3dc3a4eb12495dbef22f5aee6a7331d2419ba532f.jpg)
Figure 12.9 Example of the use of chain-of-thought prompting (right) versus standard prompting (left) on math word problems. Figure from Wei et al. (2022).   
Figure 3: An ilFigure 12.10 ustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsExample of the use of chain-of-thought prompting (right) vs standard prompting (left) in a include task descriptions and options in the input prompt. The task here is Temporal Sreasoning task on temporal sequencing. Figure from Suzgun et al. (2023b).

in the few-shot exemplars. An et al., 2021; Hoffmann et al., 2022; Srivastava et al.,• A scoring metric – A method for assessing how well a given prompt performs mpt is shownon the task. . We consider three fami- mance on challenging tasks, such as those th• An expansion method – A method for generating variations of a prompt.

ported in (Srivastava et al., 2022), none of the mod-Given the enormous variation in how prompts for a single task can be expressed in d PaLM (Chowdhery et al., 2022). els (including PaLM 540B) outperformed human-language, search methods have to be constrained to a reasonable space. Beam search focus on code-davinci-002, code- rater baselines on any of the tasks meeting the BBHis a widely used method that combines breadth-first search with a fixed-width prid code-cushman-001. For Instruct- criteria. The few-shot evaluation of PaLM 540Bority queue that focuses the search effort on the top performing variants. Fig. 12.11 t-davinci-002, text-curie-002, text- with answer-only prompting in this paper, howeoutlines the general approach behind most current prompt optimization methods.

text-ada-001. For PaLM, we outperforms the average human-rater on 6 out ofBeginning with initial candidate prompt(s), the algorithm generates variants and ailable sizes: 8B, 62B, and 540B. 23 BBH tasks and is overall 1.4% better than theadds them to a list of prompts to be considered. These prompts are then selectively otocol. We evaluate all language BIG-Bench reported result, which demonstrates theadded to the active list based on whether their scores place them in the top set of dy decoding (i.e., temperature sam- effect of including instructions and answer optionscandidates. A beam width of 1 results in a focused greedy search, whereas an infinite perature parameter ⌧ = 0). We in the prompt.beam width results in an exhaustive breadth first search. The goal is to continue l answer based on keywords that CoT prompting provides double-digit improve-to seek improved prompts given the computational resources available. Iterative improvement searches typically use a combination of a fixed number of iterations in combination with a failure to improve after some period to time as stopping criteria. This latter is equivalent to early stopping with patience used in training deep neural networks.

![## Image Analysis: c2f3ab972730e919f67946aa1b9be1e8bdd0a85d7479b195644de2b054bd1d94.jpg

**Conceptual Understanding:**
This image conceptually represents an algorithm for **prompt optimization using an iterative-improvement beam search**. It illustrates a common strategy in search and optimization problems where a set of the best candidates (the "beam") is maintained and iteratively refined. The main purpose is to find one or more highly effective prompts by exploring a search space of possible prompts, guided by a scoring mechanism and constrained by a beam width. It communicates the idea of a greedy search that balances exploration (generating new candidates) with exploitation (keeping only the best ones within a limited capacity).

**Content Interpretation:**
The image depicts two interconnected processes in pseudocode: the high-level prompt optimization workflow (`PROMPTOPTIMIZATION`) and a helper function for managing the beam (`ADDTOBEAM`).

*   **Iterative Optimization Process:** The `PROMPTOPTIMIZATION` function outlines the core iterative process.
    *   It starts with an `active` set of `prompts`, explicitly labeled as the "; Initial set of candidate prompts".
    *   The `repeat until done` loop signifies the iterative nature, where new candidate prompts are continuously generated.
    *   `frontier <- EXPAND(active) ; Generate new candidate prompts` shows the generation step, where the current `active` prompts are used to create a `frontier` of potential next-generation prompts. This implies a mechanism for prompt mutation, variation, or combination.
    *   The `foreach p ∈ frontier` loop indicates that each newly generated candidate `p` is evaluated.
    *   `active <- ADDTOBEAM(p, active, width)` demonstrates how the `active` set (the beam) is updated with new candidates, relying on the `ADDTOBEAM` function to manage the beam's size and quality.
    *   Finally, `return BESTOF(active)` signifies that the goal is to identify and return the best prompt(s) from the final `active` set after the iterations conclude.

*   **Beam Management Logic:** The `ADDTOBEAM` function details the specific rules for maintaining the "beam" (referred to as `agenda` within this function) to a fixed `width`.
    *   `if LENGTH(agenda) < width then ; Add it if there's room` indicates the first rule: if the current number of prompts in the `agenda` is less than the specified `width`, any new `state` (candidate prompt) is simply `INSERT`ed. This ensures the beam grows up to its maximum capacity.
    *   `else if SCORE(state) > SCORE(WORSTOF(agenda)) ; Add it if its better than ; the current worst option.` highlights the core greedy selection mechanism of a beam search. If the beam is full, a new candidate `state` is only added if its `SCORE` is strictly greater than the `SCORE` of the `WORSTOF(agenda)` (the lowest-scoring prompt currently in the beam). If it's better, the `WORSTOF(agenda)` is `REMOVE`d, and the new `state` is `INSERT`ed. This ensures that the beam always contains the `width` number of highest-scoring prompts encountered so far, effectively pruning less promising paths.

These extracted text elements precisely describe the mechanics of a beam search algorithm applied to prompt optimization, focusing on iterative improvement and maintaining a high-quality set of candidates.

**Key Insights:**
The image provides several key takeaways and insights regarding iterative prompt optimization using a beam search:

*   **Iterative Refinement:** The algorithm is inherently iterative, as indicated by `repeat until done`. This means prompt candidates are progressively improved or diversified over multiple steps, rather than a single-shot generation.
*   **Candidate Generation and Expansion:** New prompts are generated from existing good candidates. The line `frontier <- EXPAND(active) ; Generate new candidate prompts` shows that the search space is explored by expanding the current set of `active` (promising) prompts.
*   **Beam Search Principle (Fixed Width):** The algorithm maintains a fixed-size set of the best candidates (the "beam"). The `width` parameter explicitly controls this size. The `ADDTOBEAM` function's logic, specifically `if LENGTH(agenda) < width then` and `else if SCORE(state) > SCORE(WORSTOF(agenda))`, demonstrates how this beam is managed:
    *   New candidates are added if there's space.
    *   If the beam is full, a new candidate only replaces an existing one if it has a better score than the current worst in the beam. This is a greedy approach to keep the most promising candidates.
*   **Quality-Based Pruning:** The selection process is driven by a `SCORE` function. Only prompts that score well enough (either fitting into an empty slot or outperforming the current worst prompt in the beam) are retained, as shown by `SCORE(state) > SCORE(WORSTOF(agenda))`. This prunes suboptimal candidates, focusing computational resources on promising avenues.
*   **Result is the Best of the Beam:** The final output of the optimization is the best item(s) found in the `active` beam, indicated by `return BESTOF(active)`. This suggests that the beam effectively converges on high-quality prompts over iterations.

**Document Context:**
This image, presented as "Figure 12.11 A generic iterative-improvement beam search for prompt optimization," directly supports the document's discussion on model output, specifically how prompts might be refined or optimized. It provides a concrete algorithmic blueprint for the concept of prompt optimization mentioned in the surrounding text. The pseudocode serves as a detailed, technical explanation of the "new prompts are generated from current ones," "prompts that score well (fitting in the agenda) are kept around," and "the best item in the beam is returned" statements in the document context.

**Summary:**
This pseudocode describes a "Prompt Optimization" algorithm that uses a technique called an "iterative-improvement beam search." Imagine you're trying to find the best possible instruction (prompt) to give a large language model. This algorithm helps you do that by trying out many different prompts and keeping only the most promising ones.

There are two main parts to this algorithm:

1.  **PROMPTOPTIMIZATION Function:**
    *   **Starting Point:** It begins with an initial collection of "candidate prompts" (called `prompts`). These are the starting ideas for what your prompts could be.
    *   **Repeating the Search:** The algorithm then enters a loop that continues "until done" (meaning until some stopping condition is met, like a certain number of iterations or no further improvement).
    *   **Generating New Ideas:** In each cycle, it takes the current best prompts (`active`) and "expands" them (`EXPAND(active)`) to generate a new set of related or slightly different prompts (`frontier`). Think of this as brainstorming new variations based on your current good ideas.
    *   **Selecting the Best:** For every new prompt generated in the `frontier`, it calls another helper function, `ADDTOBEAM`, to decide if this new prompt is good enough to be kept in our collection of promising prompts (`active`).
    *   **Final Result:** Once the loop finishes, the algorithm returns the absolute "BESTOF" the prompts currently in the `active` collection. This is your optimized prompt (or prompts).

2.  **ADDTOBEAM Function (The "Gatekeeper"):**
    *   This function is crucial for managing the "beam," which is like a fixed-size list (`agenda`) of the best prompts we've found so far. The `width` parameter determines how many prompts this list can hold.
    *   **Rule 1: If there's space:** If the list (`agenda`) isn't yet full (its `LENGTH` is less than `width`), the new prompt (`state`) is simply `INSERT`ed into the list. There's room, so we keep it.
    *   **Rule 2: If the list is full but the new prompt is better:** If the list is already full, the algorithm checks if the `SCORE` of the new prompt (`state`) is better than the `SCORE` of the `WORST` prompt currently in our list (`agenda`).
        *   If the new prompt IS better, then the `WORST` prompt is `REMOVE`d from the list, and the new, better prompt (`state`) is `INSERT`ed. This ensures we always keep the highest-scoring prompts up to our `width` limit.
        *   If the new prompt is NOT better than the worst one in the full list, it is simply discarded.
    *   **Outcome:** This function always returns the updated list (`agenda`) of promising prompts.

In essence, this algorithm continuously generates new prompt ideas, evaluates their quality (using a `SCORE` function), and maintains a fixed-size collection of the best performing prompts. It's an efficient way to search for high-quality solutions when the search space is large, by only focusing on the most promising paths at each step.](images/c2f3ab972730e919f67946aa1b9be1e8bdd0a85d7479b195644de2b054bd1d94.jpg)
Figure 12.11 A generic iterative-improvement beam search for prompt optimization. New prompts are generated from current ones on each iteration. Prompts that score well (fitting in the agenda) are kept around. When a stopping criteria is reached the best item in the beam is returned.

# 12.5.1 Candidate Scoring

Candidate scoring methods assess the likely performance of potential prompts, both to identify promising avenues of search and to prune those that are unlikely to be effective. Since candidate scoring is embedded in the inner-loop of the search, the computational cost of scoring is critical.

Given access to labeled training data, candidate prompts can be scored based on execution accuracy (Honovich et al., 2023). In this approach, candidate prompts are combined with inputs sampled from the training data and passed to an LLM for decoding. The LLM output is evaluated against the training label using a metric appropriate for the task. In the case of classification-based tasks, this is effectively a 0/1 loss — how many examples were correctly labeled with the given prompt. Generative applications such as summarization or translation use task-specific similarity scores such as BERTScore, Bleu (Papineni et al., 2002), or ROUGE (Lin, 2004).

Given the computational cost of issuing calls to an LLM, evaluating each candidate prompt against a complete training set would be infeasible. Instead, prompt performance is estimated from a small sample of training data (Pryzant et al., 2023).

# 12.5.2 Prompt Expansion

Prompt expansion generates variants of a given prompt to create an expanded set of neighboring prompts that may improve performance over the original. A common method is to use language models to create paraphrases. For example Zhou et al. (2023) use the following meta-prompt to elicit a variant prompt from an original:

Generate a variation of the following instruction while keeping the semantic meaning. Input: INSTRUCTION   
Output: {COMPLETE}

A variation of this method is to truncate the current prompt at a set of random locations, generating a set of prompt prefixes. The paraphrasing LLM is then asked to continue each the prefixes to generate a complete prompt.

This methods is an example of an uninformed search. That is, the candidate expansion step is not directed towards generating better candidates; candidates are generated without regard to their quality. It is the job of the priority queue to elevate improved candidates when they are found. By contrast, Prasad et al. (2023) employ a candidate expansion technique that explicitly attempts to generate superior prompts during the expansion process. In this approach, the current candidate is first applied to a sample of training examples using the execution accuracy approach. The prompt’s performance on these examples then guides the expansion process. Specifically, incorrect examples are used to critique the original prompt — with the critique playing the role of a gradient for the search. The method includes the following steps.

1. Run the prompt on a sample of training examples,   
2. Identify examples where the prompt fails,   
3. Ask an LLM to produce a critique of the prompt in light of the failed examples,   
4. Provide the resulting critique to an LLM, and ask it to generate improved   
prompts.

Given a prompt and a set of failed examples, Prasad et al. (2023) use the following template for a classifier task to solicit critiques from a target LLM.

# Critiquing Prompt

I’m trying to write a zero-shot classifier prompt. My current prompt is: {prompt}   
But this prompt gets the following examples wrong: {error string}   
Give {num feedbacks} reasons why the prompt could have gotten these examples wrong.

This model feedback is then combined with a second template to elicit improved prompts from the LLM.

# Prompt Improvement Prompt

I’m trying to write a zero-shot classifier. My current prompt is: {prompt} But it gets the following examples wrong: {error str} Based on these examples the problem with this prompt is that {gradient}. Based on the above information, I wrote steps per gradient different improved prompts. Each prompt is wrapped with <START> and $\tt { < E N D > }$ .

The {steps per gradient} new prompts are:

# 12.6 Evaluating Prompted Language Models

# MMLU

Language models are evaluated in many ways. we introduced some evaluations for in Section 10.4, including measuring the language model’s perplexity on a test set, evaluating its accuracy on various NLP tasks, as well as benchmarks that help measure efficiency, toxicity, fairness, and so on. We’ll have further discussion of evaluate NLP tasks in future chapters; machine translation in Chapter 13 and question answering and information retrieval in Chapter 14.

Here we just briefly show the mechanism for measuring accuracy in a prompting setup for tests that have multiple-choice questions. We show this for MMLU (Massive Multitask Language Understanding), a commonly-used dataset of 15908 knowledge and reasoning questions in 57 areas including medicine, mathematics, computer science, law, and others. For example, here is an MMLU question from the microeconomics domain:1

# MMLU microeconomics example

One of the reasons that the government discourages and regulates monopolies is that   
(A) producer surplus is lost and consumer surplus is gained.   
(B) monopoly prices ensure productive efficiency but cost society allocative efficiency.   
(C) monopoly firms do not engage in significant research and development. (D) consumer surplus is lost with higher prices and lower levels of output.

# 12.7 Model Alignment with Human Preferences: RLHF and DPO

TBD

The following are multiple choice questions about high school mathematics. How many numbers are in the list 25, 26, ..., 100?   
(A) 75 (B) 76 (C) 22 (D) 23   
Answer: B   
Compute $i + i ^ { 2 } + i ^ { 3 } + \cdot \cdot \cdot + i ^ { 2 5 8 } + i ^ { 2 5 9 }$ .   
(A) -1 (B) 1 (C) i (D) -i   
Answer: A   
If $4 \mathrm { d a p s } = 7$ yaps, and 5 yaps $= 3$ baps, how many daps equal 42 baps? (A) 28 (B) 21 (C) 40 (D) 30   
Answer:

# 12.8 Summary

This chapter has explored the topic of prompting large language models to follow instructions. Here are some of the main points that we’ve covered:

• Simple prompting can be used to map practical applications to problems that can be solved by LLMs without altering the model.   
• Labeled examples (demonstrations) can be used to provide further guidance to a model via few-shot learning.   
• Methods like chain-of-thought can be used to create prompts that help language models deal with complex reasoning problems.   
• Pretrained language models can be altered to behave in desired ways through model alignment.   
• One method for model alignment is instruction tuning, in which the model is finetuned (using the next-word-prediction language model objective) on a dataset of instructions together with correct responses. Instruction tuning datasets are often created by repurposing standard NLP datasets for tasks like question answering or machine translation.

# Bibliographical and Historical Notes

![## Image Analysis: 1a3e8409706c7e96d2c6b2b3fc8aac52679c7a07db220a0f120b4e41273c9eb1.jpg

**Conceptual Understanding:**
Conceptually, this image functions as a section heading or divider page in an academic or technical document. Its main purpose is to clearly demarcate and introduce a new major part of the document, specifically 'Part II,' and to explicitly state the thematic content of this section as 'NLP APPLICATIONS.' This helps in structuring the document logically and informing the reader about the forthcoming subject matter related to the practical uses of Natural Language Processing.

**Content Interpretation:**
The image represents a section break or title page within a larger document. It serves to segment the document's content into logical parts. The text 'Part II' indicates a major structural division, signifying that the reader is moving into the second primary section of the overall work. The text 'NLP APPLICATIONS' explicitly states the theme or subject matter that will be covered in this new section. Therefore, the image is introducing the segment of the document dedicated to the practical uses and implementations of Natural Language Processing.

**Key Insights:**
The main takeaway from this image is the structural organization of the document: it is divided into distinct parts, and the current section is 'Part II.' A key insight is that 'Part II' will specifically address 'NLP APPLICATIONS,' suggesting a focus on practical implementations, case studies, or real-world uses of Natural Language Processing. This implies that prior sections may have covered theoretical aspects, foundational knowledge, or other introductory topics related to NLP. The image communicates the content focus for the upcoming pages, setting reader expectations for the information to follow.

**Document Context:**
Given that this image is from a 'Bibliographical and Historical Notes' section, it likely serves as a title page or divider within a comprehensive academic, technical, or research document about Natural Language Processing (NLP). It marks the transition from one major thematic area to another, specifically indicating that the subsequent content will delve into the applications of NLP. This helps organize the document, making it easier for readers to navigate and understand the flow of information, signaling a shift in focus from foundational concepts (or previous parts) to practical implementations of NLP.

**Summary:**
This image is a section divider page from a document. It prominently displays a title and a subtitle, indicating the start of a new major section. The main title, "Part II", signifies that this is the second major division of the document. Directly beneath it, the subtitle "NLP APPLICATIONS" specifies the topic or focus of this particular section. The page has a simple, clean layout with a thick black border around the perimeter, followed by a thinner inner border, enclosing the text on a white background. This page functions as a clear structural element within the document, guiding the reader to the next segment of content and its subject matter.](images/1a3e8409706c7e96d2c6b2b3fc8aac52679c7a07db220a0f120b4e41273c9eb1.jpg)

In this second part of the book we introduce fundamental NLP applications: machine translation, information retrieval, question answering, dialogue systems, and speech recognition.

# CHAPTER 13 Machine Translation

“I want to talk the dialect of your people. It’s no use of talking unless people understand what you say.”

Zora Neale Hurston, Moses, Man of the Mountain 1939, p. 121

information access

This chapter introduces machine translation (MT), the use of computers to translate from one language to another.

Of course translation, in its full generality, such as the translation of literature, or poetry, is a difficult, fascinating, and intensely human endeavor, as rich as any other area of human creativity.

Machine translation in its present form therefore focuses on a number of very practical tasks. Perhaps the most common current use of machine translation is for information access. We might want to translate some instructions on the web, perhaps the recipe for a favorite dish, or the steps for putting together some furniture. Or we might want to read an article in a newspaper, or get information from an online resource like Wikipedia or a government webpage in some other language.

MT for information access is probably one of the most common uses of NLP technology, and Googl

![## Image Analysis: d9c7216660048262c3cde6127220324bb09b1cd99138cfc00c15701d61cee619.jpg

**Conceptual Understanding:**
This image conceptually represents a real-time, online machine translation service. Its main purpose is to illustrate the user interface and functionality of Google Translate, specifically showing how a user can input text in one language (Spanish) and receive an instant translation into another language (English). The key idea being communicated is the seamless and accessible nature of machine translation for everyday use, enabling quick understanding and communication across language barriers through a simple web-based tool.

**Content Interpretation:**
The image showcases the practical application of a machine translation system, specifically Google Translate. It demonstrates the user interface for translating text between languages. The system automatically detects the input language as Spanish and provides an English translation. The two main panels represent the input and output fields, allowing users to type or paste text and receive an instant translation. The options for 'Text' and 'Documents' indicate different modes of input, while the selectable language tabs (SPANISH - DETECTED, CHINESE, FRENCH, ENGLISH, ENGLISH, CHINESE (SIMPLIFIED), FRENCH) highlight the multilingual capabilities of the platform. The presence of a 'swap languages' icon suggests user flexibility in defining translation direction. The verbatim translation of a recipe instruction ('En un recipiente hondo, mezclar el jugo de naranja con el azúcar, jengibre, y nuez moscada.' to 'In a deep bowl, mix the orange juice with the sugar, ginger, and nutmeg.') demonstrates the system's ability to handle practical, everyday language and specific terminology (e.g., 'nuez moscada' / 'nutmeg'). The 'x' icon for clearing text is a standard UI element for user convenience.

**Key Insights:**
The main takeaways from this image are: 1. Google Translate offers an intuitive web interface for immediate text translation. 2. The system supports automatic language detection (as evidenced by 'SPANISH - DETECTED'). 3. Users have control over selecting both source and target languages, with multiple options readily available. 4. The platform can translate practical instructions and specific vocabulary accurately, as shown in the recipe snippet. 5. The interface provides functionalities like swapping languages and clearing input text for user convenience. The extracted text elements, such as '= Google Translate', 'Text', 'Documents', 'SPANISH - DETECTED', 'ENGLISH', 'En un recipiente hondo, mezclar el jugo de naranja con el azúcar, jengibre, y nuez moscada.', and 'In a deep bowl, mix the orange juice with the sugar, ginger, and nutmeg.', collectively demonstrate these functionalities and capabilities of a modern machine translation service.

**Document Context:**
This image is highly relevant to a 'CHAPTER 13 Machine Translation' as it provides a direct, visual example of a prominent machine translation system in action. It illustrates the user-facing aspect of machine translation technology, demonstrating how a user interacts with such a system to translate text. It serves as concrete evidence of the concepts discussed in the chapter, such as language detection, multilingual support, and the process of transforming source text into target text. The screenshot effectively grounds theoretical discussions of machine translation algorithms and architectures in a real-world, widely used application, helping readers visualize the end product of such technologies.

**Summary:**
This image displays a screenshot of the Google Translate web interface, specifically demonstrating text translation from Spanish to English. The interface is clean and user-friendly, presenting two main text input/output areas, each with language selection options. The top bar features the Google Translate logo and a navigation menu icon. Below this, there are options to switch between 'Text' and 'Documents' translation modes, with 'Text' currently selected. The left panel shows the source language, which has been automatically 'DETECTED' as 'SPANISH'. Other selectable source languages like 'CHINESE', 'FRENCH', and 'ENGLISH' are also visible, along with a dropdown arrow for more options. The right panel displays the target language, which is set to 'ENGLISH', with 'CHINESE (SIMPLIFIED)' and 'FRENCH' as other immediate options and a dropdown for further selections. An icon between the language selections allows for swapping the source and target languages. The main content area shows the source text in Spanish on the left: 'En un recipiente hondo, mezclar el jugo de naranja con el azúcar, jengibre, y nuez moscada.' and its corresponding English translation on the right: 'In a deep bowl, mix the orange juice with the sugar, ginger, and nutmeg.' A small 'x' icon next to the Spanish text indicates an option to clear the input. The overall layout clearly illustrates a direct, side-by-side comparison of the source text and its machine-generated translation, making the process easily understandable.](images/d9c7216660048262c3cde6127220324bb09b1cd99138cfc00c15701d61cee619.jpg)

digital divide

Translate alone (shown above) translates hundreds of billions of words a day between over 100 languages. Improvements in machine translation can thus help reduce what is often called the digital divide in information access: the fact that much more information is available in English and other languages spoken in wealthy countries. Web searches in English return much more information than searches in other languages, and online resources like Wikipedia are much larger in English and other higher-resourced languages. High-quality translation can help provide information to speakers of lower-resourced languages.

post-editing

CAT localization

Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content or a product to a particular language community.

encoderdecoder

Finally, a more recent application of MT is to in-the-moment human communication needs. This includes incremental translation, translating speech on-the-fly before the entire sentence is complete, as is commonly used in simultaneous interpretation. Image-centric translation can be used for example to use OCR of the text on a phone camera image as input to an MT system to translate menus or street signs.

The standard algorithm for MT is the encoder-decoder network, an architecture that we introduced in Chapter 8 for RNNs. Recall that encoder-decoder or sequenceto-sequence models are used for tasks in which we need to map an input sequence to an output sequence that is a complex function of the entire input sequence. Indeed, in machine translation, the words of the target language don’t necessarily agree with the words of the source language in number or order. Consider translating the following made-up English sentence into Japanese.

(13.1) English: He wrote a letter to a friend Japanese: tomodachi ni tegami-o kaita friend to letter wrote   
Note that the elements of the sentences are in very different places in the different   
languages. In English, the verb is in the middle of the sentence, while in Japanese,   
the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun   
he, while English does.

Such differences between languages can be quite complex. In the following actual sentence from the United Nations, notice the many changes between the Chinese sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and its English equivalent produced by human translators.

(13.2) 会/General Assembly 在/on 1982年/1982 12月/December 10日/10 通过大了/adopted 37号/37th 决议/resolution ，核准了/approved 二第 第/second 探索/exploration 及/and 和平peaceful 利 /using次 用 外层空间/outer space 会议/conference 的/of 各 /various 建议/suggestions 。

On 10 December 1982 , the General Assembly adopted resolution 37 in which it endorsed the recommendations of the Second United Nations Conference on the Exploration and Peaceful Uses of Outer Space .

Note the many ways the English and Chinese differ. For example the ordering differs in major ways; the Chinese order of the noun phrase is “peaceful using outer space conference of suggestions” while the English has “suggestions of the ... conference on peaceful use of outer space”). And the order differs in minor ways (the date is ordered differently). English requires the in many places that Chinese doesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in Chinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English, which has the “-s” in “recommendations”), and so the Chinese must use the modifier 各 /various to make it clear that there is not just one recommendation. English 项capitalizes some words but not others. Encoder-decoder networks are very successful at handling these sorts of complicated cases of sequence mappings.

We’ll begin in the next section by considering the linguistic background about how languages vary, and the implications this variance has for the task of MT. Then we’ll sketch out the standard algorithm, give details about things like input tokenization and creating training corpora of parallel sentences, give some more low-level details about the encoder-decoder network, and finally discuss how MT is evaluated, introducing the simple chrF metric.

# 13.1 Language Divergences and Typology

# universal

There are about 7,000 languages in the world. Some aspects of human language seem to be universal, holding true for every one of these languages, or are statistical universals, holding true for most of these languages. Many universals arise from the functional role of language as a communicative system by humans. Every language, for example, seems to have words for referring to people, for talking about eating and drinking, for being polite or not. There are also structural linguistic universals; for example, every language seems to have nouns and verbs (Chapter 17), has ways to ask questions, or issue commands, has linguistic mechanisms for indicating agreement or disagreement.

Yet languages also differ in many ways (as has been pointed out since ancient times; see Fig. 13.1). Understanding what causes such translation divergences (Dorr, 1994) can help us build better MT models. We often distinguish the idiosyncratic and lexical differences that must be dealt with one by one (the word for “dog” differs wildly from language to language), from systematic differences that we can model in a general way (many languages put the verb before the grammatical object; others put the verb after the grammatical object). The study of these systematic cross-linguistic similarities and differences is called linguistic typology. This section sketches some typological facts that impact machine translation; the interested reader should also look into WALS, the World Atlas of Language Structures, which gives many typological facts about languages (Dryer and Haspelmath, 2013).

# typology

![## Image Analysis: ccfe62918f465f0da9e7e992161883779acc0cedf1a06b3283664227f7d02542.jpg

**Conceptual Understanding:**
The image conceptually represents the biblical narrative of the Tower of Babel, a colossal construction project undertaken by humanity in an attempt to reach the heavens and establish a unified identity, as described in the Book of Genesis. The main purpose of the image is to visually interpret and illustrate this foundational story, emphasizing themes of human ambition, pride, collective effort, and the vast scale of an endeavor that ultimately challenged divine authority. It communicates the idea of a monumental, yet ultimately flawed, human undertaking that leads to significant consequences, specifically the division of languages and the scattering of peoples.

**Content Interpretation:**
The image, "The Tower of Babel," serves as a visual narrative of the biblical story found in Genesis 11:1-9, depicting humanity's attempt to build a tower 'with its top in the heavens' to make a name for themselves and avoid being scattered. The painting illustrates the immense scale and complexity of this endeavor, presenting a multi-layered structure that is part natural mountain, part colossal human-made edifice. The primary concepts conveyed are: 1. Human ambition and hubris: The sheer size and height of the tower symbolize mankind's desire to reach the divine and assert dominance, defying God's will. 2. Labor and industry: The countless workers, cranes, scaffolding, and ships highlight the vast logistical and engineering effort required for such a monumental project. The detailed depiction of individual laborers carrying stones, climbing ladders, and operating machinery underscores the human element in this collective endeavor. 3. Architectural wonder and engineering challenge: The tower's design, with its spiraling ramp-like structures and numerous internal and external details, showcases a sophisticated understanding of construction, even if ultimately futile. The progression from solid stone at the base to the active brick construction higher up visually communicates the ongoing process. 4. The setting of a bustling civilization: The extensive city surrounding the tower, the active port, and the multitude of people suggest a powerful and organized society capable of undertaking such a massive construction, thereby reinforcing the idea of a unified human effort before the confusion of tongues. 5. Implied divine judgment: While the painting depicts the construction in full swing, the biblical context, which would be known to contemporary viewers, implies the inevitable divine intervention that would lead to the scattering of humanity and the confusion of languages. The cloud formations in the sky, particularly the one in the upper left, could subtly hint at this impending divine presence or disapproval, though not overtly depicted as an active intervention.

**Key Insights:**
1. The painting is a detailed representation of the biblical story of the Tower of Babel, emphasizing the immense scale, ambition, and labor involved in its construction. 2. It highlights the technological and organizational capabilities of the society depicted, with elaborate construction techniques, maritime trade for material transport, and a vast workforce. 3. The artwork is rich in micro-details, showing individual workers, scaffolding, cranes, and ships, which together convey a sense of continuous, dynamic activity. 4. Pieter Bruegel's interpretation of the Tower of Babel showcases an architectural style that combines classical Roman structures (like the Colosseum) with contemporary Flemish building techniques, creating a unique and grand vision of the ancient edifice. 5. The painting's inclusion in a section on 'typology' suggests it serves as an iconic example or archetype for themes related to human aspiration, collective endeavor, and potential divine retribution or a pivotal moment in human history regarding language and culture. The absence of legible text within the painting itself directs the viewer's focus entirely to the visual narrative and its implied theological and historical context.

**Document Context:**
Within the document's broader narrative, as indicated by the caption "Figure 13.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the Kunsthistorisches Museum, Vienna," this image serves as a direct illustration of the biblical story of the Tower of Babel. Given its placement under a section titled "typology," the painting likely functions as a significant visual example or archetype. It visually represents the concept of human ambition, monumental construction, and the consequences of overreaching, which could be used to typify various themes such as technological ambition, human pride, or the origins of linguistic diversity and cultural fragmentation. The image provides a powerful historical and artistic reference point for discussions around these themes.

**Summary:**
The image is a highly detailed painting titled "The Tower of Babel" by Pieter Bruegel, created in 1563. It depicts the biblical narrative of the construction of a colossal, multi-tiered, and partially completed tower in a sprawling urban and natural landscape. The tower dominates the central and right portions of the canvas, rising majestically into a cloudy sky. Its lower sections are a formidable structure of massive stone blocks, featuring numerous arched windows, doorways, and internal passageways, suggesting a vast interior. As the tower ascends, the construction progresses from a finished, lighter stone façade to a more reddish, brick-like material that is actively under construction. Scaffolding, cranes, and countless minuscule workers are visible on every level, illustrating the immense scale of the undertaking. The tower is built into and around a natural rock formation on its right side, seamlessly integrating with the rugged terrain. Below the tower, in the foreground on the left, a large gathering of figures, including a central regal figure in a golden robe (likely King Nimrod, given the biblical context), stands amidst building materials such as cut stones and lumber. Many other workers are engaged in various tasks, moving materials, and constructing foundations. To the right of the tower, a bustling harbor scene unfolds with numerous ships of various sizes, some with sails unfurled, others docked, indicating active maritime trade and transport of materials. The background features a vast, verdant landscape stretching to the horizon under a pale blue sky, dotted with numerous tiny buildings, implying a sprawling city or countryside. A small, dark cloud formation is visible in the upper left sky. The meticulous detail in the painting emphasizes both the monumental ambition of the builders and the chaotic, ongoing nature of the construction.](images/ccfe62918f465f0da9e7e992161883779acc0cedf1a06b3283664227f7d02542.jpg)
Figure 13.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the Kunsthistorisches Museum, Vienna.

# 13.1.1 Word Order Typology

As we hinted at in our example above comparing English and Japanese, languages differ in the basic word order of verbs, subjects, and objects in simple declarative clauses. German, French, English, and Mandarin, for example, are all SVO (Subject-Verb-Object) languages, meaning that the verb tends to come between the subject and object. Hindi and Japanese, by contrast, are SOV languages, meaning that the verb tends to come at the end of basic clauses, and Irish and Arabic are VSO languages. Two languages that share their basic word order type often have other similarities. For example, VO languages generally have prepositions, whereas OV languages generally have postpositions.

Let’s look in more detail at the example we saw above. In this SVO English sentence, the verb wrote is followed by its object a letter and the prepositional phrase to a friend, in which the preposition to is followed by its argument a friend. Arabic, with a VSO order, also has the verb before the object and prepositions. By contrast, in the Japanese example that follows, each of these orderings is reversed; the verb is preceded by its arguments, and the postposition follows its argument.

(13.3) English: He wrote a letter to a friend Japanese: tomodachi ni tegami-o kaita friend to letter wrote Arabic: katabt risala ¯ li sadq ˙ wrote letter to friend

Other kinds of ordering preferences vary idiosyncratically from language to language. In some SVO languages (like English and Mandarin) adjectives tend to appear before nouns, while in others languages like Spanish and Modern Hebrew, adjectives appear after the noun:

(13.4) Spanish bruja verde English green witch

![## Image Analysis: 8d0bf4d9ea3475bbfe31e152ddfb41c9b6bd1b327dab686c05acab36754aa860.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates syntactic differences and word order typology across languages. Its main purpose is to visually demonstrate how the grammatical arrangement of words and phrases (constituents) can vary significantly between different languages, specifically English, German, and Mandarin. It communicates the key idea that languages have distinct rules for word order, and direct word-for-word translation often results in ungrammatical or unnatural sentences due to these structural divergences.

**Content Interpretation:**
The image clearly shows linguistic structural differences, specifically word order variations, between English and German in example (a), and between Mandarin and English in example (b).

Processes, Concepts, Relationships:
*   Constituent Reordering: The primary concept illustrated is how entire phrases or constituents are reordered when translating between languages, rather than just individual words. This is evidenced by the arrows connecting multi-word English phrases like "The green witch" to multi-word German phrases like "die grüne Hexe", and "at home" to "zu Hause".
*   Verb-Second (V2) Phenomenon (German): In example (a), the English sentence "The green witch is at home this week" is reordered in German to "Diese Woche ist die grüne Hexe zu Hause". The text "is" maps to "ist" and maintains the second position in the German sentence, while "this week" ("Diese Woche") moves to the initial position, displacing the original subject. This demonstrates the V2 word order characteristic of German, where the finite verb (here, "ist") always occupies the second syntactic position. This is supported by the fact that "ist" is the second word in the German sentence, and the arrow from "is" points to "ist".
*   Adverbial Placement (German): The temporal adverbial "this week" appears at the end of the English sentence but moves to the initial position in German ("Diese Woche"). This highlights that adverbs can frequently occur in initial positions in German for emphasis or topicalization, as stated in the document context.
*   Prepositional Phrase Placement (Mandarin): In example (b), the Mandarin phrase "cheng long dao xiang gang qu" translates to "Jackie Chan went to Hong Kong". The Mandarin phrase "dao xiang gang" (meaning "to Hong Kong," a prepositional phrase expressing a goal) appears *before* the verb "qu" (meaning "go" or "went"). In English, its equivalent "to Hong Kong" appears *after* the verb "went". This illustrates that in Mandarin, prepositional phrases expressing goals often occur pre-verbally, a significant difference from English where they typically follow the verb.

Significance:
*   These examples are significant because they visually represent the non-isomorphic nature of syntax across languages. They demonstrate that language translation is not a simple word-for-word substitution but involves complex structural transformations due to differing grammatical rules for word order.
*   The crossing arrows visually emphasize that constituents are not merely translated but are re-arranged, underscoring the typological differences in word order.

Support from Extracted Text:
*   The complete transcription of both the source and target language phrases (e.g., "The green witch" vs. "die grüne Hexe", "this week" vs. "Diese Woche", "dao xiang gang" vs. "to Hong Kong") serves as direct evidence for the specific constituents being compared.
*   The visual arrangement of these transcribed text boxes and the directions of the arrows explicitly show the original positions and the reordered positions of the corresponding phrases, confirming the word order differences.

**Key Insights:**
Main Takeaways/Lessons:
1.  Languages vary significantly in their preferred word order: The image clearly demonstrates that a direct translation of word order from one language to another often results in an ungrammatical or unnatural sentence. This is evident in both examples (a) and (b) where the constituents are rearranged.
2.  German exhibits Verb-Second (V2) word order and allows initial adverbs: Example (a) shows "this week" (temporal adverb) moving to sentence-initial position ("Diese Woche"), and the verb "is" ("ist") consistently appearing in the second position, supporting the concept of V2 word order.
3.  Mandarin places goal-oriented prepositional phrases pre-verbally: Example (b) illustrates that "dao xiang gang" (to Hong Kong) precedes "qu" (went) in Mandarin, contrary to English where "to Hong Kong" follows "went". This highlights a distinct syntactic rule for prepositional phrase placement.

Conclusions/Insights:
*   Understanding word order typology is crucial for linguistic analysis, machine translation, and language acquisition, as it reveals fundamental differences in how languages structure information.
*   The visual representation effectively highlights the non-linear relationship between sentence structures in different languages.

Textual Evidence:
*   For Takeaway 1: The juxtaposition of "The green witch is at home this week" and "Diese Woche ist die grüne Hexe zu Hause" (a), and "cheng long dao xiang gang qu" and "Jackie Chan went to Hong Kong" (b), along with the crossing arrows, visually proves that word order is not preserved across translations.
*   For Takeaway 2: In (a), "Diese Woche" is the first word, and "ist" is the second, directly demonstrating the V2 phenomenon and initial adverbial placement. The arrow from "this week" to "Diese Woche" reinforces the movement.
*   For Takeaway 3: In (b), the sequence "dao xiang gang" followed by "qu" in the Mandarin phrase, contrasted with "went" followed by "to Hong Kong" in the English phrase, provides direct textual evidence for the pre-verbal placement of the goal-oriented prepositional phrase in Mandarin.

**Document Context:**
This image directly supports the document's section "13.1.1 Word Order Typology" by providing concrete, visual examples of how word order can differ significantly between languages. It exemplifies the theoretical concepts of word order variations, specifically illustrating German's verb-second (V2) property and Mandarin's pre-verbal placement of goal-oriented prepositional phrases, which are explicitly mentioned in the "Text after image" context. These examples are essential for understanding the broader narrative about how languages are typologically classified based on their syntactic structures.

**Summary:**
The image presents two distinct linguistic examples, labeled (a) and (b), which visually compare the word order of sentences across different languages using blue text boxes connected by arrows. Each example shows a source language sentence in the top row and its corresponding translated sentence in the bottom row. Arrows link the corresponding phrases or words between the two languages, with crossing arrows indicating differences in their sequential arrangement.

Example (a) compares English and German word order:
*   The top row displays the English sentence "The green witch is at home this week" segmented into four phrases: "The green witch", "is", "at home", and "this week".
*   The bottom row shows the German translation "Diese Woche ist die grüne Hexe zu Hause", also segmented: "Diese Woche", "ist", "die grüne Hexe", and "zu Hause".
*   The arrows illustrate how these phrases map: An arrow connects "The green witch" to "die grüne Hexe", another connects "is" to "ist", a third connects "at home" to "zu Hause", and a fourth connects "this week" to "Diese Woche".
*   Crucially, the arrows for "this week" and "The green witch" cross, demonstrating a reordering. In English, "The green witch" (subject) is at the beginning, and "this week" (temporal adverb) is at the end. However, in German, "Diese Woche" (the temporal adverb) moves to the very beginning of the sentence, and "die grüne Hexe" (the subject) moves to the third position. The verb "ist" ("is") maintains its second position in the German sentence, highlighting the characteristic German Verb-Second (V2) word order.

Example (b) compares Mandarin and English word order:
*   The top row shows the Mandarin phrase "cheng long dao xiang gang qu", segmented into "cheng long", "dao xiang gang", and "qu".
*   The bottom row presents its English translation "Jackie Chan went to Hong Kong", segmented into "Jackie Chan", "went", and "to Hong Kong".
*   The arrows connect "cheng long" to "Jackie Chan", "dao xiang gang" to "to Hong Kong", and "qu" to "went".
*   Similar to example (a), arrows for "dao xiang gang" and "qu" cross, illustrating a reordering. In Mandarin, the phrase "dao xiang gang" (meaning "to Hong Kong" and indicating a goal) appears *before* the verb "qu" (meaning "went"). In the English translation, however, the equivalent phrase "to Hong Kong" appears *after* the verb "went". This exemplifies that Mandarin places prepositional phrases expressing goals pre-verbally, which is a structural difference from English.

Overall, the image effectively uses these two examples to visually explain fundamental differences in how languages arrange their words and phrases, reinforcing the concept of word order typology.](images/8d0bf4d9ea3475bbfe31e152ddfb41c9b6bd1b327dab686c05acab36754aa860.jpg)
Figure 13.2 Examples of other word order differences: (a) In German, adverbs occur in initial position that in English are more natural later, and tensed verbs occur in second position. (b) In Mandarin, preposition phrases expressing goals often occur pre-verbally, unlike in English.

Fig. 13.2 shows examples of other word order differences. All of these word order differences between languages can cause problems for translation, requiring the system to do huge structural reorderings as it generates the output.

# 13.1.2 Lexical Divergences

Of course we also need to translate the individual words from one language to another. For any translation, the appropriate word can vary depending on the context. The English source-language word bass, for example, can appear in Spanish as the fish lubina or the musical instrument bajo. German uses two distinct words for what in English would be called a wall: Wand for walls inside a building, and Mauer for walls outside a building. Where English uses the word brother for any male sibling, Chinese and many other languages have distinct words for older brother and younger brother (Mandarin gege and didi, respectively). In all these cases, translating bass, wall, or brother from English would require a kind of specialization, disambiguating the different uses of a word. For this reason the fields of MT and Word Sense Disambiguation (Appendix G) are closely linked.

Sometimes one language places more grammatical constraints on word choice than another. We saw above that English marks nouns for whether they are singular or plural. Mandarin doesn’t. Or French and Spanish, for example, mark grammatical gender on adjectives, so an English translation into French requires specifying adjective gender.

The way that languages differ in lexically dividing up conceptual space may be more complex than this one-to-many translation problem, leading to many-to-many mappings. For example, Fig. 13.3 summarizes some of the complexities discussed by Hutchins and Somers (1992) in translating English leg, foot, and paw, to French. For example, when leg is used about an animal it’s translated as French patte; but about the leg of a journey, as French etape; if the leg is of a chair, we use French pied.

Further, one language may have a lexical gap, where no word or phrase, short of an explanatory footnote, can express the exact meaning of a word in the other language. For example, English does not have a word that corresponds neatly to Mandarin xiao\` or Japanese oyakok¯ o¯ (in English one has to make do with awkward phrases like filial piety or loving child, or good son/daughter for both).

![## Image Analysis: a82dae3a508c8c345669abc321b920f3fab99cb46088567acf6ecadb27468d1c.jpg

**Conceptual Understanding:**
This image represents a conceptual model illustrating the complex semantic overlap and divergence between related terms in English and French. Specifically, it focuses on the English words 'leg', 'foot', and 'paw' and their various French counterparts: 'etape', 'patte', 'jambe', and 'pied'. The main purpose of this diagram is to visually demonstrate that direct, one-to-one translation equivalents are rare, and instead, words often share partial meanings or apply to different contexts across languages. It conveys the idea that the semantic fields of words are fluid and context-sensitive, highlighting the intricacies of lexical semantics and cross-linguistic comparison discussed in the academic field of linguistics and translation studies. The key ideas communicated are polysemy, semantic networks, and the challenges of achieving perfect lexical equivalence between languages.

**Content Interpretation:**
This image illustrates the complex semantic relationships and lexical divergences between English words ('leg', 'foot', 'paw') and their corresponding French terms ('etape', 'patte', 'jambe', 'pied'). It visually represents how a single word in one language can map to multiple words in another, depending on the specific context or domain. The diagram shows that word meanings are not always directly translatable but often overlap in intricate ways, revealing shared semantic features in certain contexts while diverging in others. The smaller green text labels at the intersections explicitly define the shared contextual meaning for that particular overlap, for example, 'ANIMAL' as a shared context between 'leg' and 'patte', or 'HUMAN' between 'leg' and 'jambe'. This highlights the polysemy of words within and across languages and the challenges in achieving exact lexical equivalence during translation.

**Key Insights:**
The main takeaway from this image is that lexical equivalence between languages is often not a simple one-to-one mapping. Instead, words, even for concrete concepts like body parts, can have complex and overlapping semantic fields that are highly context-dependent. This diagram demonstrates that a single English word (e.g., 'leg') can correspond to several different French words ('etape', 'jambe', 'patte', 'pied'), with each correspondence being valid in a specific semantic context (e.g., 'JOURNEY', 'HUMAN', 'ANIMAL', 'CHAIR'). Conversely, a single French word (e.g., 'patte') can translate to different English words ('leg', 'paw', 'foot') based on the specific animal or context ('ANIMAL', 'BIRD'). This illustrates the concept of polysemy and the intricate nature of semantic analysis in cross-linguistic studies, emphasizing the importance of considering context when translating or comparing lexicons. The explicit textual labels like 'HUMAN', 'ANIMAL', 'JOURNEY', 'CHAIR', and 'BIRD' at the intersections provide direct evidence for these context-specific mappings and the complexity of lexical divergences.

**Document Context:**
This image directly supports the document's section '13.1.2 Lexical Divergences' and the subsequent text, 'Figure 13.3 The complex overlap between English leg, foot, etc., and various French translations as discussed by Hutchins and Somers (1992)'. It serves as a visual example of the phenomenon of lexical divergence, illustrating the core argument that words do not always have direct equivalents across languages. By showing the intricate web of overlaps and context-specific meanings between English 'leg', 'foot', and 'paw' and French 'etape', 'patte', 'jambe', and 'pied', the image provides concrete evidence for the theoretical discussion on lexical semantics and cross-linguistic comparison. It helps readers understand the specific challenges and nuances involved in translating seemingly simple terms.

**Summary:**
The image is a complex Venn diagram illustrating the semantic overlaps and divergences between certain English and French terms, specifically English 'leg', 'foot', and 'paw' and their various French translations such as 'etape', 'patte', 'jambe', and 'pied'. The diagram visually represents how these words do not have direct one-to-one equivalents across the two languages, but rather share meanings based on specific contexts. The central concept is to demonstrate the nuanced and context-dependent nature of translation and lexical fields. The description starts by identifying the main English and French terms and then meticulously details the exact points of overlap, explaining the specific contextual meanings associated with each intersection. For instance, the word 'leg' in English has connections to French 'etape' in the context of 'JOURNEY', 'jambe' for 'HUMAN' legs, 'patte' for 'ANIMAL' legs, and 'pied' when referring to a 'CHAIR' leg, highlighting the polysemy and cross-linguistic complexity. Every piece of text, including the main terms and the micro-detail context labels at the intersections, is transcribed and used to explain the relationships, ensuring no information is omitted.](images/a82dae3a508c8c345669abc321b920f3fab99cb46088567acf6ecadb27468d1c.jpg)
Figure 13.3 The complex overlap between English leg, foot, etc., and various French translations as discussed by Hutchins and Somers (1992).

Finally, languages differ systematically in how the conceptual properties of an event are mapped onto specific words. Talmy (1985, 1991) noted that languages can be characterized by whether direction of motion and manner of motion are marked on the verb or on the “satellites”: particles, prepositional phrases, or adverbial phrases. For example, a bottle floating out of a cave would be described in English with the direction marked on the particle out, while in Spanish the direction would be marked on the verb:

(13.5) English: The bottle floated out. Spanish: La botella salio´ flotando. The bottle exited floating.

satellite-framed

Verb-framed languages mark the direction of motion on the verb (leaving the satellites to mark the manner of motion), like Spanish acercarse ‘approach’, alcanzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the direction of motion on the satellite (leaving the verb to mark the manner of motion), like English crawl out, float off, jump down, run after. Languages like Japanese, Tamil, and the many languages in the Romance, Semitic, and Mayan languages families, are verb-framed; Chinese as well as non-Romance Indo-European languages like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991, Slobin 1996).

# 13.1.3 Morphological Typology

isolating polysynthetic

Morphologically, languages are often characterized along two dimensions of variation. The first is the number of morphemes per word, ranging from isolating languages like Vietnamese and Cantonese, in which each word generally has one morpheme, to polysynthetic languages like Siberian Yupik (“Eskimo”), in which a single word may have very many morphemes, corresponding to a whole sentence in English. The second dimension is the degree to which morphemes are segmentable, ranging from agglutinative languages like Turkish, in which morphemes have relatively clean boundaries, to fusion languages like Russian, in which a single affix

agglutinative fusion

may conflate multiple morphemes, like -om in the word stolom (table-SG-INSTRDECL1), which fuses the distinct morphological categories instrumental, singular, and first declension.

Translating between languages with rich morphology requires dealing with structure below the word level, and for this reason modern systems generally use subword models like the wordpiece or BPE models of Section 13.2.1.

# referential density

cold language hot language

# 13.1.4 Referential density

Finally, languages vary along a typological dimension related to the things they tend to omit. Some languages, like English, require that we use an explicit pronoun when talking about a referent that is given in the discourse. In other languages, however, we can sometimes omit pronouns altogether, as the following example from Spanish shows1:

(13.6) [El jefe]i dio con un libro. $\varnothing _ { i }$ Mostro su hallazgo a un descifrador ambulante.´ [The boss] came upon a book. [He] showed his find to a wandering decoder.

Languages that can omit pronouns are called pro-drop languages. Even among the pro-drop languages, there are marked differences in frequencies of omission. Japanese and Chinese, for example, tend to omit far more than does Spanish. This dimension of variation across languages is called the dimension of referential density. We say that languages that tend to use more pronouns are more referentially dense than those that use more zeros. Referentially sparse languages, like Chinese or Japanese, that require the hearer to do more inferential work to recover antecedents are also called cold languages. Languages that are more explicit and make it easier for the hearer are called hot languages. The terms hot and cold are borrowed from Marshall McLuhan’s 1964 distinction between hot media like movies, which fill in many details for the viewer, versus cold media like comics, which require the reader to do more inferential work to fill out the representation (Bickel, 2003).

Translating from languages with extensive pro-drop, like Chinese or Japanese, to non-pro-drop languages like English can be difficult since the model must somehow identify each zero and recover who or what is being talked about in order to insert the proper pronoun.

# 13.2 Machine Translation using Encoder-Decoder

The standard architecture for MT is the encoder-decoder transformer or sequenceto-sequence model, an architecture we saw for RNNs in Chapter 8. We’ll see the details of how to apply this architecture to transformers in Section 13.3, but first let’s talk about the overall task.

Most machine translation tasks make the simplification that we can translate each sentence independently, so we’ll just consider individual sentences for now. Given a sentence in a source language, the MT task is then to generate a corresponding sentence in a target language. For example, an MT system is given an English sentence like

The green witch arrived and must translate it into the Spanish sentence:

# Llego la bruja verde ´

MT uses supervised machine learning: at training time the system is given a large set of parallel sentences (each sentence in a source language matched with a sentence in the target language), and learns to map source sentences into target sentences. In practice, rather than using words (as in the example above), we split the sentences into a sequence of subword tokens (tokens can be words, or subwords, or individual characters). The systems are then trained to maximize the probability of the sequence of tokens in the target language $y _ { 1 } , . . . , y _ { m }$ given the sequence of tokens in the source language $x _ { 1 } , . . . , x _ { n }$ :

$$
P ( y _ { 1 } , \dots , y _ { m } | x _ { 1 } , \dots , x _ { n } )
$$

Rather than use the input tokens directly, the encoder-decoder architecture consists of two components, an encoder and a decoder. The encoder takes the input words $x = [ x _ { 1 } , \ldots , x _ { n } ]$ and produces an intermediate context h. At decoding time, the system takes $h$ and, word by word, generates the output $y$ :

$$
\begin{array} { l } { \displaystyle \mathbf { h } = \mathrm { e n c o d e r } ( x ) } \\ { y _ { t + 1 } = \mathrm { d e c o d e r } ( \mathbf { h } , y _ { 1 } , \ldots , y _ { t } ) \quad \forall t \in [ 1 , \ldots , m ] } \end{array}
$$

In the next two sections we’ll talk about subword tokenization, and then how to get parallel corpora for training, and then we’ll introduce the details of the encoderdecoder architecture.

# 13.2.1 Tokenization

Machine translation systems use a vocabulary that is fixed in advance, and rather than using space-separated words, this vocabulary is generated with subword tokenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared vocabulary is used for the source and target languages, which makes it easy to copy tokens (like names) from source to target. Using subword tokenization with tokens shared between languages makes it natural to translate between languages like English or Hindi that use spaces to separate words, and languages like Chinese or Thai that don’t.

We build the vocabulary by running a subword tokenization algorithm on a corpus that contains both source and target language data.

Rather than the simple BPE algorithm from Fig. 2.13, modern systems often use more powerful tokenization algorithms. Some systems (like BERT) use a variant of BPE called the wordpiece algorithm, which instead of choosing the most frequent set of tokens to merge, chooses merges based on which one most increases the language model probability of the tokenization. Wordpieces use a special symbol at the beginning of each token; here’s a resulting tokenization from the Google MT system (Wu et al., 2016):

words: Jet makers feud over seat width with big orders at stake wordpieces: J et makers fe ud over seat width with big orders at stake

The wordpiece algorithm is given a training corpus and a desired vocabulary size V, and proceeds as follows:

1. Initialize the wordpiece lexicon with characters (for example a subset of Unicode characters, collapsing all the remaining characters to a special unknown character token).

2. Repeat until there are V wordpieces:

unigram SentencePiece

(a) Train an n-gram language model on the training corpus, using the current set of wordpieces.   
(b) Consider the set of possible new wordpieces made by concatenating two wordpieces from the current lexicon. Choose the one new wordpiece that most increases the language model probability of the training corpus.

Recall that with BPE we had to specify the number of merges to perform; in wordpiece, by contrast, we specify the total vocabulary, which is a more intuitive parameter. A vocabulary of 8K to 32K word pieces is commonly used.

An even more commonly used tokenization algorithm is (somewhat ambiguously) called the unigram algorithm (Kudo, 2018) or sometimes the SentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raffel et al., 2020). (Because unigram is the default tokenization algorithm used in a library called SentencePiece that adds a useful wrapper around tokenization algorithms (Kudo and Richardson, 2018b), authors often say they are using SentencePiece tokenization but really mean they are using the unigram algorithm).

In unigram tokenization, instead of building up a vocabulary by merging tokens, we start with a huge vocabulary of every individual unicode character plus all frequent sequences of characters (including all space-separated words, for languages with spaces), and iteratively remove some tokens to get to a desired final vocabulary size. The algorithm is complex (involving suffix-trees for efficiently storing many tokens, and the EM algorithm for iteratively assigning probabilities to tokens), so we don’t give it here, but see Kudo (2018) and Kudo and Richardson (2018b). Roughly speaking the algorithm proceeds iteratively by estimating the probability of each token, tokenizing the input data using various tokenizations, then removing a percentage of tokens that don’t occur in high-probability tokenization, and then iterates until the vocabulary has been reduced down to the desired number of tokens.

Why does unigram tokenization work better than BPE? BPE tends to create lots of very small non-meaningful tokens (because BPE can only create larger words or morphemes by merging characters one at a time), and it also tends to merge very common tokens, like the suffix ed, onto their neighbors. We can see from these examples from Bostrom and Durrett (2020) that unigram tends to produce tokens that are more semantically meaningful:

<table><tr><td>Original: corrupted</td><td></td><td></td><td> Original: Completely preposterous suggestions</td></tr><tr><td>BPE:</td><td>cor rupted</td><td>BPE:</td><td> Comple t ely prep ost erous suggest ions</td></tr><tr><td>Unigram: corrupt ed</td><td></td><td></td><td>Unigram: Complete ly pre post er ous suggestion s</td></tr></table>

# 13.2.2 Creating the Training data

# parallel corpus

Europarl

Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages. Large numbers of parallel corpora are available. Some are governmental; the Europarl corpus (Koehn, 2005), extracted from the proceedings of the European Parliament, contains between 400,000 and 2 million sentences each from 21 European languages. The United Nations Parallel Corpus contains on the order of 10 million sentences in the six official languages of the United Nations (Arabic, Chinese, English, French, Russian, Spanish) Ziemski et al. (2016). Other parallel corpora have been made from movie and TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from general web text, like the ParaCrawl corpus of 223 million sentence pairs between 23 EU languages and English extracted from the CommonCrawl Ban˜on et al. ´ (2020).

# Sentence alignment

Standard training corpora for MT come as aligned pairs of sentences. When creating new corpora, for example for underresourced languages or new domains, these sentence alignments must be created. Fig. 13.4 gives a sample hypothetical sentence alignment.

![## Image Analysis: 6e2063c54982e679c081496a9b84df08767a29a2522f8dc6218b6296d2374195.jpg

**Conceptual Understanding:**
The image conceptually represents the task of 'sentence alignment' in parallel texts. Its main purpose is to visually illustrate how individual sentences from a source language (English) correspond to their translated counterparts in a target language (French). The image conveys the key idea that this correspondence is not always a simple one-to-one mapping, but can involve complex relationships such as multiple source sentences mapping to a single target sentence, or sentences existing without a direct translation in the other language.

**Content Interpretation:**
The image illustrates the process and outcomes of sentence alignment, a fundamental task in natural language processing and machine translation. It presents a parallel corpus segment, specifically demonstrating how sentences from an English text (e.g., E1-E8) are aligned with their corresponding translations in a French text (e.g., F1-F7). The various connecting lines reveal different alignment types: one-to-one alignments (e.g., E1 to F1), many-to-one alignments where multiple source sentences translate into a single target sentence (e.g., E2 and E3 to F2, or E7 and E8 to F7), and null alignments where a source or target sentence has no direct counterpart (e.g., F5 has no explicit English sentence connected to it in the diagram, implying a null alignment from English or an insertion in French without a direct English source). The visual representation clearly shows that translation is not always a simple one-to-one mapping of sentences.

**Key Insights:**
The main takeaway from this image is that sentence alignment in translation is not a straightforward one-to-one process but can involve various complexities. It teaches that: 1. Sentences can have direct one-to-one translations (e.g., E1 to F1, E4 to F3, E5 to F4, E6 to F6). 2. Multiple sentences in one language can collectively translate into a single sentence in another language (e.g., English sentences E2 and E3 together translate to French sentence F2, and E7 and E8 together translate to F7). 3. Sentences might exist in one language without a direct corresponding sentence in the other, indicating a 'null alignment' (e.g., French sentence F5 appears to be an insertion or elaboration without a distinct English source sentence shown in this particular alignment, or the reverse for an English sentence if it were not aligned). The image effectively demonstrates the real-world challenges and nuances in creating parallel corpora for machine translation.

**Document Context:**
This image is directly relevant to the 'Sentence alignment' section of the document. It serves as a concrete, illustrative example to explain the theoretical concept of sentence alignment introduced in the surrounding text. The figure provides visual evidence of the different alignment types (1-1, 2-1, null) mentioned in the explanatory text, thereby enhancing the reader's understanding of how sentences are matched across languages in practical applications of natural language processing.

**Summary:**
This image visually demonstrates different types of sentence alignments between English and French texts, specifically from Antoine de Saint-Exupery's Le Petit Prince. It shows how original sentences in one language correspond to their translated counterparts in another, illustrating various alignment patterns such as one-to-one, many-to-one, and null alignments. The left column presents eight English sentences, labeled E1 through E8, while the right column presents seven French sentences, labeled F1 through F7. Lines connect the corresponding sentences to show their translation alignments. The description highlights that sentence alignment involves finding minimal sets of sentences that are translations of each other, providing a clear and comprehensive example of this linguistic task.](images/6e2063c54982e679c081496a9b84df08767a29a2522f8dc6218b6296d2374195.jpg)
Figure 13.4 A sample alignment between sentences in English and French, with sentences extracted from Antoine de Saint-Exupery’s $L e$ Petit Prince and a hypothetical translation. Sentence alignment takes sentences $e _ { 1 } , . . . , e _ { n }$ , and $f _ { 1 } , . . . , f _ { m }$ and finds minimal sets of sentences that are translations of each other, including single sentence mappings like $\left( \mathrm { e } _ { 1 } , \mathrm { f } _ { 1 } \right)$ , $( \mathrm { e } _ { 4 } , \mathrm { f } _ { 3 } )$ , $( \mathrm { e } _ { 5 } , \mathrm { f } _ { 4 } )$ , $\left( \mathrm { e } _ { 6 } , \mathrm { f } _ { 6 } \right)$ as well as 2-1 alignments $( \mathrm { e } _ { 2 } / \mathrm { e } _ { 3 } \mathrm { , f } _ { 2 } )$ , $( \mathrm { e } _ { 7 } / \mathrm { e } _ { 8 } \mathrm { , f } _ { 7 } )$ , and null alignments (f5).

Given two documents that are translations of each other, we generally need two steps to produce sentence alignments:

• a cost function that takes a span of source sentences and a span of target sentences and returns a score measuring how likely these spans are to be translations.   
• an alignment algorithm that takes these scores to find a good alignment between the documents.

To score the similarity of sentences across languages, we need to make use of a multilingual embedding space, in which sentences from different languages are in the same embedding space (Artetxe and Schwenk, 2019). Given such a space, cosine similarity of such embeddings provides a natural scoring function (Schwenk, 2018). Thompson and Koehn (2019) give the following cost function between two sentences or spans $x , y$ from the source and target documents respectively:

$$
c ( x , y ) = \frac { ( 1 - \cos ( x , y ) ) \mathrm { n S e n t s } ( x ) \mathrm { n S e n t s } ( y ) } { \sum _ { s = 1 } ^ { S } 1 - \cos ( x , y _ { s } ) + \sum _ { s = 1 } ^ { S } 1 - \cos ( x _ { s } , y ) }
$$

where nSents() gives the number of sentences (this biases the metric toward many alignments of single sentences instead of aligning very large spans). The denominator helps to normalize the similarities, and so $x _ { 1 } , . . . , x _ { S } , y _ { 1 } , . . . , y _ { S }$ , are randomly selected sentences sampled from the respective documents.

Usually dynamic programming is used as the alignment algorithm (Gale and Church, 1993), in a simple extension of the minimum edit distance algorithm we introduced in Chapter 2.

Finally, it’s helpful to do some corpus cleanup by removing noisy sentence pairs. This can involve handwritten rules to remove low-precision pairs (for example removing sentences that are too long, too short, have different URLs, or even pairs that are too similar, suggesting that they were copies rather than translations). Or pairs can be ranked by their multilingual embedding cosine score and low-scoring pairs discarded.

# 13.3 Details of the Encoder-Decoder Model

![## Image Analysis: db8cb0fee69f9014105d57444d690655a34d7b788290ab5e9fec195f65077c9e.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of an Encoder-Decoder Transformer model, specifically applied to a sequence-to-sequence task like machine translation. The main purpose is to illustrate how an input sentence in one language ("The green witch arrived" - English) is transformed into an output sentence in another language ("llegó la bruja verde" - Spanish) using two interconnected neural network components: an Encoder and a Decoder. Both components are built upon "transformer blocks" and employ an explicit "cross-attention" mechanism. The key ideas communicated are the separation of encoding and decoding functions, the use of powerful "transformer blocks" for contextual understanding, and the iterative, attentive generation of the target sequence.

**Content Interpretation:**
The image depicts the "Encoder-Decoder" system, a foundational architecture in modern natural language processing for tasks like machine translation. It shows the encoding of the English sentence "The green witch arrived" and its decoding into the Spanish sentence "llegó la bruja verde". The "Encoder" processes the source language sentence "The green witch arrived" using multiple "transformer blocks" to generate a rich, context-aware representation. The explicit label "Encoder" and the input words "The green witch arrived" (blue text) confirm this. The label "transformer blocks" on the left emphasizes the underlying computational units. The "Decoder" takes the encoded representation from the "Encoder" and iteratively generates the target language sentence. It starts with a special token, "<s>", and predicts subsequent words ("llegó", "la", "bruja", "verde", "</s>") one at a time. This is evident from the "Decoder" label, the bottom row of input tokens/words starting with "<s>", and the top row of output words ending with "</s>" (red text). The vertical arrows indicate output generation. A critical relationship shown is "cross-attention", which allows each decoding step to "look at" or "attend to" different parts of the encoded source sentence. This is explicitly labeled "cross-attention" and represented by multiple lines connecting the top of the Encoder's "transformer blocks" to the middle layers of the Decoder's "transformer blocks" for each output word. This signifies that the Decoder leverages the comprehensive understanding of the source sentence provided by the Encoder to make informed decisions about the next word to generate. The dotted lines within the "Decoder" horizontally connecting the "transformer blocks" for sequential words, and vertically connecting the input words to their respective block stacks, imply the auto-regressive nature of the decoder, where it considers previously generated tokens to predict the next.

**Key Insights:**
The main takeaways from this image are: 1. **Two-Phase Machine Translation:** The image clearly illustrates that machine translation using this architecture is a two-phase process: an "Encoder" phase to understand the source language and a "Decoder" phase to generate the target language. This is directly supported by the distinct bounding boxes labeled "Encoder" and "Decoder". 2. **Transformer Blocks as Core Components:** Both the "Encoder" and "Decoder" are built using "transformer blocks". This indicates that the fundamental computational units responsible for processing sequences and understanding context are applied uniformly across both stages. This is evident from the "transformer blocks" label and the visual representation of stacked blocks within both the "Encoder" and "Decoder". 3. **Cross-Attention for Inter-Lingual Context:** The "cross-attention" mechanism is crucial for the "Decoder" to effectively translate, allowing it to focus on relevant parts of the source sentence's encoded representation. The label "cross-attention" and the lines connecting the "Encoder" output to the "Decoder" inputs provide direct evidence for this. 4. **Auto-regressive Decoding:** The "Decoder" generates the target sentence word by word, where each subsequent word prediction depends on the previously generated words. This is demonstrated by the sequential flow implied by the bottom row of red text ("<s>", "llegó", "la", "bruja", "verde") acting as inputs for subsequent predictions, and the dotted lines connecting the processing blocks for adjacent words within the "Decoder". 5. **Sentence Boundary Tokens:** Special tokens like "<s>" (start of sequence) and "</s>" (end of sequence) are used to explicitly mark the beginning and end of the generated sequence, essential for the model to know when to start and stop decoding. This is evident from their presence in the Decoder input and output.

**Document Context:**
This image fits perfectly within a document section titled "13.3 Details of the Encoder-Decoder Model," especially when followed by text explaining that the encoder uses transformer blocks from Chapter 8, and the decoder uses a more powerful block with an extra cross-attention layer to attend to all encoder words. It visually illustrates the conceptual framework being discussed, serving as a concrete example of the architectural components and their interplay in a machine translation task. It clarifies how the abstract "transformer blocks" from earlier discussions are integrated into a complete, functional model for sequence-to-sequence processing.

**Summary:**
The diagram presents a visual explanation of an Encoder-Decoder Transformer architecture, a sophisticated neural network design commonly used for tasks like machine translation. This particular example shows the translation of the English sentence "The green witch arrived" into the Spanish sentence "llegó la bruja verde". The architecture is divided into two main parts: The Encoder: Located on the left, the "Encoder" is responsible for understanding the input sentence in the source language (English, in this case). The words "The", "green", "witch", and "arrived" are fed into the Encoder. Each word, or its representation, passes through multiple layers of "transformer blocks". These blocks are powerful processing units that analyze the entire input sequence simultaneously, allowing each word to be understood in the context of all other words in the sentence. The lines originating from the top of the Encoder's blocks represent the rich, contextualized representation of the input sentence that the Encoder produces. The Decoder: Situated on the right, the "Decoder" takes the Encoder's understanding of the source sentence and iteratively generates the translated sentence in the target language (Spanish). The decoding process begins with a special "start of sequence" token, "<s>". For each word it needs to generate, the Decoder processes a sequence of previously generated words (starting with "<s>") through its own set of "transformer blocks". Crucially, during each step of generation, the Decoder uses a mechanism called "cross-attention". This is indicated by the lines labeled "cross-attention" that connect from the top of the Encoder's blocks to the middle layers of the Decoder's blocks. "Cross-attention" allows the Decoder to "look back" at the entire encoded source sentence and determine which parts are most relevant for predicting the current target word. The Decoder outputs one word at a time: first "llegó", then "la", then "bruja", and finally "verde". The dotted lines within the Decoder illustrate its auto-regressive nature: to predict a word, it considers the words it has already generated. For example, to predict "la", it would have considered "<s>" and "llegó" as part of its input context, in addition to the Encoder's output. The process concludes when the Decoder generates an "end of sequence" token, "</s>", signaling that the translation is complete. In summary, this diagram effectively visualizes how an Encoder first comprehends an entire input sentence, and then a Decoder, aided by "cross-attention" to that comprehension, constructs the translated sentence word by word. Both components leverage "transformer blocks" for their internal processing, making this a powerful and widely used architecture for sequence-to-sequence tasks.](images/db8cb0fee69f9014105d57444d690655a34d7b788290ab5e9fec195f65077c9e.jpg)
Figure 13.5 The encoder-decoder transformer architecture for machine translation. The encoder uses the transformer blocks we saw in Chapter 8, while the decoder uses a more powerful block with an extra crossattention layer that can attend to all the encoder words. We’ll see this in more detail in the next section.

The standard architecture for MT is the encoder-decoder transformer. The encoderdecoder architecture was introduced already for RNNs in Chapter 8, and the transformer version has the same idea. Fig. 13.5 shows the intuition of the architecture at a high level. You’ll see that the encoder-decoder architecture is made up of two transformers: an encoder, which is the same as the basic transformers from Chapter 9, and a decoder, which is augmented with a special new layer called the cross-attention layer. The encoder takes the source language input word tokens $\mathbf { \boldsymbol { x } } = \mathbf { \boldsymbol { x } } _ { 1 } , . . . , \mathbf { \boldsymbol { x } } _ { n }$ and maps them to an output representation ${ \mathsf { \mathbf { \mathbf { H } } } } ^ { e n c } = { \mathsf { \mathbf { h } } } _ { 1 } , . . . , { \mathsf { \mathbf { h } } } _ { n }$ ; via a stack of encoder blocks.

The decoder is essentially a conditional language model that attends to the encoder representation and generates the target words one by one, at each timestep conditioning on the source sentence and the previously generated target language words to generate a token. Decoding can use any of the decoding methods discussed in Chapter 9 like greedy, or temperature or nucleus sampling. But the most common decoding algorithm for MT is the beam search algorithm that we’ll introduce in Section 13.4.

But the components of the architecture differ somewhat from the transformer block we’ve seen. First, in order to attend to the source language, the transformer blocks in the decoder have an extra cross-attention layer. Recall that the transformer block of Chapter 9 consists of a self-attention layer that attends to the input from the previous layer, followed by layer norm, a feed forward layer, and another layer norm. The decoder transformer block includes an extra layer with a special kind of attention, cross-attention (also sometimes called encoder-decoder attention or source attention). Cross-attention has the same form as the multi-head attention in a normal transformer block, except that while the queries as usual come from the previous layer of the decoder, the keys and values come from the output of the encoder.

![## Image Analysis: e07e9bdd963e3f263149ce406cb2c048d33b393cb8d2ade9bd95ef5193935dca.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural details of a Transformer model, specifically illustrating the interaction between its encoder and decoder components. The main purpose is to provide a detailed visual explanation of how an input sequence is processed to generate an output sequence using self-attention, cross-attention, and feedforward neural networks within a multi-layered block structure. It communicates the core ideas of sequence-to-sequence learning through attention mechanisms, the modularity of the Transformer, and the flow of information from the encoder to the decoder to provide context for output generation.

**Content Interpretation:**
The image displays the full architecture of a Transformer model, specifically detailing its encoder and decoder components and their interconnections. It illustrates the internal structure of a single encoder block and a single decoder block, and how multiple such blocks are stacked to form the complete encoder and decoder stacks. Key concepts demonstrated are different types of attention mechanisms (Multi-Head Attention, Causal Multi-Head Attention, Cross-Attention), Layer Normalization, Feedforward networks, and residual connections (add and normalize). The primary process shown is the transformation of an input sequence (x1...xn) into an output sequence (y1...ym) by first encoding the input into a contextual representation (H^enc) and then decoding this representation, along with the decoder's own input, to generate the final output. The significance lies in how the encoder's output (H^enc) serves as a crucial context for the decoder via the Cross-Attention layers, enabling the decoder to generate an output sequence that is conditioned on the entire input sequence.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Modular Architecture:** The Transformer model uses a stack of identical encoder and decoder blocks (Block 2 to Block K/L) to build deep networks.
2.  **Encoder Function:** The Encoder processes an input sequence (x1...xn) through self-attention (Multi-Head Attention), residual connections, layer normalization, and feedforward networks to produce a rich contextual representation (H^enc = h1...hn).
3.  **Decoder Function:** The Decoder generates an output sequence (y1...ym) by first performing causal self-attention (Causal Multi-Head Attention) to prevent looking at future tokens, then cross-attention to incorporate context from the encoder's output (H^enc), followed by feedforward networks, layer normalization, and residual connections. The output then passes through a Language Modeling Head for final prediction.
4.  **Role of H^enc:** The final output of the encoder (H^enc) is explicitly passed to *each* "Cross-Attention" layer within the decoder, providing the necessary context (Keys and Values, as per the surrounding text) for the decoder to attend to the input sequence.
5.  **Attention Mechanisms:** The image clearly differentiates between three types of attention: "Multi-Head Attention" in the encoder (self-attention on input), "Causal Multi-Head Attention" in the decoder (self-attention on decoder's input, masked for causality), and "Cross-Attention" in the decoder (attending to encoder output using decoder's previous output).
6.  **Key Components:** "Layer Normalize" is used after attention and feedforward layers, combined with residual connections (implied by the '+' symbols and curved arrows), which are crucial for stable training of deep neural networks.

**Textual Evidence:**
*   "Encoder" and "Decoder" labels define the two main parts.
*   "x1, x2, x3, ..., xn" explicitly label the encoder input.
*   "h1, h2, h3, ..., hn" and "H^enc" explicitly label the encoder output.
*   "y1, y2, y3, ..., ym" explicitly label the decoder output.
*   "Multi-Head Attention", "Causal Multi-Head Attention", and "Cross-Attention" clearly name the distinct attention mechanisms.
*   "Layer Normalize" and "Feedforward" are explicitly labeled as processing steps within each block.
*   The arrows from H^enc pointing to the "Cross-Attention" layers in the decoder visually confirm the flow of context.
*   "Block 1", "Block 2", "Block K", "Block L" indicate the stacking of multiple identical blocks.

**Document Context:**
This image is presented in Section 13.3, titled "Details of the Encoder-Decoder Model," which directly aligns with its content. The text following the image, "Figure 13.6 The transformer block for the encoder and the decoder. The final output of the encoder H^enc = h1, ..., hn is the context used in the decoder. The decoder is a standard transformer except with one extra layer, the cross-attention layer, which takes that encoder output H^enc and uses it to form its K and V inputs," confirms that the image visually explains the specific details of the transformer architecture, particularly highlighting the role of H^enc and the cross-attention layer in the decoder. It serves as a detailed visual aid for understanding the conceptual model described in the surrounding text, providing a block-level and internal-component-level view of how the encoder and decoder function and interact.

**Summary:**
This image illustrates the detailed architecture of a Transformer encoder-decoder model, which is a foundational component in many sequence-to-sequence tasks like machine translation. The model is composed of two main parts: an Encoder on the left and a Decoder on the right.

**Encoder:** The encoder takes an input sequence (x1, x2, x3, ..., xn) at the bottom. This input first passes through "Encoder Block 1". Within this block, the sequence goes into a "Multi-Head Attention" layer. The output of this layer is added (indicated by '+') to its input, and then normalized by a "Layer Normalize" component. This normalized output is then fed into a "Feedforward" network. The output of the Feedforward network is also added (via a skip connection, indicated by the curved arrow and '+') to its input from the Layer Normalize step, and then normalized again by another "Layer Normalize" component. This completes one "Encoder Block". The image shows that this process repeats for "Block 2" up to "Block K", indicating a stack of K identical encoder blocks. The final output of the encoder stack is denoted as H^enc, which consists of individual outputs (h1, h2, h3, ..., hn).

**Decoder:** The decoder takes an input (implicitly from a previous token or start token, which then passes through the Causal Multi-Head Attention layer). Similar to the encoder, this input first enters "Decoder Block 1". Within this block, the sequence goes through a "Causal Multi-Head Attention" layer (distinguished from the encoder's Multi-Head Attention as it prevents attending to future positions in the sequence). The output of this causal attention layer is added to its input and normalized by a "Layer Normalize" component. This normalized output is then fed into a "Cross-Attention" layer. Crucially, the "Cross-Attention" layer receives additional input from the Encoder's final output, H^enc (h1, ..., hn), as indicated by the arrows connecting H^enc to this layer in Decoder Block 1, and similarly for subsequent blocks. The output of the Cross-Attention layer is added to its input and normalized by a "Layer Normalize" component. Finally, this output passes through a "Feedforward" network, whose output is again added to its input and normalized by a final "Layer Normalize" component within the block. This completes one "Decoder Block". The image shows that this process repeats for "Block 2" up to "Block L", indicating a stack of L identical decoder blocks. The output of the last decoder block (Block L) is then fed into a "Language Modeling Head". This head consists of an "Unembedding Matrix" which processes the decoder's output to produce the final output sequence (y1, y2, y3, ..., ym), each potentially representing a probability distribution over vocabulary words (implied by the small bar-graph-like icons above y1...ym).

In summary, the encoder processes the input sequence, and its output provides contextual information to each cross-attention layer within the decoder, which then generates the output sequence.](images/e07e9bdd963e3f263149ce406cb2c048d33b393cb8d2ade9bd95ef5193935dca.jpg)
Figure 13.6 The transformer block for the encoder and the decoder. The final output of the encoder ${ \boldsymbol { \mathsf { H } } } ^ { e n c } =$ $\mathbf { h } _ { 1 } , . . . , \mathbf { h } _ { n }$ is the context used in the decoder. The decoder is a standard transformer except with one extra layer, the cross-attention layer, which takes that encoder output $\mathsf { H } ^ { e n c }$ and uses it to form its $\mathsf { \pmb K }$ and $\pmb { v }$ inputs.

That is, where in standard multi-head attention the input to each attention layer is $\pmb { \times }$ , in cross attention the input is the the final output of the encoder ${ \mathsf { \mathbf { H } } } ^ { e n c } = { \mathsf { \mathbf { h } } } _ { 1 } , . . . , { \mathsf { \mathbf { h } } } _ { n }$ . ${ \sf H } ^ { e n c }$ is of shape $[ n \times d ]$ , each row representing one input token. To link the keys and values from the encoder with the query from the prior layer of the decoder, we multiply the encoder output ${ \sf H } ^ { e n c }$ by the cross-attention layer’s key weights $\boldsymbol { \mathsf { W } } ^ { \mathsf { K } }$ and value weights $\boldsymbol { \mathsf { W } } ^ { \boldsymbol { \mathsf { v } } }$ . The query comes from the output from the prior decoder layer $\mathsf { \pm } d e c [ \ell - 1 ]$ , which is multiplied by the cross-attention layer’s query weights $\boldsymbol { \mathsf { W } } ^ { \mathbf { Q } }$ :

$$
\mathbf { Q } = \mathsf { \mathbf { H } } ^ { d e c [ \ell - 1 ] } \mathsf { \mathbf { W } } ^ { \mathbf { Q } } ; \mathsf { \nabla } \mathsf { K } = \mathsf { \mathbf { H } } ^ { e n c } \mathsf { \mathbf { W } } ^ { \mathsf { K } } ; \mathsf { \nabla } \mathsf { V } = \mathsf { \mathbf { H } } ^ { e n c } \mathsf { \mathbf { W } } ^ { \mathsf { V } }
$$

$$
\mathrm { C r o s s A t t e n t i o n } ( \mathbf { Q } , \mathsf { K } , \mathsf { V } ) = \mathrm { s o f t m a x } \left( \frac { \mathbf { Q } \mathsf { K } ^ { \intercal } } { \sqrt { d _ { k } } } \right) \mathsf { V }
$$

The cross attention thus allows the decoder to attend to each of the source language words as projected into the entire encoder final output representations. The other attention layer in each decoder block, the multi-head attention layer, is the same causal (left-to-right) attention that we saw in Chapter 9. The multi-head attention in the encoder, however, is allowed to look ahead at the entire source language text, so it is not masked.

To train an encoder-decoder model, we use the same self-supervision model we used for training encoder-decoders RNNs in Chapter 8. The network is given the source text and then starting with the separator token is trained autoregressively to predict the next token using cross-entropy loss. Recall that cross-entropy loss for language modeling is determined by the probability the model assigns to the correct next word. So at time $t$ the CE loss is the negative log probability the model assigns to the next word in the training sequence:

$$
L _ { C E } ( \hat { \mathbf { y } _ { t } } , \mathbf { y } _ { t } ) ~ = ~ - \log \hat { \mathbf { y } } _ { t } [ w _ { t + 1 } ]
$$

# teacher forcing

As in that case, we use teacher forcing in the decoder. Recall that in teacher forcing, at each time step in decoding we force the system to use the gold target token from training as the next input $x _ { t + 1 }$ , rather than allowing it to rely on the (possibly erroneous) decoder output $\hat { y _ { t } }$ .

# 13.4 Decoding in MT: Beam Search

Recall the greedy decoding algorithm from Chapter 9: at each time step $t$ in generation, the output $y _ { t }$ is chosen by computing the probability for each word in the vocabulary and then choosing the highest probability word (the argmax):

$$
\hat { w } _ { t } ~ = ~ \mathrm { a r g m a x } _ { w \in V } P ( w | \mathbf { w } _ { < t } )
$$

A problem with greedy decoding is that what looks high probability at word $t$ might turn out to have been the wrong choice once we get to word $t + 1$ . The beam search algorithm maintains multiple choices until later when we can see which one is best.

In beam search we model decoding as searching the space of possible generations, represented as a search tree whose branches represent actions (generating a token), and nodes represent states (having generated a particular prefix). We search for the best action sequence, i.e., the string with the highest probability.

# An illustration of the problem

Fig. 13.7 shows a made-up example. The most probable sequence is ok ok EOS (its probability is $. 4 \times . 7 \times 1 . 0$ ). But greedy search doesn’t find it, incorrectly choosing yes as the first word since it has the highest local probability (0.5).

![## Image Analysis: cede6256928463a3dddf169ad5293008797f3306b74bd65cfba6ff1aec3e2c0a.jpg

**Conceptual Understanding:**
The image conceptually represents a probabilistic finite-state automaton or a search space for sequence generation. Its main purpose is to demonstrate the process of generating a sequence of tokens from a defined vocabulary, specifically highlighting the conditional probabilities involved at each step and illustrating a common pitfall of greedy search algorithms, where locally optimal choices do not guarantee a globally optimal outcome in terms of overall sequence probability.

**Content Interpretation:**
The image shows a search tree (also known as a prefix tree or decision tree) that models the generation of a sequence of tokens from a vocabulary {yes, ok, EOS} based on conditional probabilities. Each node represents a state or a token generated at a particular time step. The directed edges (arrows) represent transitions between states, and the numerical labels on these edges denote the conditional probabilities of these transitions. The red labels 'p(t₁|start)', 'p(t₂|t₁)', and 'p(t₃|t₁,t₂)' indicate the conditional probability distributions being modeled at each stage of sequence generation. The blue labels 't₁', 't₂', 't₃' at the bottom signify the position or time step in the generated sequence. The 'EOS' (End Of Sequence) token represents the termination of a generated sequence. The significance of the probabilities is that they allow for the calculation of the total probability of any given sequence by multiplying the probabilities along its path.

**Key Insights:**
1.  **Search Tree Representation:** The image provides a concrete example of a search tree used in sequence generation, showing how words or tokens are chosen sequentially based on conditional probabilities. Each level of the tree corresponds to a time step (t1, t2, t3), and branches represent possible token choices. 
2.  **Conditional Probabilities:** The labels `p(t₁|start)`, `p(t₂|t₁)`, `p(t₃|t₁,t₂)` explicitly show that token generation is conditioned on previous tokens or the start state. The numerical values (e.g., .4, .5, .7, .2, etc.) represent these specific conditional probabilities. 
3.  **Greedy Search vs. Global Optimality:** The tree illustrates that making locally optimal choices (i.e., selecting the branch with the highest immediate probability at each step) does not guarantee a globally optimal (most probable) sequence. 
    *   For `t₁`, `start` has a branch to `yes` with `0.5` probability and `ok` with `0.4` probability. A greedy choice at `t₁` would be `yes`. 
    *   If `yes` was chosen at `t₁`, then at `t₂`, the branch from `yes` to `yes` has `0.4` probability, while `yes` to `ok` has `0.3` probability. A greedy choice at `t₂` would be `yes`. 
    *   This greedy path, `start` -> `yes` -> `yes` -> `EOS`, has a total probability of `0.5 * 0.4 * 1.0 = 0.20`. 
    *   In contrast, the path `start` -> `ok` -> `ok` -> `EOS` has a total probability of `0.4 * 0.7 * 1.0 = 0.28`, which is the globally most probable sequence according to the provided probabilities, despite the initial 'ok' having a lower probability (`0.4`) than 'yes' (`0.5`) at `t₁`. 
4.  **End Of Sequence (EOS) Token:** The frequent use of `EOS` with probability `1.0` in the final step `t₃` (after `t₂`) indicates that once the second token is generated, the sequence is always terminated, suggesting a fixed or maximum sequence length of two content tokens before `EOS`. The `EOS` from `start` directly with probability `.1` shows that a sequence can also be empty or very short. 
5.  **Path Probability Calculation:** The image implicitly teaches how to calculate the probability of a full sequence by multiplying the conditional probabilities along its path.

**Document Context:**
This image directly serves as 'An illustration of the problem' mentioned in the document context. It visually explains why 'Greedy search chooses yes followed by yes, instead of the globally most probable sequence ok ok.' The tree provides the specific numerical probabilities that allow a reader to trace both the greedy path ('yes yes EOS') and the globally most probable path ('ok ok EOS') and understand the discrepancy in their overall probabilities. This visual representation is crucial for comprehending the limitations of greedy search algorithms in sequence generation tasks.

**Summary:**
This image displays a search tree that illustrates the probabilistic generation of sequences of tokens. The tree begins at a 'start' node and branches out through different possible tokens ('ok', 'yes', 'EOS' - End Of Sequence) across three time steps, t1, t2, and t3. Each branch is labeled with a conditional probability, shown in red, representing the likelihood of transitioning to that token given the preceding state. The time steps t1, t2, and t3 are labeled at the bottom of the tree. The primary purpose is to demonstrate how sequences are formed and to highlight the difference between local (greedy) probability choices and globally optimal (most probable) sequences, as noted in the surrounding document context. For example, a greedy search might choose 'yes' followed by 'yes' based on local highest probabilities, but the globally most probable sequence 'ok ok' has a higher overall probability when considering the entire path.](images/cede6256928463a3dddf169ad5293008797f3306b74bd65cfba6ff1aec3e2c0a.jpg)
Figure 13.7 A search tree for generating the target string $T = t _ { 1 } , t _ { 2 } , \dots$ from vocabulary $V = \{ \mathrm { y e s } , \mathrm { o k } , < \pmb { \mathsf { s } } > \}$ , showing the probability of generating each token from that state. Greedy search chooses yes followed by yes, instead of the globally most probable sequence ok ok.

For some problems, like part-of-speech tagging or parsing as we will see in Chapter 17 or Chapter 18, we can use dynamic programming search (the Viterbi algorithm) to address this problem. Unfortunately, dynamic programming is not applicable to generation problems with long-distance dependencies between the output decisions. The only method guaranteed to find the best solution is exhaustive search: computing the probability of every one of the $V ^ { T }$ possible sentences (for some length value $T$ ) which is obviously too slow.

# beam width

# The solution: beam search

Instead, MT systems generally decode using beam search, a heuristic search method first proposed by Lowerre (1976). In beam search, instead of choosing the best token to generate at each timestep, we keep $k$ possible tokens at each step. This fixed-size memory footprint $k$ is called the beam width, on the metaphor of a flashlight beam that can be parameterized to be wider or narrower.

Thus at the first step of decoding, we compute a softmax over the entire vocabulary, assigning a probability to each word. We then select the $k$ -best options from this softmax output. These initial $k$ outputs are the search frontier and these $k$ initial words are called hypotheses. A hypothesis is an output sequence, a translation-sofar, together with its probability.

![## Image Analysis: 057416b1b93d9e2f8cdbc40eac5c30a314f5cd5164db3934c43d23091c6244e3.jpg

**Conceptual Understanding:**
The image conceptually represents a path-finding algorithm, specifically beam search, applied to sequence generation. Its main purpose is to visualize how candidate sequences are generated and pruned over multiple time steps, adhering to a fixed 'beam width'. It communicates the idea of expanding promising partial sequences and discarding less likely ones to find a high-probability complete sequence without exhaustively searching all possibilities.

**Content Interpretation:**
The image depicts the mechanism of beam search decoding, a common algorithm used in sequence generation tasks, such as natural language processing (e.g., machine translation, text generation). It shows how candidate sequences (hypotheses) are expanded step-by-step, with only the most probable 'k' (beam width) candidates being retained at each time step. The 'h^d_x' boxes represent hidden states of a decoder neural network, and the small graphs above them signify probability distributions over the vocabulary for the next word. The blue circles highlight the 'k' best hypotheses chosen at each time step.

**Key Insights:**
The main takeaway is the iterative, breadth-first nature of beam search. At each time step, the algorithm explores multiple paths (hypotheses) but prunes less promising ones to maintain a manageable set (beam width k=2). This allows for a more comprehensive search than greedy decoding while being more computationally efficient than a full breadth-first search. Specifically, the diagram shows how 'arrived' and 'the' are selected at t1, then 'the' (from 'arrived') and 'green' (from 'the') and 'witch' (from 'the') are among the top candidates at t2, demonstrating the continuous evaluation and selection of partial sequences. The decoder models illustrate that each step involves running the decoder to predict the next word given the current sequence.

**Document Context:**
This image directly supports the document's section on 'The solution: beam search' by providing a visual, step-by-step example of how the algorithm functions. The accompanying text explains that at each time step, the 'k' best hypotheses are chosen, extended by all possible vocabulary words, scored, and then the best 'k' are again selected, which is precisely what the diagram illustrates. It provides concrete examples (e.g., 'arrived', 'the', 'green', 'witch') for the abstract process described in the text, making the concept of beam search more comprehensible.

**Summary:**
The image illustrates a beam search decoding process with a beam width of k=2, spanning three time steps (t1, t2, t3). The process begins with a 'start' state. At each time step, the system generates a set of candidate words, from which the top 'k' (two, in this case) are selected to form the 'search frontier' for the next step. Each selected word then becomes the prefix for further extensions. The diagram also shows schematic decoder states (represented by h^d_1 and h^d_2 boxes with output distribution graphs) that are run to score the next words in a sequence. The process progresses from left to right, representing the generation of subsequent words in a sequence.](images/057416b1b93d9e2f8cdbc40eac5c30a314f5cd5164db3934c43d23091c6244e3.jpg)
Figure 13.8 Beam search decoding with a beam width of $k = 2$ . At each time step, we choose the $k$ best hypotheses, form the $V$ possible extensions of each, score those $k \times V$ hypotheses and choose the best $k = 2$ to continue. At time 1, the frontier has the best 2 options from the initial decoder state: arrived and the. We extend each, compute the probability of all the hypotheses so far (arrived the, arrived aardvark, the green, the witch) and again chose the best 2 (the green and the witch) to be the search frontier. The images on the arcs schematically represent the decoders that must be run at each step to score the next words (for simplicity not depicting cross-attention).

At subsequent steps, each of the $k$ best hypotheses is extended incrementally by being passed to distinct decoders, which each generate a softmax over the entire vocabulary to extend the hypothesis to every possible next token. Each of these $k \times V$ hypotheses is scored by $P ( y _ { i } | x , y _ { < i } )$ : the product of the probability of the current word choice multiplied by the probability of the path that led to it. We then prune the $k \times V$ hypotheses down to the $k$ best hypotheses, so there are never more than $k$ hypotheses at the frontier of the search, and never more than $k$ decoders. Fig. 13.8 illustrates this with a beam width of 2 for the beginning of The green witch arrived.

This process continues until an EOS is generated indicating that a complete candidate output has been found. At this point, the completed hypothesis is removed from the frontier and the size of the beam is reduced by one. The search continues until the beam has been reduced to 0. The result will be $k$ hypotheses.

To score each node by its log probability, we use the chain rule of probability to break down $p ( y | x )$ into the product of the probability of each word given its prior context, which we can turn into a sum of logs (for an output string of length $t$ ):

$$
{ \begin{array} { l } { s c o r e ( y ) \ = \ \log P ( y | x ) } \\ { \ = \ \log \left( P ( y _ { 1 } | x ) P ( y _ { 2 } | y _ { 1 } , x ) P ( y _ { 3 } | y _ { 1 } , y _ { 2 } , x ) . . . P ( y _ { t } | y _ { 1 } , . . . , y _ { t - 1 } , x ) \right) } \\ { \ = \ \displaystyle \sum _ { i = 1 } ^ { t } \log P ( y _ { i } | y _ { 1 } , . . . , y _ { i - 1 } , x ) } \end{array} }
$$

Thus at each step, to compute the probability of a partial sentence, we simply add the log probability of the prefix sentence so far to the log probability of generating the next token. Fig. 13.9 shows the scoring for the example sentence shown in Fig. 13.8, using some simple made-up probabilities. Log probabilities are negative or 0, and the max of two log probabilities is the one that is greater (closer to 0).

![## Image Analysis: 7650daf2e7d2350a04ef4cf2168a15582a4bc083915fa5ca373328da032ec47c.jpg

**Conceptual Understanding:**
The image visually represents the operational mechanics of a beam search algorithm. Conceptually, it illustrates how a probabilistic model generates sequences (such as sentences) by exploring multiple possible paths of words. Each path is scored using log probabilities, which accumulate at each step. The main purpose is to demonstrate the process of expanding hypotheses, calculating their likelihood, and pruning less probable branches to efficiently find optimal or near-optimal sequences. Key ideas conveyed include the use of log probabilities for numerical stability, the concept of conditional probability (where each word's likelihood depends on its predecessors and an external context 'x'), and the iterative nature of sequence generation from a 'BOS' (Beginning of Sentence) token to an 'EOS' (End of Sentence) token, with the pruning of 'dead-end' or low-probability paths as a core optimization strategy.

**Content Interpretation:**
The image displays a beam search tree, illustrating the generation of word sequences (hypotheses) based on conditional log probabilities. The process starts with a 'BOS' (Beginning of Sentence) token and branches out to possible next words. Each transition has an associated log probability (e.g., -1.6, -0.92). Cumulative log probabilities for hypotheses are shown above words (e.g., 'log P(arrived|x) = -1.6'). Paths with lower probabilities are pruned, marked by a crossed-out circle symbol, representing the 'beam' aspect of the search. The aim is to reach an 'EOS' (End of Sentence) token. Two complete sequences reach 'EOS' with a cumulative log probability of -2.7: 'the green witch arrived EOS' and 'the witch arrived EOS'. The top-right green box explicitly details how the total log probability of a sentence is the sum of the log conditional probabilities of each word in the sequence, including the EOS token, given an external context 'x'. The bottom annotations ('log P(y_1|x)', 'log P(y_2|y_1,x)', etc.) further clarify that each word's probability is conditioned on all preceding words and the context 'x'. The image demonstrates the core mechanics of a beam search in finding likely sequences in a probabilistic model.

**Key Insights:**
1.  **Beam Search Explores and Prunes Hypotheses:** The tree structure clearly shows the algorithm exploring multiple word sequences in parallel (branching) and then discarding less probable paths (indicated by the crossed-out circles), which is the essence of beam search's efficiency. Evidence includes the multiple branches from 'BOS' and subsequent words, and the many pruned paths like 'arrived the' and 'came'.
2.  **Sequence Scoring Uses Sum of Log Conditional Probabilities:** The overall probability of a word sequence is calculated by summing the log probabilities of individual word transitions, where each word's probability is conditioned on the preceding words and a given context 'x'. The formula in the top-right green box ('log P ("the green witch arrived"|x) = log P (the|x) + ... + log P(EOS|...x)') and the cumulative log probabilities above the nodes (e.g., 'log P(arrived|x) = -1.6') serve as direct evidence.
3.  **Conditional Dependence is Key to Language Modeling:** The probability of generating a word ('y_i') is dependent on all previously generated words ('y_1' through 'y_i-1') and an external context ('x'). This is explicitly stated by the bottom annotations 'log P(y_i|y_i-1,...,y_1,x)' for each step, highlighting the role of sequence history in probability calculation.
4.  **Beam Search Can Yield Complete Hypotheses of Varying Lengths:** Different paths can successfully terminate with an 'EOS' token after generating a different number of words. For example, 'the green witch arrived EOS' and 'the witch arrived EOS' are both valid completions with similar total log probabilities, illustrating the varying lengths.

**Document Context:**
This image is highly relevant to the document's section titled 'The solution: beam search'. It serves as a direct, detailed illustration of the beam search algorithm's operation, showing how it constructs and evaluates word sequences (hypotheses) by considering conditional probabilities. The diagram visually supports the concept of sequence generation and pruning, which are central to beam search. The mention in the text after the image about 'completed hypotheses may have different lengths' is directly evident in the diagram, as various paths lead to 'EOS' at different depths (lengths) or are pruned before completion. The detailed log probability calculations and conditional dependencies shown in the image are fundamental to understanding the quantitative aspect of how beam search determines the best sequence.

**Summary:**
This detailed diagram illustrates the process of a beam search algorithm, likely used in natural language processing tasks like machine translation or text generation, to find the most probable sequence of words. It's presented as a tree structure where each node represents a word in a sequence and each directed edge represents a transition from one word to the next.

The process begins with a "BOS" (Beginning of Sentence) token. From "BOS", the algorithm explores potential first words, such as "arrived" or "the". Each transition (edge) is associated with a negative numerical value, which represents the log probability of selecting that particular word given the preceding context. For example, the path from "BOS" to "arrived" has a log probability of -1.6, and the path from "BOS" to "the" has a log probability of -0.92.

As the algorithm progresses, it generates longer word sequences, or "hypotheses." The green text above each word node (e.g., "log P(arrived|x) = -1.6") indicates the cumulative log probability of the hypothesis up to that point. The probability of each subsequent word ("y_i") is conditioned on all previously generated words ("y_1" through "y_i-1") and an external context "x", as explicitly shown by the expressions "log P(y_1|x)", "log P(y_2|y_1,x)", etc., at the bottom of the diagram.

A key characteristic of beam search is its pruning mechanism to manage computational complexity. Less promising hypotheses, those with lower cumulative log probabilities, are discarded. These pruned paths are visually represented by a crossed-out circle symbol at the end of a branch. Examples of pruned paths include "arrived the", "arrived witch", "mage", "came", "who", and "by".

The goal is to find sequences that end with an "EOS" (End of Sentence) token, signifying a complete and valid sentence. In this illustration, two complete sentences are found:
1.  The sequence "the green witch arrived" followed by "EOS", with a cumulative log probability of -2.7. The detailed calculation for this path is shown in the top-right green box: "log P ("the green witch arrived"|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x) + logP(arrived|the,green,witch,x) + log P(EOS|the,green,witch,arrived,x)".
2.  The sequence "the witch arrived" followed by "EOS", also with a cumulative log probability of -2.7.

The diagram effectively visualizes how the beam search explores various linguistic paths, evaluates their likelihood using log probabilities, and strategically prunes branches to efficiently identify high-probability sentence completions, even when these completed sentences might differ in length.](images/7650daf2e7d2350a04ef4cf2168a15582a4bc083915fa5ca373328da032ec47c.jpg)
Fig. 13.10 gives the algorithm. One problem with this version of the algorithm is that the completed hypotheses may have different lengths. Because language mod

Figure 13.9 Scoring for beam search decoding with a beam width of $k = 2$ . We maintain the log probability of each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top $k$ paths are extended to the next step.

<table><tr><td>function BEAMDECODE(c,beam_width) returns best paths</td></tr><tr><td>yo,ho←0</td></tr><tr><td>path←(</td></tr><tr><td>complete_paths←()</td></tr><tr><td>state←(c,yo,ho,path) ;initial state frontier← (state) ;initial frontier</td></tr><tr><td></td></tr><tr><td>while frontier contains incomplete paths and beamwidth &gt; 0</td></tr><tr><td>extended_frontier← (</td></tr><tr><td>for each state E frontier do</td></tr><tr><td>y←DECODE(state)</td></tr><tr><td>for each word i ∈ Vocabulary do</td></tr><tr><td>successor←NEWSTATE(state,i,yi)</td></tr><tr><td>extended_frontier←ADDToBEAM(successor,extended_frontier, beam_width)</td></tr><tr><td></td></tr><tr><td>for each state in extended_frontier do if state is complete do</td></tr><tr><td>complete_paths←APPEND(complete_paths,state)</td></tr><tr><td>extended_frontier←REMovE(extended_frontier,state)</td></tr><tr><td>beam_width←beam_width-1</td></tr><tr><td>frontier←extendedfrontier</td></tr><tr><td>return completed_paths</td></tr><tr><td>function NEWSTATE(state,word,word_prob) returns new state</td></tr><tr><td></td></tr><tr><td>function ADDToBEAM(state,frontier, width) returns updated frontier</td></tr><tr><td>ifLENGTH(frontier)&lt;width then</td></tr><tr><td>frontier←INsERT(state,frontier)</td></tr><tr><td>else if SCORE(state) &gt; SCORE(WORSTOF(frontier))</td></tr><tr><td>frontier←REMOVE(WORSTOF(frontier))</td></tr><tr><td>frontier←INsERT(state,frontier) return frontier</td></tr></table>

els generally assign lower probabilities to longer strings, a naive algorithm would choose shorter strings for $y$ . (This is not an issue during the earlier steps of decoding; since beam search is breadth-first, all the hypotheses being compared had the same length.) For this reason we often apply length normalization methods, like dividing the logprob by the number of words:

$$
s c o r e ( y ) = \frac { 1 } { t } \log P ( y | x ) ~ = ~ \frac { 1 } { t } \sum _ { i = 1 } ^ { t } \log P ( y _ { i } | y _ { 1 } , . . . , y _ { i - 1 } , x )
$$

For MT we generally use beam widths $k$ between 5 and 10, giving us $k$ hypotheses at the end. We can pass all $k$ to the downstream application with their respective scores, or if we just need a single translation we can pass the most probable hypothesis.

# 13.4.1 Minimum Bayes Risk Decoding

Minimum Bayes risk or MBR decoding is an alternative decoding algorithm that can work even better than beam search and also tends to be better than the other decoding algorithms like temperature sampling introduced in Section 10.2.

The intuition of minimum Bayes risk is that instead of trying to choose the translation which is most probable, we choose the one that is likely to have the least error. For example, we might want our decoding algorithm to find the translation which has the highest score on some evaluation metric. For example in Section 13.6 we will introduce metrics like chrF or BERTScore that measure the goodness-of-fit between a candidate translation and a set of reference human translations. A translation that maximizes this score, especially with a hypothetically huge set of perfect human translations is likely to be a good one (have minimum risk) even if it is not the most probable translation by our particular probability estimator.

In practice, we don’t know the perfect set of translations for a given sentence. So the standard simplification used in MBR decoding algorithms is to instead choose the candidate translation which is most similar (by some measure of goodness-offit) with some set of candidate translations. We’re essentially approximating the enormous space of all possible translations $\mathcal { U }$ with a smaller set of possible candidate translations $\mathcal { Y }$ .

Given this set of possible candidate translations $\mathcal { Y }$ , and some similarity or alignment function util, we choose the best translation $\hat { y }$ as the translation which is most similar to all the other candidate translations:

$$
\hat { y } = \underset { y \in \mathcal { Y } } { \mathrm { a r g m a x } } \sum _ { c \in \mathcal { Y } } \mathrm { u t i l } ( y , c )
$$

Various util functions can be used, like chrF or BERTscore or BLEU. We can get the set of candidate translations by sampling using one of the basic sampling algorithms of Section 10.2 like temperature sampling; good results can be obtained with as few as 32 or 64 candidates.

Minimum Bayes risk decoding can also be used for other NLP tasks; indeed it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne, 2000) before being applied to machine translation (Kumar and Byrne, 2004), and has been shown to work well across many other generation tasks as well (e.g., summarization, dialogue, and image captioning (Suzgun et al., 2023a)).

# 13.5 Translating in low-resource situations

For some languages, and especially for English, online resources are widely available. There are many large parallel corpora that contain translations between English and many languages. But the vast majority of the world’s languages do not have large parallel training texts available. An important ongoing research question is how to get good translation with lesser resourced languages. The resource problem can even be true for high resource languages when we need to translate into low resource domains (for example in a particular genre that happens to have very little bitext).

Here we briefly introduce two commonly used approaches for dealing with this data sparsity: backtranslation, which is a special case of the general statistical technique called data augmentation, and multilingual models, and also discuss some socio-technical issues.

# 13.5.1 Data Augmentation

Data augmentation is a statistical technique for dealing with insufficient training data, by adding new synthetic data that is generated from the current natural data.

The most common data augmentation technique for machine translation is called backtranslation. Backtranslation relies on the intuition that while parallel corpora may be limited for particular languages or domains, we can often find a large (or at least larger) monolingual corpus, to add to the smaller parallel corpora that are available. The algorithm makes use of monolingual corpora in the target language by creating synthetic bitexts.

In backtranslation, our goal is to improve source-to-target MT, given a small parallel text (a bitext) in the source/target languages, and some monolingual data in the target language. We first use the bitext to train a MT system in the reverse direction: a target-to-source MT system . We then use it to translate the monolingual target data to the source language. Now we can add this synthetic bitext (natural target sentences, aligned with MT-produced source sentences) to our training data, and retrain our source-to-target MT model. For example suppose we want to translate from Navajo to English but only have a small Navajo-English bitext, although of course we can find lots of monolingual English data. We use the small bitext to build an MT engine going the other way (from English to Navajo). Once we translate the monolingual English text to Navajo, we can add this synthetic Navajo/English bitext to our training data.

Backtranslation has various parameters. One is how we generate the backtranslated data; we can run the decoder in greedy inference, or use beam search. Or we can do sampling, like the temperature sampling algorithm we saw in Chapter 9. Another parameter is the ratio of backtranslated data to natural bitext data; we can choose to upsample the bitext data (include multiple copies of each sentence). In general backtranslation works surprisingly well; one estimate suggests that a system trained on backtranslated text gets about 2/3 of the gain as would training on the same amount of natural bitext (Edunov et al., 2018).

# 13.5.2 Multilingual models

The models we’ve described so far are for bilingual translation: one source language, one target language. It’s also possible to build a multilingual translator.

In a multilingual translator, we train the system by giving it parallel sentences in many different pairs of languages. That means we need to tell the system which language to translate from and to! We tell the system which language is which by adding a special token $l _ { s }$ to the encoder specifying the source language we’re translating from, and a special token $l _ { t }$ to the decoder telling it the target language we’d like to translate into.

Thus we slightly update Eq. 13.9 above to add these tokens in Eq. 13.19:

$$
\begin{array} { l } { \displaystyle { \mathsf { h } } = \mathrm { e n c o d e r } ( x , l _ { s } ) } \\ { \displaystyle y _ { i + 1 } = \mathrm { d e c o d e r } ( { \mathsf { h } } , l _ { t } , y _ { 1 } , \ldots , y _ { i } ) \quad \forall i \in [ 1 , \ldots , m ] } \end{array}
$$

One advantage of a multilingual model is that they can improve the translation of lower-resourced languages by drawing on information from a similar language in the training data that happens to have more resources. Perhaps we don’t know the meaning of a word in Galician, but the word appears in the similar and higherresourced language Spanish.

# 13.5.3 Sociotechnical issues

Many issues in dealing with low-resource languages go beyond the purely technical. One problem is that for low-resource languages, especially from low-income countries, native speakers are often not involved as the curators for content selection, as the language technologists, or as the evaluators who measure performance ( $\forall$ et al., 2020). Indeed, one well-known study that manually audited a large set of parallel corpora and other major multilingual datasets found that for many of the corpora, less than $50 \%$ of the sentences were of acceptable quality, with a lot of data consisting of repeated sentences with web boilerplate or incorrect translations, suggesting that native speakers may not have been sufficiently involved in the data process (Kreutzer et al., 2022).

Other issues, like the tendency of many MT approaches to focus on the case where one of the languages is English (Anastasopoulos and Neubig, 2020), have to do with allocation of resources. Where most large multilingual systems were trained on bitexts in which English was one of the two languages, recent huge corporate systems like those of Fan et al. (2021) and Costa-jussa et al. \` (2022) and datasets like Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200 languages) and create bitexts between many more pairs of languages and not just through English.

At the smaller end, $\forall$ et al. (2020) propose a participatory design process to encourage content creators, curators, and language technologists who speak these low-resourced languages to participate in developing MT algorithms. They provide online groups, mentoring, and infrastructure, and report on a case study on developing MT algorithms for low-resource African languages. Among their conclusions was to perform MT evaluation by post-editing rather than direct evaluation, since having labelers edit an MT system and then measure the distance between the MT output and its post-edited version both was simpler to train evaluators and makes it easier to measure true errors in the MT output and not differences due to linguistic variation (Bentivogli et al., 2018).

# 13.6 MT Evaluation

Translations are evaluated along two dimensions:

# adequacy

# fluency

1. adequacy: how well the translation captures the exact meaning of the source sentence. Sometimes called faithfulness or fidelity.   
2. fluency: how fluent the translation is in the target language (is it grammatical, clear, readable, natural).

Using humans to evaluate is most accurate, but automatic metrics are also used for convenience.

# 13.6.1 Using Human Raters to Evaluate MT

The most accurate evaluations use human raters, such as online crowdworkers, to evaluate each translation along the two dimensions. For example, along the dimension of fluency, we can ask how intelligible, how clear, how readable, or how natural the MT output (the target text) is. We can give the raters a scale, for example, from 1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate each sentence or paragraph of the MT output.

We can do the same thing to judge the second dimension, adequacy, using raters to assign scores on a scale. If we have bilingual raters, we can give them the source sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale, how much of the information in the source was preserved in the target. If we only have monolingual raters but we have a good human translation of the source text, we can give the monolingual raters the human reference translation and a target machine translation and again rate how much information is preserved. An alternative is to do ranking: give the raters a pair of candidate translations, and ask them which one they prefer.

Training of human raters (who are often online crowdworkers) is essential; raters without translation expertise find it difficult to separate fluency and adequacy, and so training includes examples carefully distinguishing these. Raters often disagree (source sentences may be ambiguous, raters will have different world knowledge, raters may apply scales differently). It is therefore common to remove outlier raters, and (if we use a fine-grained enough scale) normalizing raters by subtracting the mean from their scores and dividing by the variance.

As discussed above, an alternative way of using human raters is to have them post-edit translations, taking the MT output and changing it minimally until they feel it represents a correct translation. The difference between their post-edited translations and the original MT output can then be used as a measure of quality.

# 13.6.2 Automatic Evaluation

While humans produce the best evaluations of machine translation output, running a human evaluation can be time consuming and expensive. For this reason automatic metrics are often used as temporary proxies. Automatic metrics are less accurate than human evaluation, but can help test potential system improvements, and even be used as an automatic loss function for training. In this section we introduce two families of such metrics, those based on character- or word-overlap and those based on embedding similarity.

# Automatic Evaluation by Character Overlap: chrF

# chrF

The simplest and most robust metric for MT evaluation is called chrF, which stands for character F-score (Popovic´, 2015). chrF (along with many other earlier related metrics like BLEU, METEOR, TER, and others) is based on a simple intuition derived from the pioneering work of Miller and Beebe-Center (1956): a good machine translation will tend to contain characters and words that occur in a human translation of the same sentence. Consider a test set from a parallel corpus, in which each source sentence has both a gold human target translation and a candidate MT translation we’d like to evaluate. The chrF metric ranks each MT target sentence by a function of the number of character n-gram overlaps with the human translation.

Given the hypothesis and the reference, chrF is given a parameter $k$ indicating the length of character n-grams to be considered, and computes the average of the $k$ precisions (unigram precision, bigram, and so on) and the average of the $k$ recalls (unigram recall, bigram recall, etc.):

chrP percentage of character 1-grams, 2-grams, ..., $\mathbf { k }$ -grams in the hypothesis that occur in the reference, averaged.   
chrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that occur in the hypothesis, averaged.

The metric then computes an F-score by combining chrP and chrR using a weighting parameter $\beta$ . It is common to set $\beta = 2$ , thus weighing recall twice as much as precision:

$$
{ \mathrm { c h r F } } \beta = ( 1 + \beta ^ { 2 } ) { \frac { \mathrm { c h r P \cdot c h r R } } { \beta ^ { 2 } \cdot { \mathrm { c h r P } } + { \mathrm { c h r R } } } }
$$

For $\beta = 2$ , that would be:

$$
{ \mathrm { c h r F 2 } } = { \frac { 5 \cdot { \mathrm { c h r P } } \cdot { \mathrm { c h r R } } } { 4 \cdot { \mathrm { c h r P } } + { \mathrm { c h r R } } } }
$$

For example, consider two hypotheses that we’d like to score against the reference translation witness for the past. Here are the hypotheses along with chrF values computed using parameters $k = \beta = 2$ (in real examples, $k$ would be a higher number like 6):

<table><tr><td rowspan="2">REF: witness for the past, HYP1: witness of the past， chrF2,2 =.86 HYP2: past witness</td><td></td></tr><tr><td>chrF2,2 = .62</td></tr></table>

Let’s see how we computed that chrF value for HYP1 (we’ll leave the computation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores spaces, so we’ll remove them from both the reference and hypothesis:

REF: witnessforthepast, (18 unigrams, 17 bigrams) HYP1: witnessofthepast, (17 unigrams, 16 bigrams)

Next let’s see how many unigrams and bigrams match between the reference and hypothesis:

unigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams) bigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams)

We use that to compute the unigram and bigram precisions and recalls:

unigram P: $1 7 / 1 7 = 1$ unigram R: $1 7 / 1 8 = . 9 4 4$ bigram P: $1 3 / 1 6 = . 8 1 3$ bigram R: $1 3 / 1 7 = . 7 6 5$

Finally we average to get chrP and chrR, and compute the F-score:

$$
{ \begin{array} { r l } & { { \mathrm { c h r P } } \ = \ ( 1 7 / 1 7 + 1 3 / 1 6 ) / 2 = . 9 0 6 } \\ & { { \mathrm { c h r R } } \ = \ ( 1 7 / 1 8 + 1 3 / 1 7 ) / 2 = . 8 5 5 } \\ & { { \mathrm { c h r F 2 } } , 2 \ = \ 5 { \frac { { \mathrm { c h r P } } * { \mathrm { c h r P } } } { 4 { \mathrm { c h r P } } + { \mathrm { c h r R } } } } = . 8 6 } \end{array} }
$$

chrF is simple, robust, and correlates very well with human judgments in many languages (Kocmi et al., 2021).

# Alternative overlap metric: BLEU

There are various alternative overlap metrics. For example, before the development of chrF, it was common to use a word-based overlap metric called BLEU (for BiLingual Evaluation Understudy), that is purely precision-based rather than combining precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candidate translation sentences is a function of the n-gram word precision over all the sentences combined with a brevity penalty computed over the corpus as a whole.

What do we mean by n-gram precision? Consider a corpus composed of a single sentence. The unigram precision for this corpus is the percentage of unigram tokens in the candidate translation that also occur in the reference translation, and ditto for bigrams and so on, up to 4-grams. BLEU extends this unigram metric to the whole corpus by computing the numerator as the sum over all sentences of the counts of all the unigram types that also occur in the reference translation, and the denominator is the total of the counts of all unigrams in all candidate sentences. We compute this n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the geometric mean. BLEU has many further complications, including a brevity penalty for penalizing candidate translations that are too short, and it also requires the ngram counts be clipped in a particular way.

Because BLEU is a word-based metric, it is very sensitive to word tokenization, making it impossible to compare different systems if they rely on different tokenization standards, and doesn’t work as well in languages with complex morphology. Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly for translation into English. In such cases it’s important to use packages that enforce standardization for tokenization like SACREBLEU (Post, 2018).

# Statistical Significance Testing for MT evals

Character or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly used to compare two systems, with the goal of answering questions like: did the new algorithm we just invented improve our MT system? To know if the difference between the chrF scores of two MT systems is a significant difference, we use the paired bootstrap test, or the similar randomization test.

To get a confidence interval on a single chrF score using the bootstrap test, recall from Section 4.9 that we take our test set (or devset) and create thousands of pseudotestsets by repeatedly sampling with replacement from the original test set. We now compute the chrF score of each of the pseudo-testsets. If we drop the top $2 . 5 \%$ and bottom $2 . 5 \%$ of the scores, the remaining scores will give us the $9 5 \%$ confidence interval for the chrF score of our system.

To compare two MT systems A and B, we draw the same set of pseudo-testsets, and compute the chrF scores for each of them. We then compute the percentage of pseudo-test-sets in which A has a higher chrF score than B.

# chrF: Limitations

While automatic character and word-overlap metrics like chrF or BLEU are useful, they have important limitations. chrF is very local: a large phrase that is moved around might barely change the chrF score at all, and chrF can’t evaluate crosssentence properties of a document like its discourse coherence (Chapter 24). chrF and similar automatic metrics also do poorly at comparing very different kinds of systems, such as comparing human-aided translation against machine translation, or different machine translation architectures against each other (Callison-Burch et al., 2006). Instead, automatic overlap metrics like chrF are most appropriate when evaluating changes to a single system.

# 13.6.3 Automatic Evaluation: Embedding-Based Methods

The chrF metric is based on measuring the exact character n-grams a human reference and candidate machine translation have in common. However, this criterion is overly strict, since a good translation may use alternate words or paraphrases. A solution first pioneered in early metrics like METEOR (Banerjee and Lavie, 2005) was to allow synonyms to match between the reference $x$ and candidate $\tilde { x }$ . More recent metrics use BERT or other embeddings to implement this intuition.

For example, in some situations we might have datasets that have human assessments of translation quality. Such datasets consists of tuples $( x , \tilde { x } , r )$ , where $x = ( x _ { 1 } , \ldots , x _ { n } )$ is a reference translation, $\tilde { x } = ( \tilde { x } _ { 1 } , \dots , \tilde { x } _ { m } )$ is a candidate machine translation, and $r \in \mathbb { R }$ is a human rating that expresses the quality of $\tilde { x }$ with respect to $x$ . Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam et al., 2020) train a predictor on the human-labeled datasets, for example by passing $x$ and $\tilde { x }$ through a version of BERT (trained with extra pretraining, and then finetuned on the human-labeled sentences), followed by a linear layer that is trained to predict $r$ . The output of such models correlates highly with human labels.

In other cases, however, we don’t have such human-labeled datasets. In that case we can measure the similarity of $x$ and $\tilde { x }$ by the similarity of their embeddings. The BERTSCORE algorithm (Zhang et al., 2020) shown in Fig. 13.11, for example, passes the reference $x$ and the candidate $\tilde { x }$ through BERT, computing a BERT embedding for each token $x _ { i }$ and $\tilde { x } _ { j }$ . Each pair of tokens $( x _ { i } , \tilde { x } _ { j } )$ is scored by its cosine $\frac { x _ { i } { \cdot } \tilde { x } _ { j } } { | x _ { i } | | \tilde { x } _ { j } | }$ . Each token in $x$ is matched to a token in $\tilde { x }$ to compute recall, and each token in $\tilde { x }$ is matched to a token in $x$ to compute precision (with each token greedily matched to the most similar token in the corresponding sentence). BERTSCORE provides precision and recall (and hence $\mathrm { F } _ { 1 }$ ):

$$
R _ { \mathrm { B E R T } } = \frac { 1 } { | x | } \sum _ { x _ { i } \in x } \operatorname* { m a x } _ { \tilde { x } _ { j } \in \tilde { x } } x _ { i } \cdot \tilde { x } _ { j } \quad P _ { \mathrm { B E R T } } = \frac { 1 } { | \tilde { x } | } \sum _ { \tilde { x } _ { j } \in \tilde { x } } \operatorname* { m a x } _ { x _ { i } \in x } x _ { i } \cdot \tilde { x } _ { j }
$$

![## Image Analysis: 6f0c61a42725a866f4e01ed8edcf67212450a2787101b0f988773dd316103996.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of computing the BERTScore recall metric (R_BERT), which is an embedding-based method for automatically evaluating text generation. Its main purpose is to demonstrate how a reference sentence and a candidate sentence are semantically compared, token by token, using contextual embeddings (like those generated by BERT) and cosine similarity, culminating in a single numerical score that quantifies the recall aspect of their similarity, with an optional emphasis on token importance via IDF weighting.

**Content Interpretation:**
The image details the methodology for calculating the BERTScore recall (R_BERT). It demonstrates how a reference text and a candidate text are processed through several stages: first, converting words into contextual embeddings, then calculating pairwise cosine similarities between these embeddings to form a matrix. Following this, the maximum similarity for each reference token is identified from this matrix. The final step shows an optional importance weighting using Inverse Document Frequency (IDF) values, which are then used in a weighted average formula to derive the R_BERT score. This metric quantifies the semantic overlap or coverage of the reference sentence by the candidate sentence.

**Key Insights:**
1. BERTScore uses contextual embeddings to capture semantic similarity between tokens in reference and candidate sentences, moving beyond simple surface-level matching.
2. The recall component of BERTScore (R_BERT) specifically measures how well each token in the reference sentence is covered by the candidate sentence by finding the maximum similarity for each reference token.
3. Inverse Document Frequency (IDF) weighting can be optionally applied to give more importance to rare or significant words in the reference sentence, thus influencing the final score's sensitivity to content words.
4. The final R_BERT score is a weighted average of the maximum similarity scores, where the weights are derived from IDF values, providing a robust measure of semantic similarity for text generation evaluation.

**Document Context:**
This image directly supports Section 13.6.3, 'Automatic Evaluation: Embedding-Based Methods,' by visually illustrating the computation of the BERTScore recall metric (R_BERT). It provides a concrete example and the underlying mathematical steps, enhancing the understanding of how embedding-based methods, particularly BERTScore, are used for text evaluation. The diagram explicitly shows the 'extended version of the metric in which tokens are also weighted by their idf values,' as mentioned in the accompanying text, making it highly relevant to the detailed explanation of this evaluation method.

**Summary:**
This diagram illustrates the step-by-step computation of the BERTScore recall metric (R_BERT) for comparing a reference sentence and a candidate sentence, specifically showing an extended version where tokens are weighted by their IDF values. The process begins with the input of a 'Reference x' sentence and a 'Candidate x̂' sentence. Both sentences undergo 'Contextual Embedding' to convert their tokens into semantic representations. These embeddings are then used to calculate 'Pairwise Cosine Similarity' between all tokens of the reference and candidate sentences, forming a similarity matrix. In the 'Maximum Similarity' stage, for each token in the reference sentence, the highest cosine similarity score with any token in the candidate sentence is identified. Finally, in the 'Importance Weighting (Optional)' stage, these maximum similarity scores are weighted by the IDF values of the reference tokens and aggregated to compute the final R_BERT score, reflecting how well the reference tokens are covered by the candidate tokens.](images/6f0c61a42725a866f4e01ed8edcf67212450a2787101b0f988773dd316103996.jpg)
Figure 1: IFigure 13.11 ustration of the computation of the recall metric RBThe computation of BERTSCORE recall from reference $x$ T. Given theand candidate $\hat { x }$ eference x and, from Figure 1 in candidate xˆ, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedyZhang et al. (2020). This version shows an extended version of the metric in which tokens are also weighted by matching in rtheir idf values.

# We experiment with different models (Section 4), uGiven a tokenized reference sentence x = x , . . .13.7 Bias and Ethical Issues

(Wu et al., 2016), where unknown words are split into several commonly observedMachine translation raises many of the same ethical issues that we’ve discussed in haracters. The representation for each word piece is computed with a Transformerearlier chapters. For example, consider MT systems translating from Hungarian ani et al., 2017) by repeatedly applying (which has the gender neutral pronoun $\tilde { \sigma }$ lf-attention and nonlinear transformations) or Spanish (which often drops pronouns) g fashion. BERT embeddings have been shown to benefit various NLP tasks (Devlininto English (in which pronouns are obligatory, and they have grammatical gender). u, 2019; Huang et al., 2019; Yang et al., 2019a).When translating a reference to a person described without specified gender, MT asure The vector representation allows for a soft measure of similarity instead ofsystems often default to male gender (Schiebinger 2014, Prates et al. 2019). And apineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosineMT systems often assign gender according to culture stereotypes of the sort we saw reference token x and a candidate token xˆ is x>i ˆxj . We use pre-normalizedin Section 6.11. Fig. 13.12 shows examples from Prates et al. (2019), in which Hungarian gender-neutral $\tilde { \sigma }$ k ikk j k  is a nurse is translated with she, but gender-neutral $\tilde { \sigma }$ is a $C E O$ i  is translated with he. Prates et al. (2019) find that these stereotypes can’t completely be accounted for by gender bias in US labor statistics, because the biases are amplified by MT systems, with pronouns being mapped to male or female gender with a probability higher than if the mapping was based on actual labor employment statistics.

<table><tr><td> Hungarian (gender neutral) source</td><td> English MT output</td></tr><tr><td>&quot; egy äpol6</td><td> she is a nurse</td></tr><tr><td>&quot; egy tudós</td><td>he is a scientist</td></tr><tr><td>&quot; egy mérnok</td><td> he is an engineer</td></tr><tr><td>“ egy pék</td><td>he is a baker</td></tr><tr><td>6 egy tanar</td><td>she is a teacher</td></tr><tr><td>&quot; egy eskuvoszervez&#x27;</td><td> she is a wedding organizer</td></tr><tr><td> &quot; egy vezérigazgató</td><td>he is a CEO</td></tr></table>

Figure 13.12 When translating from gender-neutral languages like Hungarian into English, current MT systems interpret people from traditionally male-dominated occupations as male, and traditionally female-dominated occupations as female (Prates et al., 2019).

Similarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019) shows that MT systems perform worse when they are asked to translate sentences that describe people with non-stereotypical gender roles, like “The doctor asked the nurse to help her in the operation”.

Many ethical questions in MT require further research. One open problem is developing metrics for knowing what our systems don’t know. This is because MT systems can be used in urgent situations where human translators may be unavailable or delayed: in medical domains, to help translate when patients and doctors don’t speak the same language, or in legal domains, to help judges or lawyers communicate with witnesses or defendants. In order to ‘do no harm’, systems need ways to assign confidence values to candidate translations, so they can abstain from giving incorrect translations that may cause harm.

# 13.8 Summary

Machine translation is one of the most widely used applications of NLP, and the encoder-decoder model, first developed for MT is a key tool that has applications throughout NLP.

• Languages have divergences, both structural and lexical, that make translation difficult.   
• The linguistic field of typology investigates some of these differences; languages can be classified by their position along typological dimensions like whether verbs precede their objects.   
• Encoder-decoder networks (for transformers just as we saw in Chapter 8 for RNNs) are composed of an encoder network that takes an input sequence and creates a contextualized representation of it, the context. This context representation is then passed to a decoder which generates a task-specific output sequence.   
• Cross-attention allows the transformer decoder to view information from all the hidden states of the encoder.   
• Machine translation models are trained on a parallel corpus, sometimes called a bitext, a text that appears in two (or more) languages.

• Backtranslation is a way of making use of monolingual corpora in the target language by running a pilot MT engine backwards to create synthetic bitexts. • MT is evaluated by measuring a translation’s adequacy (how well it captures the meaning of the source sentence) and fluency (how fluent or natural it is in the target language). Human evaluation is the gold standard, but automatic evaluation metrics like chrF, which measure character n-gram overlap with human translations, or more recent metrics based on embedding similarity, are also commonly used.

# Bibliographical and Historical Notes

MT was proposed seriously by the late 1940s, soon after the birth of the computer (Weaver, 1949/1955). In 1954, the first public demonstration of an MT system prototype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The next decade saw a great flowering of ideas, prefiguring most subsequent developments. But this work was ahead of its time—implementations were limited by, for example, the fact that pending the development of disks there was no good way to store dictionary information.

As high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus on the need for better evaluation and more basic research in the new fields of formal and computational linguistics. This consensus culminated in the famously critical ALPAC (Automatic Language Processing Advisory Committee) report of 1966 (Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT in the US. As MT research lost academic respectability, the Association for Machine Translation and Computational Linguistics dropped MT from its name. Some MT developers, however, persevered, and there were early MT systems like Met´ eo, ´ which translated weather forecasts from English to French (Chandioux, 1976), and industrial systems like Systran.

In the early years, the space of MT architectures spanned three general models. In direct translation, the system proceeds word-by-word through the sourcelanguage text, translating each word incrementally. Direct translation uses a large bilingual dictionary, each of whose entries is a small program with the job of translating one word. In transfer approaches, we first parse the input text and then apply rules to transform the source-language parse into a target language parse. We then generate the target language sentence from the parse tree. In interlingua approaches, we analyze the source language text into some abstract meaning representation, called an interlingua. We then generate into the target language from this interlingual representation. A common way to visualize these three early approaches was the Vauquois triangle shown in Fig. 13.13. The triangle shows the increasing depth of analysis required (on both the analysis and generation end) as we move from the direct approach through transfer approaches to interlingual approaches. In addition, it shows the decreasing amount of transfer knowledge needed as we move up the triangle, from huge amounts of transfer at the direct level (almost all knowledge is transfer knowledge for each word) through transfer (transfer rules only for parse trees or thematic roles) through interlingua (no specific transfer knowledge). We can view the encoder-decoder network as an interlingual approach, with attention acting as an integration of direct and transfer, allowing words or their representations to be directly accessed by the decoder.

![## Image Analysis: 6b1494a22506ebc9547ba0333a5f94087b8b12637d879041184c16eca9aa64df.jpg

**Conceptual Understanding:**
This image, known as the Vauquois Triangle, conceptually represents different architectures or levels of abstraction in machine translation systems. It illustrates the trade-off between the depth of linguistic analysis and the complexity of the translation process.

The main purpose of the diagram is to categorize and visualize three primary approaches to machine translation (Direct, Transfer, and Interlingua) based on the level of linguistic analysis performed. It suggests that as the level of analysis increases (moving up the triangle), the translation process becomes more universal and potentially yields higher quality, as it moves further away from language-specific structures towards a more abstract, meaning-based representation.

Key ideas communicated include: machine translation architectures, the concept of varying linguistic analysis depth (semantic, syntactic), the role of an interlingua (language-independent representation), the function of transfer (mapping linguistic structures), and the simplicity of direct translation (surface-level mapping).

**Content Interpretation:**
The image, known as the Vauquois Triangle, illustrates three distinct architectural approaches to machine translation, ordered by their level of abstraction and linguistic analysis:

1.  **Direct Translation:** This is the simplest and least analytical approach, depicted at the base of the triangle. The process begins with the "source text" and directly translates it to the "target text" via the "Direct Translation" method. This implies word-for-word or phrase-for-phrase translation without deep linguistic analysis.

2.  **Transfer-based Translation:** This approach involves an intermediate level of linguistic analysis. The process starts implicitly from the "source text" which undergoes analysis to extract its "Source Text: Semantic/Syntactic Structure." This structure is then subjected to a "Transfer" operation, mapping the linguistic structures of the source language to the equivalent linguistic structures of the target language. The outcome is the "Target Text: Semantic/Syntactic Structure," which is then used to generate the "target text" (implicitly).

3.  **Interlingua-based Translation:** This is the most abstract and deeply analytical approach, represented by the apex of the triangle. The process begins implicitly from the "source text" which undergoes "source language analysis" to transform it into an "Interlingua" representation. An "Interlingua" is a language-independent, abstract representation of meaning. From this "Interlingua," "target language generation" takes place to convert the abstract meaning into the specific linguistic forms of the "target text." This approach aims for high fidelity by fully understanding the meaning before generating the output.

The vertical axis of the triangle implicitly represents the depth of linguistic analysis. Moving upwards from the base to the apex signifies an increasing level of analysis, from surface-level string manipulation to deep semantic understanding. The left side (upwards arrow labeled "source language analysis") represents the analysis phase for the source language, while the right side (downwards arrow labeled "target language generation") represents the generation phase for the target language. The horizontal connections represent the translation or transfer stage between the two languages.

**Key Insights:**
The main takeaways from the Vauquois triangle are:

*   **Hierarchy of Abstraction:** Machine translation approaches can be categorized by their level of linguistic abstraction, from direct string-to-string mapping to deep, language-independent meaning representation. This is visually conveyed by the triangle shape and the placement of "Direct Translation" at the bottom, "Transfer" in the middle, and "Interlingua" at the top.
*   **Complexity vs. Universality:** As machine translation moves towards higher levels of abstraction (e.g., Interlingua), the system becomes more complex to build but potentially more generalizable across many language pairs, requiring less effort to add new target languages once the Interlingua is established. This is implied by the deeper "analysis" and "generation" stages required for "Interlingua." The labels "source language analysis" and "target language generation" further support this.
*   **Intermediate Representations:** Effective machine translation often involves intermediate representations of text, such as semantic and syntactic structures (evidenced by "Source Text: Semantic/Syntactic Structure" and "Target Text: Semantic/Syntactic Structure"), or even a universal "Interlingua" as a complete meaning representation.

In essence, the triangle illustrates a fundamental architectural spectrum in machine translation, where each level implies a different strategy for overcoming language differences and achieving translation, with varying degrees of linguistic processing.

**Document Context:**
As indicated by the document context "Figure 13.13 The Vauquois (1968) triangle" within a "Bibliographical and Historical Notes" section, this image serves as a foundational historical model for understanding machine translation architectures. It likely precedes a discussion on the evolution of machine translation, highlighting early conceptual frameworks that categorized different approaches to translating between languages. It provides a visual and conceptual anchor for discussing the methodological progression in the field of machine translation, from simpler direct methods to more complex analysis-based systems.

**Summary:**
The image presents "The Vauquois (1968) triangle," a classic model illustrating three primary architectural approaches to machine translation, categorized by their depth of linguistic analysis. The triangle's base represents the least analytical approach, while its apex signifies the most abstract and deeply analytical method.

At the very bottom of the triangle, connecting the "source text" on the left to the "target text" on the right, is the "Direct Translation" approach. This method involves translating directly from the source language to the target language, often relying on word-for-word or phrase-for-phrase equivalences with minimal linguistic analysis.

Moving upwards to the middle level of the triangle, we find the "Transfer" approach. In this method, the "source text" is first analyzed to extract its "Source Text: Semantic/Syntactic Structure." This structured representation of the source language is then subjected to a "Transfer" process, which maps these linguistic structures to equivalent "Target Text: Semantic/Syntactic Structure" in the target language. Finally, the "target text" is generated from this structured representation. This approach involves a deeper level of linguistic understanding than direct translation.

At the apex of the triangle, representing the highest level of abstraction, is the "Interlingua" approach. Here, the "source text" undergoes extensive "source language analysis" to convert it into an "Interlingua." An "Interlingua" is a language-independent, abstract representation of the meaning of the source text, conceptually divorcing the meaning from any specific language's grammatical rules or vocabulary. Once the text is represented in this universal "Interlingua," "target language generation" is performed to produce the final "target text" in the desired language. This method aims for the highest quality translation by first fully understanding the meaning, independent of the original language.

The overall diagram clearly demonstrates a progression in machine translation methodologies: from simple, surface-level "Direct Translation" at the bottom, through "Transfer" which operates on linguistic structures, to the highly abstract "Interlingua" approach at the top, which focuses on language-independent meaning.](images/6b1494a22506ebc9547ba0333a5f94087b8b12637d879041184c16eca9aa64df.jpg)
Figure 13.13 The Vauquois (1968) triangle.

statistical MT IBM Models Candide

Statistical methods began to be applied around 1990, enabled first by the development of large bilingual corpora like the Hansard corpus of the proceedings of the Canadian Parliament, which are kept in both French and English, and then by the growth of the Web. Early on, a number of researchers showed that it was possible to extract pairs of aligned sentences from bilingual corpora, using words or simple cues like sentence length (Kay and Roscheisen ¨ 1988, Gale and Church 1991, Gale and Church 1993, Kay and Roscheisen¨ 1993).

At the same time, the IBM group, drawing directly on the noisy channel model for speech recognition, proposed two related paradigms for statistical MT. These include the generative algorithms that became known as IBM Models 1 through 5, implemented in the Candide system. The algorithms (except for the decoder) were published in full detail— encouraged by the US government who had partially funded the work— which gave them a huge impact on the research community (Brown et al. 1990, Brown et al. 1993).

The group also developed a discriminative approach, called MaxEnt (for maximum entropy, an alternative formulation of logistic regression), which allowed many features to be combined discriminatively rather than generatively (Berger et al., 1996), which was further developed by Och and Ney (2002).

phrase-based translation

By the turn of the century, most academic research on machine translation used statistical MT, either in the generative or discriminative mode. An extended version of the generative approach, called phrase-based translation was developed, based on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia).

Moses

Once automatic metrics like BLEU were developed (Papineni et al., 2002), the discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics like BLEU in a method known as Minimum Error Rate Training, or MERT (Och, 2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) were widely used.

transduction grammars

There were also approaches around the turn of the century that were based on syntactic structure (Chapter 18). Models based on transduction grammars (also called synchronous grammars) assign a parallel syntactic tree structure to a pair of sentences in different languages, with the goal of translating the sentences by applying reordering operations on the trees. From a generative perspective, we can view a transduction grammar as generating pairs of aligned sentences in two languages. Some of the most widely used models included the inversion transduction grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005),

inversion transduction grammar

Neural networks had been applied at various times to various aspects of machine translation; for example Schwenk et al. (2006) showed how to use neural language models to replace n-gram language models in a Spanish-English system based on IBM Model 4. The modern neural encoder-decoder approach was pioneered by Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder, and was first applied to MT by Bahdanau et al. (2015). The transformer encoderdecoder was proposed by Vaswani et al. (2017) (see the History section of Chapter 9).

Research on evaluation of machine translation began quite early. Miller and Beebe-Center (1956) proposed a number of methods drawing on work in psycholinguistics. These included the use of cloze and Shannon tasks to measure intelligibility as well as a metric of edit distance from a human translation, the intuition that underlies all modern overlap-based automatic evaluation metrics. The ALPAC report included an early evaluation study conducted by John Carroll that was extremely influential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for fidelity and intelligibility, and had raters score them subjectively on 9-point scales. Much early evaluation work focuses on automatic word-overlap metrics like BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate) (Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR (Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (Popovic´, 2015) came later. More recent evaluation work, echoing the ALPAC report, has emphasized the importance of careful statistical methodology and the use of human evaluation (Kocmi et al., 2021; Marie et al., 2021).

The early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al. (2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions to linguistic typology.

# Exercises

13.1 Compute by hand the chrF2,2 score for HYP2 on page 282 (the answer should round to .62).

# 14 Question Answering, Informa-tion Retrieval, and RetrievalAugmented Generation

People need to know things. So pretty much as soon as there were computers we were asking them questions. Systems in the 1960s were answering questions about baseball statistics and scientific facts. Even fictional computers in the 1970s like Deep Thought, invented by Douglas Adams in The Hitchhiker’s Guide to the Galaxy, answered “the Ultimate Question Of Life, The Universe, and Everything”.1 And because so much knowledge is encoded in text, question answering (QA) systems were performing at human levels even before LLMs: IBM’s Watson system won the TV game-show Jeopardy! in 2011, surpassing humans at answering questions like:

# WILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA” INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL

Question answering systems are designed to fill human information needs. Since a lot of information is present in text form (on the web or in other data like our email, or books), question answering is closely related to the task behind search engines. Indeed, the distinction is becoming ever more fuzzy, as modern search engines are integrated with large language models trained to do question answering.

Question answering systems often focus on a useful subset of information needs: factoid questions, questions of fact or reasoning that can be answered with simple facts expressed in short or medium-length texts, like the following:

(14.1) Where is the Louvre Museum located? (14.2) Where does the energy in a nuclear explosion come from? (14.3) How to get a script l in latex?

Modern NLP systems answer these questions using large language models, in one of two ways. The first is to make use of the method from Chapter 12: prompt a pretrained and instruction-tuned LLM, an LLM that has been finetuned on question/answer datasets with the question in the prompt. For example, we could prompt a causal language model with a string like

Q: Where is the Louvre Museum located? A:

have it do conditional generation given this prefix, and take the response as the answer. The idea is that language models have read a lot of facts in their pretraining data, presumably including the location of the Louvre, and have encoded this information in their parameters.

Simply prompting an LLM can be a useful approach to answer many factoid questions. But it is not yet a complete solution for question answering.

hallucinate

The first and main problem is that large language models often give the wrong answer! Large language models hallucinate. A hallucination is a response that is not faithful to the facts of the world. That is, when asked questions, large language models simply make up answers that sound reasonable. For example, Dahl et al. (2024) found that when asked questions about the legal domain (like about particular legal cases), large language models hallucinated from $69 \%$ to $8 8 \%$ of the time!

calibrated

And it’s not always possible to tell when language models are hallucinating, partly because LLMs aren’t well-calibrated. In a calibrated system, the confidence of a system in the correctness of its answer is highly correlated with the probability of an answer being correct. So if a calibrated system is wrong, at least it might hedge its answer or tell us to go check another source. But since language models are not well-calibrated, they often give a very wrong answer with complete certainty (Zhou et al., 2024).

RAG information retrieval

A second problem is that simply prompting a large language model doesn’t allow us to ask questions about proprietary data. A common use of question answering is about data like our personal email or medical records. Or a company may have internal documents that contain answers for customer service or internal use. Or legal firms need to ask questions about legal discovery from proprietary documents.

Finally, static large language models also have problems with questions about rapidly changing information (like questions about something that happened last week) since LLMs won’t have up-to-date information from after their release data.

For this reason the most common way to do question-answering with LLMs is retrieval-augmented generation or RAG, and that is the method we will focus on in this chapter. In RAG we use information retrieval (IR) techniques to retrieve documents that are likely to have information that might help answer the question. Then we use a large language model to generate an answer given these documents.

Basing our answers on retrieved documents can solve some of the problems with using simple prompting to answer questions. First, it helps ensure that the answer is grounded in facts from some curated dataset. And the system can give the user the answer accompanied by the context of the passage or document the answer came from. This information can help users have confidence in the accuracy of the answer (or help them spot when it is wrong!). And these retrieval techniques can be used on any proprietary data we want, such as legal or medical data for those applications.

We’ll begin by introducing information retrieval, the task of choosing the most relevant document from a document set given a user’s query expressing their information need. We’ll see the classic method based on cosines of sparse tf-idf vectors, a modern neural ‘dense’ retrievers based on instead representing queries and documents neurally with BERT or other language models. We then introduce retrieverbased question answering and the retrieval-augmented generation paradigm.

Finally, we’ll discuss various QA datasets. These are used for finetuning LLMs in instruction tuning, as we saw in Chapter 12. And they are also used as benchmarks, since question answering has an important function as a benchmark for measuring the abilities of language models.

# 14.1 Information Retrieval

# information retrieval IR

Information retrieval or IR is the name of the field encompassing the retrieval of all manner of media based on user information needs. The resulting IR system is often called a search engine. Our goal in this section is to give a sufficient overview of IR to see its application to question answering. Readers with more interest specifically in information retrieval should see the Historical Notes section at the end of the chapter and textbooks like Manning et al. (2008).

ad hoc retrieval document

The IR task we consider is called ad hoc retrieval, in which a user poses a query to a retrieval system, which then returns an ordered set of documents from some collection. A document refers to whatever unit of text the system indexes and retrieves (web pages, scientific papers, news articles, or even shorter passages like paragraphs). A collection refers to a set of documents being used to satisfy user requests. A term refers to a word in a collection, but it may also include phrases. Finally, a query represents a user’s information need expressed as a set of terms. The high-level architecture of an ad hoc retrieval engine is shown in Fig. 14.1.

collection term query

![## Image Analysis: 75284c2b41f49fd23914e573974448ff52a9633918e8dced25697e177a8e74de.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural flow of a typical ad hoc Information Retrieval system. Its main purpose is to illustrate how documents are prepared and stored, how user queries are processed, and how these two components interact to facilitate the retrieval of relevant information. The key ideas communicated are the necessity of indexing a document collection, processing a query to make it searchable, and using both to perform a search that results in a ranked list of documents based on relevance.

**Content Interpretation:**
The image depicts a foundational architecture for an ad hoc Information Retrieval system. It illustrates the transformation of a collection of raw documents into an index and the transformation of a user query into a query vector, both of which are then used by a search mechanism to retrieve and rank relevant documents. The processes shown are 'Indexing,' 'Query Processing,' and 'Search.' The data components or outputs include 'document collection,' individual 'Document' entities, an 'Inverted Index,' a 'query,' a 'query vector,' and 'Ranked Documents.' The relationships show a clear data flow: documents are indexed to create an inverted index; queries are processed to create a query vector; both the inverted index and query vector are fed into the search process; and the search process outputs ranked documents.

**Key Insights:**
The main takeaway from this image is the two-pronged approach of an ad hoc Information Retrieval system: an offline phase involving "document collection" and "Indexing" to create an "Inverted Index," and an online phase involving a user "query" undergoing "Query Processing" to create a "query vector." These two processed inputs then converge at the "Search" stage, which ultimately delivers "Ranked Documents." The system highlights the importance of preprocessing both documents and queries for efficient and effective retrieval. The output being 'Ranked Documents' indicates that relevance scoring is an inherent part of the search process.

**Document Context:**
This image serves as a fundamental diagram for understanding the basic operational flow of an Information Retrieval system, specifically an ad hoc IR system. In the context of a document about Information Retrieval, it visually establishes the core components and their interactions, providing a conceptual framework before delving into the intricacies of each component (e.g., specific indexing algorithms, query processing techniques, or ranking functions). The accompanying text "Figure 14.1 The architecture of an ad hoc IR system." explicitly states its role in the document.

**Summary:**
This image illustrates the architecture of an ad hoc Information Retrieval (IR) system, detailing the two primary parallel processes: document processing and query processing, which converge at the search stage to produce ranked documents. The system begins with a "document collection," consisting of multiple "Document" entities. This collection undergoes "Indexing," a process that transforms the raw documents into an organized data structure. The output of this indexing process is an "Inverted Index," which is a crucial component for efficient searching. In parallel, a user "query" is initiated. This query goes through "Query Processing," where it is transformed into a structured representation. The result of query processing is a "query vector." Both the "Inverted Index" (from document processing) and the "query vector" (from query processing) serve as inputs to the "Search" process. The "Search" process then utilizes these inputs to find relevant documents. Finally, the output of the "Search" process is a collection of "Ranked Documents," implying that the retrieved documents are ordered by their relevance to the original query.](images/75284c2b41f49fd23914e573974448ff52a9633918e8dced25697e177a8e74de.jpg)
Figure 14.1 The architecture of an ad hoc IR system.

The basic IR architecture uses the vector space model we introduced in Chapter 6, in which we map queries and document to vectors based on unigram word counts, and use the cosine similarity between the vectors to rank potential documents (Salton, 1971). This is thus an example of the bag-of-words model introduced in Chapter 4, since words are considered independently of their positions.

# 14.1.1 Term weighting and document scoring

Let’s look at the details of how the match between a document and query is scored.

We don’t use raw word counts in IR, instead computing a term weight for each document word. Two term weighting schemes are common: the tf-idf weighting introduced in Chapter 6, and a slightly more powerful variant called BM25.

We’ll reintroduce tf-idf here so readers don’t need to look back at Chapter 6. Tf-idf (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, the term frequency tf and the inverse document frequency idf.

The term frequency tells us how frequent the word is; words that occur more often in a document are likely to be informative about the document’s contents. We usually use the $\log _ { 1 0 }$ of the word frequency, rather than the raw count. The intuition is that a word appearing 100 times in a document doesn’t make that word 100 times more likely to be relevant to the meaning of the document. We also need to do something special with counts of 0, since we can’t take the log of 0.3

$$
{ \mathrm { t f } } _ { t , d } = { \left\{ \begin{array} { l l } { 1 + \log _ { 1 0 } \operatorname { c o u n t } ( t , d ) } & { { \mathrm { ~ i f ~ } } \operatorname { c o u n t } ( t , d ) > 0 } \\ { 0 } & { { \mathrm { ~ o t h e r w i s e } } } \end{array} \right. }
$$

If we use log weighting, terms which occur 0 times in a document would have $\operatorname { t f } = 0$ , 1 times in a document $\mathrm { t f } = 1 + \log _ { 1 0 } ( 1 ) = 1 + 0 = 1$ , 10 times in a document $\mathbf { t } \mathbf { = }$ $1 + \log _ { 1 0 } ( 1 0 ) = 2$ , 100 times $\mathrm { t f } = 1 + \log _ { 1 0 } ( 1 0 0 ) = 3$ , 1000 times $\mathrm { t f } = 4$ , and so on.

The document frequency $\operatorname { d f } _ { t }$ of a term $t$ is the number of documents it occurs in. Terms that occur in only a few documents are useful for discriminating those documents from the rest of the collection; terms that occur across the entire collection aren’t as helpful. The inverse document frequency or idf term weight (Sparck Jones, 1972) is defined as:

$$
\mathrm { i d f } _ { t } = \log _ { 1 0 } { \frac { N } { \mathrm { d f } _ { t } } }
$$

where $N$ is the total number of documents in the collection, and $\operatorname { d f } _ { t }$ is the number of documents in which term $t$ occurs. The fewer documents in which a term occurs, the higher this weight; the lowest weight of 0 is assigned to terms that occur in every document.

Here are some idf values for some words in the corpus of Shakespeare plays, ranging from extremely informative words that occur in only one play like Romeo, to those that occur in a few like salad or Falstaff, to those that are very common like fool or so common as to be completely non-discriminative since they occur in all 37 plays like good or sweet.4

<table><tr><td>Word</td><td>df</td><td>idf</td></tr><tr><td>Romeo</td><td>1</td><td>1.57</td></tr><tr><td>salad</td><td>2</td><td>1.27</td></tr><tr><td>Falstaff</td><td>4</td><td>0.967</td></tr><tr><td>forest</td><td>12</td><td>0.489</td></tr><tr><td>battle</td><td>21</td><td>0.246</td></tr><tr><td>wit</td><td>34</td><td>0.037</td></tr><tr><td>fool</td><td>36</td><td>0.012</td></tr><tr><td> good</td><td>37</td><td>0</td></tr><tr><td> sweet</td><td>37</td><td>0</td></tr></table>

The tf-idf value for word $t$ in document $d$ is then the product of term frequency $\mathrm { t f } _ { t , d }$ and IDF:

$$
\mathrm { t f - i d f } ( t , d ) = \mathrm { t f } _ { t , d } \cdot \mathrm { i d f } _ { t }
$$

# 14.1.2 Document Scoring

We score document $d$ by the cosine of its vector $\mathbf { d }$ with the query vector $\mathfrak { q }$

$$
\operatorname { s c o r e } ( q , d ) = \cos ( \mathbf { q } , \mathbf { d } ) = { \frac { \mathbf { q } \cdot \mathbf { d } } { | \mathbf { q } | | \mathbf { d } | } }
$$

Another way to think of the cosine computation is as the dot product of unit vectors; we first normalize both the query and document vector to unit vectors, by dividing by their lengths, and then take the dot product:

$$
\operatorname { s c o r e } ( q , d ) = \cos ( \mathbf { q } , \mathbf { d } ) = { \frac { \mathbf { q } } { | \mathbf { q } | } } \cdot { \frac { \mathbf { d } } { | \mathbf { d } | } }
$$

We can spell out Eq. 14.8, using the tf-idf values and spelling out the dot product as a sum of products:

$$
\operatorname { s c o r e } ( q , d ) = \sum _ { t \in { \mathfrak { q } } } { \frac { { \mathrm { t f } } \mathrm { - } \mathrm { i d f } ( t , q ) } { \sqrt { \sum _ { q _ { i } \in q } { \mathrm { t f } } \mathrm { - } { \mathrm { i d f } } ^ { 2 } ( q _ { i } , q ) } } } \cdot { \frac { { \mathrm { t f } } \mathrm { - } \mathrm { i d f } ( t , d ) } { \sqrt { \sum _ { d _ { i } \in d } { \mathrm { t f } } \mathrm { - } { \mathrm { i d f } } ^ { 2 } ( d _ { i } , d ) } } }
$$

Now let’s use Eq. 14.9 to walk through an example of a tiny query against a collection of 4 nano documents, computing tf-idf values and seeing the rank of the documents. We’ll assume all words in the following query and documents are downcased and punctuation is removed:

Query: sweet love   
Doc 1: Sweet sweet nurse! Love?   
Doc 2: Sweet sorrow   
Doc 3: How sweet is love?   
Doc 4: Nurse!

Fig. 14.2 shows the computation of the tf-idf cosine between the query and Document 1, and the query and Document 2. The cosine is the normalized dot product of tf-idf values, so for the normalization we must need to compute the document vector lengths $| q |$ , $\left| d _ { 1 } \right|$ , and $\left| d _ { 2 } \right|$ for the query and the first two documents using Eq. 14.4, Eq. 14.5, Eq. 14.6, and Eq. 14.9 (computations for Documents 3 and 4 are also needed but are left as an exercise for the reader). The dot product between the vectors is the sum over dimensions of the product, for each dimension, of the values of the two tf-idf vectors for that dimension. This product is only non-zero where both the query and document have non-zero values, so for this example, in which only sweet and love have non-zero values in the query, the dot product will be the sum of the products of those elements of each vector.

Document 1 has a higher cosine with the query (0.747) than Document 2 has with the query (0.0779), and so the tf-idf cosine model would rank Document 1 above Document 2. This ranking is intuitive given the vector space model, since Document 1 has both terms including two instances of sweet, while Document 2 is missing one of the terms. We leave the computation for Documents 3 and 4 as an exercise for the reader.

In practice, there are many variants and approximations to Eq. 14.9. For example, we might choose to simplify processing by removing some terms. To see this, let’s start by expanding the formula for tf-idf in Eq. 14.9 to explicitly mention the tf and idf terms from Eq. 14.6:

$$
\operatorname { s c o r e } ( q , d ) = \sum _ { t \in { \bf q } } { \frac { \mathrm { t f } _ { t , q } \cdot \mathrm { i } \mathrm { d } { \mathrm { f } } _ { t } } { \sqrt { \sum _ { q _ { i } \in q } \mathrm { t f } ^ { - } \mathrm { i } \mathrm { d } { \mathrm { f } } ^ { 2 } ( q _ { i } , q ) } } } \cdot { \frac { \mathrm { t f } _ { t , d } \cdot \mathrm { i } \mathrm { d } { \mathrm { f } } _ { t } } { \sqrt { \sum _ { d _ { i } \in d } \mathrm { t f } ^ { - } \mathrm { i } \mathrm { d } { \mathrm { f } } ^ { 2 } ( d _ { i } , d ) } } }
$$

In one common variant of tf-idf cosine, for example, we drop the idf term for the document. Eliminating the second copy of the idf term (since the identical term is already computed for the query) turns out to sometimes result in better performance:

$$
{ \mathrm { s c o r e } } ( q , d ) = \sum _ { t \in { \bf q } } { \frac { { \mathrm { t f } } _ { t , q } \cdot { \mathrm { i d f } } _ { t } } { \sqrt { \sum _ { q _ { i } \in q } { \mathrm { t f } } ^ { - } { \mathrm { i d f } } ^ { 2 } ( q _ { i } , q ) } } } \cdot { \frac { { \mathrm { t f } } _ { t , d } \cdot { \mathrm { i d f } } _ { t } } { \sqrt { \sum _ { d _ { i } \in d } { \mathrm { t f } } ^ { - } { \mathrm { i d f } } ^ { 2 } ( d _ { i } , d ) } } }
$$

Other variants of tf-idf eliminate various other terms.

A slightly more complex variant in the tf-idf family is the BM25 weighting scheme (sometimes called Okapi BM25 after the Okapi IR system in which it was introduced (Robertson et al., 1995)). BM25 adds two parameters: $k$ , a knob that adjust the balance between term frequency and IDF, and $^ b$ , which controls the importance of document length normalization. The BM25 score of a document $d$ given a query $q$ is:

<table><tr><td colspan="9"></td></tr><tr><td>word</td><td> cnt tf df idf</td><td></td><td></td><td>Query</td><td></td><td>tf-idf n&#x27;lized =tf-idf//ql</td><td colspan="2"></td></tr><tr><td> sweet</td><td>1</td><td>1</td><td>3</td><td></td><td>0.125 0.125 0.383</td><td></td><td colspan="2"></td></tr><tr><td> nurse</td><td>0</td><td>02</td><td></td><td>0.301 0</td><td>0</td><td></td></tr><tr><td>love</td><td>1</td><td>12</td><td></td><td></td><td>0.301 0.301 0.924</td><td></td></tr><tr><td>how</td><td>0</td><td>01</td><td></td><td>0.602 0</td><td>0</td><td></td></tr><tr><td> sorrow00 1 0.6020</td><td></td><td></td><td></td><td></td><td>0</td><td></td></tr><tr><td>is</td><td>0</td><td></td><td></td><td>01 0.602 0</td><td>0</td><td></td></tr><tr><td>lq| = √.125² + .3012 = .326</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td colspan="9"></td></tr><tr><td> word</td><td>cnt tf</td><td></td><td></td><td>Document 1 tf-idf n&#x27;lized</td><td>xq</td><td>cnt tf</td><td>Document 2 tf-idf n&#x27;lized</td><td>xq</td></tr><tr><td>sweet</td><td>2</td><td></td><td></td><td>1.301 0.163 0.357</td><td>0.137</td><td>1</td><td>1.000 0.125 0.203</td><td>0.0779</td></tr><tr><td> nurse</td><td>1</td><td></td><td></td><td>1.000 0.301 0.661</td><td>0</td><td>0</td><td>0 0 0</td><td>0</td></tr><tr><td>love</td><td>1</td><td></td><td></td><td>1.000 0.301 0.661</td><td>0.610</td><td>0</td><td>0 0 0</td><td>0</td></tr><tr><td>how</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 0 0</td><td>0</td></tr><tr><td> sorrow 0</td><td></td><td>0</td><td>0</td><td>0</td><td>0</td><td>1</td><td>1.000 0.602 0.979</td><td>0</td></tr><tr><td>is</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0</td><td>0 0 0</td><td>0</td></tr><tr><td colspan="9">|d1| = √163² + .301² + .3012 = .456</td></tr><tr><td colspan="5">Cosine: ∑ of column: 0.747</td><td colspan="4">Cosine: ∑ of column: 0.0779</td></tr></table>

Figure 14.2 Computation of tf-idf cosine score between the query and nano-documents 1 (0.747) and 2 (0.0779), using Eq. 14.4, Eq. 14.5, Eq. 14.6 and Eq. 14.9.

$$
\sum _ { t \in q } \overbrace { \log \left( \frac { N } { d f _ { t } } \right) } ^ { \mathrm { I D F } } \overbrace { \frac { k \left( 1 - b + b \left( \frac { | d | } { | d \mathrm { u g } | } \right) \right) + t f _ { t , d } } { k \left( 1 - b + b \left( \frac { | d | } { | d \mathrm { u g } | } \right) \right) + t f _ { t , d } } } ^ { \mathrm { w e i g h t e d ~ t f } }
$$

where $| d _ { \mathrm { a v g } } |$ is the length of the average document. When $k$ is 0, BM25 reverts to no use of term frequency, just a binary selection of terms in the query (plus idf). A large $k$ results in raw term frequency (plus idf). $^ b$ ranges from 1 (scaling by document length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable values are $\mathbf { k } = [ 1 . 2 , 2 ]$ and ${ \bf b } = 0 . 7 5$ . Kamphuis et al. (2020) is a useful summary of the many minor variants of BM25.

Stop words In the past it was common to remove high-frequency words from both the query and document before representing them. The list of such high-frequency words to be removed is called a stop list. The intuition is that high-frequency terms (often function words like the, a, to) carry little semantic weight and may not help with retrieval, and can also help shrink the inverted index files we describe below. The downside of using a stop list is that it makes it difficult to search for phrases that contain words in the stop list. For example, common stop lists would reduce the phrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists is much less common, partly due to improved efficiency and partly because much of their function is already handled by IDF weighting, which downweights function words that occur in every document. Nonetheless, stop word removal is occasionally useful in various NLP tasks so is worth keeping in mind.

# 14.1.3 Inverted Index

In order to compute scores, we need to efficiently find documents that contain words in the query. (Any document that contains none of the query terms will have a score of 0 and can be ignored.) The basic search problem in IR is thus to find all documents $d \in C$ that contain a term $q \in { \cal Q }$ .

The data structure for this task is the inverted index, which we use for making this search efficient, and also conveniently storing useful information like the document frequency and the count of each term in each document.

An inverted index, given a query term, gives a list of documents that contain the term. It consists of two parts, a dictionary and the postings. The dictionary is a list of terms (designed to be efficiently accessed), each pointing to a postings list for the term. A postings list is the list of document IDs associated with each term, which can also contain information like the term frequency or even the exact positions of terms in the document. The dictionary can also store the document frequency for each term. For example, a simple inverted index for our 4 sample documents above, with each word containing its document frequency in $\{ \}$ , and a pointer to a postings list that contains document IDs and term counts in $[ ]$ , might look like the following:

$$
\begin{array} { r l } & { \mathrm { h o w ~ \{ 1 \} } \quad  ~ 3 ~ [ 1 ] } \\ & { \mathrm { i s ~ \{ 1 \} } } \\ & { \mathrm { l o v e ~ \{ 2 \} } \quad  ~ 1 ~ [ 1 ]  3 ~ [ 1 ] } \\ & { \mathrm { n u r s e ~ \{ 2 \} } \quad  ~ 1 ~ [ 1 ]  4 ~ [ 1 ] } \\ & { \mathrm { s o r r y ~ \{ 1 \} } } \\ & { \mathrm { s w e e t ~ \{ 3 \} } \quad  ~ 2 ~ [ 1 ] } \end{array}
$$

Given a list of terms in query, we can very efficiently get lists of all candidate documents, together with the information necessary to compute the tf-idf scores we need.

There are alternatives to the inverted index. For the question-answering domain of finding Wikipedia pages to match a user query, Chen et al. (2017a) show that indexing based on bigrams works better than unigrams, and use efficient hashing algorithms rather than the inverted index to make the search efficient.

# 14.1.4 Evaluation of Information-Retrieval Systems

We measure the performance of ranked retrieval systems using the same precision and recall metrics we have been using. We make the assumption that each document returned by the IR system is either relevant to our purposes or not relevant. Precision is the fraction of the returned documents that are relevant, and recall is the fraction of all relevant documents that are returned. More formally, let’s assume a system returns $T$ ranked documents in response to an information request, a subset $R$ of these are relevant, a disjoint subset, $N$ , are the remaining irrelevant documents, and $U$ documents in the collection as a whole are relevant to this request. Precision and recall are then defined as:

$$
P r e c i s i o n = \frac { | R | } { | T | } R e c a l l = \frac { | R | } { | U | }
$$

Unfortunately, these metrics don’t adequately measure the performance of a system that ranks the documents it returns. If we are comparing the performance of two ranked retrieval systems, we need a metric that prefers the one that ranks the relevant documents higher. We need to adapt precision and recall to capture how well a system does at putting relevant documents higher in the ranking.

<table><tr><td>Rank</td><td> Judgment</td><td>PrecisionRank</td><td>RecallRank</td></tr><tr><td>1</td><td>R</td><td>1.0</td><td>.11</td></tr><tr><td>2</td><td>N</td><td>.50</td><td>.11</td></tr><tr><td>3</td><td>R</td><td>.66</td><td></td></tr><tr><td>4</td><td>N</td><td>.50</td><td></td></tr><tr><td>5</td><td>R</td><td>.60</td><td></td></tr><tr><td>6</td><td>R</td><td>.66</td><td>2222334455</td></tr><tr><td>7</td><td>N</td><td>.57</td><td></td></tr><tr><td>8</td><td>R</td><td>.63</td><td></td></tr><tr><td>9</td><td>N</td><td>.55</td><td></td></tr><tr><td>10</td><td>N</td><td>.50</td><td></td></tr><tr><td>11</td><td>R</td><td>.55</td><td>.66</td></tr><tr><td>12</td><td>N</td><td>.50</td><td>.66</td></tr><tr><td>13</td><td>N</td><td>.46</td><td>.66</td></tr><tr><td>14</td><td>N</td><td>.43</td><td>.66</td></tr><tr><td>15</td><td>R</td><td>.47</td><td>.77</td></tr><tr><td>16</td><td>N</td><td>.44</td><td>.77</td></tr><tr><td>17</td><td>N</td><td>.44</td><td>.77</td></tr><tr><td>18</td><td>R</td><td>.44</td><td>.88</td></tr><tr><td>19</td><td>N</td><td>.42</td><td>.88</td></tr><tr><td>20</td><td>N</td><td>.40</td><td>.88</td></tr><tr><td>21</td><td>N</td><td>.38</td><td>.88</td></tr><tr><td>22</td><td>N</td><td>.36</td><td>.88</td></tr><tr><td>23</td><td>N</td><td>.35</td><td>.88</td></tr><tr><td>24</td><td>N</td><td>.33</td><td>.88</td></tr><tr><td>25</td><td>R</td><td>.36</td><td>1.0</td></tr></table>

Figure 14.3 Rank-specific precision and recall values calculated as we proceed down through a set of ranked documents (assuming the collection has 9 relevant documents).

Let’s turn to an example. Assume the table in Fig. 14.3 gives rank-specific precision and recall values calculated as we proceed down through a set of ranked documents for a particular query; the precisions are the fraction of relevant documents seen at a given rank, and recalls the fraction of relevant documents found at the same rank. The recall measures in this example are based on this query having 9 relevant documents in the collection as a whole.

Note that recall is non-decreasing; when a relevant document is encountered, recall increases, and when a non-relevant document is found it remains unchanged. Precision, on the other hand, jumps up and down, increasing when relevant documents are found, and decreasing otherwise. The most common way to visualize precision and recall is to plot precision against recall in a precision-recall curve, like the one shown in Fig. 14.4 for the data in table 14.3.

Fig. 14.4 shows the values for a single query. But we’ll need to combine values for all the queries, and in a way that lets us compare one system to another. One way of doing this is to plot averaged precision values at 11 fixed levels of recall (0 to 100, in steps of 10). Since we’re not likely to have datapoints at these exact levels, we use interpolated precision values for the 11 recall values from the data points we do have. We can accomplish this by choosing the maximum precision value achieved at any level of recall at or above the one we’re calculating. In other words,

$$
\operatorname { I n t P r e c i s i o n } ( r ) = \operatorname* { m a x } _ { i > - r } \operatorname* { P r e c i s i o n } ( i )
$$

![## Image Analysis: cecbd838e33c0494eea4651d5f441a51422d063d6a407349e5702c10decb556b.jpg

**Conceptual Understanding:**
This image represents a precision-recall curve, a fundamental graph used in the field of information retrieval. Conceptually, it illustrates the performance of an information retrieval system by plotting its precision against its recall at various retrieval cut-off points. The main purpose of this image is to visually demonstrate the inherent trade-off between retrieving only highly relevant documents (high precision) and retrieving all relevant documents (high recall). It conveys the key idea that an optimal information retrieval system would ideally maintain high precision even as it achieves high recall, typically aiming for a curve that stays as close to the top-right corner of the plot as possible. The graph shows how an actual system performs, revealing its strengths and weaknesses in balancing these two crucial aspects of relevance.

**Content Interpretation:**
The image displays a precision-recall curve, which is a standard method for evaluating the effectiveness of information retrieval (IR) systems. The graph illustrates the relationship between two critical performance metrics: Precision and Recall. Precision, shown on the y-axis, measures the accuracy of the retrieved results (i.e., the proportion of retrieved items that are relevant). Recall, shown on the x-axis, measures the completeness of the retrieved results (i.e., the proportion of relevant items that were successfully retrieved). The curve plots the precision achieved at various recall levels. The specific 'stair-step' or 'sawtooth' shape of the curve, with horizontal segments followed by vertical drops or rises, is characteristic of a precision-recall curve generated from a ranked list of documents where precision is calculated after each relevant document is retrieved, or at specific recall points. This pattern indicates how the system's accuracy (precision) changes as more of the total relevant documents (recall) are retrieved. The initial high precision at very low recall suggests that the top-ranked results are highly relevant. The subsequent fluctuations and general downward trend of precision as recall increases demonstrate the inherent trade-off: it's often challenging to maintain high precision while also achieving high recall.

**Key Insights:**
The main takeaway from this image is the visual representation of the trade-off between precision and recall in an information retrieval system. The curve demonstrates that achieving high recall often comes at the cost of lower precision, and vice-versa. Specific insights include: 1.  **Early Precision:** The curve starts with very high precision (1.0) at low recall, indicating that the initial results retrieved by the system are highly relevant. This is evident from the first data point at approximately (0.07 Recall, 1.0 Precision). 2.  **Fluctuating Performance:** The 'stair-step' pattern shows that precision does not linearly decrease as recall increases; instead, it fluctuates, rising and falling. This suggests that the relevance of retrieved documents varies as more items are considered, which is a common characteristic of ranked retrieval systems. The labels 'Precision' and 'Recall' on the axes explicitly define the metrics being evaluated, while the numerical scales from 0.0 to 1.0 provide the quantitative context for understanding these fluctuations and trade-offs. The overall shape of the curve provides evidence for how effectively an IR system prioritizes and identifies relevant information.

**Document Context:**
This image directly supports the document's section '14.1.4 Evaluation of Information-Retrieval Systems' by providing a visual example of a precision-recall curve. As stated in the text after the image, 'Figure 14.4 The precision recall curve for the data in table 14.3,' this graph is a direct visualization of data, likely from an actual or hypothetical information retrieval system. It helps readers understand the practical application of precision and recall metrics in evaluating system performance. The subsequent mention of 'Figure 14.5 Interpolated data points from Fig. 14.3' suggests that this non-interpolated curve (Figure 14.4) serves as a foundational example before potentially discussing interpolated versions or further analysis. It provides the visual evidence for the theoretical concepts discussed in the surrounding text, allowing for a deeper understanding of how an IR system's performance characteristics manifest in a graphical representation.

**Summary:**
The image displays a precision-recall curve, which is a common visualization used to evaluate the performance of information retrieval systems. The vertical axis is labeled "Precision" and ranges from 0.0 to 1.0, with major tick marks at 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0. The horizontal axis is labeled "Recall" and also ranges from 0.0 to 1.0, with major tick marks at 0.0, 0.2, 0.4, 0.6, 0.8, and 1.0. A blue line with circular data points plots the relationship between precision and recall. The curve starts at a recall of approximately 0.0 and a precision of 1.0, indicating perfect precision at the very initial retrieval stages. It then drops vertically to a precision of approximately 0.5 at a recall of about 0.07. As recall increases, the precision generally follows a 'stair-step' or 'sawtooth' pattern, showing fluctuations. For example, at a recall of about 0.2, precision is approximately 0.5, then rises to around 0.65. At a recall of about 0.45, precision is approximately 0.67, then drops to about 0.57. Towards the higher end of recall, around 0.85-0.9, there is a cluster of data points showing precision values dropping from about 0.45 down to approximately 0.32. The curve concludes at a recall of 1.0 with a precision of approximately 0.35. The graph visually represents the trade-off between precision (the fraction of retrieved documents that are relevant) and recall (the fraction of relevant documents that are retrieved) for an information retrieval system across different retrieval thresholds.](images/cecbd838e33c0494eea4651d5f441a51422d063d6a407349e5702c10decb556b.jpg)
Figure 14.4 The precision recall curve for the data in table 14.3.   
Figure 14.5 Interpolated data points from Fig. 14.3.

This interpolation scheme not only lets us average performance over a set of queries, but also helps smooth over the irregular precision values in the original data. It is designed to give systems the benefit of the doubt by assigning the maximum precision value achieved at higher levels of recall from the one being measured. Fig. 14.5 and Fig. 14.6 show the resulting interpolated data points from our example.

<table><tr><td>Interpolated Precision</td><td>Recall</td></tr><tr><td>1.0</td><td>0.0</td></tr><tr><td>1.0</td><td>.10</td></tr><tr><td>.66</td><td>.20</td></tr><tr><td>.66</td><td>.30</td></tr><tr><td>.66</td><td>.40</td></tr><tr><td>.63</td><td>.50</td></tr><tr><td>.55</td><td>.60</td></tr><tr><td>.47</td><td>.70</td></tr><tr><td>.44</td><td>.80</td></tr><tr><td>.36</td><td>.90</td></tr><tr><td>.36</td><td>1.0</td></tr></table>

Given curves such as that in Fig. 14.6 we can compare two systems or approaches by comparing their curves. Clearly, curves that are higher in precision across all recall values are preferred. However, these curves can also provide insight into the overall behavior of a system. Systems that are higher in precision toward the left may favor precision over recall, while systems that are more geared towards recall will be higher at higher levels of recall (to the right).

A second way to evaluate ranked retrieval is mean average precision (MAP), which provides a single metric that can be used to compare competing systems or approaches. In this approach, we again descend through the ranked list of items, but now we note the precision only at those points where a relevant item has been encountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 14.3). For a single query, we average these individual precision measurements over the return set (up to some fixed cutoff). More formally, if we assume that $R _ { r }$ is the set of relevant documents at or above $r$ , then the average precision (AP) for a single query is

![## Image Analysis: ccb5362c4e04e3f508ef1c1950f491b55aa502a22ac82cacebb30c1c9221a5d3.jpg

**Conceptual Understanding:**
The image conceptually represents the performance characteristics of an information retrieval system. Its main purpose is to visually depict the relationship between precision and recall, two critical metrics in evaluating how well a system retrieves relevant information while minimizing irrelevant results. It specifically showcases an 'Interpolated Precision Recall Curve,' which is a standardized way to compare system performance by smoothing out fluctuations in individual query results and showing the maximum precision achievable at various recall levels. The graph effectively communicates the trade-off inherent in information retrieval: improving one metric often comes at the expense of the other.

**Content Interpretation:**
The image illustrates the performance evaluation of information retrieval systems using precision and recall metrics. The solid line with square markers represents an 'interpolated precision-recall curve,' which provides a smoothed and generalized view of the system's performance. The scattered white circles represent the 'original measured precision-recall points,' showing the raw, potentially more variable, performance data. The downward trend of the interpolated curve from high precision at low recall to lower precision at high recall signifies the common inverse relationship between these two metrics: as more relevant documents are retrieved (higher recall), the proportion of retrieved documents that are actually relevant (precision) tends to decrease. The specific numerical values on the axes and the points plotted show the quantitative performance characteristics of the system being evaluated.

**Key Insights:**
The image teaches several key lessons about information retrieval system evaluation: 1. Interpolated precision-recall curves (e.g., the solid line with square markers) are used to provide a stable and comparable measure of system performance across different recall levels, by 'interpolating precision at each of the 11 standard recall levels... from the maximum at any higher level of recall' (as per document context). 2. There is a fundamental trade-off between precision and recall in IR systems. The curve demonstrates that increasing recall (retrieving more relevant items) typically leads to a decrease in precision (more irrelevant items are retrieved alongside the relevant ones), as shown by the general downward slope of the 'Interpolated Precision Recall Curve.' 3. The 'original measured precision recall points' (white circles) represent the raw performance data, highlighting the variability that can occur before interpolation smooths the curve. This shows that the interpolated curve is a derived metric, not the raw output. 4. An effective IR system aims to keep precision high even as recall increases, which would be represented by a curve staying high towards the right.

**Document Context:**
This image is highly relevant to the document's Section 14.1.4, titled 'Evaluation of Information-Retrieval Systems.' It serves as a direct visual aid to explain one of the primary methods for assessing IR system performance: the precision-recall curve. The accompanying text explicitly describes it as 'An 11 point interpolated precision-recall curve,' detailing how precision is interpolated from the maximum at any higher level of recall and mentioning the inclusion of original measured points. This graph directly supports the theoretical and practical discussion of how IR systems are evaluated using these metrics.

**Summary:**
This image displays an 'Interpolated Precision Recall Curve,' a standard visualization for evaluating the performance of information retrieval systems. The graph uses a Cartesian coordinate system with 'Recall' on the X-axis (ranging from 0 to 1 in increments of 0.1) and 'Precision' on the Y-axis (also ranging from 0 to 1 in increments of 0.1). The main feature is a solid black line with square markers, representing the interpolated precision-recall curve. This curve generally shows a decreasing trend, indicating the typical trade-off between precision and recall. Specifically, precision starts at 1.0 for recall levels 0 and 0.1. It then drops to approximately 0.65 at a recall of 0.2 and remains roughly at this level until around a recall of 0.4. From a recall of 0.5, precision gradually declines further, reaching approximately 0.55 at 0.55 recall, 0.47 at 0.65-0.7 recall, 0.44 at 0.8 recall, and finally settles around 0.35 for recall levels of 0.9 and 1.0. In addition to the interpolated curve, several white circular markers are scattered across the graph. These circles represent the 'original measured precision recall points' before interpolation. They show variability, generally lying below or near the interpolated curve at various recall levels, with clusters visible, for example, between recall 0.85 and 0.9, where multiple points are distributed between precision 0.35 and 0.45. The graph as a whole visually demonstrates how precision changes as recall increases, providing insights into an IR system's effectiveness.](images/ccb5362c4e04e3f508ef1c1950f491b55aa502a22ac82cacebb30c1c9221a5d3.jpg)
Figure 14.6 An 11 point interpolated precision-recall curve. Precision at each of the 11 standard recall levels is interpolated for each query from the maximum at any higher level of recall. The original measured precision recall points are also shown.

$$
\mathsf { A P } = \frac { 1 } { | R _ { r } | } \sum _ { d \in R _ { r } } \mathsf { P r e c i s i o n } _ { r } ( d )
$$

where $P r e c i s i o n _ { r } ( d )$ is the precision measured at the rank at which document $d$ was found. For an ensemble of queries $Q$ , we then average over these averages, to get our final MAP measure:

$$
\mathrm { M A P } = { \frac { 1 } { | Q | } } \sum _ { q \in Q } \mathrm { A P } ( q )
$$

The MAP for the single query (hence $= { \mathrm { A P } } .$ ) in Fig. 14.3 is 0.6.

# 14.2 Information Retrieval with Dense Vectors

The classic tf-idf or BM25 algorithms for IR have long been known to have a conceptual flaw: they work only if there is exact overlap of words between the query and document. In other words, the user posing a query (or asking a question) needs to guess exactly what words the writer of the answer might have used, an issue called the vocabulary mismatch problem (Furnas et al., 1987).

The solution to this problem is to use an approach that can handle synonymy: instead of (sparse) word-count vectors, using (dense) embeddings. This idea was first proposed for retrieval in the last century under the name of Latent Semantic Indexing approach (Deerwester et al., 1990), but is implemented in modern times via encoders like BERT.

The most powerful approach is to present both the query and the document to a single encoder, allowing the transformer self-attention to see all the tokens of both the query and the document, and thus building a representation that is sensitive to the meanings of both query and document. Then a linear layer can be put on top of the [CLS] token to predict a similarity score for the query/document tuple:

$$
\begin{array} { r } { \mathbf { z } = \mathrm { B E R T } ( \mathbf { q } ; \lbrack \mathrm { S E P } ] ; \mathbf { d } ) \left[ \mathrm { C L S } \right] } \\ { \mathrm { s c o r e } ( q , d ) = \mathrm { s o f t m a x } ( \mathbf { U } ( \mathbf { z } ) ) } \end{array}
$$

This architecture is shown in Fig. 14.7a. Usually the retrieval step is not done on an entire document. Instead documents are broken up into smaller passages, such as non-overlapping fixed-length chunks of say 100 tokens, and the retriever encodes and retrieves these passages rather than entire documents. The query and document have to be made to fit in the BERT 512-token window, for example by truncating the query to 64 tokens and truncating the document if necessary so that it, the query, [CLS], and [SEP] fit in 512 tokens. The BERT system together with the linear layer U can then be fine-tuned for the relevance task by gathering a tuning dataset of relevant and non-relevant passages.

![## Image Analysis: 966616d1127a2c5f0088a01740dfe6c6a28494e6a362237f5c3678b08886f6bf.jpg

**Conceptual Understanding:**
This image visually represents two fundamental architectures used in information retrieval with dense vectors, specifically for calculating a relevance score, `s(q,d)`, between a query (`q`) and a document (`d`).

**Main Purpose:** The primary purpose of the image is to illustrate the structural differences and data flow for two distinct methods of dense retrieval, highlighting how the query and document interact (or don't interact) during the encoding process to produce a final relevance score. It serves to clarify the architectural implications discussed in the document's text.

**Conceptual Representation:**
*   **Part (a) - Joint Encoding (Single Encoder):** This section depicts a model where the query and document are concatenated into a single input sequence and processed by a single, shared neural network encoder. The core concept here is *full interaction* between query and document tokens at every layer of the encoding process. The output is a combined representation (`z_CLS`) from which a relevance score is directly predicted.
*   **Part (b) - Dual Encoding (Separate Encoders):** This section illustrates a model where the query and the document are processed by two entirely separate and independent neural network encoders. The core concept here is *independent representation learning*. Each encoder produces a dense vector representation for its respective input (`z_CLS_Q` for the query, `z_CLS_D` for the document), and these two independent vectors are then compared (e.g., via dot product) to calculate the relevance score.

The image communicates the key idea that the choice between these two architectural patterns involves a trade-off between the depth of interaction allowed between query and document (influencing accuracy) and the computational efficiency of generating representations and scores. The "lines between layers to schematically represent self-attention" mentioned in the document context further reinforces that these are likely transformer-based models where input tokens interact across layers.

**Content Interpretation:**
The image conceptually illustrates two architectural patterns for dense information retrieval, specifically how a query and a document are encoded and compared to generate a relevance score s(q,d). Both (a) and (b) depict neural network architectures, likely based on transformers, given the visual representation of layers and implied self-attention mechanisms (as per the document's context description).

**Architecture (a) - Single Encoder:** This pattern shows a single, unified encoder processing both the query and the document as a single, concatenated input sequence, separated by a `[sep]` token. The main purpose is to allow full interaction between the query and document tokens at every layer of the encoder, leading to a context-aware combined representation. This approach computes a relevance score based on a classification token (`z_CLS`) derived from this joint encoding, then passed through a linear layer (`U`).

**Architecture (b) - Dual Encoder:** This pattern illustrates two separate and independent encoders: one dedicated to processing the query and another dedicated to processing the document. The main purpose here is to generate independent, fixed-size vector representations (`z_CLS_Q` for the query and `z_CLS_D` for the document) for each. These independent representations are then compared (e.g., via a dot product, indicated by the black dot in the diamond) to yield a relevance score. This approach prioritizes computational efficiency for retrieval by allowing pre-computation of document embeddings.

The image conveys the key idea that the choice of encoder architecture (single vs. dual) fundamentally impacts how query and document interactions are modeled and how the final relevance score is derived, with implications for both accuracy and computational cost.

**Key Insights:**
The image, in conjunction with its surrounding text, teaches several key lessons regarding dense vector information retrieval:

1.  **Two Primary Architectures for Dense Retrieval:** There are two main conceptual approaches: joint encoding (a) and dual encoding (b). This is evident from the distinct visual separation and labels (a) and (b) of the two diagrams.
2.  **Joint Encoding (a) Characteristics:**
    *   **Full Interaction:** Query and document tokens are concatenated and processed by a single encoder (`Query`, `[sep]`, `Document` input to one large encoder). This allows for deep, fine-grained interactions between query and document tokens throughout the encoding process, as indicated by the criss-crossing lines (schematic self-attention) within the single encoder block and the generation of `z_CLS` from the entire sequence.
    *   **Scoring Mechanism:** A `z_CLS` token's representation is transformed by a linear layer (`U`) to directly output the relevance score `s(q,d)`.
    *   **Computational Cost:** The document context explicitly states this method is "too compute-expensive to use except in rescoring" because every query-document pair requires a full pass through the large encoder.
3.  **Dual Encoding (b) Characteristics:**
    *   **Independent Encoding:** Query and document are encoded separately by dedicated encoders (`Query` to one encoder for `z_CLS_Q`, `Document` to another encoder for `z_CLS_D`). This means there is no interaction between query and document tokens during the encoding phase itself.
    *   **Scoring Mechanism:** The final relevance score `s(q,d)` is computed by a simpler comparison (e.g., dot product, visually indicated by the black dot in the diamond) between the independently generated `z_CLS_Q` and `z_CLS_D` representations.
    *   **Computational Efficiency vs. Accuracy:** The document context states this method is "less compute-expensive, but not as accurate." The efficiency comes from pre-computing document embeddings, but the lack of direct interaction during encoding can lead to lower accuracy.

These insights are directly supported by the distinct architectural designs in (a) and (b), the textual labels `z_CLS`, `U`, `s(q,d)`, `z_CLS_Q`, `z_CLS_D`, and the explicit descriptions in the accompanying document text.

**Document Context:**
This image directly supports Section 14.2, "Information Retrieval with Dense Vectors," by visually explaining two fundamental approaches to achieving dense retrieval. It serves as a visual aid to understand the conceptual differences between joint encoding (a) and separate encoding (b) as described in the accompanying text.

The text after the image explicitly states: "Figure 14.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically represent self-attention: (a) Use a single encoder to jointly encode query and document and finetune to produce a relevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring (b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the query and document as the score. This is less compute-expensive, but not as accurate." My detailed description, especially the systematic process mapping and content interpretation, aligns perfectly with this explanation, breaking down each component and its role as described. The image visually clarifies why (a) involves more interaction (single encoder, self-attention across query and document) and why (b) allows for independent encoding, directly linking to the computational cost and accuracy implications discussed in the text.

**Summary:**
The image illustrates two distinct architectures, labeled (a) and (b), for dense retrieval. Both architectures aim to produce a relevance score, denoted as s(q,d), between a query (q) and a document (d).

**Architecture (a): Joint Encoder**
This architecture uses a single encoder to process both the query and the document. Input tokens for the "Query", a "[sep]" token, and the "Document" (which can extend to multiple tokens, indicated by "...") are fed into a single large encoder block. This encoder consists of multiple layers of processing units, visually represented by horizontal green and purple rectangles. The criss-crossing lines between these layers schematically represent self-attention, indicating interactions between all input tokens across the entire sequence. From the output of the final layer, a single representation labeled "z_CLS" is extracted. This "z_CLS" representation is then passed through a linear layer, labeled "U" (depicted as a trapezoid), to produce the final relevance score "s(q,d)".

**Architecture (b): Separate Encoders (Dual Encoder)**
This architecture employs two distinct, separate encoders: one for the query and one for the document. The "Query" input tokens are fed into a left encoder block, which is visually similar to the encoder in (a) but specifically processes the query. From this query encoder, a representation labeled "z_CLS_Q" is extracted. Simultaneously, the "Document" input tokens are fed into a separate right encoder block, which specifically processes the document. From this document encoder, a representation labeled "z_CLS_D" is extracted. Unlike architecture (a), these two representations ("z_CLS_Q" and "z_CLS_D") are generated independently. To compute the relevance score, these two representations are combined using a scoring mechanism, represented by a diamond shape with a black dot inside, which typically signifies a dot product. The output of this scoring mechanism is the final relevance score "s(q,d)".

In essence, (a) represents a more integrated, but computationally intensive, approach where query and document interact at every layer of encoding, while (b) represents a more efficient approach where query and document are encoded independently before their representations are compared.](images/966616d1127a2c5f0088a01740dfe6c6a28494e6a362237f5c3678b08886f6bf.jpg)
Figure 14.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically represent self-attention: (a) Use a single encoder to jointly encode query and document and finetune to produce a relevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring (b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the query and document as the score. This is less compute-expensive, but not as accurate.

The problem with the full BERT architecture in Fig. 14.7a is the expense in computation and time. With this architecture, every time we get a query, we have to pass every single single document in our entire collection through a BERT encoder jointly with the new query! This enormous use of resources is impractical for real cases.

At the other end of the computational spectrum is a much more efficient architecture, the bi-encoder. In this architecture we can encode the documents in the collection only one time by using two separate encoder models, one to encode the query and one to encode the document. We encode each document, and store all the encoded document vectors in advance. When a query comes in, we encode just this query and then use the dot product between the query vector and the precomputed document vectors as the score for each candidate document (Fig. 14.7b). For example, if we used BERT, we would have two encoders $\mathtt { B E R T } _ { Q }$ and $\mathrm { B E R T } _ { D }$ and we could represent the query and document as the [CLS] token of the respective encoders (Karpukhin et al., 2020):

$$
\begin{array} { r } { \mathsf { z } _ { q } = \mathsf { B E R T } _ { \mathcal { Q } } ( \mathsf { q } ) \left[ \mathsf { C L S } \right] } \\ { \mathsf { z } _ { d } = \mathsf { B E R T } _ { D } ( \mathsf { d } ) \left[ \mathsf { C L S } \right] } \\ { \mathsf { s c o r e } ( q , d ) = \mathsf { z } _ { q } \cdot \mathsf { z } _ { d } } \end{array}
$$

The bi-encoder is much cheaper than a full query/document encoder, but is also less accurate, since its relevance decision can’t take full advantage of all the possible meaning interactions between all the tokens in the query and the tokens in the document.

There are numerous approaches that lie in between the full encoder and the biencoder. One intermediate alternative is to use cheaper methods (like BM25) as the first pass relevance ranking for each document, take the top N ranked documents, and use expensive methods like the full BERT scoring to rerank only the top N documents rather than the whole set.

Another intermediate approach is the ColBERT approach of Khattab and Zaharia (2020) and Khattab et al. (2021), shown in Fig. 14.8. This method separately encodes the query and document, but rather than encoding the entire query or document into one vector, it separately encodes each of them into contextual representations for each token. These BERT representations of each document word can be pre-stored for efficiency. The relevance score between a query $q$ and a document $d$ is a sum of maximum similarity (MaxSim) operators between tokens in $q$ and tokens in $d$ . Essentially, for each token in $q$ , ColBERT finds the most contextually similar token in $d$ , and then sums up these similarities. A relevant document will have tokens that are contextually very similar to the query.

More formally, a question $q$ is tokenized as $[ q _ { 1 } , \ldots , q _ { n } ]$ , prepended with a [CLS] and a special [Q] token, truncated to ${ \bf N } = 3 2$ tokens (or padded with [MASK] tokens if it is shorter), and passed through BERT to get output vectors $\mathbf q = [ \mathbf q _ { 1 } , \dots , \mathbf q _ { N } ]$ . The passage $d$ with tokens $[ d _ { 1 } , \ldots , d _ { m } ]$ , is processed similarly, including a [CLS] and special [D] token. A linear layer is applied on top of $\mathbf { d }$ and $\mathbf { q }$ to control the output dimension, so as to keep the vectors small for storage efficiency, and vectors are rescaled to unit length, producing the final vector sequences $\mathsf { E } _ { q }$ (length $N _ { . }$ ) and $\mathsf { E } _ { d }$ (length $m$ ). The ColBERT scoring mechanism is:

$$
\operatorname { s c o r e } ( q , d ) = \sum _ { i = 1 } ^ { N } \operatorname* { m a x } _ { j = 1 } ^ { m } \boldsymbol { \mathsf { E } } _ { q _ { i } } \cdot \boldsymbol { \mathsf { E } } _ { d _ { j } }
$$

While the interaction mechanism has no tunable parameters, the ColBERT architecture still needs to be trained end-to-end to fine-tune the BERT encoders and train the linear layers (and the special [Q] and [D] embeddings) from scratch. It is trained on triples $\langle q , d ^ { + } , d ^ { - } \rangle$ of query $q$ , positive document $d ^ { + }$ and negative document $d ^ { - }$ to produce a score for each document using Eq. 14.19, optimizing model parameters using a cross-entropy loss.

All the supervised algorithms (like ColBERT or the full-interaction version of the BERT algorithm applied for reranking) need training data in the form of queries together with relevant and irrelevant passages or documents (positive and negative examples). There are various semi-supervised ways to get labels; some datasets (like MS MARCO Ranking, Section 14.3.2) contain gold positive examples. Negative examples can be sampled randomly from the top-1000 results from some existing IR system. If datasets don’t have labeled positive examples, iterative methods like relevance-guided supervision can be used (Khattab et al., 2021) which rely on the fact that many datasets contain short answer strings. In this method, an existing IR system is used to harvest examples that do contain short answer strings (the top few are taken as positives) or don’t contain short answer strings (the top few are taken as negatives), these are used to train a new retriever, and then the process is iterated.

![## Image Analysis: f328b1afdacf18a6eae47a9a5db8dc6e493f8c95748f69c8d2fbb672da670e64.jpg

**Conceptual Understanding:**
Conceptually, this image represents the ColBERT (Contextualized Late Interaction over BERT) architecture, specifically illustrating its inference-time mechanism for computing a similarity score between a natural language query and a document. The main purpose is to demonstrate how contextualized embeddings, generated by BERT, are utilized in a fine-grained, token-level interaction to determine the relevance of a document to a query. It communicates the key idea of 'late interaction', where query and document are encoded independently up to the token embedding stage, and then their similarity is computed by aligning and summing maximal similarities between individual query and document token representations.

**Content Interpretation:**
This image illustrates the architectural flow of the ColBERT algorithm at inference time. It shows the sequential processes involved in calculating a similarity score between a given query (q) and a document (d). The primary components are: separate BERT encoders for the query and document, normalization steps for token embeddings, a MaxSim layer for computing maximum similarity between query and document tokens, and a final summation step to produce the overall similarity score. The architecture emphasizes the independent encoding of query and document followed by a token-level interaction for similarity calculation.

**Key Insights:**
The main takeaways from this image are: 
1.  **Separate Encoding:** Both the query and document are processed independently by their respective BERT encoders (represented by the layered green and purple blocks), generating contextualized token embeddings. This aligns with the text's mention of 'separate BERT encoders'.
2.  **Token-Level Normalization:** Before similarity computation, each token's embedding undergoes normalization ('norm' boxes), ensuring consistent scaling for accurate comparisons.
3.  **Maximal Similarity (MaxSim) Pooling:** The core of the similarity computation involves 'MaxSim' units. These units perform a soft alignment, where each query token's normalized embedding is compared against all normalized document token embeddings to find the maximum similarity score. This is supported by the converging lines from 'norm' outputs to 'MaxSim' units, and the description in the text.
4.  **Aggregation:** The individual 'MaxSim' scores are then summed up (Σ) to yield the final overall similarity score 's(q,d)', which is used to rank documents. This provides a detailed view of how the final similarity score is derived through aggregation of token-level interactions.
5.  **Modular Architecture:** The clear separation of encoding, normalization, and similarity computation steps highlights a modular and efficient design for dense retrieval.

**Document Context:**
This image directly supports the document's section on 'Information Retrieval with Dense Vectors' by providing a visual explanation of the ColBERT algorithm, as stated in the accompanying text (Figure 14.8). It clarifies how dense vectors from BERT encoders are utilized to compute document-query similarity, specifically highlighting the 'soft alignment' and 'MaxSim' mechanisms which are central to ColBERT's approach. The diagram is crucial for understanding the computational steps and the interaction between query and document representations in this specific retrieval model.

**Summary:**
The image illustrates the ColBERT algorithm at inference time, showing how a similarity score s(q,d) is computed between a query (q) and a document (d). The process begins with the query and document, each represented by a series of input tokens (indicated by the small red and orange circles at the bottom). 

Both the query and the document are first fed into separate BERT encoders, represented by the large rectangular blocks containing multiple horizontal layers of green (for query) and purple (for document) rectangles. These layers signify the multi-layered transformer architecture of BERT, which processes the input tokens to generate contextualized embeddings for each token. The ellipses (...) within the document's BERT encoder suggest that documents are typically longer and have more token representations than queries, or that not all layers are explicitly shown.

Following the BERT encoding, the contextualized embeddings for each token from both the query and the document undergo a normalization step, indicated by the small green boxes labeled "norm". This ensures that the vector representations are on a consistent scale, which is crucial for accurate similarity comparisons.

The normalized token embeddings from both the query and the document are then fed into multiple "MaxSim" (Maximum Similarity) computation units. Each "MaxSim" unit takes input from various normalized query token embeddings and various normalized document token embeddings (indicated by the crisscrossing black lines and dashed green lines). The dashed green lines specifically highlight the 'soft alignment' concept, where each query token is compared against all document tokens to find its maximum similarity.

Finally, the outputs from all the "MaxSim" units are aggregated by a summation operation, represented by the diamond shape containing the Greek letter Sigma (Σ). This summation yields the final similarity score, labeled "s(q,d)", which quantifies the relevance of the document to the query.](images/f328b1afdacf18a6eae47a9a5db8dc6e493f8c95748f69c8d2fbb672da670e64.jpg)
Figure 14.8 A sketch of the ColBERT algorithm at inference time. The query and document are first passed through separate BERT encoders. Similarity between query and document is computed by summing a soft alignment between the contextual representations of tokens in the query and the document. Training is end-to-end. (Various details aren’t depicted; for example the query is prepended by a [CLS] and [Q:] tokens, and the document by [CLS] and [D:] tokens). Figure adapted from Khattab and Zaharia (2020).

Efficiency is an important issue, since every possible document must be ranked for its similarity to the query. For sparse word-count vectors, the inverted index allows this very efficiently. For dense vector algorithms finding the set of dense document vectors that have the highest dot product with a dense query vector is an instance of the problem of nearest neighbor search. Modern systems therefore make use of approximate nearest neighbor vector search algorithms like Faiss (Johnson et al., 2017).

# 14.3 Answering Questions with RAG

The dominant paradigm for question answering is to answer a user’s question by first finding supportive text segments from the web or another other large collection of documents, and then generating an answer based on the documents. The method of generating based on retrieved documents is called retrieval-augmented generation or RAG, and the two components are sometimes called the retriever and the reader (Chen et al., 2017a). Fig. 14.9 sketches out this standard QA model.

In the first stage of the 2-stage retrieve and read model in Fig. 14.9 we retrieve relevant passages from a text collection, for example using the dense retrievers of the previous section. In the second reader stage, we generate the answer via retrievalaugmented generation. In this method, we take a large pretrained language model, give it the set of retrieved passages and other text as its prompt, and autoregressively generate a new answer token by token.

![## Image Analysis: b8885513b58384a5bc5c0b77248d20fb7bae01d4bdd62c860ccb2fceab97afaf.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture and workflow of a Retrieval-Augmented Generation (RAG) system for question answering. Its main purpose is to illustrate how a natural language query is processed through a two-stage pipeline: first, retrieving relevant documents, and then, using a large language model to generate an answer based on both the original query and the retrieved documents. The key ideas communicated are the integration of an information retrieval component ('Retriever') with a language generation component ('Reader/Generator' powered by an 'LLM') to provide grounded and accurate answers.

**Content Interpretation:**
The image depicts a two-stage Retrieval-Augmented Generation (RAG) system for question answering. The first stage is 'Retrieval', where a 'Retriever' component processes an input question (e.g., 'Q: When was the premiere of The Magic Flute?'). It queries a collection of 'Indexed Docs' to find and output 'Relevant Docs'. The second stage is 'Reading/Generation', where a 'Reader/Generator' component receives both the original 'query' and the 'Relevant Docs' (labeled as 'docs'). Inside the 'Reader/Generator', an 'LLM' (Large Language Model) is utilized, informed by a 'prompt' that integrates the retrieved documents and the query. This 'LLM' then generates a precise answer, such as 'A: 1791'. The significance lies in demonstrating how external knowledge retrieval enhances the question-answering capabilities of an LLM by providing context and factual grounding.

**Key Insights:**
The main takeaway from this image is the clear illustration of the two-stage RAG process. First, a 'Retriever' identifies relevant information from a larger corpus ('Indexed Docs') based on a user's 'query', producing 'Relevant Docs'. Second, a 'Reader/Generator', utilizing an 'LLM' and a 'prompt' constructed from both the 'query' and the 'docs' (Relevant Docs), synthesizes a precise 'Answer'. This demonstrates that LLMs in RAG systems don't generate answers solely from their pre-trained knowledge but are augmented with real-time, retrieved information, which enhances accuracy and reduces hallucinations. The specific example 'Q: When was the premiere of The Magic Flute?' leading to 'A: 1791' exemplifies a factual question being answered by grounding the LLM with specific, retrieved documents.

**Document Context:**
This image directly supports the document's section '14.3 Answering Questions with RAG' by visually explaining the architectural flow of a RAG system. It serves as 'Figure 14.9 Retrieval-based question answering' as described in the accompanying text, clearly showing the two distinct stages: retrieval and reading (generation). The visual diagram provides a concrete example of how a user's question is processed through these stages to yield an accurate answer, reinforcing the conceptual explanation provided in the surrounding text.

**Summary:**
This image illustrates the two-stage process of Retrieval-Augmented Generation (RAG) for answering questions. The process begins with a user posing a question, such as "Q: When was the premiere of The Magic Flute?". This query is first directed to a component labeled "Retriever". The "Retriever" accesses a collection of information called "Indexed Docs", which are depicted as multiple stacked document icons within the "Retriever" block. Based on the input question, the "Retriever" identifies and extracts a subset of these documents, which are then labeled as "Relevant Docs" and represented by fewer stacked document icons. These "Relevant Docs" are then fed into the next stage, a component labeled "Reader/Generator". Simultaneously, the original "query" (the user's question) is also sent directly to the "Reader/Generator". Inside the "Reader/Generator" component, there is a sub-component labeled "LLM" (Large Language Model), which receives input from a path labeled "prompt". Both the "Relevant Docs" (labeled "docs" on the connecting arrow) and the original "query" serve as inputs to the "Reader/Generator", specifically informing the "LLM" via the "prompt". Finally, after processing these inputs, the "Reader/Generator" outputs the definitive answer, shown as "A: 1791". This clear flow demonstrates how the system first finds pertinent information and then uses a language model to formulate an answer based on that information and the original question.](images/b8885513b58384a5bc5c0b77248d20fb7bae01d4bdd62c860ccb2fceab97afaf.jpg)
Figure 14.9 Retrieval-based question answering has two stages: retrieval, which returns relevant documents from the collection, and reading, in which an LLM generates answers given the documents as a prompt.

# 14.3.1 Retrieval-Augmented Generation

The standard reader algorithm is to generate from a large language model, conditioned on the retrieved passages. This method is known as retrieval-augmented generation, or RAG.

Recall that in simple conditional generation, we can cast the task of question answering as word prediction by giving a language model a question and a token like A: suggesting that an answer should come next:

Q: Who wrote the book ‘‘The Origin of Species"? A:

Then we generate autoregressively conditioned on this text.

More formally, recall that simple autoregressive language modeling computes the probability of a string from the previous tokens:

$$
p ( \boldsymbol { x } _ { 1 } , \ldots , \boldsymbol { x } _ { n } ) \ = \ \prod _ { i = 1 } ^ { n } p ( x _ { i } | \boldsymbol { x } _ { < i } )
$$

And simple conditional generation for question answering adds a prompt like Q: , followed by a query $q$ , and A:, all concatenated:

$$
p ( x _ { 1 } , \ldots , x _ { n } ) \ = \ \prod _ { i = 1 } ^ { n } p ( [ 0 ! ] ; q ; [ \mathtt { A } : ] ; x _ { < i } )
$$

The advantage of using a large language model is the enormous amount of knowledge encoded in its parameters from the text it was pretrained on. But as we mentioned at the start of the chapter, while this kind of simple prompted generation can work fine for many simple factoid questions, it is not a general solution for QA, because it leads to hallucination, is unable to show users textual evidence to support the answer, and is unable to answer questions from proprietary data.

The idea of retrieval-augmented generation is to address these problems by conditioning on the retrieved passages as part of the prefix, perhaps with some prompt text like “Based on these texts, answer this question:”. Let’s suppose we have a query $q$ , and call the set of retrieved passages based on it $\operatorname { R } ( q )$ . For example, we could have a prompt like:

# Schematic of a RAG Prompt

retrieved passage 1   
retrieved passage 2

retrieved passage n

Based on these texts, answer this question: Q: Who wrote the book ‘‘The Origin of Species"? A:

Or more formally,

$$
p ( x _ { 1 } , \ldots , x _ { n } ) \ = \ \prod _ { i = 1 } ^ { n } p ( x _ { i } | \mathrm { R } ( q ) ; \mathrm { p r o m p t } ; [ 0 : ] ; q ; [ \mathrm { A } : ] ; x _ { < i } )
$$

multi-hop

As with the span-based extraction reader, successfully applying the retrievalaugmented generation algorithm for QA requires a successful retriever, and often a two-stage retrieval algorithm is used in which the retrieval is reranked. Some complex questions may require multi-hop architectures, in which a query is used to retrieve documents, which are then appended to the original query for a second stage of retrieval. Details of prompt engineering also have to be worked out, like deciding whether to demarcate passages, for example with [SEP] tokens, and so on. Combinations of private data and public data involving an externally hosted large language model may lead to privacy concerns that need to be worked out (Arora et al., 2023). Much research in this area also focuses on ways to more tightly integrate the retrieval and reader stages.

# 14.3.2 Question Answering Datasets

There are scores of question answering datasets, used both for instruction tuning and for evaluation of the question answering abilities of language models.

We can distinguish the datasets along many dimensions, summarized nicely in Rogers et al. (2023). One is the original purpose of the questions in the data, whether they were natural information-seeking questions, or whether they were questions designed for probing: evaluating or testing systems or humans.

On the natural side there are datasets like Natural Questions (Kwiatkowski et al., 2019), a set of anonymized English queries to the Google search engine and their answers. The answers are created by annotators based on Wikipedia information, and include a paragraph-length long answer and a short span answer. For example the question “When are hops added to the brewing process?” has the short answer the boiling process and a long answer which is an entire paragraph from the Wikipedia page on Brewing.

A similar natural question set is the MS MARCO (Microsoft Machine Reading Comprehension) collection of datasets, including 1 million real anonymized English questions from Microsoft Bing query logs together with a human generated answer and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval ranking and question answering.

reading comprehension

MMLU

Although many datasets focus on English, natural information-seeking question datasets exist in other languages. The DuReader dataset is a Chinese QA resource based on search engine queries and community QA (He et al., 2018). TyDi QA dataset contains 204K question-answer pairs from 11 typologically diverse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark et al., 2020a). In the TYDI QA task, a system is given a question and the passages from a Wikipedia article and must (a) select the passage containing the answer (or NULL if no passage contains the answer), and (b) mark the minimal answer span (or NULL).

open book closed book

On the probing side are datasets like MMLU (Massive Multitask Language Understanding), a commonly-used dataset of 15908 knowledge and reasoning questions in 57 areas including medicine, mathematics, computer science, law, and others. MMLU questions are sourced from various exams for humans, such as the US Graduate Record Exam, Medical Licensing Examination, and Advanced Placement exams. So the questions don’t represent people’s information needs, but rather are designed to test human knowledge for academic or licensing purposes. Fig. 14.10 shows some examples, with the correct answers in bold.

Some of the question datasets described above augment each question with passage(s) from which the answer can be extracted. These datasets were mainly created for an earlier QA task called reading comprehension in which a model is given a question and a document and is required to extract the answer from the given document. We sometimes call the task of question answering given one or more documents (for example via RAG), the open book QA task, while the task of answering directly from the LM with no retrieval component at all is the closed book QA task.5 Thus datasets like Natural Questions can be treated as open book if the solver uses each question’s attached document, or closed book if the documents are not used, while datasets like MMLU are solely closed book.

Another dimension of variation is the format of the answer: multiple-choice versus freeform. And of course there are variations in prompting, like whether the model is just the question (zero-shot) or also given demonstrations of answers to similar questions (few-shot). MMLU offers both zero-shot and few-shot prompt options.

# 14.4 Evaluating Question Answering

Three techniques are commonly employed to evaluate question-answering systems, with the choice depending on the type of question and QA situation. For multiple choice questions like in MMLU, we report exact match:

Exact match: The $\%$ of predicted answers that match the gold answer exactly.

For questions with free text answers, like Natural Questions, we commonly evaluated with token $\mathbf { F } _ { 1 }$ score to roughly measure the partial string overlap between the answer and the reference answer:

$\mathbf { F } _ { 1 }$ score: The average token overlap between predicted and gold answers. Treat the prediction and gold as a bag of tokens, and compute $\mathrm { F } _ { 1 }$ for each question, then return the average $\mathrm { F } _ { 1 }$ over all questions.

# College Computer Science

Any set of Boolean operators that is sufficient to represent all Boolean expressions is said to be complete. Which of the following is NOT complete?

(A) AND, NOT(B) NOT, OR(C) AND, OR(D) NAND

# College Physics

The primary source of the Sun’s energy is a series of thermonuclear reactions in which the energy produced is $\mathrm { c } ^ { 2 }$ times the mass difference between

(A) two hydrogen atoms and one helium atom $\mathbf { ( B ) }$ four hydrogen atoms and one helium atom (C) six hydrogen atoms and two helium atoms (D) three helium atoms and one carbon atom

# International Law

Which of the following is a treaty-based human rights mechanism?

(A) The UN Human Rights Committee (B) The UN Human Rights Council (C) The UN Universal Periodic Review (D) The UN special mandates

# Prehistory

Unlike most other early civilizations, Minoan culture shows little evidence of

(A) trade.   
(B) warfare.   
(C) the development of a common religion.   
$\mathbf { \eta } ^ { ( \mathbf { D } ) }$ conspicuous consumption by elites.

Figure 14.10 Example problems from MMLU

Finally, in some situations QA systems give multiple ranked answers. In such cases we evaluated using mean reciprocal rank, or MRR (Voorhees, 1999). MRR is designed for systems that return a short ranked list of answers or passages for each test set question, which we can compare against the (human-labeled) correct answer. First, each test set question is scored with the reciprocal of the rank of the first correct answer. For example if the system returned five answers to a question but the first three are wrong (so the highest-ranked correct answer is ranked fourth), the reciprocal rank for that question is $\frac { 1 } { 4 }$ . The score for questions that return no correct answer is 0. The MRR of a system is the average of the scores for each question in the test set. In some versions of MRR, questions with a score of zero are ignored in this calculation. More formally, for a system returning ranked answers to each question in a test set $Q$ , (or in the alternate version, let $Q$ be the subset of test set questions that have non-zero scores). MRR is then defined as

$$
\mathrm { M R R } = \frac { 1 } { | Q | } \sum _ { i = 1 } ^ { | Q | } \frac { 1 } { r a n k _ { i } }
$$

# 14.5 Summary

This chapter introduced the tasks of question answering and information retrieval.

• Question answering (QA) is the task of answering a user’s questions.   
• We focus in this chapter on the task of retrieval-based question answering, in which the user’s questions are intended to be answered by the material in some set of documents (which might be the web).   
• Information Retrieval (IR) is the task of returning documents to a user based on their information need as expressed in a query. In ranked retrieval, the documents are returned in ranked order.   
• The match between a query and a document can be done by first representing each of them with a sparse vector that represents the frequencies of words, weighted by tf-idf or BM25. Then the similarity can be measured by cosine.   
• Documents or queries can instead be represented by dense vectors, by encoding the question and document with an encoder-only model like BERT, and in that case computing similarity in embedding space.   
• The inverted index is a storage mechanism that makes it very efficient to find documents that have a particular word.   
• Ranked retrieval is generally evaluated by mean average precision or interpolated precision.   
• Question answering systems generally use the retriever/reader architecture. In the retriever stage, an IR system is given a query and returns a set of documents.   
• The reader stage is implemented by retrieval-augmented generation, in which a large language model is prompted with the query and a set of documents and then conditionally generates a novel answer.   
• QA can be evaluated by exact match with a known answer if only a single answer is given, with token $\mathrm { F } _ { 1 }$ score for free text answers, or with mean reciprocal rank if a ranked set of answers is given.

# Bibliographical and Historical Notes

Question answering was one of the earliest NLP tasks, and early versions of the textbased and knowledge-based paradigms were developed by the very early 1960s. The text-based algorithms generally relied on simple parsing of the question and of the sentences in the document, and then looking for matches. This approach was used very early on (Phillips, 1960) but perhaps the most complete early system, and one that strikingly prefigures modern relation-based systems, was the Protosynthex system of Simmons et al. (1964). Given a question, Protosynthex first formed a query from the content words in the question, and then retrieved candidate answer sentences in the document, ranked by their frequency-weighted term overlap with the question. The query and each retrieved sentence were then parsed with dependency parsers, and the sentence whose structure best matches the question structure selected. Thus the question What do worms eat? would match worms eat grass: both have the subject worms as a dependent of eat, in the version of dependency grammar used at the time, while birds eat worms has birds as the subject:

![## Image Analysis: 4cd55428e7e9a86543f93dda1a1673f828b267c4a3aeb244109c76aaf8324b9a.jpg

**Conceptual Understanding:**
This image conceptually represents an abstract diagram of pathways or channels, potentially illustrating flow, connection, or a system with decision/control points. The main purpose or message being conveyed is ambiguous without any accompanying text or labels. Visually, it depicts two parallel-like structures, each with a central, elongated, oval-shaped element that could signify a valve, gate, or a point of processing/diversion. The lower structure notably branches into two distinct paths. The key ideas communicated are purely structural and abstract, such as 'flow', 'connection', 'divergence', and 'control points', but these interpretations are inferred from generic diagrammatic conventions rather than explicit information within the image itself.

**Content Interpretation:**
The image displays two distinct, abstract, curved flow lines or paths. The upper line describes a wider, arched trajectory with a central elongated oval element, potentially representing a gate, valve, or control point within a flow. The line continues downwards from this central element. The lower line is a narrower, smaller arch also featuring a similar central elongated oval element. From this lower central element, the flow diverges into two distinct downward paths. Without any textual labels or context, the specific processes, concepts, relationships, or systems being shown cannot be definitively identified. The elements suggest some form of controlled or directed flow, branching, or connection within a system, but their meaning is purely speculative based on the visual representation alone. There is no data, trends, or specific information presented beyond these abstract lines and shapes.

**Key Insights:**
Due to the complete absence of any text, labels, or annotations within the image, no specific knowledge, takeaways, lessons, conclusions, or insights can be extracted. The image is an abstract visual representation of curved lines and two oval-shaped elements, which could metaphorically represent paths, flows, or junctions, but without textual evidence, no concrete meaning or information is conveyed.

**Document Context:**
Given the document context 'Section: Bibliographical and Historical Notes' and the text after the image 'What do worms eat', the provided abstract line drawing has no apparent contextual relevance based solely on its visual content. The image contains no text or discernible elements that would link it to bibliographical notes, historical information, or the feeding habits of worms. Its abstract nature, coupled with the complete absence of any explanatory text, makes it impossible to connect it to the broader narrative or argument of the document. It appears as an isolated, unannotated diagram.

**Summary:**
The image provided is a monochrome line drawing depicting two abstract, curved paths or channels. The upper path is wider, forming a large arch that curves downwards from both its implied starting points. In the middle of this upper arch, there is a horizontally oriented, elongated oval shape, resembling a gate or a valve, through which the path continues downwards. The lower path is narrower and positioned below the upper one. It also curves downwards, forming a smaller arch. Similar to the upper path, this lower path features an elongated, oval-like shape in its middle, acting as a junction or a gate. From this lower central oval shape, two separate downward-pointing lines or paths diverge. The overall impression is a simplified diagram illustrating potential flow or connection points, but without any textual labels, it remains highly abstract and its specific function or meaning is not evident. Given the complete absence of any text, labels, numbers, or annotations within the image, its relevance to the document's context, 'Bibliographical and Historical Notes' or 'What do worms eat', cannot be determined from the visual content alone.](images/4cd55428e7e9a86543f93dda1a1673f828b267c4a3aeb244109c76aaf8324b9a.jpg)
What do worms eat

![## Image Analysis: 514bacd1f9f7081ba84384c855811a8d7fff299a91f98fda9840f1b3f18c1771.jpg

**Conceptual Understanding:**
The image conceptually illustrates a generic branching or bifurcation event. Each of the two identical symbols represents a process where a single input or pathway leads to a central node (the oval), from which two distinct output pathways diverge. The main purpose is to visually communicate the concept of divergence, splitting, or a 'one-to-many' relationship in a highly simplified, abstract form. The key ideas communicated are branching, transformation, and multiple outcomes from a single origin or stage.

**Content Interpretation:**
The image shows two identical abstract graphical symbols, each depicting a conceptual point of bifurcation or divergence. Each symbol represents a single input (implied by the horizontal line leading to the oval) undergoing a process or transformation (the oval shape), which then results in two distinct outputs or subsequent paths (the two downward-pointing arrows). The repetition of this symbol suggests either two separate, identical instances of such a split, or possibly two sequential or parallel stages within a larger, unillustrated system. There are no specific processes, concepts, relationships, or systems explicitly labeled or described within the image due to the complete absence of text.

**Key Insights:**
The main takeaway from this image is the visual representation of a 'split' or 'divergence' point in an abstract flow. It illustrates how a single conceptual input can lead to two distinct outputs or subsequent paths. The image supports the insight that processes or relationships can involve branching, where a singular entity or event leads to multiple subsequent entities or events. However, due to the complete absence of any text within the image, all insights are derived purely from the abstract geometric shapes and lines, and thus lack specific textual evidence from the image itself.

**Document Context:**
Given the document context 'Bibliographical and Historical Notes' and the accompanying text 'Worms eat grass Birds eat worms', these abstract symbols could metaphorically represent stages within a food chain or an ecological interaction. For example, each symbol might illustrate a transformation or consumption event where one entity leads to multiple outcomes or is consumed by multiple entities. The horizontal line could represent an organism or resource, the oval could symbolize the act of consumption or transformation, and the diverging arrows could represent the subsequent entities or pathways that result. However, this interpretation is highly speculative and inferred solely from the surrounding text, as the image itself provides no textual labels or explicit context to define its meaning. It fits into the broader narrative by potentially visualizing a very simplified, abstract model of interaction or flow relevant to natural processes.

**Summary:**
The image displays two identical abstract symbols arranged horizontally. Each symbol consists of a horizontal line segment that enters and is bisected by a vertically oriented oval shape. From the bottom of this oval, two distinct, curved lines emerge, arching outwards and downwards. Each of these curved lines terminates in a downward-pointing arrow, indicating a directional flow or outcome. There is no text present within the image itself, including no labels, annotations, or any other textual elements. The symbols are purely graphical representations of what appears to be a divergence or split in a path or process.](images/514bacd1f9f7081ba84384c855811a8d7fff299a91f98fda9840f1b3f18c1771.jpg)
Worms eat grass   
Birds eat worms

The alternative knowledge-based paradigm was implemented in the BASEBALL system (Green et al., 1961). This system answered questions about baseball games like “Where did the Red Sox play on July $7 ^ { \circ }$ by querying a structured database of game information. The database was stored as a kind of attribute-value matrix with values for attributes of each game:

Month $=$ July Place $=$ Boston Day $= ~ 7$ Game Serial No. $= ~ 9 6$ (Team $=$ Red Sox, Score = 5) (Team $=$ Yankees, Score = 3)

Each question was constituency-parsed using the algorithm of Zellig Harris’s TDAP project at the University of Pennsylvania, essentially a cascade of finite-state transducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen 1999). Then in a content analysis phase each word or phrase was associated with a program that computed parts of its meaning. Thus the phrase ‘Where’ had code to assign the semantics $\mathsf { P 1 a c e } = ?$ , with the result that the question “Where did the Red Sox play on July $7 ^ { \mathfrak { s } }$ was assigned the meaning

Place = ? Team $=$ Red Sox Month $=$ July Day = 7

The question is then matched against the database to return the answer. Simmons (1965) summarizes other early QA systems.

Another important progenitor of the knowledge-based paradigm for questionanswering is work that used predicate calculus as the meaning representation language. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to be a natural language interface to a database of chemical facts about lunar geology. It could answer questions like Do any samples have greater than 13 percent aluminum by parsing them into a logical form

(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16 (NPR\* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))

By a couple decades later, drawing on new machine learning approaches in NLP, Zelle and Mooney (1996) proposed to treat knowledge-based QA as a semantic parsing task, by creating the Prolog-based GEOQUERY dataset of questions about US geography. This model was extended by Zettlemoyer and Collins (2005) and 2007.

By a decade later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia and Liang 2016), and then to knowledge-based question answering by mapping text to SQL (Iyer et al., 2017).

Meanwhile, the information-retrieval paradigm for question answering was influenced by the rise of the web in the 1990s. The U.S. government-sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992, provide a testbed for evaluating information-retrieval tasks and techniques (Voorhees and Harman, 2005). TREC added an influential QA track in 1999, which led to a wide variety of factoid and non-factoid systems competing in annual evaluations.

At that same time, Hirschman et al. (1999) introduced the idea of using children’s reading comprehension tests to evaluate machine text comprehension algorithms. They acquired a corpus of 120 passages with 5 questions each designed for 3rd-6th grade children, built an answer extraction system, and measured how well the answers given by their system corresponded to the answer key from the test’s publisher. Their algorithm focused on word overlap as a feature; later algorithms added named entity features and more complex similarity between the question and the answer span (Riloff and Thelen 2000, $\mathrm { N g }$ et al. 2000).

The DeepQA component of the Watson Jeopardy! system was a large and sophisticated feature-based system developed just before neural systems became common. It is described in a series of papers in volume 56 of the IBM Journal of Research and Development, e.g., Ferrucci (2012).

Early neural reading comprehension systems drew on the insight common to early systems that answer finding should focus on question-passage similarity. Many of the architectural outlines of these neural systems were laid out in Hermann et al. (2015a), Chen et al. (2017a), and Seo et al. (2017). These systems focused on datasets like Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their successors, usually using separate IR algorithms as input to neural reading comprehension systems. The paradigm of using dense retrieval with a span-based reader, often with a single end-to-end architecture, is exemplified by systems like Lee et al. (2019) or Karpukhin et al. (2020). An important research area with dense retrieval for opendomain QA is training data: using self-supervised methods to avoid having to label positive and negative passages (Sachan et al., 2023).

Early work on large language models showed that they stored sufficient knowledge in the pretraining process to answer questions (Petroni et al., 2019; Raffel et al., 2020; Radford et al., 2019; Roberts et al., 2020), at first not competitively with special-purpose question answerers, but then surpassing them. Retrieval-augmented generation algorithms were first introduced as a way to improve language modeling (Khandelwal et al., 2019), but were quickly applied to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023).

# CHAPTER 15 Chatbots & Dialogue Systems

Les lois de la conversation sont en gen´ eral de ne s’y appesantir sur aucun ob-´ jet, mais de passer leg´ erement, sans effort et sans affectation, d’un sujet \` a un \` autre ; de savoir y parler de choses frivoles comme de choses serieuses ´

[The rules of conversation are, in general, not to dwell on any one subject, but to pass lightly from one to another without effort and without affectation; to know how to speak about trivial topics as well as serious ones;]

The 18th C. Encyclopedia of Diderot, start of the entry on conversation

conversation dialogue

The literature of the fantastic abounds in inanimate objects magically endowed with the gift of speech. From Ovid’s statue of Pygmalion to Mary Shelley’s story about

Frankenstein, we continually reinvent stories about creating something and then having a chat with it. Legend has it that after finishing his sculpture Moses, Michelangelo thought it so lifelike that he tapped it on the knee and commanded it to speak. Perhaps this shouldn’t be surprising. Language is the mark of humanity and sentience, and conversation or dialogue is the most fundamental arena of language. It is the first kind of language we learn as children, and the kind we engage in constantly, whether we are ordering lunch, buying train tickets, or talking with our families, friends, or coworkers.

![## Image Analysis: d7db35d00f844495542133b2129c2e3d169aa09f905d219105da0c15fdd6f0ff.jpg

**Conceptual Understanding:**
This image conceptually represents a masterpiece of Renaissance sculpture, specifically Michelangelo's Moses. Its main purpose is to showcase the artistic and historical significance of this particular artwork. The image communicates the idea of artistic genius, the enduring power of classical art, and the depiction of a profound biblical figure through monumental sculpture. There is no other text or annotation within the image to provide additional conceptual understanding. The image's content is purely visual and artistic.

**Content Interpretation:**
The image displays a prominent classical sculpture, specifically Michelangelo's Moses. The sculpture portrays a powerful and contemplative male figure, seated, with a long beard and muscular build, holding tablets. The figure is depicted in a moment of intense thought or emotion, characteristic of Renaissance artistic expression. It is situated within a grand architectural setting, suggesting its monumental importance as part of a larger structure, likely a tomb. The sepia tone of the photograph emphasizes its historical and artistic significance. No processes, relationships, or systems are explicitly shown; rather, it's a static representation of a work of art.

**Key Insights:**
The primary knowledge extracted is the visual identification of a renowned work of art: Michelangelo's Moses. The image serves as a visual record of this sculpture, highlighting its detailed craftsmanship, the artist's ability to convey profound emotion and power through marble, and the aesthetic qualities of historical photography (sepia tone). There are no data, trends, or specific insights related to the document's supposed subject matter (chatbots) that can be extracted from this image. The main takeaway is an appreciation of classical art rather than technical or academic information relevant to dialogue systems.

**Document Context:**
The provided image, a photograph of Michelangelo's Moses, appears to be completely incongruous with the stated document context of 'CHAPTER 15 Chatbots & Dialogue Systems.' There is no apparent connection between a classical sculpture and the topic of chatbots or dialogue systems. Its inclusion seems to be an error, a placeholder, or an unrelated image, as it does not contribute to or support any narrative pertaining to AI, technology, or conversational interfaces. Therefore, in its current context, the image lacks direct relevance.

**Summary:**
The image is a sepia-toned photograph of a large marble sculpture, identified as Michelangelo's Moses. The sculpture depicts a formidable, seated male figure with a long, flowing beard and muscular physique. He is shown with what appear to be small horns on his head, a traditional interpretation of Moses. His gaze is directed slightly to the left, and his right hand is raised to his beard, conveying an expression of intense thought, resolve, or potential anger. His left hand rests on a set of tablets or a book held against his lap. Elaborate drapery folds around his body, covering his legs and part of his torso. The sculpture is positioned within an ornate architectural niche, flanked by decorative pilasters and panels, which are also adorned with intricate carvings and what might be relief sculptures or further architectural elements. The background appears to be the recess of the niche, darker than the brightly lit figure. The sepia tone gives the photograph a classical, antique aesthetic. No textual elements such as titles, labels, captions, or annotations are present within the image itself. Given the document context of 'CHAPTER 15 Chatbots & Dialogue Systems,' the inclusion of this image of a classical sculpture appears to be entirely unrelated to the chapter's subject matter.](images/d7db35d00f844495542133b2129c2e3d169aa09f905d219105da0c15fdd6f0ff.jpg)

frame chatbot

This chapter introduces the fundamental algorithms of programs that use conversation to interact with users. We often distinguish between two kinds of architectures. Task-oriented dialogue systems converse with users to accomplish fixed tasks like controlling appliances or finding restaurants, relying on a data structure called the frame, which represents the knowledge a system needs to acquire from the user (like the time to set an alarm clock). Chatbots, by contrast, are designed to mimic the longer and more unstructured conversations or ‘chats’ characteristic of human-human interaction. Modern systems incorporate aspects of both; industrial chatbots like ChatGPT can carry on longer unstructured conversations; industrial digital assistants like Siri or Alexa are generally frame-based dialogue systems.

The fact that chatbots and dialogue systems are designed for human-computer interaction has strong implications for their design and use. Many of these implications already became clear in one of the earliest chatbots, ELIZA (Weizenbaum, 1966). ELIZA was designed to simulate a Rogerian psychologist, based on a branch of clinical psychology whose methods involve drawing the patient out by reflecting patient’s statements back at them. Rogerian interactions are the rare type of conversation in which, as Weizenbaum points out, one can “assume the pose of knowing almost nothing of the real world”. If a patient says “I went for a long boat ride” and the psychiatrist says “Tell me about boats”, you don’t assume she didn’t know what

Men are all alike.   
IN WHAT WAY   
They’re always bugging us about something or other. CAN YOU THINK OF A SPECIFIC EXAMPLE   
Well my boyfriend made me come here.   
YOUR BOYFRIEND MADE YOU COME HERE   
He says I’m depressed much of the time.   
I AM SORRY TO HEAR YOU ARE DEPRESSED

Figure 15.1 Sample ELIZA dialogue from Weizenbaum (1966).

a boat is, but rather assume she had some conversational goal.1

Weizenbaum made use of this property of Rogerian psychiatric conversations, along with clever regular expressions, to allow ELIZA to interact in ways that seemed deceptively human-like, as in the sample conversational fragment in Fig. 15.1.

As we foreshadowed in Chapter 2, ELIZA worked by simple rules roughly like: $( \cdot ^ { * } )$ YOU $( . ^ { \ast } )$ ME $- >$ WHAT MAKES YOU THINK I \2 YOU to transform a user sentence like “You hate me” into a system response like

WHAT MAKES YOU THINK I HATE YOU

Among Weizenbaum’s clever tricks are the linking of each ELIZA pattern/rule to a keyword. Consider the following user sentence:

I know everybody laughed at me

Because it has the word “I”, this sentence could match the following rule whose keyword is $I$ :

I (.\*) -> You say you \1

producing:

# YOU SAY YOU KNOW EVERYBODY LAUGHED AT YOU

Weizenbaum points out, however, that a more powerful response would rely on the keyword “everybody”, since someone using universals like everybody or always is probably thinking about a specific person or situation. So the ELIZA algorithm prefers to respond using patterns associated more specific keywords like everybody:

# WHO IN PARTICULAR ARE YOU THINKING OF?

If no keyword matches, the algorithm chooses a non-committal response like “PLEASE GO ON”, “THAT’S VERY INTERESTING”, or “I SEE”.

ELIZA illustrates a number of important issues with chatbots. First, people became deeply emotionally involved and conducted very personal conversations, even to the extent of asking Weizenbaum to leave the room while they were typing. Reeves and Nass (1996) show that people tend to assign human characteristics to computers and interact with them in ways that are typical of human-human interactions. They interpret an utterance in the way they would if it had spoken by a human, (even though they are aware they are talking to a computer). This means that chatbots can have significant influences on people’s cognitive and emotional state.

A second related issue is privacy. When Weizenbaum suggested that he might want to store the ELIZA conversations, people immediately pointed out that this would violate people’s privacy. Modern chatbots in the home are likely to overhear private information, even if they aren’t used for counseling as ELIZA was. Indeed, if a chatbot is human-like, users are more likely to disclose private information, and yet less likely to worry about the harm of this disclosure (Ischen et al., 2019).

Both of these issues (emotional engagement and privacy) mean we need to think carefully about how we deploy chatbots and the people who are interacting with them. Dialogue research that uses human participants often requires getting permission from the Institutional Review Board (IRB) of your institution.

In the next section we introduce some basic properties of human conversation. We then turn in the rest of the chapter to the two basic paradigms for conversational interaction: frame-based dialogue systems and chatbots.

# 15.1 Properties of Human Conversation

Conversation between humans is an intricate and complex joint activity. Before we attempt to design a dialogue system to converse with humans, it is crucial to understand something about how humans converse with each other. Consider some of the phenomena that occur in the conversation between a human travel agent and a human client excerpted in Fig. 15.2.

$\overline { { \mathbf { C } _ { 1 } } }$ : . . . I need to travel in May.   
${ \bf A } _ { 2 }$ : And, what day in May did you want to travel?   
C3: OK uh I need to be there for a meeting that’s from the 12th to the 15th. $\mathrm { A } _ { 4 }$ And you’re flying into what city?   
$\mathrm { C } _ { 5 }$ Seattle.   
$\mathbf { A } _ { 6 }$ And what time would you like to leave Pittsburgh?   
$\mathrm { C } _ { 7 }$ Uh $\mathrm { h m m I }$ don’t think there’s many options for non-stop.   
$\mathbf { A } _ { 8 }$ : Right. There’s three non-stops today.   
$\mathrm { C } _ { 9 }$ : What are they?   
${ \bf A } _ { 1 0 }$ : The first one departs PGH at $1 0 { : } 0 0 { \mathrm { a m } }$ arrives Seattle at 12:05 their time. The second flight departs PGH at $5 { : } 5 5 \mathrm { p m }$ , arrives Seattle at $8 \mathrm { p m }$ . And the last flight departs PGH at $8 { : } 1 5 \mathrm { p m }$ arrives Seattle at $1 0 { : } 2 8 \mathrm { p m }$ .   
$\mathrm { C } _ { 1 1 }$ : OK I’ll take the 5ish flight on the night before on the 11th.   
$\mathbf { A } _ { 1 2 }$ : On the 11th? OK. Departing at $5 { : } 5 5 \mathrm { p m }$ arrives Seattle at 8pm, U.S. Air flight 115.   
$\mathbf { C } _ { 1 3 }$ : OK.   
$\mathbf { A } _ { 1 4 }$ : And you said returning on May 15th?   
$\mathrm { C } _ { 1 5 }$ : Uh, yeah, at the end of the day.   
$\mathbf { A } _ { 1 6 }$ : OK. There’s #two non-stops . . . #   
$\mathrm { C } _ { 1 7 }$ : #Act. . . actually #, what day of the week is the 15th? $\mathbf { A } _ { 1 8 }$ : It’s a Friday.   
$\mathbf { C } _ { 1 9 }$ : Uh hmm. I would consider staying there an extra day til Sunday.   
$\mathbf { A } _ { 2 0 }$ : OK. . . OK. On Sunday I have . . .

# Turns

A dialogue is a sequence of turns $( \mathbf { C } _ { 1 } , \mathbf { A } _ { 2 } , \mathbf { C } _ { 3 }$ , and so on), each a single contribution from one speaker to the dialogue (as if in a game: I take a turn, then you take a turn, then me, and so on). There are 20 turns in Fig. 15.2. A turn can consist of a sentence (like $\mathrm { C } _ { 1 , }$ ), although it might be as short as a single word $( \mathbf { C } _ { 1 3 } )$ or as long as multiple sentences $\left( \mathsf { A } _ { 1 0 } \right)$ .

endpointing

Turn structure has important implications for spoken dialogue. A human has to know when to stop talking; the client interrupts (in $\mathbf { A } _ { 1 6 }$ and $\mathbf { C } _ { 1 7 }$ ), so a system that was performing this role must know to stop talking (and that the user might be making a correction). A system also has to know when to start talking. For example, most of the time in conversation, speakers start their turns almost immediately after the other speaker finishes, without a long pause, because people are can usually predict when the other person is about to finish talking. Spoken dialogue systems must also detect whether a user is done speaking, so they can process the utterance and respond. This task—called endpointing or endpoint detection— can be quite challenging because of noise and because people often pause in the middle of turns.

# speech acts

# Speech Acts

A key insight into conversation—due originally to the philosopher Wittgenstein (1953) but worked out more fully by Austin (1962)—is that each utterance in a dialogue is a kind of action being performed by the speaker. These actions are commonly called speech acts or dialogue acts: here’s one taxonomy consisting of 4 major classes (Bach and Harnish, 1979):

Constatives: committing the speaker to something’s being the case (answering, claiming, confirming, denying, disagreeing, stating)   
Directives: attempts by the speaker to get the addressee to do something (advising, asking, forbidding, inviting, ordering, requesting)   
Commissives: committing the speaker to some future course of action (promising, planning, vowing, betting, opposing)   
Acknowledgments: express the speaker’s attitude regarding the hearer with respect to some social action (apologizing, greeting, thanking, accepting an acknowledgment)

common ground grounding

A user asking a person or a dialogue system to do something (‘Turn up the music’) is issuing a DIRECTIVE. Asking a question that requires an answer is also a way of issuing a DIRECTIVE: in a sense when the system says $\left( \mathbf { A } _ { 2 } \right)$ “what day in May did you want to travel?” it’s as if the system is (very politely) commanding the user to answer. By contrast, a user stating a constraint (like $\mathrm { C } _ { 1 }$ ‘I need to travel in May’) is issuing a CONSTATIVE. A user thanking the system is issuing an ACKNOWLEDGMENT. The speech act expresses an important component of the intention of the speaker (or writer) in saying what they said.

# Grounding

A dialogue is not just a series of independent speech acts, but rather a collective act performed by the speaker and the hearer. Like all collective acts, it’s important for the participants to establish what they both agree on, called the common ground (Stalnaker, 1978). Speakers do this by grounding each other’s utterances. Grounding means acknowledging that the hearer has understood the speaker (Clark, 1996). (People need grounding for non-linguistic actions as well; the reason an elevator button lights up when it’s pressed is to acknowledge that the elevator has indeed been called, essentially grounding your action of pushing the button (Norman, 1988).)

Humans constantly ground each other’s utterances. We can ground by explicitly saying “OK”, as the agent does in $\mathbf { A } _ { 8 }$ or ${ \bf A } _ { 1 0 }$ . Or we can ground by repeating what the other person says; in utterance ${ \bf A } _ { 2 }$ the agent repeats “in May”, demonstrating her understanding to the client. Or notice that when the client answers a question, the agent begins the next question with “And”. The “And” implies that the new question is ‘in addition’ to the old question, again indicating to the client that the agent has successfully understood the answer to the last question.

# Subdialogues and Dialogue Structure

conversation analysis

Conversations have structure. Consider, for example, the local structure between speech acts discussed in the field of conversation analysis (Sacks et al., 1974). QUESTIONS set up an expectation for an ANSWER. PROPOSALS are followed by ACCEPTANCE (or REJECTION). COMPLIMENTS (“Nice jacket!”) often give rise to DOWNPLAYERS (“Oh, this old thing?”). These pairs, called adjacency pairs are composed of a first pair part and a second pair part (Schegloff, 1968), and these expectations can help systems decide what actions to take.

side sequence subdialogue

However, dialogue acts aren’t always followed immediately by their second pair part. The two parts can be separated by a side sequence (Jefferson 1972) or subdialogue. For example utterances $\mathrm { C } _ { 1 7 }$ to $\mathbf { A } _ { 2 0 }$ constitute a correction subdialogue (Litman 1985, Litman and Allen 1987, Chu-Carroll and Carberry 1998):

$\mathrm { C } _ { 1 7 }$ : #Act. . . actually#, what day of the week is the 15th?   
$\mathbf { A } _ { 1 8 }$ : It’s a Friday.   
$\mathrm { C _ { 1 9 } }$ : Uh hmm. I would consider staying there an extra day til Sunday. $\mathbf { A } _ { 2 0 }$ : OK. . . OK. On Sunday I have . . .

The question in $\mathrm { C } _ { 1 7 }$ interrupts the prior discourse, in which the agent was looking for a May 15 return flight. The agent must answer the question and also realize that ‘’I would consider staying...til Sunday” means that the client would probably like to change their plan, and now go back to finding return flights, but for the 17th.

Another side sequence is the clarification question, which can form a subdialogue between a REQUEST and a RESPONSE. This is especially common in dialogue systems where speech recognition errors causes the system to have to ask for clarifications or repetitions like the following:

User: What do you have going to UNKNOWN WORD on the 5th? System: Let’s see, going where on the 5th? User: Going to Hong Kong.   
System: OK, here are some flights...

In addition to side-sequences, questions often have presequences, like the following example where a user starts with a question about the system’s capabilities (“Can you make train reservations”) before making a request.

User: Can you make train reservations?   
System: Yes I can.   
User: Great, I’d like to reserve a seat on the 4pm train to New York.

# Initiative

# initiative

Sometimes a conversation is completely controlled by one participant. For example a reporter interviewing a chef might ask questions, and the chef responds. We say that the reporter in this case has the conversational initiative (Carbonell, 1970; Nickerson, 1976). In normal human-human dialogue, however, it’s more common for initiative to shift back and forth between the participants, as they sometimes answer questions, sometimes ask them, sometimes take the conversations in new directions, sometimes not. You may ask me a question, and then I respond asking you to clarify something you said, which leads the conversation in all sorts of ways. We call such interactions mixed initiative (Carbonell, 1970).

Full mixed initiative, while the norm for human-human conversations, can be difficult for dialogue systems. The most primitive dialogue systems tend to use system-initiative, where the system asks a question and the user can’t do anything until they answer it, or user-initiative like simple search engines, where the user specifies a query and the system passively responds. Even modern large language model-based dialogue systems, which come much closer to using full mixed initiative, often don’t have completely natural initiative switching. Getting this right is an important goal for modern systems.

# Inference and Implicature

Inference is also important in dialogue understanding. Consider the client’s response $\mathrm { C } _ { 2 }$ , repeated here:

${ \bf A } _ { 2 }$ : And, what day in May did you want to travel? $\mathrm { C } _ { 3 }$ : OK uh I need to be there for a meeting that’s from the 12th to the 15th.

Notice that the client does not in fact answer the agent’s question. The client merely mentions a meeting at a certain time. What is it that licenses the agent to infer that the client is mentioning this meeting so as to inform the agent of the travel dates?

implicature

# relevance

The speaker seems to expect the hearer to draw certain inferences; in other words, the speaker is communicating more information than seems to be present in the uttered words. This kind of example was pointed out by Grice (1975, 1978) as part of his theory of conversational implicature. Implicature means a particular class of licensed inferences. Grice proposed that what enables hearers to draw these inferences is that conversation is guided by a set of maxims, general heuristics that play a guiding role in the interpretation of conversational utterances. One such maxim is the maxim of relevance which says that speakers attempt to be relevant, they don’t just utter random speech acts. When the client mentions a meeting on the 12th, the agent reasons ‘There must be some relevance for mentioning this meeting. What could it be?’. The agent knows that one precondition for having a meeting (at least before Web conferencing) is being at the place where the meeting is held, and therefore that maybe the meeting is a reason for the travel, and if so, then since people like to arrive the day before a meeting, the agent should infer that the flight should be on the 11th.

These subtle characteristics of human conversations (turns, speech acts, grounding, dialogue structure, initiative, and implicature) are among the reasons it is difficult to build dialogue systems that can carry on natural conversations with humans. Many of these challenges are active areas of dialogue systems research.

# 15.2 Frame-Based Dialogue Systems

A task-based dialogue system has the goal of helping a user solve a specific task like making a travel reservation or buying a product. Task-based dialogue systems are based around frames, first introduced in the early influential GUS system for travel planning (Bobrow et al., 1977). Frames are knowledge structures representing the details of the user’s task specification. Each frame consists of a collection of slots, each of which can take a set of possible values. Together a set of frames is

sometimes called a domain ontology.

Here we’ll describe the most well-studied frame-based architecture, the dialoguestate architecture, made up of the six components shown in Fig. 15.3. In the next sections we’ll introduce four of them, after introducing the idea of frames (deferring the speech recognition and synthesis components to Chapter 16).

![## Image Analysis: ec69c0acd9fce58d5c7c2775b840a49729a6e1d96902a1e9fc2a3b04b10a51dd.jpg

**Conceptual Understanding:**
This image conceptually represents the functional architecture of a modular, frame-based dialogue system designed for task-oriented interactions. The main purpose is to illustrate the sequential processing steps and the flow of information that occurs when a user converses with an automated system to achieve a specific goal. It highlights how raw speech is transformed through various layers of linguistic and semantic processing, dialogue state management, and policy determination, ultimately leading to a system response delivered back to the user. Key ideas communicated include the decomposition of a complex dialogue system into manageable components, the probabilistic nature of understanding, the importance of maintaining a dialogue state, and the generation of contextually appropriate responses.

**Content Interpretation:**
This image illustrates the modular architecture of a dialogue-state system for task-oriented dialogue. It explicitly details the sequential flow of information and processing stages involved when a user interacts with such a system through speech. The system processes user utterances, interprets their meaning, updates a dialogue state, determines a response strategy, generates a natural language reply, and synthesizes speech for the user. Key processes shown are: Automatic Speech Recognition (ASR) for converting speech to text, Spoken Language Understanding (SLU) for extracting semantic meaning and intent, Dialog State Tracker (DST) for managing the conversation's context and user's goals, Dialog Policy for deciding the system's next action, Natural Language Generation (NLG) for formulating system responses, and Text to Speech (TTS) for vocalizing the system's reply. The image emphasizes the probabilistic nature of speech and language processing by showing confidence scores at the ASR and SLU outputs, as well as for the overall dialogue state.

**Key Insights:**
1.  **Modular Design:** Task-oriented dialogue systems are composed of distinct, specialized modules (ASR, SLU, DST, Dialog Policy, NLG, TTS), each handling a specific part of the dialogue process. This modularity allows for easier development, maintenance, and potential replacement of individual components. (Evidence: The clearly separated boxes labeled 'Automatic Speech Recognition (ASR)', 'Spoken Language Understanding (SLU)', 'Dialog State Tracker (DST)', 'Dialog Policy', 'Natural Language Generation (NLG)', and 'Text to Speech (TTS)'.)
2.  **Probabilistic Processing:** Uncertainty is an inherent part of speech and language understanding. The system manages this by assigning confidence scores or probabilities to different interpretations at various stages. (Evidence: ASR output shows 'LEAVING FROM DOWNTOWN 0.6', 'LEAVING AT ONE P M 0.2', 'ARRIVING AT ONE P M 0.1'. SLU output shows '{ from: downtown } 0.5', '{ depart-time: 1300 } 0.3', '{ arrive-time: 1300 } 0.1'. DST output includes 'score: 0.65', 'score: 0.15', 'score: 0.10'.)
3.  **Dialogue State Management:** A central component, the Dialog State Tracker, is responsible for maintaining and updating the current understanding of the user's intent, filled slots, and the overall conversational context. This state is critical for coherent dialogue progression. (Evidence: The 'Dialog State Tracker (DST)' module with its output including 'from: downtown', 'to: airport', 'depart-time: --', 'confirmed: no', and a dashed feedback loop indicating self-update capabilities.)
4.  **Policy-Based Response Generation:** The system's response is not hardcoded but determined by a 'Dialog Policy' that takes the current dialogue state as input and outputs a high-level 'dialogue act' (e.g., confirm, request, inform). (Evidence: The 'Dialog Policy' module taking input from DST and outputting '{ act: confirm from: downtown }'.)
5.  **Confirmation Strategy:** The system employs explicit confirmation questions to resolve ambiguities or ensure the correct understanding of critical information before proceeding with a task. (Evidence: The NLG output 'FROM DOWNTOWN, IS THAT RIGHT?' following the 'act: confirm' from the Dialog Policy.)
6.  **Sequential and Cyclical Flow:** The dialogue process is a continuous cycle, with information flowing sequentially through components from user input to system output, and then back to the user, ready for the next turn. (Evidence: The clear directional arrows forming a loop from the user's speech to TTS output back to the user.)

**Document Context:**
This image directly supports Section 15.2, "Frame-Based Dialogue Systems," by visually presenting the architectural components and data flow of a typical dialogue-state system, specifically for task-oriented interactions. It serves as a concrete example of how the abstract concepts of ASR, SLU, DST, Dialog Policy, NLG, and TTS are integrated into a functional system. The figure's description as "Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016)" perfectly aligns with the document's discussion of such systems, providing a visual reference for the technical details and components involved in managing a structured conversation.

**Summary:**
This image illustrates the architecture of a dialogue-state system designed for task-oriented dialogue, as per Williams et al. (2016). It depicts a cyclical process starting from a user's speech input and ending with the system's spoken response. The process begins with a user speaking into a phone, which generates an audio waveform. This waveform is fed into the Automatic Speech Recognition (ASR) module. The ASR module transcribes the audio into potential text phrases with associated confidence scores, such as "LEAVING FROM DOWNTOWN" (0.6), "LEAVING AT ONE P M" (0.2), and "ARRIVING AT ONE P M" (0.1). These probabilistic text hypotheses are then passed to the Spoken Language Understanding (SLU) module, which extracts semantic frames and their confidence scores. For instance, it might identify "{ from: downtown }" (0.5), "{ depart-time: 1300 }" (0.3), and "{ arrive-time: 1300 }" (0.1). The output from SLU is sent to the Dialog State Tracker (DST), which maintains and updates the current state of the dialogue. The DST outputs a detailed dialogue state, including identified slots like "from: downtown" and "to: airport", a "depart-time: --" (indicating it's not yet filled), a "confirmed: no" status, and various associated scores (e.g., 0.65, 0.15, 0.10) representing the confidence in the current state. The DST also has a feedback loop, indicating it can iteratively refine its understanding of the dialogue state. The updated dialogue state is then passed to the Dialog Policy module, which decides the system's next action or 'dialogue act'. In this example, the policy output is "{ act: confirm from: downtown }". This dialogue act is sent to the Natural Language Generation (NLG) module, which converts the abstract policy into a natural language phrase understandable by the user. The NLG generates the text "FROM DOWNTOWN, IS THAT RIGHT?". Finally, this generated text is fed into the Text to Speech (TTS) module, which converts it back into an audio waveform. This waveform is then played back to the user through their phone, completing one turn of the dialogue. The entire system demonstrates how a task-oriented dialogue system processes user input, tracks dialogue progress, makes decisions, and generates responses to guide the conversation towards task completion, explicitly managing uncertainty at each step.](images/ec69c0acd9fce58d5c7c2775b840a49729a6e1d96902a1e9fc2a3b04b10a51dd.jpg)
Figure 15.3 Architecture of a dialogue-state system for task-oriented dialogue from Williams et al. (2016).

# 15.2.1 Frames and Slot Filling

The frame and its slots in a task-based dialogue system specify what the system needs to know to perform its task. A hotel reservation system needs dates and locations. An alarm clock system needs a time. The system’s goal is to fill the slots in the frame with the fillers the user intends, and then perform the relevant action for the user (answering a question, or booking a flight).

Fig. 15.4 shows a sample frame for booking air travel, with some sample questions used for filling slots. In the simplest frame-based systems (including most commercial assistants until quite recently), these questions are pre-written templates, but in more sophisticated systems, questions are generated on-the-fly. The slot fillers are often constrained to a particular semantic type, like type CITY (taking on values like San Francisco, or Hong Kong) or DATE, AIRLINE, or TIME.

<table><tr><td>Slot</td><td>Type Example Question</td></tr><tr><td>ORIGIN CITY city</td><td>“From what city are you leaving?”</td></tr><tr><td>DESTINATION CITY city</td><td>&quot;Where are you going?”</td></tr><tr><td>DEPARTURE TIME time</td><td>“When would you like to leave?”</td></tr><tr><td>DEPARTUREDATE date</td><td>&quot;What day would you like to leave?”</td></tr><tr><td>ARRIVAL TIME time</td><td>&quot;When do you want to arrive?”</td></tr><tr><td>ARRIVAL DATE date</td><td>“What day would you like to arrive?”</td></tr></table>

Figure 15.4 A frame in a frame-based dialogue system, showing the type of each slot and a sample question used to fill the slot.

Many domains require multiple frames. Besides frames for car or hotel reservations, we might need other frames for things like general route information (for questions like Which airlines fly from Boston to San Francisco?), That means the system must be able to disambiguate which slot of which frame a given input is supposed to fill.

The task of slot-filling is usually combined with two other tasks, to extract 3 things from each user utterance. The first is domain classification: is this user for example talking about airlines, programming an alarm clock, or dealing with their calendar? The second is user intent determination: what general task or goal is the user trying to accomplish? For example the task could be to Find a Movie, or Show a Flight, or Remove a Calendar Appointment. Together, the domain classification and intent determination tasks decide which frame we are filling. Finally, we need to do slot filling itself: extract the particular slots and fillers that the user intends the system to understand from their utterance with respect to their intent. From a user utterance like this one:

slot filling

Show me morning flights from Boston to San Francisco on Tuesday a system might want to build a representation like:

DOMAIN: AIR-TRAVEL INTENT: SHOW-FLIGHTS ORIGIN-CITY: Boston DEST-CITY: San Francisco ORIGIN-DATE: Tuesday ORIGIN-TIME: morning

Similarly an utterance like this:

should give an intent like this:

Wake me tomorrow at 6

DOMAIN: ALARM-CLOCK INTENT: SET-ALARM TIME: 2017-07-01 0600

The simplest dialogue systems use handwritten rules for slot-filling, like this regular expression for recognizing the SET-ALARM intent:

wake me (up) | set (the|an) alarm | get me up

But most systems use supervised machine-learning: each sentence in a training set is annotated with slots, domain, and intent, and a sequence model maps from input words to slot fillers, domain and intent. For example we’ll have pairs of sentences that are labeled for domain (AIRLINE) and intent (SHOWFLIGHT), and are also labeled with BIO representations for the slots and fillers. (Recall from Chapter 17 that in BIO tagging we introduce a tag for the beginning (B) and inside (I) of each slot label, and one for tokens outside (O) any slot label.)

O O O O O B-DES I-DES O B-DEPTIME I-DEPTIME O AIRLINE-SHOWFLIGHT I want to fly to San Francisco on Monday afternoon please EOS

Fig. 15.5 shows a typical architecture for inference. The input words $w _ { 1 } . . . w _ { n }$ are passed through a pretrained language model encoder, followed by a feedforward layer and a softmax at each token position over possible BIO tags, with the output a series of BIO tags $s _ { 1 } . . . s _ { n }$ . We generally combine the domain-classification and intent-extraction tasks with slot-filling by adding a domain concatenated with an intent as the desired output for the final EOS token.

Once the sequence labeler has tagged the user utterance, a filler string can be extracted for each slot from the tags (e.g., “San Francisco”), and these word strings can then be normalized to the correct form in the ontology (perhaps the airport code ‘SFO’), for example with dictionaries that specify that SF, SFO, and San Francisco are synonyms. Often in industrial contexts, combinations of rules and machine learning are used for each of these components.

![## Image Analysis: c20f59012f11cf8bfb27fea531d4edc1b0ea76803af03a4bd0c9a9c599fdc713.jpg

**Conceptual Understanding:**
This image represents a deep learning architecture, specifically a neural network, designed for the Natural Language Understanding (NLU) task of slot filling and intent classification. The main purpose is to illustrate the step-by-step process of transforming raw input words into structured semantic information. It conveys how an input utterance, such as a request to find something 'in San Francisco on Monday', is analyzed to extract key pieces of information (slots like 'location' and 'date') and determine the overall 'domain' and 'intent' of the user's request. The conceptual flow starts with encoding the input, followed by classification to assign specific tags that identify and categorize these semantic slots.

**Content Interpretation:**
The image depicts a neural network model designed for slot filling and potentially intent classification, common tasks in Natural Language Understanding (NLU). It illustrates a sequential processing flow: input words are first embedded and contextualized by an 'Encoder'. These 'Encodings' are then processed through subsequent layers, ultimately leading to a 'Classifier +softmax' layer. This layer predicts a BIO tag for each input word, categorizing parts of the utterance into semantic 'slots'. For example, 'San Francisco' is identified as a 'DESCRIPTION' (B-DES, I-DES), 'on' is an 'O' (Outside), and 'Monday' is a 'DTIME' (Date/Time). The final output 'd+i' signifies the extraction of both the domain and the user's intent from the input sentence. The sequence of operations from raw text to structured semantic tags and a combined domain/intent output clearly demonstrates an end-to-end NLU pipeline.

**Key Insights:**
The image demonstrates several key concepts in NLU: 1. **Encoder-Decoder Architecture (implicitly):** The use of an 'Encoder' highlights the importance of generating rich, contextualized representations ('Encodings') of input words. 2. **Sequence Labeling with BIO Tags:** It clearly shows how BIO tagging is used to delineate semantic slots within an utterance, with 'B-' indicating the beginning of a slot, 'I-' indicating words inside a slot, and 'O' indicating words outside any slot. Specific tags like 'B-DES' (Beginning-Description), 'I-DES' (Inside-Description), and 'B-DTIME' (Beginning-DateTime) are exemplified. 3. **Joint Modeling of Slot Filling and Intent Classification:** The final output 'd+i' (domain + intent) indicates that this model can perform both slot filling and intent classification simultaneously, suggesting a multi-task learning approach for a more comprehensive understanding of user utterances. 4. **Probabilistic Classification:** The 'Classifier +softmax' with bar chart representations indicates that the model outputs a probability distribution over possible tags for each token, allowing for confidence scores.

**Document Context:**
This image directly illustrates the mechanism of 'Slot filling' as discussed in Section 15.2.1 'Frames and Slot Filling'. It provides a visual explanation of how input words are processed through an encoder and then a classifier with a softmax function to generate BIO tags. The 'Text after image' explicitly states: "Figure 15.5 Slot filling by passing input words through an encoder, and then using a linear or feedforward layer followed by a softmax to generate a series of BIO tags. Here we also show a final state: a domain concatenated with an intent." The image perfectly aligns with and elaborates on this textual description, showing the architectural components and the specific outputs (BIO tags and d+i) involved in the slot filling process.

**Summary:**
This image illustrates a neural network architecture for slot filling, a process crucial in natural language understanding. It begins with an input sequence of words, exemplified by "... San Francisco on Monday <EOS>", which are fed into an 'Encoder'. The Encoder processes these words and generates 'Encodings', represented by a series of red circular nodes. These Encoded representations are then passed through a subsequent layer (light blue rectangles), which in turn feeds into a 'Classifier +softmax' layer. This classifier generates a probability distribution for each word, visually represented by bar charts within rounded white rectangles. Finally, the classifier outputs a sequence of BIO (Beginning, Inside, Outside) tags for each input word: "B-DES" for "San", "I-DES" for "Francisco", "O" for "on", and "B-DTIME" for "Monday". The last output is "d+i", representing a concatenated domain and intent, derived from the entire input sequence, following the end-of-sequence token "<EOS>". This entire process demonstrates how an input utterance is transformed into structured semantic information, specifically identifying slots (like description and date/time) and the overall intent and domain.](images/c20f59012f11cf8bfb27fea531d4edc1b0ea76803af03a4bd0c9a9c599fdc713.jpg)
Figure 15.5 Slot filling by passing input words through an encoder, and then using a linear or feedforward layer followed by a softmax to generate a series of BIO tags. Here we also show a final state: a domain concatenated with an intent.

We can make a very simple frame-based dialogue system by wrapping a small amount of code around this slot extractor. Mainly we just need to ask the user questions until all the slots are full, do a database query, then report back to the user, using hand-built templates for generating sentences.

# 15.2.2 Evaluating Task-Based Dialogue

task error rate slot error rate

We evaluate task-based systems by computing the task error rate, or task success rate: the percentage of times the system booked the right plane flight, or put the right event on the calendar. A more fine-grained, but less extrinsic metric is the slot error rate, the percentage of slots filled with the correct values:

For example a system that extracted the slot structure below from this sentence: (15.2) Make an appointment with Chris at 10:30 in Gates 104

<table><tr><td>Slot</td><td>Filler</td></tr><tr><td>PERSON</td><td>Chris</td></tr><tr><td>TIME</td><td>11:30 a.m.</td></tr><tr><td>ROOM</td><td>Gates 104</td></tr></table>

efficiency costs

has a slot error rate of 1/3, since the TIME is wrong. Instead of error rate, slot precision, recall, and F-score can also be used. We can also measure efficiency costs like the length of the dialogue in seconds or turns.

# 15.3 Dialogue Acts and Dialogue State

While the naive slot-extractor system described above can handle simple dialogues, often we want more complex interactions. For example, we might want to confirm that we’ve understand the user, or ask them to repeat themselves. We can build a more sophisticated system using dialogue acts and dialogue state.

# 15.3.1 Dialogue Acts

# dialogue acts

Dialogue acts are a generalization of speech acts that also represent grounding. The set of acts can be general, or can be designed for particular dialogue tasks.

<table><tr><td>Tag</td><td> Sys User Description</td></tr><tr><td>HELLO(a = x,b = y,..) √</td><td>Open a dialogue and give info a = x,b = y,.</td></tr><tr><td>INFORM(a = x,b = y, ..) √ √</td><td>√ Give info a= x,b=y,..</td></tr><tr><td>REQUEST(a,b = x,.) √</td><td>√ Request value for a given b = x,..</td></tr><tr><td>REQALTS(a = x,..) X</td><td>√ Request alternative with a = x,...</td></tr><tr><td>CONFIRM(a = x,b = y, .) √</td><td>√ Explicitly confirm a = x,b = y,..</td></tr><tr><td>CONFREQ(a = x.,..., d)</td><td>X Implicitly confirm a = x,... and request value of d</td></tr><tr><td>SELECT(a = x,a = y) X</td><td> Implicitly confirm a = x,... and request value of d</td></tr><tr><td>AFFIRM(a = x,b =y, .) √</td><td>√ Affirm and give further info a = x,b = y,..</td></tr><tr><td>NEGATE(a = x) X √</td><td>Negate and give corrected value a = x</td></tr><tr><td>DENY(a = x) x</td><td>√ Deny that a = x</td></tr><tr><td>BYE() √</td><td>√ Close a dialogue</td></tr></table>

Figure 15.6 Dialogue acts used by the HIS restaurant recommendation system of Young et al. (2010). The Sys and User columns indicate which acts are valid as system outputs and user inputs, respectively.

Figure 15.6 shows a tagset for a restaurant recommendation system, and Fig. 15.7 shows these tags labeling a sample dialogue from the HIS system (Young et al., 2010). This example also shows the content of each dialogue act, which are the slot fillers being communicated. So the user might INFORM the system that they want Italian food near a museum, or CONFIRM with the system that the price is reasonable.

<table><tr><td>Utterance</td><td></td><td>Dialogue act</td></tr><tr><td></td><td>U: Hi, I am looking for somewhere to eat.</td><td>hello(task = find,type=restaurant)</td></tr><tr><td></td><td>S: You are looking for a restaurant. What type of food do you like?</td><td> confreq(type = restaurant， food)</td></tr><tr><td></td><td> U: I&#x27;d like an Italian near the museum.</td><td> inform(food = Italian, near=museum)</td></tr><tr><td></td><td> S: Roma is a nice Italian restaurant near </td><td>inform(name = &quot;Roma&quot;， type = restaurant, food = Italian, near = museum)</td></tr><tr><td></td><td>the museum. U: Is it reasonably priced?</td><td>confirm(pricerange = moderate)</td></tr><tr><td></td><td></td><td> S: Yes, Roma is in the moderate price affirm(name = &quot;Roma&quot;， pricerange =</td></tr><tr><td></td><td>range. U: What is the phone number?</td><td>moderate) request(phone)</td></tr><tr><td></td><td> S: The number of Roma is 385456.</td><td> inform(name = &quot;Roma&quot;， phone = &quot;385456&quot;)</td></tr><tr><td></td><td>U: Ok, thank you goodbye.</td><td>bye()</td></tr></table>

Figure 15.7 A dialogue from the HIS System of Young et al. (2010) using the dialogue acts in Fig. 15.6.

# 15.3.2 Dialogue State Tracking

The job of the dialogue-state tracker is to determine the current state of the frame (the fillers of each slot), and the user’s most recent dialogue act. The dialogue-state is not just the slot-fillers in the current sentence; it includes the entire state of the frame at this point, summarizing all of the user’s constraints. Fig. 15.8 from Mrksiˇ c´ et al. (2017) shows the dialogue state after each turn.

Dialogue act detection is done just like domain or intent classification, by passing the input sentence through an encoder and adding an act classifier. Often passing in the prior dialogue act as well can improve classification. And since dialogue acts place some constraints on the slots and values, the tasks of dialogue-act detection and slot-filling are often performed jointly. The state tracker can just take the output of a slot-filling sequence-model (Section 15.2.1) after each sentence, or do something more complicated like training a classifier to decide if a value has been changed.

<table><tr><td>USer：</td><td> I&#x27;m looking for a cheaper restaurant</td></tr><tr><td>System: Sure. What kind - and where? USer：</td><td>inform(price=cheap)</td></tr><tr><td></td><td>Thai food, somewhere downtown</td></tr><tr><td></td><td>inform(price=cheap， food=Thai， area=centre)</td></tr><tr><td></td><td>System: The House serves cheap Thai food</td></tr><tr><td>User：</td><td>Where is it?</td></tr><tr><td> System: The House is at 1O6 Regent Street</td><td> inform(price=cheap， food=Thai， area=centre); request(address)</td></tr></table>

Figure 15.8 The output of the dialogue state tracker after each turn (Mrksiˇ c et al. ´ , 2017).

# user correction acts

hyperarticulation

A special case: detecting correction acts. If a dialogue system misrecognizes or misunderstands an utterance, users will repeat or reformulate the utterance. Detecting these user correction acts is quite important, especially for spoken language. Ironically, corrections are actually harder to recognize than normal sentences (Swerts et al., 2000), because users who are frustrated adjust their speech in a way that is difficult for speech recognizers (Goldberg et al., 2003). For example speakers often use a prosodic style for corrections called hyperarticulation, in which the utterance is louder or longer or exaggerated in pitch, such as I said BAL-TI-MORE, not Boston (Wade et al. 1992, Levow 1998, Hirschberg et al. 2001). Detecting acts can be part of the general dialogue act detection classifier, or can make use of special features beyond the words, like those shown below (Levow 1998, Litman et al. 1999, Hirschberg et al. 2001, Bulyko et al. 2005, Awadallah et al. 2015).

<table><tr><td>features semantic</td><td>examples embedding similarity between correction and user&#x27;s prior utterance</td></tr><tr><td>phonetic</td><td>phonetic overlap between candidate correction act and user&#x27;s prior utterance (i.e. &quot;WhatsApp&quot; may be incorrectly recognized as &quot;What&#x27;s up&quot;)</td></tr><tr><td>prosodic</td><td> hyperarticulation, increases in FO range, pause duration, and word duration</td></tr><tr><td>ASR</td><td>ASR confidence, language model probability</td></tr></table>

dialogue policy

# 15.3.3 Dialogue Policy: Which act to generate

In early commercial frame-based systems, the dialogue policy is simple: ask questions until all the slots are full, do a database query, then report back to the user. A more sophisticated dialogue policy can help a system decide when to answer the user’s questions, when to instead ask the user a clarification question, and so on. A dialogue policy thus decides what dialogue act to generate. Choosing a dialogue act to generate, along with its arguments, is sometimes called content planning.

Let’s see how to do this for some important dialogue acts. Dialogue systems, especially speech systems, often misrecognize the users’ words or meaning. To ensure system and user share a common ground, systems must confirm understandings with the user or reject utterances that the system don’t understand. A system might use an explicit confirmation act to confirm with the user, like Is that correct? below:

U: I’d like to fly from Denver Colorado to New York City on September twenty first in the morning on United Airlines   
S: Let’s see then. I have you going from Denver Colorado to New York on September twenty first. Is that correct?

When using an implicit confirmation act, a system instead grounds more implicitly, for example by repeating the system’s understanding as part of asking the next question, as Shanghai is confirmed in passing in this example:

# U: I want to travel to to Shanghai S: When do you want to travel to Shanghai?

rejection

There’s a tradeoff. Explicit confirmation makes it easier for users to correct misrecognitions by just answering “no” to the confirmation question. But explicit confirmation is time-consuming and awkward (Danieli and Gerbino 1995, Walker et al. 1998a). We also might want an act that expresses lack of understanding: rejection, for example with a prompt like I’m sorry, I didn’t understand that. To decide among these acts, we can make use of the fact that ASR systems often compute their confidence in their transcription (often based on the log-likelihood the system assigns the sentence). A system can thus choose to explicitly confirm only low-confidence sentences. Or systems might have a four-tiered level of confidence with three thresholds $\alpha , \beta$ , and $\gamma \mathrm { : }$

$< \alpha$ low confidence reject $\geq \alpha$ above the threshold confirm explicitly $\ge \beta$ high confidence confirm implictly $\geq \gamma$ very high confidence don’t confirm at all

# 15.3.4 Natural language generation: Sentence Realization

<table><tr><td>recommend(restaurant name= Au Midi， neighborhood = midtown, cuisine = french)</td></tr><tr><td> 1 Au Midi is in Midtown and serves French food.</td></tr><tr><td>2 There is a French restaurant in Midtown called Au Midi.</td></tr></table>

Figure 15.9 Sample inputs to the sentence realization phase of NLG, showing the dialogue act and attributes prespecified by the content planner, and two distinct potential output sentences to be generated. From the restaurant recommendation system of Nayak et al. (2017).

Once a dialogue act has been chosen, we need to generate the text of the response to the user. This part of the generation process is called sentence realization. Fig. 15.9 shows a sample input/output for the sentence realization phase. The content planner has chosen the dialogue act RECOMMEND and some slots (name, neighborhood, cuisine) and fillers. The sentence realizer generates a sentence like lines 1 or 2 (by training on examples of representation/sentence pairs from a corpus of labeled dialogues). Because we won’t see every restaurant or attribute in every possible wording, we can delexicalize: generalize the training examples by replacing specific slot value words in the training set with a generic placeholder token representing the slot. Fig. 15.10 shows the sentences in Fig. 15.9 delexicalized.

We can map from frames to delexicalized sentences with an encoder decoder model (Mrksiˇ c et al. ´ 2017, inter alia), trained on hand-labeled dialogue corpora like MultiWOZ (Budzianowski et al., 2018). The input to the encoder is a sequence of

<table><tr><td>recommend(restaurant name= Au Midi， neighborhood = midtown, cuisine = french)</td></tr><tr><td>1 restaurant name is in neighborhood and serves cuisine food.</td></tr><tr><td>2 There is a cuisine restaurant in neighborhood called restaurant_name.</td></tr></table>

Figure 15.10 Delexicalized sentences that can be used for generating many different relexicalized sentences. From the restaurant recommendation system of Nayak et al. (2017).

![## Image Analysis: 9ac7d1148155a0bc86282494408c30ecf09a3129fac41313fa18fcff5814e0ec.jpg

**Conceptual Understanding:**
This image conceptually represents a natural language generation (NLG) system using an encoder-decoder neural network architecture. Its main purpose is to illustrate the process of 'sentence realization', where a structured, semantic input (composed of slots and fillers) is transformed into a coherent, grammatically correct natural language sentence. The key idea being communicated is the end-to-end transformation of abstract semantic intent into human-readable text, demonstrating a fundamental mechanism in modern NLG.

**Content Interpretation:**
The image displays an encoder-decoder model, a common architecture in natural language processing. It shows the transformation of a structured semantic input, represented as 'slots and fillers', into a natural language English sentence. The structured input, 'RECOMMEND service: decent cuisine: null', consists of a recommendation intent, a 'service' slot with a 'decent' value, and a 'cuisine' slot with a 'null' value. This input is processed by the 'ENCODER', which compresses the semantic information into a fixed-size context vector. This context vector is then passed to the 'DECODER'. The DECODER takes this encoded information and generates the target English sentence, '[name] has decent service', word by word. The absence of 'cuisine' in the output sentence, corresponding to the 'cuisine: null' input, demonstrates the model's ability to selectively realize information based on the input's semantic content. The '[name]' placeholder suggests the system can integrate specific entities into the generated sentence.

**Key Insights:**
1. **Encoder-Decoder Architecture for NLG:** The image clearly demonstrates the utility of an encoder-decoder model for natural language generation tasks, specifically sentence realization, where a structured input is translated into a natural language output. This is evident from the 'ENCODER' and 'DECODER' blocks and their respective inputs and outputs. 2. **Slot-Filler to Sentence Mapping:** It highlights how semantic slot-filler representations like 'RECOMMEND service: decent cuisine: null' are the input for such systems, and how these are effectively mapped to grammatically correct and semantically appropriate English sentences, such as '[name] has decent service'. 3. **Handling Null Values:** The inclusion of 'cuisine: null' in the input and its subsequent omission in the generated sentence '[name] has decent service' illustrates that the model can interpret and handle 'null' semantic slots by not realizing them in the final output, thus ensuring conciseness and relevance.

**Document Context:**
This image directly illustrates the concepts discussed in Section 15.3.4, titled 'Natural language generation: Sentence Realization'. It provides a concrete visual example of an encoder-decoder sentence realizer, which is a key method for mapping abstract semantic representations (slots/fillers) to natural language. The image's filename and the 'Text after image' confirmation ('Figure 15.11 An encoder decoder sentence realizer mapping slots/fillers to English') explicitly link it to the document's narrative on natural language generation techniques.

**Summary:**
The diagram illustrates an encoder-decoder architecture for natural language generation, specifically for the task of sentence realization. This process converts a structured semantic input into a natural language English sentence. The process begins with the structured input phrase: "RECOMMEND service: decent cuisine: null". This input, which defines semantic slots like 'service' with the value 'decent' and 'cuisine' with a 'null' value, is fed into the "ENCODER" component. The ENCODER processes this input, transforming it into an internal, condensed representation. This encoded information is then passed to the "DECODER" component via a connecting double-headed arrow. The DECODER then uses this encoded information to sequentially generate an English sentence. The output of the DECODER is the sentence: "[name] has decent service". The upward arrows pointing from the input text ("RECOMMEND", "service:", "decent", "cuisine: null") to the bottom of the ENCODER indicate that these semantic units are inputs. Similarly, the upward arrows pointing from the top of the DECODER to the output text ("[", "name", "]", "has", "decent", "service") indicate the sequential generation of the sentence words.](images/9ac7d1148155a0bc86282494408c30ecf09a3129fac41313fa18fcff5814e0ec.jpg)
Figure 15.11 An encoder decoder sentence realizer mapping slots/fillers to English.

tokens $x _ { t }$ that represent the dialogue act (e.g., RECOMMEND) and its arguments (e.g., service:decent, cuisine:null) (Nayak et al., 2017), as in Fig. 15.11.

The decoder outputs the delexicalized English sentence “name has decent service”, which we can then relexicalize, i.e. fill back in correct slot values, resulting in “Au Midi has decent service”.

# 15.4 Chatbots

# chatbot

Chatbots are systems that can carry on extended conversations with the goal of mimicking the unstructured conversations or ‘chats’ characteristic of informal humanhuman interaction. While early systems like ELIZA (Weizenbaum, 1966) or PARRY (Colby et al., 1971) had theoretical goals like testing theories of psychological counseling, for most of the last 50 years chatbots have been designed for entertainment. That changed with the recent rise of neural chatbots like ChatGPT, which incorporate solutions to NLP tasks like question answering, writing tools, or machine translation into a conversational interface. A conversation with ChatGPT is shown in Fig. 15.12. In this section we describe neural chatbot architectures and datasets.

[TBD] Figure 15.12 A conversation with ChatGPT.

# 15.4.1 Training chatbots

Data Chatbots are generally trained on a training set that includes standard large language model training data of the type discussed in Section 10.3.2: versions of the web from the Common Crawl, including news sites, Wikipedia, as well as books. For training chatbots, it is common to additionally add lots of dialogue data.

This can include datasets created specifically for training chatbots by hiring speakers of the language to have conversations, such as by having them take on personas or talk about knowledge provided to them. For example the Topical-Chat dataset has 11K crowdsourced conversations spanning 8 broad topics (Gopalakrishnan et al., 2019), the EMPATHETICDIALOGUES includes 25K crowdsourced conversations grounded in a specific situation where a speaker was feeling a specific emotion (Rashkin et al., 2019), and the SaFeRDialogues dataset (Ung et al., 2022) has 8k dialogues demonstrating graceful responses to conversational feedback about safety failures.

Such datasets are far too small to train a language model alone, and so it’s common to also pretrain on large datasets of pseudo-conversations drawn from Twitter (Ritter et al., 2010a), Reddit (Roller et al., 2021), Weibo ( 博), and other social 微media platforms. To turn social media data into data that has the structure of a conversation, we can treat any post on the platform as the first turn in a conversation, and the sequence of comments/replies as subsequent turns in that conversation.

Datasets from the web can be enormously toxic, so it’s crucial to filter the dialogues first. This can be done by using the same toxicity classifiers we describe below in the fine-tuning section.

Architecture For training chatbots, it’s most common to use the standard causal language model architecture, in which the model predicts each word given all the prior words, and the loss is the standard language modeling loss. Fig. 15.13 shows a standard training setup; no different than language model training in Chapter 9. The only difference is the data, which has the addition of significant conversation and pseudo-conversation data as described in the prior section. As usual, the left context can include the entire prior conversation (or as much as fits in the context window).

![## Image Analysis: af054c4bcf929ee977e13b62d8498f88bd29608ca57a6a3e5ee18859d4405cb3.jpg

**Conceptual Understanding:**
The image conceptually represents the training architecture and data flow for a causal (decoder-only) language model, specifically employing a Transformer network. Its main purpose is to illustrate how such a model learns to predict the next word in a sequence based on the preceding context by minimizing a negative log-likelihood loss function. It demonstrates the internal workings of a system that powers text generation in applications like chatbots, showing the process of learning sequential dependencies in language.

**Content Interpretation:**
The image shows the architecture and training process of a causal (decoder-only) language model based on the Transformer architecture. This model is designed to predict the next word in a sequence given all preceding words.

**Processes Shown:**
*   **Sequential Input Processing:** Words from an input sequence ("I got promoted ! <s > Congrats !") are fed into the model.
*   **Contextual Encoding via Transformer Blocks:** The "Transformer Blocks" (labeled as such) process these input words. Within these blocks, the model learns contextual representations where each word's representation is influenced by all previous words in the sequence, demonstrating a causal masking mechanism (as indicated by the specific arrow connections).
*   **Next Word Prediction:** The output from the top layer of the Transformer Blocks for each position is passed to a dedicated "LM head" (Language Model head), which is responsible for generating a probability distribution over the vocabulary for the *next* word.
*   **Loss Calculation:** A "LM Loss" is computed for each prediction. Specifically, the negative log-likelihood of the actual next word (the target word) is calculated. For instance, for the input "I", the target is "got", and the loss is "-log y_got".

**Concepts Illustrated:**
*   **Causal Language Modeling:** The model explicitly learns to predict the next token based only on past tokens, a core concept for text generation.
*   **Transformer Architecture:** The core computational units are "Transformer Blocks", which are fundamental components of large language models.
*   **Negative Log-Likelihood (NLL) Loss:** The explicit use of "-log y_" expressions signifies the standard NLL loss, commonly used in classification tasks like predicting the next word in a vocabulary.

**Relationships Depicted:**
*   **Input-Output Flow:** There is a clear hierarchical flow from input words at the bottom, through the Transformer Blocks, up to the LM heads, and finally to the loss calculation for the predicted next words at the top.
*   **Causal Dependency:** The specific arrow connections within the Transformer Blocks demonstrate that processing at a given word position is causally dependent only on the current and preceding words in the sequence. Each LM head at a given position predicts the word *following* the word it received its input from.

**Key Insights:**
**Main Takeaways and Insights:**
*   **Causal Language Model Training:** The image provides a clear, step-by-step visual of how a causal language model is trained to predict subsequent words in a sequence, forming the basis for generative AI applications like chatbots.
*   **Transformer Architecture Role:** It highlights that "Transformer Blocks" are the central computational units that process input and learn contextual relationships between words. The layered structure and internal connections underscore the complexity and depth of this architecture.
*   **Importance of Causal Masking:** The arrow connections within the "Transformer Blocks" demonstrate the causal attention mechanism, ensuring that predictions for a given word are only based on preceding context, which is critical for sequential text generation.
*   **Negative Log-Likelihood as a Training Objective:** The use of "-log y_targetWord" for "LM Loss" explicitly shows that the model is trained to maximize the likelihood of the true next word, a standard and effective objective for language modeling.
*   **End-to-End Prediction Process:** The diagram illustrates the entire flow, from raw input tokens to the calculation of loss for each predicted next word, providing a comprehensive understanding of how such models learn to generate text one token at a time.

**Document Context:**
This image directly illustrates the training mechanism of a causal language model, which is a core component for developing chatbots capable of generating coherent and contextually relevant text. Given that the document context is "Section: 15.4.1 Training chatbots" and the text after the image is "Figure 15.13 Training a causal (decoder-only) language model for a chatbot.", the image serves as a detailed visual explanation of the theoretical and computational process described in the surrounding text. It shows *how* these models learn to predict the next word, which is fundamental to their ability to produce conversational responses.

**Summary:**
This detailed diagram illustrates the training process of a causal (decoder-only) language model, which is a type of neural network often used for chatbots to generate text. The process begins with an input sequence of words, such as "I got promoted ! <s > Congrats !".

1.  **Input Words (Bottom Row):** The individual words of the input sequence, "I", "got", "promoted", "!", "<s >", "Congrats", "!", are fed into the model. These are typically first converted into numerical embeddings (not explicitly shown but implied).
2.  **Transformer Blocks:** These embeddings enter a stack of "Transformer Blocks" (the large blue rounded rectangle labeled "Transformer Blocks" on the left side). Within these blocks, the model processes the words to understand their context. The purple stacked rectangles represent individual sub-layers or computations within these blocks.
    *   **Causal Attention:** The arrows connecting the input words to the first layer of Transformer Blocks, and then between layers, demonstrate a crucial aspect: each word's representation at a given position can only "attend to" or be influenced by itself and all preceding words in the sequence. For example, when processing "promoted", the model only considers "I", "got", and "promoted" itself, but not subsequent words like "!" or "Congrats". The horizontal and vertical ellipses "..." indicate that there are multiple layers of these blocks and the sequence can be longer than shown.
3.  **LM Head:** After processing through all the "Transformer Blocks", the output representation for each word position is passed to a dedicated "LM head" (Language Model head), represented by the blue oval boxes, labeled "LM head" inside. Each "LM head" is responsible for predicting the *next* word in the sequence.
4.  **LM Loss:** For each prediction made by an "LM head", a "LM Loss" is calculated. This is specifically shown as the negative log-likelihood of the *actual* next word, labeled "LM Loss" on the left. The loss values are presented in red rectangular boxes.
    *   For the input "I", the model tries to predict "got", and the loss is "-log y_got".
    *   For the input "got", it tries to predict "promoted", and the loss is "-log y_promoted".
    *   This continues for each position: for "promoted", the target is "!", leading to "-log y_!"; for "!", the target is "<s >", leading to "-log y_<s >"; for "<s >", the target is "Congrats", leading to "-log y_Congrats"; and for "Congrats", the target is "!", leading to "-log y_!".
    *   The target words are explicitly listed in the "Next word" row at the very top, labeled "Next word" on the left.
5.  **Training (Implicit):** The calculated "LM Loss" across all positions is then used to adjust the internal parameters (weights) of the Transformer Blocks and LM heads through a process called backpropagation, teaching the model to make more accurate next-word predictions in the future.

This entire process enables the language model to learn the probabilities of word sequences, which is fundamental for generating human-like text in chatbot applications.](images/af054c4bcf929ee977e13b62d8498f88bd29608ca57a6a3e5ee18859d4405cb3.jpg)
Figure 15.13 Training a causal (decoder-only) language model for a chatbot.

An alternative is to use the encoder-decoder architecture of Chapter 13. In this case the entire conversation up to the last turn (as much as fits in the context) is presented to the encoder, and the decoder generates the next turn.

![## Image Analysis: be4d9a5ff3c2c0717c971fc37983942080ddf9b54929fd723f9035859579dd59.jpg

**Conceptual Understanding:**
This image conceptually represents an **encoder-decoder language model**, a foundational architecture in natural language processing (NLP).

The **main purpose** of the image is to illustrate the sequential process by which a textual input is transformed into a textual output, specifically demonstrating how a chatbot might generate a response to a user's utterance. It visually explains the flow of information through an encoding stage and a subsequent decoding stage.

**Key ideas and concepts being communicated include:**
*   **Sequence-to-Sequence Modeling:** The core concept of transforming one sequence of tokens (words/symbols) into another sequence.
*   **Neural Machine Translation (NMT) / Chatbot Architecture:** How these models are structured to handle input-output pairs in conversational AI or translation tasks.
*   **Encoding:** The process of converting an input sequence into a meaningful, often fixed-size, numerical representation (context vector).
*   **Decoding:** The process of generating an output sequence token by token, based on the encoded representation from the input.
*   **Tokenization:** The implicit idea that sentences are broken down into individual units (words, punctuation, special symbols like `<s\>`) for processing.

**Content Interpretation:**
The image displays a simplified **encoder-decoder language model** architecture, which is a key system in natural language processing (NLP) for sequence-to-sequence tasks, particularly for chatbot functionality. It depicts the flow of information where a user's input phrase is transformed into an appropriate conversational response.

**Processes, Concepts, and Systems Shown:**
*   **Encoder-Decoder Architecture:** The core system is composed of two main components: an "ENCODER" and a "DECODER", clearly labeled in distinct rounded rectangular shapes. This architecture is fundamental to many modern AI systems dealing with sequences.
*   **Input Sequence Processing:** The ENCODER receives a multi-token input sequence: "I", "got", "promoted", "!", and "<s\>". This represents a complete sentence or utterance, with "<s\>" likely signifying a special token like 

**Key Insights:**
**Main Takeaway 1: Encoder-Decoder models are designed for sequence-to-sequence transformations in natural language processing.**
*   **Evidence:** The diagram explicitly shows an input sequence ("I", "got", "promoted", "!", "<s\>") being processed by an "ENCODER" and then the "DECODER" generating a distinct output sequence ("Congrats", "!"). This flow is the essence of sequence-to-sequence transformation.

**Main Takeaway 2: This architecture is a suitable and common approach for building conversational AI agents like chatbots.**
*   **Evidence:** The specific example used—an input conversational statement ("I got promoted!") leading to a contextually appropriate conversational response ("Congrats!")—directly illustrates its application in dialogue systems. The document context referring to "Training chatbots" further reinforces this insight.

**Main Takeaway 3: Textual inputs and outputs in these models are typically tokenized, meaning they are broken down into individual words or sub-word units and special symbols.**
*   **Evidence:** The inputs to the ENCODER are distinctly separated as "I", "got", "promoted", "!", and "<s\>". Similarly, the outputs from the DECODER are "Congrats" and "!". This granular representation highlights the token-based processing inherent in such models, including the use of special tokens like "<s\>" for sequence boundaries or other purposes.

**Main Takeaway 4: The ENCODER's primary role is to comprehend and encapsulate the meaning of the input, which is then passed to the DECODER to generate the response.**
*   **Evidence:** The direct arrow connection from the "ENCODER" to the "DECODER" signifies the transfer of the processed input's information. The ENCODER takes the initial sentence, and the DECODER then uses that encoded meaning to construct the relevant reply, demonstrating a clear division of labor in understanding versus generation.

**Document Context:**
This image directly supports the document's discussion in "Section: 15.4.1 Training chatbots" by visually presenting an "alternative" method for chatbot implementation, as highlighted by the text after the image, "Figure 15.14 An alternative: an encoder-decoder language model for a chatbot." It provides a clear, conceptual diagram of how such a model processes a user's input to generate a conversational response, thereby enhancing the reader's understanding of chatbot mechanisms in the context of language models.

**Summary:**
This diagram illustrates the fundamental architecture of an **encoder-decoder language model** as an alternative method for training chatbots. The process begins with an **input sequence**, which in this example is the phrase "I got promoted !" along with a special token "<s\>". Each part of this input, specifically "I", "got", "promoted", "!", and "<s\>", is fed sequentially into the **ENCODER** component.

The **ENCODER** processes this entire input sequence to generate a compressed, fixed-size representation of the input's meaning (often referred to as a context vector or hidden state). This encoded representation is then passed as input to the **DECODER** component.

The **DECODER** takes the information provided by the ENCODER and uses it to generate an **output sequence**. In this specific example, the DECODER produces the response "Congrats !". Each part of the output, "Congrats" and "!", is generated sequentially.

In essence, the image demonstrates a system where a conversational input (a user saying "I got promoted!") is understood by the ENCODER, and then a relevant, conversational output (the chatbot replying "Congrats!") is generated by the DECODER, showcasing a complete sequence-to-sequence transformation for chatbot interaction.](images/be4d9a5ff3c2c0717c971fc37983942080ddf9b54929fd723f9035859579dd59.jpg)
Figure 15.14 An alternative: an encoder-decoder language model for a chatbot.

In practice, dialogue systems require additional customization beyond just pretraining on dialogue data. In the next few sections we’ll discuss various stages of fine-tuning that can be used for this customization.

# 15.4.2 Fine Tuning for Quality and Safety

It is a common practice for dialogue systems to use further labeled data for finetuning. One function of this fine-tuning step is to improve the quality of the dialogue, training the system to produce responses that are sensible and interesting. Another function might be to improve safety, keeping a dialogue system from suggesting harmful actions (like financial fraud, medical harm, inciting hatred, or abusing the user or other people).

In the simplest method for improving quality and safety, speakers of the language are given an initial prompt and instructions to have high-quality, safe dialogues. They then interact with an initial dialogue system and their responses are used to finetune the model, usually as part of the instruct tuning step we introduced in Chapter 12. Thus a dialogue system learns to answer questions, follow other instructions, and also carry on high-quality, safe dialogues, in a single multi-task learning format.

While fine-tuning on positive examples is helpful, it is generally insufficient and so it is common to add more discriminative data that specifically downweights lowquality or harmful responses. The simplest paradigm for this is to train a model to predict turn-level safety and quality values, by training on human-labeled ratings. Such ratings might be collected by first having speakers of the language carry on dialogues with a system, and then a second set of people act as labelers to label every system turn for its quality and safety, resulting in a binary label for quality and safety for each turn.

Once a dataset has been created with these labels, a language model can be used in a classification task to label the quality and safety of a turn. For example in the Lamda system (Cohen et al., 2022), a single language model is used in two phases, roughly corresponding to generative and discriminative tasks: first generating a response, and then generating a label. In the generative phase, the model is given the prior turn and a special RESPONSE token and generates the blue response turn. (In training, the training loss is given only for the blue response):

“What’s up? RESPONSE Not much.

In a second, discriminative phase, the model is fine-tuned to see an attribute (SENSIBLE, INTERESTING, UNSAFE) and then to predict a 0 or 1 value, again with training losses given only for the blue value.

What’s up? RESPONSE Not much. SENSIBLE 1   
What’s up? RESPONSE Not much. INTERESTING 0   
What’s up? RESPONSE Not much. UNSAFE 0

To use the system in inference, the model first generates a response given the context, and then it is given the attribute and asked to generate a rating. The result is a generated turn along with a label. This label isn’t shown to the user but can be use for filtering, either at training time or at deployment time. For example, the system can generate multiple potential responses, filter out any response that is unsafe, and return to the user the highest ranking response.

# 15.4.3 Learning to perform retrieval as part of responding

Modern chatbots are also trained to do retrieval (calls to Google or Bing or other search engines) as a component of generating a response. This is generally done by having a sort of fake dialogue participant called something like Search Query whose utterances are passed to a search engine. The system can be encourged to do searches either by prompting or by fine-tuning.

For example in the Sparrow chatbot (Glaese et al., 2022), the prompt includes a few sample conversations that include two special users, Search Query, and Search Results:

<table><tr><td>User： Sparrow: User: Search Query:</td><td>...[this is preceded by other parts of the prompt] What color is the sky? Blue. Why? why is sky blue</td></tr><tr><td></td><td>Search Results: Page title: Sky Except for direct sunlight, most of the light in the daytime sky is caused by scattering, which is dominated by a small-particle limit called Rayleigh scattering. The scattering due to molecule-sized particles (as in air) is greater in the di- rections both toward and away from the source of light than it is in directions perpendicular to the incident path. Scattering is significant for light at all visible wavelengths, but is stronger at the shorter (bluer) end of the visible spectrum, meaning that the scattered light is bluer than its source: the Sun. The remain-</td></tr><tr><td>Sparrow:</td><td>ing direct sunlight, having lost some of its shorter-wavelength components, appears slightly less blue. Because of the Rayleigh scattering. It causes short waves of blue light to be scattered more than the other colours in the spec-</td></tr><tr><td>User:</td><td>trum. Who was Raleigh?</td></tr><tr><td>Search Query: who was Rayleigh</td><td></td></tr></table>

From these prompts, the system learns to generate texts with Search Query turns for fact-based questions from the user, and these are passed to a search engine to generate the Search Results turns.

Alternatively, systems can be finetuned to to know when to use a search engine. For example, labelers can interact with a system, fact check each of the responses, and whenever the system emits an incorrect response, perform the web search queries that the system should have used to check its answer, and then the interation is recorded and used for fine-tuning. Or labelers can look at a transcript of a language model carrying on a dialogue, and similarly mark every place where a fact was wrong (or out-of-date) and write the set of search queries that would have been appropriate. A system is then fine-tuned to generate search query turns which are again passed to a search engine to generate the search responses. The set of pages or snippets returned by the search engine in the search response turn are then treated as the context for generation, similarly to the retrieval-based questionanswering methods of Chapter 14.

# 15.4.4 RLHF

A more sophisticated family of methods uses reinforcement learning to learn to match human preferences for generated turns. In this method, RLHF for Reinforcement Learning from Human Feedback, we give a system a dialogue context and sample two possible turns from the language model. We then have humans label which of the two is better, creating a large dataset of sentence pairs with human preferences. These pairs are used to train a dialogue policy, and reinforcement learning is used to train the language model to generate turns that have higher rewards (Christiano et al., 2017; Ouyang et al., 2022). While using RLHF is the current state of the art at the time of this writing, a number of alternatives have been recently developed that don’t require reinforcement learning (Rafailov et al., 2023, e.g.,) and so this aspect of the field is changing very quickly.

# 15.4.5 Evaluating Chatbots

Chatbots are evaluated by humans, who assign a score. This can be the human who talked to the chatbot (participant evaluation) or a third party who reads a transcript of a human/chatbot conversation (observer evaluation). In the participant evaluation of See et al. (2019), the human evaluator chats with the model for six turns and rates the chatbot on 8 dimensions capturing conversational quality: avoiding repetition, interestingness, making sense, fluency, listening, inquisitiveness, humanness and engagingness on Likert scales like these:

Engagingness How much did you enjoy talking to this user? Not at all  A little  Somewhat  A lot   
Making sense How often did this user say something which did NOT make sense? Never made any sense  Most responses didn’t make sense  Some responses didn’t make sense • Everything made perfect sense

Observer evaluations use third party annotators to look at the text of a complete conversation. Sometimes we’re interested in having raters assign a score to each system turn; for example (Artstein et al., 2009) have raters mark how coherent each turn is. Often, however, we just want a single high-level score to know if system A is better than system B. The acute-eval metric (Li et al., 2019a) is such an observer evaluation in which annotators look at two separate human-computer conversations and choose the system which performed better on four metrics: engagingness, interestingness, humanness, and knowledgability.

# 15.5 Dialogue System Design

# voice user interface

Because of the important role of the user, the field of dialogue systems is closely linked with Human-Computer Interaction (HCI). This is especially true for taskoriented dialogue and assistants, where the design of dialogue strategies, sometimes called voice user interface design, generally follows user-centered design principles (Gould and Lewis, 1985):

1. Study the user and task: Understand the users and the task by interviewing users, investigating similar systems, and studying related human-human dialogues.

2. Build simulations and prototypes: A crucial tool in building dialogue systems is the Wizard-of-Oz system. In wizard systems, the users interact with what they think is a program but is in fact a human “wizard” disguised by a software interface (Gould et al. 1983, Good et al. 1984, Fraser and Gilbert 1991). The name comes from the children’s book The Wizard of $O z$ (Baum, 1900), in which the wizard turned out to be a simulation controlled by a man behind a curtain or screen. A wizard system can be used to test out an architecture before implementation; only the interface software and databases need to be in place. The wizard gets input from the user, uses a database interface to run queries based on the user utterance, and then outputs sentences, either by typing them or speaking them.

Wizard-of-Oz systems are not a perfect simulation, since the wizard doesn’t exactly simulate the errors or limitations of a real system; but wizard studies can still provide a useful first idea of the domain issues.

![## Image Analysis: edf471d58c1c8e2a19f7f61f87228c32ff8a44b9fbd8c576b0bc684c5d80e9c7.jpg

**Conceptual Understanding:**
This image conceptually represents a pivotal meeting or encounter between several distinct characters within a fantastical narrative, specifically from L. Frank Baum's 'The Wonderful Wizard of Oz'. The main purpose of the image is to visually depict a specific scene where the protagonists confront or are introduced to the true form of a powerful, perhaps deceptive, figure, revealing a key moment of character interaction and plot development. The key ideas conveyed are surprise, revelation, the true nature of power (or lack thereof), and the assembly of a diverse group of companions. There is no process flow, swimlanes, boxes, decision diamonds, or any other textual elements associated with a process flow present in this image. No title, notes, arrow labels, timeline information, headers, footers, or any other explicit textual annotations or metadata are visible in the image. There is a small, stylized, dark mark in the bottom-left corner of the image, which appears to be an artist's signature or emblem, but it is not decipherable as text. As no process flow elements or textual instructions were found in the image, a systematic process mapping cannot be provided.

**Content Interpretation:**
The image shows a group of characters from 'The Wonderful Wizard of Oz' gathered in what appears to be a chamber or throne room. From left to right, the identifiable characters are: the Scarecrow (round head, jester-like costume, spectacles), Dorothy (young girl with pigtails), the Tin Woodman (metallic body, axe, funnel hat, spectacles), the Wizard (small, older man with a beard, holding a small object, standing in front of a folding screen), the Cowardly Lion (large, anthropomorphic lion, spectacles, seated), and Toto (small dog at the lion's feet). At the bottom-left, a witch's hat, a basket, and a small funnel are on the floor. The central interaction involves the protagonists confronting the small man, who is revealed from behind the screen, signifying a key narrative moment where the mysterious 'Oz' is unmasked. This is an illustrative image, so it does not present data or trends. Its significance lies in depicting a key plot twist where the powerful 'Oz' is revealed to be an ordinary man hiding behind a facade.

**Key Insights:**
The image powerfully conveys the theme of appearance versus reality, showing how a grand, intimidating illusion can be created by a seemingly insignificant individual. It also highlights a moment of truth and confrontation, where the characters discover the true nature of the figure they have sought. The primary insight is the demystification of a powerful figure, revealing that perceived power can often be a carefully constructed facade. It suggests a moment of emotional impact for the characters as they process this revelation. Given the absence of textual elements in the image, these insights are drawn entirely from visual analysis of the characters' expressions, their arrangement, and the context suggested by the narrative elements.

**Document Context:**
Given that the document section is identified as 'voice user interface,' this illustration does not appear to directly relate to the topic of voice user interfaces. It is an artistic depiction of a scene from a fantasy story, most notably 'The Wonderful Wizard of Oz,' where the protagonists discover the true identity of the Wizard behind a screen. It might be included in a document as an example of storytelling, character design, or perhaps even as an abstract metaphor, but its direct relevance to 'voice user interface' is not evident from the image itself.

**Summary:**
The image is a monochrome illustration depicting a scene from a classic fantasy tale. In the center, a small, balding man with a short beard and a seemingly benign expression stands in front of a tall, vertically striped folding screen, holding a small, round object in his cupped hands. To his left, from left to right, are three figures: a smiling, round-headed man wearing spectacles and a ruffled collar, dressed in what appears to be a jester-like or clown costume, carrying a staff over his left shoulder; a young girl with braided pigtails, wearing a dress, looking upwards and slightly to the right with an expression that suggests surprise or awe; and a metallic-bodied figure with visible joints, holding an axe, wearing a funnel-shaped hat, and also sporting spectacles, looking intently towards the central small man. To the right of the small man, a large, anthropomorphic lion with a full mane, wearing spectacles on his nose, sits upright, also gazing towards the central figure. A small, dark-colored dog with a wagging tail sits on the ground near the lion's feet, looking in the same direction. On the ground in the bottom-left corner of the image are several scattered objects: a pointed hat, a wicker basket with a handle, and a small funnel. The overall scene depicts a pivotal moment of confrontation or revelation, strongly suggesting the famous encounter where the protagonists from 'The Wonderful Wizard of Oz' discover the true identity of the Wizard behind his imposing illusions.](images/edf471d58c1c8e2a19f7f61f87228c32ff8a44b9fbd8c576b0bc684c5d80e9c7.jpg)

3. Iteratively test the design on users: An iterative design cycle with embedded user testing is essential in system design (Nielsen 1992, Cole et al. 1997, Yankelovich et al. 1995, Landauer 1995). For example in a well-known incident, an early dialogue system required the user to press a key to interrupt the system (Stifelman et al., 1993). But user testing showed users barged in (interrupted, talking over the system), which led to a redesign of the system to recognize overlapped speech. It’s also important to incorporate value sensitive design, in which we carefully consider during the design process the benefits, harms and possible stakeholders of the resulting system (Friedman et al. 2017, Friedman and Hendry 2019).

# 15.5.1 Ethical Issues in Dialogue System Design

Ethical issues have been key to how we think about designing artificial agents since well before we had dialogue systems. Mary Shelley (depicted below) centered her novel Frankenstein around the problem of creating artificial agents without considering

![## Image Analysis: aa7d57817d083a119cbb77b5c885155ce3167a20243f8f86f73c96167909197c.jpg

**Conceptual Understanding:**
This image conceptually represents a historical portrait, specifically of Ada Lovelace. Its main purpose is to visually identify and acknowledge a foundational figure in the history of computing, particularly relevant to a discussion on 'Ethical Issues in Dialogue System Design.' The image communicates the concept of heritage and the human origins of complex technological fields like AI and dialogue systems.

**Content Interpretation:**
The image is a painted portrait of a woman. It depicts a female figure from the chest up, facing the viewer directly. The woman has dark hair, styled simply, and wears a dark, off-the-shoulder dress. Her expression is calm and direct. The painting style appears to be traditional portraiture, likely from the 19th century, given the subject's attire and the artistic conventions of the time. The significance of this portrait, particularly in the context of 'Ethical Issues in Dialogue System Design,' is that the subject is Ada Lovelace. She is widely recognized as the first computer programmer for her work on Charles Babbage's Analytical Engine. The portrait visually grounds discussions about the historical foundations of computing and artificial intelligence, providing a human face to the origins of the field whose ethical implications are now being debated.

**Key Insights:**
The primary knowledge extracted from this image is the visual identification of Ada Lovelace, a pivotal figure in the history of computer science. Her presence in a section on 'Ethical Issues in Dialogue System Design' suggests a linkage between the historical development of computing and contemporary ethical debates. The image implicitly conveys that the study of computing's ethical dimensions is rooted in a rich history, with pioneers like Lovelace laying the groundwork for the complex systems we have today. It reinforces the idea that understanding the past is crucial for navigating the future of technology and its ethical challenges. The image itself, as a portrait, provides a historical record of her likeness.

**Document Context:**
Given the document section title '15.5.1 Ethical Issues in Dialogue System Design,' this image is highly relevant as it depicts Ada Lovelace (1815-1852), often considered the first computer programmer. Her contributions to the theoretical understanding of computing, including her insights into the Analytical Engine's potential beyond mere calculation (foreshadowing general-purpose computing and even AI concepts), make her an emblematic figure in discussions about the origins, development, and ethical considerations of technology. The inclusion of her portrait serves to acknowledge the historical roots of computing and implicitly connects modern ethical debates to the foundational figures of the field. It reminds the reader of the long history of human thought behind the technological advancements being discussed.

**Summary:**
This image is a portrait of Augusta Ada King, Countess of Lovelace, commonly known as Ada Lovelace. She is depicted in a formal, dark gown with off-the-shoulder styling, revealing her neck and shoulders. Her hair is dark and parted in the middle, pulled back, framing her face. She has a composed expression, looking directly forward, with a slight, subtle smile. The background is dark and indistinct, with hints of a reddish-brown color, possibly a chair or drapery, behind her. The lighting highlights her face and shoulders. The portrait's purpose is to visually represent Ada Lovelace, a significant figure in the history of computing.](images/aa7d57817d083a119cbb77b5c885155ce3167a20243f8f86f73c96167909197c.jpg)

ethical and humanistic concerns. One issue is the safety of users. If users seek information from dialogue systems in safety-critical situations like asking medical advice, or in emergency situations, or when indicating the intentions of self-harm, incorrect advice can be dangerous and even life-threatening. For example (Bickmore et al., 2018) gave participants medical problems to pose to three commercial dialogue systems (Siri, Alexa, Google Assistant) and asked them to determine an action to take based on the system responses; many of the proposed actions, if actually taken, would have led to harm or death.

A system can also harm users by verbally attacking them, or creating representational harms (Blodgett et al., 2020) by generating abusive or harmful stereotypes that demean particular groups of people. Both abuse and stereotypes can cause psychological harm to users. Microsoft’s 2016 Tay chatbot, for example, was taken offline 16 hours after it went live, when it began posting messages with racial slurs, conspiracy theories, and personal attacks on its users. Tay had learned these biases and actions from its training data, including from users who seemed to be purposely teaching the system to repeat this kind of language (Neff and Nagy 2016). Henderson et al. (2017) examined dialogue datasets used to train corpus-based chatbots and found toxic and abusive language, especially in social media corpora like Twitter and Reddit, and indeed such language then appears in the text generated by language models and dialogue systems (Gehman et al. 2020; Xu et al. 2020) which can even amplify the bias from the training data (Dinan et al., 2020). Liu et al. (2020) developed another method for investigating bias, testing how neural dialogue systems responded to pairs of simulated user turns that are identical except for mentioning different genders or race. They found, for example, that simple changes like using the word ‘she’ instead of ‘he’ in a sentence caused systems to respond more offensively and with more negative sentiment.

Another important ethical issue is privacy. Already in the first days of ELIZA, Weizenbaum pointed out the privacy implications of people’s revelations to the chatbot. The ubiquity of in-home dialogue systems means they may often overhear private information (Henderson et al., 2017). If a chatbot is human-like, users are also more likely to disclose private information, and less likely to worry about the harm of this disclosure (Ischen et al., 2019). In general, chatbots that are trained on transcripts of human-human or human-machine conversation must anonymize personally identifiable information.

Finally, chatbots raise important issues of gender equality in addition to textual bias. Current chatbots are overwhelmingly given female names, likely perpetuating the stereotype of a subservient female servant (Paolino, 2017). And when users use sexually harassing language, most commercial chatbots evade or give positive responses rather than responding in clear negative ways (Fessler, 2017).

These ethical issues are an important area of investigation, including finding ways to mitigate problems of abuse and toxicity, like detecting and responding appropriately to toxic contexts (Wolf et al. 2017, Dinan et al. 2020, Xu et al. 2020). Value sensitive design, carefully considering possible harms in advance (Friedman et al. 2017, Friedman and Hendry 2019) is also important; (Dinan et al., 2021) give a number of suggestions for best practices in dialogue system design. For example getting informed consent from participants, whether they are used for training, or whether they are interacting with a deployed system is important. Because dialogue systems by definition involve human participants, researchers also work on these issues with the Institutional Review Boards (IRB) at their institutions, who help protect the safety of experimental subjects.

# 15.6 Summary

Chatbots and dialogue systems are crucial speech and language processing applications that are already widely used commercially.

• In human dialogue, speaking is a kind of action; these acts are referred to as speech acts or dialogue acts. Speakers also attempt to achieve common ground by acknowledging that they have understand each other. Conversation also is characterized by turn structure and dialogue structure. • Chatbots are conversational systems designed to mimic the appearance of informal human conversation. Rule-based chatbots like ELIZA and its modern

descendants use rules to map user sentences into system responses. Corpusbased chatbots mine logs of human conversation to learn to automatically map user sentences into system responses.   
• For task-based dialogue, most commercial dialogue systems use the GUS or frame-based architecture, in which the designer specifies frames consisting of slots that the system must fill by asking the user.   
• The dialogue-state architecture augments the GUS frame-and-slot architecture with richer representations and more sophisticated algorithms for keeping track of user’s dialogue acts, policies for generating its own dialogue acts, and a natural language component.   
• Dialogue systems are a kind of human-computer interaction, and general HCI principles apply in their design, including the role of the user, simulations such as Wizard-of-Oz systems, and the importance of iterative design and testing on real users.

# Bibliographical and Historical Notes

The linguistic, philosophical, and psychological literature on dialogue is quite extensive. For example the idea that utterances in a conversation are a kind of action being performed by the speaker was due originally to the philosopher Wittgenstein (1953) but worked out more fully by Austin (1962) and his student John Searle. Various sets of speech acts have been defined over the years, and a rich linguistic and philosophical literature developed, especially focused on explaining the use of indirect speech acts. The idea of dialogue acts draws also from a number of other sources, including the ideas of adjacency pairs, pre-sequences, and other aspects of the interactional properties of human conversation developed in the field of conversation analysis (see Levinson (1983) for an introduction to the field). This idea that acts set up strong local dialogue expectations was also prefigured by Firth (1935, p. 70), in a famous quotation:

Most of the give-and-take of conversation in our everyday life is stereotyped and very narrowly conditioned by our particular type of culture. It is a sort of roughly prescribed social ritual, in which you generally say what the other fellow expects you, one way or the other, to say.

Another important research thread modeled dialogue as a kind of collaborative behavior, including the ideas of common ground (Clark and Marshall, 1981), reference as a collaborative process (Clark and Wilkes-Gibbs, 1986), joint intention (Levesque et al., 1990), and shared plans (Grosz and Sidner, 1980).

The earliest conversational systems were simple pattern-action chatbots like ELIZA (Weizenbaum, 1966). ELIZA had a widespread influence on popular perceptions of artificial intelligence, and brought up some of the first ethical questions in natural language processing —such as the issues of privacy we discussed above as well the role of algorithms in decision-making— leading its creator Joseph Weizenbaum to fight for social responsibility in AI and computer science in general.

Computational-implemented theories of dialogue blossomed in the 1970. That period saw the very influential GUS system (Bobrow et al., 1977), which in the late 1970s established the frame-based paradigm that became the dominant industrial paradigm for dialogue systems for over 30 years.

Another influential line of research from that decade focused on modeling the hierarchical structure of dialogue. Grosz’s pioneering 1977b dissertation first showed that “task-oriented dialogues have a structure that closely parallels the structure of the task being performed” (p. 27), leading to her work with Sidner and others showing how to use similar notions of intention and plans to model discourse structure and coherence in dialogue. See, e.g., Lochbaum et al. (2000) for a summary of the role of intentional structure in dialogue.

Yet a third line, first suggested by Bruce (1975), suggested that since speech acts are actions, they should be planned like other actions, and drew on the AI planning literature (Fikes and Nilsson, 1971). A system seeking to find out some information can come up with the plan of asking the interlocutor for the information. A system hearing an utterance can interpret a speech act by running the planner “in reverse”, using inference rules to infer from what the interlocutor said what the plan might have been. Plan-based models of dialogue are referred to as BDI models because such planners model the beliefs, desires, and intentions (BDI) of the system and interlocutor. BDI models of dialogue were first introduced by Allen, Cohen, Perrault, and their colleagues in a number of influential papers showing how speech acts could be generated (Cohen and Perrault, 1979) and interpreted (Perrault and Allen 1980, Allen and Perrault 1980). At the same time, Wilensky (1983) introduced plan-based models of understanding as part of the task of interpreting stories.

In the 1990s, machine learning models that had first been applied to natural language processing began to be applied to dialogue tasks like slot filling (Miller et al. 1994, Pieraccini et al. 1991). This period also saw lots of analytic work on the linguistic properties of dialogue acts and on machine-learning-based methods for their detection. (Sag and Liberman 1975, Hinkelman and Allen 1989, Nagata and Morimoto 1994, Goodwin 1996, Chu-Carroll 1998, Shriberg et al. 1998, Stolcke et al. 2000, Gravano et al. 2012. This work strongly informed the development of the dialogue-state model (Larsson and Traum, 2000). Dialogue state tracking quickly became an important problem for task-oriented dialogue, and there has been an influential annual evaluation of state-tracking algorithms (Williams et al., 2016).

The turn of the century saw a line of work on applying reinforcement learning to dialogue, which first came out of AT&T and Bell Laboratories with work on MDP dialogue systems (Walker 2000, Levin et al. 2000, Singh et al. 2002) along with work on cue phrases, prosody, and rejection and confirmation. Reinforcement learning research turned quickly to the more sophisticated POMDP models (Roy et al. 2000, Lemon et al. 2006, Williams and Young 2007) applied to small slotfilling dialogue tasks. Neural reinforcement learning models have been used both for chatbot systems, for example simulating dialogues between two dialogue systems, rewarding good conversational properties like coherence and ease of answering (Li et al., 2016a), and for task-oriented dialogue (Williams et al., 2017).

By around 2010 the GUS architecture finally began to be widely used commercially in dialogue systems on phones like Apple’s SIRI (Bellegarda, 2013) and other digital assistants.

The rise of the web gave rise to corpus-based chatbot architectures around the turn of the century, first using information retrieval models and then in the 2010s, after the rise of deep learning, with sequence-to-sequence models.

[TBD: Modern history of neural chatbots]

Other important dialogue areas include the study of affect in dialogue (Rashkin et al. 2019, Lin et al. 2019) and conversational interface design (Cohen et al. 2004, Harris 2005, Pearl 2017, Deibel and Evanhoe 2021).

# Exercises

dispreferred response

15.1 Write a finite-state automaton for a dialogue manager for checking your bank balance and withdrawing money at an automated teller machine.   
15.2 A dispreferred response is a response that has the potential to make a person uncomfortable or embarrassed in the conversational context; the most common example dispreferred responses is turning down a request. People signal their discomfort with having to say no with surface cues (like the word well), or via significant silence. Try to notice the next time you or someone else utters a dispreferred response, and write down the utterance. What are some other cues in the response that a system might use to detect a dispreferred response? Consider non-verbal cues like eye gaze and body gestures.   
15.3 When asked a question to which they aren’t sure they know the answer, people display their lack of confidence by cues that resemble other dispreferred responses. Try to notice some unsure answers to questions. What are some of the cues? If you have trouble doing this, read Smith and Clark (1993) and listen specifically for the cues they mention.   
15.4 Implement a small air-travel help system based on text input. Your system should get constraints from users about a particular flight that they want to take, expressed in natural language, and display possible flights on a screen. Make simplifying assumptions. You may build in a simple flight database or you may use a flight information system on the Web as your backend.

# 16 Automatic Speech Recognitionand Text-to-Speech

I KNOW not whether I see your meaning: if I do, it lies Upon the wordy wavelets of your voice, Dim as an evening shadow in a brook, Thomas Lovell Beddoes, 1851

Understanding spoken language, or at least transcribing the words into writing, is one of the earliest goals of computer language processing. In fact, speech processing

predates the computer by many decades! The first machine that recognized speech was a toy from the 1920s. “Radio Rex”, shown to the right, was a celluloid dog that moved (by means of a spring) when the spring was released by ${ 5 0 0 } \mathrm { H z }$ acoustic energy. Since $5 0 0 ~ \mathrm { H z }$ is roughly the first formant of the vowel [eh] in “Rex”, Rex seemed to come when he was called (David, Jr. and Selfridge, 1962).

![## Image Analysis: e5c47b0c4bfa591f17f42ed59712aeb3c69da618d275110afa44b7d191161675.jpg

**Conceptual Understanding:**
This image represents a vintage 'Radio Rex' toy, a historical novelty item from the early 20th century, accompanied by its original packaging. Conceptually, it illustrates a rudimentary form of a voice-activated or sound-responsive device from an era when radio technology was rapidly developing. The main purpose of the image is to showcase this specific artifact, providing a visual example of early mechanical attempts to interact with human speech or sound, which serves as a historical context for the development of Automatic Speech Recognition and Text-to-Speech technologies. It communicates the idea of early technological innovation in entertainment and human-machine interaction, specifically concerning sound and voice.

**Content Interpretation:**
The image illustrates a vintage 'Radio Rex' toy and its original packaging. The toy itself is a small, light brown doghouse with a red roof, and a small dark brown bulldog figure emerging from its arched entrance. A green sign above the entrance, clearly stating 'RADIO REX' with lightning bolt symbols, indicates the toy's name and potentially its operational principle (electricity/radio waves). The accompanying box features an illustration depicting a doghouse in a snowy scene, a bulldog, and a person playing a musical instrument, further reinforcing the concept of sound or radio. The repeated 'TELETCH' text on the box border could be a brand name, a sound effect, or a phonetic representation linked to the toy's function. The manufacturer, 'THE JOHN PONTIFEX MFG. CO. NEW HAVEN.', is clearly identified, and a faint '1923' suggests its production year. The toy likely involved sound activation or radio-controlled movement, with the dog emerging when a specific sound (perhaps 'Rex' or 'Teletch') was made or a radio signal received. This represents an early form of interactive or automated toy, utilizing then-novel communication technologies.

**Key Insights:**
The main takeaway is that 'Radio Rex' was a significant early 20th-century toy that utilized radio technology to respond to a specific sound/word. This demonstrates an early, albeit primitive, attempt at voice-activated interaction, predating modern Automatic Speech Recognition. The extracted text 'RADIO REX' on both the toy and the box identifies the product. 'THE JOHN PONTIFEX MFG. CO. NEW HAVEN.' provides the manufacturer and location, indicating its American origin. The faint '1923' on the box suggests its early production year, placing it within the nascent period of radio and sound technology development. The repeated 'TELETCH' on the box border, possibly a phonetic representation or brand element, further emphasizes the sound-related nature of the toy. These details highlight the historical efforts to create responsive mechanical devices through acoustic or radio means, directly linking to the themes of speech and sound technology.

**Document Context:**
Given the document section '16 Automatic Speech Recognition and Text-to-Speech', this image of 'Radio Rex' is highly relevant. 'Radio Rex' was a pioneering novelty toy, first patented in 1922, that responded to the spoken word 'Rex' by having the dog figure emerge from its house. The toy achieved this not through what we now know as true speech recognition, but by being tuned to the resonant frequency of the 'e' sound in 'Rex' transmitted via a specific radio frequency. Therefore, it serves as a historical precursor or an early, rudimentary example of a device responding to human speech, illustrating the very early concepts that would eventually evolve into modern Automatic Speech Recognition (ASR) systems. It highlights the historical context of attempts to create voice-activated mechanisms.

**Summary:**
The image displays an antique toy, consisting of a miniature doghouse and its original packaging box. The doghouse is light brown with a red shingled roof, featuring an arched entrance. A small, dark brown bulldog figure is positioned partially inside and partially outside the entrance, facing forward. Above the entrance, a green sign clearly reads 'RADIO REX', flanked by two stylized lightning bolt symbols. The accompanying box is reddish-brown and shows an illustration of a doghouse in a snowy landscape with a bulldog in the foreground and a figure playing a violin-like instrument to the left. A prominent blue banner across the top of the illustration on the box also says 'RADIO REX'. In the bottom right corner of the illustration, the manufacturer's text reads 'THE JOHN PONTIFEX MFG. CO. NEW HAVEN.'. Faintly in the top right of the illustration, the year '1923' is visible. The border of the main illustration on the box repeatedly displays the word 'TELETCH' multiple times on all four sides. The overall presentation suggests a historical toy related to sound or radio technology, likely a novelty item from the early 20th century.](images/e5c47b0c4bfa591f17f42ed59712aeb3c69da618d275110afa44b7d191161675.jpg)

In modern times, we expect more of our automatic systems. The task of automatic speech recognition (ASR) is to map any waveform like this:

![## Image Analysis: 4a5ad21d067fb8a31ea0f9bc7a615a0502167ae7156021dba23458bd16556c0e.jpg

**Conceptual Understanding:**
This image conceptually represents an audio waveform, likely a segment of spoken language. Its main purpose is to visually demonstrate how sound, particularly speech, can be depicted as an electrical or digital signal that varies in amplitude over time. The key idea being communicated is the transformation of an auditory phenomenon into a measurable, analyzable signal, which is the cornerstone for developing technologies in the field of Automatic Speech Recognition and Text-to-Speech.

**Content Interpretation:**
The image illustrates an audio waveform, specifically a visual representation of sound energy (amplitude) as it changes over time. The waveform consists of a series of vertical lines originating from a central horizontal axis, with their height indicating the amplitude of the sound signal at a given point in time. The varying patterns, such as the two prominent high-amplitude segments and the quieter sections in between, indicate different characteristics of the sound. These could correspond to different speech sounds (phonemes or words), with the louder parts likely being voiced sounds and the quieter parts potentially representing unvoiced sounds, fricatives, or silences. This visual decomposition of sound into amplitude variations over time is a fundamental concept in digital audio processing and forms the basis for systems like Automatic Speech Recognition (ASR) and Text-to-Speech (TTS).

**Key Insights:**
The main takeaway from this image is the visual representation of a dynamic speech signal. It teaches that speech is not a continuous, uniform entity but rather a complex signal with distinct, measurable variations in amplitude over time. These variations correspond to different phonetic events and energy levels within spoken language. The image supports the conclusion that speech can be mathematically modeled and processed by observing these amplitude changes, which is crucial for building robust ASR and TTS systems. The visual evidence of varying amplitude and density across the waveform directly illustrates the type of information that signal processing algorithms analyze to identify phonemes, words, and prosody.

**Document Context:**
This image is highly relevant to a document section titled '16 Automatic Speech Recognition and Text-to-Speech'. It provides a foundational visual example of the raw input data (an acoustic waveform) that an ASR system would process or the raw output that a TTS system would generate. It visually grounds the abstract concepts of speech and sound into a tangible, measurable signal, which is essential for understanding how machines 'hear' or 'speak'. The varying patterns in the waveform directly relate to the features that would be extracted for speech recognition or synthesized for speech generation.

**Summary:**
The image displays a black and white audio waveform on a white background. The waveform represents variations in sound amplitude over time. It shows distinct segments of varying intensity, characterized by vertical lines extending above and below a central horizontal axis. There are two major bursts of high-amplitude activity, indicating energetic sound segments, separated by regions of lower amplitude and less dense lines, suggesting quieter sounds or brief silences. The first high-amplitude section is followed by a short period of lower activity, then a second, more prolonged, and generally higher-amplitude section, which gradually tapers off. The entire waveform demonstrates the dynamic nature of a speech signal, fundamental for understanding how speech is processed computationally. There is no explicit textual content within the image itself, such as labels, titles, or annotations.](images/4a5ad21d067fb8a31ea0f9bc7a615a0502167ae7156021dba23458bd16556c0e.jpg)

to the appropriate string of words:

Automatic transcription of speech by any speaker in any environment is still far from solved, but ASR technology has matured to the point where it is now viable for many practical tasks. Speech is a natural interface for communicating with smart home appliances, personal assistants, or cellphones, where keyboards are less convenient, in telephony applications like call-routing (“Accounting, please”) or in sophisticated dialogue applications (“I’d like to change the return date of my flight”). ASR is also useful for general transcription, for example for automatically generating captions for audio or video text (transcribing movies or videos or live discussions). Transcription is important in fields like law where dictation plays an important role. Finally, ASR is important as part of augmentative communication (interaction between computers and humans with some disability resulting in difficulties or inabilities in typing or audition). The blind Milton famously dictated Paradise Lost to his daughters, and Henry James dictated his later novels after a repetitive stress injury.

What about the opposite problem, going from text to speech? This is a problem with an even longer history. In Vienna in 1769, Wolfgang von Kempelen built for

speech synthesis text-to-speech

the Empress Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting of a wooden box filled with gears, behind which sat a robot mannequin who played chess by moving pieces with his mechanical arm. The Turk toured Europe and the Americas for decades, defeating Napoleon Bonaparte and even playing Charles Babbage. The Mechanical Turk might have been one of the early successes of artificial intelligence were it not for the fact that it was, alas, a hoax, powered by a human chess player hidden inside the box.

What is less well known is that von Kempelen, an extraordinarily

prolific inventor, also built between 1769 and 1790 what was definitely not a hoax: the first full-sentence speech synthesizer, shown partially to the right. His device consisted of a bellows to simulate the lungs, a rubber mouthpiece and a nose aperture, a reed to simulate the vocal folds, various whistles for the fricatives, and a

TTS

![## Image Analysis: e2867b86072a9e90d94e9510fea62d6350efde198f54c729d6636095a9415526.jpg

**Conceptual Understanding:**
This image conceptually represents an historical, mechanical apparatus designed for the generation of sound, likely in an experimental context related to acoustics or rudimentary speech synthesis. Its main purpose is to illustrate an early methodology for converting controlled airflow into audible sounds, possibly for scientific study or demonstration. The core ideas communicated involve the physical components and their arrangement to achieve air pressure, direct it through various channels, and modulate it into different tones or qualities of sound.

**Content Interpretation:**
The image illustrates a mechanical apparatus for generating sounds. Figure 1 details the air pressure generation and delivery system: large bellows ('r') are used to supply air, controlled by a weighted pulley mechanism ('f', 'h', 'i', 'g') to ensure a consistent air flow. This air is directed towards a platform ('a') with multiple small outlets ('p'). Figure 2 shows a sound-modulating or resonance chamber: a box with a large circular outlet ('o') and several smaller holes on its top, likely designed to be placed over the outlets ('p') of the platform in Figure 1. This setup suggests an experimental device for exploring acoustics, possibly an early attempt at synthesizing vocal sounds or musical notes by varying airflow and resonance.

**Key Insights:**
The main takeaways from this image are: 1. Early attempts at sound/speech generation were mechanical, relying on physical manipulation of air pressure and resonance. 2. Bellows ('r') were a primary means of creating and sustaining air pressure. 3. Weighted pulley systems ('f', 'h', 'i', 'g') were employed to regulate the airflow for consistency or specific effects. 4. Multiple outlets ('p') and interchangeable sound-modulating chambers (Figure 2, with openings 'o' and holes on top) suggest an understanding that different sounds could be produced by varying air pathways and resonance. 5. The design points to an experimental approach to understanding how to mechanically mimic complex sounds, laying groundwork for later advancements in acoustics and speech technology. All extracted text labels ('r', 'a', 'p', 'o', 'n', 'k', 'l', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i') specifically denote the individual components involved in this mechanical sound production system.

**Document Context:**
This image serves as a foundational illustration within the document's 'Automatic Speech Recognition and Text-to-Speech' section. It visually represents the mechanical precursors to modern speech synthesis and acoustic engineering. By showcasing an early apparatus that uses controlled air pressure and resonance chambers to produce sound, it provides historical context for the evolution of technologies that manipulate and generate speech. The device demonstrates the fundamental principles of using airflow to create sound, which is essential to understanding both natural speech production and its artificial replication.

**Summary:**
This image, labeled 'Tab. XVII.', presents two figures illustrating components of an early mechanical apparatus, likely related to sound or speech generation, as suggested by the document's section on 'Automatic Speech Recognition and Text-to-Speech'. Figure 1 shows the primary assembly: a wooden platform ('a') supported by legs ('c') and a base ('b'). On this platform, a large set of bellows ('r') is positioned. The bellows are connected to an opening ('k') on the platform, and have a small wooden block ('l') underneath them. A vertical arched structure, formed by pillars ('d' and 'e'), stands over the bellows. A pulley ('f') is mounted at the top of this arch. A string ('h') passes over the pulley, with a weight ('i') attached to one end and a hook ('g') attached to the other, which is connected to the top plate of the bellows, indicating a mechanism for applying consistent pressure. The platform ('a') also features several small circular openings labeled 'p'. Figure 2 depicts a separate, rectangular wooden box. This box has a large circular opening ('o') on one side, and several smaller circular holes on its top surface. Two projections labeled 'n' extend downwards from its bottom corners. This box is likely designed to be placed onto or connect with the platform in Figure 1, specifically over the 'p' openings, to generate sounds by directing the air from the bellows through its various openings.](images/e2867b86072a9e90d94e9510fea62d6350efde198f54c729d6636095a9415526.jpg)

small auxiliary bellows to provide the puff of air for plosives. By moving levers with both hands to open and close apertures, and adjusting the flexible leather “vocal tract”, an operator could produce different consonants and vowels.

More than two centuries later, we no longer build our synthesizers out of wood and leather, nor do we need human operators. The modern task of speech synthesis, also called text-to-speech or TTS, is exactly the reverse of ASR; to map text:

to an acoustic waveform:

![## Image Analysis: dbcbdab96ab2373368a37dc0f70b5e4281d283ae46e645777514ba89bf73c4c2.jpg

**Conceptual Understanding:**
This image conceptually represents an audio waveform, which is a graphical depiction of sound amplitude over time. The presence of the text "ASR AND TTS." explicitly links this waveform to the domain of Automatic Speech Recognition and Text-to-Speech technologies.

The main purpose of this image is to illustrate the raw acoustic data that forms the basis of speech processing. It visually conveys how sound, specifically speech, manifests as a varying signal over time, characterized by changes in amplitude (loudness) and patterns of oscillation (representing different speech sounds).

Key ideas communicated include: the physical nature of spoken language as an acoustic signal, the variability of speech sounds over time, and the fundamental input/output representation used by speech recognition and synthesis systems.

**Content Interpretation:**
The image primarily displays an audio waveform, which is a graphical representation of sound amplitude over time. This visual is fundamental to understanding speech processing. The fluctuations in the waveform (peaks and troughs) indicate variations in sound intensity.

Sections with higher amplitude (taller peaks) correspond to louder sounds, while flatter sections represent quieter sounds or silence. The varying density and regularity of the oscillations reflect different speech sounds, such as vowels (often more regular, higher amplitude) versus consonants (potentially more irregular, lower or burst-like amplitudes).

The significance of this waveform in the context of "Automatic Speech Recognition and Text-to-Speech" is profound:
*   **For Automatic Speech Recognition (ASR):** The system must analyze this raw audio signal to identify acoustic patterns, segment the speech into phonemes or words, and then convert those into text. The different characteristics of the waveform (e.g., specific frequencies, amplitudes, and durations) are the features that an ASR model extracts and interprets to recognize spoken content.
*   **For Text-to-Speech (TTS):** A TTS system generates such a waveform from text. The primary goal of TTS is to produce an artificial speech signal that sounds natural. This requires accurately synthesizing the complex amplitude and frequency variations observed in natural speech waveforms, as depicted.

All extracted text elements, specifically "ASR AND TTS.", support these interpretations by explicitly linking the generic audio waveform illustration to the specific domain of speech technology. This text serves as a direct label indicating that this waveform is a core component or representation within systems that convert speech to text (ASR) and text to speech (TTS). The visual properties of the waveform itself – its varying amplitude, duration, and discernible sections – are the primary evidence of the raw data that these technologies interact with.

**Key Insights:**
The main takeaways from this image, supported by the textual evidence, are:
1.  **Speech is a Complex Signal:** The irregular yet patterned nature of the waveform demonstrates that speech is not a simple, uniform signal but a complex, time-varying acoustic phenomenon. Different sections of the waveform visually represent distinct sounds, silences, and variations in loudness and pitch.
2.  **Fundamental Data for Speech Technologies:** The waveform is the most basic and fundamental input for Automatic Speech Recognition (ASR) systems and the essential output for Text-to-Speech (TTS) systems. Its analysis is critical for ASR, and its accurate synthesis is crucial for TTS.
3.  **Visualizing Sound Properties:** The image provides a direct visual means to understand the physical properties of spoken language, such as changes in loudness (amplitude) and the presence of different sounds over time, which are key features for speech processing.

These conclusions and insights are directly supported by the text "ASR AND TTS.", which frames the purpose of showing this waveform. It indicates that the image serves as an introductory or illustrative example of the kind of audio data that both speech recognition systems (which process such waveforms) and speech synthesis systems (which generate such waveforms) operate upon. The visual content itself, as a detailed audio waveform, is the primary evidence for the physical representation and complexity of speech.

**Document Context:**
This image is placed within a document section titled "16 Automatic Speech Recognition and Text-to-Speech." Its contextual relevance is to serve as a foundational visual element, illustrating the raw acoustic signal that both ASR and TTS technologies manipulate. For ASR, it visually represents the input sound waves that need to be processed and converted into linguistic units. For TTS, it represents the desired output sound waves that need to be artificially generated from text. By displaying this fundamental data form, the image sets the stage for understanding the underlying principles and challenges involved in these advanced speech technologies.

**Summary:**
The image displays a black and white audio waveform against a white background. The waveform represents sound amplitude over time, with the horizontal axis implicitly representing time and the vertical deflections indicating amplitude (loudness). The waveform shows a series of varying oscillations. 

Starting from the left, there's an initial burst of activity with moderate amplitude, followed by a brief period of lower amplitude, almost silence. This is succeeded by a segment of more intense, relatively regular oscillations with higher amplitudes, suggesting a voiced sound like a vowel. After another brief dip in amplitude, the waveform shows a very prominent, sustained section with significantly higher and denser oscillations, indicating a louder and possibly longer spoken sound or series of sounds. This high-amplitude section gradually tapers off towards the right, ending with low-amplitude activity that fades into silence.

In the top-left corner of the image, the text "ASR AND TTS." is clearly visible. This annotation indicates that the waveform is presented within the context of Automatic Speech Recognition (ASR) and Text-to-Speech (TTS) technologies. It visually grounds the discussion of these complex systems by showing the raw auditory data they process or produce. The different patterns in the waveform visually illustrate the varying acoustic properties of speech, which are critical for both recognizing spoken words and synthesizing natural-sounding speech.](images/dbcbdab96ab2373368a37dc0f70b5e4281d283ae46e645777514ba89bf73c4c2.jpg)

Modern speech synthesis has a wide variety of applications. TTS is used in conversational agents that conduct dialogues with people, plays a role in devices that read out loud for the blind or in games, and can be used to speak for sufferers of neurological disorders, such as the late astrophysicist Steven Hawking who, after he lost the use of his voice because of ALS, spoke by manipulating a TTS system.

In the next sections we’ll show how to do ASR with encoder-decoders, introduce the CTC loss functions, the standard word error rate evaluation metric, and describe how acoustic features are extracted. We’ll then see how TTS can be modeled with almost the same algorithm in reverse, and conclude with a brief mention of other speech tasks.

# 16.1 The Automatic Speech Recognition Task

Before describing algorithms for ASR, let’s talk about how the task itself varies. One dimension of variation is vocabulary size. Some ASR tasks can be solved with extremely high accuracy, like those with a 2-word vocabulary (yes versus no) or an 11 word vocabulary like digit recognition (recognizing sequences of digits including zero to nine plus oh). Open-ended tasks like transcribing videos or human conversations, with large vocabularies of up to 60,000 words, are much harder.

read speech conversational speech

A second dimension of variation is who the speaker is talking to. Humans speaking to machines (either dictating or talking to a dialogue system) are easier to recognize than humans speaking to humans. Read speech, in which humans are reading out loud, for example in audio books, is also relatively easy to recognize. Recognizing the speech of two humans talking to each other in conversational speech, for example, for transcribing a business meeting, is the hardest. It seems that when humans talk to machines, or read without an audience present, they simplify their speech quite a bit, talking more slowly and more clearly.

A third dimension of variation is channel and noise. Speech is easier to recognize if it’s recorded in a quiet room with head-mounted microphones than if it’s recorded by a distant microphone on a noisy city street, or in a car with the window open.

A final dimension of variation is accent or speaker-class characteristics. Speech is easier to recognize if the speaker is speaking the same dialect or variety that the system was trained on. Speech by speakers of regional or ethnic dialects, or speech by children can be quite difficult to recognize if the system is only trained on speakers of standard dialects, or only adult speakers.

LibriSpeech

A number of publicly available corpora with human-created transcripts are used to create ASR test and training sets to explore this variation; we mention a few of them here since you will encounter them in the literature. LibriSpeech is a large open-source read-speech $1 6 \mathrm { k H z }$ dataset with over 1000 hours of audio books from the LibriVox project, with transcripts aligned at the sentence level (Panayotov et al., 2015). It is divided into an easier (“clean”) and a more difficult portion (“other”) with the clean portion of higher recording quality and with accents closer to US English. This was done by running a speech recognizer (trained on read speech from the Wall Street Journal) on all the audio, computing the WER for each speaker based on the gold transcripts, and dividing the speakers roughly in half, with recordings from lower-WER speakers called “clean” and recordings from higher-WER speakers “other”.

Switchboard

CALLHOME

The Switchboard corpus of prompted telephone conversations between strangers was collected in the early 1990s; it contains 2430 conversations averaging 6 minutes each, totaling 240 hours of $8 \ \mathrm { k H z }$ speech and about 3 million words (Godfrey et al., 1992). Switchboard has the singular advantage of an enormous amount of auxiliary hand-done linguistic labeling, including parses, dialogue act tags, phonetic and prosodic labeling, and discourse and information structure. The CALLHOME corpus was collected in the late 1990s and consists of 120 unscripted 30-minute telephone conversations between native speakers of English who were usually close friends or family (Canavan et al., 1997).

The Santa Barbara Corpus of Spoken American English (Du Bois et al., 2005) is a large corpus of naturally occurring everyday spoken interactions from all over the United States, mostly face-to-face conversation, but also town-hall meetings, food preparation, on-the-job talk, and classroom lectures. The corpus was anonymized by removing personal names and other identifying information (replaced by pseudonyms in the transcripts, and masked in the audio).

CORAAL

# CHiME

CORAAL is a collection of over 150 sociolinguistic interviews with African American speakers, with the goal of studying African American Language (AAL), the many variations of language used in African American communities (Kendall and Farrington, 2020). The interviews are anonymized with transcripts aligned at the utterance level. The CHiME Challenge is a series of difficult shared tasks with corpora that deal with robustness in ASR. The CHiME 5 task, for example, is ASR of conversational speech in real home environments (specifically dinner parties). The corpus contains recordings of twenty different dinner parties in real homes, each with four participants, and in three locations (kitchen, dining area, living room), recorded both with distant room microphones and with body-worn mikes. The

HKUST Mandarin Telephone Speech corpus has 1206 ten-minute telephone conversations between speakers of Mandarin across China, including transcripts of the conversations, which are between either friends or strangers (Liu et al., 2006). The

AISHELL-1 corpus contains 170 hours of Mandarin read speech of sentences taken from various domains, read by different speakers mainly from northern China (Bu et al., 2017).

Figure 16.1 shows the rough percentage of incorrect words (the word error rate, or WER, defined on page 346) from state-of-the-art systems on some of these tasks. Note that the error rate on read speech (like the LibriSpeech audiobook corpus) is around $2 \%$ ; this is a solved task, although these numbers come from systems that require enormous computational resources. By contrast, the error rate for transcribing conversations between humans is much higher; 5.8 to $11 \%$ for the Switchboard and CALLHOME corpora. The error rate is higher yet again for speakers of varieties like African American Vernacular English, and yet again for difficult conversational tasks like transcription of 4-speaker dinner party speech, which can have error rates as high as $8 1 . 3 \%$ . Character error rates (CER) are also much lower for read Mandarin speech than for natural conversation.

<table><tr><td>English Tasks</td><td>WER%</td></tr><tr><td>LibriSpeech audiobooks 960hour clean</td><td>1.4</td></tr><tr><td>LibriSpeech audiobooks 960hour other</td><td>2.6</td></tr><tr><td>Switchboard telephone conversations between strangers CALLHOME telephone conversations between family</td><td>5.8</td></tr><tr><td> Sociolinguistic interviews, CORAAL (AAL)</td><td>11.0 27.0</td></tr><tr><td> CHiMe5 dinner parties with body-worn microphones</td><td></td></tr><tr><td> CHiMe5 dinner parties with distant microphones</td><td>47.9</td></tr><tr><td></td><td>81.3</td></tr><tr><td>Chinese (Mandarin) Tasks</td><td>CER%</td></tr><tr><td>AISHELL-1 Mandarin read speech corpus HKUST Mandarin Chinese telephone conversations</td><td>6.7</td></tr></table>

Figure 16.1 Rough Word Error Rates $( \mathrm { W E R } = \%$ of words misrecognized) reported around 2020 for ASR on various American English recognition tasks, and character error rates (CER) for two Chinese recognition tasks.

# 16.2 Feature Extraction for ASR: Log Mel Spectrum

# feature vector

The first step in ASR is to transform the input waveform into a sequence of acoustic feature vectors, each vector representing the information in a small time window of the signal. Let’s see how to convert a raw wavefile to the most commonly used features, sequences of log mel spectrum vectors. A speech signal processing course is recommended for more details.

# 16.2.1 Sampling and Quantization

The input to a speech recognizer is a complex series of changes in air pressure. These changes in air pressure obviously originate with the speaker and are caused by the specific way that air passes through the glottis and out the oral or nasal cavities. We represent sound waves by plotting the change in air pressure over time. One metaphor which sometimes helps in understanding these graphs is that of a vertical plate blocking the air pressure waves (perhaps in a microphone in front of a speaker’s mouth, or the eardrum in a hearer’s ear). The graph measures the amount of compression or rarefaction (uncompression) of the air molecules at this plate. Figure 16.2 shows a short segment of a waveform taken from the Switchboard corpus of telephone speech of the vowel [iy] from someone saying “she just had a baby”.

![## Image Analysis: 47c8b8fa00078de4d1a47fc025e701b60748ab946def1c0579fbd1e56bd3ea43.jpg

**Conceptual Understanding:**
The image conceptually represents an analog sound signal, specifically the acoustic waveform of the vowel [iy]. Its main purpose is to visually illustrate how air pressure changes over a short period when a specific vowel sound is produced. The key idea being communicated is the periodic nature of vowel sounds, characterized by regular, repeating patterns of air pressure fluctuations above and below normal atmospheric pressure. It also demonstrates the time-domain representation of a sound signal, showing amplitude variations as a function of time.

**Content Interpretation:**
The image shows a time-domain waveform of a speech sound, specifically identified as an instance of the vowel [iy]. It illustrates the fluctuation of air pressure over time when this vowel is produced. The waveform's oscillating pattern around the zero-pressure line represents the compressions and rarefactions of air that constitute the sound. The clear periodicity of the wave, with regularly repeating patterns, is a key characteristic of voiced sounds like vowels. The y-axis quantifies the amplitude of these pressure changes, while the x-axis quantifies the duration of the sound segment shown.

**Key Insights:**
The main takeaway from this image is the visual demonstration of the periodic nature of vowel sounds in the time domain. This periodicity is crucial for understanding speech acoustics. The specific amplitude values (0.02283, -0.01697) on the y-axis and the time duration (0.03875 s) on the x-axis provide quantitative insights into the intensity and duration of the captured sound segment. The regular crests and troughs of the waveform, explicitly confirmed by the document's text 'Notice that the wave repeats regularly,' indicate a fundamental frequency component, which is a defining characteristic of voiced speech. This visual evidence directly supports the understanding of how continuous analog signals are represented before digital processing.

**Document Context:**
This image is highly relevant to the document section '16.2.1 Sampling and Quantization' as it provides a concrete, visual example of the analog signal (a continuous sound wave) that these processes apply to. By displaying a real-world instance of a vowel waveform, it effectively sets the stage for understanding why sampling (discretizing time) and quantization (discretizing amplitude) are necessary to convert such continuous analog signals into digital data for processing and storage. The visual evidence of the wave's regular repetition directly supports the introductory text, which notes that 'the wave repeats regularly,' thereby linking the image's content to the broader discussion of speech signal analysis.

**Summary:**
The image displays a waveform graph, specifically illustrating a portion of an instance of the vowel [iy]. The graph is set within a coordinate system with a clearly defined x-axis representing 'Time (s)' (time in seconds) and a y-axis representing the level of air pressure, indicated by numerical values. The waveform itself is a continuous, oscillating line that fluctuates above and below a central horizontal dotted line, which represents the zero level of air pressure (normal atmospheric pressure). The y-axis ranges from a minimum of -0.01697 to a maximum of 0.02283. The x-axis starts at 0 and extends to 0.03875 seconds, capturing a brief segment of the sound. The waveform exhibits a distinct, regularly repeating pattern, signifying the periodic nature of the vowel sound. Each cycle of the wave represents a single oscillation of air pressure. This visual representation allows readers to understand the physical characteristics of sound waves, particularly the amplitude and frequency components over time, which are crucial for comprehending concepts like sampling and quantization.](images/47c8b8fa00078de4d1a47fc025e701b60748ab946def1c0579fbd1e56bd3ea43.jpg)
Figure 16.2 A waveform of an instance of the vowel [iy] (the last vowel in the word “baby”). The $y .$ -axis shows the level of air pressure above and below normal atmospheric pressure. The $x \cdot$ -axis shows time. Notice that the wave repeats regularly.

# sampling

# Nyquist frequency

The first step in digitizing a sound wave like Fig. 16.2 is to convert the analog representations (first air pressure and then analog electric signals in a microphone) into a digital signal. This analog-to-digital conversion has two steps: sampling and quantization. To sample a signal, we measure its amplitude at a particular time; the sampling rate is the number of samples taken per second. To accurately measure a wave, we must have at least two samples in each cycle: one measuring the positive part of the wave and one measuring the negative part. More than two samples per cycle increases the amplitude accuracy, but fewer than two samples causes the frequency of the wave to be completely missed. Thus, the maximum frequency wave that can be measured is one whose frequency is half the sample rate (since every cycle needs two samples). This maximum frequency for a given sampling rate is called the Nyquist frequency. Most information in human speech is in frequencies below $1 0 { , } 0 0 0 ~ \mathrm { H z }$ ; thus, a $2 0 { , } 0 0 0 ~ \mathrm { H z }$ sampling rate would be necessary for complete accuracy. But telephone speech is filtered by the switching network, and only frequencies less than $4 { , } 0 0 0 ~ \mathrm { H z }$ are transmitted by telephones. Thus, an $8 { , } 0 0 0 ~ \mathrm { H z }$ sampling rate is sufficient for telephone-bandwidth speech like the Switchboard corpus, while $1 6 { , } 0 0 0 \mathrm { H z }$ sampling is often used for microphone speech.

quantization

Although using higher sampling rates produces higher ASR accuracy, we can’t combine different sampling rates for training and testing ASR systems. Thus if we are testing on a telephone corpus like Switchboard (8 KHz sampling), we must downsample our training corpus to $8 ~ \mathrm { K H z }$ . Similarly, if we are training on multiple corpora and one of them includes telephone speech, we downsample all the wideband corpora to 8Khz.

Amplitude measurements are stored as integers, either 8 bit (values from -128– 127) or 16 bit (values from -32768–32767). This process of representing real-valued numbers as integers is called quantization; all values that are closer together than the minimum granularity (the quantum size) are represented identically. We refer to each sample at time index $n$ in the digitized, quantized waveform as $x [ n ]$ .

Once data is quantized, it is stored in various formats. One parameter of these formats is the sample rate and sample size discussed above; telephone speech is often sampled at $8 \ \mathrm { k H z }$ and stored as 8-bit samples, and microphone data is often sampled at $1 6 \mathrm { k H z }$ and stored as 16-bit samples. Another parameter is the number of

# channel

# PCM

channels. For stereo data or for two-party conversations, we can store both channels in the same file or we can store them in separate files. A final parameter is individual sample storage—linearly or compressed. One common compression format used for telephone speech is $\mu$ -law (often written u-law but still pronounced mu-law). The intuition of log compression algorithms like $\mu$ -law is that human hearing is more sensitive at small intensities than large ones; the log represents small values with more faithfulness at the expense of more error on large values. The linear (unlogged) values are generally referred to as linear PCM values (PCM stands for pulse code modulation, but never mind that). Here’s the equation for compressing a linear PCM sample value $x$ to 8-bit $\mu$ -law, (where $\mu { = } 2 5 5$ for 8 bits):

$$
\begin{array} { r } { F ( x ) \ = \ \frac { \mathrm { s g n } ( x ) \log ( 1 + \mu \left| x \right| ) } { \log ( 1 + \mu ) } \quad - 1 \leq x \leq 1 } \end{array}
$$

There are a number of standard file formats for storing the resulting digitized wavefile, such as Microsoft’s .wav and Apple’s AIFF all of which have special headers; simple headerless “raw” files are also used. For example, the .wav format is a subset of Microsoft’s RIFF format for multimedia files; RIFF is a general format that can represent a series of nested chunks of data and control information. Figure 16.3 shows a simple .wav file with a single data chunk together with its format chunk.

![## Image Analysis: 58265b29278ec34c2456d9ab06f26010dd81295887e5a0804205c0a5d75e9566.jpg

**Conceptual Understanding:**
The image conceptually represents the architectural layout or blueprint of a Microsoft wavefile header. Its main purpose is to visually define the sequence and nature of the metadata fields that precede the actual audio data in a WAV file. It conveys the key idea that a standard wavefile header is composed of several distinct, ordered fields (chunks and sub-chunks) which collectively provide all the necessary information to interpret and play the subsequent audio content, such as its length, format, compression, channel count, sampling rate, and bit depth.

**Content Interpretation:**
The image illustrates the byte-level structure of a Microsoft wavefile header. It details the various fields that make up this header, each providing critical metadata about the audio content. The 'RIFF' and 'WAVEfmt_' sections are fundamental for identifying the file type and describing the audio format. Specifically, 'RIFF' identifies the file as a Resource Interchange File Format, and the subsequent 'length' field provides the total size of the file, excluding the 'RIFF' identifier and its own length field. The 'WAVEfmt_' section contains the core audio parameters: 'Format chunk / data length (16)' specifies the size of the format information itself, commonly 16 bytes for PCM. 'Compression type' indicates the audio encoding (e.g., PCM). '# channels' specifies if it's mono (1), stereo (2), etc. 'Sampling rate' (e.g., 44100 Hz) defines the number of samples per second. 'bytes/second' gives the average data transfer rate. 'bytes/sample' is the size of one audio sample across all channels. 'bits/channel' indicates the resolution of each sample. Finally, the 'data' block marks the beginning of the actual audio data, with 'Data length' indicating its size. All extracted text elements directly label and define these specific components, thereby collectively detailing the comprehensive structure and content definition of a wavefile header.

**Key Insights:**
The main takeaway from this image is a detailed understanding of the Microsoft wavefile header's structure. It highlights that a wavefile header is organized into distinct chunks ('RIFF', 'WAVEfmt_', 'data') each containing specific metadata. Key insights include: 1. The 'RIFF' chunk establishes the file type and provides overall file length. 2. The 'WAVEfmt_' chunk is essential for defining the audio's technical specifications such as 'Compression type', '# channels', 'Sampling rate', 'bytes/second', 'bytes/sample', and 'bits/channel'. These parameters are critical for correct audio playback and processing. 3. The 'data' chunk's 'Data length' field specifies the size of the actual audio information. The verbatim transcription provides the exact labels for these parameters, demonstrating the granular information required to correctly parse and interpret a wavefile, emphasizing the importance of precise metadata for digital audio.

**Document Context:**
This image is presented in the 'PCM' section of the document and is explicitly referred to as 'Figure 16.3 Microsoft wavefile header format, assuming simple file with one chunk.' This context indicates that the image serves to visually explain the foundational structure of a WAV audio file's header, particularly for Pulse Code Modulation (PCM) audio, which is a common uncompressed digital audio format. Understanding this header is crucial for processing, playing, or manipulating WAV files, as it contains all necessary metadata (like sampling rate, number of channels, bit depth) that dictates how the subsequent audio data chunk should be interpreted. The detailed transcription of each field helps a reader understand the exact byte-level layout of this critical file metadata.

**Summary:**
The image presents a detailed diagram illustrating the Microsoft wavefile header format for a simple file with a single chunk. The header is depicted as a series of concatenated blocks, each representing a specific part of the file's metadata, crucial for interpreting the audio data that follows. It begins with the 'RIFF' identifier, followed by a 'length' field indicating the overall file size. The next major section is 'WAVEfmt_', which contains parameters defining the audio format. Within this 'WAVEfmt_' section, there's a 'Format chunk / data length (16)' field, specifying the size of the format chunk itself. This is followed by 'Compression type', indicating how the audio is encoded. Then, the number of audio channels is specified by '# channels', and the 'Sampling rate' defines how many samples are taken per second. Further details include 'bytes/second', representing the average data rate, 'bytes/sample', indicating the size of each audio sample, and 'bits/channel', which specifies the bit depth per channel. The header concludes with a 'data' block, which has an associated 'Data length' field, denoting the size of the actual audio data payload.](images/58265b29278ec34c2456d9ab06f26010dd81295887e5a0804205c0a5d75e9566.jpg)
Figure 16.3 Microsoft wavefile header format, assuming simple file with one chunk. Following this 44-byte header would be the data chunk.

# 16.2.2 Windowing

stationary non-stationary

From the digitized, quantized representation of the waveform, we need to extract spectral features from a small window of speech that characterizes part of a particular phoneme. Inside this small window, we can roughly think of the signal as stationary (that is, its statistical properties are constant within this region). (By contrast, in general, speech is a non-stationary signal, meaning that its statistical properties are not constant over time). We extract this roughly stationary portion of speech by using a window which is non-zero inside a region and zero elsewhere, running this window across the speech signal and multiplying it by the input waveform to produce a windowed waveform.

stride

The speech extracted from each window is called a frame. The windowing is characterized by three parameters: the window size or frame size of the window (its width in milliseconds), the frame stride, (also called shift or offset) between successive windows, and the shape of the window.

To extract the signal we multiply the value of the signal at time $n , s [ n ]$ by the value of the window at time $n , w [ n ]$ :

$$
y [ n ] = w [ n ] s [ n ]
$$

The window shape sketched in Fig. 16.4 is rectangular; you can see the extracted windowed signal looks just like the original signal. The rectangular window, however, abruptly cuts off the signal at its boundaries, which creates problems when we do Fourier analysis. For this reason, for acoustic feature creation we more commonly use the Hamming window, which shrinks the values of the signal toward zero at the window boundaries, avoiding discontinuities. Figure 16.5 shows both; the equations are as follows (assuming a window that is $L$ frames long):

![## Image Analysis: 3ff9826f7a2ccd7cb2f8dba0f3f2f2e9c9331b81f28925c1efa49c70ce1d8a06.jpg

**Conceptual Understanding:**
This image conceptually represents the process of applying a 'window' to a continuous signal to extract and analyze smaller, discrete segments. The main purpose is to visually explain how a fixed-size window is shifted across a signal over time, illustrating the parameters of window length and stride (or overlap). The image communicates the idea of segmenting a signal for localized analysis, particularly highlighting the use of overlapping windows.

**Content Interpretation:**
The image illustrates the process of 'windowing' a continuous signal, a technique commonly used in signal processing to analyze segments of a longer waveform. It shows how a fixed-duration window (25 ms) is moved across the signal with a specified step size or stride (10 ms), resulting in overlapping segments. The zoomed-in section highlights the detail captured within a single window.

**Key Insights:**
The main takeaway from this image is the practical application of windowing with specific parameters. It demonstrates that: 1. A 'Window' defines a specific duration (25 ms) of the signal to be analyzed. 2. A 'Shift' (or stride) determines the amount by which the window moves forward (10 ms) for the next analysis segment. 3. Windowing typically results in overlapping segments when the shift is less than the window duration (10 ms shift < 25 ms window). 4. This process allows for detailed, sequential analysis of a continuous signal by breaking it into manageable, overlapping chunks. The text "Window 25 ms" and "Shift 10 ms" are crucial evidence for these insights, defining the parameters of the windowing operation.

**Document Context:**
This image directly supports and visually explains the concept of "Windowing" as discussed in Section 16.2.2 of the document. It clarifies the terms "25 ms rectangular window" and "10ms stride" mentioned in the text following the image (Figure 16.4), by showing the visual application of these parameters to a representative signal. This enhances the reader's understanding of how windowing is practically implemented and its effect on a signal.

**Summary:**
The image illustrates the concept of windowing a signal, specifically demonstrating a 25 ms rectangular window applied with a 10 ms stride. At the top of the image, a continuous, complex waveform is depicted, representing the original signal. Below this main signal, a series of three blue rectangular boxes, each labeled "Window" and "25 ms", are shown. These boxes represent consecutive applications of a 25-millisecond window to the signal. Each subsequent window is preceded by a green rectangular box labeled "Shift" and "10 ms", indicating that the start of the next 25 ms window is shifted by 10 milliseconds from the start of the previous window. Dotted vertical lines connect the beginning and end of each "Window 25 ms" block to the corresponding segments of the main waveform above, visually defining the portion of the signal being captured by each window. On the right side of the image, a dashed line connects the third "Window 25 ms" block to a magnified view of that specific 25 ms segment of the waveform, enclosed in a black rectangular box. This zoomed-in view provides a clearer representation of the signal content within a single window. The overall diagram clearly demonstrates how a signal is segmented into overlapping windows for analysis, with specified window length and shift (stride) parameters.](images/3ff9826f7a2ccd7cb2f8dba0f3f2f2e9c9331b81f28925c1efa49c70ce1d8a06.jpg)
Figure 16.4 Windowing, showing a $2 5 \mathrm { m s }$ rectangular window with a 10ms stride.

$$
\begin{array} { l l l } { { g u l a r } } & { { \quad w [ n ] ~ = ~ \left\{ \begin{array} { l l } { { 1 } } & { { ~ 0 \leq n \leq L - 1 ~ } } \\ { { 0 } } & { { \mathrm { o t h e r w i s e } } } \end{array} \right. } } \\ { { \imath m i n g } } & { { \quad w [ n ] ~ = ~ \left\{ \begin{array} { l l } { { 0 . 5 4 - 0 . 4 6 \cos ( \frac { 2 \pi n } { L } ) } } & { { ~ 0 \leq n \leq L - 1 ~ } } \\ { { 0 } } & { { \mathrm { o t h e r w i s e } } } \end{array} \right. } } \end{array}
$$

![## Image Analysis: a5d140acca42bfb8ed98a288a7f60aa6f014b783e32cefe8d09515795de325a6.jpg

**Conceptual Understanding:**
This image conceptually represents the process of 'windowing' a time-domain signal, specifically a sine wave, and illustrates the different outcomes when applying two common window functions: the rectangular window and the Hamming window. The main purpose is to visually demonstrate the effect of these window functions on a signal segment, highlighting the key difference in how they handle the signal's boundaries. The image communicates the idea that windowing involves isolating a portion of a longer signal, and that the choice of window function significantly impacts the characteristics of the isolated segment, particularly regarding its continuity at the edges.

**Content Interpretation:**
The image displays a series of three 2D line graphs illustrating the application of windowing functions to a sine wave. The top graph shows the original, continuous sine wave over a larger time domain, with a segment highlighted in blue. This highlighted segment is then processed by two different windowing techniques, shown in the two lower graphs. The bottom-left graph depicts the effect of a 'Rectangular window', which extracts the segment with sharp, unaltered boundaries. The bottom-right graph shows the effect of a 'Hamming window', which extracts the same segment but tapers its amplitude smoothly to zero at the edges. The key concepts demonstrated are signal windowing, the difference between abrupt and smooth windowing functions, and their visual impact on a waveform. The graphs show the amplitude (from approximately -0.5 to 0.4999) versus time (in seconds). For the full sine wave, time ranges from 0 to 0.0475896 seconds. For the windowed segments, time ranges from 0.00455938 seconds to 0.0256563 seconds. The Hamming window's minimum amplitude is slightly different at -0.4826, indicating the tapering effect.

**Key Insights:**
The main takeaway from this image is the distinct visual effect of applying a rectangular versus a Hamming window to a segment of a sine wave. The 'Rectangular window' operation results in an abrupt truncation of the signal, meaning the extracted segment starts and ends at its original amplitude values, as evidenced by the sharp cut-offs at '0.00455938' and '0.0256563' on the time axis, and peak amplitudes reaching '0.4999' and minimums at '-0.5'. In contrast, the 'Hamming window' applies a smoothing function, gradually reducing the amplitude of the signal towards zero at its edges, as seen by the waveform tapering off at the start and end of the '0.00455938' to '0.0256563' time interval, and the slightly reduced minimum amplitude of '-0.4826'. This visual comparison highlights that while both windows extract a segment, the Hamming window is designed to minimize spectral leakage by avoiding sharp discontinuities at the segment boundaries, a critical concept in signal analysis.

**Document Context:**
This image is directly relevant to Section '16.2.2 Windowing' of the document. It serves as a visual aid to explain and differentiate between two fundamental windowing techniques: the rectangular window and the Hamming window. By showing the effect of these windows on a simple sine wave, the image helps readers understand the practical implications and visual outcomes of applying different window functions, which is crucial for comprehending signal processing concepts such as spectral leakage and side lobes. The visual comparison provides concrete examples to support the theoretical discussion in the text, illustrating why different window functions might be chosen based on the desired characteristics of the extracted signal.

**Summary:**
This image visually demonstrates the concept of windowing a sine wave using two different window functions: the rectangular window and the Hamming window. The top graph displays a continuous sine wave over an extended time period. A specific segment of this sine wave is highlighted in light blue, indicating the portion of the signal that will be 'windowed'. This highlighted segment serves as the input for the two windowing operations shown in the bottom graphs. The bottom-left graph shows the result of applying a 'Rectangular window' to the highlighted segment. This operation effectively isolates the segment, cutting off the signal abruptly at its start and end points without modifying the amplitude within the window. The bottom-right graph illustrates the effect of applying a 'Hamming window' to the same highlighted segment. In contrast to the rectangular window, the Hamming window smoothly tapers the amplitude of the signal towards zero at both its beginning and end, thereby reducing sharp discontinuities. All graphs share similar amplitude ranges, generally from -0.5 to 0.4999, while the time axis for the windowed segments ranges from 0.00455938 s to 0.0256563 s, clearly showing the localized effect of windowing.](images/a5d140acca42bfb8ed98a288a7f60aa6f014b783e32cefe8d09515795de325a6.jpg)
Figure 16.5 Windowing a sine wave with the rectangular or Hamming windows.

# 16.2.3 Discrete Fourier Transform

The next step is to extract spectral information for our windowed signal; we need to know how much energy the signal contains at different frequency bands. The tool for extracting spectral information for discrete frequency bands for a discrete-time (sampled) signal is the discrete Fourier transform or DFT.

The input to the DFT is a windowed signal $x [ n ] . . . x [ m ]$ , and the output, for each of $N$ discrete frequency bands, is a complex number $X [ k ]$ representing the magnitude and phase of that frequency component in the original signal. If we plot the magnitude against the frequency, we can visualize the spectrum (see Appendix H for more on spectra). For example, Fig. 16.6 shows a $2 5 ~ \mathrm { m s }$ Hamming-windowed portion of a signal and its spectrum as computed by a DFT (with some additional smoothing).

![## Image Analysis: 93982cac835b43c7462c36787414d1e48294acc02faa71d8e0d31ee799dfc4ce.jpg

**Conceptual Understanding:**
The image conceptually represents the transformation of a signal from the time domain to the frequency domain using the Discrete Fourier Transform (DFT). Its main purpose is to visually demonstrate how a segment of a complex, real-world signal (specifically an audio signal of a vowel) can be analyzed to reveal its constituent frequency components and their relative strengths. It communicates the key idea that while a signal's amplitude changes over time, the DFT allows us to understand its underlying spectral characteristics, which are often more insightful for analysis in fields like acoustics or signal processing. The two plots effectively contrast the temporal evolution of the signal with its static frequency fingerprint.

**Content Interpretation:**
The image presents a time-domain signal and its corresponding frequency-domain spectrum. Plot (a) shows the waveform of a signal (likely an audio signal, given the 'Sound pressure level' in plot (b)) as a function of time. The signal's amplitude varies between -0.04121 and 0.04414 over a time window from 0.0141752 s to 0.039295 s. This waveform suggests a complex, non-pure tone signal. Plot (b) depicts the Discrete Fourier Transform (DFT) of the signal shown in (a). The y-axis, 'Sound pressure level (dB/Hz)', indicates the intensity of different frequency components. The x-axis, 'Frequency (Hz)', shows the range of frequencies from 0 Hz to 8000 Hz. The spectrum reveals the frequency composition of the signal. The peaks in the spectrum correspond to the dominant frequencies present in the original time-domain signal. The presence of multiple peaks across a broad frequency range (e.g., between 1000 Hz and 4000 Hz) indicates that the signal is rich in harmonics or contains multiple distinct frequency components, which is characteristic of complex sounds like speech or music. The values 0.0141752 s and 0.039295 s for the time axis in (a) define the specific 25 ms window (0.039295 - 0.0141752 = 0.02512) from which the signal was extracted, as mentioned in the document context. The maximum frequency of 8000 Hz on the x-axis in (b) sets the analysis bandwidth.

**Key Insights:**
The main takeaway from this image is the visualization of the Discrete Fourier Transform (DFT)'s role in converting a time-domain signal into its frequency-domain representation. From plot (a), we understand that the signal is a complex, oscillating waveform over a specific time window (0.0141752 s to 0.039295 s). From plot (b), labeled 'Sound pressure level (dB/Hz)' versus 'Frequency (Hz)', we learn that this complex time-domain signal is composed of multiple dominant frequency components, indicated by the various peaks in the spectrum up to 8000 Hz. This highlights that signals, especially those like speech (as suggested by 'vowel [iy]' in the context), are not simple pure tones but rather a combination of frequencies. The image provides evidence that DFT effectively decomposes such signals, allowing for the identification and analysis of their underlying frequency content, which is essential for tasks like speech recognition, audio processing, and acoustic analysis. The contrast between the jumbled appearance of the signal in the time domain and the structured peaks in the frequency domain (specifically the presence of prominent peaks between 1000 Hz and 4000 Hz) underscores the power of Fourier analysis in revealing hidden patterns.

**Document Context:**
This image directly supports Section 16.2.3, which discusses the Discrete Fourier Transform. Figure 16.6 (a) provides a concrete example of a time-domain signal, specifically a 25 ms Hamming-windowed portion of a signal from the vowel [iy]. Figure 16.6 (b) then illustrates the result of applying the Discrete Fourier Transform to this signal, showing its spectrum. This visual representation is crucial for understanding how a signal's properties, which are evident in the time domain (amplitude over time), are transformed and represented in the frequency domain (sound pressure level at different frequencies). The image demonstrates the practical application of DFT in analyzing speech or audio signals, helping readers grasp the theoretical concepts presented in the text by showing a real-world example of spectral analysis.

**Summary:**
This image displays two plots side-by-side, labeled (a) and (b), illustrating a signal in the time domain and its corresponding spectrum in the frequency domain, computed using a Discrete Fourier Transform. Plot (a) shows the sound pressure level of a signal over a short duration of time. The y-axis, which is unlabeled, ranges from approximately -0.04121 to 0.04414, centered around 0. The x-axis is labeled 'Time (s)' and spans from 0.0141752 seconds to 0.039295 seconds. The signal depicted is an oscillating waveform with varying amplitude, starting near zero, increasing in amplitude, and then decreasing again. Plot (b) presents the spectrum of this signal. The y-axis is labeled 'Sound pressure level (dB/Hz)' and ranges from -20 dB/Hz to over 20 dB/Hz, with a clear marking at 0 dB/Hz. The x-axis is labeled 'Frequency (Hz)' and ranges from 0 Hz to 8000 Hz. The spectrum shows several peaks, indicating dominant frequencies in the signal. Notably, there is a strong peak near 0 Hz, and then several prominent peaks appear across the frequency range, particularly a broad region of activity between approximately 1000 Hz and 4000 Hz, with amplitude fluctuations. The plot overall demonstrates the transformation of a complex time-domain signal into its constituent frequencies and their respective sound pressure levels.](images/93982cac835b43c7462c36787414d1e48294acc02faa71d8e0d31ee799dfc4ce.jpg)
Figure 16.6 (a) A 25 ms Hamming-windowed portion of a signal from the vowel [iy] and (b) its spectrum computed by a DFT.

We do not introduce the mathematical details of the DFT here, except to note that Fourier analysis relies on Euler’s formula, with $j$ as the imaginary unit:

$$
e ^ { j \theta } = \cos \theta + j \sin \theta
$$

As a brief reminder for those students who have already studied signal processing, the DFT is defined as follows:

$$
X [ k ] = \sum _ { n = 0 } ^ { N - 1 } x [ n ] e ^ { - j { \frac { 2 \pi } { N } } k n }
$$

fast Fourier transform FFT

A commonly used algorithm for computing the DFT is the fast Fourier transform or FFT. This implementation of the DFT is very efficient but only works for values of $N$ that are powers of 2.

# 16.2.4 Mel Filter Bank and Log

The results of the FFT tell us the energy at each frequency band. Human hearing, however, is not equally sensitive at all frequency bands; it is less sensitive at higher frequencies. This bias toward low frequencies helps human recognition, since information in low frequencies (like formants) is crucial for distinguishing vowels or nasals, while information in high frequencies (like stop bursts or fricative noise) is less crucial for successful recognition. Modeling this human perceptual property improves speech recognition performance in the same way.

mel

We implement this intuition by collecting energies, not equally at each frequency band, but according to the mel scale, an auditory frequency scale. A mel (Stevens et al. 1937, Stevens and Volkmann 1940) is a unit of pitch. Pairs of sounds that are perceptually equidistant in pitch are separated by an equal number of mels. The mel frequency $m$ can be computed from the raw acoustic frequency by a log transformation:

$$
m e l ( f ) = 1 1 2 7 \ln ( 1 + \frac { f } { 7 0 0 } )
$$

We implement this intuition by creating a bank of filters that collect energy from each frequency band, spread logarithmically so that we have very fine resolution at low frequencies, and less resolution at high frequencies. Figure 16.7 shows a sample bank of triangular filters that implement this idea, that can be multiplied by the spectrum to get a mel spectrum.

![## Image Analysis: 9c886a0d1f20486e5b671417f1b38b8380dc5127ab3576af960902f26d61398d.jpg

**Conceptual Understanding:**
Conceptually, this image illustrates the Mel Filter Bank, a crucial component in audio processing, particularly in the Mel-Frequency Cepstral Coefficients (MFCC) feature extraction. The main purpose is to show how the continuous frequency spectrum is transformed into a discrete mel spectrum, mimicking the non-linear human perception of sound frequency. It highlights that filters are designed to be more detailed at lower frequencies and less detailed at higher frequencies, which aligns with human auditory perception. Key ideas conveyed include the frequency-domain representation (Frequency (Hz) as x-axis, Amplitude as y-axis), the use of triangular filters, the implied logarithmic spacing of these filters, and the resulting mel spectrum output (m₁, m₂, ..., mM).

**Content Interpretation:**
The image shows the application of a filter bank where each triangular filter conceptually "collects energy" from a specific frequency range. The resulting aggregated energy for each filter contributes to a component in the `mel spectrum`. The primary graph plots Amplitude (0 to 1) against Frequency (0 to 8K Hz), defining the characteristics of the filters, with a peak amplitude of 1 suggesting a normalized gain. The non-uniform spacing of the triangular filters, being narrow and closely packed at lower frequencies and becoming wider and more spread out at higher frequencies, is direct evidence of the "mel scale" and its perceptual properties. The horizontal bar labeled "mel spectrum" with components `m₁`, `m₂`, `...`, `mM` represents the output of this filtering process, where each `m` value corresponds to the weighted sum of the energy from a specific frequency band. The dashed arrows explicitly demonstrate the relationship, indicating that specific frequency ranges (or the energy collected by filters in those ranges) are mapped to individual components of the "mel spectrum", thus signifying a transformation from a continuous frequency representation to a discrete, perceptually-scaled representation.

**Key Insights:**
1. **Mel filters are triangular:** The image clearly shows numerous blue triangular shapes as the filters, each reaching an "Amplitude" of 1 at its peak. This shape is a defining characteristic of mel filter banks. 2. **Mel filters are not uniformly spaced on a linear frequency scale:** Observing the "Frequency (Hz)" axis from "0" to "8K", the bases of the triangular filters are much closer together at the "0" end and progressively widen as they approach "8K". This visual distribution directly supports the concept of logarithmic spacing in the mel scale, making the system more sensitive to lower frequencies, similar to human hearing. 3. **The mel filter bank transforms the frequency domain into a mel spectrum:** The lower section explicitly labels the output as "mel spectrum" with components `m₁`, `m₂`, ..., `mM`. The dashed arrows clearly illustrate this mapping: inputs from specific frequency bands (filtered by triangular filters) contribute to corresponding `m` values. 4. **The output of the mel filter bank is a set of M coefficients:** The labels `m₁`, `m₂`, `...`, `mM` indicate that the mel spectrum consists of `M` distinct values, each representing the energy in a specific mel-frequency band. The "..." implies that `M` can be a significant number of coefficients.

**Document Context:**
As indicated by the section title "16.2.4 Mel Filter Bank and Log" and the text after the image "Figure 16.7 The mel filter bank (Davis and Mermelstein, 1980). Each triangular filter, spaced logarithmically along the mel scale, collects energy from a given frequency range.", this image is central to understanding the architecture and function of the mel filter bank. It visually grounds the theoretical explanation of how sound frequencies are processed to extract perceptually relevant features, which is fundamental to topics like Mel-Frequency Cepstral Coefficients (MFCCs) used in speech recognition and audio analysis. The image specifically illustrates the "mel filter bank" referred to in the surrounding text and provides the visual evidence for the "triangular filter" and "logarithmically spaced" descriptions.

**Summary:**
The image displays a conceptual diagram of a mel filter bank, a fundamental component in audio signal processing that transforms sound frequencies into a perceptually-weighted representation. The upper part of the diagram is a two-dimensional graph. The vertical axis is labeled "Amplitude" with numerical ticks at "0", "0.5", and "1". The horizontal axis is labeled "Frequency (Hz)" with ticks at "0" and "8K" (representing 8000 Hz). Plotted across this frequency range are numerous blue triangular shapes, representing individual filters in the mel filter bank. Each triangular filter starts at an amplitude of 0, rises to a peak amplitude of 1, and then returns to an amplitude of 0. A critical visual characteristic is the spacing of these triangular filters along the "Frequency (Hz)" axis. At lower frequencies (closer to "0 Hz"), the filters are narrow and tightly packed together, overlapping significantly. As the frequency increases, the filters become progressively wider and more spread out, indicating a non-linear, specifically logarithmic, spacing across the frequency spectrum. This non-uniform distribution reflects the human auditory system's higher sensitivity to changes in lower frequencies compared to higher frequencies. Below this graph, a horizontal bar is labeled "mel spectrum". This bar is divided into several colored segments: a blue segment labeled "m₁", a green segment labeled "m₂", an ellipsis "...", and a final red segment labeled "mM". Dashed arrows connect the frequency axis of the main graph to these segments in the "mel spectrum" bar. Specifically, the filters covering the initial, dense low-frequency range are linked via a dashed arrow to the "m₁" segment. A filter in an intermediate frequency range is linked to the "m₂" segment. The filters covering the high-frequency range, extending up to "8K Hz", are linked via a dashed arrow to the "mM" segment. This entire diagram illustrates that the mel filter bank takes the raw frequency information, processes it through these perceptually-weighted triangular filters, and then generates a simplified "mel spectrum" composed of M coefficients (m₁, m₂, ..., mM). Each m coefficient represents the aggregated energy within a specific frequency band, weighted by the corresponding triangular filter, thus providing a representation of sound that aligns more closely with human auditory perception.](images/9c886a0d1f20486e5b671417f1b38b8380dc5127ab3576af960902f26d61398d.jpg)
Figure 16.7 The mel filter bank (Davis and Mermelstein, 1980). Each triangular filter, spaced logarithmically along the mel scale, collects energy from a given frequency range.

Finally, we take the log of each of the mel spectrum values. The human response to signal level is logarithmic (like the human response to frequency). Humans are less sensitive to slight differences in amplitude at high amplitudes than at low amplitudes. In addition, using a log makes the feature estimates less sensitive to variations in input such as power variations due to the speaker’s mouth moving closer or further from the microphone.

# 16.3 Speech Recognition Architecture

The basic architecture for ASR is the encoder-decoder (implemented with either RNNs or Transformers), exactly the same architecture introduced for MT in Chapter 13. Generally we start from the log mel spectral features described in the previous section, and map to letters, although it’s also possible to map to induced morphemelike chunks like wordpieces or BPE.

Fig. 16.8 sketches the standard encoder-decoder architecture, which is commonly referred to as the attention-based encoder decoder or AED, or listen attend and spell (LAS) after the two papers which first applied it to speech (Chorowski et al. 2014, Chan et al. 2016). The input is a sequence of $t$ acoustic feature vectors $F = f _ { 1 } , f _ { 2 } , . . . , f _ { t }$ , one vector per $1 0 ~ \mathrm { m s }$ frame. The output can be letters or wordpieces; we’ll assume letters here. Thus the output sequence $Y = \left( \langle \mathrm { S O S } \rangle , y _ { 1 } , . . . , y _ { m } \langle \mathrm { E O S } \rangle \right)$ , assuming special start of sequence and end of sequence tokens $\langle \mathrm { s o s } \rangle$ and $\left. \mathrm { e o s } \right.$ and each $y _ { i }$ is a character; for English we might choose the set:

$$
y _ { i } \in \{ a , b , c , . . . , z , 0 , . . . . , 9 , \langle \mathrm { s p a c e } \rangle , \langle \mathrm { c o m m a } \rangle , \langle \mathrm { p e r i o d } \rangle , \langle \mathrm { a p o s t r o p h e } \rangle , \langle \mathrm { u n k } \rangle \}
$$

Of course the encoder-decoder architecture is particularly appropriate when input and output sequences have stark length differences, as they do for speech, with very long acoustic feature sequences mapping to much shorter sequences of letters or words. A single word might be 5 letters long but, supposing it lasts about 2 seconds, would take 200 acoustic frames (of 10ms each).

![## Image Analysis: de4cc7823354f2598690593818f454ee269184d78926d9c5be2b3f3a119c7319.jpg

**Conceptual Understanding:**
This image represents a high-level conceptual architecture for an encoder-decoder speech recognizer. Its main purpose is to illustrate the sequential data flow and the functional modules involved in transforming an audio input into a textual output. The key idea communicated is the end-to-end process of how speech is first processed into features, encoded into a meaningful representation, and then decoded into a sequence of characters that form the recognized spoken words. This architecture is central to understanding neural network-based speech recognition systems.

**Content Interpretation:**
The image displays the architecture of an encoder-decoder model specifically designed for speech recognition. It illustrates the transformation of an audio signal into a textual transcription through a series of computational steps. The 'Feature Computation' and 'Subsampling' modules prepare the raw audio into a suitable input format for the neural network components. The 'ENCODER' module processes the input feature sequence to create a condensed, context-rich representation ('H'). The 'DECODER' then utilizes this context representation 'H' and its own past predictions to sequentially generate the output character sequence, which forms the recognized speech. The significance of the '80-dimensional log Mel spectrum per frame' is that it represents a common and effective acoustic feature used in speech processing, capturing the spectral characteristics of the sound over time. The 'Shorter sequence X' produced by 'Subsampling' suggests a reduction in the temporal resolution of the features, which is often done to manage computational complexity while retaining essential information. The 'H' vector is crucial as it encapsulates the entire input speech utterance's meaning into a fixed-size representation that the decoder can condition its output on. The auto-regressive nature of the 'DECODER' (feeding its own output '<s i t ' s t i m ...' as input) is central to sequence generation models, allowing it to build the output word by word or character by character.

**Key Insights:**
The main takeaways from this image are: 1. Encoder-decoder architectures are a key component of modern speech recognition systems. 2. Speech recognition involves a multi-stage process starting from raw audio, moving through feature extraction, and finally sequence generation. 3. 'Feature Computation' converts raw audio (waveform) into acoustic features like '80-dimensional log Mel spectrum per frame' ('f1 ... ft'). 4. 'Subsampling' is used to reduce the length of the feature sequence, creating a 'Shorter sequence X' ('x1 ... xn'), which helps in managing complexity. 5. The 'ENCODER' processes the input feature sequence ('x1 ... xn') to produce a contextual representation ('H'). 6. The 'DECODER' takes this contextual representation ('H') and its own previous outputs (e.g., '<s i t ' s t i m ...') to generate the final character sequence ('i t ' s t i m e ...'). 7. The system uses a character-level output, predicting individual characters 'y1' through 'ym' to form words. These insights are directly supported by the verbatim transcription of the module names, input/output labels, and specific text elements within the diagram.

**Document Context:**
This image is highly relevant to the document's section '16.3 Speech Recognition Architecture' as it visually represents a fundamental and widely used architectural pattern for modern speech recognition systems. It provides a concrete example of an encoder-decoder structure, which is a cornerstone of sequence-to-sequence learning applied to speech. The diagram helps to explain the flow of information and the roles of different components in converting an audio signal into text, directly supporting the conceptual understanding of how such systems are built and operate. It clarifies the input and output transformations at each stage of the recognition process, making the technical details more accessible to the reader.

**Summary:**
This image illustrates the schematic architecture for an encoder-decoder speech recognizer, detailing the sequence of operations from raw audio input to transcribed text output. The process begins with an audio waveform, which undergoes 'Feature Computation' to extract an '80-dimensional log Mel spectrum per frame'. This spectrum, represented as a sequence 'f1 ... ft', is then passed to 'Subsampling', which reduces it into a 'Shorter sequence X' denoted as 'x1 ... xn'. This shorter sequence 'x1 ... xn' serves as the input to the 'ENCODER'. The 'ENCODER' processes this input and generates a hidden representation 'H', which is then fed into the 'DECODER'. The 'DECODER' receives this hidden representation 'H' from the encoder, along with a sequence of previously predicted characters (or a start-of-sequence token '<s'). This sequence, shown as '<s i t ' s t i m ...', represents the target output shifted by one position, feeding the decoder its own previous output as context. The 'DECODER' then predicts the output character sequence, denoted as 'y1 y2 y3 y4 y5 y6 y7 y8 y9 ym', which corresponds to the transcribed text 'i t ' s t i m e ...'. Essentially, the system takes an audio input, transforms it into a compact feature representation, encodes this into a context vector, and then decodes that vector into a sequence of characters, forming the recognized speech.](images/de4cc7823354f2598690593818f454ee269184d78926d9c5be2b3f3a119c7319.jpg)
Figure 16.8 Schematic architecture for an encoder-decoder speech recognizer.

Because this length difference is so extreme for speech, encoder-decoder architectures for speech need to have a special compression stage that shortens the acoustic feature sequence before the encoder stage. (Alternatively, we can use a loss function that is designed to deal well with compression, like the CTC loss function we’ll introduce in the next section.)

The goal of the subsampling is to produce a shorter sequence $X = x _ { 1 } , . . . , x _ { n }$ that will be the input to the encoder. The simplest algorithm is a method sometimes called low frame rate (Pundak and Sainath, 2016): for time $i$ we stack (concatenate) the acoustic feature vector $f _ { i }$ with the prior two vectors $f _ { i - 1 }$ and $f _ { i - 2 }$ to make a new vector three times longer. Then we simply delete $f _ { i - 1 }$ and $f _ { i - 2 }$ . Thus instead of (say) a 40-dimensional acoustic feature vector every $1 0 \mathrm { m s }$ , we have a longer vector (say 120-dimensional) every $3 0 \mathrm { m s }$ , with a shorter sequence length $\begin{array} { r } { n = \frac { t } { 3 } } \end{array}$

After this compression stage, encoder-decoders for speech use the same architecture as for MT or other text, composed of either RNNs (LSTMs) or Transformers.

For inference, the probability of the output string $Y$ is decomposed as:

$$
p ( y _ { 1 } , . . . , y _ { n } ) = \prod _ { i = 1 } ^ { n } p ( y _ { i } | y _ { 1 } , . . . , y _ { i - 1 } , X )
$$

We can produce each letter of the output via greedy decoding:

$$
\hat { y } _ { i } ~ = ~ \mathrm { a r g m a x } _ { \mathrm { c h a r e } \ : \mathrm { A l p h a b e t } } P ( \mathrm { c h a r } | y _ { 1 } . . . y _ { i - 1 } , X )
$$

Alternatively we can use beam search as described in the next section. This is particularly relevant when we are adding a language model.

Adding a language model Since an encoder-decoder model is essentially a conditional language model, encoder-decoders implicitly learn a language model for the output domain of letters from their training data. However, the training data (speech paired with text transcriptions) may not include sufficient text to train a good language model. After all, it’s easier to find enormous amounts of pure text training data than it is to find text paired with speech. Thus we can can usually improve a model at least slightly by incorporating a very large language model.

The simplest way to do this is to use beam search to get a final beam of hypothesized sentences; this beam is sometimes called an n-best list. We then use a language model to rescore each hypothesis on the beam. The scoring is done by interpolating the score assigned by the language model with the encoder-decoder score used to create the beam, with a weight $\lambda$ tuned on a held-out set. Also, since most models prefer shorter sentences, ASR systems normally have some way of adding a length factor. One way to do this is to normalize the probability by the number of characters in the hypothesis $| Y | _ { c }$ . The following is thus a typical scoring function (Chan et al., 2016):

$$
s c o r e ( Y | X ) = { \frac { 1 } { | Y | _ { c } } } \log P ( Y | X ) + \lambda \log P _ { \mathrm { L } M } ( Y ) 
$$

# 16.3.1 Learning

Encoder-decoders for speech are trained with the normal cross-entropy loss generally used for conditional language models. At timestep $i$ of decoding, the loss is the log probability of the correct token (letter) $y _ { i }$ :

$$
L _ { C E } \ = \ - \log p ( y _ { i } | y _ { 1 } , \ldots , y _ { i - 1 } , X )
$$

The loss for the entire sentence is the sum of these losses:

$$
L _ { C E } \ = \ - \sum _ { i = 1 } ^ { m } \log p ( y _ { i } | y _ { 1 } , . . . , y _ { i - 1 } , X )
$$

This loss is then backpropagated through the entire end-to-end model to train the entire encoder-decoder.

As we described in Chapter 13, we normally use teacher forcing, in which the decoder history is forced to be the correct gold $y _ { i }$ rather than the predicted $\hat { y } _ { i }$ . It’s also possible to use a mixture of the gold and decoder output, for example using the gold output $90 \%$ of the time, but with probability .1 taking the decoder output instead:

$$
L _ { C E } \ = \ - \log p ( y _ { i } | y _ { 1 } , \ldots , \hat { y } _ { i - 1 } , X )
$$

# 16.4 CTC

We pointed out in the previous section that speech recognition has two particular properties that make it very appropriate for the encoder-decoder architecture, where the encoder produces an encoding of the input that the decoder uses attention to explore. First, in speech we have a very long acoustic input sequence $X$ mapping to a much shorter sequence of letters $Y$ , and second, it’s hard to know exactly which part of $X$ maps to which part of Y .

CTC

In this section we briefly introduce an alternative to encoder-decoder: an algorithm and loss function called CTC, short for Connectionist Temporal Classification (Graves et al., 2006), that deals with these problems in a very different way. The intuition of CTC is to output a single character for every frame of the input, so that the output is the same length as the input, and then to apply a collapsing function that combines sequences of identical letters, resulting in a shorter sequence.

Let’s imagine inference on someone saying the word dinner, and let’s suppose we had a function that chooses the most probable letter for each input spectral frame representation $x _ { i }$ . We’ll call the sequence of letters corresponding to each input frame an alignment, because it tells us where in the acoustic signal each letter aligns to. Fig. 16.9 shows one such alignment, and what happens if we use a collapsing function that just removes consecutive duplicate letters.

![## Image Analysis: f8502f4fa259af21f7f6f1d850e021fe286a71dd5562825267f0f1a5826f500a.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of mapping a sequence of input features, derived from a continuous source like a wavefile, to a shorter, meaningful output sequence of characters. Its main purpose is to demonstrate a 'naive algorithm for collapsing an alignment' by showing how multiple input steps can correspond to repeated intermediate characters, which are then consolidated into a final, unique character output. The image primarily communicates the idea of handling temporal variability and redundancy in sequence-to-sequence tasks, particularly in the context of generating text from an input signal.

**Content Interpretation:**
The image shows a process of sequence transformation, specifically for converting a continuous or long sequence of inputs (like a waveform or its features) into a shorter, distinct sequence of output characters. The core concept is an 'alignment' layer (A) that serves as an intermediate representation, allowing a variable number of input frames (X) to map to a single character in the final output (Y). The repetitions of characters in the 'A (alignment)' layer (e.g., 'i', 'n', 'r' appearing multiple times) signify that multiple input frames (x_i) might correspond to the same phoneme or character, which are then 'collapsed' to form the final, non-repeating output string 'dinner' in the 'Y (output)' layer.

**Key Insights:**
1.  **Alignment Handling:** The image demonstrates how input features (x_i) are aligned with character predictions (A), allowing for variable-length inputs to map to a fixed set of characters. Multiple input frames can correspond to the same character, as seen by 'x₂' and 'x₃' both mapping to 'i', and 'x₄' through 'x₇' mapping to 'n'.
2.  **Collapsing Mechanism:** A key insight is the 'collapsing' step from alignment (A) to output (Y). Adjacent identical characters in the alignment are reduced to a single character in the output, which is the essence of a 'naive algorithm for collapsing an alignment'. This mechanism effectively handles the problem of temporal redundancy in the alignment, where a single phoneme might span multiple time steps in the input.
3.  **Intermediate Representation:** The 'A (alignment)' layer acts as an essential intermediate representation, bridging the gap between the granular, time-dependent 'X (input)' and the concise, discrete 'Y (output)'. It shows how the word 'dinner' is formed from a sequence of 14 input frames.

**Document Context:**
The image is presented in the context of 'Section: 16.4 CTC' and is identified as 'Figure 16.9 A naive algorithm for collapsing an alignment between input and letters.' This strongly suggests it illustrates a fundamental concept in Connectionist Temporal Classification (CTC), a method commonly used in speech recognition and handwriting recognition. The diagram visually explains how CTC handles the alignment problem where the length of the input sequence (e.g., audio frames) is typically much longer than the target output sequence (e.e., transcribed text). The 'naive algorithm for collapsing an alignment' refers to the final step where repeated characters in the intermediate alignment are condensed to produce the final, meaningful output string, which is crucial for understanding how CTC produces a coherent transcription from a sequence of probabilistic character predictions.

**Summary:**
The image illustrates a conceptual mapping process from an input waveform to a final output string, demonstrating how an intermediate alignment step handles variable-length sequences. It shows four horizontal layers: 'Y (output)', 'A (alignment)', 'X (input)', and 'wavefile'.

The 'wavefile' layer at the bottom displays a continuous, fluctuating waveform, representing raw audio data. Above it, the 'X (input)' layer consists of 14 discrete blue boxes, labeled sequentially from 'x₁' to 'x₁₄'. Each 'x' box represents a segment or feature vector extracted from the wavefile.

Directly above the 'X (input)' layer is the 'A (alignment)' layer, composed of 14 red boxes. Each box contains a single character. This layer shows how the input segments map to characters, often with repetitions. The characters are: 'd', 'i', 'i', 'n', 'n', 'n', 'n', 'e', 'r', 'r', 'r', 'r', 'r', 'r'. Vertical arrows connect each 'xᵢ' box from the 'X (input)' layer to its corresponding character in the 'A (alignment)' layer, indicating a direct mapping (e.g., 'x₁' maps to 'd', 'x₂' to 'i', 'x₃' to 'i', and so on).

Finally, at the top is the 'Y (output)' layer, consisting of 5 light blue boxes. This layer represents the collapsed or finalized output sequence. The characters are: 'd', 'i', 'n', 'e', 'r'. The 'Y (output)' layer is derived from the 'A (alignment)' layer by collapsing repeated adjacent characters into a single instance.](images/f8502f4fa259af21f7f6f1d850e021fe286a71dd5562825267f0f1a5826f500a.jpg)
alignment   
Figure 16.9 A naive algorithm for collapsing an alignment between input and letters.

Well, that doesn’t work; our naive algorithm has transcribed the speech as diner, not dinner! Collapsing doesn’t handle double letters. There’s also another problem with our naive function; it doesn’t tell us what symbol to align with silence in the input. We don’t want to be transcribing silence as random letters!

The CTC algorithm solves both problems by adding to the transcription alphabet a special symbol for a blank, which we’ll represent as . The blank can be used in the alignment whenever we don’t want to transcribe a letter. Blank can also be used between letters; since our collapsing function collapses only consecutive duplicate letters, it won’t collapse across . More formally, let’s define the mapping $B : a  y$ between an alignment $a$ and an output $y$ , which collapses all repeated letters and then removes all blanks. Fig. 16.10 sketches this collapsing function $B$ .

![## Image Analysis: 50750792cb825952325c38937fd95e650e46ee40c7b27519dda7db9fc0e8a056.jpg

**Conceptual Understanding:**
This image conceptually represents the Connectionist Temporal Classification (CTC) collapsing function, a crucial post-processing step in sequence-to-sequence neural networks, particularly used in tasks like speech recognition. Its main purpose is to demonstrate how a raw, potentially redundant or gapped, alignment of predicted characters (A) is transformed into a clean, unique, and meaningful output sequence (Y). The image illustrates the two primary operations involved: the merging of consecutive duplicate characters and the removal of all blank characters. The key idea being communicated is the systematic reduction of an over-complete or noisy character sequence (including special blank tokens) to its most parsimonious and accurate transcription, highlighting how the CTC algorithm handles repetitions and non-character segments to produce a final word or phrase.

**Content Interpretation:**
The image depicts a two-stage process for collapsing an aligned sequence (A) into a final output sequence (Y), characteristic of the Connectionist Temporal Classification (CTC) collapsing function. The process starts with a raw input (X) mapped to an intermediate 'A (alignment)' layer. The 'A (alignment)' sequence shows how individual input frames (x₁ through x₁₄) correspond to specific characters, including a blank character '⅃'. This alignment can contain repetitions of characters (e.g., 'n n', 'r r r r') and multiple blank characters. The first transformation, 'merge duplicates', processes the 'A (alignment)' sequence by consolidating consecutive identical non-blank characters into a single instance. For example, 'n n' from the alignment becomes 'n', and 'r r r r' becomes 'r'. Notably, blank characters ('⅃') are not merged with other blanks in this step, nor do they cause non-blank characters separated by them to merge. This results in the intermediate sequence 'd', 'i', '⅃', 'n', '⅃', 'n', 'e', 'r', '⅃', '⅃'. The second transformation, 'remove blanks', then systematically eliminates all occurrences of the '⅃' character from this intermediate sequence. This clean-up step produces the final 'Y (output)' sequence, which is 'd', 'i', 'n', 'n', 'e', 'r'. The overall system illustrates a method to derive a meaningful, non-redundant transcription from a variable-length sequence of predictions, accounting for temporal variations and the presence of non-character (blank) segments.

**Key Insights:**
The image demonstrates several key concepts of the CTC collapsing function: 1. **Sequential Transformation:** The process is a clear, step-by-step transformation from an input (X) to an alignment (A), then through 'merge duplicates' and 'remove blanks' to a final output (Y). 2. **Role of Alignment (A):** The alignment 'A' acts as an intermediate representation, mapping individual input segments (x₁, ..., x₁₄) to predicted characters or blanks ('d', 'i', '⅃', 'n', 'n', '⅃', 'n', 'e', 'r', 'r', 'r', 'r', '⅃', '⅃'). This shows how a longer input can be aligned to a character sequence with repetitions and blanks. 3. **'Merge Duplicates' Rule:** Consecutive identical non-blank characters are consolidated. This is evidenced by 'n n' in 'A' becoming 'n' in 'merge duplicates', and 'r r r r' becoming 'r'. Crucially, the blanks act as separators, preventing merging across them, as 'n ⅃ n' from 'A' results in 'n ⅃ n' in 'merge duplicates', and consecutive blanks '⅃ ⅃' remain '⅃ ⅃', indicating they are not merged in this step. 4. **'Remove Blanks' Rule:** All blank characters ('⅃') are completely eliminated from the sequence. This is demonstrated by the transition from 'd i ⅃ n ⅃ n e r ⅃ ⅃' to 'd i n n e r'. 5. **Final Output (Y):** The final output is a clean, unique sequence of characters, 'd i n n e r', which represents the word 'dinner', derived from a potentially much longer and redundant alignment. This illustrates how CTC effectively reduces complex temporal alignments to a concise result, making it robust to variations in timing and pronunciation.

**Document Context:**
This image, Figure 16.10, is placed within Section 16.4 CTC of the document and directly illustrates 'The CTC collapsing function B'. It provides a concrete visual example of how an 'alignment A' (generated from an 'input X') is processed to 'form the output Y'. The explanation specifically highlights the role of the 'space blank character' (⅃) and how 'repeated (consecutive) characters' are handled. Therefore, the image serves as a fundamental visual aid for understanding the mechanics of CTC in sequence-to-sequence tasks, particularly in contexts like speech recognition where alignments can be noisy or contain repetitions and gaps. It clarifies the transformation rules that lead from a detailed alignment to a compact, final output, directly supporting the technical discussion of the CTC algorithm's post-processing step.

**Summary:**
The image illustrates the CTC (Connectionist Temporal Classification) collapsing function, denoted as B, which transforms a raw input sequence into a simplified output by first aligning it, then processing it through two distinct stages: 'merge duplicates' and 'remove blanks'. The process begins with an input sequence, X, consisting of individual data points labeled from x₁ to x₁₄. These inputs are then mapped to an 'A (alignment)' sequence, which is a series of characters including standard letters and a special blank character '⅃'. The 'A (alignment)' sequence is 'd', 'i', '⅃', 'n', 'n', '⅃', 'n', 'e', 'r', 'r', 'r', 'r', '⅃', '⅃'. The next step is 'merge duplicates'. In this stage, consecutive identical characters (excluding the blank character) are merged into a single instance. For example, 'n n' becomes 'n', and 'r r r r' becomes 'r'. The blank characters '⅃' do not merge with other blanks in this step if they are consecutive, nor do they prevent non-blank characters separated by them from being treated as distinct. After 'merge duplicates', the sequence becomes 'd', 'i', '⅃', 'n', '⅃', 'n', 'e', 'r', '⅃', '⅃'. The final step is 'remove blanks', where all instances of the blank character '⅃' are eliminated from the sequence. This results in the sequence 'd', 'i', 'n', 'n', 'e', 'r'. This final sequence is labeled as 'Y (output)'. The visual representation clearly shows the transformation of a potentially redundant or noisy aligned sequence into a clean, concise output, demonstrating the core logic of the CTC collapsing function.](images/50750792cb825952325c38937fd95e650e46ee40c7b27519dda7db9fc0e8a056.jpg)
Figure 16.10 The CTC collapsing function $B$ , showing the space blank character ; repeated (consecutive) characters in an alignment $A$ are removed to form the output Y .

The CTC collapsing function is many-to-one; lots of different alignments map to the same output string. For example, the alignment shown in Fig. 16.10 is not the only alignment that results in the string dinner. Fig. 16.11 shows some other alignments that would produce the same output.

It’s useful to think of the set of all alignments that might produce the same output Y . We’ll use the inverse of our $B$ function, called $B ^ { - 1 }$ , and represent that set as $B ^ { - 1 } ( Y )$ .

![## Image Analysis: 5b4ae4eaebaa09e477d464f31ff4b76e06ba491491fd90f4501550b49eaee921.jpg

**Conceptual Understanding:**
This image conceptually illustrates the process of character alignment in the context of Connectionist Temporal Classification (CTC), a technique used in sequence modeling, particularly for tasks like speech recognition. The main purpose of the image is to demonstrate that a single target word (in this case, "dinner") can be generated from multiple, different input sequences or 'alignments' when using CTC's collapsing rules. It highlights the 'many-to-one' mapping property inherent in CTC, where the model can predict a sequence of characters, including repetitions and 'blank' symbols, that ultimately resolve to the same final output transcript. The key idea being communicated is the flexibility and robustness of CTC in handling temporal variations in sequences, allowing for various paths to achieve the correct output by strategically placing character repetitions and blank symbols.

**Content Interpretation:**
The image displays three distinct conceptual alignments of characters that, when processed through Connectionist Temporal Classification (CTC) rules, all result in the word "dinner". Each row represents a sequence of character predictions (including blank characters denoted by '_') over time. The significance is to demonstrate the many-to-one mapping capability of CTC, showing how different temporal sequences of character predictions, which may include character repetitions and null predictions (blanks), can all resolve to the same target word.

**Extracted Text Elements and Their Support for Interpretation:**
*   **Row 1 Content: "d", "i", "i", "n", "_", "n", "n", "e", "e", "e", "r", "r", "r", "_"**: This sequence shows multiple repetitions of 'i', 'n', 'e', and 'r' characters, interspersed with blank symbols. This supports the interpretation that CTC handles sustained or repeated character predictions over several input frames. The collapsing of 'ii' to 'i', 'nn' to 'n' (first 'n' then the second 'n' after the blank), 'eee' to 'e', and 'rrr' to 'r' demonstrates the core CTC collapsing mechanism.
*   **Row 2 Content: "d", "d", "i", "n", "n", "_", "n", "e", "r", "r", "_", "_", "_", "_"**: Here, 'd' is repeated, 'n' is repeated, and 'r' is repeated. The presence of the blank character '_' between 'nn' and 'n' (i.e., 'n n _ n') is crucial. In CTC, this specific arrangement would collapse to 'd', 'i', 'n', 'n', 'e', 'r' (the blank separates the two 'n' predictions), confirming how blanks prevent unintended collapses of identical consecutive characters that are meant to be distinct. The trailing blanks signify the end of the word prediction and periods of no specific character output.
*   **Row 3 Content: "d", "d", "d", "i", "n", "_", "n", "_", "_", "_", "e", "r", "r"**: This row shows even more repetitions like 'ddd'. The sequence 'n _ n' again highlights how blanks can ensure two instances of the same character are preserved as distinct. After collapsing 'ddd' to 'd' and 'rr' to 'r', and removing all blanks, this sequence also correctly yields "dinner".

These elements collectively illustrate the flexibility of CTC in allowing for varied alignment paths that account for temporal distortions, character repetitions, and periods of non-prediction, all while consistently producing the correct target label.

**Key Insights:**
The main takeaways from this image are: 
1.  **Multiple Legitimate Alignments:** There isn't a single, unique way to align an input sequence to a target output word in CTC. As shown by the three distinct rows, various combinations of character repetitions and blank symbols can all legitimately produce the same transcript ("dinner").
2.  **Role of Character Repetitions:** CTC inherently handles character repetitions. When the same non-blank character appears consecutively (e.g., 'ii', 'eee', 'rrr', 'ddd'), it is collapsed into a single instance of that character. This is evident in all three rows where characters like 'i', 'n', 'e', 'r', and 'd' are often repeated more than once.
3.  **Critical Function of Blank Symbols:** The underscore-like symbol ('_') represents a 'blank' prediction. Blanks serve a crucial purpose: 
    *   They act as separators, preventing the collapsing of two identical adjacent characters that are meant to be distinct in the final transcript (e.g., 'n _ n' resulting in 'nn'). 
    *   They account for periods where no specific character of the target word is being predicted, or for transitions between characters.

**Textual Evidence for these Insights:**
*   **Row 1 ("d i i n _ n n e e e r r r _")**: The sequence 'ii' and 'eee' demonstrates character repetition handling. The blank before 'nn' shows its role in ensuring two 'n's are formed, rather than one 'n' from 'nnn'.
*   **Row 2 ("d d i n n _ n e r r _ _ _ _")**: The 'dd' and 'nn' illustrate character repetition. The blank between 'nn' and 'n' explicitly shows how a blank preserves the two distinct 'n's required for "dinner". The multiple trailing blanks ('_ _ _ _') highlight that varying lengths of null predictions are tolerated.
*   **Row 3 ("d d d i n _ n _ _ _ e r r")**: 'ddd' further emphasizes the collapsing of multiple repetitions. The 'n _ n' pattern again showcases the blank's role in separating and preserving distinct identical characters.

These specific character arrangements and blank placements across the three rows collectively provide strong evidence for the flexibility, repetition handling, and blank-symbol mechanics fundamental to CTC.

**Document Context:**
The image is directly relevant to Section 16.4, which focuses on Connectionist Temporal Classification (CTC). In the broader document's narrative, this image serves as a concrete visual example illustrating how CTC addresses the challenge of aligning a variable-length input sequence (such as acoustic features in speech recognition) with a shorter, fixed-length output sequence (like a word transcript). The accompanying text, "Figure 16.11 Three other legitimate alignments producing the transcript dinner," explicitly links the image to the concept of multiple valid alignments within the CTC framework.

The image reinforces the theoretical discussion of CTC by demonstrating that: 
1.  A single output word can be generated from diverse input paths.
2.  Character repetitions in the input alignment are handled (collapsed).
3.  Blank symbols play a critical role in separating characters or indicating null predictions, without contributing to the final transcript. 
This helps readers understand the practical implications of CTC's 'many-to-one' mapping property and its robustness against temporal variations in sequence data.

**Summary:**
This image visually presents three distinct character sequences, each representing a "legitimate alignment" that decodes to the word "dinner" within the context of Connectionist Temporal Classification (CTC). Each row consists of individual characters, including letters and an underscore-like symbol representing a blank, enclosed in pink boxes. These alignments demonstrate the flexibility of CTC in mapping a longer, potentially redundant, input sequence to a shorter, meaningful output sequence. The decoding process involves collapsing consecutive identical non-blank characters into a single instance and then removing all blank characters.

**First Alignment (Top Row):** The sequence is "d i i n _ n n e e e r r r _". When processed, 'd' remains 'd', 'ii' collapses to 'i', 'n' then a blank then 'nn' collapses to 'nn' (the blank prevents the 'n's from merging into one 'n' before the final blank removal), 'eee' collapses to 'e', and 'rrr' collapses to 'r'. All blanks are removed, resulting in "dinner". This shows how character repetitions and blanks are integrated.

**Second Alignment (Middle Row):** The sequence is "d d i n n _ n e r r _ _ _ _". 'dd' collapses to 'd', 'i' remains 'i', 'nn' collapses to 'n', the blank ensures the next 'n' is distinct, 'e' remains 'e', 'rr' collapses to 'r'. The four trailing blanks are removed. This also yields "dinner". This highlights the strategic use of blanks to delineate characters that should form a double letter.

**Third Alignment (Bottom Row):** The sequence is "d d d i n _ n _ _ _ e r r". 'ddd' collapses to 'd', 'i' remains 'i', 'n' then a blank then 'n' collapses to 'nn', 'e' remains 'e', and 'rr' collapses to 'r'. The three intermediate blanks are removed. This likewise results in "dinner". This further exemplifies how various repetitions and blank placements can all lead to the same target word.

Overall, the image clearly illustrates the robustness of CTC, allowing for variations in timing and character predictions in the input sequence while consistently producing the correct output transcript by a set of defined collapsing rules.](images/5b4ae4eaebaa09e477d464f31ff4b76e06ba491491fd90f4501550b49eaee921.jpg)
Figure 16.11 Three other legitimate alignments producing the transcript dinner.

# 16.4.1 CTC Inference

Before we see how to compute $P _ { \mathrm { C T C } } ( Y | X )$ let’s first see how CTC assigns a probability to one particular alignment $\hat { A } = \{ \hat { a } _ { 1 } , \dots , \hat { a } _ { n } \}$ . CTC makes a strong conditional independence assumption: it assumes that, given the input $X$ , the CTC model output $a _ { t }$ at time $t$ is independent of the output labels at any other time $a _ { i }$ . Thus:

$$
P _ { \mathrm { C T C } } ( A | X ) \ = \ \prod _ { t = 1 } ^ { T } p ( a _ { t } | X )
$$

Thus to find the best alignment $\hat { A } = \{ \hat { a } _ { 1 } , \dots , \hat { a } _ { T } \}$ we can greedily choose the character with the max probability at each time step $t$ :

$$
\hat { a } _ { t } = \underset { c \in C } { \operatorname { a r g m a x } } p _ { t } ( c | X )
$$

We then pass the resulting sequence $A$ to the CTC collapsing function $B$ to get the output sequence $Y$ .

Let’s talk about how this simple inference algorithm for finding the best alignment A would be implemented. Because we are making a decision at each time point, we can treat CTC as a sequence-modeling task, where we output one letter $\hat { y } _ { t }$ at time $t$ corresponding to each input token $x _ { t }$ , eliminating the need for a full decoder. Fig. 16.12 sketches this architecture, where we take an encoder, produce a hidden state $h _ { t }$ at each timestep, and decode by taking a softmax over the character vocabulary at each time step.

![## Image Analysis: 4fd23c988658b15d5b516981af2f4cf0ce856d18f7724a1707e4249821e3a424.jpg

**Conceptual Understanding:**
This image conceptually represents the forward pass, or inference stage, of a speech recognition system that utilizes a Connectionist Temporal Classification (CTC) decoder. Its main purpose is to illustrate the sequential transformation of an audio signal into a predicted sequence of letters. The diagram explains how raw audio is processed through feature extraction and subsampling, then encoded, and finally classified using softmax to produce an output character sequence. It highlights the multi-layered approach to convert continuous acoustic input into discrete textual output, demonstrating the mapping from a potentially longer input sequence to a shorter, more compact output letter sequence through the implicit alignment capabilities of CTC (as suggested by the context).

**Content Interpretation:**
This image illustrates the step-by-step inference process of a neural network architecture, likely for Automatic Speech Recognition (ASR), specifically leveraging the Connectionist Temporal Classification (CTC) approach. It shows how a raw audio input is progressively transformed and processed through different layers to ultimately produce an output letter sequence. The core components include feature extraction, sequence length reduction, encoding, and classification with softmax probabilities. The diagram emphasizes the mapping from a continuous or high-resolution audio signal to a discrete sequence of characters.

**Key Insights:**
1. **Multi-Stage Feature Extraction and Transformation:** The process begins with a raw audio waveform, demonstrating the necessity of initial 'Feature Computation' to derive meaningful acoustic features like the 'log Mel spectrum' (f₁...fₜ). This highlights that raw audio is not directly fed into the high-level models. 2. **Sequence Length Reduction (Subsampling):** A critical step is 'Subsampling', which reduces the temporal resolution of the input, transforming f₁...fₜ into a 'Shorter input sequence X' (x₁...xₙ). This is a common practice in sequence modeling to manage computational complexity and potentially capture broader temporal contexts. 3. **Encoder-Only Architecture for Representation Learning:** The 'ENCODER' is central, taking the subsampled input sequence X and transforming it into higher-level representations (hidden states). This confirms the 

**Document Context:**
This image is crucial for understanding Section 16.4.1 titled 'CTC Inference' in the document. It visually explains the 'Inference with CTC: using an encoder-only model, with decoding done by simple softmaxes over the hidden state h_t at each output step', as stated in the text immediately following the image (Figure 16.12). The diagram provides a clear visual representation of how an encoder-only model processes an input sequence and how a classifier with softmax is used for decoding to produce the final output letter sequence. It details the data transformations at each stage, from audio waveform to log Mel spectrum, then to a shorter input sequence, and finally to the character output, which is central to comprehending CTC-based inference.

**Summary:**
The image illustrates the inference process for a Connectionist Temporal Classification (CTC) model, specifically for converting an audio waveform into an output letter sequence. The process begins with an audio waveform at the bottom, which undergoes 'Feature Computation' to transform it into a 'log Mel spectrum' represented by features f₁ through fₜ. This log Mel spectrum is then fed into a 'Subsampling' component, which reduces the sequence length to produce a 'Shorter input sequence X', denoted by x₁ through xₙ. This shorter sequence X is then passed to an 'ENCODER'. The ENCODER processes this input and outputs a sequence of hidden states, represented by light blue rectangles with internal bar graphs, at each time step. These hidden states are then fed into a 'Classifier +softmax' layer. This layer predicts a probability distribution over possible characters for each hidden state, resulting in an 'output letter sequence Y', which is shown as y₁ through yₙ. An example sequence of predicted letters 'i i i t t ...' is explicitly provided below the y labels, demonstrating how repeated characters are output by the classifier. The entire flow is directed upwards, indicating a sequential transformation from raw audio input to a transcribed letter sequence.](images/4fd23c988658b15d5b516981af2f4cf0ce856d18f7724a1707e4249821e3a424.jpg)
Figure 16.12 Inference with CTC: using an encoder-only model, with decoding done by simple softmaxes over the hidden state $h _ { t }$ at each output step.

Alas, there is a potential flaw with the inference algorithm sketched in (Eq. 16.15) and Fig. 16.11. The problem is that we chose the most likely alignment $A$ , but the most likely alignment may not correspond to the most likely final collapsed output string $Y$ . That’s because there are many possible alignments that lead to the same output string, and hence the most likely output string might not correspond to the most probable alignment. For example, imagine the most probable alignment $A$ for an input $X = [ x _ { 1 } x _ { 2 } x _ { 3 } ]$ is the string [a b ] but the next two most probable alignments are $[ \mathsf { b } \in \mathsf { b } ]$ and $[ \epsilon \textbf { b } \mathbf { b } ]$ . The output $Y = [ \boldsymbol { \mathsf { b } } \ \boldsymbol { \mathsf { b } } ]$ , summing over those two alignments, might be more probable than $Y = [ \mathbf { a } \mathbf { b } ]$ .

For this reason, the most probable output sequence $Y$ is the one that has, not the single best CTC alignment, but the highest sum over the probability of all its possible alignments:

$$
\begin{array} { l } { { P _ { C T C } ( Y | X ) ~ = ~ \displaystyle \sum _ { A \in B ^ { - 1 } ( Y ) } P ( A | X ) } } \\ { { ~ = ~ \displaystyle \sum _ { A \in B ^ { - 1 } ( Y ) } \prod _ { t = 1 } ^ { T } p ( a _ { t } | h _ { t } ) } } \\ { { \hat { Y } ~ = ~ \displaystyle \operatorname* { a r g m a x } _ { Y } \sum _ { T \ne T } ( Y | X ) } } \end{array}
$$

Alas, summing over all alignments is very expensive (there are a lot of alignments), so we approximate this sum by using a version of Viterbi beam search that cleverly keeps in the beam the high-probability alignments that map to the same output string, and sums those as an approximation of (Eq. 16.16). See Hannun (2017) for a clear explanation of this extension of beam search for CTC.

Because of the strong conditional independence assumption mentioned earlier (that the output at time $t$ is independent of the output at time $t - 1$ , given the input), CTC does not implicitly learn a language model over the data (unlike the attentionbased encoder-decoder architectures). It is therefore essential when using CTC to interpolate a language model (and some sort of length factor $L ( Y ) )$ using interpolation weights that are trained on a devset:

$$
s c o r e _ { \mathrm { C T C } } ( Y | X ) = \log P _ { \mathrm { C T C } } ( Y | X ) + \lambda _ { 1 } \log P _ { \mathrm { L } M } ( Y ) \lambda _ { 2 } L ( Y )
$$

# 16.4.2 CTC Training

To train a CTC-based ASR system, we use negative log-likelihood loss with a special CTC loss function. Thus the loss for an entire dataset $D$ is the sum of the negative log-likelihoods of the correct output $Y$ for each input $X$ :

$$
L _ { \mathrm C T C } = \sum _ { ( X , Y ) \in D } - \log P _ { \mathrm { C T C } } ( Y | X )
$$

To compute CTC loss function for a single input pair $( X , Y )$ , we need the probability of the output $Y$ given the input $X$ . As we saw in Eq. 16.16, to compute the probability of a given output $Y$ we need to sum over all the possible alignments that would collapse to $Y$ . In other words:

$$
P _ { \mathrm { C T C } } ( Y | X ) \ = \ \sum _ { A \in B ^ { - 1 } ( Y ) } \ \prod _ { t = 1 } ^ { T } p ( a _ { t } | h _ { t } )
$$

Naively summing over all possible alignments is not feasible (there are too many alignments). However, we can efficiently compute the sum by using dynamic programming to merge alignments, with a version of the forward-backward algorithm also used to train HMMs (Appendix A) and CRFs. The original dynamic programming algorithms for both training and inference are laid out in (Graves et al., 2006); see (Hannun, 2017) for a detailed explanation of both.

# 16.4.3 Combining CTC and Encoder-Decoder

It’s also possible to combine the two architectures/loss functions we’ve described, the cross-entropy loss from the encoder-decoder architecture, and the CTC loss. Fig. 16.13 shows a sketch. For training, we can simply weight the two losses with a $\lambda$ tuned on a devset:

$$
L = - \lambda \log P _ { e n c d e c } ( Y | X ) - ( 1 - \lambda ) \log P _ { c t c } ( Y | X )
$$

For inference, we can combine the two with the language model (or the length penalty), again with learned weights:

$$
\hat { Y } = \underset { Y } { \mathrm { a r g m a x } } \left[ \lambda \log P _ { \mathrm e n c d e c } ( Y | X ) - ( 1 - \lambda ) \log P _ { C T C } ( Y | X ) + \gamma \log P _ { \mathrm L M } ( Y ) \right]
$$

![## Image Analysis: 773559972e357b9be5724ceddb8249bad59a8f936e13d1b8425c3dbb2856b632.jpg

**Conceptual Understanding:**
The image conceptually represents a hybrid deep learning architecture for sequence processing, specifically demonstrating how Connectionist Temporal Classification (CTC) and a standard Encoder-Decoder framework can be integrated and trained using their respective loss functions. The main purpose is to illustrate the simultaneous application and interaction of these two loss mechanisms. It conveys the idea that by combining these losses, a model can potentially achieve better performance by simultaneously optimizing for robust sequence alignment (CTC) and accurate sequence generation (Encoder-Decoder) for tasks where inputs and outputs are sequences, like speech recognition or handwriting recognition, which often deal with variable-length inputs and outputs.

**Content Interpretation:**
The image displays a neural network architecture designed for sequence-to-sequence tasks, incorporating two distinct loss functions for training: CTC Loss and Encoder-Decoder Loss. This system is structured with an 'ENCODER' that processes input sequences and a 'DECODER' that generates output sequences. The 'CTC Loss' component is applied to the raw outputs of the 'ENCODER' (specifically, after some intermediate processing steps indicated by light blue boxes and histogram-like icons), aligning the encoder's predictions with the target sequence 'i t ' s t i m e ...'. Concurrently, the 'DECODER' receives inputs from the 'ENCODER' (via the 'H' hidden state) and a shifted version of the target sequence ('<s > i t ' s t i m ...'). The 'Encoder-Decoder Loss' is then calculated based on the 'DECODER's outputs compared to the target sequence 'i t ' s t i m e ...'. The significance lies in combining these two loss mechanisms, leveraging the strengths of both: CTC for robust alignment and end-to-end training of the encoder, and encoder-decoder loss for sequential prediction and attention-based mechanisms in the decoder.

**Key Insights:**
The main takeaway from this image is the architectural pattern for synergistically using CTC Loss and Encoder-Decoder Loss to train sequence models. The image teaches that an 'ENCODER' processes initial inputs 'x₁', '...', 'xₙ'. One path from the 'ENCODER' leads to the 'CTC Loss' calculation, which processes intermediate representations (light blue boxes and histogram icons) to align with a target sequence, 'i t ' s t i m e ...'. A parallel path involves the 'DECODER', which receives a hidden state 'H' from the 'ENCODER' and a shifted input sequence ('<s > i t ' s t i m ...'). The 'DECODER' then produces outputs that are used to compute the 'Encoder-Decoder Loss' against the same target sequence 'i t ' s t i m e ...'. This combined approach allows the model to benefit from the advantages of both loss functions, such as the sequence alignment capabilities of CTC and the context-aware generation of encoder-decoder models.

**Document Context:**
This image directly illustrates the concept described in the document section '16.4.3 Combining CTC and Encoder-Decoder'. It visually explains how these two distinct loss functions can be integrated into a single neural network architecture for tasks such as speech recognition or machine translation, where both alignment and sequence prediction are crucial. The figure, along with its caption 'Figure 16.13 Combining the CTC and encoder-decoder loss functions.', provides a foundational diagram for understanding the practical implementation of this combined approach, detailing the data flow from input through encoding and decoding to the final loss computations. It visually clarifies how the encoder's output is utilized by both loss components and how the decoder builds upon the encoder's representation.

**Summary:**
This image illustrates an architecture for combining the CTC (Connectionist Temporal Classification) and Encoder-Decoder loss functions in a sequence processing model. The system starts with an 'ENCODER' module that takes a sequence of inputs, represented as 'x₁', '...', and 'xₙ'. The output of the 'ENCODER' feeds into two main paths. 

The first path is for the 'CTC Loss'. The encoder's output first goes through a series of intermediate light blue rectangular layers, followed by small histogram-like icons, with ellipses '...' indicating more such units. These intermediate outputs are then used to compute the 'CTC Loss' against a target sequence: 'i t ' s t i m e ...'. 

The second path involves a 'DECODER' module. The 'ENCODER' also provides a hidden state, labeled 'H', as an input to the 'DECODER'. The 'DECODER' receives its own sequential inputs, starting with '<s >' followed by 'i', 't', ''', 's', 't', 'i', 'm', '...', which represent a shifted version of the target sequence. The outputs of the 'DECODER' are then used to compute the 'Encoder-Decoder Loss' against the same target sequence: 'i t ' s t i m e ...'. Both loss functions are ultimately combined to train the overall model. The diagram clearly depicts the flow of information and the distinct points at which each loss function is applied, highlighting their parallel roles in guiding the model's learning process.](images/773559972e357b9be5724ceddb8249bad59a8f936e13d1b8425c3dbb2856b632.jpg)
Figure 16.13 Combining the CTC and encoder-decoder loss functions.

# 16.4.4 Streaming Models: RNN-T for improving CTC

Because of the strong independence assumption in CTC (assuming that the output at time $t$ is independent of the output at time $t - 1 \rangle$ ), recognizers based on CTC don’t achieve as high an accuracy as the attention-based encoder-decoder recognizers. CTC recognizers have the advantage, however, that they can be used for streaming. Streaming means recognizing words on-line rather than waiting until the end of the sentence to recognize them. Streaming is crucial for many applications, from commands to dictation, where we want to start recognition while the user is still talking. Algorithms that use attention need to compute the hidden state sequence over the entire input first in order to provide the attention distribution context, before the decoder can start decoding. By contrast, a CTC algorithm can input letters from left to right immediately.

If we want to do streaming, we need a way to improve CTC recognition to remove the conditional independent assumption, enabling it to know about output history. The RNN-Transducer (RNN-T), shown in Fig. 16.14, is just such a model (Graves 2012, Graves et al. 2013). The RNN-T has two main components: a CTC acoustic model, and a separate language model component called the predictor that conditions on the output token history. At each time step $t$ , the CTC encoder outputs a hidden state $h _ { t } ^ { \mathrm { e n c } }$ given the input $x _ { 1 } . . . x _ { t }$ . The language model predictor takes as input the previous output token (not counting blanks), outputting a hidden state hpredu . The two are passed through another network whose output is then passed through a softmax to predict the next character.

$$
\begin{array} { l } { P _ { { \mathrm R N N - T } } ( Y | X ) \ = \ \displaystyle \sum _ { A \in B ^ { - 1 } ( Y ) } P ( A | X ) } \\ { \displaystyle \ = \ \sum _ { A \in B ^ { - 1 } ( Y ) } \prod _ { t = 1 } ^ { T } p ( a _ { t } | h _ { t } , y _ { < u _ { t } } ) } \end{array}
$$

![## Image Analysis: d189ee23c8e18ac80abf17926ba4a8084d34a0f618845974c0c1197da314e0a0.jpg

**Conceptual Understanding:**
This image represents a conceptual block diagram of the Recurrent Neural Network Transducer (RNN-T) model, a type of end-to-end neural network used for sequence-to-sequence tasks, particularly in automatic speech recognition. 

The main purpose of this diagram is to illustrate the architectural components of the RNN-T model and explain how it computes the probability distribution of an output token `y_t,u` at a given time `t` and output step `u`, by integrating information from an input sequence (`x_t`) and previously predicted output tokens (`y_u-1`). It highlights the distinct roles of the `ENCODER`, `PREDICTION NETWORK`, `JOINT NETWORK`, and `SOFTMAX` layers in this process, and how they combine to form a `DECODER` portion. The diagram visually explains the flow of data and the dependencies involved in generating each output token, showing the conditional probability `P (y_t,u | x_[1..t], y_[1..u-1])` as the final output.

**Content Interpretation:**
The image illustrates the core components and data flow of an RNN-T model, a neural network architecture designed for sequence-to-sequence tasks, particularly relevant for speech recognition as suggested by the document context (RNN-T for improving CTC). 

It shows a clear separation of concerns:
- An **Encoder** (`ENCODER`) processes the input signal (`x_t`, likely acoustic features) to create a hidden representation (`h^enc_t`). This signifies the model's ability to extract features from the input sequence.
- A **Prediction Network** (`PREDICTION NETWORK`) processes the previously generated output token (`y_u-1`) to predict the next output token's context (`h^pred_u`). This component acts as a language model, capturing dependencies between output tokens.
- A **Joint Network** (`JOINT NETWORK`) combines the information from both the encoder's output (`h^enc_t`) and the prediction network's output (`h^pred_u`) to create a unified representation (`z_t,u`). This fusion is crucial for generating output tokens that are conditioned on both the input features and the history of previously generated tokens.
- A **Softmax** layer (`SOFTMAX`) then transforms this joint representation (`z_t,u`) into a probability distribution (`P (y_t,u | x_[1..t], y_[1..u-1])`) over possible output tokens. This distribution indicates the model's confidence for each possible next token.

The brackets labeled `DECODER` indicate that the `JOINT NETWORK` and `SOFTMAX` layers collectively perform the decoding function, generating the output sequence based on the encoded input and predicted context.

**Key Insights:**
The main takeaways from this image, supported by the extracted text, are:
1.  **Modular Architecture:** The RNN-T model has a clear modular architecture comprising an `ENCODER`, a `PREDICTION NETWORK`, a `JOINT NETWORK`, and a `SOFTMAX` output layer. This modularity allows for specialized processing of input and previous output contexts.
2.  **Dual Conditioning:** The model's prediction of the current output token (`y_t,u`) is conditioned on two distinct sources of information: the input sequence up to the current time step (`x_[1..t]`) via the `ENCODER` and the history of previously generated output tokens (`y_[1..u-1]`) via the `PREDICTION NETWORK`.
3.  **Integration Mechanism:** The `JOINT NETWORK` is the crucial component that integrates the hidden representations from both the `ENCODER` (`h^enc_t`) and the `PREDICTION NETWORK` (`h^pred_u`) to form a combined representation (`z_t,u`), which is essential for making an informed decision about the next output token.
4.  **Probabilistic Output:** The `SOFTMAX` layer ensures that the model's output is a well-formed probability distribution over the vocabulary of possible output tokens, reflecting the likelihood of `y_t,u` under the given conditions.
5.  **Decoder Identification:** The `JOINT NETWORK` and `SOFTMAX` components are collectively identified as the `DECODER` part of the RNN-T model, highlighting their role in transforming internal representations into the final output sequence.

These insights are directly evident from the textual labels of the components and the directional flow of information within the diagram.

**Document Context:**
This image is highly relevant to the document's section '16.4.4 Streaming Models: RNN-T for improving CTC' as it provides a direct visual representation of the RNN-T model architecture being discussed. The text after the image, 'Figure 16.14 The RNN-T model computing the output token distribution at time t by integrating the output of a CTC acoustic encoder and a separate 'predictor' language model,' perfectly aligns with the diagram's illustration of how the `ENCODER` (acting as a CTC acoustic encoder) and `PREDICTION NETWORK` (acting as a separate 'predictor' language model) are integrated via the `JOINT NETWORK` and `SOFTMAX` to compute the output token distribution `P (y_t,u | x_[1..t], y_[1..u-1])`. The diagram serves to visually clarify the conceptual description of the RNN-T model's operation, particularly its ability to stream and integrate information from different sources for sequence generation.

**Summary:**
This diagram illustrates the architecture of an RNN-T (Recurrent Neural Network Transducer) model, specifically detailing how the model computes the output token distribution at a given time `t`. The model takes two primary inputs: `x_t` (the current input from the acoustic encoder) and `y_u-1` (the previously predicted output token from the predictor language model). 

The process begins with `x_t` feeding into an `ENCODER` component, which processes this input to produce an encoded representation denoted as `h^enc_t`. Simultaneously, the previous output token `y_u-1` is fed into a `PREDICTION NETWORK`, which generates a predicted hidden state `h^pred_u`. 

These two outputs, `h^enc_t` from the `ENCODER` and `h^pred_u` from the `PREDICTION NETWORK`, are then combined as inputs to a `JOINT NETWORK`. The `JOINT NETWORK` processes these combined inputs to produce a joint representation `z_t,u`. This `z_t,u` value is then passed to a `SOFTMAX` layer. The `SOFTMAX` layer applies a function to `z_t,u` to normalize it into a probability distribution, which is the final output of the model: `P (y_t,u | x_[1..t], y_[1..u-1])`. This represents the probability of the current output token `y_t,u` given the input sequence up to time `t` and the previously predicted output sequence up to time `u-1`. 

The `JOINT NETWORK` and the `SOFTMAX` components together form what is labeled as the `DECODER` part of the RNN-T model.](images/d189ee23c8e18ac80abf17926ba4a8084d34a0f618845974c0c1197da314e0a0.jpg)
Figure 16.14 The RNN-T model computing the output token distribution at time t by integrating the output of a CTC acoustic encoder and a separate ‘predictor’ language model.

# 16.5 ASR Evaluation: Word Error Rate

# word error

The standard evaluation metric for speech recognition systems is the word error rate. The word error rate is based on how much the word string returned by the recognizer (the hypothesized word string) differs from a reference transcription. The first step in computing word error is to compute the minimum edit distance in words between the hypothesized and correct strings, giving us the minimum number of word substitutions, word insertions, and word deletions necessary to map between the correct and hypothesized strings. The word error rate (WER) is then defined as follows (note that because the equation includes insertions, the error rate can be greater than $100 \%$ ):

$$
{ \mathrm { W o r d ~ E r r o r ~ R a t e ~ } } = \ 1 0 0 \times { \frac { \mathrm { I n s e r t i o n s + S u b s t i t u t i o n s + D e l e t i o n s } } { \mathrm { T o t a l ~ W o r d s ~ i n ~ C o r r e c t ~ T r a n s c r i p t } } }
$$

alignment

Here is a sample alignment between a reference and a hypothesis utterance from the CallHome corpus, showing the counts used to compute the error rate:

<table><tr><td>REF:</td><td>i***</td><td></td><td></td><td>**UM the PHONE IS</td><td></td><td>iLEFT THE portable ****</td><td></td><td></td><td></td><td>PHONEUPSTAIRSlast night</td><td></td></tr><tr><td>HYP:</td><td></td><td></td><td></td><td>i GOT IT TO the *****</td><td>FULLEST iLOVE TO</td><td></td><td></td><td>portable FORM OF</td><td></td><td>STORES</td><td>last night</td></tr><tr><td>Eval:</td><td>I</td><td>IS</td><td></td><td>D</td><td>S</td><td>S</td><td>S</td><td>I</td><td>S</td><td>S</td><td></td></tr></table>

This utterance has six substitutions, three insertions, and one deletion:

$$
\mathrm { W o r d ~ E r r o r ~ R a t e ~ } = \ 1 0 0 { \frac { 6 + 3 + 1 } { 1 3 } } = 7 6 . 9 \%
$$

The standard method for computing word error rates is a free script called sclite, available from the National Institute of Standards and Technologies (NIST) (NIST, 2005). Sclite is given a series of reference (hand-transcribed, gold-standard) sentences and a matching set of hypothesis sentences. Besides performing alignments, and computing word error rate, sclite performs a number of other useful tasks. For example, for error analysis it gives useful information such as confusion matrices showing which words are often misrecognized for others, and summarizes statistics of words that are often inserted or deleted. sclite also gives error rates by speaker (if sentences are labeled for speaker ID), as well as useful statistics like the sentence error rate, the percentage of sentences with at least one word error.

# Statistical significance for ASR: MAPSSWE or MacNemar

As with other language processing algorithms, we need to know whether a particular improvement in word error rate is significant or not.

The standard statistical tests for determining if two word error rates are different is the Matched-Pair Sentence Segment Word Error (MAPSSWE) test, introduced in Gillick and Cox (1989).

The MAPSSWE test is a parametric test that looks at the difference between the number of word errors the two systems produce, averaged across a number of segments. The segments may be quite short or as long as an entire utterance; in general, we want to have the largest number of (short) segments in order to justify the normality assumption and to maximize power. The test requires that the errors in one segment be statistically independent of the errors in another segment. Since ASR systems tend to use trigram LMs, we can approximate this requirement by defining a segment as a region bounded on both sides by words that both recognizers get correct (or by turn/utterance boundaries). Here’s an example from NIST (2007) with four regions:

I II III IV   
REF: |it was|the best|of|times it|was the worst|of times| |it was 1   
SYS A:|ITS |the best|of|times it|IS the worst |of times|OR|it was   
SYS B:|it was|the best| |times it|WON the TEST |of times| |it was

In region I, system A has two errors (a deletion and an insertion) and system B has zero; in region III, system A has one error (a substitution) and system B has two. Let’s define a sequence of variables $Z$ representing the difference between the errors in the two systems as follows:

$N _ { A } ^ { i }$ the number of errors made on segment $i$ by system $A$ $N _ { B } ^ { i }$ the number of errors made on segment $i$ by system $B$ $Z$ $N _ { A } ^ { i } - N _ { B } ^ { i } , i = 1 , 2 , \cdots , n$ where $n$ is the number of segments

In the example above, the sequence of $Z$ values is $\{ 2 , - 1 , - 1 , 1 \}$ . Intuitively, if the two systems are identical, we would expect the average difference, that is, the average of the $Z$ values, to be zero. If we call the true average of the differences $m u _ { z }$ , we would thus like to know whether $m u _ { z } = 0$ . Following closely the original proposal and notation of Gillick and Cox (1989), we can estimate the true average from our limited sample as $\textstyle { \hat { \mu } } _ { z } = \sum _ { i = 1 } ^ { n } Z _ { i } / n$ . The estimate of the variance of the $Z _ { i }$ ’s is

$$
\sigma _ { z } ^ { 2 } = { \frac { 1 } { n - 1 } } \sum _ { i = 1 } ^ { n } { ( Z _ { i } - \mu _ { z } ) } ^ { 2 }
$$

$$
W = \frac { \hat { \mu } _ { z } } { \sigma _ { z } / \sqrt { n } }
$$

For a large enough n $( > 5 0 )$ , $W$ will approximately have a normal distribution with unit variance. The null hypothesis is $H _ { 0 } : \mu _ { z } = 0$ , and it can thus be rejected if $2 * P ( Z \geq | w | ) \leq 0 . 0 5$ (two-tailed) or $P ( Z \geq | w | ) \leq 0 . 0 5$ (one-tailed), where $Z$ is standard normal and $w$ is the realized value $W$ ; these probabilities can be looked up in the standard tables of the normal distribution.

McNemar’s test

Earlier work sometimes used McNemar’s test for significance, but McNemar’s is only applicable when the errors made by the system are independent, which is not true in continuous speech recognition, where errors made on a word are extremely dependent on errors made on neighboring words.

Could we improve on word error rate as a metric? It would be nice, for example, to have something that didn’t give equal weight to every word, perhaps valuing content words like Tuesday more than function words like a or of. While researchers generally agree that this would be a good idea, it has proved difficult to agree on a metric that works in every application of ASR. For dialogue systems, however, where the desired semantic output is more clear, a metric called slot error rate or concept error rate has proved extremely useful; it is discussed in Chapter 15 on page 317.

# 16.6 TTS

The goal of text-to-speech (TTS) systems is to map from strings of letters to waveforms, a technology that’s important for a variety of applications from dialogue systems to games to education.

Like ASR systems, TTS systems are generally based on the encoder-decoder architecture, either using LSTMs or Transformers. There is a general difference in training. The default condition for ASR systems is to be speaker-independent: they are trained on large corpora with thousands of hours of speech from many speakers because they must generalize well to an unseen test speaker. By contrast, in TTS, it’s less crucial to use multiple voices, and so basic TTS systems are speaker-dependent: trained to have a consistent voice, on much less data, but all from one speaker. For example, one commonly used public domain dataset, the LJ speech corpus, consists of 24 hours of one speaker, Linda Johnson, reading audio books in the LibriVox project (Ito and Johnson, 2017), much smaller than standard ASR corpora which are hundreds or thousands of hours.2

We generally break up the TTS task into two components. The first component is an encoder-decoder model for spectrogram prediction: it maps from strings of letters to mel spectrographs: sequences of mel spectral values over time. Thus we

might map from this string:

It’s time for lunch!

to the following mel spectrogram:

![## Image Analysis: 0defb1f21d42a021c1823fcb6e7028909e35835f0aaa98da218bfa9b96685399.jpg

**Conceptual Understanding:**
Conceptually, the image illustrates a spectrogram, a widely used tool in signal processing for the visual analysis of time-varying frequencies in a signal, most commonly sound. Its main purpose is to show how the spectral content of a signal evolves over time. In the context of a document discussing 'TTS' (Text-to-Speech), this image specifically conveys the acoustic properties of speech, visualizing the frequencies (formants) and their intensities that constitute an utterance. It communicates the idea that speech is not just a simple waveform but a complex interplay of varying frequencies across time.

**Content Interpretation:**
The image represents a spectrogram, which is a visual depiction of the frequency content of a sound signal over time. It shows varying intensities of grayscale, where darker shades correspond to higher energy or amplitude at specific frequencies and time points, and lighter shades indicate lower energy. The vertical axis would typically represent frequency, and the horizontal axis would represent time. The blurred and pixelated nature of the image makes it difficult to discern fine details, but general patterns of frequency distribution and temporal changes are visible. These patterns suggest the presence of acoustic events, possibly speech formants, which are concentrations of acoustic energy around specific frequencies.

**Key Insights:**
The primary knowledge extracted from this image is the visual representation of a sound's frequency content over time. Although specific details are obscured by blurring, it demonstrates the concept of a spectrogram, which is crucial in audio processing and speech technology. Darker regions suggest periods of higher acoustic energy at certain frequencies, which could correspond to vocalic sounds or resonant frequencies in speech. Lighter regions would represent quieter periods or frequencies with less energy. In the context of TTS, it implies that the section is dealing with the acoustic properties and generation of speech, where the precise control over these spectrogram patterns is key to producing natural-sounding synthetic speech. The image visually supports the idea that speech is composed of varying frequency components that change over time, a core concept in speech synthesis.

**Document Context:**
Given the document context 'Section: 16.6 TTS' (Text-to-Speech), this spectrogram is highly relevant. In Text-to-Speech synthesis, spectrograms are fundamental for visualizing and analyzing speech signals. Researchers and engineers use them to understand the acoustic characteristics of natural speech, evaluate the quality of synthesized speech, and identify features for speech modeling (e.g., for vocoders, neural networks). The image likely serves to illustrate the output or an intermediate representation of a speech signal within a TTS system, demonstrating how phonetic information is encoded in the frequency domain over time. It could be showing the acoustic features that a TTS model aims to generate or analyze.

**Summary:**
The image provided is a grayscale spectrogram, which is a visual representation of the spectrum of frequencies of a sound or other signal as it varies with time. The horizontal axis typically represents time, and the vertical axis represents frequency, while the intensity or color (in this case, shades of gray) indicates the amplitude or energy of the frequencies at a given time. In this specific image, the details are quite blurred and pixelated, but it clearly displays vertical striations and horizontal bands of varying grayscale intensity. Darker areas suggest higher energy or amplitude at particular frequencies and times, while lighter areas indicate lower energy. Without specific labels or a legend, it is challenging to identify exact frequencies or time durations, but the pattern suggests the presence of distinct acoustic events or speech formants. Given the document context of '16.6 TTS' (Text-to-Speech), this spectrogram likely illustrates the acoustic properties of a synthesized or natural speech utterance, showing how different phonetic elements manifest across the frequency spectrum over time. The blurred nature might be due to downsampling, compression, or a representation of an abstract feature map rather than a raw spectrogram.](images/0defb1f21d42a021c1823fcb6e7028909e35835f0aaa98da218bfa9b96685399.jpg)

vocoding vocoder

The second component maps from mel spectrograms to waveforms. Generating waveforms from intermediate representations like spectrograms is called vocoding and this second component is called a vocoder:

![## Image Analysis: f6fb860eea17e3999171a78f522aacdf776f20a6e3d04489edb2c723e9a44618.jpg

**Conceptual Understanding:**
This image conceptually represents an audio waveform, specifically a visual depiction of sound. Its main purpose is to illustrate the amplitude of a sound signal as it changes over time. It communicates the idea of varying sound intensity and activity across a duration, which is fundamental to understanding audio processing and speech synthesis.

**Content Interpretation:**
The image shows a waveform, which is a visual representation of an audio signal. Specifically, it illustrates the amplitude fluctuations of a sound over a period of time. The varying heights and densities of the vertical lines indicate changes in the sound's intensity or loudness. Different segments of the waveform suggest distinct sound events, potentially representing speech segments, musical notes, or other audio phenomena, with periods of silence or lower sound levels in between. The absence of explicit text means interpretations are based solely on the visual characteristics of the waveform itself.

**Key Insights:**
The primary knowledge extracted from this image is the visual representation of sound intensity changes over time. It demonstrates how an audio signal's amplitude varies, indicating louder (higher amplitude) and softer (lower amplitude) sound segments, as well as periods of relative silence or low activity. The varying density of the oscillations can also implicitly suggest the presence of different frequencies or the complexity of the sound. While no textual evidence supports specific conclusions, the visual patterns strongly suggest dynamic changes in the audio content.

**Document Context:**
Given the document context is 'Section: 16.6 TTS' (Text-to-Speech), this image most likely serves to illustrate the audio output generated by a Text-to-Speech system, or a component of audio processing involved in TTS. It could be depicting the synthesized speech waveform, demonstrating characteristics like prosody, volume variations, or the temporal segmentation of speech. The image helps a reader visualize the physical manifestation of the sound produced by a TTS engine, allowing for a better understanding of how the digital signal translates into an audible output.

**Summary:**
The image displays a monophonic waveform, a visual representation of an audio signal's amplitude over time. The horizontal axis implicitly represents time, and the vertical displacement from the central line represents the amplitude of the sound wave. The waveform shows distinct segments with varying amplitudes. On the left, there is a segment of moderate amplitude, followed by a brief period of low amplitude. This is succeeded by a segment exhibiting higher and denser oscillations, indicating a louder or more active sound. A very brief pause or low-amplitude segment separates this from the final, most prominent section, which displays the highest amplitude and densest oscillations, suggesting the loudest and most sustained sound event in the depicted timeframe. The overall pattern suggests a progression of sound events with increasing intensity or complexity. There is no explicit text present within the image, such as labels, titles, or annotations.](images/f6fb860eea17e3999171a78f522aacdf776f20a6e3d04489edb2c723e9a44618.jpg)

These standard encoder-decoder algorithms for TTS are still quite computationally intensive, so a significant focus of modern research is on ways to speed them up.

# 16.6.1 TTS Preprocessing: Text normalization

Before either of these two steps, however, TTS systems require text normalization preprocessing for handling non-standard words: numbers, monetary amounts, dates, and other concepts that are verbalized differently than they are spelled. A TTS system seeing a number like 151 needs to know to verbalize it as one hundred fifty one if it occurs as $\$ 151$ but as one fifty one if it occurs in the context 151 Chapultepec Ave.. The number 1750 can be spoken in at least four different ways, depending on the context:

seventeen fifty: (in “The European economy in 1750”) one seven five zero: (in “The password is 1750”) seventeen hundred and fifty: (in “1750 dollars”) one thousand, seven hundred, and fifty: (in “1750 dollars”)

Often the verbalization of a non-standard word depends on its meaning (what Taylor (2009) calls its semiotic class). Fig. 16.15 lays out some English nonstandard word types.

Many classes have preferred realizations. A year is generally read as paired digits (e.g., seventeen fifty for 1750). $\$ 3.2$ billion must be read out with the word dollars at the end, as three point two billion dollars. Some abbreviations like N.Y. are expanded (to New York), while other acronyms like GPU are pronounced as letter sequences. In languages with grammatical gender, normalization may depend on morphological properties. In French, the phrase 1 mangue (‘one mangue’) is normalized to une mangue, but $I$ ananas (‘one pineapple’) is normalized to un ananas. In German, Heinrich IV (‘Henry IV’) can be normalized to Heinrich der Vierte, Heinrich des Vierten, Heinrich dem Vierten, or Heinrich den Vierten depending on the grammatical case of the noun (Demberg, 2006).

<table><tr><td> semiotic class</td><td>examples</td><td>verbalization</td></tr><tr><td>abbreviations</td><td> gov&#x27;t, N.Y., mph</td><td> government</td></tr><tr><td> acronyms read as letters</td><td>GPU, D.C., PC, UN, IBM</td><td>GPU</td></tr><tr><td>cardinal numbers</td><td>12, 45, 1/2, 0.6</td><td>twelve</td></tr><tr><td>ordinal numbers</td><td>May 7,3rd, Bill Gates II</td><td>seventh</td></tr><tr><td> numbers read as digits</td><td> Room 101</td><td>one oh one</td></tr><tr><td>times</td><td> 3.20, 11:45</td><td> eleven forty five</td></tr><tr><td>dates</td><td>28/02 (or in US, 2/28)</td><td> February twenty eighth</td></tr><tr><td> years</td><td>1999, 80s, 1900s, 2045</td><td> nineteen ninety nine</td></tr><tr><td> money</td><td>$3.45, €250, $200K</td><td> three dollars forty five</td></tr><tr><td> money in tr/m/billions</td><td> $3.45 billion</td><td> three point four five billion dollars</td></tr><tr><td>percentage</td><td>75% 3.4%</td><td> seventy five percent</td></tr></table>

Figure 16.15 Some types of non-standard words in text normalization; see Sproat et al. (2001) and (van Esch and Sproat, 2018) for many more.

Modern end-to-end TTS systems can learn to do some normalization themselves, but TTS systems are only trained on a limited amount of data (like the 220,000 words we mentioned above for the LJ corpus (Ito and Johnson, 2017)), and so a separate normalization step is important.

Normalization can be done by rule or by an encoder-decoder model. Rule-based normalization is done in two stages: tokenization and verbalization. In the tokenization stage we hand-write rules to detect non-standard words. These can be regular expressions, like the following for detecting years:

/(1[89][0-9][0-9])|(20[0-9][0-9]/

A second pass of rules express how to verbalize each semiotic class. Larger TTS systems instead use more complex rule-systems, like the Kestral system of (Ebden and Sproat, 2015), which first classifies and parses each input into a normal form and then produces text using a verbalization grammar. Rules have the advantage that they don’t require training data, and they can be designed for high precision, but can be brittle, and require expert rule-writers so are hard to maintain.

The alternative model is to use encoder-decoder models, which have been shown to work better than rules for such transduction tasks, but do require expert-labeled training sets in which non-standard words have been replaced with the appropriate verbalization; such training sets for some languages are available (Sproat and Gorman 2018, Zhang et al. 2019).

In the simplest encoder-decoder setting, we simply treat the problem like machine translation, training a system to map from:

They live at 224 Mission St.

to

They live at two twenty four Mission Street

While encoder-decoder algorithms are highly accurate, they occasionally produce errors that are egregious; for example normalizing 45 minutes as forty five millimeters. To address this, more complex systems use mechanisms like lightweight covering grammars, which enumerate a large set of possible verbalizations but don’t try to disambiguate, to constrain the decoding to avoid such outputs (Zhang et al., 2019).

# 16.6.2 TTS: Spectrogram prediction

The exact same architecture we described for ASR—the encoder-decoder with attention– can be used for the first component of TTS. Here we’ll give a simplified overview

of the Tacotron2 architecture (Shen et al., 2018), which extends the earlier Tacotron (Wang et al., 2017) architecture and the Wavenet vocoder (van den Oord et al., 2016). Fig. 16.16 sketches out the entire architecture.

The encoder’s job is to take a sequence of letters and produce a hidden representation representing the letter sequence, which is then used by the attention mechanism in the decoder. The Tacotron2 encoder first maps every input grapheme to a 512-dimensional character embedding. These are then passed through a stack of 3 convolutional layers, each containing 512 filters with shape $5 \times 1$ , i.e. each filter spanning 5 characters, to model the larger letter context. The output of the final convolutional layer is passed through a biLSTM to produce the final encoding. It’s common to use a slightly higher quality (but slower) version of attention called location-based attention, in which the computation of the $\alpha$ values (Eq. 8.36 in Chapter 8) makes use of the $\alpha$ values from the prior time-state.

In the decoder, the predicted mel spectrum from the prior time slot is passed through a small pre-net as a bottleneck. This prior output is then concatenated with the encoder’s attention vector context and passed through 2 LSTM layers. The output of this LSTM is used in two ways. First, it is passed through a linear layer, and some output processing, to autoregressively predict one 80-dimensional log-mel filterbank vector frame $5 0 \mathrm { m s }$ , with a $1 2 . 5 \mathrm { m s }$ stride) at each step. Second, it is passed through another linear layer to a sigmoid to make a “stop token prediction” decision about whether to stop producing output.

![## Image Analysis: eb3e6dccdcb1131365aea4b0f0beb27698028bf1507fb51ae41be5f338629e23.jpg

**Conceptual Understanding:**
This image represents the conceptual architecture of the Tacotron 2 system, a prominent neural network model for text-to-speech (TTS) synthesis. Its main purpose is to transform written text (graphemes) into natural-sounding speech (waveform samples). The diagram illustrates a two-stage process: first, an encoder-decoder model with an attention mechanism predicts a mel spectrogram from the input text, and second, a vocoder synthesizes the audio waveform from the predicted mel spectrogram. Key ideas communicated are the modularity of the system (Encoder, Decoder, Vocoder), the auto-regressive nature of the decoder, and the role of attention for aligning text features with acoustic features.

**Content Interpretation:**
The image systematically shows the flow of information through the Tacotron 2 architecture for speech synthesis.

*   **Encoder (Blue Blocks):** This section, starting with "Input Text" and ending with "Bidirectional LSTM", processes the input text to create a high-level, contextualized representation.
    *   "Input Text" is the initial textual data.
    *   "Character Embedding" converts discrete characters into continuous vector representations, as explicitly labeled.
    *   "3 Conv Layers" extracts local features from these embeddings, implying a hierarchical feature learning.
    *   "Bidirectional LSTM" captures long-range dependencies in both forward and backward directions, providing a rich contextual representation of the input sequence.

*   **Attention Mechanism (Gray Block):** The "Location Sensitive Attention" mechanism acts as a bridge between the encoder and decoder. It selectively focuses on relevant parts of the encoded text representation at each decoding step. The arrows indicate it receives information from the "Bidirectional LSTM" (encoder output) and feeds into the "2 Layer Pre-Net" of the decoder, while also receiving a recurrent input from "2 LSTM Layers" (decoder state) to guide its attention, thus ensuring proper alignment of text and speech features.

*   **Decoder (Orange Blocks):** This auto-regressive component predicts the mel spectrogram frame by frame.
    *   "2 Layer Pre-Net" processes the attention context and previous predicted mel spectrogram frame (via feedback from "5 Conv Layer Post-Net") before feeding into the core LSTMs.
    *   "2 LSTM Layers" are the central recurrent units that maintain the decoder's state and generate sequences. The feedback to "Location Sensitive Attention" is crucial for dynamic alignment.
    *   The "2 LSTM Layers" branch into two "Linear Projection" paths.
        *   The upper "Linear Projection" directly outputs a mel spectrogram prediction.
        *   The "5 Conv Layer Post-Net" refines this initial prediction. The "Summation" block then combines this refined output with the initial linear projection, indicating a residual connection or a mechanism to incorporate both raw and processed predictions to form the final "Mel Spectrogram" output.
        *   The feedback loop from "5 Conv Layer Post-Net" to "2 Layer Pre-Net" highlights the auto-regressive nature, where the decoder uses its own previous output as input for the next step, as indicated by the labels.
        *   The lower "Linear Projection" leads to the "Stop Token" prediction, which is a mechanism to determine when to stop generating mel spectrogram frames, preventing unnecessarily long or truncated outputs.

*   **Vocoder (Green/Yellow Blocks on Right):** This module converts the acoustic features (mel spectrogram) into an audible waveform.
    *   The "Mel Spectrogram" image visually represents the acoustic features predicted by the decoder, showing "Channel" and "Frame" axes.
    *   "WaveNet MoL" (Mixture of Logistics) is explicitly identified as the vocoder, indicating it's responsible for synthesizing high-fidelity audio from the mel spectrogram.
    *   "Waveform Samples" is the final desired output – the raw audio signal.

All extracted text elements, such as "Input Text", "Character Embedding", "Location Sensitive Attention", "Mel Spectrogram", and "WaveNet MoL", directly identify the specific functions and components within this complex speech synthesis pipeline. The labels like "Encoder", "Decoder", and "Vocoder" clearly delineate the major architectural divisions.

**Key Insights:**
The image provides several key insights into the Tacotron 2 system:

*   **Modular Architecture:** The system is clearly divided into three main components: "Encoder", "Decoder", and "Vocoder". This modularity, explicitly shown by the braces and labels, suggests that different parts of the speech synthesis task are handled by specialized networks.
*   **Two-Stage Synthesis:** Tacotron 2 employs a two-stage approach: first, it converts text into an intermediate acoustic representation ("Mel Spectrogram"), and then a separate vocoder converts this representation into audio. This is evident from the flow: "Input Text" -> "Encoder" -> "Decoder" -> "Mel Spectrogram" -> "WaveNet MoL" -> "Waveform Samples".
*   **Encoder-Decoder with Attention for Text-to-Spectrogram:** The core text-to-spectrogram mapping is handled by an encoder-decoder architecture. The "Bidirectional LSTM" in the "Encoder" processes text, and the "Decoder" (with "2 LSTM Layers") generates the spectrogram. The "Location Sensitive Attention" mechanism is crucial for aligning the encoded text features with the generated spectrogram frames, as its connections show it bridging the encoder and decoder.
*   **Auto-regressive Decoding:** The feedback loop from the "5 Conv Layer Post-Net" back to the "2 Layer Pre-Net" signifies that the decoder is auto-regressive, meaning it uses its own previously predicted output (acoustic features) to predict the next output, ensuring temporal coherence.
*   **Post-Net Refinement:** The "5 Conv Layer Post-Net" and the "Summation" operation suggest that the decoder applies a refinement step to its initial spectrogram predictions, improving their quality.
*   **Explicit Stop Condition:** The presence of a "Stop Token" output indicates that the model learns to determine the appropriate length of the output spectrogram, preventing it from generating silence indefinitely or cutting off speech prematurely.
*   **WaveNet for High-Fidelity Audio:** The use of "WaveNet MoL" as the "Vocoder" highlights that Tacotron 2 leverages a powerful, generative model for synthesizing high-quality, natural-sounding raw audio ("Waveform Samples") from the predicted mel spectrograms.

**Document Context:**
This image is crucial in the document's section "16.6.2 TTS: Spectrogram prediction" as it provides a detailed visual explanation of the Tacotron 2 system architecture, a leading model for Text-to-Speech synthesis. The surrounding text indicates that it maps graphemes to mel spectrograms, followed by a vocoder mapping to wavefiles, which directly corresponds to the diagram's structure.

**Summary:**
The diagram illustrates the Tacotron 2 system, a sophisticated neural network designed to convert written text into spoken audio. The system operates in a sequential, modular fashion, structured into three main parts: an Encoder, a Decoder, and a Vocoder.

The process begins with the **Encoder** section (shown on the left, grouped by a brace).
1.  **Input Text:** The system starts by receiving raw text.
2.  **Character Embedding:** This text is first converted into numerical representations by a "Character Embedding" layer, transforming individual characters into meaningful vectors.
3.  **3 Conv Layers:** These convolutional layers then process the character embeddings to extract higher-level features from the input text.
4.  **Bidirectional LSTM:** A "Bidirectional LSTM" (Long Short-Term Memory) network further processes these features, capturing contextual information from both preceding and succeeding parts of the text, creating a comprehensive representation of the input.

The output of the Encoder then interacts with the **Decoder** section (middle-left, grouped by a brace) via an **Attention Mechanism**.
1.  **Location Sensitive Attention:** This crucial component acts as a bridge, allowing the decoder to selectively focus on relevant parts of the encoded text. It receives input from the Encoder's "Bidirectional LSTM" and also receives feedback from the Decoder's "2 LSTM Layers" to intelligently decide which parts of the text are important for generating the current segment of speech.

The **Decoder** is an auto-regressive model responsible for predicting the mel spectrogram, an acoustic representation of speech.
1.  **2 Layer Pre-Net:** The attention output is fed into a "2 Layer Pre-Net". This layer also receives a feedback input from the "5 Conv Layer Post-Net", meaning the decoder uses its own previously generated acoustic features to predict the next ones, creating a smooth, continuous output.
2.  **2 LSTM Layers:** The "Pre-Net" output is processed by "2 LSTM Layers", which are the core memory units of the decoder, maintaining its state and generating sequential outputs. These LSTM layers also feed back into the "Location Sensitive Attention" to refine the attention mechanism.
3.  **Parallel Projections:** From the "2 LSTM Layers", the process splits into two parallel "Linear Projection" paths:
    *   **Mel Spectrogram Prediction:** One "Linear Projection" directly predicts an initial mel spectrogram frame. This prediction is then further refined by a "5 Conv Layer Post-Net". The outputs from both the initial "Linear Projection" and the "Post-Net" are combined through a "Summation" (a circled plus sign) to produce the final "Mel Spectrogram" frame. The "Post-Net" output is also fed back to the "2 Layer Pre-Net" for the next prediction step.
    *   **Stop Token Prediction:** The other "Linear Projection" path is dedicated to predicting a "Stop Token". This token signals to the system when the entire mel spectrogram generation should conclude, ensuring the output speech has an appropriate length.

Finally, the generated "Mel Spectrogram" is passed to the **Vocoder** section (right side, grouped by a brace).
1.  **Mel Spectrogram:** The "Mel Spectrogram" itself is a visual representation of the acoustic features, plotted with "Channel" on the Y-axis (from 0 to 70) and "Frame" on the X-axis (from 0 to 50), showing the frequency content over time.
2.  **WaveNet MoL:** This mel spectrogram is then fed into a "WaveNet MoL" (Mixture of Logistics) module, which is a powerful vocoder.
3.  **Waveform Samples:** The "WaveNet MoL" synthesizes the actual raw audio, producing the final "Waveform Samples" – the audible speech output.

In essence, Tacotron 2 takes text, encodes it, uses an attention mechanism to align it with desired acoustic features, generates these features (mel spectrograms) step-by-step, and then uses a vocoder to convert these features into natural-sounding speech.](images/eb3e6dccdcb1131365aea4b0f0beb27698028bf1507fb51ae41be5f338629e23.jpg)
conditioning inpuFigure 16.16 Fig. 1. Block diagram of the Tacotron 2 system architecture.The Tacotron2 architecture: An encoder-decoder maps from graphemes to ncrease in temporalmel spectrograms, followed by a vocoder that maps to wavefiles. Figure modified from Shen ciation issues.et al. (2018).

n, the filterbank We minimize the summed mean squared error (MSE) from beforeThe system is trained on gold log-mel filterbank features, using teacher forcing, alue of 0.01 in ordern.         with a log-likelihood loss by modeling the output distribution withthat is the decoder is fed the correct log-model spectral feature at each decoder step a decoder with atten- a Mixture Density Network [23, 24] to avoid assuming ainstead of the predicted decoder output from the prior step.

# ed 512-dimensionala stack of 3 convolu- In parallel to sdecoder LSTM ou16.6.3 TTS: Vocoding

normalization [18] probability that the output sequence has completed. This “stop token”The vocoder for Tacotron 2 is an adaptation of the WaveNet vocoder (van den Oord convolutional layers prediction is used during inference to allow the model to dynamicallyet al., 2016). Here we’ll give a somewhat simplified description of vocoding using ayer is passed iWaveNet.

512 units (256 frame for which this probability exceeds a threshold of 0.5.Recall that the goal of the vocoding process here will be to invert a log mel specntion network which        dropout [25] with probability 0.5, and LSTM layers are regularizedtrum representations back into a time-domain waveform representation. WaveNet is length context vector using zoneout [26] with probability 0.1. In order to introduce outputan autoregressive network, like the language models we introduced in Chapter 8. It takes spectrograms as input and produces audio output represented as sequences of 8-bit mu-law (page 336). The probability of a waveform , a sequence of 8-bit mulaw values $Y = y _ { 1 } , . . . , y _ { t }$ , given an intermediate input mel spectrogram $h$ is computed as:

$$
p ( Y ) = \prod _ { t = 1 } ^ { t } P ( y _ { t } | y _ { 1 } , . . . , y _ { t - 1 } , h _ { 1 } , . . . , h _ { t } )
$$

This probability distribution is modeled by a stack of special convolution layers,in Fig. 2 the receptive field is only 5 (= #layers + filter length - 1). In this paper w which include a specific convolutional structure called dilated convolutions, and a convolutions to increase the receptive field by orders of magnitude, without greatly incr specific non-linearity function.

A dilated convolution is a subtype of causal convolutional layer. Causal or convolution (also called a trous \` , or convolution with holes) is a convolution whe masked convolutions look only at the past input, rather than the future; the pre-s applied over an area larger than its length by skipping input values with a certain ste diction of lent to a co $y _ { t + 1 }$ can only depend on ution with a larger filte $y _ { 1 } , . . . , y _ { t }$ , useful for autoregressive left-to-right from the original filter by dilating it with processing. In dilated convolutions, at each successive layer we apply the convolutional filter over a span longer than its length by skipping input values. Thus at time $t$ with a dilation value of 1, a convolutional filter of length 2 would see input valuess the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1, $x _ { t }$ and ilat $x _ { t - 1 }$ . But a filter with a distillation value of 2 would skip an input, so wouldnvolutions have previously been used in various contexts, e.g. signal proc see input values chneider et al., 1 $x _ { t }$ and9; D $x _ { t - 1 }$ . Fig. 16.17 shows the computation of the output at timeeux, 1989), and image segmentation (Chen et al., 2015; $t$ with 4 dilated convolution layers with dilation values, 1, 2, 4, and 8. 2016).

![## Image Analysis: 7cf4c6319c65255d9924462437473d18d80a79076d5f48834e3a5c7c86c2f55e.jpg

**Conceptual Understanding:**
This image conceptually represents the mechanism of dilated convolutions, a specialized type of convolutional neural network layer. Its main purpose is to illustrate how an expanded 'receptive field'—the span of input features that a specific output feature depends on—is achieved by 'dilating' or spacing out the filter's weights. The image conveys the idea of hierarchical processing where information from widely separated input points can be aggregated efficiently into a single output point by increasing the 'Dilation' rate across successive 'Hidden Layer's. This allows the model to capture both fine-grained local features and coarse-grained global context.

**Content Interpretation:**
The image depicts the architecture of dilated convolutions, a technique used in deep learning, particularly for processing sequential data. It illustrates how the 'receptive field'—the area of the input that a particular output neuron is influenced by—can be expanded exponentially without increasing the number of parameters or losing resolution, by introducing gaps (dilations) between the convolution filter elements. The blue circles represent the initial 'Input' data. White circles represent nodes in 'Hidden Layer's, and orange circles represent the final 'Output'. The connections, shown by arrows, skip elements in the preceding layer based on the 'Dilation' rate. Specifically, 'Dilation = 1' implies contiguous connections, while 'Dilation = 2', 'Dilation = 4', and 'Dilation = 8' indicate increasingly sparse connections across layers, allowing a wider span of the input to influence the output.

**Key Insights:**
The main takeaway from this image is that dilated convolutions offer an effective method for vastly expanding the receptive field of a neural network without incurring significant computational cost or loss of information. By systematically increasing the 'Dilation' rate (1, 2, 4, 8), the network can incorporate increasingly broader context from the 'Input' layer to generate the 'Output'. This is evidenced by the direct visual representation of how an 'Output' node, with a 'Dilation = 8' layer, connects to widely separated 'Input' nodes through intermediate 'Hidden Layer's with progressively higher dilation values. This enables models to efficiently learn long-range dependencies, which is critical for tasks involving sequential data like speech synthesis, where the context of many previous timesteps is necessary for accurate predictions.

**Document Context:**
Given the document context 'ed 512-dimensional stack of 3 convolu- In parallel to sdecoder LSTM ou16.6.3 TTS: Vocoding' and 'TTS: Vocoding', this image is highly relevant as it explains a fundamental component of neural network architectures often employed in tasks like Text-to-Speech (TTS) synthesis. Dilated convolutions are crucial for efficiently capturing long-range dependencies in sequential data, such as audio waveforms or linguistic features, which is essential for generating natural-sounding speech. The explicit mention of '512-dimensional stack of 3 convolu-' suggests that dilated convolutions are being used as part of a larger network structure (e.g., a WaveNet-like architecture) to process high-dimensional sequential data for vocoding.

**Summary:**
The image illustrates the concept of dilated convolutions within a neural network architecture, showing how the receptive field expands across multiple layers with increasing dilation values. The process begins with an 'Input' layer, represented by blue circles at the bottom. This input feeds into a series of 'Hidden Layer's, represented by white circles. The first 'Hidden Layer' has a 'Dilation = 1', meaning its connections to the preceding layer are contiguous. Subsequent 'Hidden Layer's progressively increase their dilation: the second 'Hidden Layer' has 'Dilation = 2', the third 'Hidden Layer' has 'Dilation = 4'. Finally, an 'Output' layer, represented by orange circles at the top, processes information from the last hidden layer with an effective 'Dilation = 8'. Solid black arrows show specific connections that contribute to the receptive field of a particular output node, demonstrating how a single output can draw information from widely separated input nodes due to the increasing gaps in connections at higher dilation rates. Dashed lines illustrate the full extent of potential connections, or the overall receptive field, at each layer.](images/7cf4c6319c65255d9924462437473d18d80a79076d5f48834e3a5c7c86c2f55e.jpg)
Figure 16.17 Dilated convolutions, showing one dilation cycle size of 4, i.e., dilation values of 1, 2, 4, 8. Figure from van den Oord et al. (2016).

The Tacotron 2 synthesizer uses 12 convolutional layers in two cycles with a hile preserving the input resolution throughout the network as well as computational efficdilation cycle size of 6, meaning that the first 6 layers have dilations of 1, 2, 4, 8, 16, paper, the dilation is doubled for every layer up to a limit and then repeated: e.g.and 32. and the next 6 layers again have dilations of 1, 2, 4, 8, 16, and 32. Dilated convolutions allow the vocoder to grow the receptive field exponentially with depth.

WaveNet predicts mu-law audio samples. Recall from page 336 that this is a tuition behind this configuration is two-fold. First, exponentially increasing the dilationstandard compression for audio in which the values at each sampling timestep are compressed into 8-bits. This means that we can predict the value of each sample ative (non-linear) counterpart of a 1 1024 convolution. Second, stacking these blocks fwith a simple 256-way categorical classifier. The output of the dilated convolutions ses the model capacity and the receptive field size.is thus passed through a softmax which makes this 256-way decision.

The spectrogram prediction encoder-decoder and the WaveNet vocoder are trained SOFTMAX DISTRIBUTIONSseparately. After the spectrogram predictor is trained, the spectrogram prediction network is run in teacher-forcing mode, with each predicted spectral frame condipproach to modeling the conditional distributions p (xt | x1, . . . , xt 1) over the inditioned on the encoded text input and the previous frame from the ground truth spectrogram. This sequence of ground truth-aligned spectral features and gold audio output is then used to train the vocoder.

This has been only a high-level sketch of the TTS process. There are numerreasons is that a categorical distribution is more flexible and can more easily model arbous important details that the reader interested in going further with TTS may want to look into. For example WaveNet uses a special kind of a gated activation function as its non-linearity, and contains residual and skip connections. In practice, predicting 8-bit audio values doesn’t as work as well as 16-bit, for which a simple softmax is insufficient, so decoders use fancier ways as the last step of predicting audio sample values, like mixtures of distributions. Finally, the WaveNet vocoder as we have described it would be so slow as to be useless; many different kinds of efficiency improvements are necessary in practice, for example by finding ways to do non-autoregressive generation, avoiding the latency of having to wait to generate each frame until the prior frame has been generated, and instead making predictions in parallel. We encourage the interested reader to consult the original papers and various version of the code.

# AB tests

# 16.6.4 TTS Evaluation

Speech synthesis systems are evaluated by human listeners. (The development of a good automatic metric for synthesis evaluation, one that would eliminate the need for expensive and time-consuming human listening experiments, remains an open and exciting research topic.)

We evaluate the quality of synthesized utterances by playing a sentence to listeners and ask them to give a mean opinion score (MOS), a rating of how good the synthesized utterances are, usually on a scale from 1–5. We can then compare systems by comparing their MOS scores on the same sentences (using, e.g., paired t-tests to test for significant differences).

If we are comparing exactly two systems (perhaps to see if a particular change actually improved the system), we can use AB tests. In AB tests, we play the same sentence synthesized by two different systems (an A and a B system). The human listeners choose which of the two utterances they like better. We do this for say 50 sentences (presented in random order) and compare the number of sentences preferred for each system.

# 16.7 Other Speech Tasks

# wake word

While we have focused on speech recognition and TTS in this chapter, there are a wide variety of speech-related tasks.

speaker diarization

The task of wake word detection is to detect a word or short phrase, usually in order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant. The goal with wake words is build the detection into small devices at the computing edge, to maintain privacy by transmitting the least amount of user speech to a cloudbased server. Thus wake word detectors need to be fast, small footprint software that can fit into embedded devices. Wake word detectors usually use the same frontend feature extraction we saw for ASR, often followed by a whole-word classifier.

Speaker diarization is the task of determining ‘who spoke when’ in a long multi-speaker audio recording, marking the start and end of each speaker’s turns in the interaction. This can be useful for transcribing meetings, classroom speech, or medical interactions. Often diarization systems use voice activity detection (VAD) to find segments of continuous speech, extract speaker embedding vectors, and cluster the vectors to group together segments likely from the same speaker. More recent work is investigating end-to-end algorithms to map directly from input speech to a sequence of speaker labels for each frame.

Speaker recognition, is the task of identifying a speaker. We generally distinguish the subtasks of speaker verification, where we make a binary decision (is this speaker $X$ or not?), such as for security when accessing personal information over the telephone, and speaker identification, where we make a one of $N$ decision trying to match a speaker’s voice against a database of many speakers . These tasks are related to language identification, in which we are given a wavefile and must identify which language is being spoken; this is useful for example for automatically directing callers to human operators that speak appropriate languages.

# 16.8 Summary

This chapter introduced the fundamental algorithms of automatic speech recognition (ASR) and text-to-speech (TTS).

• The task of speech recognition (or speech-to-text) is to map acoustic waveforms to sequences of graphemes.   
• The input to a speech recognizer is a series of acoustic waves. that are sampled, quantized, and converted to a spectral representation like the log mel spectrum.   
Two common paradigms for speech recognition are the encoder-decoder with attention model, and models based on the CTC loss function. Attentionbased models have higher accuracies, but models based on CTC more easily adapt to streaming: outputting graphemes online instead of waiting until the acoustic input is complete.   
• ASR is evaluated using the Word Error Rate; the edit distance between the hypothesis and the gold transcription.   
• TTS systems are also based on the encoder-decoder architecture. The encoder maps letters to an encoding, which is consumed by the decoder which generates mel spectrogram output. A neural vocoder then reads the spectrogram and generates waveforms.   
• TTS systems require a first pass of text normalization to deal with numbers and abbreviations and other non-standard words.   
• TTS is evaluated by playing a sentence to human listeners and having them give a mean opinion score (MOS) or by doing AB tests.

# Bibliographical and Historical Notes

ASR A number of speech recognition systems were developed by the late 1940s and early 1950s. An early Bell Labs system could recognize any of the 10 digits from a single speaker (Davis et al., 1952). This system had 10 speaker-dependent stored patterns, one for each digit, each of which roughly represented the first two vowel formants in the digit. They achieved $9 7 \% - 9 9 \%$ accuracy by choosing the pattern that had the highest relative correlation coefficient with the input. Fry (1959) and Denes (1959) built a phoneme recognizer at University College, London, that recognized four vowels and nine consonants based on a similar pattern-recognition principle. Fry and Denes’s system was the first to use phoneme transition probabilities to constrain the recognizer.

warping

The late 1960s and early 1970s produced a number of important paradigm shifts. First were a number of feature-extraction algorithms, including the efficient fast Fourier transform (FFT) (Cooley and Tukey, 1965), the application of cepstral processing to speech (Oppenheim et al., 1968), and the development of LPC for speech coding (Atal and Hanauer, 1971). Second were a number of ways of handling warping; stretching or shrinking the input signal to handle differences in speaking rate and segment length when matching against stored patterns. The natural algorithm for solving this problem was dynamic programming, and, as we saw in Appendix A, the algorithm was reinvented multiple times to address this problem. The first application to speech processing was by Vintsyuk (1968), although his result was not picked up by other researchers, and was reinvented by Velichko and Zagoruyko (1970) and Sakoe and Chiba (1971) (and 1984). Soon afterward, Itakura (1975) combined this dynamic programming idea with the LPC coefficients that had previously been used only for speech coding. The resulting system extracted LPC features from incoming words and used dynamic programming to match them against stored LPC templates. The non-probabilistic use of dynamic programming to match a template against incoming speech is called dynamic time warping.

The third innovation of this period was the rise of the HMM. Hidden Markov models seem to have been applied to speech independently at two laboratories around 1972. One application arose from the work of statisticians, in particular Baum and colleagues at the Institute for Defense Analyses in Princeton who applied HMMs to various prediction problems (Baum and Petrie 1966, Baum and Eagon 1967). James Baker learned of this work and applied the algorithm to speech processing (Baker, 1975a) during his graduate work at CMU. Independently, Frederick Jelinek and collaborators (drawing from their research in information-theoretical models influenced by the work of Shannon (1948)) applied HMMs to speech at the IBM Thomas J. Watson Research Center (Jelinek et al., 1975). One early difference was the decoding algorithm; Baker’s DRAGON system used Viterbi (dynamic programming) decoding, while the IBM system applied Jelinek’s stack decoding algorithm (Jelinek, 1969). Baker then joined the IBM group for a brief time before founding the speech-recognition company Dragon Systems.

The use of the HMM, with Gaussian Mixture Models (GMMs) as the phonetic component, slowly spread through the speech community, becoming the dominant paradigm by the 1990s. One cause was encouragement by ARPA, the Advanced Research Projects Agency of the U.S. Department of Defense. ARPA started a five-year program in 1971 to build 1000-word, constrained grammar, few speaker speech understanding (Klatt, 1977), and funded four competing systems of which Carnegie-Mellon University’s Harpy system (Lowerre, 1976), which used a simplified version of Baker’s HMM-based DRAGON system was the best of the tested systems. ARPA (and then DARPA) funded a number of new speech research programs, beginning with 1000-word speaker-independent read-speech tasks like “Resource Management” (Price et al., 1988), recognition of sentences read from the Wall Street Journal (WSJ), Broadcast News domain (LDC 1998, Graff 1997) (transcription of actual news broadcasts, including quite difficult passages such as on-the-street interviews) and the Switchboard, CallHome, CallFriend, and Fisher domains (Godfrey et al. 1992, Cieri et al. 2004) (natural telephone conversations between friends or strangers). Each of the ARPA tasks involved an approximately annual bakeoff at which systems were evaluated against each other. The ARPA competitions resulted in wide-scale borrowing of techniques among labs since it was easy to see which ideas reduced errors the previous year, and the competitions were probably an important factor in the eventual spread of the HMM paradigm.

By around 1990 neural alternatives to the HMM/GMM architecture for ASR arose, based on a number of earlier experiments with neural networks for phoneme recognition and other speech tasks. Architectures included the time-delay neural network (TDNN)—the first use of convolutional networks for speech— (Waibel et al. 1989, Lang et al. 1990), RNNs (Robinson and Fallside, 1991), and the hybrid HMM/MLP architecture in which a feedforward neural network is trained as a phonetic classifier whose outputs are used as probability estimates for an HMM-based architecture (Morgan and Bourlard 1990, Bourlard and Morgan 1994, Morgan and Bourlard 1995).

While the hybrid systems showed performance close to the standard HMM/GMM models, the problem was speed: large hybrid models were too slow to train on the CPUs of that era. For example, the largest hybrid system, a feedforward network, was limited to a hidden layer of 4000 units, producing probabilities over only a few dozen monophones. Yet training this model still required the research group to design special hardware boards to do vector processing (Morgan and Bourlard, 1995). A later analytic study showed the performance of such simple feedforward MLPs for ASR increases sharply with more than 1 hidden layer, even controlling for the total number of parameters (Maas et al., 2017). But the computational resources of the time were insufficient for more layers.

Over the next two decades a combination of Moore’s law and the rise of GPUs allowed deep neural networks with many layers. Performance was getting close to traditional systems on smaller tasks like TIMIT phone recognition by 2009 (Mohamed et al., 2009), and by 2012, the performance of hybrid systems had surpassed traditional HMM/GMM systems (Jaitly et al. 2012, Dahl et al. 2012, inter alia). Originally it seemed that unsupervised pretraining of the networks using a technique like deep belief networks was important, but by 2013, it was clear that for hybrid HMM/GMM feedforward networks, all that mattered was to use a lot of data and enough layers, although a few other components did improve performance: using log mel features instead of MFCCs, using dropout, and using rectified linear units (Deng et al. 2013, Maas et al. 2013, Dahl et al. 2013).

Meanwhile early work had proposed the CTC loss function by 2006 (Graves et al., 2006), and by 2012 the RNN-Transducer was defined and applied to phone recognition (Graves 2012, Graves et al. 2013), and then to end-to-end speech recognition rescoring (Graves and Jaitly, 2014), and then recognition (Maas et al., 2015), with advances such as specialized beam search (Hannun et al., 2014). (Our description of CTC in the chapter draws on Hannun (2017), which we encourage the interested reader to follow).

The encoder-decoder architecture was applied to speech at about the same time by two different groups, in the Listen Attend and Spell system of Chan et al. (2016) and the attention-based encoder decoder architecture of Chorowski et al. (2014) and Bahdanau et al. (2016). By 2018 Transformers were included in this encoderdecoder architecture. Karita et al. (2019) is a nice comparison of RNNs vs Transformers in encoder-architectures for ASR, TTS, and speech-to-speech translation.

Popular toolkits for speech processing include Kaldi (Povey et al., 2011) and ESPnet (Watanabe et al. 2018, Hayashi et al. 2020).

TTS As we noted at the beginning of the chapter, speech synthesis is one of the earliest fields of speech and language processing. The 18th century saw a number of physical models of the articulation process, including the von Kempelen model mentioned above, as well as the 1773 vowel model of Kratzenstein in Copenhagen

using organ pipes.

The early 1950s saw the development of three early paradigms of waveform synthesis: formant synthesis, articulatory synthesis, and concatenative synthesis.

Modern encoder-decoder systems are distant descendants of formant synthesizers. Formant synthesizers originally were inspired by attempts to mimic human speech by generating artificial spectrograms. The Haskins Laboratories Pattern Playback Machine generated a sound wave by painting spectrogram patterns on a moving transparent belt and using reflectance to filter the harmonics of a waveform (Cooper et al., 1951); other very early formant synthesizers include those of Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant synthesizers were the Klatt formant synthesizer and its successor systems, including the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital Equipment Corporation’s DECtalk (Klatt, 1982). See Klatt (1975) for details.

A second early paradigm, concatenative synthesis, seems to have been first proposed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of magnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) proposed a theoretical model based on diphones, including a database with multiple copies of each diphone with differing prosody, each labeled with prosodic features including F0, stress, and duration, and the use of join costs based on F0 and formant distance between neighboring units. But such diphone synthesis models were not actually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The 1980s and 1990s saw the invention of unit selection synthesis, based on larger units of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al. 1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000).

A third paradigm, articulatory synthesizers attempt to synthesize speech by modeling the physics of the vocal tract as an open tube. Representative models include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt (1975) and Flanagan (1972) for more details.

Most early TTS systems used phonemes as input; development of the text analysis components of TTS came somewhat later, drawing on NLP. Indeed the first true text-to-speech system seems to have been the system of Umeda and Teranishi (Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a parser that assigned prosodic boundaries, as well as accent and stress.

# Exercises

16.1 Analyze each of the errors in the incorrectly recognized transcription of “um the phone is I left the. . . ” on page 346. For each one, give your best guess as to whether you think it is caused by a problem in signal processing, pronunciation modeling, lexicon size, language model, or pruning in the decoding search.

# Part III

ANNOTATING LINGUISTIC STRUCTURE

In the final part of the book we discuss the task of detecting linguistic structure. In the early history of NLP these structures were an intermediate step toward deeper language processing. In modern NLP, we don’t generally make explicit use of parse or other structures inside the neural language models we introduced in Part I, or directly in applications like those we discussed in Part II.

Instead linguistic structure plays a number of new roles. One important role is for interpretability: to provide a useful interpretive lens on neural networks. Knowing that a particular layer or neuron may be computing something related to a particular kind of structure can help us break open the ‘black box’ and understand what the components of our language models are doing.

A second important role for linguistic structure is as a practical tool for social scientific studies of text: knowing which adjective modifies which noun, or whether a particular implicit metaphor is being used, can be important for measuring attitudes toward groups or individuals. Detailed semantic structure can be helpful, for example in finding particular clauses that have particular meanings in legal contracts. Word sense labels can help keep any corpus study from measuring facts about the wrong word sense. Relation structures can be used to help build knowledge bases from text.

Finally, computation of linguistic structure is an important tool for answering questions about language itself, a research area called computational linguistics that is sometimes distinguished from natural language processing. To answer linguistic questions about how language changes over time or across individuals we’ll need to be able, for example, to parse entire documents from different time periods. To understand how certain linguistic structures are learned or processed by people, it’s necessary to be able to automatically label structures for arbitrary text.

In our study of linguistic structure, we begin with one of the oldest tasks in computational linguistics: the extraction of syntactic structure, and give two sets of algorithms for parsing: extracting syntactic structure, including constituency parsing and dependency parsing. We then introduce a variety of structures related to meaning, including semantic roles, word senses, entity relations, and events. We conclude with linguistic structures that tend to be related to discourse and meaning over larger texts, including coreference and discourse coherence. In each case we’ll give algorithms for automatically annotating the relevant structure.

# CHAPTER 17 Sequence Labeling for Parts ofSpeech and Named Entities

To each word a warbling note A Midsummer Night’s Dream, V.I

parts of speech

Dionysius Thrax of Alexandria $\left( c . \ 1 0 0 \ \mathbf { B } . \mathbf { C } . \right)$ , or perhaps someone else (it was a long time ago), wrote a grammatical sketch of Greek (a “techne¯”) that summarized the linguistic knowledge of his day. This work is the source of an astonishing proportion of modern linguistic vocabulary, including the words syntax, diphthong, clitic, and analogy. Also included are a description of eight parts of speech: noun, verb, pronoun, preposition, adverb, conjunction, participle, and article. Although earlier scholars (including Aristotle as well as the Stoics) had their own lists of parts of speech, it was Thrax’s set of eight that became the basis for descriptions of European languages for the next 2000 years. (All the way to the Schoolhouse Rock educational television shows of our childhood, which had songs about 8 parts of speech, like the late great Bob Dorough’s Conjunction Junction.) The durability of parts of speech through two millennia speaks to their centrality in models of human language.

named entity

Proper names are another important and anciently studied linguistic category. While parts of speech are generally assigned to individual words or morphemes, a proper name is often an entire multiword phrase, like the name “Marie Curie”, the location “New York City”, or the organization “Stanford University”. We’ll use the term named entity for, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization, although as we’ll see the term is commonly extended to include things that aren’t entities per se.

Parts of speech (also known as POS) and named entities are useful clues to sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction.

In this chapter we’ll introduce the task of part-of-speech tagging, taking a sequence of words and assigning each word a part of speech like NOUN or VERB, and the task of named entity recognition (NER), assigning words or phrases tags like PERSON, LOCATION, or ORGANIZATION.

Such tasks in which we assign, to each word $x _ { i }$ in an input word sequence, a label $y _ { i }$ , so that the output sequence Y has the same length as the input sequence $X$ are called sequence labeling tasks. We’ll introduce classic sequence labeling algorithms, one generative— the Hidden Markov Model (HMM)—and one discriminative— the Conditional Random Field (CRF). In following chapters we’ll introduce modern sequence labelers based on RNNs and Transformers.

# 17.1 (Mostly) English Word Classes

Until now we have been using part-of-speech terms like noun and verb rather freely. In this section we give more complete definitions. While word classes do have semantic tendencies—adjectives, for example, often describe properties and nouns people— parts of speech are defined instead based on their grammatical relationship with neighboring words or the morphological properties about their affixes.

<table><tr><td>Tag</td><td>Description</td><td>Example</td></tr><tr><td>ADJ ADV NOUN VERB PROPN INTJ</td><td>Adjective: noun modifiers describing properties Adverb: verb modifiers of time, place, manner words for persons, places, things, etc. words for actions and processes Proper noun: name of a person, organization, place, etc.. Interjection: exclamation, greeting, yes/no response, etc.</td><td>red,young,awesome very, slowly, home,yesterday algorithm,cat,mango,beauty draw, provide, go Regina,IBM, Colorado</td></tr><tr><td>ADP AUX DET NUM PART</td><td>Adposition (Preposition/Postposition): marks a noun&#x27;s spacial, temporal, or other relation Auxiliary: helping verb marking tense, aspect, mood, etc., CCONJ Coordinating Conjunction: joins two phrases/clauses Determiner: marks noun phrase properties Numeral Particle: a function word that must be associated with an- other word</td><td>oh, um, yes, hello in, on, by,under can, may, should, are and, or, but a, an, the, this one, two, 2026,11:00, hundred &#x27;s,not,(infinitive) to</td></tr><tr><td>PRON SCONJ</td><td>Pronoun: a shorthand for referring to an entity or event Subordinating Conjunction: joins a main clause with a subordinate clause such as a sentential complement SYNCT Psymbols ikes or eahi</td><td>she, who,I, others whether,because</td></tr></table>

Figure 17.1 The 17 parts of speech in the Universal Dependencies tagset (de Marneffe et al., 2021). Features can be added to make finer-grained distinctions (with properties like number, case, definiteness, and so on).

# closed class open class

# function word

Parts of speech fall into two broad categories: closed class and open class. Closed classes are those with relatively fixed membership, such as prepositions— new prepositions are rarely coined. By contrast, nouns and verbs are open classes— new nouns and verbs like iPhone or to fax are continually being created or borrowed. Closed class words are generally function words like of, it, and, or you, which tend to be very short, occur frequently, and often have structuring uses in grammar.

Four major open classes occur in the languages of the world: nouns (including proper nouns), verbs, adjectives, and adverbs, as well as the smaller open class of interjections. English has all five, although not every language does.

noun common noun count noun mass noun

Nouns are words for people, places, or things, but include others as well. Common nouns include concrete terms like cat and mango, abstractions like algorithm and beauty, and verb-like terms like pacing as in His pacing to and fro became quite annoying. Nouns in English can occur with determiners (a goat, this bandwidth) take possessives (IBM’s annual revenue), and may occur in the plural (goats, abaci). Many languages, including English, divide common nouns into count nouns and mass nouns. Count nouns can occur in the singular and plural (goat/goats, relationship/relationships) and can be counted (one goat, two goats). Mass nouns are used when something is conceptualized as a homogeneous group. So snow, salt, and communism are not counted (i.e., \*two snows or \*two communisms). Proper nouns, like Regina, Colorado, and IBM, are names of specific persons or entities.

# verb

Verbs refer to actions and processes, including main verbs like draw, provide, and go. English verbs have inflections (non-third-person-singular (eat), third-personsingular (eats), progressive (eating), past participle (eaten)). While many scholars believe that all human languages have the categories of noun and verb, others have argued that some languages, such as Riau Indonesian and Tongan, don’t even make this distinction (Broschart 1997; Evans 2000; Gil 2000) .

adjective

Adjectives often describe properties or qualities of nouns, like color (white, black), age (old, young), and value (good, bad), but there are languages without adjectives. In Korean, for example, the words corresponding to English adjectives act as a subclass of verbs, so what is in English an adjective “beautiful” acts in Korean like a verb meaning “to be beautiful”.

adverb

Adverbs are a hodge-podge. All the italicized words in this example are adverbs:

Actually, I ran home extremely quickly yesterday

locative degree manner

Adverbs generally modify something (often verbs, hence the name “adverb”, but also other adverbs and entire verb phrases). Directional adverbs or locative adverbs (home, here, downhill) specify the direction or location of some action; degree adverbs (extremely, very, somewhat) specify the extent of some action, process, or property; manner adverbs (slowly, slinkily, delicately) describe the manner of some action or process; and temporal adverbs describe the time that some action or event took place (yesterday, Monday).

temporal interjection preposition

Interjections (oh, hey, alas, uh, um) are a smaller open class that also includes greetings (hello, goodbye) and question responses (yes, no, uh-huh).

English adpositions occur before nouns, hence are called prepositions. They can indicate spatial or temporal relations, whether literal (on it, before then, by the house) or metaphorical (on time, with gusto, beside herself), and relations like marking the agent in Hamlet was written by Shakespeare.

# particle

phrasal verb

A particle resembles a preposition or an adverb and is used in combination with a verb. Particles often have extended meanings that aren’t quite the same as the prepositions they resemble, as in the particle over in she turned the paper over. A verb and a particle acting as a single unit is called a phrasal verb. The meaning of phrasal verbs is often non-compositional—not predictable from the individual meanings of the verb and the particle. Thus, turn down means ‘reject’, rule out ‘eliminate’, and go on ‘continue’.

determiner article

Determiners like this and that (this chapter, that page) can mark the start of an English noun phrase. Articles like a, an, and the, are a type of determiner that mark discourse properties of the noun and are quite frequent; the is the most common word in written English, with a and an right behind.

conjunction

Conjunctions join two phrases, clauses, or sentences. Coordinating conjunctions like and, or, and but join two elements of equal status. Subordinating conjunctions are used when one of the elements has some embedded status. For example, the subordinating conjunction that in “I thought that you might like some milk” links the main clause I thought with the subordinate clause you might like some milk. This clause is called subordinate because this entire clause is the “content” of the main verb thought. Subordinating conjunctions like that which link a verb to its argument in this way are also called complementizers.

complementizer pronoun

Pronouns act as a shorthand for referring to an entity or event. Personal pronouns refer to persons or entities (you, she, I, it, me, etc.). Possessive pronouns are forms of personal pronouns that indicate either actual possession or more often just an abstract relation between the person and some object (my, your, his, her, its, one’s, our, their). Wh-pronouns (what, who, whom, whoever) are used in certain question forms, or act as complementizers (Frida, who married Diego. . . ).

copula modal

Auxiliary verbs mark semantic features of a main verb such as its tense, whether it is completed (aspect), whether it is negated (polarity), and whether an action is necessary, possible, suggested, or desired (mood). English auxiliaries include the copula verb be, the two verbs do and have, forms, as well as modal verbs used to mark the mood associated with the event depicted by the main verb: can indicates ability or possibility, may permission or possibility, must necessity.

An English-specific tagset, the Penn Treebank tagset (Marcus et al., 1993), shown in Fig. 17.2, has been used to label many syntactically annotated corpora like the Penn Treebank corpora, so it is worth knowing about.

<table><tr><td></td><td>Tag Description</td><td>Example</td><td>Tag</td><td>Description</td><td>Example</td><td>Tag</td><td>Description</td><td> Example</td></tr><tr><td></td><td>CC coord. conj.</td><td>and, but, or NNP</td><td></td><td> proper noun, sing.</td><td>IBM</td><td>TO</td><td> infinitive to</td><td>to</td></tr><tr><td>CD</td><td> cardinal number</td><td> one, two</td><td></td><td> NNPS proper noun, plu.</td><td>Carolinas UH</td><td></td><td> interjection</td><td>ah, oops</td></tr><tr><td>DT</td><td>determiner</td><td>a, the</td><td>NNS</td><td> noun, plural </td><td>llamas</td><td>VB</td><td>verb base</td><td>eat</td></tr><tr><td>EX</td><td>existential ‘there’</td><td>there</td><td> PDT</td><td> predeterminer</td><td>all, both</td><td></td><td> VBD verb past tense</td><td>ate</td></tr><tr><td>IN</td><td>FW foreign word</td><td>mea culpa</td><td>POS PRP</td><td> possessive ending</td><td>&#x27;s</td><td></td><td> VBG verb gerund</td><td>eating</td></tr><tr><td></td><td>preposition/ subordin-conj</td><td>of, in, by</td><td></td><td> personal pronoun </td><td> I, you, he</td><td></td><td>VBN verb past partici- eaten ple</td><td></td></tr><tr><td>JJ</td><td>adjective</td><td>yellow</td><td> PRP$</td><td> possess. pronoun</td><td>your</td><td></td><td>VBP verb non-3sg-pr</td><td>eat</td></tr><tr><td> JJR</td><td> comparative adj</td><td>bigger</td><td>RB</td><td>adverb</td><td> quickly</td><td></td><td>VBZ verb 3sg pres</td><td>eats</td></tr><tr><td> JJS</td><td> superlative adj</td><td>wildest</td><td>RBR</td><td> comparative adv</td><td>faster</td><td></td><td> WDT wh-determ.</td><td>which, that</td></tr><tr><td>LS</td><td> list item marker</td><td>1, 2, One</td><td>RBS</td><td> superlatv. adv</td><td>fastest</td><td>WP</td><td> wh-pronoun </td><td>what, who</td></tr><tr><td> MD modal</td><td></td><td> can, should RP</td><td></td><td>particle</td><td>up,off</td><td></td><td>WP$ wh-possess.</td><td>whose</td></tr><tr><td></td><td> NN sing or mass noun llama</td><td></td><td> SYM</td><td> symbol</td><td>+,%,&amp;</td><td></td><td>WRB wh-adverb</td><td>how, where</td></tr></table>

Figure 17.2 Penn Treebank core 36 part-of-speech tags.

Below we show some examples with each word tagged according to both the UD (in blue) and Penn (in red) tagsets. Notice that the Penn tagset distinguishes tense and participles on verbs, and has a special tag for the existential there construction in English. Note that since London Journal of Medicine is a proper noun, both tagsets mark its component nouns as PROPN/NNP, including journal and medicine, which might otherwise be labeled as common nouns (NOUN/NN).

(17.1) There/PRON/EX are/VERB/VBP 70/NUM/CD children/NOUN/NNS there/ADV/RB ./PUNC/.   
(17.2) Preliminary/ADJ/JJ findings/NOUN/NNS were/AUX/VBD reported/VERB/VBN in/ADP/IN today/NOUN/NN ’s/PART/POS London/PROPN/NNP Journal/PROPN/NNP of/ADP/IN Medicine/PROPN/NNP

# 17.2 Part-of-Speech Tagging

# part-of-speech tagging

Part-of-speech tagging is the process of assigning a part-of-speech to each word in a text. The input is a sequence $x _ { 1 } , x _ { 2 } , . . . , x _ { n }$ of (tokenized) words and a tagset, and the output is a sequence $y _ { 1 } , y _ { 2 } , . . . , y _ { n }$ of tags, each output $y _ { i }$ corresponding exactly to one input $x _ { i }$ , as shown in the intuition in Fig. 17.3.

Tagging is a disambiguation task; words are ambiguous —have more than one possible part-of-speech—and the goal is to find the correct tag for the situation. For example, book can be a verb (book that flight) or a noun (hand me that book). That can be a determiner (Does that flight serve dinner) or a complementizer (I thought that your flight was earlier). The goal of POS-tagging is to resolve these ambiguities, choosing the proper tag for the context.

![## Image Analysis: 2ca39a2f66734235dfaeca4d627868ba70b8870a5f0b7884f3eb7d8f6bb37005.jpg

**Conceptual Understanding:**
This image represents the conceptual flow of part-of-speech tagging, a core task in natural language processing. Its main purpose is to illustrate how individual words in a sentence are identified and assigned their respective grammatical categories. The key idea being communicated is the transformation of a raw word sequence into a sequence of corresponding grammatical tags by a specialized tagging system.

**Content Interpretation:**
The image illustrates the process of part-of-speech (POS) tagging. It depicts a system that takes a sequence of words as input and assigns a grammatical category (part-of-speech tag) to each word in the sequence as output. The orange rectangular shape labeled 'Part of Speech Tagger' represents the computational process or model that performs this task. The individual words are shown with their sequential indices (x_1 to x_5), and their corresponding assigned tags are shown with sequential indices (y_1 to y_5), demonstrating a direct mapping.

**Key Insights:**
The main takeaway from this image is a clear visualization of the part-of-speech tagging process. It demonstrates that for every input word in a sequence (e.g., 'Janet', 'will', 'back', 'the', 'bill' labeled x_1 to x_5), a specific grammatical category (e.g., 'NOUN', 'AUX', 'VERB', 'DET', 'NOUN' labeled y_1 to y_5) is assigned. The image highlights the sequential and deterministic nature of this mapping, where a dedicated 'Part of Speech Tagger' performs this classification. It provides a foundational understanding of how words are categorized grammatically within a computational framework.

**Document Context:**
The image directly supports the document's section on 'part-of-speech tagging' by providing a concrete visual example of the task. The accompanying text, 'Figure 17.3 The task of part-of-speech tagging: mapping from input words x_1, x_2, ..., x_n to output POS tags y_1, y_2, ..., y_n', explicitly defines the mapping shown in the diagram. The diagram visually clarifies how a sentence, broken into individual words, is processed to yield a corresponding sequence of grammatical tags, thus enhancing the reader's understanding of this fundamental natural language processing concept.

**Summary:**
This image visually represents the process of part-of-speech tagging. It illustrates a sequence of input words, each labeled as an 'x' variable with an index, which are processed by a component labeled 'Part of Speech Tagger'. The output of this tagger is a corresponding sequence of part-of-speech tags, each labeled as a 'y' variable with an index. For example, the input word 'Janet' (x₁) is tagged as 'NOUN' (y₁), 'will' (x₂) as 'AUX' (y₂), 'back' (x₃) as 'VERB' (y₃), 'the' (x₄) as 'DET' (y₄), and 'bill' (x₅) as 'NOUN' (y₅). The diagram clearly shows a one-to-one mapping from each input word to its assigned grammatical category.](images/2ca39a2f66734235dfaeca4d627868ba70b8870a5f0b7884f3eb7d8f6bb37005.jpg)
Figure 17.3 The task of part-of-speech tagging: mapping from input words $x _ { 1 } , x _ { 2 } , . . . , x _ { n }$ to output POS tags $y _ { 1 } , y _ { 2 } , . . . , y _ { n }$ .

The accuracy of part-of-speech tagging algorithms (the percentage of test set tags that match human gold labels) is extremely high. One study found accuracies over $9 7 \%$ across 15 languages from the Universal Dependency (UD) treebank ( $\mathrm { W u }$ and Dredze, 2019). Accuracies on various English treebanks are also $9 7 \%$ (no matter the algorithm; HMMs, CRFs, BERT perform similarly). This $9 7 \%$ number is also about the human performance on this task, at least for English (Manning, 2011).

<table><tr><td colspan="2">Types:</td><td colspan="2">WSJ</td><td colspan="2">Brown</td></tr><tr><td>Unambiguous</td><td>(1 tag)</td><td>44,432</td><td>(86%)</td><td>45,799</td><td>(85%)</td></tr><tr><td> Ambiguous</td><td>(2+ tags)</td><td>7,025</td><td>(14%)</td><td>8,050</td><td>(15%)</td></tr><tr><td>Tokens:</td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>Unambiguous (1 tag)</td><td></td><td>577,421</td><td>(45%)</td><td>384,349</td><td>(33%)</td></tr><tr><td>Ambiguous</td><td>(2+ tags)</td><td>711,780</td><td>(55%)</td><td>786,646</td><td>(67%)</td></tr></table>

Figure 17.4 Tag ambiguity in the Brown and WSJ corpora (Treebank-3 45-tag tagset).

We’ll introduce algorithms for the task in the next few sections, but first let’s explore the task. Exactly how hard is it? Fig. 17.4 shows that most word types $( 8 5 - 8 6 \% )$ are unambiguous (Janet is always NNP, hesitantly is always RB). But the ambiguous words, though accounting for only $1 4 \mathrm { - } 1 5 \%$ of the vocabulary, are very common, and $5 5 \mathrm { - } 6 7 \%$ of word tokens in running text are ambiguous. Particularly ambiguous common words include that, back, down, put and set; here are some examples of the 6 different parts of speech for the word back:

earnings growth took a back/JJ seat a small building in the back/NN a clear majority of senators back/VBP the bill Dave began to back/VB toward the door enable the country to buy back/RP debt I was twenty-one back/RB then

Nonetheless, many words are easy to disambiguate, because their different tags aren’t equally likely. For example, a can be a determiner or the letter $a$ , but the determiner sense is much more likely.

This idea suggests a useful baseline: given an ambiguous word, choose the tag which is most frequent in the training corpus. This is a key concept:

Most Frequent Class Baseline: Always compare a classifier against a baseline at least as good as the most frequent class baseline (assigning each token to the class it occurred in most often in the training set).

The most-frequent-tag baseline has an accuracy of about $9 2 \% ^ { 1 }$ . The baseline thus differs from the state-of-the-art and human ceiling $( 9 7 \% )$ by only $5 \%$ .

# 17.3 Named Entities and Named Entity Tagging

named entity named entity named entity recognition NER

Part of speech tagging can tell us that words like Janet, Stanford University, and Colorado are all proper nouns; being a proper noun is a grammatical property of these words. But viewed from a semantic perspective, these proper nouns refer to different kinds of entities: Janet is a person, Stanford University is an organization, and Colorado is a location.

Here we re-introduce the concept of a named entity, which was also introduced in Section 11.5 for readers who haven’t yet read Chapter 11.

A named entity is, roughly speaking, anything that can be referred to with a proper name: a person, a location, an organization. The task of named entity recognition (NER) is to find spans of text that constitute proper names and tag the type of the entity. Four entity tags are most common: PER (person), LOC (location), ORG (organization), or GPE (geo-political entity). However, the term named entity is commonly extended to include things that aren’t entities per se, including dates, times, and other kinds of temporal expressions, and even numerical expressions like prices. Here’s an example of the output of an NER tagger:

Citing high fuel prices, [ORG United Airlines] said $\mathrm { [ _ { T I M E } }$ Friday] it has increased fares by [MONEY $\$ 6]$ per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect [TIME Thursday] and applies to most routes where it competes against discount carriers, such as $\operatorname { I } _ { \mathrm { L O C } }$ Chicago] to [LOC Dallas] and $\operatorname { I } _ { \mathrm { L O C } }$ Denver] to [LOC San Francisco].

The text contains 13 mentions of named entities including 5 organizations, 4 locations, 2 times, 1 person, and 1 mention of money. Figure 17.5 shows typical generic named entity types. Many applications will also need to use specific entity types like proteins, genes, commercial products, or works of art.

<table><tr><td>Type</td><td>Tag Sample Categories</td><td>Example sentences</td><td></td></tr><tr><td>People</td><td>PER</td><td> people, characters</td><td> Turing is a giant of computer science.</td></tr><tr><td>Organization</td><td>ORG</td><td> companies, sports teams</td><td> The IPCC warned about the cyclone.</td></tr><tr><td>Location</td><td>LOC</td><td> regions, mountains, seas</td><td> Mt. Sanitas is in Sunshine Canyon.</td></tr><tr><td>Geo-Political Entity</td><td>GPE</td><td>countries, states</td><td> Palo Alto is raising the fees for parking.</td></tr></table>

Figure 17.5 A list of generic named entity types with the kinds of entities they refer to.

Named entity tagging is a useful first step in lots of natural language processing tasks. In sentiment analysis we might want to know a consumer’s sentiment toward a particular entity. Entities are a useful first stage in question answering, or for linking text to information in structured knowledge sources like Wikipedia. And named entity tagging is also central to tasks involving building semantic representations, like extracting events and the relationship between participants.

Unlike part-of-speech tagging, where there is no segmentation problem since each word gets one tag, the task of named entity recognition is to find and label spans of text, and is difficult partly because of the ambiguity of segmentation; we need to decide what’s an entity and what isn’t, and where the boundaries are. Indeed, most words in a text will not be named entities. Another difficulty is caused by type ambiguity. The mention JFK can refer to a person, the airport in New York, or any number of schools, bridges, and streets around the United States. Some examples of this kind of cross-type confusion are given in Figure 17.6.

<table><tr><td>[PER Washington] was born into slavery on the farm of James Burroughs. [ORG Washington] went up 2 games to 1 in the four-game series.</td></tr><tr><td> Blair arrived in [LOc Washington] for what may wellbe his last state visit.</td></tr><tr><td> In June, [GPE Washington] passed a primary seatbelt law.</td></tr></table>

Figure 17.6 Examples of type ambiguities in the use of the name Washington.

The standard approach to sequence labeling for a span-recognition problem like NER is BIO tagging (Ramshaw and Marcus, 1995). This is a method that allows us to treat NER like a word-by-word sequence labeling task, via tags that capture both the boundary and the named entity type. Consider the following sentence:

[PER Jane Villanueva ] of [ORG United] , a unit of [ORG United Airlines Holding] , said the fare applies to the [LOC Chicago ] route.

Figure 17.7 shows the same excerpt represented with BIO tagging, as well as variants called IO tagging and BIOES tagging. In BIO tagging we label any token that begins a span of interest with the label B, tokens that occur inside a span are tagged with an I, and any tokens outside of any span of interest are labeled O. While there is only one O tag, we’ll have distinct B and I tags for each named entity class. The number of tags is thus $2 n + 1$ tags, where $n$ is the number of entity types. BIO tagging can represent exactly the same information as the bracketed notation, but has the advantage that we can represent the task in the same simple sequence modeling way as part-of-speech tagging: assigning a single label $y _ { i }$ to each input word $x _ { i }$ :

<table><tr><td>Words</td><td>IO Label</td><td>BIO Label</td><td>BIOES Label</td></tr><tr><td>Jane</td><td>I-PER</td><td>B-PER</td><td>B-PER</td></tr><tr><td>Villanueva</td><td>I-PER</td><td>I-PER</td><td>E-PER</td></tr><tr><td>of</td><td>0</td><td>0</td><td>0</td></tr><tr><td>United</td><td>I-ORG</td><td>B-ORG</td><td>B-ORG</td></tr><tr><td> Airlines</td><td>I-ORG</td><td>I-ORG</td><td>I-ORG</td></tr><tr><td>Holding</td><td>I-ORG</td><td>I-ORG</td><td>E-ORG</td></tr><tr><td>discussed</td><td>0</td><td>0</td><td>0</td></tr><tr><td> the</td><td>0</td><td>0</td><td>0</td></tr><tr><td>Chicago</td><td>I-LOC</td><td>B-LOC</td><td> S-LOC</td></tr><tr><td> route</td><td>0</td><td>0</td><td>0</td></tr><tr><td></td><td>0</td><td>0</td><td>0</td></tr></table>

Figure 17.7 NER as a sequence model, showing IO, BIO, and BIOES taggings.

We’ve also shown two variant tagging schemes: IO tagging, which loses some information by eliminating the B tag, and BIOES tagging, which adds an end tag E for the end of a span, and a span tag S for a span consisting of only one word. A sequence labeler (HMM, CRF, RNN, Transformer, etc.) is trained to label each token in a text with tags that indicate the presence (or absence) of particular kinds of named entities.

# 17.4 HMM Part-of-Speech Tagging

In this section we introduce our first sequence labeling algorithm, the Hidden Markov Model, and show how to apply it to part-of-speech tagging. Recall that a sequence labeler is a model whose job is to assign a label to each unit in a sequence, thus mapping a sequence of observations to a sequence of labels of the same length. The HMM is a classic model that introduces many of the key concepts of sequence modeling that we will see again in more modern models.

An HMM is a probabilistic sequence model: given a sequence of units (words, letters, morphemes, sentences, whatever), it computes a probability distribution over possible sequences of labels and chooses the best label sequence.

# 17.4.1 Markov Chains

# Markov chain

The HMM is based on augmenting the Markov chain. A Markov chain is a model that tells us something about the probabilities of sequences of random variables, states, each of which can take on values from some set. These sets can be words, or tags, or symbols representing anything, for example the weather. A Markov chain makes a very strong assumption that if we want to predict the future in the sequence, all that matters is the current state. All the states before the current state have no impact on the future except via the current state. It’s as if to predict tomorrow’s weather you could examine today’s weather but you weren’t allowed to look at yesterday’s weather.

![## Image Analysis: e302142f2afcd07006e98476badb4e96ac4711d6a77db8ece10f9a679e27fc25.jpg

**Conceptual Understanding:**
The image conceptually represents two examples of discrete-time Markov chains. These are mathematical models that describe a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.

The main purpose is to illustrate how Markov chains define states and the probabilities of transitioning between these states. It shows two distinct applications: modeling sequential weather states and modeling sequences of words.

The key ideas communicated are states, transitions, and transition probabilities. Each state is a condition or item, and the arrows represent the likelihood of moving from one state to another, including remaining in the same state. The sum of probabilities leaving any given state must equal 1.0.

**Content Interpretation:**
The image shows the fundamental structure of a Markov chain. Each circle represents a "state," and the arrows represent "transitions" between states. The numbers on the arrows are "transition probabilities," indicating the likelihood of moving from the source state to the destination state.

Significance:
*   Weather Markov Chain (a): This diagram models daily weather changes.
    *   States: HOT₁ (state 1), COLD₂ (state 2), WARM₃ (state 3). The subscript numbers (1, 2, 3) denote state indices.
    *   Transitions: If today is HOT₁, there's a 0.6 probability it will be HOT₁ tomorrow, a 0.1 probability it will be COLD₂ tomorrow, and a 0.3 probability it will be WARM₃ tomorrow. If today is COLD₂, there's a 0.8 probability it will be COLD₂ tomorrow, a 0.1 probability it will be HOT₁ tomorrow, and a 0.1 probability it will be WARM₃ tomorrow. If today is WARM₃, there's a 0.6 probability it will be WARM₃ tomorrow, a 0.3 probability it will be HOT₁ tomorrow, and a 0.1 probability it will be COLD₂ tomorrow.
    *   Interpretation: This model suggests that cold weather is highly persistent (0.8 probability of staying cold), while hot and warm weather are less so (0.6 probability of staying the same), but with notable probabilities of transitioning to other states. For each state, the sum of probabilities of all outgoing transitions (including self-loops) is 1.0.
*   Words Markov Chain (b): This diagram models sequences of words in a simplified language model.
    *   States: uniformly, are, charming.
    *   Transitions: If the current word is "uniformly", there's a 0.1 probability the next word is "uniformly", a 0.4 probability the next word is "are", and a 0.5 probability the next word is "charming". If the current word is "are", there's a 0.5 probability the next word is "uniformly" and a 0.5 probability the next word is "charming". If the current word is "charming", there's a 0.2 probability the next word is "charming", a 0.6 probability the next word is "uniformly", and a 0.2 probability the next word is "are".
    *   Interpretation: This model provides probabilities for word sequences, implying a context where "uniformly" is often followed by "charming" (0.5) or "are" (0.4), and "charming" is quite likely to be followed by "uniformly" (0.6). The word "are" seems to act as a connector between "uniformly" and "charming". For each state, the sum of probabilities of all outgoing transitions (including self-loops) is 1.0.

All interpretations are directly supported by the verbatim text (state names) and numerical values (transition probabilities) extracted from the diagrams. For example, the persistence of cold weather is evidenced by the ".8" self-loop on "COLD₂".

**Key Insights:**
Main Takeaways:
*   Markov chains provide a powerful way to model systems that change states over time, where future states depend only on the current state (the Markov property).
*   They are characterized by discrete states and a matrix of transition probabilities between these states.
*   These models are versatile, applicable to diverse fields like weather forecasting and natural language processing.

Conclusions/Insights:
*   The sum of probabilities leaving any given state must always equal 1.0, which is confirmed by summing the probabilities for each state in both diagrams (e.g., for HOT₁: 0.6 + 0.1 + 0.3 = 1.0; for "uniformly": 0.1 + 0.4 + 0.5 = 1.0).
*   Self-loops indicate the probability of remaining in the same state in the next step (e.g., a 0.6 probability that HOT₁ remains HOT₁, or a 0.2 probability that "charming" is followed by "charming").
*   Different states can have different levels of "stickiness" (likelihood of staying in the same state) or likelihood of transitioning to other specific states, as seen in the varying probabilities.

Textual Evidence: The specific state labels (HOT₁, COLD₂, WARM₃, uniformly, are, charming) and all the numerical transition probabilities (0.6, 0.1, 0.3, 0.8, 0.4, 0.5, 0.2, etc.) are the direct textual evidence for these insights. For instance, the transition probability of 0.8 for COLD₂ to COLD₂ shows the 

**Document Context:**
This image directly supports the "Markov chain" section of the document by providing concrete, visual examples of how Markov chains are structured and applied. It helps to ground the abstract concept of a Markov chain in practical scenarios, making the theoretical discussion more understandable. The accompanying text, which mentions a "start distribution 
$\pi$
" and provides an example like setting 
$\pi = [ 0 . 1 , 0 . 7 , 0 . 2 ]$
 for (a) (meaning a 0.7 probability of starting in state 2 (cold), 0.1 for state 1 (hot), etc.), further clarifies that these diagrams represent the *transition probabilities* between states, and an initial probability distribution is needed to begin a chain and fully understand its evolution.

**Summary:**
The image presents two distinct Markov chains, labeled (a) and (b), which illustrate how systems can transition between different states with specific probabilities. Each chain consists of nodes (circles) representing "states" and directed arrows representing "transitions" between these states. The numbers on the arrows indicate the probability of moving from one state to another.

Diagram (a): Markov Chain for Weather
This diagram models the daily weather as transitioning between three states:
*   HOT₁: Represents a hot weather state (State 1).
*   COLD₂: Represents a cold weather state (State 2).
*   WARM₃: Represents a warm weather state (State 3).

The transitions and their probabilities are as follows:
*   From HOT₁:
    *   There is a 0.6 probability that hot weather will remain HOT₁ the next day (indicated by the arrow looping back to HOT₁).
    *   There is a 0.1 probability that hot weather will change to COLD₂ the next day.
    *   There is a 0.3 probability that hot weather will change to WARM₃ the next day.
    (Note: 0.6 + 0.1 + 0.3 = 1.0, summing all probabilities from HOT₁)
*   From COLD₂:
    *   There is a 0.8 probability that cold weather will remain COLD₂ the next day.
    *   There is a 0.1 probability that cold weather will change to HOT₁ the next day.
    *   There is a 0.1 probability that cold weather will change to WARM₃ the next day.
    (Note: 0.8 + 0.1 + 0.1 = 1.0, summing all probabilities from COLD₂)
*   From WARM₃:
    *   There is a 0.6 probability that warm weather will remain WARM₃ the next day.
    *   There is a 0.3 probability that warm weather will change to HOT₁ the next day.
    *   There is a 0.1 probability that warm weather will change to COLD₂ the next day.
    (Note: 0.6 + 0.3 + 0.1 = 1.0, summing all probabilities from WARM₃)

This weather model suggests that cold days tend to be followed by more cold days with high probability, while hot and warm days have a slightly lower chance of repeating, with significant probabilities of transitioning to other weather types.

Diagram (b): Markov Chain for Words
This diagram models the sequence of words, illustrating the probability of one word following another, using three specific words as states:
*   uniformly
*   are
*   charming

The transitions and their probabilities are as follows:
*   From "uniformly":
    *   There is a 0.1 probability that "uniformly" will be followed by "uniformly" (e.g., "uniformly uniformly").
    *   There is a 0.4 probability that "uniformly" will be followed by "are".
    *   There is a 0.5 probability that "uniformly" will be followed by "charming".
    (Note: 0.1 + 0.4 + 0.5 = 1.0, summing all probabilities from "uniformly")
*   From "are":
    *   There is a 0.5 probability that "are" will be followed by "uniformly".
    *   There is a 0.5 probability that "are" will be followed by "charming".
    (Note: 0.5 + 0.5 = 1.0, summing all probabilities from "are")
*   From "charming":
    *   There is a 0.2 probability that "charming" will be followed by "charming".
    *   There is a 0.6 probability that "charming" will be followed by "uniformly".
    *   There is a 0.2 probability that "charming" will be followed by "are".
    (Note: 0.2 + 0.6 + 0.2 = 1.0, summing all probabilities from "charming")

This word model demonstrates how Markov chains can predict the likelihood of the next word in a sequence, a basic principle used in natural language processing and text generation.

Overall, these diagrams visually represent the concept of a Markov chain by showing discrete states and the probabilistic transitions between them, applicable to both natural phenomena like weather and linguistic structures like word sequences. The start distribution mentioned in the document context would define the initial state's probability before any transitions occur.](images/e302142f2afcd07006e98476badb4e96ac4711d6a77db8ece10f9a679e27fc25.jpg)
Figure 17.8 A Markov chain for weather (a) and one for words (b), showing states and transitions. A start distribution $\pi$ is required; setting $\pi = [ 0 . 1 , 0 . 7 , 0 . 2 ]$ for (a) would mean a probability 0.7 of starting in state 2 (cold), probability 0.1 of starting in state 1 (hot), etc.

More formally, consider a sequence of state variables $q _ { 1 } , q _ { 2 } , . . . , q _ { i }$ . A Markov model embodies the Markov assumption on the probabilities of this sequence: that when predicting the future, the past doesn’t matter, only the present.

$$
P ( q _ { i } = a | q _ { 1 } . . . q _ { i - 1 } ) = P ( q _ { i } = a | q _ { i - 1 } )
$$

Figure 17.8a shows a Markov chain for assigning a probability to a sequence of weather events, for which the vocabulary consists of HOT, COLD, and WARM. The states are represented as nodes in the graph, and the transitions, with their probabilities, as edges. The transitions are probabilities: the values of arcs leaving a given state must sum to 1. Figure 17.8b shows a Markov chain for assigning a probability to a sequence of words $w _ { 1 } . . . w _ { t }$ . This Markov chain should be familiar; in fact, it represents a bigram language model, with each edge expressing the probability $p ( w _ { i } | w _ { j } )$ ! Given the two models in Fig. 17.8, we can assign a probability to any sequence from our vocabulary.

Formally, a Markov chain is specified by the following components:

<table><tr><td>Q= q1q2.qN</td><td> a set of N states</td></tr><tr><td>A= a11a12...aN1.. aNN</td><td>a transition probability matrix A, each aij represent- ing the probability of moving from state i to state j, s.t.</td></tr><tr><td>π= π1,π2,,πN</td><td>£=1aij=1Vi an initial probability distribution over states. πi is the probability that the Markov chain will start in state i. Some states j may have πj = O, meaning that they cannot</td></tr></table>

Before you go on, use the sample probabilities in Fig. 17.8a (with $\pi = [ 0 . 1 , 0 . 7 , 0 .$ 2]) to compute the probability of each of the following sequences:

(17.4) hot hot hot hot (17.5) cold hot cold hot

What does the difference in these probabilities tell you about a real-world weather fact encoded in Fig. 17.8a?

# 17.4.2 The Hidden Markov Model

# hidden

A Markov chain is useful when we need to compute a probability for a sequence of observable events. In many cases, however, the events we are interested in are hidden: we don’t observe them directly. For example we don’t normally observe part-of-speech tags in a text. Rather, we see words, and must infer the tags from the word sequence. We call the tags hidden because they are not observed.

hidden Markov model   

<table><tr><td>Q= q1q2..qN</td><td> a set of N states</td></tr><tr><td></td><td>A = a11...aij ...aNN a transition probability matrix A, each aij representing the probability of moving from state i to state j, s.t. ∑j=1 aij =1 Vi</td></tr><tr><td>B=bi(0t)</td><td>a sequence of observation likelihoods, also called emission probabili- ties, each expressing the probability of an observation ot (drawn from a</td></tr><tr><td>π=π1,π2，.,πN</td><td>vocabulary V = v1, v2,.*, v) being generated from a state qi an initial probability distribution over states. πi is the probability that the Markov chain will start in state i. Some states j may have πj = 0, meaning that they cannot be initial states. Also, ∑&#x27;=1 πi = 1</td></tr></table>

A hidden Markov model (HMM) allows us to talk about both observed events (like words that we see in the input) and hidden events (like part-of-speech tags) that we think of as causal factors in our probabilistic model. An HMM is specified by the following components:

The HMM is given as input $O = o _ { 1 } o _ { 2 } \dots o _ { T }$ : a sequence of $T$ observations, each one drawn from the vocabulary $V$ .

A first-order hidden Markov model instantiates two simplifying assumptions. First, as with a first-order Markov chain, the probability of a particular state depends only on the previous state:

Second, the probability of an output observation $o _ { i }$ depends only on the state that produced the observation $q _ { i }$ and not on any other states or any other observations:

Output Independence: $P ( o _ { i } | q _ { 1 } , \dots q _ { i } , \dots , q _ { T } , o _ { 1 } , \dots , o _ { i } , \dots , o _ { T } ) = P ( o _ { i } | q _ { i } )$

# 17.4.3 The components of an HMM tagger

An HMM has two components, the A and $B$ probabilities, both estimated by counting on a tagged training corpus. (For this example we’ll use the tagged WSJ corpus.)

The $A$ matrix contains the tag transition probabilities $P ( t _ { i } | t _ { i - 1 } )$ which represent the probability of a tag occurring given the previous tag. For example, modal verbs like will are very likely to be followed by a verb in the base form, a VB, like race, so we expect this probability to be high. We compute the maximum likelihood estimate of this transition probability by counting, out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second:

$$
P ( t _ { i } | t _ { i - 1 } ) = \frac { C ( t _ { i - 1 } , t _ { i } ) } { C ( t _ { i - 1 } ) }
$$

In the WSJ corpus, for example, MD occurs 13124 times of which it is followed by VB 10471, for an MLE estimate of

$$
P ( V B | M D ) = { \frac { C ( M D , V B ) } { C ( M D ) } } = { \frac { 1 0 4 7 1 } { 1 3 1 2 4 } } = . 8 0
$$

The $B$ emission probabilities, $P ( w _ { i } | t _ { i } )$ , represent the probability, given a tag (say MD), that it will be associated with a given word (say will). The MLE of the emission probability is

$$
P ( w _ { i } | t _ { i } ) = \frac { C ( t _ { i } , w _ { i } ) } { C ( t _ { i } ) }
$$

Of the 13124 occurrences of MD in the WSJ corpus, it is associated with will 4046 times:

$$
P ( w i l l | M D ) = \frac { C ( M D , w i l l ) } { C ( M D ) } = \frac { 4 0 4 6 } { 1 3 1 2 4 } = . 3 1 
$$

We saw this kind of Bayesian modeling in Chapter 4; recall that this likelihood term is not asking “which is the most likely tag for the word will?” That would be the posterior $P ( \mathrm { M D } | \mathrm { w i l l } )$ . Instead, $P ( \mathrm { w i l l } | \mathbf { M D } )$ answers the slightly counterintuitive question “If we were going to generate a MD, how likely is it that this modal would be will?”

![## Image Analysis: 7f9826320634ad08c29df5541ae665cee913a0546fe7b47764d0ca631ee49221.jpg

**Conceptual Understanding:**
This image conceptually represents the architecture of a Hidden Markov Model (HMM), specifically illustrating its application in a part-of-speech (POS) tagging context. The main purpose is to visualize and explain the two probabilistic mechanisms that define an HMM: the state transition probabilities and the observation (or emission) likelihoods.

The circular nodes (VB₁, MD₂, NN₃) represent the 'hidden states' of the model, which correspond to part-of-speech tags like Verb, Modal Verb, and Noun. These states are 'hidden' because they are not directly observed but inferred. The solid arrows between these states, labeled 'aᵢⱼ', symbolize the 'transition probabilities' (often denoted as the 'A' matrix), indicating the likelihood of moving from one hidden state to another in a sequence.

The rectangular boxes (B₁, B₂, B₃) connected to each state represent the 'observation likelihoods' (often denoted as the 'B' matrix). These probabilities, like P("aardvark" | VB), describe the likelihood of observing a particular word given that the system is in a specific hidden state. This shows how the hidden states 'generate' the observable sequence of words. Therefore, the image clearly separates and illustrates these two probabilistic components, which are fundamental to how an HMM models sequential data for tasks like POS tagging.

**Content Interpretation:**
The image depicts the two fundamental components of a Hidden Markov Model (HMM) as applied to a tagger, likely for Part-of-Speech (POS) tagging. It shows three hidden states, labeled VB₁, MD₂, and NN₃, which represent different grammatical categories (Verb, Modal Verb, and Noun, respectively). These states form the core of the Markov chain.

The 'A' transition probabilities, represented by the labels 'aᵢⱼ' on the solid arrows connecting the circular state nodes, illustrate the likelihood of moving from one hidden state to another. For example, 'a₁₂' is the probability of transitioning from state VB₁ to state MD₂. Loops like 'a₁₁', 'a₂₂', and 'a₃₃' indicate the probability of staying in the same state.

The 'B' observation likelihoods (also known as emission probabilities) are shown in the rectangular boxes B₁, B₂, and B₃, each connected to its respective hidden state (VB₁, MD₂, NN₃) by a dashed arrow. These boxes list the probabilities of observing a specific word given that the system is in a particular hidden state. For instance, P("aardvark" | VB) is the probability of observing the word 'aardvark' when the hidden state is a Verb. The listed words ('aardvark', 'will', 'the', 'back', 'zebra') are examples of observations, and the '...' implies that a full list would include all possible words in the vocabulary. Thus, the image clearly separates and illustrates these two probabilistic mechanisms that define an HMM.

**Key Insights:**
The main takeaways from this image are the clear delineation and visual representation of the two core probabilistic components of a Hidden Markov Model (HMM) for tagging purposes:

1.  **State Transition Probabilities (A matrix):** This is represented by the interconnected circular nodes (VB₁, MD₂, NN₃) and the labels 'aᵢⱼ' on the arrows. For example, 'a₁₁' shows the probability of a state transitioning to itself, while 'a₁₂' shows the probability of transitioning from VB₁ to MD₂. This demonstrates how the sequence of hidden states (tags) is modeled.

2.  **Observation Likelihoods (B matrix):** This is represented by the rectangular boxes (B₁, B₂, B₃) linked to each state. Each entry, such as 'P("aardvark" | VB)', provides the probability of a particular observable event (word) occurring given a specific hidden state (tag). This illustrates how the hidden states generate the observable sequence of words.

These two components, the 'A' matrix of transition probabilities and the 'B' matrix of observation likelihoods, are essential for computing the overall probability of an observed sequence and for determining the most likely sequence of hidden states, which is the primary goal of an HMM tagger. The image clearly segments these two distinct sets of probabilities and their relationships within the HMM framework. The notation P("word" | Tag) is a specific piece of textual evidence that highlights how observed words are conditioned on the hidden part-of-speech tags.

**Document Context:**
This image directly supports Section 17.4.3, titled 'The components of an HMM tagger', by providing a visual illustration of an HMM's structure. The text immediately following the image, 'Figure 17.9 An illustration of the two parts of an HMM representation: the A transition probabilities used to compute the prior probability, and the B observation likelihoods that are associated with each state, one likelihood for each possible observation word,' perfectly describes the content depicted. The image visually clarifies what is meant by 'A transition probabilities' through the 'aᵢⱼ' labels on the state transitions and what 'B observation likelihoods' represent through the 'P("word" | Tag)' lists in the associated boxes. It serves as a concrete example of how these abstract probabilistic components are structured in a part-of-speech tagging context, enhancing the reader's understanding of HMM mechanics discussed in the surrounding text.

**Summary:**
The image illustrates the core components of a Hidden Markov Model (HMM) used for a tagger, specifically highlighting the interaction between hidden states (part-of-speech tags) and observable words. It is structured into three central circular nodes representing hidden states, each associated with a rectangular box detailing observation likelihoods for various words given that state. The circular nodes are interconnected by solid arrows, each labeled with a transition probability, demonstrating the likelihood of moving from one state to another. Dashed arrows connect each hidden state node to its corresponding observation likelihood box.

The central part of the diagram features three circular nodes, representing hidden states in the HMM. These are labeled 'VB₁', 'MD₂', and 'NN₃'. 'VB' likely stands for Verb, 'MD' for Modal verb, and 'NN' for Noun. The subscript numbers '1', '2', and '3' serve as identifiers for these states.

Interconnecting these states are solid arrows with labels indicating transition probabilities. These probabilities, denoted 'aᵢⱼ', represent the likelihood of transitioning from state 'i' to state 'j'. Specifically:
- From VB₁ to VB₁: a₁₁
- From VB₁ to MD₂: a₁₂
- From VB₁ to NN₃: a₁₃
- From MD₂ to VB₁: a₂₁
- From MD₂ to MD₂: a₂₂
- From MD₂ to NN₃: a₂₃
- From NN₃ to VB₁: a₃₁
- From NN₃ to MD₂: a₃₂
- From NN₃ to NN₃: a₃₃

Associated with each hidden state is a rectangular box, connected by a dashed arrow, which lists observation likelihoods. These likelihoods, denoted 'P("word" | Tag)', represent the probability of observing a particular word given the current hidden state (tag).
- **Box B₁**, connected to VB₁, contains: P("aardvark" | VB), ..., P("will" | VB), ..., P("the" | VB), ..., P("back" | VB), ..., P("zebra" | VB). The '...' indicates that other words would also have associated probabilities.
- **Box B₂**, connected to MD₂, contains: P("aardvark" | MD), ..., P("will" | MD), ..., P("the" | MD), ..., P("back" | MD), ..., P("zebra" | MD).
- **Box B₃**, connected to NN₃, contains: P("aardvark" | NN), ..., P("will" | NN), ..., P("the" | NN), ..., P("back" | NN), ..., P("zebra" | NN).

In essence, the diagram visually separates the two fundamental probability distributions of an HMM: the 'A' matrix of transition probabilities between states and the 'B' matrix of observation likelihoods (emissions) of words from each state.](images/7f9826320634ad08c29df5541ae665cee913a0546fe7b47764d0ca631ee49221.jpg)
Figure 17.9 An illustration of the two parts of an HMM representation: the $A$ transition probabilities used to compute the prior probability, and the $B$ observation likelihoods that are associated with each state, one likelihood for each possible observation word.

The A transition probabilities, and $B$ observation likelihoods of the HMM are illustrated in Fig. 17.9 for three states in an HMM part-of-speech tagger; the full tagger would have one state for each tag.

# 17.4.4 HMM tagging as decoding

For any model, such as an HMM, that contains hidden variables, the task of determining the hidden variables sequence corresponding to the sequence of observations is called decoding. More formally,

decoding

Decoding: Given as input an HMM $\lambda = ( A , B )$ and a sequence of observations $O = o _ { 1 } , o _ { 2 } , . . . , o _ { T }$ , find the most probable sequence of states $Q = q _ { 1 } q _ { 2 } q _ { 3 } \ldots q _ { T } .$ .

For part-of-speech tagging, the goal of HMM decoding is to choose the tag sequence $t _ { 1 } \ldots t _ { n }$ that is most probable given the observation sequence of $n$ words $w _ { 1 } \ldots w _ { n }$ :

$$
\hat { t } _ { 1 : n } = \mathop { \mathrm { a r g m a x } } _ { t _ { 1 } \ldots t _ { n } } P ( t _ { 1 } \ldots t _ { n } | w _ { 1 } \ldots \ldots w _ { n } )
$$

The way we’ll do this in the HMM is to use Bayes’ rule to instead compute:

$$
\hat { t } _ { 1 : n } = \operatorname * { a r g m a x } _ { t _ { 1 } \ldots t _ { n } } \frac { P ( w _ { 1 } \ldots w _ { n } | t _ { 1 } \ldots t _ { n } ) P ( t _ { 1 } \ldots t _ { n } ) } { P ( w _ { 1 } \ldots w _ { n } ) }
$$

Furthermore, we simplify Eq. 17.13 by dropping the denominator $P ( w _ { 1 } ^ { n } )$ :

$$
\hat { t } _ { 1 : n } = \operatorname * { a r g m a x } _ { t _ { 1 } . . . t _ { n } } P ( w _ { 1 } \ldots w _ { n } | t _ { 1 } \ldots t _ { n } ) P ( t _ { 1 } \ldots t _ { n } )
$$

HMM taggers make two further simplifying assumptions. The first (output independence, from Eq. 17.7) is that the probability of a word appearing depends only on its own tag and is independent of neighboring words and tags:

$$
P ( w _ { 1 } \dots w _ { n } | t _ { 1 } \dots t _ { n } ) \ \approx \ \prod _ { i = 1 } ^ { n } P ( w _ { i } | t _ { i } )
$$

The second assumption (the Markov assumption, Eq. 17.6) is that the probability of a tag is dependent only on the previous tag, rather than the entire tag sequence;

$$
P ( t _ { 1 } . . . t _ { n } ) \approx \prod _ { i = 1 } ^ { n } P ( t _ { i } | t _ { i - 1 } )
$$

Plugging the simplifying assumptions from Eq. 17.15 and Eq. 17.16 into Eq. 17.14 results in the following equation for the most probable tag sequence from a bigram tagger:

$$
\widehat { t } _ { 1 : n } = \operatorname * { a r g m a x } _ { t _ { 1 } \ldots t _ { n } } P ( t _ { 1 } \ldots t _ { n } | w _ { 1 } \ldots w _ { n } ) \approx \operatorname * { a r g m a x } _ { t _ { 1 } \ldots t _ { n } } \prod _ { i = 1 } ^ { n } \widetilde { P ( w _ { i } | t _ { i } ) } \overbrace { P ( t _ { i } | t _ { i - 1 } ) } ^ { \mathrm { e m i s s i o n } \mathrm { t r a n s i t i o n } }
$$

The two parts of Eq. 17.17 correspond neatly to the $B$ emission probability and $A$ transition probability that we just defined above!

function VITERBI(observations of len T,state-graph of len $N$ ) returns best-path, path-prob   
create a path probability matrix viterbi[N,T]   
for each state $s$ from 1 to $N$ do ; initialization step $\nu i t e r b i [ s , 1 ]  \pi _ { s } * b _ { s } ( o _ { 1 } )$ backpointer $\mathbf { \partial } \cdot [ \mathbf { s } , 1 ] \gets 0$   
for each time step $t$ from 2 to $T$ do ; recursion step for each state $s$ from 1 to $N$ do   
$\begin{array} { r l } & { ~ \nu i t e r b i [ { \mathrm { s } } , { \mathrm { t } } ] \longleftarrow \underset { s ^ { \prime } = 1 } { N } ~ \nu i t e r b i [ s ^ { \prime } , t - 1 ] * { a } _ { s ^ { \prime } , s } * b _ { s } ( o _ { t } ) } \\ & { ~ b a c k p o i n t e r [ { \mathrm { s } } , { \mathrm { t } } ] \longleftarrow \underset { s ^ { \prime } = 1 } { \operatorname* { m a x } } ~ \nu i t e r b i [ s ^ { \prime } , t - 1 ] * { a } _ { s ^ { \prime } , s } * b _ { s } ( o _ { t } ) } \\ & { b e s t p a t h p r o b  \underset { s = 1 } { N } ~ \nu i t e r b i [ s , T ] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad } \\ & { b e s t p a t h p o i n t e r  ~ \operatorname* { a r g m a x } ~ \nu i t e r b i [ s , T ] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad } \\ & { b e s t p a t h p o i n t e r  ~ \operatorname* { a r g m a x } ~ \nu i t e r b i [ s , T ] \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad } \end{array}$   
bestpath the path starting at state bestpathpointer, that follows backpointer[] to states back in time   
return bestpath, bestpathprob

Figure 17.10 Viterbi algorithm for finding the optimal sequence of tags. Given an observation sequence and an HMM $\lambda = ( A , B )$ , the algorithm returns the state path through the HMM that assigns maximum likelihood to the observation sequence.

# 17.4.5 The Viterbi Algorithm

# Viterbi algorithm

The decoding algorithm for HMMs is the Viterbi algorithm shown in Fig. 17.10. As an instance of dynamic programming, Viterbi resembles the dynamic programming minimum edit distance algorithm of Chapter 2.

The Viterbi algorithm first sets up a probability matrix or lattice, with one column for each observation $o _ { t }$ and one row for each state in the state graph. Each column thus has a cell for each state $q _ { i }$ in the single combined automaton. Figure 17.11 shows an intuition of this lattice for the sentence Janet will back the bill.

Each cell of the lattice, $\nu _ { t } ( j )$ , represents the probability that the HMM is in state $j$ after seeing the first $t$ observations and passing through the most probable state sequence $q _ { 1 } , . . . , q _ { t - 1 }$ , given the HMM $\lambda$ . The value of each cell $\nu _ { t } ( j )$ is computed by recursively taking the most probable path that could lead us to this cell. Formally, each cell expresses the probability

$$
\nu _ { t } ( j ) = \operatorname* { m a x } _ { q _ { 1 } , \dots , q _ { t - 1 } } P ( q _ { 1 } \dots q _ { t - 1 } , o _ { 1 } , o _ { 2 } \dots o _ { t } , q _ { t } = j | \lambda )
$$

We represent the most probable path by taking the maximum over all possible previous state sequences max . Like other dynamic programming algorithms, $q _ { 1 } , . . . , q _ { t - 1 }$ Viterbi fills each cell recursively. Given that we had already computed the probability of being in every state at time $t - 1$ , we compute the Viterbi probability by taking the most probable of the extensions of the paths that lead to the current cell. For a given state $q _ { j }$ at time $t$ , the value $\nu _ { t } ( j )$ is computed as

$$
\nu _ { t } ( j ) ~ = ~ \operatorname* { m a x } _ { i = 1 } ^ { N } \nu _ { t - 1 } ( i ) a _ { i j } b _ { j } ( o _ { t } )
$$

The three factors that are multiplied in Eq. 17.19 for extending the previous paths to compute the Viterbi probability at time $t$ are

![## Image Analysis: cf9b4be8067a937b194f549232e16fbd7d2401cf936d120d2f798851a99c82e8.jpg

**Conceptual Understanding:**
This image conceptually represents a Viterbi lattice, a dynamic programming algorithm used in natural language processing for Part-of-Speech (POS) tagging. It visualizes all possible sequences of POS tags for a given sentence ("Janet will back the bill") and highlights the single most probable or "correct" sequence. The main purpose is to demonstrate how the Viterbi algorithm works by showing the states (POS tags) at each time step (word) and the transitions between them, ultimately revealing the optimal path through these states.

**Content Interpretation:**
The image demonstrates the process of Part-of-Speech (POS) tagging using the Viterbi algorithm.

**Processes Shown:** It illustrates the computation of the most likely sequence of hidden states (POS tags) given a sequence of observed words. Each column represents a word in the sentence, and each row within a column represents a potential POS tag for that word.

**Concepts Shown:**
*   **Lattice Structure:** The grid-like arrangement of possible tags for each word and the connections between them form a lattice.
*   **Hidden States:** The POS tags (DT, RB, NN, JJ, VB, MD, NNP) are the hidden states, as they are not directly observed but inferred from the words.
*   **Observations:** The words ("Janet", "will", "back", "the", "bill") are the observations.
*   **Probability (implied):** The greyed-out ovals, such as "DT" for "Janet" or "MD" for "bill", signify a very low or zero probability of that word having that particular tag according to the underlying model (e.g., the B matrix mentioned in the document context). The bold, blue-outlined path represents the sequence with the highest cumulative probability.

**Relationships:** The lines (arrows) connecting tags from one word to the next represent possible transitions between POS tags. The Viterbi algorithm evaluates the probability of each transition and the probability of each word given its tag to find the most likely sequence.

**Significance of Information:**
*   The presence of multiple tag options for words like "will" (NN, VB, MD) and "back" (RB, NN, JJ, VB) highlights the lexical ambiguity inherent in language.
*   The greyed-out tags (e.g., "DT" for "Janet") demonstrate how the algorithm prunes improbable paths, effectively reducing the search space and making the computation efficient.
*   The single bolded, blue-outlined path (NNP -> MD -> VB -> DT -> NN) represents the algorithm's final output: the most probable sequence of tags for the entire sentence.

**Textual Evidence Supporting Interpretation:**
*   The words "Janet", "will", "back", "the", "bill" are explicitly laid out as sequential observations.
*   The tags "DT", "RB", "NN", "JJ", "VB", "MD", "NNP" are clearly labeled as the potential parts of speech.
*   The visual distinction between greyed-out tags and blue-outlined tags, and the bold arrows, explicitly marks the high-probability path chosen by the Viterbi algorithm and low-probability (or impossible) paths, respectively.

**Key Insights:**
**Main Takeaway 1: Ambiguity in Language and Probabilistic Tagging.**
*   The image clearly shows that a single word can have multiple possible Part-of-Speech tags (e.g., "will" can be an NN, VB, or MD, and "back" can be an RB, NN, JJ, or VB).
*   *Evidence:* The multiple blue-outlined ovals for "will" and "back" (e.g., NN, VB, MD for "will").

**Main Takeaway 2: Pruning Improbable Paths.**
*   The Viterbi algorithm effectively prunes (eliminates) tag assignments that are highly improbable or impossible for a given word.
*   *Evidence:* The greyed-out tags, such as "DT" (Determiner) for "Janet" (a proper noun), indicate that this combination has a near-zero probability and is excluded from consideration for the optimal path. Similarly, "MD" (Modal verb) is greyed out for "bill".

**Main Takeaway 3: Identifying the Optimal Sequence.**
*   The Viterbi algorithm's primary function is to identify the single most probable sequence of hidden states (tags) given the observations (words) and the underlying probabilistic model.
*   *Evidence:* The highlighted, bold path from NNP (Janet) -> MD (will) -> VB (back) -> DT (the) -> NN (bill) represents this unique optimal sequence, demonstrating the algorithm's output. This path explicitly defines the "correct tag sequence" through the "hidden states" as stated in the document context.

**Main Takeaway 4: Lattice Visualization.**
*   The lattice provides a clear visual representation of all possible states and transitions, making the dynamic programming approach more intuitive.
*   *Evidence:* The entire structure with nodes and arrows clearly displays the state space and the connections considered.

**Document Context:**
This image is critically relevant to the "Viterbi algorithm" section of the document. It serves as a visual explanation of how the Viterbi algorithm is applied in Part-of-Speech (POS) tagging. It directly illustrates the concept described in the surrounding text: "A sketch of the lattice for Janet will back the bill, showing the possible tags (q_i) for each word and highlighting the path corresponding to the correct tag sequence through the hidden states. States (parts of speech) which have a zero probability of generating a particular word according to the B matrix (such as the probability that a determiner DT will be realized as Janet) are greyed out." The image visually confirms and elaborates on these points, providing a concrete example of the algorithm in action for a specific sentence.

**Summary:**
This diagram, known as a Viterbi lattice, illustrates how a computational algorithm, specifically the Viterbi algorithm, determines the most likely grammatical role (Part-of-Speech or POS tag) for each word in a sentence. The sentence being analyzed here is "Janet will back the bill."

The diagram is organized into columns, with each column representing a word from the sentence: "Janet," "will," "back," "the," and "bill." Above each word, there is a vertical stack of ovals. Each oval contains a two-letter abbreviation, which stands for a possible Part-of-Speech tag for that word. For example:
*   **DT:** Determiner (e.g., "the," "a")
*   **RB:** Adverb (e.g., "quickly," "very")
*   **NN:** Noun, singular or mass (e.g., "table," "water")
*   **JJ:** Adjective (e.g., "happy," "big")
*   **VB:** Verb, base form (e.g., "eat," "run")
*   **MD:** Modal verb (e.g., "will," "can")
*   **NNP:** Proper noun, singular (e.g., "Janet," "London")

For each word, some of these possible tags are colored grey, while others have a blue outline. The greyed-out ovals represent tags that are highly improbable or impossible for that particular word in this context, effectively having a "zero probability" as per the document's B matrix (which encodes probabilities of a word given a tag). For instance, "Janet" is a proper noun, so tags like "DT" (Determiner) are greyed out, indicating they are not a plausible tag for "Janet."

The blue-outlined ovals represent plausible POS tags for the word. For example, for the word "will," plausible tags include "NN" (Noun), "VB" (Verb), and "MD" (Modal verb), because "will" can function in different ways in English.

Thin black lines connect the blue-outlined tags from one word to the next, representing possible transitions between parts of speech. The Viterbi algorithm evaluates all these possible paths, considering both the likelihood of a word having a certain tag and the likelihood of transitioning from one tag to another.

The most important feature of this diagram is the bold, thick black path, which connects a sequence of blue-outlined ovals. This bold path represents the *single most probable* (or "correct") sequence of Part-of-Speech tags for the entire sentence, as determined by the Viterbi algorithm. Tracing this path, we can see the algorithm's chosen tags:
*   "Janet" is tagged as **NNP** (Proper Noun).
*   "will" is tagged as **MD** (Modal verb).
*   "back" is tagged as **VB** (Verb, base form).
*   "the" is tagged as **DT** (Determiner).
*   "bill" is tagged as **NN** (Noun, singular).

Therefore, the Viterbi algorithm determines the most likely grammatical structure of "Janet will back the bill" to be "Proper Noun - Modal Verb - Verb - Determiner - Noun." This diagram vividly illustrates the process of exploring multiple possibilities and then selecting the optimal sequence based on probabilistic calculations.](images/cf9b4be8067a937b194f549232e16fbd7d2401cf936d120d2f798851a99c82e8.jpg)
Figure 17.11 A sketch of the lattice for Janet will back the bill, showing the possible tags $( q _ { i } )$ for each word and highlighting the path corresponding to the correct tag sequence through the hidden states. States (parts of speech) which have a zero probability of generating a particular word according to the $B$ matrix (such as the probability that a determiner DT will be realized as Janet) are greyed out.

$\nu _ { t - 1 } ( i )$ the previous Viterbi path probability from the previous time step $a _ { i j }$ the transition probability from previous state $q _ { i }$ to current state $q _ { j }$ $b _ { j } ( o _ { t } )$ the state observation likelihood of the observation symbol $o _ { t }$ given the current state $j$

# 17.4.6 Working through an example

Let’s tag the sentence Janet will back the bill; the goal is the correct series of tags (see also Fig. 17.11):

(17.20) Janet/NNP will/MD back/VB the/DT bill/NN   

<table><tr><td></td><td>NNP</td><td>MD</td><td>VB</td><td>JJ</td><td>NN</td><td>RB</td><td>DT</td></tr><tr><td>&lt;s&gt;</td><td>0.2767</td><td>0.0006</td><td>0.0031 0.0453 0.0449 0.0510 0.2026</td><td></td><td></td><td></td><td></td></tr><tr><td>NNP</td><td>0.3777</td><td>0.0110</td><td>0.0009</td><td></td><td>0.0084 0.0584</td><td>0.0090 0.0025</td><td></td></tr><tr><td>MD</td><td>0.0008</td><td>0.0002</td><td>0.7968</td><td>0.0005</td><td>0.0008</td><td>0.1698</td><td>0.0041</td></tr><tr><td>VB</td><td>0.0322</td><td>0.0005</td><td>0.0050</td><td>0.0837</td><td>0.0615</td><td>0.0514</td><td>0.2231</td></tr><tr><td>JJ</td><td>0.0366</td><td>0.0004</td><td>0.0001</td><td>0.0733</td><td>0.4509</td><td>0.0036</td><td>0.0036</td></tr><tr><td>NN</td><td>0.0096</td><td>0.0176</td><td>0.0014</td><td></td><td>0.0086 0.1216</td><td>0.0177</td><td>0.0068</td></tr><tr><td>RB</td><td>0.0068</td><td>0.0102</td><td></td><td></td><td></td><td>0.1011 0.1012 0.0120 0.0728 0.0479</td><td></td></tr><tr><td>DT</td><td>0.1147</td><td>0.0021</td><td></td><td></td><td></td><td>0.0002 0.2157 0.4744 0.0102 0.0017</td><td></td></tr></table>

Figure 17.12 The A transition probabilities $P ( t _ { i } | t _ { i - 1 } )$ computed from the WSJ corpus without smoothing. Rows are labeled with the conditioning event; thus $P ( V B | M D )$ is 0.7968. $< s >$ is the start token.

Let the HMM be defined by the two tables in Fig. 17.12 and Fig. 17.13. Figure 17.12 lists the $a _ { i j }$ probabilities for transitioning between the hidden states (partof-speech tags). Figure 17.13 expresses the $b _ { i } ( o _ { t } )$ probabilities, the observation likelihoods of words given tags. This table is (slightly simplified) from counts in the WSJ corpus. So the word Janet only appears as an NNP, back has 4 possible parts of speech, and the word the can appear as a determiner or as an NNP (in titles like “Somewhere Over the Rainbow” all words are tagged as NNP).

<table><tr><td></td><td>Janet</td><td>will</td><td>back</td><td>the</td><td>bill</td></tr><tr><td>NNP</td><td>0.000032 0</td><td></td><td>0</td><td>0.000048 0</td><td></td></tr><tr><td>MD</td><td>0</td><td>0.308431</td><td>0</td><td>0</td><td>0</td></tr><tr><td>VB</td><td>0</td><td></td><td>0.000028 0.000672</td><td>0</td><td>0.000028</td></tr><tr><td>JJ</td><td>0</td><td>0</td><td>0.000340</td><td>0</td><td>0</td></tr><tr><td>NN</td><td>0</td><td></td><td>0.000200 0.000223</td><td>0</td><td>0.002337</td></tr><tr><td>RB</td><td>0</td><td>0</td><td>0.010446 0</td><td></td><td>0</td></tr><tr><td>DT</td><td>0</td><td>0</td><td>0</td><td>0.506099</td><td>0</td></tr></table>

Figure 17.13 Observation likelihoods $B$ computed from the WSJ corpus without smoothing, simplified slightly.

![## Image Analysis: fbfa8841fda07840324b55856e848fe2d0a20cf70c4b373a47114e297e08bd07.jpg

**Conceptual Understanding:**
The image conceptually represents a Viterbi lattice, or trellis, which is a key component of the Viterbi algorithm. Its main purpose is to find the single most probable sequence of hidden states (in this context, Part-of-Speech tags like NNP, MD, VB, etc.) that corresponds to a given sequence of observed events (the words 'Janet will back the bill'). The lattice visually illustrates the dynamic programming approach, where the optimal path to a state at a given time step is computed based on the optimal paths to previous states, thereby efficiently solving the problem without exhaustively enumerating all possible sequences. It communicates the core idea of how probabilities are accumulated and maximized over time to identify the best hidden state sequence.

**Content Interpretation:**
The image vividly demonstrates the Viterbi algorithm's dynamic programming process for finding the most probable sequence of hidden states (Part-of-Speech tags) given an observed sequence of words. It explicitly details the forward pass where cumulative probabilities of optimal paths are computed for each state at each time step. The `v_k(n)` cells show the values of these highest probabilities, and the `max` operator within these cells (e.g., `v_2(2)= max * .308 = 2.772e-8`) underscores the selection of the best path from preceding states. The diagram also illustrates the role of initial state probabilities (`P(q_n|start)`), such as `P(NNP|start) = .28`, and state-to-state transition probabilities (`P(q_n|q_m)`), like `* P(MDINNP)`. Although not explicitly labeled, emission probabilities (likelihood of an observed word given a hidden state) are implied multipliers within the cell calculations (e.g., the `.00032` for `v_1(1)` or `.308` for `v_2(2)`). Furthermore, the `backtrace` labels and dashed arrows (e.g., from `v_2(2)` to `v_1(1)`) clearly depict the backward pass, which is crucial for reconstructing the final optimal sequence of hidden states by following the stored pointers.

**Key Insights:**
The Viterbi algorithm employs dynamic programming to efficiently determine the single most likely sequence of hidden states. This is evident from the lattice structure where each cell `v_k(n)` stores the maximum probability of a path up to that (state, time) point, building upon previously computed optimal sub-paths (e.g., `v_1(1)= .28 * .00032 = .00009` and `v_2(2)= max * .308 = 2.772e-8`). Each cell implicitly stores a 'backpointer' to the previous state that led to its maximum probability, which is used during the final 'backtrace' step to reconstruct the optimal sequence. This is explicitly shown by the dashed 'backtrace' arrows from `v_2(2)` to `v_1(1)`. The presence of numerous empty cells or cells with '0' values (e.g., `v_1(2)= .0006 x 0 = 0`) highlights that certain state-observation or state-transition combinations have zero probability, simplifying computations and reflecting model constraints, as also noted by the surrounding text about avoiding clutter for zero-value cells. The algorithm requires initial state probabilities (`P(q_n|start) = .28`), transition probabilities between states (`* P(MDINNP) = .01`), and emission probabilities (implied, e.g., `.00032` for 'Janet' as NNP).

**Document Context:**
This image directly supports Section 17.4.6, titled "Working through an example," by providing a concrete, step-by-step visual illustration of the Viterbi algorithm. It functions as a "fleshed-out version" of a previous sketch (Fig. 17.11), making the abstract concepts of the Viterbi lattice tangible. The image directly addresses the document's goal of demonstrating how to compute "the best hidden state sequence for the observation sequence Janet will back the bill," which is a core task in sequence labeling with Hidden Markov Models. By showing the calculations for the first few words, it guides the reader through the initial stages of filling the Viterbi trellis, providing a foundation for understanding the entire process and the final state sequence reconstruction (NNP MD VB DT NN) mentioned in the surrounding text.

**Summary:**
The image displays a Viterbi lattice, which is a tabular representation used by the Viterbi algorithm to find the most probable sequence of hidden states (like Parts-of-Speech) for a given sequence of observed words. On the far left, there is a column labeled `q_n` listing the possible hidden states: `q_7 DT` (Determiner), `q_6 RB` (Adverb), `q_5 NN` (Noun), `q_4 JJ` (Adjective), `q_3 VB` (Verb), `q_2 MD` (Modal), and `q_1 NNP` (Proper Noun). At the very bottom, a starting node `π` is present. Along the bottom of the diagram, the observed words of the sentence "Janet will back the bill" are shown, each preceded by a "start" label and indexed as `o_1` to `o_5`: `Janet (o_1)`, `will (o_2)`, `back (o_3)`, `the (o_4)`, `bill (o_5)`. The main part of the diagram is a grid of light blue rectangles, representing the `v_k(n)` cells, where `k` is the observation index (time step) and `n` is the hidden state. Each cell in this lattice holds the highest probability of any path leading to that specific hidden state at that particular time step, along with an implicit pointer to the previous state that yielded this maximum probability. Let's trace the calculations for the first two observation words, "Janet" and "will": For the first word, "Janet" (column `o_1`): From the `π` start node, arrows lead to various `q_n` states, indicating initial probabilities: To `q_4 JJ`: `P(JJ|start) = .045`; To `q_3 VB`: `P(VB|start) = .0031`; To `q_2 MD`: `P(MD|start) = .0006`; To `q_1 NNP`: `P(NNP|start) = .28`. The `v_1(n)` cells for "Janet" are filled by multiplying the initial probability `P(q_n|start)` by the likelihood of "Janet" being emitted by state `q_n` (this emission probability is implicit in the calculations shown). `v_1(1)` (for NNP): `(.28 * .00032) = .00009`. This implies "Janet" has a likelihood of `.00032` when in the NNP state. `v_1(2)` (for MD): `(.0006 x 0) = 0`. This indicates "Janet" has zero likelihood as MD. `v_1(3)` (for VB): `(.0031 x 0) = 0`. This indicates "Janet" has zero likelihood as VB. `v_1(4)` (for JJ): `(.045*0) = 0`. This indicates "Janet" has zero likelihood as JJ. Cells `v_1(5)`, `v_1(6)`, `v_1(7)` are empty, meaning their probabilities are also effectively 0. For the second word, "will" (column `o_2`): The `v_2(n)` cells are calculated by considering all possible previous states from column `o_1`. For each `v_2(n)` cell, the algorithm finds the maximum probability of reaching `q_n` at `o_2` from any `q_m` at `o_1`. This involves: `(probability of best path to v_1(m) * transition probability P(q_n|q_m) * emission probability of "will" from q_n)`. `v_2(2)` (for MD): It considers paths from `v_1` states. An arrow labeled `* P(MDINNP) = .00009*.01 = .9e-8` points from `v_1(1)` (NNP) to `v_2(2)` (MD). This indicates that the probability of the path through NNP at `o_1` to MD at `o_2` is `.9e-8`. Other transition probabilities, `P(MDIJJ)=0`, `P(MDIVB)=0`, `P(MDIMD)=0` also lead to this cell, but with 0 probability, so they don't contribute to the maximum. The cell `v_2(2)` itself shows `max * .308 = 2.772e-8`, where `.308` is the likelihood of "will" being emitted from MD. This `2.772e-8` is the highest probability path ending in MD at `o_2`. `v_2(3)` (for VB): Shows `max * .000028 = 2.5e-13`. (`.000028` is the likelihood of "will" as VB). `v_2(5)` (for NN): Shows `max * .0002 = .00000001`. (`.0002` is the likelihood of "will" as NN). Cells `v_2(1), v_2(4), v_2(6), v_2(7)` are empty, implying 0 probability. For the third word, "back" (column `o_3`): Similarly, `v_3(n)` cells are calculated based on `v_2` cells. Arrows like `* P(RBINN)` and `* P(NNINN)` show transition probabilities being used. `v_3(3)` (for VB): `max * .00067`. `v_3(4)` (for JJ): `max * .00034`. `v_3(5)` (for NN): `max * .000223`. `v_3(6)` (for RB): `max * .0104`. Other `v_3` cells are empty. Columns `o_4` ("the") and `o_5` ("bill") are shown with empty cells, indicating that their calculations would follow the same pattern, extending the lattice. Finally, after all cells are filled, the optimal sequence of hidden states is found through a **backtrace**. This is indicated by dashed arrows and the label "backtrace". For instance, a dashed arrow from `v_2(2)` (MD) points back to `v_1(1)` (NNP), meaning that the most probable path reaching MD at `o_2` came from NNP at `o_1`. Another backtrace arrow points from `v_2(5)` (NN) to an implied previous state in `v_1`. By following these backpointers from the last word back to the start, the algorithm reconstructs the most probable sequence of Part-of-Speech tags for the entire sentence. The document context explicitly mentions that the reconstructed sequence should be "NNP MD VB DT NN".](images/fbfa8841fda07840324b55856e848fe2d0a20cf70c4b373a47114e297e08bd07.jpg)
Figure 17.14 The first few entries in the individual state columns for the Viterbi algorithm. Each cell keeps the probability of the best path so far and a pointer to the previous cell along that path. We have only filled out columns 1 and 2; to avoid clutter most cells with value 0 are left empty. The rest is left as an exercise for the reader. After the cells are filled in, backtracing from the end state, we should be able to reconstruct the correct state sequence NNP MD VB DT NN.   
Figure 17.14 shows a fleshed-out version of the sketch we saw in Fig. 17.11, the Viterbi lattice for computing the best hidden state sequence for the observation sequence Janet will back the bill.

There are $N = 5$ state columns. We begin in column 1 (for the word Janet) by setting the Viterbi value in each cell to the product of the $\pi$ transition probability (the start probability for that state $i$ , which we get from the $< s >$ entry of Fig. 17.12), and the observation likelihood of the word Janet given the tag for that cell. Most of the cells in the column are zero since the word Janet cannot be any of those tags. The reader should find this in Fig. 17.14.

Next, each cell in the will column gets updated. For each state, we compute the value viterb $[ s , t ]$ by taking the maximum over the extensions of all the paths from the previous column that lead to the current cell according to Eq. 17.19. We have shown the values for the MD, VB, and NN cells. Each cell gets the max of the 7 values from the previous column, multiplied by the appropriate transition probability; as it happens in this case, most of them are zero from the previous column. The remaining value is multiplied by the relevant observation probability, and the (trivial) max is taken. In this case the final value, 2.772e-8, comes from the NNP state at the previous column. The reader should fill in the rest of the lattice in Fig. 17.14 and backtrace to see whether or not the Viterbi algorithm returns the gold state sequence NNP MD VB DT NN.

# 17.5 Conditional Random Fields (CRFs)

# unknown words

While the HMM is a useful and powerful model, it turns out that HMMs need a number of augmentations to achieve high accuracy. For example, in POS tagging as in other tasks, we often run into unknown words: proper names and acronyms are created very often, and even new common nouns and verbs enter the language at a surprising rate. It would be great to have ways to add arbitrary features to help with this, perhaps based on capitalization or morphology (words starting with capital letters are likely to be proper nouns, words ending with -ed tend to be past tense (VBD or VBN), etc.) Or knowing the previous or following words might be a useful feature (if the previous word is the, the current tag is unlikely to be a verb).

Although we could try to hack the HMM to find ways to incorporate some of these, in general it’s hard for generative models like HMMs to add arbitrary features directly into the model in a clean way. We’ve already seen a model for combining arbitrary features in a principled way: log-linear models like the logistic regression model of Chapter 5! But logistic regression isn’t a sequence model; it assigns a class to a single observation.

CRF

Luckily, there is a discriminative sequence model based on log-linear models: the conditional random field (CRF). We’ll describe here the linear chain CRF, the version of the CRF most commonly used for language processing, and the one whose conditioning closely matches the HMM.

Assuming we have a sequence of input words $X = x _ { 1 } . . . x _ { n }$ and want to compute a sequence of output tags $Y = y _ { 1 } . . . y _ { n }$ . In an HMM to compute the best tag sequence that maximizes $P ( \boldsymbol { Y } | \boldsymbol { X } )$ we rely on Bayes’ rule and the likelihood $P ( X | Y )$ :

$$
\begin{array} { r c l } { { } } & { { } } & { { \hat { Y } ~ = ~ \underset { Y } { \mathrm { a r g m a x } } p ( Y | X ) } } \\ { { } } & { { } } & { { ~ = ~ \underset { Y } { \mathrm { a r g m a x } } p ( X | Y ) p ( Y ) } } \\ { { } } & { { } } & { { ~ = ~ \underset { Y } { \mathrm { a r g m a x } } \displaystyle \prod _ { i } p ( x _ { i } | y _ { i } ) \prod _ { i } p ( y _ { i } | y _ { i - 1 } ) } } \end{array}
$$

In a CRF, by contrast, we compute the posterior $p ( Y | X )$ directly, training the CRF

to discriminate among the possible tag sequences:

$$
{ \hat { Y } } \ = \ \operatorname { a r g m a x } _ { Y \in { \mathcal { Y } } } P ( Y | X )
$$

However, the CRF does not compute a probability for each tag at each time step. Instead, at each time step the CRF computes log-linear functions over a set of relevant features, and these local features are aggregated and normalized to produce a global probability for the whole sequence.

Let’s introduce the CRF more formally, again using $X$ and $Y$ as the input and output sequences. A CRF is a log-linear model that assigns a probability to an entire output (tag) sequence $Y$ , out of all possible sequences $\mathcal { Y }$ , given the entire input (word) sequence $X$ . We can think of a CRF as like a giant sequential version of the multinomial logistic regression algorithm we saw for text categorization. Recall that we introduced the feature function $f$ in regular multinomial logistic regression for text categorization as a function of a tuple: the input text $x$ and a single class $y$ (page 86). In a CRF, we’re dealing with a sequence, so the function $F$ maps an entire input sequence $X$ and an entire output sequence $Y$ to a feature vector. Let’s assume we have $K$ features, with a weight $w _ { k }$ for each feature $F _ { k }$ :

$$
p ( Y | X ) \ = \ { \frac { \displaystyle \exp \left( \sum _ { k = 1 } ^ { K } w _ { k } F _ { k } ( X , Y ) \right) } { \displaystyle \sum _ { Y ^ { \prime } \in \mathbb { Y } } \exp \left( \sum _ { k = 1 } ^ { K } w _ { k } F _ { k } ( X , Y ^ { \prime } ) \right) } }
$$

It’s common to also describe the same equation by pulling out the denominator into a function $\mathbf { { Z } ( X ) }$ :

$$
\begin{array} { r } { p ( { \boldsymbol { Y } } | { \boldsymbol { X } } ) ~ = ~ \frac { 1 } { Z ( { \boldsymbol { X } } ) } \mathrm { e x p } \left( \displaystyle \sum _ { k = 1 } ^ { K } w _ { k } F _ { k } ( { \boldsymbol { X } } , { \boldsymbol { Y } } ) \right) } \\ { Z ( { \boldsymbol { X } } ) ~ = ~ \displaystyle \sum _ { Y ^ { \prime } \in \mathcal { Y } } \mathrm { e x p } \left( \displaystyle \sum _ { k = 1 } ^ { K } w _ { k } F _ { k } ( { \boldsymbol { X } } , { \boldsymbol { Y } } ^ { \prime } ) \right) } \end{array}
$$

We’ll call these $K$ functions $F _ { k } ( X , Y )$ global features, since each one is a property of the entire input sequence $X$ and output sequence $Y$ . We compute them by decomposing into a sum of local features for each position $i$ in $Y$ :

$$
F _ { k } ( X , Y ) = \sum _ { i = 1 } ^ { n } f _ { k } ( y _ { i - 1 } , y _ { i } , X , i )
$$

Each of these local features $f _ { k }$ in a linear-chain CRF is allowed to make use of the current output token $y _ { i }$ , the previous output token $y _ { i - 1 }$ , the entire input string $X$ (or any subpart of it), and the current position $i$ . This constraint to only depend on the current and previous output tokens $y _ { i }$ and $y _ { i - 1 }$ are what characterizes a linear chain CRF. As we will see, this limitation makes it possible to use versions of the efficient Viterbi and Forward-Backwards algorithms from the HMM. A general CRF, by contrast, allows a feature to make use of any output token, and are thus necessary for tasks in which the decision depend on distant output tokens, like $y _ { i - 4 }$ . General CRFs require more complex inference, and are less commonly used for language processing.

# 17.5.1 Features in a CRF POS Tagger

Let’s look at some of these features in detail, since the reason to use a discriminative sequence model is that it’s easier to incorporate a lot of features.2

Again, in a linear-chain CRF, each local feature $f _ { k }$ at position $i$ can depend on any information from: $( y _ { i - 1 } , y _ { i } , X , i )$ . So some legal features representing common situations might be the following:

$$
\begin{array} { r l } & { \mathbb { 1 } \left\{ x _ { i } = t h e , y _ { i } = \mathrm { D E T } \right\} } \\ & { \mathbb { 1 } \left\{ y _ { i } = \mathrm { P R O P N } , x _ { i + 1 } = S t r e e t , y _ { i - 1 } = \mathrm { N U M } \right\} } \\ & { \mathbb { 1 } \left\{ y _ { i } = \mathrm { V E R B } , y _ { i - 1 } = \mathrm { A U X } \right\} } \end{array}
$$

For simplicity, we’ll assume all CRF features take on the value 1 or 0. Above, we explicitly use the notation $\mathbb { 1 } \{ x \}$ to mean $^ { * * } 1$ if $x$ is true, and 0 otherwise”. From now on, we’ll leave off the $\mathbb { 1 }$ when we define features, but you can assume each feature has it there implicitly.

Although the idea of what features to use is done by the system designer by hand, the specific features are automatically populated by using feature templates as we briefly mentioned in Chapter 5. Here are some templates that only use information from $( y _ { i - 1 } , y _ { i } , X , i )$ :

$$
\langle y _ { i } , x _ { i } \rangle , \langle y _ { i } , y _ { i - 1 } \rangle , \langle y _ { i } , x _ { i - 1 } , x _ { i + 2 } \rangle
$$

These templates automatically populate the set of features from every instance in the training and test set. Thus for our example Janet/NNP will/MD back/VB the/DT bill/NN, when $x _ { i }$ is the word back, the following features would be generated and have the value 1 (we’ve assigned them arbitrary feature numbers):

$$
\begin{array} { r l } & { f _ { 3 7 4 3 } \colon \boldsymbol { y } _ { i } = \boldsymbol { \mathrm { V B ~ a n d ~ } } \boldsymbol { x } _ { i } = \boldsymbol { \mathrm { b a c k } } } \\ & { f _ { 1 5 6 } \colon \boldsymbol { y } _ { i } = \boldsymbol { \mathrm { V B ~ a n d ~ } } \boldsymbol { y } _ { i - 1 } = \boldsymbol { \mathrm { M D } } } \\ & { f _ { 9 9 7 3 2 } \colon \boldsymbol { y } _ { i } = \boldsymbol { \mathrm { V B ~ a n d ~ } } \boldsymbol { x } _ { i - 1 } = \boldsymbol { \mathrm { w i l l ~ a n d ~ } } \boldsymbol { x } _ { i + 2 } = \boldsymbol { \mathrm { b i l l } } } \end{array}
$$

It’s also important to have features that help with unknown words. One of the most important is word shape features, which represent the abstract letter pattern of the word by mapping lower-case letters to $\mathbf { \epsilon } \cdot \mathbf { \epsilon } _ { \mathbf { X } } \mathbf { \epsilon } ^ { \prime }$ , upper-case to $\mathbf { \delta } ^ { \bullet } \mathbf { X } ^ { \bullet }$ , numbers to ’d’, and retaining punctuation. Thus for example I.M.F. would map to X.X.X. and DC10-30 would map to XXdd-dd. A second class of shorter word shape features is also used. In these features consecutive character types are removed, so words in all caps map to $\mathrm { X } .$ , words with initial-caps map to Xx, DC10-30 would be mapped to Xd-d but I.M.F would still map to X.X.X. Prefix and suffix features are also useful. In summary, here are some sample feature templates that help with unknown words:

$x _ { i }$ contains a particular prefix (perhaps from all prefixes of length $\leq 2$ ) $x _ { i }$ contains a particular suffix (perhaps from all suffixes of length $\leq 2$ ) $x _ { i }$ ’s word shape   
$x _ { i }$ ’s short word shape

For example the word well-dressed might generate the following non-zero valued feature values:

prefix $( x _ { i } ) = \mathtt { w }$   
prefix $\mathbf { \eta } _ { : } ( x _ { i } ) = \mathbf { w } \mathbf { e }$   
$\mathrm { s u f f i x } ( x _ { i } ) = \mathbf { e d }$   
s $\mathrm { \mathfrak { u f f i x } } ( x _ { i } ) = \mathrm { d }$   
word-shape $( x _ { i } ) =$ xxxx-xxxxxxx   
short-word-shape $( x _ { i } ) = \mathbf { x } - \mathbf { x }$

The known-word templates are computed for every word seen in the training set; the unknown word features can also be computed for all words in training, or only on training words whose frequency is below some threshold. The result of the known-word templates and word-signature features is a very large set of features. Generally a feature cutoff is used in which features are thrown out if they have count $< 5$ in the training set.

Remember that in a CRF we don’t learn weights for each of these local features $f _ { k }$ . Instead, we first sum the values of each local feature (for example feature $f _ { 3 7 4 3 } )$ over the entire sentence, to create each global feature (for example $F _ { 3 7 4 3 } \mathrm { \dot { } }$ ). It is those global features that will then be multiplied by weight $w _ { 3 7 4 3 }$ . Thus for training and inference there is always a fixed set of $K$ features with $K$ weights, even though the length of each sentence is different.

# 17.5.2 Features for CRF Named Entity Recognizers

A CRF for NER makes use of very similar features to a POS tagger, as shown in Figure 17.15.

<table><tr><td>identity of wi, identity of neighboring words embeddings for wi, embeddings for neighboring words part of speech of wi, part of speech of neighboring words</td></tr><tr><td>presence of wi in a gazetteer</td></tr><tr><td> Wi contains a particular prefix (from all prefixes of length ≤ 4)</td></tr><tr><td>Wi contains a particular suffix (from all suffixes of length ≤ 4)</td></tr><tr><td>word shape of wi, word shape of neighboring words</td></tr><tr><td>short word shape of wi, short word shape of neighboring words gazetteer features</td></tr><tr><td></td></tr></table>

Figure 17.15 Typical features for a feature-based NER system.

One feature that is especially useful for locations is a gazetteer, a list of place names, often providing millions of entries for locations with detailed geographical and political information.3 This can be implemented as a binary feature indicating a phrase appears in the list. Other related resources like name-lists, for example from the United States Census Bureau4, can be used, as can other entity dictionaries like lists of corporations or products, although they may not be as helpful as a gazetteer (Mikheev et al., 1999).

The sample named entity token L’Occitane would generate the following nonzero valued feature values (assuming that L’Occitane is neither in the gazetteer nor the census).

prefix $( x _ { i } ) = \mathtt { L }$ $\begin{array} { l } { { \mathrm { s u f f i x } } ( x _ { i } ) = { \mathrm { t a n e } } } \\ { { \mathrm { s u f f i x } } ( x _ { i } ) = { \mathrm { a n e } } } \\ { { \mathrm { s u f f i x } } ( x _ { i } ) = { \mathrm { n e } } } \\ { { \mathrm { s u f f i x } } ( x _ { i } ) = { \mathrm { e } } } \end{array}$   
prefix $( x _ { i } ) = \mathtt { L }$ ’   
prefix $\left( x _ { i } \right) = \mathbf { L } ^ { \prime } 0$   
prefi $\mathtt { \iota } ( x _ { i } ) = \mathtt { L } ^ { \prime } 0 \mathtt { c }$   
word-shape $( x _ { i } ) = \mathbf { X } ^ { } $ Xxxxxxxx short-word-shape $( x _ { i } ) = \mathbf { X } ^ { \prime } \mathbf { X } \mathbf { x }$

Figure 17.16 illustrates the result of adding part-of-speech tags and some shape information to our earlier example.   

<table><tr><td>Words</td><td>POS</td><td> Short shape</td><td>Gazetteer</td><td>BIO Label</td></tr><tr><td> Jane</td><td>NNP</td><td>Xx</td><td>0</td><td>B-PER</td></tr><tr><td>Villanueva</td><td>NNP</td><td>Xx</td><td>1</td><td> I-PER</td></tr><tr><td>of</td><td>IN</td><td>X</td><td>0</td><td>0</td></tr><tr><td>United</td><td>NNP</td><td>Xx</td><td>0</td><td>B-ORG</td></tr><tr><td>Airlines</td><td>NNP</td><td>Xx</td><td>0</td><td>I-ORG</td></tr><tr><td>Holding</td><td>NNP</td><td>Xx</td><td>0</td><td>I-ORG</td></tr><tr><td>discussed</td><td>VBD</td><td>X</td><td>0</td><td>0</td></tr><tr><td>the</td><td>DT</td><td>X</td><td>0</td><td>0</td></tr><tr><td>Chicago</td><td>NNP</td><td>Xx</td><td>1</td><td>B-LOC</td></tr><tr><td> route</td><td>NN</td><td>X</td><td>0</td><td>0</td></tr><tr><td></td><td></td><td>·</td><td>0</td><td>0</td></tr></table>

Figure 17.16 Some NER features for a sample sentence, assuming that Chicago and Villanueva are listed as locations in a gazetteer. We assume features only take on the values 0 or 1, so the first POS feature, for example, would be represented as $\mathbb { 1 } \left\{ \mathbf { P O S } = \mathbf { N N P } \right\}$ .

# 17.5.3 Inference and Training for CRFs

How do we find the best tag sequence $\hat { Y }$ for a given input $X ?$ We start with Eq. 17.22:

$$
{ \begin{array} { r l } { { \bar { Y } } } & { = { \underset { \bar { Y } \in \bar { \Psi } } { \operatorname { a r g m a x } } } P ( Y \vert X ) } \\ & { = { \underset { \bar { Y } \in \bar { \Psi } } { \operatorname { a r g m a x } } } { \frac { 1 } { Z ( X ) } } \exp \left( { \underset { k = 1 } { \overset { K } { \sum } } } w _ { k } F _ { k } ( X , Y ) \right) } \\ & { = { \underset { \bar { Y } \in \bar { \Psi } } { \operatorname { a r g m a x } } } \exp \left( { \underset { k = 1 } { \overset { K } { \sum } } } w _ { k } ^ { 1 } { \underset { k = 1 } { \overset { n } { \sum } } } f _ { k } ( y _ { i - 1 , \Psi _ { i } , X _ { i } } ) \right) } \\ & { = { \underset { \bar { Y } \in \bar { \Psi } } { \operatorname { a r g m a x } } } { \underset { k = 1 } { \overset { K } { \sum } } } w _ { k } { \underset { i = 1 } { \overset { n } { \sum } } } f _ { k } ( y _ { i - 1 , \Psi _ { i } , X _ { i } } , i ) } \\ & { = { \underset { \bar { Y } \in \bar { \Psi } } { \operatorname { a r g m a x } } } { \underset { i = 1 } { \overset { N } { \sum } } } { \underset { k = 1 } { \overset { K } { \sum } } } w _ { k } f _ { k } ( y _ { i - 1 , \Psi _ { i } , X _ { i } , i } ) } \end{array} }
$$

We can ignore the exp function and the denominator $Z ( X )$ , as we do above, because exp doesn’t change the argmax, and the denominator $Z ( X )$ is constant for a given observation sequence $X$ .

How should we decode to find this optimal tag sequence $\hat { y } ?$ Just as with HMMs, we’ll turn to the Viterbi algorithm, which works because, like the HMM, the linearchain CRF depends at each timestep on only one previous output token $y _ { i - 1 }$ .

Concretely, this involves filling an $N \times T$ array with the appropriate values, maintaining backpointers as we proceed. As with HMM Viterbi, when the table is filled, we simply follow pointers back from the maximum value in the final column to retrieve the desired set of labels.

The requisite changes from HMM Viterbi have to do only with how we fill each cell. Recall from Eq. 17.19 that the recursive step of the Viterbi equation computes the Viterbi value of time $t$ for state $j$ as

$$
\nu _ { t } ( j ) ~ = ~ \operatorname* { m a x } _ { i = 1 } ^ { N } ~ \nu _ { t - 1 } ( i ) a _ { i j } b _ { j } ( o _ { t } ) ; ~ 1 \leq j \leq N , 1 < t \leq T
$$

which is the HMM implementation of

$$
\nu _ { t } ( j ) ~ = ~ \operatorname* { m a x } _ { i = 1 } ^ { N } ~ \nu _ { t - 1 } ( i ) ~ P ( s _ { j } | s _ { i } ) ~ P ( o _ { t } | s _ { j } ) ~ 1 \leq j \leq N , 1 < t \leq T
$$

The CRF requires only a slight change to this latter formula, replacing the $a$ and $^ b$ prior and likelihood probabilities with the CRF features:

$$
\nu _ { t } ( j ) ~ = ~ \operatorname* { m a x } _ { i = 1 } ^ { N } \left[ ~ \nu _ { t - 1 } ( i ) + \sum _ { k = 1 } ^ { K } w _ { k } f _ { k } ( y _ { t - 1 } , y _ { t } , X , t ) ~ 1 \leq j \leq N , 1 < t \leq T \right] !
$$

Learning in CRFs relies on the same supervised learning algorithms we presented for logistic regression. Given a sequence of observations, feature functions, and corresponding outputs, we use stochastic gradient descent to train the weights to maximize the log-likelihood of the training corpus. The local nature of linear-chain CRFs means that the forward-backward algorithm introduced for HMMs in Appendix A can be extended to a CRF version that will efficiently compute the necessary derivatives. As with logistic regression, L1 or L2 regularization is important.

# 17.6 Evaluation of Named Entity Recognition

Part-of-speech taggers are evaluated by the standard metric of accuracy. Named entity recognizers are evaluated by recall, precision, and $\mathbf { F } _ { 1 }$ measure. Recall that recall is the ratio of the number of correctly labeled responses to the total that should have been labeled; precision is the ratio of the number of correctly labeled responses to the total labeled; and $F .$ -measure is the harmonic mean of the two.

To know if the difference between the $\mathrm { F } _ { 1 }$ scores of two NER systems is a significant difference, we use the paired bootstrap test, or the similar randomization test (Section 4.9).

For named entity tagging, the entity rather than the word is the unit of response. Thus in the example in Fig. 17.16, the two entities Jane Villanueva and United Airlines Holding and the non-entity discussed would each count as a single response.

The fact that named entity tagging has a segmentation component which is not present in tasks like text categorization or part-of-speech tagging causes some problems with evaluation. For example, a system that labeled Jane but not Jane Villanueva as a person would cause two errors, a false positive for O and a false negative for I-PER. In addition, using entities as the unit of response but words as the unit of training means that there is a mismatch between the training and test conditions.

# 17.7 Further Details

In this section we summarize a few remaining details of the data and models for part-of-speech tagging and NER, beginning with data. Since the algorithms we have presented are supervised, having labeled data is essential for training and testing. A wide variety of datasets exist for part-of-speech tagging and/or NER. The Universal Dependencies (UD) dataset (de Marneffe et al., 2021) has POS tagged corpora in over a hundred languages, as do the Penn Treebanks in English, Chinese, and Arabic. OntoNotes has corpora labeled for named entities in English, Chinese, and Arabic (Hovy et al., 2006). Named entity tagged corpora are also available in particular domains, such as for biomedical (Bada et al., 2012) and literary text (Bamman et al., 2019).

# 17.7.1 Rule-based Methods

While machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013). For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018).

One common approach is to make repeated rule-based passes over a text, starting with rules with very high precision but low recall, and, in subsequent stages, using machine learning methods that take the output of the first pass into account (an approach first worked out for coreference (Lee et al., 2017a)):

1. First, use high-precision rules to tag unambiguous entity mentions.   
2. Then, search for substring matches of the previously detected names.   
3. Use application-specific name lists to find likely domain-specific mentions.   
4. Finally, apply supervised sequence labeling techniques that use tags from pre  
vious stages as additional features.

Rule-based methods were also the earliest methods for part-of-speech tagging. Rule-based taggers like the English Constraint Grammar system (Karlsson et al. 1995, Voutilainen 1999) use a two-stage formalism invented in the 1950s and 1960s: (1) a morphological analyzer with tens of thousands of word stem entries returns all parts of speech for a word, then (2) a large set of thousands of constraints are applied to the input sentence to rule out parts of speech inconsistent with the context.

# 17.7.2 POS Tagging for Morphologically Rich Languages

Augmentations to tagging algorithms become necessary when dealing with languages with rich morphology like Czech, Hungarian and Turkish.

These productive word-formation processes result in a large vocabulary for these languages: a 250,000 word token corpus of Hungarian has more than twice as many word types as a similarly sized corpus of English (Oravecz and Dienes, 2002), while a 10 million word token corpus of Turkish contains four times as many word types as a similarly sized English corpus (Hakkani-Tur et al.¨ , 2002). Large vocabularies mean many unknown words, and these unknown words cause significant performance degradations in a wide variety of languages (including Czech, Slovene, Estonian, and Romanian) (Hajicˇ, 2000).

Highly inflectional languages also have much more information than English coded in word morphology, like case (nominative, accusative, genitive) or gender (masculine, feminine). Because this information is important for tasks like parsing and coreference resolution, part-of-speech taggers for morphologically rich languages need to label words with case and gender information. Tagsets for morphologically rich languages are therefore sequences of morphological tags rather than a single primitive tag. Here’s a Turkish example, in which the word izin has three possible morphological/part-of-speech tags and meanings (Hakkani-Tur et al. ¨ , 2002):

1. Yerdeki izin temizlenmesi gerek. $\mathrm { i z } + \mathrm { N o u n } { + } \mathsf { A } 3 s \mathsf { g } { + } \mathsf { P n o n } { + } \mathsf { G e n }$ The trace on the floor should be cleaned.   
2. Uzerinde parmak ¨ izin kalmis¸. iz + Noun+A3sg+P2sg+Nom Your finger print is left on (it).   
3. Ic¸eri girmek ic¸in izin alman gerekiyor. izin $^ +$ Noun+A3sg+Pnon+Nom You need permission to enter.

Using a morphological parse sequence like $\mathtt { N o u n } { + } \mathtt { A } 3 \mathtt { s g } { + } \mathtt { P n o n } { + } \mathtt { G e n }$ as the partof-speech tag greatly increases the number of parts of speech, and so tagsets can be 4 to 10 times larger than the 50–100 tags we have seen for English. With such large tagsets, each word needs to be morphologically analyzed to generate the list of possible morphological tag sequences (part-of-speech tags) for the word. The role of the tagger is then to disambiguate among these tags. This method also helps with unknown words since morphological parsers can accept unknown stems and still segment the affixes properly.

# 17.8 Summary

This chapter introduced parts of speech and named entities, and the tasks of partof-speech tagging and named entity recognition:

• Languages generally have a small set of closed class words that are highly frequent, ambiguous, and act as function words, and open-class words like nouns, verbs, adjectives. Various part-of-speech tagsets exist, of between 40 and 200 tags.   
• Part-of-speech tagging is the process of assigning a part-of-speech label to each of a sequence of words.   
• Named entities are words for proper nouns referring mainly to people, places, and organizations, but extended to many other types that aren’t strictly entities or even proper nouns.   
• Two common approaches to sequence modeling are a generative approach, HMM tagging, and a discriminative approach, CRF tagging. We will see a neural approach in following chapters.   
• The probabilities in HMM taggers are estimated by maximum likelihood estimation on tag-labeled training corpora. The Viterbi algorithm is used for decoding, finding the most likely tag sequence   
• Conditional Random Fields or CRF taggers train a log-linear model that can choose the best tag sequence given an observation sequence, based on features that condition on the output tag, the prior output tag, the entire input sequence, and the current timestep. They use the Viterbi algorithm for inference, to choose the best sequence of tags, and a version of the Forward-Backward algorithm (see Appendix A) for training,

# Bibliographical and Historical Notes

What is probably the earliest part-of-speech tagger was part of the parser in Zellig Harris’s Transformations and Discourse Analysis Project (TDAP), implemented between June 1958 and July 1959 at the University of Pennsylvania (Harris, 1962), although earlier systems had used part-of-speech dictionaries. TDAP used 14 handwritten rules for part-of-speech disambiguation; the use of part-of-speech tag sequences and the relative frequency of tags for a word prefigures modern algorithms. The parser was implemented essentially as a cascade of finite-state transducers; see Joshi and Hopely (1999) and Karttunen (1999) for a reimplementation.

The Computational Grammar Coder (CGC) of Klein and Simmons (1963) had three components: a lexicon, a morphological analyzer, and a context disambiguator. The small 1500-word lexicon listed only function words and other irregular words. The morphological analyzer used inflectional and derivational suffixes to assign part-of-speech classes. These were run over words to produce candidate parts of speech which were then disambiguated by a set of 500 context rules by relying on surrounding islands of unambiguous words. For example, one rule said that between an ARTICLE and a VERB, the only allowable sequences were ADJ-NOUN, NOUNADVERB, or NOUN-NOUN. The TAGGIT tagger (Greene and Rubin, 1971) used the same architecture as Klein and Simmons (1963), with a bigger dictionary and more tags (87). TAGGIT was applied to the Brown corpus and, according to Francis and Kucera ˇ (1982, p. 9), accurately tagged $7 7 \%$ of the corpus; the remainder of the Brown corpus was then tagged by hand. All these early algorithms were based on a two-stage architecture in which a dictionary was first used to assign each word a set of potential parts of speech, and then lists of handwritten disambiguation rules winnowed the set down to a single part of speech per word.

Probabilities were used in tagging by Stolz et al. (1965) and a complete probabilistic tagger with Viterbi decoding was sketched by Bahl and Mercer (1976). The Lancaster-Oslo/Bergen (LOB) corpus, a British English equivalent of the Brown corpus, was tagged in the early 1980’s with the CLAWS tagger (Marshall 1983; Marshall 1987; Garside 1987), a probabilistic algorithm that approximated a simplified HMM tagger. The algorithm used tag bigram probabilities, but instead of storing the word likelihood of each tag, the algorithm marked tags either as rare $( P ( \log | \mathrm { w o r d } ) <$ .01) infrequent $( P ( \mathrm { t a g } | \mathrm { w o r d } ) < . 1 0 )$ or normally frequent $( P ( \mathrm { t a g } | \mathrm { w o r d } ) > . 1 0 )$ .

DeRose (1988) developed a quasi-HMM algorithm, including the use of dynamic programming, although computing $P ( t | w ) P ( w )$ instead of $P ( w | t ) P ( w )$ . The same year, the probabilistic PARTS tagger of Church 1988, 1989 was probably the first implemented HMM tagger, described correctly in Church (1989), although Church (1988) also described the computation incorrectly as $P ( t | w ) P ( w )$ instead of $P ( w | t ) P ( w )$ . Church (p.c.) explained that he had simplified for pedagogical purposes because using the probability $P ( t | w )$ made the idea seem more understandable as “storing a lexicon in an almost standard form”.

Later taggers explicitly introduced the use of the hidden Markov model (Kupiec 1992; Weischedel et al. 1993; Schutze and Singer ¨ 1994). Merialdo (1994) showed that fully unsupervised EM didn’t work well for the tagging task and that reliance on hand-labeled data was important. Charniak et al. (1993) showed the importance of the most frequent tag baseline; the $9 2 . 3 \%$ number we give above was from Abney et al. (1999). See Brants (2000) for HMM tagger implementation details, including the extension to trigram contexts, and the use of sophisticated unknown word features; its performance is still close to state of the art taggers.

Log-linear models for POS tagging were introduced by Ratnaparkhi (1996), who introduced a system called MXPOST which implemented a maximum entropy Markov model (MEMM), a slightly simpler version of a CRF. Around the same time, sequence labelers were applied to the task of named entity tagging, first with HMMs (Bikel et al., 1997) and MEMMs (McCallum et al., 2000), and then once CRFs were developed (Lafferty et al. 2001), they were also applied to NER (McCallum and Li, 2003). A wide exploration of features followed (Zhou et al., 2005). Neural approaches to NER mainly follow from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net. BiLSTMs with word and character-based embeddings as input followed shortly and became a standard neural algorithm for NER (Huang et al. 2015, Ma and Hovy 2016, Lample et al. 2016) followed by the more recent use of Transformers and BERT.

The idea of using letter suffixes for unknown words is quite old; the early Klein and Simmons (1963) system checked all final letter suffixes of lengths 1-5. The unknown word features described on page 378 come mainly from Ratnaparkhi (1996), with augmentations from Toutanova et al. (2003) and Manning (2011).

State of the art POS taggers use neural algorithms, either bidirectional RNNs or Transformers like BERT; see Chapter 8 to Chapter 11. HMM (Brants 2000; Thede and Harper 1999) and CRF tagger accuracies are likely just a tad lower.

Manning (2011) investigates the remaining $2 . 7 \%$ of errors in a high-performing tagger (Toutanova et al., 2003). He suggests that a third or half of these remaining errors are due to errors or inconsistencies in the training data, a third might be solvable with richer linguistic models, and for the remainder the task is underspecified or unclear.

Supervised tagging relies heavily on in-domain training data hand-labeled by experts. Ways to relax this assumption include unsupervised algorithms for clustering words into part-of-speech-like classes, summarized in Christodoulopoulos et al. (2010), and ways to combine labeled and unlabeled data, for example by co-training (Clark et al. 2003; Søgaard 2010).

See Householder (1995) for historical notes on parts of speech, and Sampson (1987) and Garside et al. (1997) on the provenance of the Brown and other tagsets.

# Exercises

17.1 Find one tagging error in each of the following sentences that are tagged with the Penn Treebank tagset:

1. I/PRP need/VBP a/DT flight/NN from/IN Atlanta/NN   
2. Does/VBZ this/DT flight/NN serve/VB dinner/NNS   
3. I/PRP have/VB a/DT friend/NN living/VBG in/IN Denver/NNP   
4. Can/VBP you/PRP list/VB the/DT nonstop/JJ afternoon/NN flights/NNS

17.2 Use the Penn Treebank tagset to tag each word in the following sentences from Damon Runyon’s short stories. You may ignore punctuation. Some of these are quite difficult; do your best.

1. It is a nice night.   
2. This crap game is over a garage in Fifty-second Street. . .   
3. . . . Nobody ever takes the newspapers she sells . . .   
4. He is a tall, skinny guy with a long, sad, mean-looking kisser, and a   
mournful voice.

5. . . . I am sitting in Mindy’s restaurant putting on the gefillte fish, which is a dish I am very fond of, . . .   
6. When a guy and a doll get to taking peeks back and forth at each other, why there you are indeed.

17.3 Now compare your tags from the previous exercise with one or two friend’s answers. On which words did you disagree the most? Why?

17.4 Implement the “most likely tag” baseline. Find a POS-tagged training set, and use it to compute for each word the tag that maximizes $p ( t | w )$ . You will need to implement a simple tokenizer to deal with sentence boundaries. Start by assuming that all unknown words are NN and compute your error rate on known and unknown words. Now write at least five rules to do a better job of tagging unknown words, and show the difference in error rates.

17.5 Build a bigram HMM tagger. You will need a part-of-speech-tagged corpus. First split the corpus into a training set and test set. From the labeled training set, train the transition and observation probabilities of the HMM tagger directly on the hand-tagged data. Then implement the Viterbi algorithm so you can decode a test sentence. Now run your algorithm on the test set. Report its error rate and compare its performance to the most frequent tag baseline.

17.6 Do an error analysis of your tagger. Build a confusion matrix and investigate the most frequent errors. Propose some features for improving the performance of your tagger on these errors.

17.7 Develop a set of regular expressions to recognize the character shape features described on page 378.

17.8 The BIO and other labeling schemes given in this chapter aren’t the only possible one. For example, the B tag can be reserved only for those situations where an ambiguity exists between adjacent entities. Propose a new set of BIO tags for use with your NER system. Experiment with it and compare its performance with the schemes presented in this chapter.

17.9 Names of works of art (books, movies, video games, etc.) are quite different from the kinds of named entities we’ve discussed in this chapter. Collect a list of names of works of art from a particular category from a Web-based source (e.g., gutenberg.org, amazon.com, imdb.com, etc.). Analyze your list and give examples of ways that the names in it are likely to be problematic for the techniques described in this chapter.

17.10 Develop an NER system specific to the category of names that you collected in the last exercise. Evaluate your system on a collection of text likely to contain instances of these named entities.

# 18 Context-Free Grammars andConstituency Parsing

Because the Night by Bruce Springsteen and Patti Smith   
The Fire Next Time by James Baldwin   
If on a winter’s night a traveler by Italo Calvino   
Love Actually by Richard Curtis   
Suddenly Last Summer by Tennessee Williams   
A Scanner Darkly by Philip K. Dick Six titles that are not constituents, from Geoffrey K. Pullum on Language Log (who was pointing out their incredible rarity). One morning I shot an elephant in my pajamas.   
How he got into my pajamas I don’t know.

Groucho Marx, Animal Crackers, 1930

# syntax

The study of grammar has an ancient pedigree. The grammar of Sanskrit was described by the Indian grammarian Pan¯ . ini sometime between the 7th and 4th centuries BCE, in his famous treatise the As.t.adhy ¯ ay¯ ¯ı (‘8 books’). And our word syntax comes from the Greek syntaxis ´ , meaning “setting out together or arrangement”, and refers to the way words are arranged together. We have seen syntactic notions in previous chapters like the use of part-of-speech categories (Chapter 17). In this chapter and the next one we introduce formal models for capturing more sophisticated notions of grammatical structure and algorithms for parsing these structures.

Our focus in this chapter is context-free grammars and the CKY algorithm for parsing them. Context-free grammars are the backbone of many formal models of the syntax of natural language (and, for that matter, of computer languages). Syntactic parsing is the task of assigning a syntactic structure to a sentence. Parse trees (whether for context-free grammars or for the dependency or CCG formalisms we introduce in following chapters) can be used in applications such as grammar checking: sentence that cannot be parsed may have grammatical errors (or at least be hard to read). Parse trees can be an intermediate stage of representation for formal semantic analysis. And parsers and the grammatical structure they assign a sentence are a useful text analysis tool for text data science applications that require modeling the relationship of elements in sentences.

In this chapter we introduce context-free grammars, give a small sample grammar of English, introduce more formal definitions of context-free grammars and grammar normal form, and talk about treebanks: corpora that have been annotated with syntactic structure. We then discuss parse ambiguity and the problems it presents, and turn to parsing itself, giving the famous Cocke-Kasami-Younger (CKY) algorithm (Kasami 1965, Younger 1967), the standard dynamic programming approach to syntactic parsing. The CKY algorithm returns an efficient representation of the set of parse trees for a sentence, but doesn’t tell us which parse tree is the right one. For that, we need to augment CKY with scores for each possible constituent. We’ll see how to do this with neural span-based parsers. Finally, we’ll introduce the standard set of metrics for evaluating parser accuracy.

# 18.1 Constituency

# noun phrase

Syntactic constituency is the idea that groups of words can behave as single units, or constituents. Part of developing a grammar involves building an inventory of the constituents in the language. How do words group together in English? Consider the noun phrase, a sequence of words surrounding at least one noun. Here are some examples of noun phrases (thanks to Damon Runyon):

<table><tr><td>Harry the Horse</td><td> a high-class spot such as Mindy&#x27;s</td></tr><tr><td>the Broadway coppers</td><td> the reason he comes into the Hot Box</td></tr><tr><td>they</td><td>three parties from Brooklyn</td></tr><tr><td></td><td></td></tr></table>

What evidence do we have that these words group together (or “form constituents”)? One piece of evidence is that they can all appear in similar syntactic environments, for example, before a verb.

three parties from Brooklyn arrive. . . a high-class spot such as Mindy’s attracts. . . the Broadway coppers love. . . they sit

But while the whole noun phrase can occur before a verb, this is not true of each of the individual words that make up a noun phrase. The following are not grammatical sentences of English (recall that we use an asterisk $( ^ { \ast } )$ to mark fragments that are not grammatical English sentences):

Thus, to correctly describe facts about the ordering of these words in English, we must be able to say things like “Noun Phrases can occur before verbs”. Let’s now see how to do this in a more formal way!

# 18.2 Context-Free Grammars

A widely used formal system for modeling constituent structure in natural lanCFG guage is the context-free grammar, or CFG. Context-free grammars are also called phrase-structure grammars, and the formalism is equivalent to Backus-Naur form, or BNF. The idea of basing a grammar on constituent structure dates back to the psychologist Wilhelm Wundt (1900) but was not formalized until Chomsky (1956) and, independently, Backus (1959).

rules

# lexicon NP

A context-free grammar consists of a set of rules or productions, each of which expresses the ways that symbols of the language can be grouped and ordered together, and a lexicon of words and symbols. For example, the following productions express that an NP (or noun phrase) can be composed of either a ProperNoun or a determiner (Det) followed by a Nominal; a Nominal in turn can consist of one or

more Nouns.1

$$
\begin{array} { c } { { N P \mathrm { ~  ~ } D e t N o m i n a l } } \\ { { N P \mathrm { ~  ~ } P r o p e r N o u n } } \\ { { N o m i n a l \mathrm { ~  ~ } N o u n \mathrm { ~ \mid ~ } N o m i n a l N o u n } } \end{array}
$$

Context-free rules can be hierarchically embedded, so we can combine the previous rules with others, like the following, that express facts about the lexicon:

$$
\begin{array} { c } { { D e t \mathrm { ~ \tiny ~ \to ~ } a } } \\ { { D e t \mathrm { ~ \tiny ~ \to ~ } t h e } } \\ { { N o u n \mathrm { ~ \tiny ~ \to ~ } f i g h t } } \end{array}
$$

The symbols that are used in a CFG are divided into two classes. The symbols that correspond to words in the language (“the”, “nightclub”) are called terminal symbols; the lexicon is the set of rules that introduce these terminal symbols. The symbols that express abstractions over these terminals are called non-terminals. In each context-free rule, the item to the right of the arrow $(  )$ is an ordered list of one or more terminals and non-terminals; to the left of the arrow is a single non-terminal symbol expressing some cluster or generalization. The non-terminal associated with each word in the lexicon is its lexical category, or part of speech.

A CFG can be thought of in two ways: as a device for generating sentences and as a device for assigning a structure to a given sentence. Viewing a CFG as a generator, we can read the arrow as “rewrite the symbol on the left with the string of symbols on the right”.

So starting from the symbol: NP we can use our first rule to rewrite NP as: Det Nominal and then rewrite Nominal as: Noun and finally rewrite these parts-of-speech as: a flight

derivation parse tree

We say the string a flight can be derived from the non-terminal NP. Thus, a CFG can be used to generate a set of strings. This sequence of rule expansions is called a derivation of the string of words. It is common to represent a derivation by a parse tree (commonly shown inverted with the root at the top). Figure 18.1 shows the tree representation of this derivation.

![## Image Analysis: e78f4a95714bb521a225fbe7406bb398a7413b5b8f76db3eccfadcd49165ec85.jpg

**Conceptual Understanding:**
This image represents a syntactic parse tree. Conceptually, it illustrates the grammatical structure of the English noun phrase "a flight". The main purpose is to visually demonstrate how this specific phrase is composed of smaller grammatical units, showing the hierarchical relationships between a Noun Phrase (NP), its constituent Determiner (Det) and Nominal (Nom), and further breaking down the Nominal into a Noun, ultimately leading to the words 'a' and 'flight'. It serves as an example of linguistic analysis, specifically syntactic parsing, to show the internal structure of a phrase.

**Content Interpretation:**
The image shows a syntactic parse tree for the noun phrase "a flight". It depicts the hierarchical structure of this phrase according to linguistic principles. The processes shown are: first, the identification of the entire phrase as a Noun Phrase (NP). Second, the breakdown of the NP into its immediate constituents: a Determiner (Det) and a Nominal (Nom). Third, the realization of the Determiner (Det) as the word 'a'. Fourth, the breakdown of the Nominal (Nom) into a Noun. Fifth, the realization of the Noun as the word 'flight'. The relationship is one of constituency, where larger phrases are composed of smaller constituent parts. For instance, 'a' and 'Nom' are constituents of 'NP', and 'Noun' is a constituent of 'Nom'. The significance of this representation is to visually demonstrate the grammatical roles of 'a' as a determiner and 'flight' as a noun, and how they combine to form a noun phrase. All extracted text elements (NP, Det, Nom, a, Noun, flight) directly support this interpretation by labeling each node in the syntactic hierarchy.

**Key Insights:**
The main takeaway from this image is the clear illustration of the hierarchical syntactic structure of a simple noun phrase. It teaches that a Noun Phrase (NP) can be broken down into a Determiner (Det) and a Nominal (Nom). Furthermore, a Nominal (Nom) can consist of a Noun. The specific text 'a' provides evidence for a determiner, and 'flight' provides evidence for a noun. The overall structure demonstrates how individual words combine into meaningful grammatical units (like 'Noun' and 'Det') which then form larger phrases (like 'Nom' and 'NP'). This image highlights the constituency relationships in linguistic parsing, showing that 'a flight' is a Noun Phrase comprised of 'a' as a Determiner and 'flight' as a Noun within a Nominal structure. The labels 'NP', 'Det', 'Nom', 'Noun' are the key elements defining the grammatical categories and their relationships.

**Document Context:**
This image is highly relevant to a document section titled "lexicon NP", as indicated by the document context and the provided filename (e78f4a95714bb521a225fbe7406bb398a7413b5b8f76db3eccfadcd49165ec85.jpg) which is further clarified by the accompanying text "Figure 18.1 A parse tree for 'a flight'". It serves as a visual example of how a noun phrase (NP) is analyzed syntactically. The parse tree visually demonstrates the theoretical concepts discussed in the surrounding text, likely related to lexical categories and phrase structure rules for noun phrases. It provides a concrete illustration of the abstract grammatical rules for forming NPs in English, specifically using the example "a flight".

**Summary:**
This image displays a parse tree, a common linguistic tool used to represent the syntactic structure of a sentence or phrase. It visually breaks down the noun phrase "a flight" into its constituent grammatical parts. The tree starts with the overarching category, NP (Noun Phrase), at the top. This NP branches down into two main components: Det (Determiner) and Nom (Nominal). The Determiner 'Det' directly leads to the word 'a', indicating that 'a' functions as the determiner in this phrase. On the other side, the Nominal 'Nom' branches down further to 'Noun', and this 'Noun' finally leads to the word 'flight'. This structure clearly illustrates that 'flight' is the head noun modified by the determiner 'a', forming the complete noun phrase 'a flight'. The vertical lines indicate direct constituent relationships, showing how smaller grammatical units combine to form larger ones. The explanation is organized from the main concept (NP) down to the individual words, providing a full understanding of the phrase's grammatical breakdown.](images/e78f4a95714bb521a225fbe7406bb398a7413b5b8f76db3eccfadcd49165ec85.jpg)
Figure 18.1 A parse tree for “a flight”.

# dominates

In the parse tree shown in Fig. 18.1, we can say that the node NP dominates all the nodes in the tree (Det, Nom, Noun, a, flight). We can say further that it immediately dominates the nodes Det and Nom.

The formal language defined by a CFG is the set of strings that are derivable from the designated start symbol. Each grammar must have one designated start

verb phrase

symbol, which is often called S. Since context-free grammars are often used to define sentences, $s$ is usually interpreted as the “sentence” node, and the set of strings that are derivable from $s$ is the set of sentences in some simplified version of English.

Let’s add a few additional rules to our inventory. The following rule expresses the fact that a sentence can consist of a noun phrase followed by a verb phrase:

$$
S ~  ~ N P ~ V P \quad \mathrm { I p r e f e r ~ a ~ m o r n i n g ~ f l i g h t }
$$

A verb phrase in English consists of a verb followed by assorted other things; for example, one kind of verb phrase consists of a verb followed by a noun phrase:

$$
V P ~  ~ V e r b ~ N P { \mathrm { p r e f e r ~ a ~ m o r n i n g ~ f i g h t } }
$$

Or the verb may be followed by a noun phrase and a prepositional phrase:

$$
V P ~  ~ V e r b ~ N P ~ P P \mathrm { l e a v e ~ B o s t o n ~ i n ~ t h e ~ m o r n i n g }
$$

Or the verb phrase may have a verb followed by a prepositional phrase alone:

$$
V P ~ \to ~ V e r b ~ P P \quad \mathrm { l e a v i n g ~ o n ~ T h u r s d a y }
$$

A prepositional phrase generally has a preposition followed by a noun phrase. For example, a common type of prepositional phrase in the ATIS corpus is used to indicate location or direction:

$$
P P ~  ~ P r e p o s i t i o n ~ N P \mathrm { f r o m } \mathrm { L o s } \mathrm { A n g e l e s }
$$

The $N P$ inside a $P P$ need not be a location; $P P s$ are often used with times and dates, and with other nouns as well; they can be arbitrarily complex. Here are ten examples from the ATIS corpus:

<table><tr><td> to Seattle</td><td> on these flights</td></tr><tr><td> in Minneapolis</td><td> about the ground transportation in Chicago</td></tr><tr><td>on Wednesday</td><td> of the round trip flight on United Airlines</td></tr><tr><td> in the evening</td><td>of the AP fifty seven flight</td></tr><tr><td> on the ninth of July</td><td>with a stopover in Nashville</td></tr></table>

Figure 18.2 gives a sample lexicon, and Fig. 18.3 summarizes the grammar rules we’ve seen so far, which we’ll call $\mathcal { L } _ { 0 }$ . Note that we can use the or-symbol | to indicate that a non-terminal has alternate possible expansions.

Figure 18.2 The lexicon for $\mathcal { L } _ { 0 }$ .   

<table><tr><td rowspan="2">Adjective→ cheapest</td><td rowspan="2">Verb→is丨prefer</td><td rowspan="2">non-stop | first |latest</td><td colspan="2">Noun→ flights| flight |breeze|trip| morning</td><td rowspan="2"> want | fly | do</td></tr><tr><td>like</td><td>need</td></tr><tr><td></td><td colspan="6">other | direct</td></tr><tr><td></td><td rowspan="2"></td><td rowspan="2">Pronoun → me| I| you | it</td><td colspan="2"> Los Angeles</td><td rowspan="2"></td></tr><tr><td>Proper-Noun→ Alaska</td><td>Baltimore</td></tr><tr><td></td><td>Chicago|</td><td colspan="2"></td></tr><tr><td></td><td>Determiner→ the|a|</td><td>an</td><td>this| these| that</td><td>United | American</td><td></td></tr><tr><td>Preposition→ from</td><td></td><td></td><td> to| on | near | in</td><td></td><td></td></tr><tr><td>Conjunction→ and</td><td></td><td>or|</td><td>but</td><td></td><td></td></tr></table>

We can use this grammar to generate sentences of this “ATIS-language”. We start with S, expand it to $N P V P$ , then choose a random expansion of NP (let’s say, to

<table><tr><td colspan="2">Grammar Rules</td><td>Examples</td></tr><tr><td></td><td>S →NPVP</td><td>I + want a morning flight</td></tr><tr><td></td><td>NP → Pronoun</td><td>I</td></tr><tr><td rowspan="4"></td><td>Proper-Noun</td><td>Los Angeles</td></tr><tr><td>Det Nominal</td><td> a + flight</td></tr><tr><td>Nominal → Nominal Noun</td><td> morning + flight</td></tr><tr><td>Noun</td><td>flights</td></tr><tr><td rowspan="4">VP</td><td>→ Verb</td><td>do</td></tr><tr><td>Verb NP</td><td> want + a flight</td></tr><tr><td>Verb NP PP</td><td> leave + Boston + in the morning</td></tr><tr><td>Verb PP</td><td> leaving + on Thursday</td></tr><tr><td colspan="2">PP → Preposition NP from + Los Angeles</td><td></td></tr></table>

![## Image Analysis: 084c5f89f187d4f70bcec9353d6dabb8f614a02db70a73bf4674d44de7a44466.jpg

**Conceptual Understanding:**
This image conceptually represents a **syntactic parse tree**, a fundamental tool in computational linguistics and natural language processing. Its main purpose is to visualize the **grammatical structure** of the sentence "I prefer a morning flight" by breaking it down into its constituent parts and assigning syntactic categories. The key ideas communicated are the **hierarchical organization of language**, demonstrating how words form phrases, and how these phrases combine to form a complete, grammatically correct sentence according to the rules of a specific grammar (grammar $\mathcal { L } _ { 0 }$). It highlights concepts like **constituency**, **part-of-speech tagging**, and **phrase structure**.

**Content Interpretation:**
The image depicts a syntactic parse tree for the English sentence "I prefer a morning flight". It illustrates the hierarchical decomposition of the sentence into its constituent grammatical categories and terminal words. The top-level 'S' (Sentence) node branches into an 'NP' (Noun Phrase) and a 'VP' (Verb Phrase). The subject 'NP' is formed by the 'Pro' (Pronoun) "I". The 'VP' consists of the 'Verb' "prefer" and an object 'NP'. This object 'NP' is further analyzed as a 'Det' (Determiner) "a" and a 'Nom' (Nominal) "morning flight". The 'Nom' is shown to consist of another 'Nom' which holds the 'Noun' "morning", and a 'Noun' "flight", indicating that "morning" modifies "flight" within the nominal phrase.

**Key Insights:**
The image effectively teaches several key concepts about sentence structure and parsing:
1.  **Sentence Decomposition:** Sentences (S) are fundamentally composed of a Noun Phrase (NP) and a Verb Phrase (VP). (Evidence: 'S' branches to 'NP' and 'VP').
2.  **Grammatical Categorization:** Each word is assigned a specific part of speech (e.g., 'I' as 'Pro', 'prefer' as 'Verb', 'a' as 'Det', 'morning' as 'Noun', 'flight' as 'Noun'). (Evidence: The direct labels above each word).
3.  **Hierarchical Phrasing:** Words combine into larger units (phrases) in a hierarchical manner. For example, 'a morning flight' is an 'NP', and 'morning flight' is a 'Nom'. (Evidence: The branching structure where 'NP' dominates 'Det' and 'Nom', and 'Nom' dominates 'Nom' and 'Noun').
4.  **Implicit Grammatical Rules:** The tree implicitly illustrates the production rules of grammar $\mathcal { L } _ { 0 }$, such as S $\rightarrow$ NP VP, NP $\rightarrow$ Pro, VP $\rightarrow$ Verb NP, NP $\rightarrow$ Det Nom, Nom $\rightarrow$ Nom Noun. (Evidence: Each parent-child relationship in the tree represents a grammatical rule application).

**Document Context:**
This image is Figure 18.4, described as "The parse tree for 'I prefer a morning flight' according to grammar $\mathcal { L } _ { 0 }$". It directly follows Figure 18.3, which outlines the grammar $\mathcal { L } _ { 0 }$. Therefore, this parse tree serves as a practical example demonstrating how the defined grammar $\mathcal { L } _ { 0 }$ can parse and represent the grammatical structure of a specific English sentence, providing a concrete illustration of the theoretical grammar's application within the document's broader narrative on linguistic structures and parsing.

**Summary:**
This image displays a **parse tree**, which is a diagram used in linguistics and computer science to represent the grammatical structure of a sentence. Specifically, it shows how the sentence "I prefer a morning flight" is broken down into its constituent parts according to a formal set of rules, referred to as grammar $\mathcal { L } _ { 0 }$. Starting from the very top, the entire sentence is represented by the node **S**, which stands for "Sentence". This 'S' node immediately branches into two main components:
1.  **NP (Noun Phrase)**: Located on the left, representing the subject of the sentence.
2.  **VP (Verb Phrase)**: Located on the right, representing the predicate of the sentence.

Let's follow the branches down to the individual words:

**A. The Subject Noun Phrase (Left Side):**
*   The 'NP' for the subject further branches down to **Pro (Pronoun)**.
*   The 'Pro' node then leads directly to the word **"I"**. This indicates that "I" is functioning as a pronoun and forms the subject noun phrase of the sentence.

**B. The Verb Phrase (Right Side):**
*   The 'VP' node branches into two parts:
    *   **Verb**: This branch leads to the word **"prefer"**, identifying it as the main verb of the sentence.
    *   **NP (Noun Phrase)**: This second branch from 'VP' represents the object of the verb "prefer". This noun phrase further breaks down.

**C. The Object Noun Phrase (Under the VP):**
*   The 'NP' for the object first branches into:
    *   **Det (Determiner)**: This branch leads to the word **"a"**, indicating it as a determiner (like an article) modifying the following nominal.
    *   **Nom (Nominal)**: This branch represents a nominal phrase, which is a word or group of words that functions as a noun. This 'Nom' then further branches:
        *   **Nom (Nominal)**: This sub-branch of the nominal leads to another 'Nom', indicating a modification structure. This inner 'Nom' branches down to a **Noun**.
            *   This **Noun** is the word **"morning"**. In this context, "morning" acts as a modifier (like an adjective) for the head noun.
        *   **Noun**: This final sub-branch from the main 'Nom' leads to the word **"flight"**. This is the head noun, being modified by "morning".

In summary, the parse tree systematically breaks down "I prefer a morning flight" into its grammatical constituents: "I" (Pronoun/Subject NP), "prefer" (Verb), and "a morning flight" (Object NP). The object NP is further analyzed as a determiner "a" followed by a nominal "morning flight", where "morning" modifies "flight". This detailed hierarchy visually clarifies the syntactic relationships between all words and phrases within the sentence according to grammar $\mathcal { L } _ { 0 }$.](images/084c5f89f187d4f70bcec9353d6dabb8f614a02db70a73bf4674d44de7a44466.jpg)
Figure 18.3 The grammar for $\mathcal { L } _ { 0 }$ , with example phrases for each rule.   
Figure 18.4 The parse tree for “I prefer a morning flight” according to grammar $\mathcal { L } _ { 0 }$ .

$I )$ , and a random expansion of $V P$ (let’s say, to Verb NP), and so on until we generate the string I prefer a morning flight. Figure 18.4 shows a parse tree that represents a complete derivation of I prefer a morning flight.

We can also represent a parse tree in a more compact format called bracketed notation; here is the bracketed representation of the parse tree of Fig. 18.4:

(18.1) [S [NP $[ { \boldsymbol { P r o } }$ I]] [VP [V prefer] [NP $[ _ { D e t }$ a] [Nom [N morning] [Nom [N flight]]]]]]

A CFG like that of $\mathcal { L } _ { 0 }$ defines a formal language. Sentences (strings of words) that can be derived by a grammar are in the formal language defined by that grammar, and are called grammatical sentences. Sentences that cannot be derived by a given formal grammar are not in the language defined by that grammar and are referred to as ungrammatical. This hard line between “in” and “out” characterizes all formal languages but is only a very simplified model of how natural languages really work. This is because determining whether a given sentence is part of a given natural language (say, English) often depends on the context. In linguistics, the use of formal languages to model natural languages is called generative grammar since the language is defined by the set of possible sentences “generated” by the grammar. (Note that this is a different sense of the word ‘generate’ than when we talk about

language models generating text.)

# 18.2.1 Formal Definition of Context-Free Grammar

We conclude this section with a quick, formal description of a context-free grammar and the language it generates. A context-free grammar $G$ is defined by four parameters: $N , \Sigma , R , S$ (technically it is a “4-tuple”).

$N$ a set of non-terminal symbols (or variables)   
$\Sigma$ a set of terminal symbols (disjoint from $N$ )   
$R$ a set of rules or productions, each of the form $A  \beta$ , where $A$ is a non-terminal, $\beta$ is a string of symbols from the infinite set of strings $( \Sigma \cup N ) ^ { * }$   
$s$ a designated start symbol and a member of $N$

For the remainder of the book we adhere to the following conventions when discussing the formal properties of context-free grammars (as opposed to explaining particular facts about English or other languages).

<table><tr><td>Capital letters like A, B, and S</td><td>Non-terminals</td></tr><tr><td>S</td><td>The start symbol</td></tr><tr><td>Lower-case Greek letters like α, β, and γ</td><td> Strings drawn from (ΣUN)*</td></tr><tr><td> Lower-case Roman letters like u, v, and w</td><td> Strings of terminals</td></tr></table>

A language is defined through the concept of derivation. One string derives another one if it can be rewritten as the second one by some series of rule applications. More formally, following Hopcroft and Ullman (1979),

# directly derives

if $A  \beta$ is a production of $R$ and $\alpha$ and $\gamma$ are any strings in the set $( \Sigma \cup N ) ^ { * }$ , then we say that $\alpha A \gamma$ directly derives $\alpha \beta \gamma ,$ or $\alpha A \gamma \Rightarrow \alpha \beta \gamma .$ .

Derivation is then a generalization of direct derivation:

Let $\alpha _ { 1 } , \alpha _ { 2 } , \ldots , \alpha _ { m }$ be strings in $( \Sigma \cup N ) ^ { * } , m \geq 1$ , such that

$$
\alpha _ { 1 } \Rightarrow \alpha _ { 2 } , \alpha _ { 2 } \Rightarrow \alpha _ { 3 } , \ldots , \alpha _ { m - 1 } \Rightarrow \alpha _ { m }
$$

# derives

We say that $\alpha _ { 1 }$ derives $\alpha _ { m }$ , or $\alpha _ { 1 } \stackrel { * } { \Rightarrow } \alpha _ { m }$

We can then formally define the language $\mathcal { L } _ { G }$ generated by a grammar $G$ as the set of strings composed of terminal symbols that can be derived from the designated start symbol $s$ .

$$
\mathcal { L } _ { G } = \left\{ w | w \mathrm { i s } \mathrm { i n } \Sigma ^ { * } \mathrm { a n d } S \stackrel { * } { \Rightarrow } w \right\}
$$

# syntactic parsing

The problem of mapping from a string of words to its parse tree is called syntactic parsing, as we’ll see in Section 18.6.

# 18.3 Treebanks

A corpus in which every sentence is annotated with a parse tree is called a treebank.

Treebanks play an important role in parsing as well as in linguistic investigations of syntactic phenomena.

Treebanks are generally made by running a parser over each sentence and then having the resulting parse hand-corrected by human linguists. Figure 18.5 shows sentences from the Penn Treebank project, which includes various treebanks in English, Arabic, and Chinese. The Penn Treebank part-of-speech tagset was defined in Chapter 17, but we’ll see minor formatting differences across treebanks. The use of LISP-style parenthesized notation for trees is extremely common and resembles the bracketed notation we saw earlier in (18.1). For those who are not familiar with it we show a standard node-and-line tree representation in Fig. 18.6.

<table><tr><td colspan="2">((s</td></tr><tr><td>(NP-SBJ (DT That)</td><td>((S</td></tr><tr><td>(J] cold)（，,)</td><td>(NP-SBJ The/DT flight/NN ）</td></tr><tr><td>（J］ empty） (NN sky））</td><td>(VP should/MD</td></tr><tr><td>(VP (VBD Was)</td><td>(VP arrive/VB</td></tr><tr><td>(ADJP-PRD (JJ full)</td><td>(PP-TMP at/IN</td></tr><tr><td>(PP (IN of)</td><td>(NP eleven/CD a.m/RB ))</td></tr><tr><td>(NP (NN fire)</td><td>(NP-TMP tomorrow/NN )))))</td></tr><tr><td>(CC and)</td><td></td></tr><tr><td>(NN light））)))</td><td></td></tr><tr><td>(..）)) (a)</td><td>(b)</td></tr></table>

![## Image Analysis: 32f334980779fb5b022c47a09b21ef617e429eb4cdca2eadf0c35b3806d1cc6a.jpg

**Conceptual Understanding:**
This image conceptually represents a syntactic parse tree, a fundamental data structure in computational linguistics for modeling sentence grammar. Its main purpose is to visually illustrate the hierarchical grammatical relationships between words and phrases in a given sentence. The tree provides a detailed breakdown of the sentence, assigning part-of-speech tags to individual words and grouping them into larger syntactic constituents (like noun phrases, verb phrases, adjective phrases, and prepositional phrases). This process, known as parsing, is crucial for understanding the structural meaning of sentences, which in turn supports various natural language processing applications. The specific sentence being parsed is "That cold, empty sky was full of fire and light" from the Brown corpus.

**Content Interpretation:**
The image represents a phrase-structure parse tree, which is a common way in computational linguistics and natural language processing to show the grammatical structure of a sentence. It breaks down the sentence "That cold, empty sky was full of fire and light" into its constituent parts (phrases and words) and assigns part-of-speech tags and phrase labels to them. This specific tree depicts a declarative sentence ('S') whose main components are a subject noun phrase ('NP-SBJ') and a verb phrase ('VP'). The subject is "That cold, empty sky", which is composed of a determiner, adjectives, and a noun. The predicate starts with the verb "was" and continues with an adjective phrase "full of fire and light", further broken down into an adjective, a prepositional phrase, and a nested noun phrase. The significance is to visually represent the hierarchical relationships between words and phrases, which is crucial for understanding sentence semantics and for tasks like machine translation, information extraction, and grammar checking.

**Key Insights:**
The main takeaway from this image is the detailed illustration of syntactic parsing. It demonstrates how a complex sentence can be decomposed into a hierarchical structure of grammatical constituents. Key insights include: 1.  **Hierarchical Structure:** Sentences are not just linear sequences of words but possess a deeper, nested structure (e.g., 'NP-SBJ' containing 'DT', 'JJ', ',', 'JJ', 'NN'). 2.  **Part-of-Speech Tagging:** Each word is assigned a specific part-of-speech tag (e.g., 'DT' for determiner, 'JJ' for adjective, 'NN' for noun, 'VBD' for past tense verb, 'IN' for preposition, 'CC' for conjunction). 3.  **Phrase Grouping:** Words combine to form meaningful phrases (e.g., 'NP' for Noun Phrase, 'VP' for Verb Phrase, 'ADJP' for Adjective Phrase, 'PP' for Prepositional Phrase). 4.  **Semantic Role Indicators:** Labels like '-SBJ' (Subject) and '-PRD' (Predicate) provide additional semantic or functional information about the phrase. The complete verbatim transcription of all nodes and words, such as 'S', 'NP-SBJ', 'DT', 'That', 'JJ', 'cold', ',', 'empty', 'NN', 'sky', 'VP', 'VBD', 'was', 'ADJP-PRD', 'full', 'PP', 'IN', 'of', 'NP', 'NN', 'fire', 'CC', 'and', 'NN', 'light', provides direct evidence for these insights, showing the granular detail of linguistic annotation in a treebank.

**Document Context:**
This image is presented in the context of '18.3 Treebanks', as indicated by the document context. Treebanks are collections of sentences parsed by human annotators, serving as crucial resources for training and evaluating natural language processing (NLP) models. This specific parse tree is labeled as corresponding to a sentence from the Brown corpus, one of the foundational treebanks. It directly supports the discussion on treebanks by providing a concrete example of a sentence's parse structure, illustrating how raw text is annotated with syntactic information. The subsequent text refers to this figure as 'Figure 18.6 The tree corresponding to the Brown corpus sentence in the previous figure', confirming its role as an example of a parsed sentence within a treebank.

**Summary:**
The image displays a parse tree representing the syntactic structure of the sentence "That cold, empty sky was full of fire and light". The tree is rooted at 'S' (Sentence), which branches into a 'NP-SBJ' (Noun Phrase - Subject) and a 'VP' (Verb Phrase), followed by an ellipsis indicating further potential branches not fully depicted. The 'NP-SBJ' node further breaks down into a 'DT' (Determiner) for "That", a 'JJ' (Adjective) for "cold", a comma ",", another 'JJ' for "empty", and an 'NN' (Noun) for "sky". The 'VP' node expands into a 'VBD' (Verb, past tense) for "was" and an 'ADJP-PRD' (Adjective Phrase - Predicate). The 'ADJP-PRD' then splits into a 'JJ' for "full" and a 'PP' (Prepositional Phrase). The 'PP' node consists of an 'IN' (Preposition) for "of" and an 'NP' (Noun Phrase). Finally, this 'NP' further comprises an 'NN' for "fire", a 'CC' (Coordinating Conjunction) for "and", and another 'NN' for "light". The tree systematically illustrates the part-of-speech tags and their hierarchical grouping into larger grammatical constituents, demonstrating the parsed structure of the sentence for linguistic analysis.](images/32f334980779fb5b022c47a09b21ef617e429eb4cdca2eadf0c35b3806d1cc6a.jpg)
Figure 18.5 Parses from the LDC Treebank3 for (a) Brown and (b) ATIS sentences.   
Figure 18.6 The tree corresponding to the Brown corpus sentence in the previous figure.

The sentences in a treebank implicitly constitute a grammar of the language. For example, from the parsed sentences in Fig. 18.5 we can extract the CFG rules shown in Fig. 18.7 (with rule suffixes (-SBJ) stripped for simplicity). The grammar used to parse the Penn Treebank is very flat, resulting in very many rules. For example, among the approximately 4,500 different rules for expanding VPs are separate rules for PP sequences of any length and every possible arrangement of verb arguments:

<table><tr><td>Grammar</td><td colspan="2">Lexicon</td><td></td></tr><tr><td>S→ NP VP.</td><td>DT→the</td><td>that</td><td></td></tr><tr><td>S → NP VP</td><td>JJ→cold |</td><td> empty | full</td><td></td></tr><tr><td>NP →CD RB</td><td></td><td>NN→sky|fire|light|flight|tomorrow</td><td></td></tr><tr><td> NP → DT NN</td><td>CC→and</td><td></td><td></td></tr><tr><td>NP → NN CC NN</td><td>IN→ of| at</td><td></td><td></td></tr><tr><td>NP→DT JJ,JJ NN</td><td>CD→eleven</td><td></td><td></td></tr><tr><td>NP → NN</td><td>RB → a.m.</td><td></td><td></td></tr><tr><td>VP → MD VP</td><td>VB → arrive</td><td></td><td></td></tr><tr><td>VP→VBD ADJP</td><td>VBD→was| said</td><td></td><td></td></tr><tr><td>VP → MD VP</td><td>MD→should| would</td><td></td><td></td></tr><tr><td>VP→VB PP NP</td><td></td><td></td><td></td></tr><tr><td>ADJP→JJPP</td><td></td><td></td><td></td></tr><tr><td>PP →IN NP</td><td></td><td></td><td></td></tr></table>

$\mathsf { V P }  \mathsf { V B D }$ PP $\mathsf { V P }  \mathsf { V B D }$ PP PP $\mathsf { V P }  \mathsf { V B D }$ PP PP PP $\mathsf { V P }  \mathsf { V B D }$ PP PP PP PP $\nabla \mathsf { P } \ \to \ \mathsf { V B }$ ADVP PP $\nabla \mathsf { P } \ \to \ \mathsf { V B }$ PP ADVP $\tt V P $ ADVP VB PP

# 18.4 Grammar Equivalence and Normal Form

strongly equivalent

weakly equivalent normal form

Chomsky normal form

binary branching

A formal language is defined as a (possibly infinite) set of strings of words. This suggests that we could ask if two grammars are equivalent by asking if they generate the same set of strings. In fact, it is possible to have two distinct context-free grammars generate the same language. We say that two grammars are strongly equivalent if they generate the same set of strings and if they assign the same phrase structure to each sentence (allowing merely for renaming of the non-terminal symbols). Two grammars are weakly equivalent if they generate the same set of strings but do not assign the same phrase structure to each sentence.

It is sometimes useful to have a normal form for grammars, in which each of the productions takes a particular form. For example, a context-free grammar is in Chomsky normal form (CNF) (Chomsky, 1963) if it is $\epsilon$ -free and if in addition each production is either of the form $A  B$ $C$ or $A \to a$ . That is, the right-hand side of each rule either has two non-terminal symbols or one terminal symbol. Chomsky normal form grammars are binary branching, that is they have binary trees (down to the prelexical nodes). We make use of this binary branching property in the CKY parsing algorithm in Section 18.6.

Any context-free grammar can be converted into a weakly equivalent Chomsky normal form grammar. For example, a rule of the form

$$
\textit { A } \to \textit { B C D }
$$

can be converted into the following two CNF rules (Exercise 18.1 asks the reader to

Figure 18.8 The $\mathcal { L } _ { 1 }$ miniature English grammar and lexicon.   

<table><tr><td>Grammar</td><td>Lexicon</td><td></td></tr><tr><td>S → NP VP</td><td>Det→that|this|the</td><td>a</td></tr><tr><td>S → Aux NP VP</td><td></td><td>Noun → book | flight | meal| money</td></tr><tr><td>S → VP</td><td>Verb→book丨 include丨prefer</td><td></td></tr><tr><td>NP → Pronoun</td><td>Pronoun → I| she| me</td><td></td></tr><tr><td>NP → Proper-Noun</td><td>Proper-Noun → Houston | United</td><td></td></tr><tr><td>NP →DetNominal</td><td>Aux →does</td><td></td></tr><tr><td>Nominal → Noun</td><td></td><td>Preposition →from | to| on| near| through</td></tr><tr><td>Nominal → Nominal Noun</td><td></td><td></td></tr><tr><td>Nominal → Nominal PP</td><td></td><td></td></tr><tr><td>VP →Verb</td><td></td><td></td></tr><tr><td>VP → Verb NP</td><td></td><td></td></tr><tr><td>VP Verb NP PP</td><td></td><td></td></tr><tr><td>VP → Verb PP</td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td>VP → VP PP PP → Preposition NP</td><td></td><td></td></tr></table>

formulate the complete algorithm):

$$
\begin{array} { l } { A \ \to \ B \ X } \\ { X \ \to \ C \ D } \end{array}
$$

Sometimes using binary branching can actually produce smaller grammars. For example, the sentences that might be characterized as

$$
\tt V P \to \tt V B D \mathbb { N P \ P P ^ { * } }
$$

are represented in the Penn Treebank by this series of rules:

$$
\begin{array} { r l } & { \mathrm { V P ~  ~ V B D ~ N P ~ P P } } \\ & { \mathrm { V P ~  ~ V B D ~ N P ~ P P ~ P P } } \\ & { \mathrm { V P ~  ~ V B D ~ N P ~ P P ~ P P ~ P P } } \\ & { \mathrm { V P ~  ~ V B D ~ N P ~ P P ~ P P ~ P P ~ P P } } \\ & { \mathrm { . . . } } \end{array}
$$

but could also be generated by the following two-rule grammar:

$$
\begin{array} { l } { { \tt V P } \  \ { \tt V B D } \ { \tt N P } \ { \tt P P } } \\ { { \tt V P } \  \ { \tt V P } \ { \tt P P } } \end{array}
$$

# Chomskyadjunction

The generation of a symbol A with a potentially infinite sequence of symbols B with a rule of the form $\texttt { A }  \texttt { A B }$ is known as Chomsky-adjunction.

# 18.5 Ambiguity

Ambiguity is the most serious problem faced by syntactic parsers. Chapter 17 introduced the notions of part-of-speech ambiguity and part-of-speech disambiguation. Here, we introduce a new kind of ambiguity, called structural ambiguity, illustrated with a new toy grammar $\mathcal { L } _ { 1 }$ , shown in Figure 18.8, which adds a few rules to the $\mathcal { L } _ { 0 }$ grammar.

Structural ambiguity occurs when the grammar can assign more than one parse to a sentence. Groucho Marx’s well-known line as Captain Spaulding in Animal

![## Image Analysis: ec746a8ed198a2406da7bf856af1c88dabd8d902cc7bce68222ac1d4b5c44784.jpg

**Conceptual Understanding:**
This image conceptually represents two distinct syntactic analyses, or parse trees, for a single, grammatically ambiguous English sentence: "I shot an elephant in my pajamas." The main purpose is to illustrate how the same sequence of words can be assigned different underlying grammatical structures, leading to different semantic interpretations. It visually demonstrates the concept of structural ambiguity in natural language processing and linguistics.

**Content Interpretation:**
The image shows two different syntactic parse trees for the sentence "I shot an elephant in my pajamas," revealing two possible meanings. These trees illustrate the process of syntactic parsing, breaking a sentence into its constituent parts (Noun Phrase, Verb Phrase, Prepositional Phrase, etc.) and their hierarchical relationships. Each tree demonstrates how different attachments of phrases lead to drastically different semantic interpretations.

In the **Left Parse Tree**, the Prepositional Phrase (PP) "in my pajamas" is attached to the 'Nominal' phrase containing "elephant". This is supported by the hierarchy: `S -> VP -> NP -> Nominal -> PP (in my pajamas)`, with `Noun elephant` appearing directly above `PP in my pajamas` within the same `Nominal` phrase. This structure implies that "in my pajamas" modifies "elephant," suggesting the elephant was wearing the pajamas.

In the **Right Parse Tree**, the 'PP' "in my pajamas" is attached directly to the main 'VP' of the sentence. This is evident from the hierarchy: `S -> VP -> VP -> PP (in my pajamas)`. This structure indicates that "in my pajamas" modifies the action of shooting, implying the subject ("I") was wearing the pajamas during the act of shooting. The textual evidence `VP` (inner) and `PP in my pajamas` as direct children of the higher `VP` confirms this interpretation.

The text labels `S`, `NP`, `VP`, `Pronoun`, `Verb`, `Det`, `Nominal`, `Noun`, `PP` are standard abbreviations for syntactic categories, representing abstract structures used in linguistic theory and computational linguistics to model sentence structure.

**Key Insights:**
The main takeaway from this image is a clear demonstration of **syntactic ambiguity**, showing that a single sentence can have multiple valid parse trees, each corresponding to a different meaning.

1.  **Grammatical Structure Dictates Meaning:** The precise way phrases are grouped and attached within a sentence's grammatical structure determines its meaning. The varying attachment points for "in my pajamas" (to the `Nominal` containing `elephant` in the left tree, versus to the main `VP` in the right tree) are direct textual evidence for the two distinct interpretations.
2.  **Prepositional Phrase Attachment Ambiguity:** Prepositional phrases are a common source of ambiguity. The phrase "in my pajamas" can modify either the `Noun` (`elephant`) or the `Verb Phrase` (`shot an elephant`), leading to the humorous and literal readings, respectively. This is explicitly shown by its different parent nodes (`Nominal` vs. `VP`) in the two parse trees.
3.  **Importance of Parsing in Language Understanding:** For machines (and humans) to correctly understand sentences, they must be able to resolve such ambiguities by identifying the correct underlying syntactic structure. The image shows the two possible structures that a parsing algorithm might generate.
4.  **Visualizing Linguistic Structure:** Parse trees are an effective way to visually represent the hierarchical and constituent structure of sentences, making abstract grammatical relationships concrete. The consistent use of nodes like `S`, `NP`, `VP`, `PP` and lexical items like `I`, `shot`, `an`, `elephant`, `in my pajamas` provides the textual evidence for this visualization.

**Document Context:**
This image directly supports Section 18.5, titled "Ambiguity," within the document. It serves as a visual example of a specific type of ambiguity known as syntactic ambiguity or structural ambiguity. The document context explicitly mentions that the left parse corresponds to the humorous reading (elephant in pajamas), and the right parse to the reading where Captain Spaulding (the shooter) is in pajamas. This image provides the concrete linguistic analysis that explains *why* the sentence "I shot an elephant in my pajamas" can be understood in these two different ways, based on how the prepositional phrase "in my pajamas" attaches to different parts of the sentence structure.

**Summary:**
The image presents two hierarchical diagrams, known as parse trees, which illustrate the grammatical structure of the sentence "I shot an elephant in my pajamas." These trees reveal why this sentence is ambiguous, meaning it can be interpreted in two different ways.

**The Left Parse Tree (Humorous Reading: Elephant in Pajamas):**
This tree shows the sentence (S) at the top, which breaks down into a Noun Phrase (NP) "I" and a Verb Phrase (VP) "shot an elephant in my pajamas." Within the VP, the verb is "shot," and the object is a Noun Phrase "an elephant in my pajamas." Crucially, the Prepositional Phrase (PP) "in my pajamas" is attached directly to the "Nominal" phrase that contains the "Noun" "elephant." This grammatical grouping signifies that "in my pajamas" describes the "elephant."
*   **Structure:**
    *   S (Sentence)
        *   NP (Noun Phrase)
            *   Pronoun
                *   I
        *   VP (Verb Phrase)
            *   Verb
                *   shot
            *   NP (Noun Phrase)
                *   Det (Determiner)
                    *   an
                *   Nominal
                    *   Nominal
                        *   Noun
                            *   elephant
                    *   PP (Prepositional Phrase)
                        *   in my pajamas
*   **Meaning:** This structure leads to the interpretation where the elephant itself was wearing the pajamas.

**The Right Parse Tree (Literal Reading: Shooter in Pajamas):**
This tree also starts with a Sentence (S), breaking into a Noun Phrase (NP) "I" and a Verb Phrase (VP). However, in this parse, the Verb Phrase "shot an elephant in my pajamas" is broken down differently. The main VP directly contains an inner Verb Phrase "shot an elephant" and the Prepositional Phrase (PP) "in my pajamas." This means "in my pajamas" modifies the action of "shot an elephant" or the entire Verb Phrase. The inner VP then further breaks down into the Verb "shot" and the Noun Phrase "an elephant." The Noun Phrase "an elephant" consists of the Determiner "an" and the Noun "elephant."
*   **Structure:**
    *   S (Sentence)
        *   NP (Noun Phrase)
            *   Pronoun
                *   I
        *   VP (Verb Phrase)
            *   VP (Verb Phrase - inner)
                *   Verb
                    *   shot
                *   NP (Noun Phrase)
                    *   Det (Determiner)
                        *   an
                    *   Nominal
                        *   Noun
                            *   elephant
            *   PP (Prepositional Phrase)
                *   in my pajamas
*   **Meaning:** This structure leads to the interpretation where the subject ("I," the person who did the shooting) was wearing the pajamas while performing the action.

In summary, these two parse trees visually demonstrate that the ambiguous meaning of the sentence "I shot an elephant in my pajamas" arises from the different ways the prepositional phrase "in my pajamas" can grammatically attach to other parts of the sentence. The left tree attaches it to the "elephant," while the right tree attaches it to the action of "shooting." This is a fundamental concept in understanding how syntax influences semantics in language.](images/ec746a8ed198a2406da7bf856af1c88dabd8d902cc7bce68222ac1d4b5c44784.jpg)
Figure 18.9 Two parse trees for an ambiguous sentence. The parse on the left corresponds to the humorous reading in which the elephant is in the pajamas, the parse on the right corresponds to the reading in which Captain Spaulding did the shooting in his pajamas.

PP-attachment ambiguity

Crackers is ambiguous because the phrase in my pajamas can be part of the NP headed by elephant or a part of the verb phrase headed by shot. Figure 18.9 illustrates these two analyses of Marx’s line using rules from $\mathcal { L } _ { 1 }$ .

Structural ambiguity, appropriately enough, comes in many forms. Two common kinds of ambiguity are attachment ambiguity and coordination ambiguity. A sentence has an attachment ambiguity if a particular constituent can be attached to the parse tree at more than one place. The Groucho Marx sentence is an example of PP-attachment ambiguity: the preposition phrase can be attached either as part of the NP or as part of the VP. Various kinds of adverbial phrases are also subject to this kind of ambiguity. For instance, in the following example the gerundive-VP flying to Paris can be part of a gerundive sentence whose subject is the Eiffel Tower or it can be an adjunct modifying the VP headed by saw:

(18.2) We saw the Eiffel Tower flying to Paris.

In coordination ambiguity phrases can be conjoined by a conjunction like and. For example, the phrase old men and women can be bracketed as [old [men and women]], referring to old men and old women, or as [old men] and [women], in which case it is only the men who are old. These ambiguities combine in complex ways in real sentences, like the following news sentence from the Brown corpus:

(18.3) President Kennedy today pushed aside other White House business to devote all his time and attention to working on the Berlin crisis address he will deliver tomorrow night to the American people over nationwide television and radio.

This sentence has a number of ambiguities, although since they are semantically unreasonable, it requires a careful reading to see them. The last noun phrase could be parsed [nationwide [television and radio]] or [[nationwide television] and radio]. The direct object of pushed aside should be other White House business but could also be the bizarre phrase [other White House business to devote all his time and attention to working] (i.e., a structure like Kennedy affirmed [his intention to propose a new budget to address the deficit]). Then the phrase on the Berlin crisis address he will deliver tomorrow night to the American people could be an adjunct modifying the verb pushed. A $P P$ like over nationwide television and radio could be attached to any of the higher $V P s$ or NPs (e.g., it could modify people or night).

The fact that there are many grammatically correct but semantically unreasonable parses for naturally occurring sentences is an irksome problem that affects all parsers. Fortunately, the CKY algorithm below is designed to efficiently handle structural ambiguities. And as we’ll see in the following section, we can augment CKY with neural methods to choose a single correct parse by syntactic disambiguation.

# 18.6 CKY Parsing: A Dynamic Programming Approach

Dynamic programming provides a powerful framework for addressing the problems caused by ambiguity in grammars. Recall that a dynamic programming approach systematically fills in a table of solutions to subproblems. The complete table has the solution to all the subproblems needed to solve the problem as a whole. In the case of syntactic parsing, these subproblems represent parse trees for all the constituents detected in the input.

The dynamic programming advantage arises from the context-free nature of our grammar rules—once a constituent has been discovered in a segment of the input we can record its presence and make it available for use in any subsequent derivation that might require it. This provides both time and storage efficiencies since subtrees can be looked up in a table, not reanalyzed. This section presents the Cocke-KasamiYounger (CKY) algorithm, the most widely used dynamic-programming based approach to parsing. Chart parsing (Kaplan 1973, Kay 1982) is a related approach, and dynamic programming methods are often referred to as chart parsing methods.

# 18.6.1 Conversion to Chomsky Normal Form

The CKY algorithm requires grammars to first be in Chomsky Normal Form (CNF). Recall from Section 18.4 that grammars in CNF are restricted to rules of the form $A  B C$ or $A  w$ . That is, the right-hand side of each rule must expand either to two non-terminals or to a single terminal. Restricting a grammar to CNF does not lead to any loss in expressiveness, since any context-free grammar can be converted into a corresponding CNF grammar that accepts exactly the same set of strings as the original grammar.

Let’s start with the process of converting a generic CFG into one represented in CNF. Assuming we’re dealing with an $\epsilon$ -free grammar, there are three situations we need to address in any generic grammar: rules that mix terminals with non-terminals on the right-hand side, rules that have a single non-terminal on the right-hand side, and rules in which the length of the right-hand side is greater than 2.

The remedy for rules that mix terminals and non-terminals is to simply introduce a new dummy non-terminal that covers only the original terminal. For example, a rule for an infinitive verb phrase such as I ${ \mathsf { V } } F { \mathsf { - } } V P \to t o V P$ would be replaced by the two rules $I N F { \cdot } V P  T O V P$ and $T O  t o$ .

Rules with a single non-terminal on the right are called unit productions. We can eliminate unit productions by rewriting the right-hand side of the original rules with the right-hand side of all the non-unit production rules that they ultimately lead to. More formally, if $A \stackrel { * } { \Rightarrow } B$ by a chain of one or more unit productions and $B \to \gamma$ is a non-unit production in our grammar, then we add $A  \gamma$ for each such rule in the grammar and discard all the intervening unit productions. As we demonstrate with our toy grammar, this can lead to a substantial flattening of the grammar and a consequent promotion of terminals to fairly high levels in the resulting trees.

Rules with right-hand sides longer than 2 are normalized through the introduction of new non-terminals that spread the longer sequences over several new rules. Formally, if we have a rule like

$$
A \to B C \gamma
$$

we replace the leftmost pair of non-terminals with a new non-terminal and introduce a new production, resulting in the following new rules:

$$
\begin{array} { r } { A \  \ X I \ \gamma } \\ { \ X I \  \ B C } \end{array}
$$

In the case of longer right-hand sides, we simply iterate this process until the offending rule has been replaced by rules of length 2. The choice of replacing the leftmost pair of non-terminals is purely arbitrary; any systematic scheme that results in binary rules would suffice.

In our current grammar, the rule $S  A u x N P V P$ would be replaced by the two rules $S  X I \ V P$ and $X I  A u x N P$ .

The entire conversion process can be summarized as follows:

1. Copy all conforming rules to the new grammar unchanged.   
2. Convert terminals within rules to dummy non-terminals.   
3. Convert unit productions.   
4. Make all rules binary and add them to new grammar.

Figure 18.10 shows the results of applying this entire conversion procedure to the $\mathcal { L } _ { 1 }$ grammar introduced earlier on page 395. Note that this figure doesn’t show the original lexical rules; since these original lexical rules are already in CNF, they all carry over unchanged to the new grammar. Figure 18.10 does, however, show the various places where the process of eliminating unit productions has, in effect, created new lexical rules. For example, all the original verbs have been promoted to both $V P s$ and to $S \mathrm { s }$ in the converted grammar.

# 18.6.2 CKY Recognition

With our grammar now in CNF, each non-terminal node above the part-of-speech level in a parse tree will have exactly two daughters. A two-dimensional matrix can be used to encode the structure of an entire tree. For a sentence of length $n$ , we will work with the upper-triangular portion of an $( n + 1 ) \times ( n + 1 )$ matrix. Each cell $[ i , j ]$ in this matrix contains the set of non-terminals that represent all the constituents that span positions $i$ through $j$ of the input. Since our indexing scheme begins with 0, it’s natural to think of the indexes as pointing at the gaps between the input words (as in $\phantom { } _ { 0 } B o o k \phantom { } _ { 1 }$ that 2 flight 3). These gaps are often called fenceposts, on the metaphor of the posts between segments of fencing. It follows then that the cell that represents the entire input resides in position $[ 0 , n ]$ in the matrix.

Since each non-terminal entry in our table has two daughters in the parse, it follows that for each constituent represented by an entry $[ i , j ]$ , there must be a position in the input, $k$ , where it can be split into two parts such that $i < k < j$ . Given such a position $k$ , the first constituent $[ i , k ]$ must lie to the left of entry $[ i , j ]$ somewhere along row $i$ , and the second entry $[ k , j ]$ must lie beneath it, along column $j$ .

<table><tr><td>L1 Grammar</td><td>L1 in CNF</td></tr><tr><td>S →NP VP</td><td>S →NPVP</td></tr><tr><td>S → Aux NP VP</td><td>S →X1 VP X1 → Aux NP</td></tr><tr><td>S →VP</td><td>S →book丨 include|prefer S → Verb NP S → X2 PP S →Verb PP</td></tr><tr><td>NP → Pronoun</td><td>S → VP PP NP → I| she| me</td></tr><tr><td>NP → Proper-Noun</td><td>NP →United| Houston</td></tr><tr><td>NP → Det Nominal</td><td>NP → DetNominal</td></tr><tr><td>Nominal → Noun</td><td> Nominal -→ book | flight | meal | money</td></tr><tr><td>Nominal → Nominal Noun</td><td>Nominal → Nominal Noun</td></tr><tr><td>Nominal → Nominal PP</td><td>Nominal → Nominal PP</td></tr><tr><td>VP →Verb</td><td>VP →book 丨 include丨 prefer</td></tr><tr><td>VP → Verb NP</td><td>VP → Verb NP</td></tr><tr><td>VP → Verb NP PP</td><td>VP → X2 PP</td></tr><tr><td></td><td>X2 →VerbNP</td></tr><tr><td>VP → Verb PP</td><td>VP →Verb PP</td></tr><tr><td>VP → VP PP</td><td>VP →VP PP</td></tr><tr><td>PP → Preposition NP</td><td>PP → Preposition NP</td></tr></table>

Figure 18.10 $\mathcal { L } _ { 1 }$ Grammar and its conversion to CNF. Note that although they aren’t shown here, all the original lexical entries from $\mathcal { L } _ { 1 }$ carry over unchanged as well.

To make this more concrete, consider the following example with its completed parse matrix, shown in Fig. 18.11.

(18.4) Book the flight through Houston.

The superdiagonal row in the matrix contains the parts of speech for each word in the input. The subsequent diagonals above that superdiagonal contain constituents that cover all the spans of increasing length in the input.

Given this setup, CKY recognition consists of filling the parse table in the right way. To do this, we’ll proceed in a bottom-up fashion so that at the point where we are filling any cell $[ i , j ]$ , the cells containing the parts that could contribute to this entry (i.e., the cells to the left and the cells below) have already been filled. The algorithm given in Fig. 18.12 fills the upper-triangular matrix a column at a time working from left to right, with each column filled from bottom to top, as the right side of Fig. 18.11 illustrates. This scheme guarantees that at each point in time we have all the information we need (to the left, since all the columns to the left have already been filled, and below since we’re filling bottom to top). It also mirrors online processing, since filling the columns from left to right corresponds to processing each word one at a time.

The outermost loop of the algorithm given in Fig. 18.12 iterates over the columns, and the second loop iterates over the rows, from the bottom up. The purpose of the innermost loop is to range over all the places where a substring spanning $i$ to $j$ in the input might be split in two. As $k$ ranges over the places where the string can be split, the pairs of cells we consider move, in lockstep, to the right along row $i$ and down along column $j$ . Figure 18.13 illustrates the general case of filling cell $[ i , j ]$ .

![## Image Analysis: de24e762d402c3d1daad701af5c888a828084b644ccd0a7af91f473a60ecc81b.jpg

**Conceptual Understanding:**
The image represents a parse table generated by the CKY (Cocke-Kasami-Younger) algorithm, which is a method for syntactic parsing of sentences. Conceptually, it illustrates how a sentence is analyzed by breaking it down into all possible constituent parts according to a predefined grammar. The main purpose is to demonstrate the step-by-step process of identifying grammatical categories for substrings of a sentence, leading to a complete parse. The key idea communicated is the application of dynamic programming to efficiently determine the syntactic structure of a sentence, by storing and reusing intermediate parsing results in a tabular format.

**Content Interpretation:**
The image primarily illustrates the application of the CKY (Cocke-Kasami-Younger) algorithm for parsing a specific English sentence. The CKY algorithm is a dynamic programming approach used in computational linguistics to parse context-free grammars, especially those in Chomsky Normal Form (CNF). The left part, the CKY parse table, represents the core data structure used by the algorithm. It shows a systematic way of identifying all possible syntactic constituents for every substring of the input sentence "Book the flight through Houston." The cells are populated with non-terminal symbols (e.g., S, VP, NP, Det, Noun, Prep, PP, Proper-Noun) from a grammar, indicating that a particular span of words can be categorized as that non-terminal. The table builds up from individual words to larger phrases. The right diagram visually depicts the typical bottom-up and left-to-right or diagonal filling order of the parse table, indicating how constituents for smaller spans are computed before they are used to build constituents for larger spans. The significance of the information presented is to show the intermediate steps and final result of a parsing process, highlighting how complex sentences are broken down into their constituent parts and how different grammatical categories are assigned.

**Key Insights:**
The main takeaways from this image are: 1. **Dynamic Programming Principle:** The CKY algorithm uses dynamic programming, building solutions for larger problems (longer word spans) from solutions to smaller problems (shorter word spans). This is evident from the table's structure where cells representing larger spans depend on combinations of cells representing smaller, adjacent spans. 2. **Exhaustive Parsing:** The table attempts to find all possible constituent categories for every substring of the input sentence, demonstrating an exhaustive approach to parsing. For instance, 'Book' is listed as multiple categories (S, VP, Verb, Nominal, Noun) in cell [0,1], reflecting its potential ambiguity. 3. **Bottom-Up Construction:** The parse table is conceptually filled from the bottom-left upwards and to the right, as depicted by the right-hand diagram. This means single-word constituents are identified first, then two-word constituents, and so on. 4. **Grammatical Categories:** The image provides examples of common grammatical categories (S, VP, NP, Det, Nominal, Noun, Prep, PP, Proper-Noun) and how they apply to words and phrases within a sentence. 5. **Sentence Validity:** The ultimate goal, though not explicitly shown as 'S' in the very top-right, is to determine if the entire sentence (span [0,5]) can be categorized as an 'S', indicating a syntactically valid sentence according to the grammar. The text in the cells, such as `S, VP, Verb, Nominal, Noun` in `[0,1]`, `Det` in `[1,2]`, `NP` in `[0,3]`, `Nominal, Noun` in `[1,3]`, `Prep` in `[2,4]`, and `NP, Proper-Noun` in `[3,5]`, directly provides evidence for these takeaways by showing the specific constituents assigned to different word spans.

**Document Context:**
This image is directly relevant to the section "18.6.2 CKY Recognition" and serves as a visual example of the CKY algorithm's application. Figure 18.11, as described in the accompanying text, refers to the "Completed parse table for Book the flight through Houston," which is the detailed table shown on the left. Figure 18.12, described as "The CKY algorithm," is likely represented by the conceptual diagram on the right, which shows the general filling strategy. Together, these two parts of the image visually explain how the CKY parsing algorithm works in practice to determine the syntactic structure of a given sentence, providing a concrete example for the theoretical discussion of the algorithm in the document.

**Summary:**
The image displays two main components side-by-side: a CKY parse table on the left and a conceptual diagram illustrating the filling order for such a table on the right. The CKY parse table systematically shows the possible syntactic categories (constituents) for all substrings of the sentence "Book the flight through Houston." Each cell [i,j] represents the span of words from index i (inclusive) to j (exclusive), and it contains the non-terminal categories that can derive that specific substring. The table is filled in a bottom-up, left-to-right diagonal fashion, starting with single words and combining them to form larger constituents. For example, 'Book' can be an 'S', 'VP', 'Verb', 'Nominal', or 'Noun'. 'the' is a 'Det'. These are then combined to form 'NP' for 'the flight', and so on. The right diagram provides a visual metaphor for how the cells of such a table are typically filled, with arrows pointing upwards, implying that smaller spans are processed before larger spans, and cells are filled column by column from bottom to top, or diagonally. This structured approach ensures that all possible derivations for a given substring are considered, making the process of parsing efficient and exhaustive. The goal is to determine if the top-rightmost cell contains the 'S' (Sentence) category, indicating that the entire input string is a valid sentence according to the grammar.](images/de24e762d402c3d1daad701af5c888a828084b644ccd0a7af91f473a60ecc81b.jpg)
Figure 18.11 Completed parse table for Book the flight through Houston.   
Figure 18.12 The CKY algorithm.

<table><tr><td>function CKY-PARSE(words,grammar) returns table</td></tr><tr><td>for j←from 1 to LENGTH(words) do</td></tr><tr><td>for all {A|A →words[j] ∈ grammar}</td></tr><tr><td>table[j-1,j]←table[j-1,j] UA</td></tr><tr><td>for i←from j-2 down to 0 do</td></tr><tr><td>fork←i+1to j-1do</td></tr><tr><td>for all {A|A →BC ∈ grammar and B ∈ table[i,k] andC ∈ table[k,jl} table[i,j]←table[i,j]UA</td></tr></table>

At each such split, the algorithm considers whether the contents of the two cells can be combined in a way that is sanctioned by a rule in the grammar. If such a rule exists, the non-terminal on its left-hand side is entered into the table.

Figure 18.14 shows how the five cells of column 5 of the table are filled after the word Houston is read. The arrows point out the two spans that are being used to add an entry to the table. Note that the action in cell [0, 5] indicates the presence of three alternative parses for this input, one where the $P P$ modifies the flight, one where it modifies the booking, and one that captures the second argument in the original $V P  V e r b N P P P P P$ rule, now captured indirectly with the $V P  X 2 \ : P P$ rule.

# 18.6.3 CKY Parsing

The algorithm given in Fig. 18.12 is a recognizer, not a parser. That is, it can tell us whether a valid parse exists for a given sentence based on whether or not if finds an $s$ in cell $[ 0 , n ]$ , but it can’t provide the derivation, which is the actual job for a parser. To turn it into a parser capable of returning all possible parses for a given input, we can make two simple changes to the algorithm: the first change is to augment the entries in the table so that each non-terminal is paired with pointers to the table entries from which it was derived (more or less as shown in Fig. 18.14), the second change is to permit multiple versions of the same non-terminal to be entered into the table (again as shown in Fig. 18.14). With these changes, the completed table contains all the possible parses for a given input. Returning an arbitrary single parse consists of choosing an $s$ from cell $[ 0 , n ]$ and then recursively retrieving its component constituents from the table. Of course, instead of returning every parse for a sentence, we usually want just the best parse; we’ll see how to do that in the next section.

![## Image Analysis: fe53c3977e441e535f8d920f6ca749a53a2a59cbd3fcff5caef9e06a6a865c64.jpg

**Conceptual Understanding:**
This image represents the computational dependencies within a CKY parsing table for a specific cell [i, j]. Conceptually, it illustrates the recursive process of constructing a parse for a span [i, j] by combining parses of its constituent sub-spans [i, k] and [k, j] for all possible split points k. The main purpose is to visualize the 

**Content Interpretation:**
The image illustrates the core recursive dependencies for filling the cell [i, j] in a CKY parsing table. It shows that the computation of [i, j] relies on combining results from two types of subproblems: [i, k] (horizontal dependencies) and [k, j] (vertical dependencies), for all possible split points k between i and j. The various labeled cells like [i,i+1], [i,j-1], [i+1,j], and [j-1,j], along with the connecting arrows, explicitly demonstrate how specific smaller spans contribute to the larger span [i,j]. The ellipses (...) signify that all intermediate split points are considered, highlighting the iterative nature of the CKY algorithm over the length of the span.

**Key Insights:**
The main takeaways from this image are:

*   **CKY Algorithm's Core Recurrence:** The image visually explains the central idea of the CKY algorithm: to fill cell [i,j], one must consider all possible split points k (where i < k < j) and combine the results from cell [i,k] and cell [k,j]. This is evidenced by arrows originating from cells like [i,i+1], [i,j-1], [i+1,j], and [j-1,j] and pointing directly into [i,j], coupled with the ellipses (...) indicating the full range of k.
*   **Bottom-Up Parsing:** The dependency arrows pointing from smaller span cells (e.g., [i,k] and [k,j]) towards the larger span cell [i,j] demonstrate the bottom-up nature of the CKY algorithm, where solutions for smaller constituents are built up to form larger ones.
*   **Dynamic Programming Dependencies:** The diagram illustrates the principle of dynamic programming where solutions to subproblems (smaller spans) are stored and reused to solve larger problems. The specific labels and arrows show exactly which subproblems are required to compute a given cell.

**Document Context:**
This image is highly relevant to the document's section '18.6.3 CKY Parsing' as it provides a direct visual representation of the fundamental dynamic programming step in the CKY algorithm. The diagram directly illustrates 'All the ways to fill the [i, j]th cell in the CKY table,' complementing the textual explanation of the algorithm by clearly showing the dependencies between different cells (spans) within the parsing table. It visually explains the recurrence relation where larger spans are derived from combinations of smaller, overlapping spans.

**Summary:**
This diagram visually represents the core dependencies involved in filling the [i, j]th cell of a CKY (Cocke–Kasami–Younger) parsing table, which is a key step in context-free grammar parsing using dynamic programming.

The overall structure is a triangular matrix, with specific cells labeled by their span indices [start_index, end_index]. For example, [0,1] represents a span of length one starting at index 0, and [0,n] represents the full input sentence from index 0 to n. [n-1,n] is another example of a small span at the end of the input.

The central focus of the diagram is on how to fill the cell labeled [i, j]. This cell represents a constituent spanning from index i to index j in the input sentence. To fill [i, j], the CKY algorithm relies on combining results from two types of subproblems, iterating through all possible "split points" k such that i < k < j.

Specifically, the diagram shows the following dependencies for [i, j]:

1.  **Horizontal Dependencies (from cells [i, k]):** Information from cells that start at index i and end at an intermediate index k (where k is between i+1 and j-1) contributes to [i, j].
    *   The diagram explicitly shows [i,i+1] and [i,j-1] as contributors. An arrow originates from [i,i+1] and points directly to [i,j]. Another arrow originates from [i,j-1] and points directly to [i,j].
    *   Intermediate cells such as [i,i+2] and [i,j-2] are also shown, connected by bi-directional arrows to their neighbors, with an ellipsis (...) indicating that all cells in this horizontal range ([i, k] for i < k < j) are considered.

2.  **Vertical Dependencies (from cells [k, j]):** Information from cells that start at an intermediate index k (where k is between i+1 and j-1) and end at index j contributes to [i, j].
    *   The diagram explicitly shows [i+1,j] and [j-1,j] as contributors. An arrow originates from [i+1,j] and points directly to [i,j]. Another arrow originates from [j-1,j] and points directly to [i,j].
    *   Intermediate cells such as [i+2,j] and [j-2,j] are also shown, with an implied ellipsis (...) between [i+2,j] and [j-2,j], indicating that all cells in this vertical range ([k, j] for i < k < j) are considered.

In essence, to compute [i, j], the algorithm systematically checks all ways to divide the span [i, j] into two smaller spans, [i, k] and [k, j], utilizing the pre-computed results stored in those respective cells. This bottom-up approach builds up parses for larger segments of the sentence from parses of smaller segments.](images/fe53c3977e441e535f8d920f6ca749a53a2a59cbd3fcff5caef9e06a6a865c64.jpg)
Figure 18.13 All the ways to fill the $[ i , j ]$ th cell in the CKY table.

# 18.6.4 CKY in Practice

Finally, we should note that while the restriction to CNF does not pose a problem theoretically, it does pose some non-trivial problems in practice. The returned CNF trees may not be consistent with the original grammar built by the grammar developers, and will complicate any syntax-driven approach to semantic analysis.

One approach to getting around these problems is to keep enough information around to transform our trees back to the original grammar as a post-processing step of the parse. This is trivial in the case of the transformation used for rules with length greater than 2. Simply deleting the new dummy non-terminals and promoting their daughters restores the original tree.

In the case of unit productions, it turns out to be more convenient to alter the basic CKY algorithm to handle them directly than it is to store the information needed to recover the correct trees. Exercise 18.3 asks you to make this change. Many of the probabilistic parsers presented in Appendix C use the CKY algorithm altered in

![## Image Analysis: d93a0744d91b1362b7b0c5c31da89b5334c3fcc6bec659549c1a64dd9a3047a4.jpg

**Conceptual Understanding:**
This image conceptually illustrates the Cocke-Kasami-Younger (CKY) parsing algorithm, a dynamic programming approach used in computational linguistics to parse context-free grammars. Its main purpose is to visualize the step-by-step process of how a sentence, "Book the flight through Houston," is analyzed to determine its underlying syntactic structure. The key idea being communicated is the iterative, bottom-up construction of grammatical constituents (like Noun Phrases, Prepositional Phrases, and ultimately Sentence structures) by combining categories of smaller, adjacent word spans, thereby demonstrating how a parser can systematically discover all possible grammatical interpretations of a sentence.

**Content Interpretation:**
The image illustrates the Cocke-Kasami-Younger (CKY) parsing algorithm's iterative process of building grammatical constituents for the sentence "Book the flight through Houston." It shows the step-by-step filling of a CKY table, where cells represent spans of words and their possible grammatical categories.

**Processes Shown:**
*   **Lexical Analysis:** The initial state populates diagonal cells (span length 1) with basic part-of-speech tags and non-terminal categories for individual words (e.g., "Book" as S, VP, Verb, Nominal, Noun; "the" as Det; "flight" as NP/Nominal, Noun; "through" as Prep; "Houston" as NP/Nominal, Proper-Noun).
*   **Constituent Formation:** Subsequent states demonstrate the derivation of higher-level grammatical categories for longer spans by combining categories from smaller adjacent spans. Examples include:
    *   Formation of a `PP` (Prepositional Phrase) for "through Houston" in cell `[2,5]` by combining `Prep` from `[2,4]` and `NP, Proper-Noun` from `[3,5]` (Image State 2).
    *   Formation of a `Nominal` for "flight through" in cell `[1,4]` by combining `Nominal, Noun` from `[1,3]` and `Prep` from `[2,4]` (Image State 3).
    *   Formation of an `NP` (Noun Phrase) for "the flight" in cell `[0,3]` by combining `Det` from `[0,2]` and `Nominal, Noun` from `[1,3]` (Image State 4).
*   **Handling Ambiguity and Multiple Derivations:** The final state (Image State 5) particularly highlights cell `[0,3]` which represents the span "Book the flight". This cell shows multiple possible derivations (`S, VP`, `X2`, `S2, VP`, `S3`, `NP`) with complex incoming arrows from other cells (`[0,1]`, `[0,2]`, `[1,3]`, `[0,5]`) and an external annotation (`S1, VP, X2` associated with `[0,4]`). This indicates that the parser is exploring different grammatical interpretations for the span and addressing inherent syntactic ambiguity.

**Significance of Information:**
*   The progression illustrates the dynamic programming principle of CKY, where solutions to smaller subproblems (shorter spans) are stored and reused to solve larger ones.
*   The use of standard grammatical categories (S, VP, NP, Det, Prep, Noun, Nominal) shows the core task of syntactic parsing.
*   The `[i,j]` coordinates precisely define word spans, fundamental to the CKY matrix structure.
*   The complex derivations in cell `[0,3]` in the final state are crucial, demonstrating the algorithm's ability to find all possible parses according to the grammar, thereby revealing the sentence's potential structural ambiguities.

**Key Insights:**
The image offers several key insights into CKY parsing and natural language processing:

*   **Bottom-Up Parsing:** The progressive filling of the table from smaller word spans to larger ones demonstrates the bottom-up nature of the CKY algorithm. This is evident in derivations like `PP` from `Prep` and `NP, Proper-Noun` in `[2,5]` (State 2), `Nominal` from `Nominal, Noun` and `Prep` in `[1,4]` (State 3), and `NP` from `Det` and `Nominal, Noun` in `[0,3]` (State 4).
*   **Dynamic Programming Efficiency:** The structured way the cells are filled, where previously computed categories for shorter spans are utilized to derive categories for longer spans, exemplifies the dynamic programming principle. This avoids recomputing results and ensures efficiency.
*   **Exhaustive Ambiguity Handling:** The complex and multi-categorized cell `[0,3]` in the final state (State 5), showing `S, VP`, `X2`, `S2, VP`, `S3`, and `NP` with numerous incoming arrows, provides strong evidence for CKY's ability to identify and represent all possible grammatical parses for a given span according to the grammar. This is crucial for handling the inherent ambiguity of natural language.
*   **Constituent Structure Identification:** The primary lesson is how the CKY algorithm systematically identifies and builds the hierarchical constituent structure of a sentence, segment by segment, assigning appropriate grammatical labels to each recognized span.

**Document Context:**
This image directly supports Section 18.6.4, "CKY in Practice," by providing a detailed visual example of the CKY algorithm's operation. The accompanying document text, "Figure 18.14 Filling the cells of column 5 after reading the word Houston," contextualizes the diagram as a step-by-step illustration of the parsing process for a specific sentence. It clarifies how the abstract algorithmic steps translate into concrete cell-filling actions within the parsing table, helping readers understand the practical implementation of CKY parsing for natural language sentences.

**Summary:**
This image presents five sequential snapshots of a CKY (Cocke-Kasami-Younger) parsing table as it processes the sentence "Book the flight through Houston." The CKY algorithm is a method for analyzing the grammatical structure of sentences using a dynamic programming approach, building up larger phrases from smaller ones.

Each grid in the diagram represents a CKY table. The words "Book", "the", "flight", "through", and "Houston" are positioned above the columns, corresponding to the words of the sentence. Cells within the table are identified by [row, column] coordinates, indicating the span of words they represent. The contents of these cells are grammatical categories derived from the grammar rules.

The process unfolds in five stages:

1.  **Initial Lexical Assignment (Top-Left Diagram):** The first table shows the initial assignment of basic grammatical categories to individual words (spans of length one). For example, "Book" ([0,1]) is identified as potentially an S (Sentence), VP (Verb Phrase), Verb, Nominal, or Noun. "the" ([0,2]) is a Det (Determiner), "flight" ([0,3]) is an NP (Noun Phrase), "through" ([2,4]) is a Prep (Preposition), and "Houston" ([0,5]) is an NP and Proper-Noun. Other cells representing longer spans are empty at this stage.

2.  **Forming "through Houston" as a Prepositional Phrase (Top-Right Diagram):** The table progresses to show the first derivation. The cell [2,5] is filled with PP (Prepositional Phrase). This PP is explicitly shown to be formed by combining the Prep category from [2,4] ("through") and the NP, Proper-Noun category from [3,5] ("Houston"). This demonstrates how the algorithm uses categories of adjacent shorter spans to form a larger constituent.

3.  **Forming "flight through" as a Nominal (Middle-Left Diagram):** Next, the cell [1,4] is filled with Nominal. This Nominal is derived from combining the Nominal, Noun category in [1,3] ("flight") and the Prep category in [2,4] ("through"). This illustrates how noun phrases can be modified by prepositional phrases.

4.  **Forming "the flight" as a Noun Phrase (Middle-Right Diagram):** The table then shows the cell [0,3] being further developed to contain NP (Noun Phrase). This NP is explicitly derived from the Det category in [0,2] ("the") and the Nominal, Noun category in [1,3] ("flight"). This is a common rule for forming noun phrases.

5.  **Exploring Complex Sentence Structures (Bottom Diagram):** The final table depicts a more advanced state of parsing, particularly focusing on cell [0,3] which represents the span "Book the flight". This cell now shows multiple, more complex categories:
    *   NP (still indicating "the flight" as a noun phrase).
    *   S, VP (derived from [0,1] which contains "Book").
    *   X2 (derived from an external S1, VP, X2 annotation which seems to relate to a longer span including "through Houston").
    *   S2, VP (also derived from [0,1]).
    *   S3 (derived from [0,5] which contains "Houston").
    The numerous arrows pointing into [0,3] from different cells and external labels signify that the algorithm is actively exploring various ways to combine these constituents into larger phrases and potentially a full sentence structure. This highlights the CKY algorithm's capability to identify and represent syntactic ambiguity by providing multiple possible parse options for a given segment of the sentence, ultimately aiming to find a complete sentence parse (S) for "Book the flight through Houston."

The large white arrows between the diagrams visually indicate the progression from one state of the CKY table to the next as more cells are filled and more complex grammatical structures are derived.](images/d93a0744d91b1362b7b0c5c31da89b5334c3fcc6bec659549c1a64dd9a3047a4.jpg)
Figure 18.14 Filling the cells of column 5 after reading the word Houston.

just this manner.

# 18.7 Span-Based Neural Constituency Parsing

While the CKY parsing algorithm we’ve seen so far does great at enumerating all the possible parse trees for a sentence, it has a large problem: it doesn’t tell us which parse is the correct one! That is, it doesn’t disambiguate among the possible parses. To solve the disambiguation problem we’ll use a simple neural extension of the CKY algorithm. The intuition of such parsing algorithms (often called span-based constituency parsing, or neural CKY), is to train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree.

Here we’ll describe a version of the algorithm from Kitaev et al. (2019). This parser learns to map a span of words to a constituent, and, like CKY, hierarchically combines larger and larger spans to build the parse-tree bottom-up. But unlike classic CKY, this parser doesn’t use the hand-written grammar to constrain what constituents can be combined, instead just relying on the learned neural representations of spans to encode likely combinations.

# 18.7.1 Computing Scores for a Span

# span

Let’s begin by considering just the constituent (we’ll call it a span) that lies between fencepost positions $i$ and $j$ with non-terminal symbol label $l$ . We’ll build a system to assign a score $s ( i , j , l )$ to this constituent span.

![## Image Analysis: 2c892980360a030b325af79b47a06e51d67dff38801d9b424bd8ddad4855e8cf.jpg

**Conceptual Understanding:**
This image conceptually represents a computational architecture for natural language parsing, specifically designed to identify and score linguistic spans within a sentence. Its main purpose is to illustrate the process of how raw text (`[START] Book the flight through Houston [END]`) is transformed and analyzed through various neural network layers (`map to subwords`, `ENCODER`, `map back to words`, `postprocessing layers`) to generate span representations (`hⱼ-hᵢ`), compute scores for these spans (`Compute score for span` using `MLP`), and finally integrate these scores into a `CKY for computing best parse` framework to identify grammatical constituents like `NP`. The image conveys the idea of a multi-stage, data-driven approach to syntactic analysis, moving from low-level token processing to high-level structural identification.

**Content Interpretation:**
The image depicts an architecture for natural language processing, specifically focusing on how a neural network computes a 'span score' and utilizes it within a CKY (Cocke–Kasami–Younger) parsing framework. The core process involves taking a sequence of input words, transforming them into subword representations, encoding them using an 'ENCODER', and then mapping them back to words. These processed representations then go through 'postprocessing layers'. A key part of the diagram shows the creation of a 'span representation' denoted as 'hⱼ-hᵢ', where 'hᵢ' and 'hⱼ' correspond to the representations of the i-th and j-th tokens, respectively (specifically `i=1` and `j=3` are highlighted in this example). This span representation is then fed into an 'MLP' (Multilayer Perceptron) to 'Compute score for span'. The output of the MLP contributes to a 'CKY for computing best parse' grid, which is shown as a triangular matrix structure, where 'NP' (Noun Phrase) is indicated in one of the cells, suggesting the identification of a grammatical constituent within a span.

**Key Insights:**
The main takeaways from this image are: 1.  **Hierarchical Processing:** The system processes text through multiple layers, starting from `[START] Book the flight through Houston [END]`, undergoing `map to subwords`, `ENCODER` processing, `map back to words`, and `postprocessing layers`. 2.  **Span Representation:** Spans are represented by combining information from different word positions (`hⱼ-hᵢ` from `i=1` and `j=3`), explicitly linking two points in the sequence. 3.  **Span Scoring:** A `MLP` is used to `Compute score for span`, indicating a learnable component for evaluating spans. 4.  **Integration with CKY Parsing:** The computed span scores are an input to a `CKY for computing best parse` algorithm, which is a standard method for constituency parsing. 5.  **Constituent Identification:** The CKY parse grid ultimately identifies grammatical constituents, such as `NP` (Noun Phrase), demonstrating the goal of the parsing process. These insights are directly supported by the verbatim labels and the flow of arrows between components.

**Document Context:**
This image, "Figure 18.15," is directly relevant to a section discussing natural language processing techniques, particularly parsing and span scoring, as indicated by the surrounding text mentioning "the label NP" and "input word tokens are embedded by." It serves as a visual explanation of the architectural components and data flow involved in computing a span score for a given sequence of words and how this score is integrated into a CKY parsing algorithm to identify grammatical structures like Noun Phrases (NP). The diagram provides a simplified, yet detailed, outline of the computational steps, from word embedding to the final parse, enhancing the reader's understanding of the underlying process.

**Summary:**
The image, titled "Figure 18.15 A simplified outline of computing the span score for the span the flight with the label NP," illustrates a neural network architecture for natural language processing, specifically for computing a span score and identifying noun phrases (NP). The process begins with an input sequence of word tokens. Each token is processed through several layers to eventually derive a span representation that is then used to compute a score, which feeds into a CKY parsing process. The diagram visually explains the flow of data from raw text input to a structured parse representation. Each component in the architecture is clearly labeled, showing the transformation of the input through embedding, encoding, and post-processing, leading to the identification of linguistic spans and their scores. The system processes the input sequence `[START] Book the flight through Houston [END]`, transforming it into subwords, encoding it, mapping it back to words, and then passing it through postprocessing layers. Specific tokens `1` and `3` are highlighted for forming a span `hⱼ-hᵢ` with `i=1` and `j=3`, which is then processed by a Multilayer Perceptron (MLP) to compute a score that contributes to the CKY parsing grid where a span `NP` is identified.](images/2c892980360a030b325af79b47a06e51d67dff38801d9b424bd8ddad4855e8cf.jpg)
the label NP.   
Fig. 18.15 sketches the architecture. The input word tokens are embedded by

passing them through a pretrained language model like BERT. Because BERT operates on the level of subword (wordpiece) tokens rather than words, we’ll first need to convert the BERT outputs to word representations. One standard way of doing this is to simply use the first subword unit as the representation for the entire word; using the last subword unit, or the sum of all the subword units are also common. The embeddings can then be passed through some postprocessing layers; Kitaev et al. (2019), for example, use 8 Transformer layers.

The resulting word encoder outputs $y _ { t }$ are then used to compute a span score. First, we must map the word encodings (indexed by word positions) to span encodings (indexed by fenceposts). We do this by representing each fencepost with two separate values; the intuition is that a span endpoint to the right of a word represents different information than a span endpoint to the left of a word. We convert each word output $y _ { t }$ into a (leftward-pointing) value for spans ending at this fencepost, $\left. \right.$ , and a (rightward-pointing) value $\smash { \vec { y } } _ { t }$ for spans beginning at this fencepost, by splitting $y _ { t }$ into two halves. Each span then stretches from one double-vector fencepost to another, as in the following representation of the flight, which is span $( 1 , 3 )$ :

<table><tr><td>START0</td><td colspan="3">Book</td><td colspan="3">flight</td><td colspan="3">through</td></tr><tr><td></td><td>yo</td><td>y 1</td><td></td><td>the yy</td><td></td><td>③</td><td></td><td>y45 4</td><td></td></tr></table>

A traditional way to represent a span, developed originally for RNN-based models (Wang and Chang, 2016), but extended also to Transformers, is to take the difference between the embeddings of its start and end, i.e., representing span $( i , j )$ by subtracting the embedding of $i$ from the embedding of $j$ . Here we represent a span by concatenating the difference of each of its fencepost components:

$$
\nu ( i , j ) = [ \overrightarrow { y _ { j } } - \overrightarrow { y _ { i } } \ ; \overleftarrow { y _ { j + 1 } } - \overleftarrow { y _ { i + 1 } } ]
$$

The span vector $\nu$ is then passed through an MLP span classifier, with two fullyconnected layers and one ReLU activation function, whose output dimensionality is the number of possible non-terminal labels:

$$
s ( i , j , \cdot ) = \mathsf { W } _ { 2 } \mathrm { R e L U } ( \mathrm { L a y e r N o r m } ( \mathsf { W } _ { 1 } \mathsf { v } ( i , j ) ) )
$$

The MLP then outputs a score for each possible non-terminal.

# 18.7.2 Integrating Span Scores into a Parse

Now we have a score for each labeled constituent span $s ( i , j , l )$ . But we need a score for an entire parse tree. Formally a tree $T$ is represented as a set of $| T |$ such labeled spans, with the $t ^ { \mathrm { { t h } } }$ span starting at position $i _ { t }$ and ending at position $j _ { t }$ , with label $l _ { t }$ :

$$
T = \{ ( i _ { t } , j _ { t } , l _ { t } ) : t = 1 , \ldots , | T | \}
$$

Thus once we have a score for each span, the parser can compute a score for the whole tree $s ( T )$ simply by summing over the scores of its constituent spans:

$$
s ( T ) = \sum _ { ( i , j , l ) \in T } s ( i , j , l )
$$

And we can choose the final parse tree as the tree with the maximum score:

$$
\hat { T } = \underset { T } { \mathrm { a r g m a x } } s ( T )
$$

The simplest method to produce the most likely parse is to greedily choose the highest scoring label for each span. This greedy method is not guaranteed to produce a tree, since the best label for a span might not fit into a complete tree. In practice, however, the greedy method tends to find trees; in their experiments Gaddy et al. (2018) finds that $9 5 \%$ of predicted bracketings form valid trees.

Nonetheless it is more common to use a variant of the CKY algorithm to find the full parse. The variant defined in Gaddy et al. (2018) works as follows. Let’s define $s _ { \mathrm { b e s t } } ( i , j )$ as the score of the best subtree spanning $( i , j )$ . For spans of length one, we choose the best label:

$$
s _ { \mathrm { b e s t } } ( i , i + 1 ) = \operatorname* { m a x } _ { l } s ( i , i + 1 , l )
$$

For other spans $( i , j )$ , the recursion is:

$$
\begin{array} { l } { { s _ { \mathrm { b e s t } } ( i , j ) ~ = ~ \displaystyle { \operatorname* { m a x } _ { l } s ( i , j , l ) } } } \\ { { ~ + ~ \displaystyle { \operatorname* { m a x } _ { k } \bigl [ s _ { \mathrm { b e s t } } ( i , k ) + s _ { \mathrm { b e s t } } ( k , j ) \bigr ] } } } \end{array}
$$

Note that the parser is using the max label for span $( i , j ) +$ the max labels for spans $( i , k )$ and $( k , j )$ without worrying about whether those decisions make sense given a grammar. The role of the grammar in classical parsing is to help constrain possible combinations of constituents (NPs like to be followed by VPs). By contrast, the neural model seems to learn these kinds of contextual constraints during its mapping from spans to non-terminals.

For more details on span-based parsing, including the margin-based training algorithm, see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019).

# 18.8 Evaluating Parsers

# PARSEVAL

The standard tool for evaluating parsers that assign a single parse tree to a sentence is the PARSEVAL metrics (Black et al., 1991). The PARSEVAL metric measures how much the constituents in the hypothesis parse tree look like the constituents in a hand-labeled, reference parse. PARSEVAL thus requires a human-labeled reference (or “gold standard”) parse tree for each sentence in the test set; we generally draw these reference parses from a treebank like the Penn Treebank.

A constituent in a hypothesis parse $C _ { h }$ of a sentence $s$ is labeled correct if there is a constituent in the reference parse $C _ { r }$ with the same starting point, ending point, and non-terminal symbol. We can then measure the precision and recall just as for tasks we’ve seen already like named entity tagging:

labeled recall: $=$ # of correct constituents in hypothesis parse of $s$ # of total constituents in reference parse of $s$ labeled precision: $= \frac { \# \ : \mathrm { c } } { \# }$ f correct constituents in hypothesis parse of of total constituents in hypothesis parse of $s$ $s$

![## Image Analysis: f30fdcbd2f2a6dc1348ad1a5db9700deb2ab67f1c64958504a78cfd3c424f532.jpg

**Conceptual Understanding:**
This image conceptually represents a **lexicalized parse tree**, which is a linguistic construct used in computational linguistics and natural language processing. Its main purpose is to visually and formally depict the syntactic structure of a given sentence, in this case, "workers dumped sacks into a bin." Beyond standard constituency parsing, it explicitly associates a 'head word' with each non-terminal (phrase) node. This lexicalization enhances the tree by embedding crucial semantic information directly into the syntactic structure.

The key ideas communicated are the hierarchical organization of a sentence into constituent phrases (e.g., Noun Phrases, Verb Phrases, Prepositional Phrases), the assignment of Parts of Speech to individual words (e.g., plural noun, past tense verb, preposition, determiner, singular noun), and the importance of lexical heads for capturing deeper dependencies and semantic relationships within the sentence. It illustrates how words group together to form meaningful grammatical units and how the 'most important' word within each group governs its overall function and meaning.

**Content Interpretation:**
The image shows a lexicalized parse tree for the sentence "workers dumped sacks into a bin". It depicts the hierarchical syntactic structure of the sentence, breaking it down into constituents such as Sentence (S), Noun Phrase (NP), Verb Phrase (VP), and Prepositional Phrase (PP), along with their respective parts of speech (NNS, VBD, P, DT, NN) and individual lexical items. Crucially, each non-terminal node (phrase) is associated with its headword in parentheses, indicating the most significant word within that constituent.

The parsing flow begins with the entire sentence, S(dumped), which then splits into the subject Noun Phrase, NP(workers), and the predicate Verb Phrase, VP(dumped). NP(workers) decomposes into the plural noun NNS(workers) and finally the word "workers". VP(dumped) further breaks into the main verb VBD(dumped) ("dumped"), the object Noun Phrase NP(sacks) (headed by "sacks", a plural noun), and the Prepositional Phrase PP(into). PP(into) consists of the preposition P ("into") and its object Noun Phrase NP(bin). NP(bin) is then detailed as the determiner DT(a) ("a") and the singular noun NN(bin) ("bin").

**Key Insights:**
The main takeaways from this image are:
1.  **Hierarchical Sentence Structure**: Sentences are organized hierarchically, with words forming phrases that combine into larger syntactic units, as demonstrated by the step-by-step breakdown from S(dumped) to individual words. For example, S(dumped) -> NP(workers) + VP(dumped).
2.  **Parts of Speech and Syntactic Categories**: Each word is assigned a specific part of speech (e.g., NNS for plural noun, VBD for past tense verb, P for preposition, DT for determiner, NN for singular noun), and these form higher-level syntactic categories (e.g., NP for Noun Phrase, VP for Verb Phrase, PP for Prepositional Phrase). This is evident in nodes like NNS(workers) for "workers" and VBD(dumped) for "dumped".
3.  **Lexicalized Parsing for Enriched Information**: The inclusion of a headword (e.g., "(workers)", "(dumped)", "(sacks)", "(into)", "(bin)") at each non-terminal node provides crucial lexical information, which is more robust for dependency tracking and disambiguation in parsing. For instance, NP(bin) specifies that "bin" is the semantic head of that noun phrase, which is more informative than just the NP category.
4.  **Complete Sentence Representation**: The tree comprehensively illustrates the full grammatical structure of "workers dumped sacks into a bin", showing how each word contributes to the overall syntax and meaning. These insights highlight the value of lexicalized parse trees in natural language processing for deeper linguistic analysis and as a foundation for parser evaluation.

**Document Context:**
This image, labeled "Figure 18.16 A lexicalized tree from Collins (1999)," directly relates to the document's "PARSEVAL" section. PARSEVAL is a standard evaluation methodology for assessing the quality of parse trees generated by computational linguistic parsers. As such, this figure serves as a concrete example of the type of detailed syntactic and lexicalized output that a sophisticated parser, such as the one developed by Collins, would produce. It provides a visual illustration of the data structure whose accuracy and completeness would be measured using the PARSEVAL metrics, demonstrating the target output against which parsing algorithms are benchmarked.

**Summary:**
This image displays a lexicalized parse tree, which is a hierarchical diagram illustrating the grammatical structure of the English sentence "workers dumped sacks into a bin".

At the very top, the entire sentence is represented by the **root node S(dumped)**. The "S" stands for Sentence, and "(dumped)" indicates that "dumped" is considered the main, or "head," word of the entire sentence.

This sentence S(dumped) then branches into two main parts:
1. On the left, there's **NP(workers)**, representing the Noun Phrase that acts as the subject. "NP" stands for Noun Phrase, and "(workers)" signifies "workers" as its headword. This NP(workers) further breaks down into NNS(workers) (Plural Noun), which finally leads to the word "workers" itself.
2. On the right, there's **VP(dumped)**, representing the Verb Phrase, headed by the word "dumped". This VP(dumped) expands into three components:
    *   **VBD(dumped)**: This is the main verb of the sentence, a Past Tense Verb ("VBD"). It directly leads to the word "dumped".
    *   **NP(sacks)**: This is another Noun Phrase, acting as the direct object of the verb. It's headed by "sacks". This NP(sacks) contains NNS(sacks) (Plural Noun), which then leads to the word "sacks".
    *   **PP(into)**: This is a Prepositional Phrase, headed by "into", indicating where the sacks were dumped. This PP(into) splits into two parts:
        *   **P**: A Preposition, which directly leads to the word "into".
        *   **NP(bin)**: The Noun Phrase within the prepositional phrase, headed by "bin". This NP(bin) further divides into two words:
            *   **DT(a)**: A Determiner, which leads to the word "a".
            *   **NN(bin)**: A Singular Noun, which leads to the word "bin".

In essence, this diagram shows how the sentence is built up from individual words, through their parts of speech, into larger grammatical phrases, all the way to a complete sentence. The parenthetical words at each higher-level node (like (dumped), (workers), (sacks), (into), (bin)) are called "headwords," which are crucial for understanding the core meaning and dependencies within each phrase and the sentence as a whole. This type of detailed structural representation is fundamental in natural language processing for analyzing sentence syntax and is often used as a benchmark for evaluating the performance of language parsing systems.](images/f30fdcbd2f2a6dc1348ad1a5db9700deb2ab67f1c64958504a78cfd3c424f532.jpg)
Figure 18.16 A lexicalized tree from Collins (1999).

As usual, we often report a combination of the two, $\mathrm { F } _ { 1 }$ :

$$
\displaystyle \mathrm { F } _ { 1 } = \frac { 2 P R } { P + R }
$$

We additionally use a new metric, crossing brackets, for each sentence $s$ :

cross-brackets: the number of constituents for which the reference parse has a bracketing such as ((A B) C) but the hypothesis parse has a bracketing such as (A (B C)).

For comparing parsers that use different grammars, the PARSEVAL metric includes a canonicalization algorithm for removing information likely to be grammarspecific (auxiliaries, pre-infinitival “to”, etc.) and for computing a simplified score (Black et al., 1991). The canonical implementation of the PARSEVAL metrics is called evalb (Sekine and Collins, 1997).

evalb

# 18.9 Heads and Head-Finding

Syntactic constituents can be associated with a lexical head; $N$ is the head of an $N P$ , $V$ is the head of a $V P$ . This idea of a head for each constituent dates back to Bloomfield 1914, and is central to the dependency grammars and dependency parsing we’ll introduce in Chapter 19. Indeed, heads can be used as a way to map between constituency and dependency parses. Heads are also important in probabilistic parsing (Appendix C) and in constituent-based grammar formalisms like Head-Driven Phrase Structure Grammar (Pollard and Sag, 1994)..

In one simple model of lexical heads, each context-free rule is associated with a head (Charniak 1997, Collins 1999). The head is the word in the phrase that is grammatically the most important. Heads are passed up the parse tree; thus, each non-terminal in a parse tree is annotated with a single word, which is its lexical head. Figure 18.16 shows an example of such a tree from Collins (1999), in which each non-terminal is annotated with its head.

For the generation of such a tree, each CFG rule must be augmented to identify one right-side constituent to be the head child. The headword for a node is then set to the headword of its head child. Choosing these head children is simple for textbook examples (NN is the head of $N P _ { . }$ ) but is complicated and indeed controversial for most phrases. (Should the complementizer to or the verb be the head of an infinite verb phrase?) Modern linguistic theories of syntax generally include a component that defines heads (see, e.g., (Pollard and Sag, 1994)).

An alternative approach to finding a head is used in most practical computational systems. Instead of specifying head rules in the grammar itself, heads are identified dynamically in the context of trees for specific sentences. In other words, once a sentence is parsed, the resulting tree is walked to decorate each node with the appropriate head. Most current systems rely on a simple set of handwritten rules, such as a practical one for Penn Treebank grammars given in Collins (1999) but developed originally by Magerman (1995). For example, the rule for finding the head of an $N P$ is as follows (Collins, 1999, p. 238):

• If the last word is tagged POS, return last-word.   
• Else search from right to left for the first child which is an NN, NNP, NNPS, NX, POS, or JJR.   
• Else search from left to right for the first child which is an NP.   
• Else search from right to left for the first child which is a $\$ 1$ , ADJP, or PRN.   
• Else search from right to left for the first child which is a CD.   
• Else search from right to left for the first child which is a JJ, JJS, RB or QP.   
• Else return the last word

Selected other rules from this set are shown in Fig. 18.17. For example, for $V P$ rules of the form $V P  Y _ { 1 } \cdots Y _ { n }$ , the algorithm would start from the left of $Y _ { 1 } \cdots$ $Y _ { n }$ looking for the first $Y _ { i }$ of type TO; if no TOs are found, it would search for the first $Y _ { i }$ of type VBD; if no VBDs are found, it would search for a VBN, and so on. See Collins (1999) for more details.

<table><tr><td>Parent</td><td>Direction</td><td>PriorityList</td></tr><tr><td>ADJP</td><td>Left</td><td>NNS QP NN $ ADVP JJ VBN VBG ADJP JJR NP JJS DT FW RBR RBS SBAR RB</td></tr><tr><td>ADVP PRN</td><td>Right Left</td><td>RB RBR RBS FW ADVP TO CD JJR JJ IN NP JJS NN</td></tr><tr><td> PRT</td><td>Right</td><td>RP</td></tr><tr><td>QP</td><td>Left</td><td> $ IN NNS NN JJ RB DT CD NCD QP JJR JJS</td></tr><tr><td>S</td><td>Left</td><td>TO IN VP S SBAR ADJP UCP NP</td></tr><tr><td> SBAR</td><td>Left</td><td>WHNP WHPP WHADVP WHADJP IN DT S SQ SINV SBAR FRAG</td></tr><tr><td>VP</td><td>Left</td><td> TO VBD VBN MD VBZ VB VBG VBP VP ADJP NN NNS NP</td></tr></table>

Figure 18.17 Some head rules from Collins (1999). The head rules are also called a head percolation table.

# 18.10 Summary

This chapter introduced constituency parsing. Here’s a summary of the main points:

• In many languages, groups of consecutive words act as a group or a constituent, which can be modeled by context-free grammars (which are also known as phrase-structure grammars).   
• A context-free grammar consists of a set of rules or productions, expressed over a set of non-terminal symbols and a set of terminal symbols. Formally, a particular context-free language is the set of strings that can be derived from a particular context-free grammar.   
• Structural ambiguity is a significant problem for parsers. Common sources of structural ambiguity include PP-attachment and coordination ambiguity.   
• Dynamic programming parsing algorithms, such as CKY, use a table of partial parses to efficiently parse ambiguous sentences.   
• CKY restricts the form of the grammar to Chomsky normal form (CNF).   
• The basic CKY algorithm compactly represents all possible parses of the sentence but doesn’t choose a single best parse.   
• Choosing a single parse from all possible parses (disambiguation) can be done by neural constituency parsers.   
• Span-based neural constituency parses train a neural classifier to assign a score to each constituent, and then use a modified version of CKY to combine these constituent scores to find the best-scoring parse tree.   
• Parsers are evaluated with three metrics: labeled recall, labeled precision, and cross-brackets.   
• Partial parsing and chunking are methods for identifying shallow syntactic constituents in a text. They are solved by sequence models trained on syntactically-annotated data.

# Bibliographical and Historical Notes

According to Percival (1976), the idea of breaking up a sentence into a hierarchy of constituents appeared in the Volkerpsychologie ¨ of the groundbreaking psychologist Wilhelm Wundt (Wundt, 1900):

...den sprachlichen Ausdruck fur die willk ¨ urliche Gliederung einer Ge- ¨ sammtvorstellung in ihre in logische Beziehung zueinander gesetzten Bestandteile

[the linguistic expression for the arbitrary division of a total idea into its constituent parts placed in logical relations to one another]

Wundt’s idea of constituency was taken up into linguistics by Leonard Bloomfield in his early book An Introduction to the Study of Language (Bloomfield, 1914). By the time of his later book, Language (Bloomfield, 1933), what was then called “immediate-constituent analysis” was a well-established method of syntactic study in the United States. By contrast, traditional European grammar, dating from the Classical period, defined relations between words rather than constituents, and European syntacticians retained this emphasis on such dependency grammars, the subject of Chapter 19. (And indeed, both dependency and constituency grammars have been in vogue in computational linguistics at different times).

American Structuralism saw a number of specific definitions of the immediate constituent, couched in terms of their search for a “discovery procedure”: a methodological algorithm for describing the syntax of a language. In general, these attempt to capture the intuition that “The primary criterion of the immediate constituent is the degree in which combinations behave as simple units” (Bazell, 1952/1966, p. 284). The most well known of the specific definitions is Harris’ idea of distributional similarity to individual units, with the substitutability test. Essentially, the method proceeded by breaking up a construction into constituents by attempting to substitute simple structures for possible constituents—if a substitution of a simple form, say, man, was substitutable in a construction for a more complex set (like intense young man), then the form intense young man was probably a constituent. Harris’s test was the beginning of the intuition that a constituent is a kind of equivalence class.

The context-free grammar was a formalization of this idea of hierarchical constituency defined in Chomsky (1956) and further expanded upon (and argued against) in Chomsky (1957) and Chomsky (1956/1975). Shortly after Chomsky’s initial work, the context-free grammar was reinvented by Backus (1959) and independently by Naur et al. (1960) in their descriptions of the ALGOL programming language; Backus (1996) noted that he was influenced by the productions of Emil Post and that Naur’s work was independent of his (Backus’) own. After this early work, a great number of computational models of natural language processing were based on context-free grammars because of the early development of efficient parsing algorithms.

# WFST

Dynamic programming parsing has a history of independent discovery. According to the late Martin Kay (personal communication), a dynamic programming parser containing the roots of the CKY algorithm was first implemented by John Cocke in 1960. Later work extended and formalized the algorithm, as well as proving its time complexity (Kay 1967, Younger 1967, Kasami 1965). The related wellformed substring table (WFST) seems to have been independently proposed by Kuno (1965) as a data structure that stores the results of all previous computations in the course of the parse. Based on a generalization of Cocke’s work, a similar data structure had been independently described in Kay (1967) (and Kay 1973). The top-down application of dynamic programming to parsing was described in Earley’s Ph.D. dissertation (Earley 1968, Earley 1970). Sheil (1976) showed the equivalence of the WFST and the Earley algorithm. Norvig (1991) shows that the efficiency offered by dynamic programming can be captured in any language with a memoization function (such as in LISP) simply by wrapping the memoization operation around a simple top-down parser.

The earliest disambiguation algorithms for parsing were based on probabilistic context-free grammars, first worked out by Booth (1969) and Salomaa (1969); see Appendix C for more history. Neural methods were first applied to parsing at around the same time as statistical parsing methods were developed (Henderson, 1994). In the earliest work neural networks were used to estimate some of the probabilities for statistical constituency parsers (Henderson, 2003, 2004; Emami and Jelinek, 2005) . The next decades saw a wide variety of neural parsing algorithms, including recursive neural architectures (Socher et al., 2011, 2013), encoder-decoder models (Vinyals et al., 2015; Choe and Charniak, 2016), and the idea of focusing on spans (Cross and Huang, 2016). For more on the span-based self-attention approach we describe in this chapter see Stern et al. (2017), Gaddy et al. (2018), Kitaev and Klein (2018), and Kitaev et al. (2019). See Chapter 20 for the parallel history of neural dependency parsing.

The classic reference for parsing algorithms is Aho and Ullman (1972); although the focus of that book is on computer languages, most of the algorithms have been applied to natural language.

# Exercises

18.1 Implement the algorithm to convert arbitrary context-free grammars to CNF.

Apply your program to the $\mathcal { L } _ { 1 }$ grammar.

18.2 Implement the CKY algorithm and test it with your converted $\mathcal { L } _ { 1 }$ grammar.   
18.3 Rewrite the CKY algorithm given in Fig. 18.12 on page 400 so that it can accept grammars that contain unit productions.   
18.4 Discuss how to augment a parser to deal with input that may be incorrect, for example, containing spelling errors or mistakes arising from automatic speech recognition.   
18.5 Implement the PARSEVAL metrics described in Section 18.8. Next, use a parser and a treebank, compare your metrics against a standard implementation. Analyze the errors in your approach.

# CHAPTER 19 Dependency Parsing

Tout mot qui fait partie d’une phrase... Entre lui et ses voisins, l’esprit aperc¸oit des connexions, dont l’ensemble forme la charpente de la phrase.

[Between each word in a sentence and its neighbors, the mind perceives connections. These connections together form the scaffolding of the sentence.] Lucien Tesniere. 1959. \` El ´ ements de syntaxe structurale, A.1.§4 ´

# dependency grammars

The focus of the last chapter was on context-free grammars and constituentbased representations. Here we present another important family of grammar formalisms called dependency grammars. In dependency formalisms, phrasal constituents and phrase-structure rules do not play a direct role. Instead, the syntactic structure of a sentence is described solely in terms of directed binary grammatical relations between the words, as in the following dependency parse:

![## Image Analysis: d23acb58d2835ed0b710f1456067d8799ead7d51a85e0487db0cdeaf7dbab26e.jpg

**Conceptual Understanding:**
This image conceptually represents a dependency grammar parse tree for the English sentence "I prefer the morning flight through Denver." Its main purpose is to visually illustrate the syntactic structure of this sentence by mapping the grammatical relationships between its words. It conveys the idea that words in a sentence are connected through directed, labeled arcs, where each arc represents a specific type of dependency from a head word to a dependent word. The core concept being communicated is how dependency grammar analyzes and represents sentence structure by identifying the central 'root' word and the various grammatical roles (e.g., subject, object, modifiers) that other words play relative to their heads.

**Content Interpretation:**
The image illustrates the syntactic analysis of the sentence "I prefer the morning flight through Denver" using a dependency parse tree. This type of diagram shows directed grammatical relationships between words, where each arrow points from a head word to its dependent. The labels on the arcs define the nature of these dependencies. The central verb "prefer" is identified as the root of the sentence, acting as the ultimate head from which all other words are directly or indirectly dependent. The diagram explicitly shows the subject-verb relationship ("I" as nominal subject of "prefer"), the verb-object relationship ("flight" as object of "prefer"), determiners and compound modifiers of the noun ("the" and "morning" for "flight"), and how prepositional phrases are structured, with "through" modifying "flight" and "Denver" being the argument of "through" via a 'case' relationship.

**Key Insights:**
The image provides several key insights into dependency grammar: 1.  **Head-Dependent Relationships:** It clearly shows how words like 'I', 'flight', 'the', 'morning', 'through', and 'Denver' are dependents of a specific head word ('prefer', 'flight', 'through'). 2.  **Root Identification:** The diagram explicitly identifies 'prefer' as the 'root' of the sentence, highlighting its central role in the syntactic structure. 3.  **Typed Dependencies:** Each connection is labeled with a specific grammatical function (e.g., 'nsubj', 'obj', 'det', 'compound', 'nmod', 'case'), illustrating the rich semantic information embedded in dependency relations. 4.  **Prepositional Phrase Analysis:** It demonstrates how prepositional phrases are parsed, with 'through' acting as an 'nmod' of 'flight' and governing 'Denver' via a 'case' relation. These explicit labels and connections, extracted verbatim from the diagram, provide direct evidence for understanding the fine-grained syntactic structure of the sentence according to dependency grammar principles.

**Document Context:**
In the context of dependency grammars, this image serves as a practical example demonstrating the principles of dependency parsing. It visually articulates how words in a sentence relate to each other syntactically through head-dependent relationships, a core concept in dependency grammar. This particular example showcases several common dependency types (subject, object, determiner, compound modifier, nominal modifier, case), thereby illustrating the depth and detail dependency grammar provides in analyzing sentence structure. It helps readers understand how a concrete sentence is broken down into its fundamental grammatical components according to this linguistic theory, reinforcing any theoretical discussions about dependency grammar in the document.

**Summary:**
This image displays a dependency grammar parse tree for the sentence "I prefer the morning flight through Denver." It visually represents the syntactic relationships between the words, where arrows indicate a dependency from a head word to a dependent word, and labels on the arrows specify the type of grammatical relationship. The word "prefer" is identified as the central element, or root, of the sentence. "I" acts as the nominal subject (nsubj) of "prefer," indicating the performer of the action. "flight" is the direct object (obj) of "prefer," signifying what is preferred. "the" functions as a determiner (det) for "flight." "morning" is a compound modifier (compound) for "flight," specifying the type of flight. "through" modifies "flight" as a nominal modifier (nmod), introducing a prepositional phrase. Finally, "Denver" is related to "through" by a case dependency (case), with "through" acting as the preposition that governs "Denver."](images/d23acb58d2835ed0b710f1456067d8799ead7d51a85e0487db0cdeaf7dbab26e.jpg)

typed dependency

Relations among the words are illustrated above the sentence with directed, labeled arcs from heads to dependents. We call this a typed dependency structure because the labels are drawn from a fixed inventory of grammatical relations. A root node explicitly marks the root of the tree, the head of the entire structure.

Figure 19.1 on the next page shows the dependency analysis from Eq. 19.1 but visualized as a tree, alongside its corresponding phrase-structure analysis of the kind given in the prior chapter. Note the absence of nodes corresponding to phrasal constituents or lexical categories in the dependency parse; the internal structure of the dependency parse consists solely of directed relations between words. These headdependent relationships directly encode important information that is often buried in the more complex phrase-structure parses. For example, the arguments to the verb prefer are directly linked to it in the dependency structure, while their connection to the main verb is more distant in the phrase-structure tree. Similarly, morning and Denver, modifiers of flight, are linked to it directly in the dependency structure. This fact that the head-dependent relations are a good proxy for the semantic relationship between predicates and their arguments is an important reason why dependency grammars are currently more common than constituency grammars in natural language processing.

Another major advantage of dependency grammars is their ability to deal with languages that have a relatively free word order. For example, word order in Czech can be much more flexible than in English; a grammatical object might occur before or after a location adverbial. A phrase-structure grammar would need a separate rule for each possible place in the parse tree where such an adverbial phrase could occur. A dependency-based approach can have just one link type representing this particular adverbial relation; dependency grammar approaches can thus abstract away a bit more from word order information.

![## Image Analysis: a79ce3bee242a2d050567827183d2672e5445700a0764052023623d92fcdc97e.jpg

**Conceptual Understanding:**
The image represents two distinct linguistic parsing frameworks used to analyze the syntactic structure of the English sentence "I prefer the morning flight through Denver." Conceptually, the left diagram illustrates a 'dependency grammar' analysis, which focuses on directed links (dependencies) between words, where one word (the head) governs another (its dependent). The right diagram illustrates a 'constituent grammar' or 'phrase-structure grammar' analysis, which focuses on how words group together into phrases (constituents) that form a hierarchical structure. The main purpose is to demonstrate and contrast these two fundamental approaches to syntactic analysis using a concrete example sentence, highlighting their different ways of representing grammatical relationships and sentence structure. It communicates the key idea that the same sentence can be analyzed and represented in structurally different but equally valid ways depending on the theoretical framework applied.

**Content Interpretation:**
The image displays two different approaches to parsing sentence structure: dependency grammar and constituent grammar (phrase-structure grammar). The dependency tree (left) illustrates direct head-dependent relationships between individual words. For the sentence "I prefer the morning flight through Denver," "prefer" is the main verb and the head of the sentence. "I" is a dependent of "prefer." "Flight" is also a dependent of "prefer." "The," "morning," and "Denver" are dependents of "flight." Crucially, "through" is a dependent of "Denver," showing that the prepositional phrase "through Denver" modifies "Denver" directly within the noun phrase headed by "flight." This type of analysis focuses on the lexical relationships. The constituent tree (right) shows how words group into phrases, which then combine to form larger phrases, ultimately forming the sentence. Here, "S" (Sentence) is the root, composed of an "NP" (Noun Phrase - "I") and a "VP" (Verb Phrase - "prefer the morning flight through Denver"). The VP contains the verb "prefer" and another "NP" ("the morning flight through Denver"). This NP breaks down into a "Det" ("the") and a "Nom" ("morning flight through Denver"). The "Nom" further divides into "Nom" ("morning flight") and "PP" ("through Denver"). The "PP" then contains "P" ("through") and "NP" ("Denver"). This analysis emphasizes hierarchical constituency and phrase formation rather than direct word-to-word dependencies.

**Key Insights:**
The main takeaway is the fundamental difference in how dependency grammar and constituent grammar represent sentence structure. Dependency grammar (left tree) focuses on direct, binary, head-dependent relationships between words, where each word (except the root) is linked to exactly one head, as evidenced by "prefer" being the head for "I" and "flight," and "flight" for "the," "morning," and "Denver," with "through" depending on "Denver." This highlights the direct syntactic connections between lexical items. Constituent grammar (right tree) focuses on hierarchical grouping of words into phrases (constituents) and then into larger phrases, represented by labels like 'NP', 'VP', 'Nom', 'PP', and parts of speech 'Pro', 'Verb', 'Det', 'Noun', 'P'. This emphasizes the 'parts' that make up the sentence and their nested structure. For example, the right tree shows 'the morning flight through Denver' as a single NP, which is then broken down, while the left tree shows direct dependencies. The image visually clarifies that dependency analysis results in a flatter structure with direct links, while constituent analysis results in a deeper, more nested tree of phrases.

**Document Context:**
This image directly supports a discussion on dependency grammars by providing a visual comparison with constituent analysis for the same sentence. The document context indicates the section is titled 'dependency grammars,' making this figure a core illustration for understanding the differences and characteristics of dependency analysis in contrast to the more commonly known constituent analysis. It serves to clarify the conceptual distinctions between these two major linguistic parsing frameworks for readers. The example sentence "I prefer the morning flight through Denver" is used to concretely demonstrate how each grammatical theory represents the underlying syntactic structure.

**Summary:**
The image presents two distinct linguistic analyses for the sentence "I prefer the morning flight through Denver." The left-hand diagram illustrates a dependency grammar analysis, where words are directly linked to each other based on head-dependent relationships. The word "prefer" acts as the central head, with "I" and "flight" as its direct dependents. "Flight" further serves as a head for "the," "morning," and "Denver," while "Denver" is the head for "through." This structure emphasizes the direct syntactic relationships between words. The right-hand diagram displays a constituent analysis, also known as a phrase-structure tree. This analysis segments the sentence into hierarchical constituents or phrases. The sentence (S) branches into a Noun Phrase (NP) and a Verb Phrase (VP). The NP consists of a Pronoun (Pro) "I." The VP comprises a Verb "prefer" and another NP. This nested NP is further broken down into a Determiner (Det) "the" and a Nominal (Nom). This Nominal then splits into another Nominal, which contains a Noun "morning," and a Prepositional Phrase (PP). The PP consists of a Preposition (P) "through" and a final Noun Phrase (NP), which contains a Pronoun (Pro) "Denver." This analysis highlights the grouping of words into phrases that act as single units.](images/a79ce3bee242a2d050567827183d2672e5445700a0764052023623d92fcdc97e.jpg)
Figure 19.1 Dependency and constituent analyses for I prefer the morning flight through Denver.

In the following sections, we’ll give an inventory of relations used in dependency parsing, discuss two families of parsing algorithms (transition-based, and graphbased), and discuss evaluation.

# 19.1 Dependency Relations

# grammatical relation

head dependent

# grammatical function

The traditional linguistic notion of grammatical relation provides the basis for the binary relations that comprise these dependency structures. The arguments to these relations consist of a head and a dependent. The head plays the role of the central organizing word, and the dependent as a kind of modifier. The head-dependent relationship is made explicit by directly linking heads to the words that are immediately dependent on them.

In addition to specifying the head-dependent pairs, dependency grammars allow us to classify the kinds of grammatical relations, or grammatical function that the dependent plays with respect to its head. These include familiar notions such as subject, direct object and indirect object. In English these notions strongly correlate with, but by no means determine, both position in a sentence and constituent type and are therefore somewhat redundant with the kind of information found in phrase-structure trees. However, in languages with more flexible word order, the information encoded directly in these grammatical relations is critical since phrasebased constituent syntax provides little help.

Linguists have developed taxonomies of relations that go well beyond the familiar notions of subject and object. While there is considerable variation from theory to theory, there is enough commonality that cross-linguistic standards have been developed. The Universal Dependencies (UD) project (de Marneffe et al., 2021), an open community effort to annotate dependencies and other aspects of grammar across more than 100 languages, provides an inventory of 37 dependency relations. Fig. 19.2 shows a subset of the UD relations and Fig. 19.3 provides some examples.

<table><tr><td>Clausal Argument Relations Description</td><td></td></tr><tr><td>NSUBJ</td><td>Nominal subject</td></tr><tr><td>OBJ</td><td>Direct object</td></tr><tr><td>IOBJ</td><td> Indirect object</td></tr><tr><td>CCOMP</td><td>Clausal complement</td></tr><tr><td>Nominal Modifier Relations</td><td>Description</td></tr><tr><td>NMOD</td><td>Nominal modifier</td></tr><tr><td>AMOD</td><td>Adjectival modifier</td></tr><tr><td>APPOS</td><td>Appositional modifier</td></tr><tr><td>DET</td><td>Determiner</td></tr><tr><td>CASE</td><td> Prepositions, postpositions and other case markers</td></tr><tr><td>Other Notable Relations</td><td>Description</td></tr><tr><td>CONJ</td><td>Conjunct</td></tr><tr><td>CC</td><td>Coordinating conjunction</td></tr></table>

Figure 19.2 Some of the Universal Dependency relations (de Marneffe et al., 2021).

The motivation for all of the relations in the Universal Dependency scheme is beyond the scope of this chapter, but the core set of frequently used relations can be broken into two sets: clausal relations that describe syntactic roles with respect to a predicate (often a verb), and modifier relations that categorize the ways that words can modify their heads.

Consider, for example, the following sentence:

![## Image Analysis: 0ffd405ccd76755861641a6118932f5b13cbf0f3237dad7c3a0360bff2cba789.jpg

**Conceptual Understanding:**
The image conceptually represents the syntactic structure of the English sentence "United canceled the morning flights to Houston" using a dependency parse tree. The main purpose is to illustrate the grammatical functions and relationships between the individual words in the sentence, showing which words modify or depend on others and what role they play in the overall sentence structure.

**Content Interpretation:**
The image is a dependency parse tree that visually represents the grammatical relationships between words in the sentence "United canceled the morning flights to Houston." It identifies the head of the sentence and the specific types of dependencies that link other words to their governors. For instance, "nsubj" indicates a nominal subject, "obj" indicates a direct object, "det" indicates a determiner, "compound" indicates a compound modifier, "nmod" indicates a nominal modifier, and "case" indicates a case marker.

**Key Insights:**
The main takeaway is an understanding of how dependency parsing works to reveal the grammatical structure of a sentence. Specific insights include: the identification of the verb "canceled" as the sentence's 'root', "United" as the 'nsubj' of "canceled," "flights" as the 'obj' of "canceled," "the" as the 'det' of "flights," "morning" as a 'compound' modifier of "flights," "Houston" as an 'nmod' of "flights," and "to" as the 'case' of "Houston." This demonstrates how each word plays a specific, labeled grammatical role relative to another word in the sentence.

**Document Context:**
This image is directly relevant to a section on "grammatical function" as it provides a concrete example of how grammatical functions (like subject, object, determiner, modifier) are identified and represented in a dependency grammar framework. It serves to illustrate the syntactic structure of a sentence by breaking it down into its constituent parts and their functional relationships, which is a core concept in the study of grammar.

**Summary:**
This image displays a dependency parse tree for the sentence "United canceled the morning flights to Houston," illustrating the grammatical relationships between the words. The central action, "canceled," is identified as the root of the sentence. "United" is the nominal subject, performing the action of canceling. The direct object of the cancellation is "flights." Further details about "flights" are provided: "the" acts as a determiner, and "morning" functions as a compound modifier. The destination "Houston" is an indirect object, introduced by the case marker "to," forming a nominal modifier describing "flights." The diagram clearly maps out the hierarchical and functional connections, making the syntactic structure of the sentence visually explicit and easy to understand for readers studying grammatical functions.](images/0ffd405ccd76755861641a6118932f5b13cbf0f3237dad7c3a0360bff2cba789.jpg)

Here the clausal relations NSUBJ and OBJ identify the subject and direct object of the predicate cancel, while the NMOD, DET, and CASE relations denote modifiers of the nouns flights and Houston.

# 19.1.1 Dependency Formalisms

A dependency structure can be represented as a directed graph $G = \left( V , A \right)$ , consisting of a set of vertices $V$ , and a set of ordered pairs of vertices $A$ , which we’ll call arcs.

For the most part we will assume that the set of vertices, $V$ , corresponds exactly to the set of words in a given sentence. However, they might also correspond to punctuation, or when dealing with morphologically complex languages the set of vertices might consist of stems and affixes. The set of arcs, $A$ , captures the headdependent and grammatical function relationships between the elements in $V$ .

Different grammatical theories or formalisms may place further constraints on these dependency structures. Among the more frequent restrictions are that the structures must be connected, have a designated root node, and be acyclic or planar. Of most relevance to the parsing approaches discussed in this chapter is the common,

<table><tr><td>Relation</td><td>Examples with head and dependent</td></tr><tr><td>NSUBJ</td><td>United canceled the flight.</td></tr><tr><td>OBJ</td><td>United diverted the flight to Reno. We booked her the first flight to Miami.</td></tr><tr><td>IOBJ</td><td>We booked her the flight to Miami.</td></tr><tr><td>COMPOUND</td><td>We took the morning flight.</td></tr><tr><td>NMOD</td><td>flight to Houston.</td></tr><tr><td>AMOD</td><td>Book the cheapest flight.</td></tr><tr><td>APPOS</td><td> United, a unit of UAL, matched the fares.</td></tr><tr><td>DET</td><td>The flight was canceled.</td></tr><tr><td>CONJ</td><td>Which flight was delayed?</td></tr><tr><td>CC</td><td>We flew to Denver and drove to Steamboat. We flew to Denver and drove to Steamboat.</td></tr><tr><td>CASE</td><td>Book the flight through Houston.</td></tr></table>

Figure 19.3 Examples of some Universal Dependency relations.

dependency tree

computationally-motivated, restriction to rooted trees. That is, a dependency tree is a directed graph that satisfies the following constraints:

1. There is a single designated root node that has no incoming arcs.   
2. With the exception of the root node, each vertex has exactly one incoming arc.   
3. There is a unique path from the root node to each vertex in $V$ .

Taken together, these constraints ensure that each word has a single head, that the dependency structure is connected, and that there is a single root node from which one can follow a unique directed path to each of the words in the sentence.

# 19.1.2 Projectivity

The notion of projectivity imposes an additional constraint that is derived from the order of the words in the input. An arc from a head to a dependent is said to be projective if there is a path from the head to every word that lies between the head and the dependent in the sentence. A dependency tree is then said to be projective if all the arcs that make it up are projective. All the dependency trees we’ve seen thus far have been projective. There are, however, many valid constructions which lead to non-projective trees, particularly in languages with relatively flexible word order.

Consider the following example.

![## Image Analysis: daa9e66fbe3e1e99b5a559b1537fbc8c6588e1a4871acce768a2f104c7d8ef3d.jpg

**Conceptual Understanding:**
This image conceptually represents a **dependency parse tree** for a segment of a sentence. It illustrates the grammatical relationships and hierarchical structure between words (or constituents) in a sentence, where each word (except the root) is linked to another word by a directed edge, indicating a dependency relationship.

The main purpose of this image is to visually demonstrate how different syntactic roles (like subject, object, determiner, oblique, relative clause, copula, adverb) relate to each other within a sentence's grammatical structure. It communicates the key idea that sentence structure can be analyzed by identifying head-dependent relationships, rather than constituent phrases.

**Content Interpretation:**
The image displays a syntactic dependency structure, specifically highlighting various types of grammatical relationships. The processes being shown are those of dependency parsing, where words are analyzed for their grammatical functions relative to other words.

The significance of the information presented lies in understanding how individual words or word groups contribute to the overall meaning and structure of a sentence. Each label represents a specific type of grammatical dependency:

*   **`root`**: This indicates the main predicate or the highest head of the sentence. It is the entry point of the parse tree.
*   **`nsubj` (nominal subject)**: This labels a nominal (noun or noun phrase) that functions as the subject of a clause. The image shows two instances of `nsubj`, one directly under `root` and another under `acl:relcl`, indicating subjects of different clauses or parts of the sentence.
*   **`obj` (object)**: This labels a nominal that functions as the direct object of a verb or preposition. It is shown as dependent on `obl`.
*   **`det` (determiner)**: This labels a determiner modifying a noun. The diagram shows multiple `det` nodes, indicating that several nouns in the underlying sentence are modified by determiners. For example, "a car" would have "a" labeled as `det` of "car". One `det` is under `obj`, and two others are under `obl`, suggesting different parts of the sentence contain determiners.
*   **`obl` (oblique nominal)**: This labels a nominal functioning as an oblique argument, typically introduced by a preposition or indicating an adjunct. It is shown as dependent on `root`.
*   **`acl:relcl` (clausal modifier of noun: relative clause)**: This labels a finite or non-finite clause that modifies a nominal, functioning as a relative clause. It is shown as dependent on `obl`, indicating it modifies the head of the oblique phrase.
*   **`cop` (copula)**: This labels a copular verb (e.g., "be", "seem") that links a subject to a predicate complement. It is shown as dependent on `acl:relcl`.
*   **`adv` (adverbial modifier)**: This labels an adverbial modifier. It is shown as dependent on `cop`.

The extracted text elements (`root`, `nsubj`, `obj`, `det`, `obl`, `acl:relcl`, `cop`, `adv`) are the direct evidence for these interpretations. They are standard labels used in Universal Dependencies (UD) or similar dependency parsing frameworks to categorize the syntactic relationships between words. The arrows show the direction of dependency, from head to dependent.

**Key Insights:**
The main takeaways and lessons from this image are:

1.  **Hierarchical Sentence Structure:** Sentences have a hierarchical organization that can be represented by dependency trees, where words are connected through direct grammatical relationships. The presence of a `root` node as the highest head, and subsequent branching to `obl`, `nsubj`, and further dependents like `obj`, `det`, `acl:relcl`, `cop`, and `adv`, demonstrates this hierarchy.
2.  **Specific Grammatical Roles:** The diagram highlights that each word or constituent plays a specific grammatical role (e.g., `nsubj` for subject, `obj` for object, `det` for determiner, `adv` for adverbial). These labels are critical for a fine-grained syntactic analysis.
3.  **Complex Clause Structures:** The inclusion of `acl:relcl` (clausal modifier of noun: relative clause) shows that dependency parsing can represent complex sentence structures, including embedded clauses, and how they relate to their modified nominals.
4.  **Head-Dependent Relationships:** The directed arrows, implicitly pointing from a head to its dependent, illustrate the fundamental principle of dependency grammar: that all words in a sentence, except for the `root`, depend on another word. For instance, `obj` depends on `obl`, `det` depends on `obj`, `adv` depends on `cop`, and `cop` depends on `acl:relcl`.
5.  **Tools for Linguistic Analysis:** This image provides insight into the type of visual output generated by computational linguistic tools for dependency parsing, which are used to analyze and understand the grammatical structure of text.

The specific text elements (`root`, `nsubj`, `obj`, `det`, `obl`, `acl:relcl`, `cop`, `adv`) directly provide evidence for these insights by explicitly naming the grammatical relations that constitute the hierarchical structure of the sentence.

**Document Context:**
This image, given its context in "Section: 19.1.2 Projectivity", is highly relevant to understanding the concept of **projectivity** in dependency grammar. Projectivity is a constraint often applied to dependency trees, stating that if a word A is the head of word B, then all words between A and B in the linear order of the sentence must also be part of the dependency subtree rooted at A. While the actual words of the sentence are not shown, the way the arcs (lines) are drawn—generally without crossing over words or other arcs in a non-projective manner—is typical of projective dependency trees. It sets the stage for discussing the properties and constraints of such syntactic representations.

**Summary:**
This diagram illustrates a **dependency parse tree**, a fundamental concept in computational linguistics and natural language processing used to represent the grammatical structure of sentences. Unlike traditional phrase-structure trees that group words into constituents, a dependency tree focuses on direct binary relationships between words, where one word (the **head**) governs another (its **dependent**).

At the top of the hierarchy is the `root` node, which typically represents the main verb or the central element around which the entire sentence structure is built. From this `root`, various grammatical relationships branch out, each labeled with a specific dependency type:

*   **`nsubj` (nominal subject):** This indicates a noun or noun phrase that performs the action of a verb or is described by a predicate. The diagram shows two `nsubj` instances, suggesting two different subjects within the sentence or its clauses.
*   **`obl` (oblique nominal):** This represents an argument or adjunct that is typically introduced by a preposition, providing additional information like location, time, or manner. In this diagram, `obl` is a direct dependent of the `root`.
*   **`obj` (object):** This refers to a direct object, receiving the action of a verb, and is shown here as dependent on `obl`.
*   **`det` (determiner):** These nodes, appearing three times in the diagram, indicate words like "a," "the," "this," or "many" that modify nouns. One `det` is shown modifying the `obj`, and two others modify the `obl` phrase, highlighting how determiners specify nouns.
*   **`acl:relcl` (clausal modifier of noun: relative clause):** This label points to an entire clause that modifies a noun, often introduced by words like "who," "which," or "that." Here, it's dependent on `obl`, meaning the oblique phrase contains a noun that is further described by a relative clause.
*   **`cop` (copula):** This refers to linking verbs such as "is," "was," "become," which connect a subject to a predicate complement. The `cop` is shown as dependent on `acl:relcl`, indicating a copular construction within the relative clause.
*   **`adv` (adverbial modifier):** This labels words or phrases that modify verbs, adjectives, or other adverbs, providing information about manner, place, time, etc. It is shown here as dependent on the `copula`.

Each downward arrow from these labeled boxes implicitly points to the actual word in the sentence that corresponds to that grammatical function, though the words themselves are not shown. The overall structure demonstrates how complex sentences can be broken down into these fundamental head-dependent relationships, allowing for a detailed and systematic analysis of their grammar. This representation is particularly useful in fields like natural language understanding, where accurately identifying these relationships is crucial for interpreting meaning. The absence of crossing lines for dependency arcs (where all words between a head and its dependent are also dependents of that head) suggests that this diagram might exemplify a **projective** dependency tree, a common property in linguistic analysis.](images/daa9e66fbe3e1e99b5a559b1537fbc8c6588e1a4871acce768a2f104c7d8ef3d.jpg)
JetBlue canceled our flight this morning which was already late

In this example, the arc from flight to its modifier late is non-projective since there is no path from flight to the intervening words this and morning. As we can see from this diagram, projectivity (and non-projectivity) can be detected in the way we’ve been drawing our trees. A dependency tree is projective if it can be drawn with no crossing edges. Here there is no way to link flight to its dependent late without crossing the arc that links morning to its head.

Our concern with projectivity arises from two related issues. First, the most widely used English dependency treebanks were automatically derived from phrasestructure treebanks through the use of head-finding rules. The trees generated in such a fashion will always be projective, and hence will be incorrect when non-projective examples like this one are encountered.

Second, there are computational limitations to the most widely used families of parsing algorithms. The transition-based approaches discussed in Section 19.2 can only produce projective trees, hence any sentences with non-projective structures will necessarily contain some errors. This limitation is one of the motivations for the more flexible graph-based parsing approach described in Section 19.3.

# 19.1.3 Dependency Treebanks

Treebanks play a critical role in the development and evaluation of dependency parsers. They are used for training parsers, they act as the gold labels for evaluating parsers, and they also provide useful information for corpus linguistics studies.

Dependency treebanks are created by having human annotators directly generate dependency structures for a given corpus, or by hand-correcting the output of an automatic parser. A few early treebanks were also based on using a deterministic process to translate existing constituent-based treebanks into dependency trees.

The largest open community project for building dependency trees is the Universal Dependencies project at https://universaldependencies.org/ introduced above, which currently has almost 200 dependency treebanks in more than 100 languages (de Marneffe et al., 2021). Here are a few UD examples showing dependency trees for sentences in Spanish, Basque, and Mandarin Chinese:

![## Image Analysis: 7f18655f357a51638b1a9feebc22866e5a3ec2ac9efc809ab7bad436295db823.jpg

**Conceptual Understanding:**
This image conceptually represents a dependency tree, which is a graph-based syntactic representation of a sentence. Its main purpose is to illustrate the grammatical relationships between words in the Spanish sentence "Subiremos a el tren a las cinco ." and its English translation "we-will-board on the train at the five .". The image communicates key ideas about linguistic analysis, specifically dependency parsing, part-of-speech tagging, and how syntactic structures are formally represented to show which words modify or depend on others.

**Content Interpretation:**
The image shows a dependency parse tree for a Spanish sentence. It illustrates the grammatical relationships and syntactic structure of the sentence "Subiremos a el tren a las cinco ." (we-will-board on the train at the five .). Each word is annotated with its Part-of-Speech (POS) tag, and directed arrows indicate dependencies between words, with labels specifying the type of dependency. For instance, the main verb "Subiremos" (we-will-board) is the head of the sentence. It has an oblique argument (obl) related to "a" (on) which points to "tren" (train), an oblique temporal modifier (obl:tmod) related to "a" (at) which points to "cinco" (five), and a punctuation dependency (punct) to the final period. The prepositions "a" (on) and "a" (at) are marked with 'case' dependencies to their respective determiners "el" (the) and "las" (the), which in turn have 'det' dependencies to their nouns "tren" (train) and "cinco" (five). This visual representation is a fundamental concept in computational linguistics and natural language processing for analyzing sentence structure. The extracted text elements confirm the POS tags for each word (VERB, ADP, DET, NOUN, ADP, DET, NUM, PUNCT) and the specific dependency labels (punct, obl:tmod, obl, case, det) that define their relationships.

**Key Insights:**
The main takeaway from this image is a clear demonstration of how dependency trees are constructed to represent the grammatical relationships within a sentence. It teaches that each word in a sentence has a specific Part-of-Speech (POS) tag (e.g., VERB, NOUN, ADP, DET, NUM, PUNCT) and that words relate to each other through directed dependencies, each labeled with a specific grammatical function (e.g., 'obl' for oblique, 'obl:tmod' for oblique temporal modifier, 'case' for case marker, 'det' for determiner, 'punct' for punctuation). The image highlights that the main verb often serves as the root of the dependency tree, and other words attach to it or its dependents in a hierarchical manner. Specifically, the labels 'obl' and 'obl:tmod' show how different types of oblique arguments (non-core arguments, like locative or temporal phrases) are distinguished. The 'case' label demonstrates the role of prepositions in marking the case of a noun phrase, and 'det' shows the relationship between a determiner and the noun it modifies. The comprehensive transcription of all text, including POS tags, Spanish words, English glosses, and dependency labels, provides concrete evidence for these insights into syntactic structure and dependency parsing.

**Document Context:**
This image is presented in the section "19.1.3 Dependency Treebanks", indicating its role as a concrete example of a dependency tree. It directly illustrates the concepts and structures discussed in the surrounding text, providing a visual aid to understand how dependency grammar represents sentence syntax. By showing a real-world Spanish sentence parsed into a dependency tree with English glosses, it helps readers grasp the practical application of dependency parsing, especially for different languages, as well as the specific types of dependencies and part-of-speech tags used in linguistic analysis.

**Summary:**
This image displays a dependency tree for the Spanish sentence "Subiremos a el tren a las cinco .", which translates to "we-will-board on the train at the five .". A dependency tree visually represents the grammatical relationships between words in a sentence, showing how words modify or depend on others. Each word is labeled with its Part-of-Speech (POS) tag, and connections between words are labeled with dependency types. The main verb, "Subiremos" (we-will-board), acts as the root of the sentence. It has three direct dependents: an oblique argument related to the train, an oblique temporal modifier related to the time, and the final punctuation mark. The preposition "a" (on) introduces the noun phrase "el tren" (the train), where "el" acts as a determiner for "tren". Similarly, the preposition "a" (at) introduces the time expression "las cinco" (the five), with "las" determining "cinco". This detailed breakdown illustrates the syntactic structure and the specific roles each word plays within the sentence.](images/7f18655f357a51638b1a9feebc22866e5a3ec2ac9efc809ab7bad436295db823.jpg)

[Spanish] Subiremos al tren a las cinco. “We will be boarding the train at five.”(19.4)

![## Image Analysis: 2caf7939933bfe3bc1ccc8002df2c837070e46da3b790a2260b8efdf07c69e8e.jpg

**Conceptual Understanding:**
The image represents a dependency tree, a linguistic diagram that illustrates the grammatical relationships between words in a sentence. Conceptually, it shows how each word in a sentence depends on another word, forming a hierarchical structure where a 'head' word governs its 'dependent' words. The main purpose of this specific diagram is to visually analyze and explain the syntactic structure of the Basque sentence "Ekatzak itsasontzia hondoratu du ." (The storm has sunk the ship.), by assigning part-of-speech tags to each word and labeling the specific grammatical dependencies between them. It communicates the key idea of dependency grammar, where syntactic relationships are modeled as directed links from a head word to its dependent words, rather than through phrase-structure constituents. The diagram also highlights the role of cases (Ergative and Absolutive) in the sentence's structure.

**Content Interpretation:**
The image displays a dependency tree diagram that illustrates the grammatical relationships between words in a sentence. Specifically, it shows how different words depend on a head word (typically a verb) and what their grammatical function is in relation to that head. The diagram uses labeled arrows to denote these dependency types.

Here's a breakdown of the processes, concepts, and relationships shown:
- **Words and their Properties:** Each word in the sentence ("Ekatzak", "itsasontzia", "hondoratu", "du", ".") is shown along with its Part-of-Speech (POS) tag (NOUN, VERB, AUX, PUNCT) and its English translation, including grammatical case information for the nouns ("(Erg.)" for Ergative and "(Abs.)" for Absolutive).
- **Head-Dependent Relationships:** The core concept is that of head-dependent relationships. "hondoratu" (sunk) acts as the central head of the sentence, as all other content words and the auxiliary directly depend on it.
- **Dependency Labels:** The labels on the arrows define the specific grammatical role of the dependent word relative to its head:
    - **`nsubj` (nominal subject):** This label connects "hondoratu" (VERB) to "Ekatzak" (NOUN). This indicates that "Ekatzak" (storm) is the nominal subject of the verb "hondoratu" (sunk).
    - **`obj` (object):** This label connects "hondoratu" (VERB) to "itsasontzia" (NOUN). This indicates that "itsasontzia" (ship) is the direct object of the verb "hondoratu" (sunk).
    - **`aux` (auxiliary):** This label connects "hondoratu" (VERB) to "du" (AUX). This signifies that "du" (has) is an auxiliary verb helping the main verb "hondoratu" (sunk).
    - **`punct` (punctuation):** This label connects "hondoratu" (VERB) to the period "." (PUNCT). This shows that the punctuation mark is dependent on the main verb of the sentence.

All extracted text elements directly support these interpretations by explicitly stating the words, their POS tags, their meanings, and the precise grammatical relationships (nsubj, obj, aux, punct) that form the sentence's structure. The presence of Ergative and Absolutive cases for the nouns further indicates the grammatical system of the language being analyzed.

**Key Insights:**
The main takeaways and insights from this image are:
1.  **Dependency Parsing Visualization:** The image effectively demonstrates how dependency parsing visually represents the grammatical structure of a sentence, showing head-dependent relationships through directed, labeled arcs.
2.  **Core Grammatical Relations:** It illustrates fundamental dependency relations such as nominal subject (`nsubj`), direct object (`obj`), auxiliary verb (`aux`), and punctuation (`punct`), which are common across many linguistic analyses.
3.  **Head-Driven Structure:** The diagram clearly indicates that the verb "hondoratu" (sunk) acts as the head of the clause, with all other significant elements depending on it. This highlights the verb-centric nature of many sentence structures in dependency grammar.
4.  **Cross-Linguistic Application:** The use of a Basque sentence, complete with Ergative ("Ekatzak" - storm (Erg.)) and Absolutive ("itsasontzia" - ship (Abs.)) case markings, provides an example of how dependency parsing can be applied to languages with different morphological and syntactic properties than English. This underscores the universality and adaptability of dependency grammar as a linguistic analysis tool.
5.  **Role of Lexical and Grammatical Information:** The diagram integrates lexical information (words and their translations), morphological information (case markings like Ergative/Absolutive), and syntactic information (POS tags and dependency relations) to provide a comprehensive view of the sentence's structure.

These insights are directly supported by the verbatim transcription of all words, their part-of-speech tags, their English translations (including case notes), and the explicit dependency labels (`nsubj`, `obj`, `aux`, `punct`) on the connecting arrows.

**Document Context:**
This image, placed within section "19.1.3 Dependency Treebanks", serves as a direct example of a dependency treebank entry. It illustrates how sentences are parsed and represented in a dependency grammar framework, which is crucial for computational linguistics and natural language processing. The diagram provides a concrete visual aid to understand the abstract concepts of dependency relations (like `nsubj`, `obj`, `aux`, `punct`) discussed in the text.

The image helps readers understand:
- **What a Dependency Treebank is:** By showing an actual entry, it concretizes the idea of a collection of sentences annotated with their dependency structures.
- **How Dependency Relations are Marked:** It visually demonstrates the use of arrows and labels to show grammatical relationships between words.
- **The Structure of a Sample Sentence:** It breaks down a sentence into its constituent words, their parts of speech, and their functional roles, which is key to understanding the underlying linguistic principles.

This specific example, involving a Basque sentence with ergative and absolutive cases, also hints at the potential complexities and cross-linguistic applications of dependency parsing, making the concept more tangible and relatable to diverse linguistic structures beyond just English.

**Summary:**
The image displays a dependency tree diagram illustrating the grammatical structure of a Basque sentence, along with its English translation and part-of-speech tags. The diagram is read from left to right, showing five distinct word/punctuation elements, each with its corresponding Part-of-Speech (POS) tag and English translation. Curved arrows connect the words, indicating dependency relationships labeled in rectangular boxes.

The sentence components are:
- **Ekatzak:** Labeled as NOUN, translated as "storm (Erg.)".
- **itsasontzia:** Labeled as NOUN, translated as "ship (Abs.)".
- **hondoratu:** Labeled as VERB, translated as "sunk".
- **du:** Labeled as AUX, translated as "has".
- **. (period):** Labeled as PUNCT, representing a punctuation mark.

The dependency relationships are:
- A curved arrow originates from "hondoratu" (VERB) and points to "Ekatzak" (NOUN), with the label "nsubj" (nominal subject).
- A curved arrow originates from "hondoratu" (VERB) and points to "itsasontzia" (NOUN), with the label "obj" (object).
- A curved arrow originates from "hondoratu" (VERB) and points to "du" (AUX), with the label "aux" (auxiliary).
- A curved arrow originates from "hondoratu" (VERB) and points to the period "." (PUNCT), with the label "punct" (punctuation).

In essence, the diagram visually represents that "hondoratu" (sunk) is the head verb. "Ekatzak" (storm) is its nominal subject, "itsasontzia" (ship) is its object, "du" (has) is its auxiliary, and the period is a punctuation dependent on the main verb.](images/2caf7939933bfe3bc1ccc8002df2c837070e46da3b790a2260b8efdf07c69e8e.jpg)

[Basque] Ekaitzak itsasontzia hondoratu du. “The storm has sunk the ship.”(19.5)

![## Image Analysis: 36de604149bb2947745734248dade073486347ccb06dc8e99badfb69535a7f46.jpg

**Conceptual Understanding:**
This image conceptually represents a dependency tree, a linguistic structure used in computational linguistics and natural language processing. Its main purpose is to visually depict the grammatical relationships between words in a sentence, illustrating how words modify or relate to one another. The image communicates the syntactic analysis of the Chinese sentence '但 我 昨天 才 收 到 信 .' (But I yesterday only-then receive arrive letter .), showing each word's part of speech and its specific dependency link to another word, ultimately forming a hierarchical structure where one word acts as the head of the entire sentence or clause (in this case, '收' (receive)).

**Content Interpretation:**
This image presents a dependency parse tree for a Chinese sentence. It illustrates the grammatical relationships and syntactic structure of the sentence '但 我 昨天 才 收 到 信 .' (But I yesterday only-then receive arrive letter .). Each node in the tree represents a word, annotated with its part of speech and English translation. The arcs between words represent directed dependency relations, with specific labels indicating the type of grammatical relationship (e.g., subject, object, adverbial modifier, temporal modifier). The central concept is to visually decompose a sentence into its constituent words and their hierarchical, functional connections, where each word (except the root) is dependent on one other word, its 'head'.

**Key Insights:**
The main takeaway from this image is the explicit illustration of how dependency parsing analyzes a sentence into its constituent words and their grammatical relationships. Key insights include: 1.  **Head-Dependent Relationships:** The arrows clearly show which word is the head and which is the dependent (e.g., '我' (I) is dependent on '收' (receive) as its nominal subject). 2.  **Part-of-Speech Tagging:** Each word is assigned a specific part-of-speech (e.g., 'ADV', 'PRON', 'NOUN', 'VERB'), which is foundational for understanding its role in the sentence. 3.  **Specific Dependency Labels:** The use of precise labels like 'nsubj', 'obj', 'advmod', 'obj:tmod', 'adv', and 'compound:vv' provides detailed information about the syntactic function of each word in relation to its head. For instance, 'obj:tmod' for '昨天' (yesterday) indicates its dual role as an object and temporal modifier. The 'compound:vv' label for '到' (arrive) dependent on '收' (receive) highlights the presence of compound verbs in Chinese, where '收到' functions as a single semantic unit 'receive-arrive'. This diagram shows how different words contribute to the overall meaning and structure of the sentence, specifically indicating that '收' (receive) is the central verb around which other sentence elements are structured.

**Document Context:**
This image is directly relevant to the '19.1.3 Dependency Treebanks' section. It serves as an example of a dependency tree, which is a fundamental component of dependency treebanks. In this context, the image visually demonstrates how sentences are syntactically analyzed and annotated with dependency relations, which are crucial for natural language processing tasks such as parsing, machine translation, and information extraction. It provides a concrete illustration of the data structures and labeling conventions used in such linguistic resources, helping to explain the concept of a dependency tree in a practical way for the reader.

**Summary:**
This image displays a dependency tree illustrating the grammatical relationships within a Chinese sentence, providing both the Chinese characters and their English translations, along with their parts of speech and the dependency labels connecting them. The sentence shown is '但 我 昨天 才 收 到 信 .', which translates to 'But I yesterday only-then receive arrive letter .'. Each word is labeled with its part of speech: '但' (but) is an ADV (adverb), '我' (I) is a PRON (pronoun), '昨天' (yesterday) is a NOUN, '才' (only-then) is an ADV (adverb), '收' (receive) is a VERB, '到' (arrive) is a VERB, and '信' (letter) is a NOUN. The dependency relationships are shown by directed arcs with specific labels. All dependencies ultimately point towards '收' (receive) as the head verb of the clause. '但' (but) connects to '收' (receive) with an 'adv' label, indicating an adverbial relationship. '我' (I) connects to '收' (receive) with an 'nsubj' label, marking it as the nominal subject. '昨天' (yesterday) connects to '收' (receive) with an 'obj:tmod' label, indicating an object with a temporal modifier role. '才' (only-then) connects to '收' (receive) with an 'advmod' label, signifying an adverbial modifier. '到' (arrive) connects to '收' (receive) with a 'compound:vv' label, showing a verb-verb compound relationship, with '收' as the head. Finally, '信' (letter) connects to '收' (receive) with an 'obj' label, designating it as the direct object of the verb.](images/36de604149bb2947745734248dade073486347ccb06dc8e99badfb69535a7f46.jpg)

[Chinese] 但 昨 收到信 “But I didn’t receive the letter until yesterday”(19.6)

# 19.2 Transition-Based Dependency Parsing

# transition-based

Our first approach to dependency parsing is called transition-based parsing. This architecture draws on shift-reduce parsing, a paradigm originally developed for analyzing programming languages (Aho and Ullman, 1972). In transition-based parsing we’ll have a stack on which we build the parse, a buffer of tokens to be parsed, and a parser which takes actions on the parse via a predictor called an oracle, as illustrated in Fig. 19.4.

![## Image Analysis: a33f64da1dbd4ce47ce2530ebf6c9f103171e1ca60948d101058a00344d372df.jpg

**Conceptual Understanding:**
This image conceptually represents the architectural components and data flow of a basic transition-based parser, a method used in computational linguistics for syntactic analysis. Its main purpose is to illustrate how an input sequence of words is processed to generate a set of grammatical dependency relations between them. The key ideas communicated are the interaction between an input buffer and a stack, the role of a parser with an oracle for decision-making, and a set of predefined actions that lead to the construction of a dependency graph.

**Content Interpretation:**
The image illustrates the operational flow of a transition-based parser, a system designed to construct dependency trees for sentences. It depicts the core components: an 'Input buffer' holding the sentence words (w1, w2, ..., wn), a 'Stack' used for intermediate storage during parsing (s1, s2, ..., sn), and the 'Parser' itself. The 'Parser' contains an 'Oracle', which is responsible for making decisions on which 'Action' to take based on the current state of the stack and input buffer. The available 'Actions' are 'LEFTARC', 'RIGHTARC', and 'SHIFT', which are fundamental operations in dependency parsing. The final output is 'Dependency Relations', exemplified by an arrow from 'w3' to 'w2', representing a grammatical relationship between these words. The significance is to show how a parser incrementally builds a dependency structure by making a sequence of local decisions.

**Key Insights:**
The main takeaway is the high-level architecture of a transition-based dependency parser. It demonstrates that such a parser operates by manipulating two primary data structures: an 'Input buffer' and a 'Stack'. The 'Parser' acts as the control unit, making decisions guided by an 'Oracle'. These decisions, referred to as 'Actions' ('LEFTARC', 'RIGHTARC', 'SHIFT'), incrementally build 'Dependency Relations'. The diagram highlights that the parser's decision-making process is based on the current configuration of the 'Stack' (specifically its top two elements, 's1' and 's2') and the 'Input buffer', which directly supports the document text's description of how the parser selects an action by consulting an oracle that examines the current configuration. The final output is a set of directed dependencies, such as 'w3' depending on 'w2'.

**Document Context:**
This image, titled 'Figure 19.4 Basic transition-based parser' and appearing in a 'transition-based' section, perfectly illustrates the core mechanism described in the surrounding text. The accompanying text states, 'The parser examines the top two elements of the stack and selects an action by consulting an oracle that examines the current configuration,' which directly corresponds to the visual representation of the 'Stack' feeding into the 'Parser' (specifically s1 and s2), the 'Input buffer' interacting with the 'Parser', the 'Oracle' within the 'Parser', and the resulting 'Action' choices. The diagram visually defines the components and their interactions that constitute a basic transition-based parsing system.

**Summary:**
This diagram illustrates the fundamental architecture and operational flow of a basic transition-based parser, a method used in natural language processing to analyze sentence structure and establish dependency relations between words. The process begins with an 'Input buffer' containing a sequence of words, labeled 'w1', 'w2', up to 'wn', representing the input sentence. Concurrently, a 'Stack', which is a vertical data structure, holds elements 's1', 's2', down to 'sn'. The core of the system is the 'Parser' module, which contains an 'Oracle'. The parser's operation involves examining the top two elements of the 'Stack' (specifically 's1' and 's2' as indicated by the bracket and arrow) and the current state of the 'Input buffer'. Based on this configuration, the 'Oracle' within the 'Parser' determines an 'Action' to be performed. There are three possible actions: 'LEFTARC', 'RIGHTARC', and 'SHIFT'. These actions dictate how words are moved between the input buffer and stack, and how dependency relations are formed. The ultimate output of this process is stored in 'Dependency Relations', which is represented by a green box showing an example of a dependency: a curved arrow from 'w3' pointing to 'w2', signifying that 'w2' is dependent on 'w3'. This sequential process transforms an input word sequence into a structured representation of grammatical dependencies.](images/a33f64da1dbd4ce47ce2530ebf6c9f103171e1ca60948d101058a00344d372df.jpg)
Figure 19.4 Basic transition-based parser. The parser examines the top two elements of the stack and selects an action by consulting an oracle that examines the current configuration.

The parser walks through the sentence left-to-right, successively shifting items from the buffer onto the stack. At each time point we examine the top two elements on the stack, and the oracle makes a decision about what transition to apply to build the parse. The possible transitions correspond to the intuitive actions one might take in creating a dependency tree by examining the words in a single pass over the input from left to right (Covington, 2001):

• Assign the current word as the head of some previously seen word, • Assign some previously seen word as the head of the current word, • Postpone dealing with the current word, storing it for later processing.

We’ll formalize this intuition with the following three transition operators that will operate on the top two elements of the stack:

• LEFTARC: Assert a head-dependent relation between the word at the top of the stack and the second word; remove the second word from the stack. • RIGHTARC: Assert a head-dependent relation between the second word on the stack and the word at the top; remove the top word from the stack;

• SHIFT: Remove the word from the front of the input buffer and push it onto the stack.

We’ll sometimes call operations like LEFTARC and RIGHTARC reduce operations, based on a metaphor from shift-reduce parsing, in which reducing means combining elements on the stack. There are some preconditions for using operators. The LEFTARC operator cannot be applied when ROOT is the second element of the stack (since by definition the ROOT node cannot have any incoming arcs). And both the LEFTARC and RIGHTARC operators require two elements to be on the stack to be applied.

arc standard

This particular set of operators implements what is known as the arc standard approach to transition-based parsing (Covington 2001, Nivre 2003). In arc standard parsing the transition operators only assert relations between elements at the top of the stack, and once an element has been assigned its head it is removed from the stack and is not available for further processing. As we’ll see, there are alternative transition systems which demonstrate different parsing behaviors, but the arc standard approach is quite effective and is simple to implement.

The specification of a transition-based parser is quite simple, based on representing the current state of the parse as a configuration: the stack, an input buffer of words or tokens, and a set of relations representing a dependency tree. Parsing means making a sequence of transitions through the space of possible configurations. We start with an initial configuration in which the stack contains the ROOT node, the buffer has the tokens in the sentence, and an empty set of relations represents the parse. In the final goal state, the stack and the word list should be empty, and the set of relations will represent the final parse. Fig. 19.5 gives the algorithm.

![## Image Analysis: 5b7e8151929bb9535923976e1f0970d7c93652996b42ca902eb8bdc416ef9a36.jpg

**Conceptual Understanding:**
This image illustrates the core algorithm for a transition-based dependency parser. Conceptually, it represents an iterative process where a sequence of actions (transitions) are applied to a 'parsing state' until a complete dependency tree is formed. The main purpose is to transform a sequence of words into a structural representation (a dependency tree) that shows the grammatical relationships between those words. The key idea communicated is that dependency parsing can be framed as a series of local decisions guided by an 'oracle' that determines the optimal transition at each step.

**Content Interpretation:**
The image depicts a generic framework for a transition-based dependency parsing system. It outlines the algorithmic steps involved in transforming a sequence of words into a dependency tree through a series of iterative state changes and learned transitions.

*   **`function DEPENDENCYPARSE(words) returns dependency tree`**: This line defines the system's entry point, taking `words` as input and aiming to output a `dependency tree`, signifying the core task of structural prediction in natural language processing.
*   **`state ← { [root], [words], [ ] } ; initial configuration`**: This initializes the parsing `state`. The `[root]` typically represents a dummy node, `[words]` is the buffer of unparsed words, and the empty `[ ]` likely represents the stack or collection of formed dependency arcs. This sets up the parser's initial conditions.
*   **`while state not final`**: This control flow indicates an iterative process that continues until a complete dependency tree has been built and all words have been processed, reaching a terminal `state`.
*   **`t ← ORACLE(state) ; choose a transition operator to apply`**: The `ORACLE` is a critical component, typically a machine learning model, that predicts the best next `transition operator (t)` based on the current `state`. It represents the dynamic and context-dependent decision-making aspect of the parser, as the comment explicitly states its purpose: `choose a transition operator to apply`.
*   **`state ← APPLY(t, state) ; apply it, creating a new state`**: This line shows the state transformation. The chosen `transition operator (t)` modifies the current `state`, leading to a `new state`. Transition operators perform actions like shifting words or creating dependency arcs. The comment `apply it, creating a new state` reinforces that each transition advances the parser.
*   **`return state`**: Once the `while` loop terminates (when `state` is `final`), the final `state`, which now embodies the complete `dependency tree`, is returned as the result. All extracted text elements work together to explain the mechanism of a transition-based parser: initializing a state, iteratively applying transitions chosen by an oracle, and terminating when a final state (the dependency tree) is reached.

**Key Insights:**
The image provides several key takeaways regarding transition-based dependency parsing:

*   **Iterative Nature of Parsing:** The `while state not final` loop clearly indicates that dependency parsing is a step-by-step, iterative process, rather than a single, monolithic computation. Each `state` transformation contributes incrementally to the final `dependency tree`.
*   **State-Based Parsing:** The concept of a `state` is central. The parsing process is defined by its current `state`, and all decisions and actions operate on and transform this `state`. The initial `state` is explicitly defined as `{ [root], [words], [ ] }`.
*   **Role of an Oracle (Decision-Making Component):** The `ORACLE(state)` function highlights that the choice of which `transition operator` to apply at each step is crucial. In practical implementations, this `ORACLE` is typically a learned model, emphasizing the machine learning aspect of modern dependency parsers. The text `choose a transition operator to apply` reinforces this decision-making role.
*   **Transition Operators as Actions:** The `APPLY(t, state)` function demonstrates that parsing actions are encapsulated as `transition operator`s. These operators are responsible for `creating a new state`, effectively moving the parsing process forward.
*   **Goal-Oriented Process:** The algorithm explicitly `returns dependency tree`, showing that its ultimate goal is to produce a structural representation of the input sentence. The `while state not final` condition further implies that the process continues until this goal is met.

These insights are directly supported by the verbatim text, which precisely defines the function's input and output, the initial configuration, the iterative loop, the oracle's role in choosing transitions, the application of these transitions to update the state, and the final return value.

**Document Context:**
This algorithm is presented in the "transition-based" section, indicating its relevance to a specific paradigm in dependency parsing within natural language processing. Transition-based parsers are a popular and efficient class of algorithms for constructing dependency trees by making a sequence of local, greedy decisions (transitions) to transform a partially built parse into a complete one. This algorithm serves as a foundational representation of how such a parser functions at a high level. It's crucial for understanding the operational mechanics discussed in the surrounding document, especially if the document delves into the specifics of various `ORACLE` implementations, types of `transition operator`s, or representations of `state`.

**Summary:**
This image displays a pseudo-code algorithm named DEPENDENCYPARSE that processes a list of words and generates a dependency tree. This algorithm outlines a common approach in natural language processing known as "transition-based dependency parsing."

Here's how the process works step-by-step:

1.  **Start and Initialization:** The function DEPENDENCYPARSE begins by taking an input called words, which is the sentence or sequence of words to be parsed. It then establishes an initial configuration for its internal state. This state is composed of three parts:
    *   [root]: A special starting point for building the tree.
    *   [words]: The original list of input words that still need to be processed.
    *   [ ]: An empty placeholder, typically representing an empty stack where words are moved or an initial set of partially formed dependency links.

2.  **Iterative Parsing Loop:** The algorithm then enters a while loop that continues as long as the current state is not final. A final state means that the dependency tree has been fully constructed, and all words have been integrated.

3.  **Decision Making (Oracle):** Inside this loop, the parser makes a critical decision at each step. It calls a component called ORACLE, passing in the current state. The ORACLE acts like an intelligent guide, deciding the best next action or transition operator (t) to apply to the current state. The goal of the ORACLE is to move the parser closer to a final state.

4.  **Applying the Transition:** Once the ORACLE has chosen a transition operator (t), this operator is then APPLYed to the current state. This action creating a new state by modifying the components within the state (e.g., moving words from the input buffer to a stack, or establishing a dependency link between two words).

5.  **Loop Continuation:** The process then repeats: the algorithm checks if the new state is final. If not, the ORACLE chooses another transition operator, which is then APPLYed, and so on.

6.  **Completion and Return:** When the state eventually becomes final (meaning the loop condition state not final is no longer true), the loop terminates. The algorithm then returns this final state, which now fully represents the completed dependency tree for the input words.

In essence, this algorithm describes how a parser iteratively builds a dependency tree by making informed decisions (guided by an ORACLE) about how to transform its internal state through a series of transition operators, until the tree is complete.](images/5b7e8151929bb9535923976e1f0970d7c93652996b42ca902eb8bdc416ef9a36.jpg)

At each step, the parser consults an oracle (we’ll come back to this shortly) that provides the correct transition operator to use given the current configuration. It then applies that operator to the current configuration, producing a new configuration. The process ends when all the words in the sentence have been consumed and the ROOT node is the only element remaining on the stack.

The efficiency of transition-based parsers should be apparent from the algorithm. The complexity is linear in the length of the sentence since it is based on a single left to right pass through the words in the sentence. (Each word must first be shifted onto the stack and then later reduced.)

Note that unlike the dynamic programming and search-based approaches discussed in Chapter 18, this approach is a straightforward greedy algorithm—the oracle provides a single choice at each step and the parser proceeds with that choice, no other options are explored, no backtracking is employed, and a single parse is returned in the end.

Figure 19.6 illustrates the operation of the parser with the sequence of transitions leading to a parse for the following example.

![## Image Analysis: fda77d0ee074db235723b8f41bf343dc40ded6a871ac47e8eda5c6540ad52bf1.jpg

**Conceptual Understanding:**
This image represents a dependency parse tree, which is a grammatical analysis of a sentence. Its main purpose is to show the syntactic relationships, or dependencies, between words in the sentence 'Book me the morning flight'. The diagram illustrates which words are dependents of other words and the specific type of grammatical relationship they hold. The key idea communicated is the hierarchical and relational structure of a sentence's grammar, where each word is linked to another as a 'head' or 'dependent' with a labeled relationship.

**Content Interpretation:**
The image depicts a dependency parse tree, a linguistic representation that shows grammatical relationships between words in a sentence. Specifically, it illustrates the syntactic structure of the sentence “Book me the morning flight”. It identifies the head of the sentence ('Book') and then branches out to show how other words depend on each other and their specific roles (e.g., indirect object, direct object, determiner, compound modifier). The visual representation clearly indicates which words modify or are governed by other words.

**Key Insights:**
The main takeaway from this image is an understanding of how dependency parsing works by identifying syntactic relations between words. It clearly shows: 1. The concept of a 'root' word for a sentence ('Book'). 2. The assignment of grammatical roles like 'iobj' (indirect object for 'me'), 'obj' (direct object for 'flight'), 'det' (determiner for 'the'), and 'compound' (compound modifier for 'morning'). 3. The hierarchical nature of sentence structure, where modifiers like 'the' and 'morning' are attached to their head ('flight'), which in turn is a dependent of the main verb ('Book'). This illustrates the detailed grammatical insights provided by dependency parsing.

**Document Context:**
Given the document section 'transition-based', this image most likely serves as an illustrative example of the output generated by a transition-based dependency parser or a concept that such parsing models aim to produce. It demonstrates the kind of structured linguistic analysis that is crucial for natural language processing tasks, providing a concrete instance of how grammatical dependencies are represented. It helps to understand the target structure that 'transition-based' methods are designed to build.

**Summary:**
The image displays a dependency parse tree for the sentence “Book me the morning flight”. The tree visually represents the grammatical relationships between the words in the sentence. Starting from a 'root' node, it branches down to the main verb 'Book'. From 'Book', two primary dependency relationships are shown: 'iobj' (indirect object) connecting to 'me', and 'obj' (direct object) connecting to 'flight'. The word 'flight' further has two dependents: 'det' (determiner) connecting to 'the', and 'compound' connecting to 'morning'. The structure shows 'Book' as the head of the sentence, with 'me' as its indirect object and 'flight' as its direct object. 'The' and 'morning' modify 'flight', specifying which flight. This diagram systematically breaks down the sentence into its constituent syntactic relationships, making the grammatical structure explicitly clear.](images/fda77d0ee074db235723b8f41bf343dc40ded6a871ac47e8eda5c6540ad52bf1.jpg)

Let’s consider the state of the configuration at Step 2, after the word me has been pushed onto the stack.

<table><tr><td>Stack</td><td>Word List</td><td>Relations</td></tr><tr><td></td><td>[root, book, me][the, morning, flight]</td><td></td></tr></table>

The correct operator to apply here is RIGHTARC which assigns book as the head of me and pops me from the stack resulting in the following configuration.

<table><tr><td>Stack</td><td>Word List</td><td>Relations</td></tr><tr><td></td><td>[root, book][the, morning, flight]|(book → me)</td><td></td></tr></table>

After several subsequent applications of the SHIFT operator, the configuration in Step 6 looks like the following:

<table><tr><td>Stack</td><td>Word List</td><td>Relations</td></tr><tr><td>[root, book, the, morning, flight]</td><td>[]</td><td>(book→me)</td></tr></table>

Here, all the remaining words have been passed onto the stack and all that is left to do is to apply the appropriate reduce operators. In the current configuration, we employ the LEFTARC operator resulting in the following state.

<table><tr><td>Stack</td><td>Word List</td><td>Relations</td></tr><tr><td>[root,book,the,flight]</td><td>[</td><td>(book→me) (morning ← flight)</td></tr></table>

At this point, the parse for this sentence consists of the following structure.

There are several important things to note when examining sequences such as the one in Figure 19.6. First, the sequence given is not the only one that might lead to a reasonable parse. In general, there may be more than one path that leads to the same result, and due to ambiguity, there may be other transition sequences that lead to different equally valid parses.

Second, we are assuming that the oracle always provides the correct operator at each point in the parse—an assumption that is unlikely to be true in practice. As a result, given the greedy nature of this algorithm, incorrect choices will lead to incorrect parses since the parser has no opportunity to go back and pursue alternative choices. Section 19.2.4 will introduce several techniques that allow transition-based approaches to explore the search space more fully.

<table><tr><td>Step</td><td>Stack</td><td>Word List</td><td>Action</td><td>Relation Added</td></tr><tr><td>0</td><td>[root] [root, book]</td><td rowspan="6">[book, me, the, morning, flight] [me, the, morning, flight] [root, book, me] [the, morning, flight] [root, book] [the, morning, flight]</td><td rowspan="4">SHIFT SHIFT RIGHTARC SHIFT</td><td rowspan="4">(book → me)</td></tr><tr><td>1 2</td></tr><tr><td>3</td></tr><tr><td></td></tr><tr><td>4</td></tr><tr><td></td><td>[morning, flight]</td></tr><tr><td>5</td><td>[root, book, the] [root, book, the, morning]</td></tr><tr><td>6</td><td>[root, book, the, morning, flight]</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td>[flight]</td></tr><tr><td></td><td></td></tr><tr><td>7</td><td></td></tr><tr><td>[root, book, the, flight] [root, book, flight] [root, book]</td><td> LEFTARC</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td> LEFTARC</td></tr><tr><td>Done</td><td>(morning ← flight) (the ← flight) RIGHTARC (book → flight) RIGHTARC (root -→ book)</td></tr><tr><td>8 9</td><td>[</td></tr><tr><td></td><td></td></tr></table>

Figure 19.6 Trace of a transition-based parse.

Finally, for simplicity, we have illustrated this example without the labels on the dependency relations. To produce labeled trees, we can parameterize the LEFTARC and RIGHTARC operators with dependency labels, as in LEFTARC(NSUBJ) or RIGHTARC(OBJ). This is equivalent to expanding the set of transition operators from our original set of three to a set that includes LEFTARC and RIGHTARC operators for each relation in the set of dependency relations being used, plus an additional one for the SHIFT operator. This, of course, makes the job of the oracle more difficult since it now has a much larger set of operators from which to choose.

# 19.2.1 Creating an Oracle

The oracle for greedily selecting the appropriate transition is trained by supervised machine learning. As with all supervised machine learning methods, we will need training data: configurations annotated with the correct transition to take. We can draw these from dependency trees. And we need to extract features of the configuration. We’ll introduce neural classifiers that represent the configuration via embeddings, as well as classic systems that use hand-designed features.

# Generating Training Data

The oracle from the algorithm in Fig. 19.5 takes as input a configuration and returns a transition operator. Therefore, to train a classifier, we will need configurations paired with transition operators (i.e., LEFTARC, RIGHTARC, or SHIFT). Unfortunately, treebanks pair entire sentences with their corresponding trees, not configurations with transitions.

To generate the required training data, we employ the oracle-based parsing algorithm in a clever way. We supply our oracle with the training sentences to be parsed along with their corresponding reference parses from the treebank. To produce training instances, we then simulate the operation of the parser by running the algorithm and relying on a new training oracle to give us correct transition operators for each successive configuration.

To see how this works, let’s first review the operation of our parser. It begins with a default initial configuration where the stack contains the ROOT, the input list is just the list of words, and the set of relations is empty. The LEFTARC and RIGHTARC operators each add relations between the words at the top of the stack to the set of relations being accumulated for a given sentence. Since we have a gold-standard reference parse for each training sentence, we know which dependency relations are valid for a given sentence. Therefore, we can use the reference parse to guide the selection of operators as the parser steps through a sequence of configurations.

<table><tr><td>Step</td><td>Stack</td><td>Word List</td><td>Predicted Action</td></tr><tr><td>0</td><td>[root]</td><td>[book, the, flight, through, houston]</td><td> SHIFT</td></tr><tr><td>1</td><td>[root, book]</td><td>[the, flight, through, houston]</td><td> SHIFT</td></tr><tr><td>2</td><td>[root, book, the]</td><td>[flight, through, houston]</td><td> SHIFT</td></tr><tr><td>3</td><td>[root, book, the, flight]</td><td>[through, houston]</td><td> LEFTARC</td></tr><tr><td>4</td><td>[root, book, flight]</td><td>[through, houston]</td><td> SHIFT</td></tr><tr><td>5</td><td>[root, book, flight, through]</td><td>[houston]</td><td> SHIFT</td></tr><tr><td>6</td><td>[root, book, flight, through, houston]</td><td>[</td><td> LEFTARC</td></tr><tr><td>7</td><td>[root, book, flight, houston ]</td><td>[</td><td>RIGHTARC</td></tr><tr><td>8</td><td>[root, book, flight]</td><td>[</td><td>RIGHTARC</td></tr><tr><td>9</td><td>[root, book]</td><td>[</td><td>RIGHTARC</td></tr><tr><td>10</td><td>[root]</td><td>[</td><td>Done</td></tr></table>

Figure 19.7 Generating training items consisting of configuration/predicted action pairs by simulating a parse with a given reference parse.

To be more precise, given a reference parse and a configuration, the training oracle proceeds as follows:

• Choose LEFTARC if it produces a correct head-dependent relation given the reference parse and the current configuration,   
• Otherwise, choose RIGHTARC if (1) it produces a correct head-dependent relation given the reference parse and (2) all of the dependents of the word at the top of the stack have already been assigned,   
• Otherwise, choose SHIFT.

The restriction on selecting the RIGHTARC operator is needed to ensure that a word is not popped from the stack, and thus lost to further processing, before all its dependents have been assigned to it.

More formally, during training the oracle has access to the following:

• A current configuration with a stack $s$ and a set of dependency relations $R _ { c }$ • A reference parse consisting of a set of vertices $V$ and a set of dependency relations $R _ { p }$

Given this information, the oracle chooses transitions as follows:

LEFTARC(r): if $( S _ { 1 } r S _ { 2 } ) \in R _ { p }$   
RIGHTARC(r): if $( S _ { 2 } r S _ { 1 } ) \in R _ { p }$ and $\forall r ^ { \prime } , w s . t . ( S _ { 1 } r ^ { \prime } w ) \in R _ { p }$ then $( S _ { 1 } r ^ { \prime } w ) \in R _ { c }$   
SHIFT: otherwise

Let’s walk through the processing of the following example as shown in Fig. 19.7.

![## Image Analysis: 2c593fe5617574db446caab9b72dbb61cfb9f9734024fdf72a6f3580c525ca75.jpg

**Conceptual Understanding:**
This image conceptually represents a **dependency parse tree** for a given sentence. The main purpose is to **illustrate the grammatical relationships and syntactic dependencies** between the words in the sentence "Book the flight through Houston." It aims to break down the sentence's structure, showing which words modify or are dependent on other words, and what specific grammatical role they play in those relationships. The key idea being communicated is how a sentence can be analyzed into a hierarchical structure of dependencies, which is a fundamental concept in computational linguistics and natural language processing.

**Content Interpretation:**
The image illustrates a dependency parse tree for the English sentence "Book the flight through Houston." It demonstrates the grammatical relationships between individual words in the sentence using directed arrows and labeled nodes. The system shows a hierarchical and relational structure: 'Book' is the main action (root), 'flight' is the object of 'Book', 'the' modifies 'flight' as a determiner, 'through' modifies 'flight' as a nominal modifier, and 'through' establishes the case for 'Houston'. This diagram clarifies how words modify or depend on others to form a coherent sentence.

**Key Insights:**
**Main Takeaways:**
1.  **Dependency Parsing:** The image demonstrates the concept of dependency parsing, which analyzes grammatical relationships between words, identifying which words modify or depend on others.
2.  **Sentence Structure Representation:** It provides a visual and explicit representation of a sentence's syntactic structure, making complex linguistic relationships understandable.
3.  **Role of Grammatical Labels:** Each label (e.g., `root`, `obj`, `det`, `nmod`, `case`) represents a specific grammatical function or relationship within the sentence.
4.  **Foundation for NLP:** Such structured representations of text are fundamental for generating training data in Natural Language Processing, enabling machines to understand and process human language more effectively.

**Insights from Textual Evidence:**
*   The `root` label assigned to "Book" indicates that it is the central predicate of the sentence.
*   The `obj` label between "Book" and "flight" explicitly states that "flight" is the direct object of the verb "Book."
*   The `det` label for "the" modifying "flight" signifies that "the" functions as a determiner for "flight."
*   The `nmod` label connecting "flight" to "through" highlights a nominal modification, where "through Houston" provides additional information about the flight.
*   The `case` label for "through" modifying "Houston" specifies the grammatical role of the preposition "through" in establishing the relationship with "Houston."
These labels provide concrete evidence for the interpretation of each word's role and relationship within the sentence structure.

**Document Context:**
This image is presented in the context of "Generating Training Data." Its relevance lies in demonstrating how raw textual data (a sentence) can be transformed into a structured, annotated format suitable for training natural language processing (NLP) models. A dependency parse tree, as shown, provides rich linguistic features that can be used as input for various NLP tasks, such as sentiment analysis, named entity recognition, or machine translation. By illustrating these grammatical dependencies, the image highlights a crucial step in preparing high-quality, linguistically informed training data.

**Summary:**
This image displays a dependency parse tree for the sentence "Book the flight through Houston." It visually represents the grammatical relationships between the words in the sentence. The word "Book" is identified as the `root` of the sentence. "Book" is related to "flight" as its direct `obj`ect. The word "the" functions as a `det`erminer for "flight." Additionally, "flight" has a `nmod` (nominal modifier) relationship with "through." The word "through" acts as a `case` marker for "Houston," indicating a prepositional relationship. The diagram effectively breaks down the sentence into its constituent parts and their syntactic roles, making the underlying grammatical structure explicit. This detailed breakdown is crucial for tasks like natural language processing, where understanding sentence structure is a prerequisite for generating training data or performing semantic analysis. The connections and labels explicitly define how each word contributes to the overall meaning and grammatical correctness of the phrase.](images/2c593fe5617574db446caab9b72dbb61cfb9f9734024fdf72a6f3580c525ca75.jpg)

At Step 1, LEFTARC is not applicable in the initial configuration since it asserts a relation, $( \mathrm { r o o t }  \mathrm { b o o k } )$ ), not in the reference answer; RIGHTARC does assert a relation contained in the final answer $( \mathrm { r o o t } \to \mathrm { b o o k } )$ ), however book has not been attached to any of its dependents yet, so we have to defer, leaving SHIFT as the only possible action. The same conditions hold in the next two steps. In step 3, LEFTARC is selected to link the to its head.

Now consider the situation in Step 4.

$$
\frac { \mathrm { S t a c k } } { [ \mathrm { r o o t , b o o k , f i i g h t } ] [ \mathrm { t h r o u g h , H o u s t o n } ] ( \mathrm { t h e  \mathrm { H i g h t } } ) }
$$

Here, we might be tempted to add a dependency relation between book and flight, which is present in the reference parse. But doing so now would prevent the later attachment of Houston since flight would have been removed from the stack. Fortunately, the precondition on choosing RIGHTARC prevents this choice and we’re again left with SHIFT as the only viable option. The remaining choices complete the set of operators needed for this example.

To recap, we derive appropriate training instances consisting of configurationtransition pairs from a treebank by simulating the operation of a parser in the context of a reference dependency tree. We can deterministically record correct parser actions at each step as we progress through each training example, thereby creating the training set we require.

# 19.2.2 A feature-based classifier

We’ll now introduce two classifiers for choosing transitions, here a classic featurebased algorithm and in the next section a neural classifier using embedding features.

Featured-based classifiers generally use the same features we’ve seen with partof-speech tagging and partial parsing: Word forms, lemmas, parts of speech, the head, and the dependency relation to the head. Other features may be relevant for some languages, for example morphosyntactic features like case marking on subjects or objects. The features are extracted from the training configurations, which consist of the stack, the buffer and the current set of relations. Most useful are features referencing the top levels of the stack, the words near the front of the buffer, and the dependency relations already associated with any of those elements.

We’ll use a feature template as we did for sentiment analysis and part-of-speech tagging. Feature templates allow us to automatically generate large numbers of specific features from a training set. For example, consider the following feature templates that are based on single positions in a configuration.

$$
\begin{array} { r } { \langle s _ { 1 } . w , o p \rangle , \langle s _ { 2 } . w , o p \rangle \langle s _ { 1 } . t , o p \rangle , \langle s _ { 2 } . t , o p \rangle } \\ { \langle b _ { 1 } . w , o p \rangle , \langle b _ { 1 } . t , o p \rangle \langle s _ { 1 } . w t , o p \rangle } \end{array}
$$

Here features are denoted as location.property, where $s =$ stack, $b =$ the word buffer, $w =$ word forms, $t =$ part-of-speech, and $o p =$ operator. Thus the feature for the word form at the top of the stack would be $s _ { 1 } . \boldsymbol { w }$ , the part of speech tag at the front of the buffer $b _ { 1 } . t$ , and the concatenated feature $s _ { 1 } . w t$ represents the word form concatenated with the part of speech of the word at the top of the stack. Consider applying these templates to the following intermediate configuration derived from a training oracle for (19.2).

<table><tr><td>Stack</td><td>Word buffer</td><td>Relations</td></tr><tr><td>[root, canceled, flights]</td><td>[to Houston]</td><td>(canceled →United) (flights → morning) (flights→ the)</td></tr></table>

The correct transition here is SHIFT (you should convince yourself of this before proceeding). The application of our set of feature templates to this configuration would result in the following set of instantiated features.

$$
\begin{array} { r } { \langle s _ { 1 } . w = f i i g h t s , o p = s h i f t \rangle } \\ { \langle s _ { 2 } . w = c a n c e l e d , o p = s h i f t \rangle } \\ { \langle s _ { 1 } . t = N N S , o p = s h i f t \rangle } \\ { \langle s _ { 2 } . t = V B D , o p = s h i f t \rangle } \\ { \langle b _ { 1 } . w = t o , o p = s h i f t \rangle } \\ { \langle b _ { 1 } . t = T O , o p = s h i f t \rangle } \\ { \langle b _ { 1 } . w t = f i i g h t s N S , o p = s h i f t \rangle } \end{array}
$$

Given that the left and right arc transitions operate on the top two elements of the stack, features that combine properties from these positions are even more useful. For example, a feature like $s _ { 1 } . t \circ s _ { 2 } . t$ concatenates the part of speech tag of the word at the top of the stack with the tag of the word beneath it.

$$
\langle s _ { 1 } . t \circ s _ { 2 } . t = N N S V B D , o p = s h i f t \rangle
$$

Given the training data and features, any classifier, like multinomial logistic regression or support vector machines, can be used.

# 19.2.3 A neural classifier

The oracle can also be implemented by a neural classifier. A standard architecture is simply to pass the sentence through an encoder, then take the presentation of the top 2 words on the stack and the first word of the buffer, concatenate them, and present to a feedforward network that predicts the transition to take (Kiperwasser and Goldberg, 2016; Kulmizev et al., 2019). Fig. 19.8 sketches this model. Learning can be done with cross-entropy loss.

![## Image Analysis: 76f7a07408d86ae1293d9a3105aecbec5763cad6a047757bc87988d994f48dc5.jpg

**Conceptual Understanding:**
The image conceptually represents the architecture of a neural network component, termed a 'Parser Oracle', within a larger system for natural language parsing, specifically transition-based dependency parsing. Its main purpose is to determine the optimal next 'Action' or 'transition' required to build a dependency parse tree for a given sentence. The image communicates the key idea that parsing decisions can be learned and predicted by a neural classifier that takes into account both local contextual features (top of stack and buffer) and globally encoded sentence representations (via the ENCODER).

**Content Interpretation:**
The image represents a neural classifier designed as a 'Parser Oracle' for a transition-based dependency parser. It illustrates how the parser determines the next 'Action' to build dependency relations. The core system shown is the 'Parser Oracle' which takes various inputs and processes them through neural network layers to output a specific parsing action. The image demonstrates a computational model where linguistic parsing decisions are made by a neural network, integrating global sentence context (via the ENCODER) with local parsing state (stack and buffer).

**Key Insights:**
The main takeaways from this image are: 1. A neural network-based 'Parser Oracle' is used to make decisions in a transition-based parser. 2. The parser's decisions ('Action') are derived from three key pieces of information: the first word in the 'Input buffer' ('w'), and the top two words on the 'Stack' ('s1' and 's2'). 3. These words are first converted into embeddings ('e(w)', 'e(s1)', 'e(s2)') which are influenced by a broader 'ENCODER' that processes the entire sentence ('w1' through 'w6'). 4. The neural network within the 'Parser Oracle' consists of an 'FFN' and a 'Softmax' layer, which classify the inputs into one of the predefined parsing actions: 'LEFTARC', 'RIGHTARC', or 'SHIFT'. 5. These 'Action' outputs directly contribute to building 'Dependency Relations', as exemplified by 'w3' being dependent on 'w2'. This indicates a data-driven approach to constructing syntactic structures.

**Document Context:**
This image, labeled as Figure 19.8 and described as a 'Neural classifier for the oracle for the transition-based parser' in Section 19.2.3, directly illustrates the architectural details of the neural classifier being discussed in the surrounding text. The accompanying text explains that 'The parser takes the top 2 words on the stack and the first word of the buffer, represents them by their encodings (from running the whole sentence through the encoder), concatenates the embeddings and passes through a softmax to choose a parser action (transition)'. The diagram provides a visual representation of these exact steps, showing the 'Input buffer' word 'w', 'Stack' words 's1' and 's2', their 'e(w)', 'e(s1)', 'e(s2)' encodings, the 'ENCODER' processing 'w1' through 'w6', and the 'FFN' and 'Softmax' layers leading to the 'Action' output (LEFTARC, RIGHTARC, SHIFT) which then affects 'Dependency Relations'. This visual directly supports and elaborates on the textual description of the parser's mechanism.

**Summary:**
The image illustrates the architecture of a neural classifier acting as a Parser Oracle for a transition-based parser. The process begins with an 'ENCODER' which processes an entire sequence of input words, explicitly labeled 'w1', 'w2', 'w3', 'w4', 'w5', 'w6'. The output of this 'ENCODER' feeds into the main 'Parser Oracle' component. Simultaneously, the Parser Oracle takes specific inputs from an 'Input buffer' and a 'Stack'. The 'Input buffer' contains a word 'w' and ellipses '...' indicating other words, with 'w' being explicitly fed to the Parser Oracle. The 'Stack' contains words 's1', 's2', and ellipses '...', with 's1' and 's2' being explicitly fed to the Parser Oracle. Inside the Parser Oracle, these three specific inputs ('w', 's1', 's2') are transformed into their respective embeddings: 'e(w)', 'e(s1)', and 'e(s2)'. These embeddings are then fed into a 'FFN' (Feedforward Network) layer, which in turn feeds into a 'Softmax' layer. The 'Softmax' layer produces an 'Action' as its output. The possible actions are 'LEFTARC', 'RIGHTARC', and 'SHIFT'. This 'Action' then influences the 'Dependency Relations', which is visually represented by an example dependency where 'w3' is shown to have a dependency on 'w2' via an arrow pointing from 'w3' to 'w2'. The overall flow demonstrates how a neural network processes contextual word embeddings along with current parser state information to predict the next parsing action.](images/76f7a07408d86ae1293d9a3105aecbec5763cad6a047757bc87988d994f48dc5.jpg)
Figure 19.8 Neural classifier for the oracle for the transition-based parser. The parser takes the top 2 words on the stack and the first word of the buffer, represents them by their encodings (from running the whole sentence through the encoder), concatenates the embeddings and passes through a softmax to choose a parser action (transition).

# 19.2.4 Advanced Methods in Transition-Based Parsing

The basic transition-based approach can be elaborated in a number of ways to improve performance by addressing some of the most obvious flaws in the approach.

# Alternative Transition Systems

The arc-standard transition system described above is only one of many possible systems. A frequently used alternative is the arc eager transition system. The arc eager approach gets its name from its ability to assert rightward relations much sooner than in the arc standard approach. To see this, let’s revisit the arc standard trace of Example 19.9, repeated here.

![## Image Analysis: e24442ee96cbd48618f3d09173fa64c086a9d7221dfe8164b755df3a25c5c4ac.jpg

**Conceptual Understanding:**
This image represents a partial dependency parse tree, a linguistic data structure that shows grammatical relationships between words in a sentence. Conceptually, it illustrates how a central 'root' word or phrase can establish connections to other grammatical components, such as objects ('obj') and nominal modifiers ('nmod'). The main purpose is to visualize the syntactic structure of a sentence, making explicit the head-dependent relationships between its constituent elements. The key ideas communicated are hierarchical linguistic dependencies and the categorization of grammatical roles using labels like 'root', 'obj', 'det', 'nmod', and 'case'.

**Content Interpretation:**
This image illustrates a dependency parse tree, a graphical representation used in natural language processing and linguistics to show grammatical relationships between words in a sentence. It specifically demonstrates a fragment of how a 'root' word or phrase might relate to its 'obj' (object) and 'nmod' (nominal modifier) dependents. The 'obj' further depends on a 'det' (determiner), and the 'nmod' depends on a 'case' marker. This depicts a hierarchical structure where words or phrases (represented abstractly by the nodes) are linked by grammatical relations, showing which word modifies or depends on another. The significance lies in showing the syntactic structure of a sentence, which is crucial for understanding its meaning and for tasks like machine translation, information extraction, and sentiment analysis. The extracted text elements 'root', 'obj', 'det', 'nmod', and 'case' are standard labels for grammatical dependencies or parts of speech in dependency parsing frameworks.

**Key Insights:**
The main takeaway from this image is an understanding of how dependency parse trees visually represent grammatical relationships in a hierarchical manner. It teaches that a 'root' element can have direct dependents like an 'obj' and an 'nmod', and these dependents can, in turn, have their own dependents (e.g., 'det' for 'obj', 'case' for 'nmod'). This system provides a structured way to analyze sentence syntax, showing 'who did what to whom, where, and when' by mapping out the dependencies. The specific text elements 'root', 'obj', 'det', 'nmod', and 'case' are key concepts in linguistic analysis, highlighting the roles and types of modifications within a sentence structure. For instance, identifying an 'obj' helps pinpoint the recipient of an action, while 'nmod' provides additional descriptive information, often related to prepositions ('case').

**Document Context:**
Given the document context 'Alternative Transition Systems' and the following text 'Book the flight through Houston', this image likely serves as an example of how linguistic structures, specifically dependency relationships, can be analyzed or represented. In the context of 'Alternative Transition Systems,' this diagram might be illustrating a method for parsing sentences to understand the 'transition' or relationship between words, perhaps in a system designed to process or generate natural language. For instance, understanding the dependency parse of 'Book the flight through Houston' would involve identifying 'Book' as the root, 'flight' as the object, 'the' as a determiner for 'flight', 'Houston' as a nominal modifier (or similar) indicating the location, and 'through' as its case marker (preposition). The diagram provides a conceptual or abstract representation of these types of grammatical connections.

**Summary:**
The image displays a partial dependency parse tree, a common representation in computational linguistics to show grammatical relationships between words in a sentence. It illustrates a hierarchical structure starting from a 'root' node. The 'root' node directly relates to an 'obj' (object) node and an 'nmod' (nominal modifier) node. The 'obj' node, in turn, has a dependent 'det' (determiner) node. Similarly, the 'nmod' node has a dependent 'case' node. Each connection is represented by a curved arrow, indicating a dependency relationship where the node at the tail of the arrow depends on the node at the head. Downward arrows from 'root', 'det', and 'case' suggest these nodes would connect to specific words in a sentence, which are not depicted in this excerpt. The diagram visually explains how different grammatical components are linked.](images/e24442ee96cbd48618f3d09173fa64c086a9d7221dfe8164b755df3a25c5c4ac.jpg)
Book the flight through Houston

Consider the dependency relation between book and flight in this analysis. As is shown in Fig. 19.7, an arc-standard approach would assert this relation at Step 8, despite the fact that book and flight first come together on the stack much earlier at Step 4. The reason this relation can’t be captured at this point is due to the presence of the postnominal modifier through Houston. In an arc-standard approach, dependents are removed from the stack as soon as they are assigned their heads. If flight had been assigned book as its head in Step 4, it would no longer be available to serve as the head of Houston.

While this delay doesn’t cause any issues in this example, in general the longer a word has to wait to get assigned its head the more opportunities there are for something to go awry. The arc-eager system addresses this issue by allowing words to be attached to their heads as early as possible, before all the subsequent words dependent on them have been seen. This is accomplished through minor changes to the LEFTARC and RIGHTARC operators and the addition of a new REDUCE operator.

• LEFTARC: Assert a head-dependent relation between the word at the front of the input buffer and the word at the top of the stack; pop the stack.   
• RIGHTARC: Assert a head-dependent relation between the word on the top of the stack and the word at the front of the input buffer; shift the word at the front of the input buffer to the stack.   
• SHIFT: Remove the word from the front of the input buffer and push it onto the stack.   
• REDUCE: Pop the stack.

The LEFTARC and RIGHTARC operators are applied to the top of the stack and the front of the input buffer, instead of the top two elements of the stack as in the arc-standard approach. The RIGHTARC operator now moves the dependent to the stack from the buffer rather than removing it, thus making it available to serve as the head of following words. The new REDUCE operator removes the top element from the stack. Together these changes permit a word to be eagerly assigned its head and still allow it to serve as the head for later dependents. The trace shown in Fig. 19.9 illustrates the new decision sequence for this example.

In addition to demonstrating the arc-eager transition system, this example demonstrates the power and flexibility of the overall transition-based approach. We were able to swap in a new transition system without having to make any changes to the

<table><tr><td>Step</td><td>Stack</td><td>Word List</td><td>Action</td><td>Relation Added</td></tr><tr><td>0 1 2</td><td>[root] [root, book] [root, book, the] [root, book] [root, book, flight]</td><td>[book, the, flight, through, houston] [the, flight, through, houston] [flight, through, houston] [flight, through, houston] [through, houston] [root, book, flight, through] [houston]</td><td>RIGHTARC SHIFT LEFTARC RIGHTARC</td><td>(root-→book) (the ← flight) (book → flight)</td></tr></table>

Figure 19.9 A processing trace of Book the flight through Houston using the arc-eager transition operators.

beam search

underlying parsing algorithm. This flexibility has led to the development of a diverse set of transition systems that address different aspects of syntax and semantics including: assigning part of speech tags (Choi and Palmer, 2011a), allowing the generation of non-projective dependency structures (Nivre, 2009), assigning semantic roles (Choi and Palmer, 2011b), and parsing texts containing multiple languages (Bhat et al., 2017).

# Beam Search

The computational efficiency of the transition-based approach discussed earlier derives from the fact that it makes a single pass through the sentence, greedily making decisions without considering alternatives. Of course, this is also a weakness – once a decision has been made it can not be undone, even in the face of overwhelming evidence arriving later in a sentence. We can use beam search to explore alternative decision sequences. Recall from Chapter 9 that beam search uses a breadth-first search strategy with a heuristic filter that prunes the search frontier to stay within a fixed-size beam width.

In applying beam search to transition-based parsing, we’ll elaborate on the algorithm given in Fig. 19.5. Instead of choosing the single best transition operator at each iteration, we’ll apply all applicable operators to each state on an agenda and then score the resulting configurations. We then add each of these new configurations to the frontier, subject to the constraint that there has to be room within the beam. As long as the size of the agenda is within the specified beam width, we can add new configurations to the agenda. Once the agenda reaches the limit, we only add new configurations that are better than the worst configuration on the agenda (removing the worst element so that we stay within the limit). Finally, to insure that we retrieve the best possible state on the agenda, the while loop continues as long as there are non-final states on the agenda.

The beam search approach requires a more elaborate notion of scoring than we used with the greedy algorithm. There, we assumed that the oracle would be a supervised classifier that chose the best transition operator based on features of the current configuration. This choice can be viewed as assigning a score to all the possible transitions and picking the best one.

$$
\hat { T } ( c ) = \operatorname * { a r g m a x } \operatorname { S c o r e } ( t , c )
$$

With beam search we are now searching through the space of decision sequences, so it makes sense to base the score for a configuration on its entire history. So we can define the score for a new configuration as the score of its predecessor plus the

score of the operator used to produce it.

$$
\begin{array} { l } { { C o n f i g S c o r e ( c _ { 0 } ) \ = \ 0 . 0 } } \\ { { C o n f i g S c o r e ( c _ { i } ) \ = \ C o n f i g S c o r e ( c _ { i - 1 } ) + S c o r e ( t _ { i } , c _ { i - 1 } ) } } \end{array}
$$

This score is used both in filtering the agenda and in selecting the final answer. The new beam search version of transition-based parsing is given in Fig. 19.10.

<table><tr><td>function DEPENDENCYBEAMPARSE(words, width) returns dependency tree</td></tr><tr><td>state ←{[root],[words],[],O.O};initial configuration agenda← (state) ;initial agenda</td></tr><tr><td>while agenda contains non-final states</td></tr><tr><td>newagenda← (&gt;</td></tr><tr><td>for each state E agenda do</td></tr><tr><td>for all {t| t ∈ VALIDOPERATORS(state)} do</td></tr><tr><td>child←APPLY(t,state) newagenda←ADDToBEAM(child,newagenda,width)</td></tr><tr><td>agenda←newagenda</td></tr><tr><td>return BESTOF(agenda)</td></tr><tr><td>function ADDToBEAM(state, agenda, width) returns updated agenda</td></tr><tr><td>if LENGTH(agenda) &lt;width then</td></tr><tr><td>agenda←INsERT(state,agenda)</td></tr><tr><td>else if SCORE(state) &gt; SCORE(WORSTOF(agenda))</td></tr><tr><td>agenda←REMOVE(WORSTOF(agenda))</td></tr><tr><td>agenda←INsERT(state, agenda)</td></tr><tr><td>return agenda</td></tr><tr><td></td></tr></table>

# 19.3 Graph-Based Dependency Parsing

Graph-based methods are the second important family of dependency parsing algorithms. Graph-based parsers are more accurate than transition-based parsers, especially on long sentences; transition-based methods have trouble when the heads are very far from the dependents (McDonald and Nivre, 2011). Graph-based methods avoid this difficulty by scoring entire trees, rather than relying on greedy local decisions. Furthermore, unlike transition-based approaches, graph-based parsers can produce non-projective trees. Although projectivity is not a significant issue for English, it is definitely a problem for many of the world’s languages.

Graph-based dependency parsers search through the space of possible trees for a given sentence for a tree (or trees) that maximize some score. These methods encode the search space as directed graphs and employ methods drawn from graph theory to search the space for optimal solutions. More formally, given a sentence $s$ we’re looking for the best dependency tree in ${ \mathcal G } _ { s }$ , the space of all possible trees for that sentence, that maximizes some score.

$$
{ \hat { T } } ( S ) = \mathop { \underset { t \in { \mathcal { G } } _ { S } } { \operatorname { a r g m a x } } } \operatorname { S c o r e } ( t , S )
$$

# edge-factored

We’ll make the simplifying assumption that this score can be edge-factored, meaning that the overall score for a tree is the sum of the scores of each of the scores of the edges that comprise the tree.

$$
\operatorname { S c o r e } ( t , S ) = \sum _ { e \in t } \operatorname { S c o r e } ( e )
$$

Graph-based algorithms have to solve two problems: (1) assigning a score to each edge, and (2) finding the best parse tree given the scores of all potential edges. In the next few sections we’ll introduce solutions to these two problems, beginning with the second problem of finding trees, and then giving a feature-based and a neural algorithm for solving the first problem of assigning scores.

# 19.3.1 Parsing via finding the maximum spanning tree

In graph-based parsing, given a sentence $s$ we start by creating a graph $G$ which is a fully-connected, weighted, directed graph where the vertices are the input words and the directed edges represent all possible head-dependent assignments. We’ll include an additional ROOT node with outgoing edges directed at all of the other vertices. The weights of each edge in $G$ reflect the score for each possible head-dependent relation assigned by some scoring algorithm.

It turns out that finding the best dependency parse for $s$ is equivalent to finding the maximum spanning tree over $G$ . A spanning tree over a graph $G$ is a subset of $G$ that is a tree and covers all the vertices in G; a spanning tree over $G$ that starts from the ROOT is a valid parse of $s$ . A maximum spanning tree is the spanning tree with the highest score. Thus a maximum spanning tree of $G$ emanating from the ROOT is the optimal dependency parse for the sentence.

A directed graph for the example Book that flight is shown in Fig. 19.11, with the maximum spanning tree corresponding to the desired parse shown in blue. For ease of exposition, we’ll describe here the algorithm for unlabeled dependency parsing.

![## Image Analysis: 396c78793036f0c2e5c0246ba517cd54bd072a12b80d4671a297f4a39fb79caa.jpg

**Conceptual Understanding:**
This image conceptually represents the initial stage of a dependency parsing process for the sentence "Book that flight". It illustrates how words in a sentence, along with a 'root' element, can be modeled as nodes in a directed graph. The main purpose is to show all possible directed relationships (dependencies) between these words, each assigned a numerical weight indicating the strength or likelihood of that particular dependency. The graph serves as the foundation upon which algorithms, such as those finding a maximum spanning tree, will operate to determine the most plausible syntactic structure of the sentence.

**Content Interpretation:**
The image shows a directed graph where nodes represent words from the sentence "Book that flight" and a special "root" node. The edges between nodes are directed, indicating a potential dependency or relationship, and are assigned numerical weights. The weights likely represent scores, probabilities, or costs associated with that particular dependency. The presence of both forward and backward edges between words suggests that dependencies can flow in multiple directions (e.g., "Book" to "that" and "that" to "Book"). The purple highlighted edges (root -> Book with weight 12, Book -> flight with weight 7, and flight -> Book with weight 7) indicate a selected or preferred set of dependencies, possibly forming part of a maximum spanning tree or a significant path within the graph. This visual representation is a common method in computational linguistics for parsing sentences by modeling possible relationships between words.

**Key Insights:**
1.  **Sentence Representation for Parsing:** Sentences can be represented as directed, weighted graphs where words are nodes and potential syntactic dependencies are weighted edges. This allows for computational analysis of sentence structure.
2.  **Root Node Importance:** A "root" node is used to establish a central starting point for dependencies, often representing the main predicate or starting point of the parse tree.
3.  **Weighted Dependencies:** The numerical weights on edges are crucial for parsing algorithms. Higher weights typically imply stronger or more probable dependencies, influencing which connections are chosen for the final parse.
4.  **Directionality of Dependencies:** Edges are directed, indicating the head-dependent relationship (e.g., which word modifies or governs another).
5.  **Highlighting of Significant Paths:** The purple edges (root -> Book with 12, Book -> flight with 7, flight -> Book with 7) suggest a specific parse or a set of strong dependencies that could be part of the final syntactic structure of "Book that flight", although the edge from flight to Book might suggest a more complex or non-projective dependency if interpreted as a linear sentence parse.

**Document Context:**
The image directly illustrates the concept of an "Initial rooted, directed graph" as mentioned in the section title "19.3.1 Parsing via finding the maximum spanning tree" and the caption "Figure 19.11 Initial rooted, directed graph for Book that flight." It provides a concrete example of how the sentence "Book that flight" is transformed into a graph structure for the purpose of dependency parsing. The graph serves as the input for an algorithm (likely a maximum spanning tree algorithm) that will identify the most probable or structurally sound parse tree for the sentence, by selecting a subset of these weighted edges. The highlighted purple edges might represent the edges selected by such an algorithm in an initial or intermediate step, or a specific interpretation of the sentence.

**Summary:**
This image displays a rooted, directed graph representing the phrase "Book that flight". The graph consists of four circular nodes: "root", "Book", "that", and "flight". These nodes are interconnected by directed edges, each with an associated numerical weight. The edges indicate relationships or dependencies between the words. The purple colored edges represent a specific, highlighted path or set of connections within the graph. 

Starting from the "root" node:
- An edge goes from "root" to "Book" with a weight of 12 (colored purple).
- An edge goes from "root" to "that" with a weight of 4.
- An edge goes from "root" to "flight" with a weight of 4.

Interconnections between the other words:
- An edge goes from "Book" to "that" with a weight of 5.
- An edge goes from "Book" to "flight" with a weight of 7 (colored purple).
- An edge goes from "that" to "Book" with a weight of 6.
- An edge goes from "that" to "flight" with a weight of 8.
- An edge goes from "flight" to "Book" with a weight of 7 (colored purple).
- An edge goes from "flight" to "that" with a weight of 5.

The overall structure shows a network of potential syntactic dependencies or semantic relationships, with specific weights assigned to each possible connection, and a particular set of connections highlighted in purple.](images/396c78793036f0c2e5c0246ba517cd54bd072a12b80d4671a297f4a39fb79caa.jpg)
Figure 19.11 Initial rooted, directed graph for Book that flight.

Before describing the algorithm it’s useful to consider two intuitions about directed graphs and their spanning trees. The first intuition begins with the fact that every vertex in a spanning tree has exactly one incoming edge. It follows from this that every connected component of a spanning tree (i.e., every set of vertices that are linked to each other by paths over edges) will also have one incoming edge. The second intuition is that the absolute values of the edge scores are not critical to determining its maximum spanning tree. Instead, it is the relative weights of the edges entering each vertex that matters. If we were to subtract a constant amount from each edge entering a given vertex it would have no impact on the choice of the maximum spanning tree since every possible spanning tree would decrease by exactly the same amount.

The first step of the algorithm itself is quite straightforward. For each vertex in the graph, an incoming edge (representing a possible head assignment) with the highest score is chosen. If the resulting set of edges produces a spanning tree then we’re done. More formally, given the original fully-connected graph $G = \left( V , E \right)$ , a subgraph $T = \left( V , F \right)$ is a spanning tree if it has no cycles and each vertex (other than the root) has exactly one edge entering it. If the greedy selection process produces such a tree then it is the best possible one.

Unfortunately, this approach doesn’t always lead to a tree since the set of edges selected may contain cycles. Fortunately, in yet another case of multiple discovery, there is a straightforward way to eliminate cycles generated during the greedy selection phase. Chu and Liu (1965) and Edmonds (1967) independently developed an approach that begins with greedy selection and follows with an elegant recursive cleanup phase that eliminates cycles.

The cleanup phase begins by adjusting all the weights in the graph by subtracting the score of the maximum edge entering each vertex from the score of all the edges entering that vertex. This is where the intuitions mentioned earlier come into play. We have scaled the values of the edges so that the weights of the edges in the cycle have no bearing on the weight of any of the possible spanning trees. Subtracting the value of the edge with maximum weight from each edge entering a vertex results in a weight of zero for all of the edges selected during the greedy selection phase, including all of the edges involved in the cycle.

Having adjusted the weights, the algorithm creates a new graph by selecting a cycle and collapsing it into a single new node. Edges that enter or leave the cycle are altered so that they now enter or leave the newly collapsed node. Edges that do not touch the cycle are included and edges within the cycle are dropped.

Now, if we knew the maximum spanning tree of this new graph, we would have what we need to eliminate the cycle. The edge of the maximum spanning tree directed towards the vertex representing the collapsed cycle tells us which edge to delete in order to eliminate the cycle. How do we find the maximum spanning tree of this new graph? We recursively apply the algorithm to the new graph. This will either result in a spanning tree or a graph with a cycle. The recursions can continue as long as cycles are encountered. When each recursion completes we expand the collapsed vertex, restoring all the vertices and edges from the cycle with the exception of the single edge to be deleted.

Putting all this together, the maximum spanning tree algorithm consists of greedy edge selection, re-scoring of edge costs and a recursive cleanup phase when needed. The full algorithm is shown in Fig. 19.12.

Fig. 19.13 steps through the algorithm with our Book that flight example. The first row of the figure illustrates greedy edge selection with the edges chosen shown in blue (corresponding to the set $F$ in the algorithm). This results in a cycle between that and flight. The scaled weights using the maximum value entering each node are shown in the graph to the right.

Collapsing the cycle between that and flight to a single node (labelled $t f )$ and recursing with the newly scaled costs is shown in the second row. The greedy selection step in this recursion yields a spanning tree that links root to book, as well as an edge that links book to the contracted node. Expanding the contracted node, we can see that this edge corresponds to the edge from book to flight in the original graph. This in turn tells us which edge to drop to eliminate the cycle.

![## Image Analysis: 96132db9fbb2c9e158994ae5c51ac87588f108774c2545d635162ff4c9b0fc9e.jpg

**Conceptual Understanding:**
This image represents the pseudo-code for the Chu-Liu Edmonds algorithm. Conceptually, it illustrates a greedy approach combined with a recursive strategy to find a maximum spanning tree (also known as a maximum spanning arborescence) in a *weighted directed graph*. Unlike undirected graphs where algorithms like Kruskal's or Prim's are used, directed graphs require a more complex approach due to edge directionality and the concept of 'incoming' edges for a spanning tree.

The main purpose of this algorithm is to outline the computational steps involved in identifying a subset of edges that connect all vertices in a directed graph to a specified root, such that the sum of the edge weights (scores) is maximized, and no cycles are formed. It systematically addresses the problem of cycles by contracting them into a single 'super-node' and recursively solving the problem on this simplified graph, then expanding the solution back into the original graph.

Key ideas or concepts being communicated include:
*   **Greedy Selection:** Initially selecting the highest-scoring incoming edge for each vertex.
*   **Score Adjustment:** A mechanism to re-weight edges to correctly handle the presence of cycles without losing the 'maximum' property.
*   **Graph Contraction:** Simplifying the graph by collapsing cycles into single nodes.
*   **Recursion:** Solving sub-problems on contracted graphs.
*   **Graph Expansion:** Reconstructing the full solution by 'uncontracting' the graph.
*   **Arborescence:** The specific type of directed spanning tree it constructs, where every node (except the root) has exactly one incoming edge.

**Content Interpretation:**
The image displays pseudo-code for the Chu-Liu Edmonds algorithm, designed to find a maximum spanning tree (or arborescence) in a weighted directed graph. It illustrates a complex greedy algorithm with a recursive cycle-handling mechanism.

**Processes Shown:**
*   **Initialization:** Setting up empty data structures (`F`, `T'`, `score'`).
*   **Greedy Edge Selection:** Iterating `for each v ∈ V` to find the `bestInEdge ← argmax_{e=(u,v) ∈ E} score[e]` for each vertex `v` and adding it to `F`.
*   **Score Adjustment:** Iterating `for each e=(u,v) ∈ E` to calculate `score'[e] ← score[e] - score[bestInEdge]`, modifying edge weights to account for potential cycles.
*   **Spanning Tree Check:** A conditional `if T=(V,F) is a spanning tree then` to determine if the current edge set forms a valid spanning tree.
*   **Cycle Detection and Contraction:** If not a spanning tree (`else`), a `cycle C` is identified in `F`, and the graph `G` is transformed into a `contracted graph G'` using the `CONTRACT(G, C)` function.
*   **Recursive Call:** The `MAXSPANNINGTREE` algorithm is called recursively on the `contracted graph G'` with the `adjusted score'`.
*   **Expansion:** The resulting tree `T'` from the contracted graph is then `EXPAND`ed back into the original graph context using `EXPAND(T', C)`.
*   **Return Values:** The function returns the final `spanning tree` `T`.

**Concepts and Relationships:**
*   **Greedy Algorithm:** The initial selection of `bestInEdge` for each vertex demonstrates a greedy approach.
*   **Directed Graph Spanning Tree (Arborescence):** The algorithm specifically addresses directed graphs, where the concept of an incoming edge is crucial.
*   **Cycle Resolution:** The core mechanism for handling cycles involves `CONTRACT`ing the cycle into a single node, recursively solving the problem on the simplified graph, and then `EXPAND`ing the solution.
*   **Recursion:** The algorithm is inherently recursive, calling itself on a smaller, modified version of the graph until a base case (no cycles) is met.
*   **Score Weighting:** Edge scores are essential for determining the 

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Iterative Greedy Selection:** The algorithm begins by making a greedy choice for each vertex, selecting the highest-scoring incoming edge (`bestInEdge ← argmax_{e=(u,v) ∈ E} score[e]`). This establishes a set of candidate edges `F`.
2.  **Score Re-weighting for Cycle Handling:** A crucial insight is the adjustment of edge scores (`score'[e] ← score[e] - score[bestInEdge]`). This re-weighting prepares the graph for cycle resolution by ensuring that the relative costs/benefits of edges are maintained even after cycle contraction, which is vital for finding the global maximum.
3.  **Recursive Contraction-Expansion Strategy for Cycles:** The primary mechanism to deal with cycles in the initial greedy selection is a recursive `CONTRACT` and `EXPAND` strategy:
    *   **Contraction:** If `F` forms a cycle, the `CONTRACT(G, C)` function effectively collapses the cycle `C` into a single 'super-vertex,' simplifying the graph (`G'`).
    *   **Recursive Solution:** The `MAXSPANNINGTREE` is then recursively applied to this smaller, `contracted graph G'` with adjusted scores (`score'`). This demonstrates a divide-and-conquer approach.
    *   **Expansion:** The solution `T'` from the contracted graph is 'uncontracted' by `EXPAND(T', C)` back into the original graph context, carefully selecting edges from the original cycle to maintain a spanning tree property while preserving maximality.
4.  **Applicability to Dependency Parsing:** The algorithm is highly relevant to dependency parsing in NLP, where finding the maximum spanning tree of words and their potential dependencies is a common method for determining syntactic structure.

**Conclusions/Insights:**
*   The Chu-Liu Edmonds algorithm effectively extends the concept of finding maximum spanning trees from undirected graphs to weighted directed graphs, crucial for problems where directionality matters (like dependency relations).
*   Its recursive nature, combined with the intelligent score adjustment and contraction/expansion steps, provides a robust method to resolve cycles that would otherwise prevent the formation of a tree structure.
*   The algorithm prioritizes edge weights (scores) at each step, ensuring the 'maximum' property of the resulting spanning tree.

**Document Context:**
This code snippet is directly relevant to Section 19.3.1, 'Parsing via finding the maximum spanning tree.' In computational linguistics, specifically dependency parsing, the task of identifying the grammatical structure of a sentence can be modeled as finding the maximum spanning arborescence (a directed tree) in a graph where words are nodes and potential dependency relations are directed edges with scores indicating their likelihood. The Chu-Liu Edmonds algorithm provides an efficient method to solve this problem, making it a foundational component for many dependency parsers. The algorithm's ability to handle weighted directed graphs and ensure a tree structure without cycles is crucial for correctly identifying syntactic dependencies within a sentence.

**Summary:**
This image presents the pseudo-code for the `MAXSPANNINGTREE` algorithm, also known as the Chu-Liu Edmonds algorithm. It is used to find a maximum spanning tree (or arborescence) in a weighted directed graph, a common problem in fields like natural language processing for tasks such as dependency parsing.

The algorithm works by iteratively building a set of edges and resolving any cycles that may form. Let's break down the main `MAXSPANNINGTREE` function step-by-step, along with its helper functions, `CONTRACT` and `EXPAND`:

1.  **Function Definition:** The algorithm starts with `function MAXSPANNINGTREE(G=(V,E), root, score) returns spanning tree`. This means it takes a directed graph `G` (defined by its set of vertices `V` and edges `E`), a designated `root` vertex, and a `score` mapping for each edge (representing its weight or desirability). It aims to `return` a `spanning tree`.

2.  **Initialization:**
    *   `F ← []`: An empty list `F` is created. This list will accumulate the best incoming edges for each vertex.
    *   `T' ← []`: Another empty list `T'` is initialized, which will temporarily hold a spanning tree from a recursively solved, contracted graph.
    *   `score' ← []`: An empty list `score'` is set up to store adjusted edge scores.

3.  **First Greedy Selection (Identifying Best Incoming Edges):**
    *   The code enters a loop: `for each v ∈ V do`. For every vertex `v` in the graph:
        *   `bestInEdge ← argmax_{e=(u,v) ∈ E} score[e]`: It identifies the `bestInEdge`, which is the edge `e` (from any vertex `u` to the current vertex `v`) that has the highest `score`. This is a greedy choice, picking the "strongest" incoming connection for each vertex.
        *   `F ← F ∪ bestInEdge`: This `bestInEdge` is then added to the set `F`. After this loop completes, `F` will contain one incoming edge for every vertex in the graph.

4.  **Edge Score Adjustment:**
    *   A second loop runs: `for each e=(u,v) ∈ E do`. For every edge `e` in the entire graph:
        *   `score'[e] ← score[e] - score[bestInEdge]`: The `score'` of this edge `e` is calculated by subtracting the `score` of the `bestInEdge` (which was found for `v`, the target of `e`) from the original `score` of `e`. This adjustment is crucial for handling cycles correctly in the subsequent steps.

5.  **Cycle Check and Recursive Process:**
    *   **Decision Point:** `if T=(V,F) is a spanning tree then return it`. The algorithm checks if the graph formed by all vertices `V` and the collected edges `F` (which has exactly one incoming edge for each non-root vertex) is already a valid spanning tree (an arborescence).
        *   **If Yes:** If `F` forms a spanning tree, it means no cycles were formed during the greedy selection. In this ideal case, the algorithm has found the maximum spanning tree, and `return it`.
        *   **If No (else):** If `F` does *not* form a spanning tree, it means that at least one cycle exists within `F`. The algorithm proceeds to handle this cycle recursively:
            *   `C ← a cycle in F`: A specific `cycle C` is identified from the edges in `F`.
            *   `G' ← CONTRACT(G, C)`: The `CONTRACT` function is called. This function effectively shrinks the entire `cycle C` into a single "super-vertex," creating a `contracted graph G'`. This simplifies the problem.
            *   `T' ← MAXSPANNINGTREE(G', root, score')`: The `MAXSPANNINGTREE` function then calls itself recursively, but this time on the smaller, `contracted graph G'` and using the previously `adjusted score'`s. This recursive call finds a maximum spanning tree `T'` within the contracted graph.
            *   `T ← EXPAND(T', C)`: Once the recursive call returns `T'`, the `EXPAND` function is invoked. This function reverses the `CONTRACT`ion. It takes the tree `T'` from the simplified graph and the original `cycle C`, carefully "un-merging" the super-vertex back into the cycle while ensuring that the final graph `T` is still a valid maximum spanning tree in the original graph context.
            *   `return T`: The final, expanded maximum spanning tree `T` is then returned.

**Helper Functions Explained:**
*   `function CONTRACT(G, C) returns contracted graph`: This function's role is to take the original graph `G` and a detected `cycle C`, and transform `G` into a new, smaller `contracted graph`. All vertices within `C` are effectively merged into a single representative node.
*   `function EXPAND(T, C) returns expanded graph`: This function takes a spanning tree `T` (which was found in a contracted graph) and the original `cycle C`. Its purpose is to reconstruct the original structure by replacing the contracted super-vertex with the full `cycle C` and making the appropriate choice of which edge from the cycle should be part of the final spanning tree, thus `returns expanded graph`.

In summary, the Chu-Liu Edmonds algorithm efficiently finds the highest-scoring directed spanning tree by iteratively selecting best incoming edges, adjusting scores to account for cycles, and recursively contracting and expanding cycles until a valid, optimal tree is formed.](images/96132db9fbb2c9e158994ae5c51ac87588f108774c2545d635162ff4c9b0fc9e.jpg)
Figure 19.12 The Chu-Liu Edmonds algorithm for finding a maximum spanning tree in a weighted directed graph.

On arbitrary directed graphs, this version of the CLE algorithm runs in $O ( m n )$ time, where $m$ is the number of edges and $n$ is the number of nodes. Since this particular application of the algorithm begins by constructing a fully connected graph $m = n ^ { 2 }$ yielding a running time of $O ( n ^ { 3 } )$ . Gabow et al. (1986) present a more efficient implementation with a running time of $O ( m + n l o g n )$ .

# 19.3.2 A feature-based algorithm for assigning scores

Recall that given a sentence, $s$ , and a candidate tree, $T$ , edge-factored parsing models make the simplification that the score for the tree is the sum of the scores of the edges that comprise the tree:

$$
\operatorname { s c o r e } ( S , T ) ~ = ~ \sum _ { e \in T } \operatorname { s c o r e } ( S , e )
$$

In a feature-based algorithm we compute the edge score as a weighted sum of features extracted from it:

$$
\operatorname { s c o r e } ( S , e ) \ = \ \sum _ { i = 1 } ^ { N } w _ { i } f _ { i } ( S , e )
$$

Or more succinctly.

$$
\operatorname { s c o r e } ( S , e ) ~ = ~ w \cdot f
$$

Given this formulation, we need to identify relevant features and train the weights.

The features (and feature combinations) used to train edge-factored models mirror those used in training transition-based parsers, such as

![## Image Analysis: 13cd8c671e9a28d9e21f3628b4a12328f7242e69a055e7fbb7ca97a1b85c5750.jpg

**Conceptual Understanding:**
This image conceptually illustrates the Chu-Liu-Edmonds algorithm, a method used for finding a maximum spanning arborescence (a directed tree with a unique root) in a directed graph. In the context of the document, which is about feature-based algorithms for assigning scores, this specific example applies to dependency parsing for the phrase 'Book that flight'. The main purpose is to demonstrate the step-by-step process of how this algorithm transforms an initial graph with weighted edges into a final dependency structure by iteratively adjusting weights, identifying cycles, and removing specific edges. The core idea communicated is how complex structural dependencies between words can be derived through a series of deterministic graph operations based on assigned scores.

**Content Interpretation:**
The image presents a series of five directed graphs, each representing a stage in a graph-based algorithm, likely the Chu-Liu-Edmonds algorithm, for dependency parsing or structure prediction. The nodes represent words ('root', 'Book', 'that', 'flight', 'tf'), and the directed edges represent potential dependencies between them. The numerical labels on the edges are weights or scores, which change across the graphs, reflecting modifications made by the algorithm. The blue-highlighted edges likely indicate the chosen or 'highest scoring' incoming edges to each node at different steps of the process, ultimately aiming to form a maximum spanning arborescence. The process involves identifying and collapsing cycles, and then deleting edges that are part of these cycles, as explicitly shown in the final graph with the annotation 'Deleted from cycle'. The 'tf' node in the middle graphs appears to be a contracted node, likely representing 'that flight' as a single entity after a cycle involving 'that' and 'flight' has been processed. The numbers inside the nodes ('12', '7', '8', '0', '-1') also change, potentially representing updated scores or internal states of the nodes.

**Key Insights:**
**Main Takeaways/Lessons:**
1.  **Iterative Nature of Graph Algorithms:** The sequence of five graphs clearly demonstrates the iterative application of an algorithm (Chu-Liu-Edmonds) to transform a graph, adjust weights, and refine a structure. This shows that complex linguistic or structural problems can be solved through sequential graph manipulations.
2.  **Importance of Edge Weights:** The numerical weights on the edges are critical, driving the algorithm's decisions. Their modification and the selection of edges based on these weights are central to forming the desired structure (e.g., maximum spanning arborescence).
3.  **Cycle Detection and Resolution:** The appearance of the 'Deleted from cycle' annotation in the final graph, and the implied cycle formation and collapse in the intermediate steps (e.g., forming 'tf' from 'that' and 'flight'), highlight the importance of cycle detection and resolution in graph-based dependency parsing algorithms.
4.  **Dependency Parsing Illustration:** The example 'Book that flight' showcases how a sentence's words can be represented as nodes, and potential grammatical dependencies as weighted edges, ultimately leading to a structured interpretation of the sentence.

**Evidence from Transcribed Text:**
*   **Graph 1:** Shows initial raw or assigned weights (e.g., 'root' to 'Book' with '12', 'that' to 'flight' with '8'), indicating the starting state of the scoring.
*   **Graph 2:** Displays a transformation where many weights become negative or zero (e.g., 'root' to 'Book' with '0', 'that' to 'flight' with '0'), suggesting an adjustment phase (e.g., 'cost' transformation or taking maximum incoming edge weights).
*   **Graph 3 & 4:** The introduction of the 'tf' node, replacing 'that' and 'flight', and the changing internal node values (e.g., 'Book -6' to 'Book 0', 'tf' to 'tf -1') strongly indicates a node contraction step, common in Edmonds' algorithm when a cycle is detected and collapsed.
*   **Graph 5:** The annotation 'Deleted from cycle' explicitly points to a key step in the Chu-Liu-Edmonds algorithm where an edge, previously part of a cycle, is removed to break the cycle and ensure an arborescence structure. The blue-highlighted edges here represent the resulting maximum spanning arborescence after this deletion.
*   **Consistent Node Labels:** The consistent use of 'root', 'Book', 'that', 'flight' across graphs provides textual evidence that these are the fixed elements being analyzed, while 'tf' represents a transient or collapsed form of 'that flight'.

**Document Context:**
The image is presented in Section 19.3.2, which is titled 'A feature-based algorithm for assigning scores'. The text after the image, 'Figure 19.13 Chu-Liu-Edmonds graph-based example for Book that flight', directly identifies the algorithm and the specific example sentence being processed. Therefore, this image serves as a concrete, step-by-step visual example of how the Chu-Liu-Edmonds algorithm works to assign scores and build a dependency structure for the sentence 'Book that flight'. It illustrates the graph transformations, weight adjustments, and cycle detection/deletion mechanisms that are central to this algorithm, making the abstract process comprehensible through a series of visual states.

**Summary:**
This image displays a sequence of five directed graphs, illustrating steps in a Chu-Liu-Edmonds graph-based algorithm, specifically applied to the phrase 'Book that flight'. Each graph shows nodes representing words ('root', 'Book', 'that', 'flight', 'tf') and directed edges connecting them, with numerical weights assigned to these edges. Some edges are highlighted in blue, indicating selected or preferred paths at various stages of the algorithm. The sequence of graphs demonstrates the iterative process of finding a maximum spanning arborescence by adjusting edge weights and identifying cycles for deletion. The final graph illustrates the resulting arborescence after a cycle has been identified and an edge 'Deleted from cycle'. The overall purpose is to show the dynamic computation of dependencies between words in a sentence.](images/13cd8c671e9a28d9e21f3628b4a12328f7242e69a055e7fbb7ca97a1b85c5750.jpg)
Figure 19.13 Chu-Liu-Edmonds graph-based example for Book that flight

• Wordforms, lemmas, and parts of speech of the headword and its dependent.   
• Corresponding features from the contexts before, after and between the words.   
• Word embeddings.   
• The dependency relation itself.   
• The direction of the relation (to the right or left).   
• The distance from the head to the dependent.

Given a set of features, our next problem is to learn a set of weights corresponding to each. Unlike many of the learning problems discussed in earlier chapters, here we are not training a model to associate training items with class labels, or parser actions. Instead, we seek to train a model that assigns higher scores to correct trees than to incorrect ones. An effective framework for problems like this is to use inference-based learning combined with the perceptron learning rule. In this framework, we parse a sentence (i.e, perform inference) from the training set using some initially random set of initial weights. If the resulting parse matches the corresponding tree in the training data, we do nothing to the weights. Otherwise, we find those features in the incorrect parse that are not present in the reference parse and we lower their weights by a small amount based on the learning rate. We do this incrementally for each sentence in our training data until the weights converge.

# 19.3.3 A neural algorithm for assigning scores

State-of-the-art graph-based multilingual parsers are based on neural networks. Instead of extracting hand-designed features to represent each edge between words $w _ { i }$ and $w _ { j }$ , these parsers run the sentence through an encoder, and then pass the encoded representation of the two words $w _ { i }$ and $w _ { j }$ through a network that estimates a score for the edge $i  j$ .

![## Image Analysis: 9067ff61fbbb30546702b10dc76455d098b2b2e5602d3a26f190574ed033988b.jpg

**Conceptual Understanding:**
This image conceptually represents a specific architecture within a neural network-based dependency parser, known as a biaffine parser. Its main purpose is to illustrate the mechanism by which a score is assigned to a potential syntactic dependency edge between two words in a sentence. The key idea being communicated is how word embeddings are transformed into specialized head and dependent representations, and then how these representations are combined via a biaffine transformation to yield a scalar score indicative of a dependency relationship.

**Content Interpretation:**
This image details the computational processes involved in scoring a dependency edge within a biaffine parser system. The relationships are established between input words, their encoded representations, their head/dependent specific embeddings, and finally the derived dependency score.

*   **Input Layer ("book", "that", "flight"):** These are the raw words from a sentence, serving as the initial input to the parsing system, implying the parser operates on sequences.
*   **ENCODER:** This component processes the words and produces "r1", "r2", "r3", signifying that words are first processed by an encoder (e.g., Bi-LSTM or Transformer) to generate contextualized word representations (embeddings). 'r' likely stands for 'raw' or 'encoded' representations.
*   **FFN head / FFN dep:** For each encoded representation (e.g., "r1"), two distinct Feedforward Networks are used: "FFN head" and "FFN dep". This indicates a critical step where a word's general contextualized embedding is transformed into two specialized vectors: one optimized for its role as a "head" (e.g., "h₁ head") and another for its role as a "dependent" (e.g., "h₁ dep"). This specialization allows the model to learn distinct features relevant to each role.
*   **h₁ head, h₃ dep (and hₓ head/dep):** These labels (e.g., "h₁ head", "h₁ dep", "h₂ head", "h₂ dep", "h₃ head", "h₃ dep") represent the specialized head and dependent embeddings for each word. The indices (1, 2, 3) correspond to the word's position, and the diagram highlights "h₁ head" and "h₃ dep" as inputs for the biaffine function to score a particular edge.
*   **Biaffine Layer (U, W, b, ⊕, Σ):** This shaded box labeled "Biaffine" represents the core mathematical operation for computing the score.
    *   "U" and "W" are typically matrices or tensors performing linear transformations on the head and dependent embeddings. The trapezoidal shape suggests a transformation or projection.
    *   "⊕" represents an element-wise addition, combining transformed head and dependent information.
    *   "b" is a bias term, adding a constant offset.
    *   "Σ" represents a summation or dot product combining the transformed embeddings and bias into a single scalar score.
*   **score(h₁ head, h₃ dep):** This final output is a scalar value, explicitly labeled as the score for the dependency edge where the first word's head representation ("h₁ head") and the third word's dependent representation ("h₃ dep") are involved. This quantifies the model's confidence in a dependency link between "book" (word 1) and "flight" (word 3), with "book" as the head and "flight" as the dependent. The diagram focuses on this specific edge.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Modular Neural Architecture:** The parsing process is broken down into distinct, sequential modules: an "ENCODER", multiple "FFN head" and "FFN dep" networks, and a "Biaffine" layer. This modularity allows for specialized processing at each stage.
2.  **Contextualized Word Representations:** Words are first processed by an "ENCODER" (producing "r1", "r2", "r3"), indicating that context is crucial. The output representations capture the word's meaning within its sentence.
3.  **Role-Specific Embeddings for Dependency Parsing:** A critical insight is the use of separate "FFN head" and "FFN dep" networks to transform the generic contextualized representations ("r") into "h head" and "h dep" vectors. This signifies that the features relevant for a word acting as a head are different from those relevant for it acting as a dependent, and the model learns these distinct representations. For example, "h₁ head" for "book" and "h₃ dep" for "flight" are explicitly shown as inputs for scoring.
4.  **Biaffine Transformation for Scoring:** The "Biaffine" layer, composed of "U", "W", "b", "⊕", and "Σ", is the mechanism for computing the dependency score. This specific type of transformation (biaffine) is powerful for modeling relationships between two distinct entities (head and dependent) and is a common technique in modern dependency parsers. The score "score(h₁ head, h₃ dep)" is the ultimate output of this calculation.
5.  **Targeted Edge Scoring:** The diagram specifically highlights the computation for one particular dependency edge, involving "h₁ head" and "h₃ dep". This suggests that such a parser computes scores for many potential head-dependent pairs and then selects the most probable ones to form the dependency tree.

The specific text elements like "ENCODER," "FFN head," "FFN dep," "h₁ head," "h₃ dep," "Biaffine," "U," "W," "b," "⊕," "Σ," and "score(h₁ head, h₃ dep)" directly provide evidence for these insights by naming the components and the transformations involved. They illustrate the decomposition of the complex task of dependency parsing into manageable, learnable steps within a neural network.

**Document Context:**
This image fits directly into the document's narrative by visually explaining "19.3.3 A neural algorithm for assigning scores," specifically for the biaffine parser mentioned in the text: "Figure 19.14 Computing scores for a single edge book flight) in the biaffine parser of Dozat and Manning (2017); Dozat et al. (2017)." It serves as a detailed diagram illustrating the computational steps involved in generating a dependency score, which is a fundamental operation within such a parser. The image answers the question of *how* a neural network determines the strength of a potential syntactic link between two words, like "book" and "flight."

**Summary:**
This diagram illustrates a neural network architecture, specifically a biaffine parser, used to calculate a score for a potential dependency relationship between two words in a sentence. Let's break down the process step-by-step:

1.  **Initial Word Input:** The process begins with a sequence of words from a sentence, for example, "book," "that," and "flight."
2.  **Encoding Context:** These individual words are first fed into an "ENCODER." This "ENCODER" could be a complex neural network layer (like an LSTM or Transformer) that processes the words in context and generates a rich numerical representation (an embedding) for each word. The outputs from this encoder are labeled "r1" for "book," "r2" for "that," and "r3" for "flight." These 'r' values capture the contextual meaning of each word.
3.  **Specializing Representations:** For each contextualized word representation (e.g., "r1"), the system then creates two specialized versions:
    *   A "head" representation, produced by a "FFN head" (Feedforward Network for heads). For "book," this results in "h₁ head."
    *   A "dependent" representation, produced by a "FFN dep" (Feedforward Network for dependents). For "book," this results in "h₁ dep."
    This step is crucial because a word might behave differently when it's the grammatical "head" of a phrase versus when it's a "dependent" of another word. The diagram shows this for "book" (h₁ head, h₁ dep), "that" (h₂ head, h₂ dep), and "flight" (h₃ head, h₃ dep).
4.  **Biaffine Scoring (The Core Calculation):** The main computation for determining a dependency score occurs within the large shaded box labeled "Biaffine." This specific diagram focuses on calculating the score for a dependency where "book" is the head and "flight" is the dependent.
    *   The "head" representation of "book" ("h₁ head") is fed into a component labeled "U," which performs a linear transformation.
    *   The "dependent" representation of "flight" ("h₃ dep") is fed into another component labeled "W," which also performs a linear transformation.
    *   The outputs from "U" and "W" are then combined through an element-wise addition operation, indicated by the "⊕" symbol.
    *   A constant "bias" term, labeled "b," is also added into the calculation.
    *   Finally, all these combined values are integrated through a summation step, indicated by the "Σ" symbol.
5.  **Final Score:** The output of this entire "Biaffine" process is a single numerical value labeled "score(h₁ head, h₃ dep)". This score quantifies the likelihood or strength of the dependency relationship where "book" is the head and "flight" is its dependent.

In essence, this diagram illustrates how a neural network can take words, convert them into rich, role-specific embeddings, and then use a sophisticated mathematical function (the biaffine transformation) to calculate a score that indicates how strongly two words are related in a dependency structure.](images/9067ff61fbbb30546702b10dc76455d098b2b2e5602d3a26f190574ed033988b.jpg)
Figure 19.14 Computing scores for a single edge $\mathbf { b o o k } $ flight) in the biaffine parser of Dozat and Manning (2017); Dozat et al. (2017). The parser uses distinct feedforward networks to turn the encoder output for each word into a head and dependent representation for the word. The biaffine function turns the head embedding of the head and the dependent embedding of the dependent into a score for the dependency edge.

Here we’ll sketch the biaffine algorithm of Dozat and Manning (2017) and Dozat et al. (2017) shown in Fig. 19.14, drawing on the work of Grunewald et al.¨ (2021) who tested many versions of the algorithm via their STEPS system. The algorithm first runs the sentence $X = x _ { 1 } , . . . , x _ { n }$ through an encoder to produce a contextual embedding representation for each token $R = r _ { 1 } , . . . , r _ { n }$ . The embedding for each token is now passed through two separate feedforward networks, one to produce a representation of this token as a head, and one to produce a representation of this token as a dependent:

$$
\begin{array} { r } { \mathsf { h } _ { i } ^ { h e a d } = \mathrm { F F N } ^ { h e a d } ( \mathsf { r } _ { i } ) } \\ { \mathsf { h } _ { i } ^ { d e p } = \mathrm { F F N } ^ { d e p } ( \mathsf { r } _ { i } ) } \end{array}
$$

Now to assign a score to the directed edge $i  j$ , $w _ { i }$ is the head and $w _ { j }$ is the dependent), we feed the head representation of $i .$ , $\mathbf { h } _ { i } ^ { h e a d }$ , and the dependent representation of $j , \boldsymbol { \mathsf { h } } _ { j } ^ { d e p }$ , into a biaffine scoring function:

$$
\begin{array} { r c l } { { } } & { { } } & { { \mathrm { S c o r e } ( i \to j ) ~ = ~ \mathsf { B i a f f } ( \mathsf { h } _ { i } ^ { h e a d } , \mathsf { h } _ { j } ^ { d e p } ) } } \\ { { } } & { { } } & { { \mathrm { B i a f f } ( \mathbf { x } , \mathbf { y } ) ~ = ~ \mathbf { x } ^ { \mathsf { T } } \mathbf { U } \mathbf { y } + \mathbf { W } ( \mathbf { x } \oplus \mathbf { y } ) + b } } \end{array}
$$

where $\mathbf { u } , \mathbf { w }$ , and $^ b$ are weights learned by the model. The idea of using a biaffine function is to allow the system to learn multiplicative interactions between the vectors $\pmb { \times }$ and $\pmb { \ y }$ .

If we pass Score $( i  j )$ through a softmax, we end up with a probability distribution, for each token $j$ , over potential heads $i$ (all other tokens in the sentence):

$$
p ( i \to j ) = \operatorname { s o f t m a x } ( [ \operatorname { S c o r e } ( k \to j ) ; \forall k \neq j , 1 \leq k \leq n ] )
$$

This probability can then be passed to the maximum spanning tree algorithm of Section 19.3.1 to find the best tree.

This $p ( i  j )$ classifier is trained by optimizing the cross-entropy loss.

Note that the algorithm as we’ve described it is unlabeled. To make this into a labeled algorithm, the Dozat and Manning (2017) algorithm actually trains two classifiers. The first classifier, the edge-scorer, the one we described above, assigns a probability $p ( i  j )$ to each word $w _ { i }$ and $w _ { j }$ . Then the Maximum Spanning Tree algorithm is run to get a single best dependency parse tree for the second. We then apply a second classifier, the label-scorer, whose job is to find the maximum probability label for each edge in this parse. This second classifier has the same form as (19.15-19.17), but instead of being trained to predict with binary softmax the probability of an edge existing between two words, it is trained with a softmax over dependency labels to predict the dependency label between the words.

# 19.4 Evaluation

As with phrase structure-based parsing, the evaluation of dependency parsers proceeds by measuring how well they work on a test set. An obvious metric would be exact match (EM)—how many sentences are parsed correctly. This metric is quite pessimistic, with most sentences being marked wrong. Such measures are not finegrained enough to guide the development process. Our metrics need to be sensitive enough to tell if actual improvements are being made.

For these reasons, the most common method for evaluating dependency parsers are labeled and unlabeled attachment accuracy. Labeled attachment refers to the proper assignment of a word to its head along with the correct dependency relation. Unlabeled attachment simply looks at the correctness of the assigned head, ignoring the dependency relation. Given a system output and a corresponding reference parse, accuracy is simply the percentage of words in an input that are assigned the correct head with the correct relation. These metrics are usually referred to as the labeled attachment score (LAS) and unlabeled attachment score (UAS). Finally, we can make use of a label accuracy score (LS), the percentage of tokens with correct labels, ignoring where the relations are coming from.

As an example, consider the reference parse and system parse for the following example shown in Fig. 19.15.

(19.18) Book me the flight through Houston.

The system correctly finds 4 of the 6 dependency relations present in the reference parse and receives an LAS of 2/3. However, one of the 2 incorrect relations found by the system holds between book and flight, which are in a head-dependent relation in the reference parse; the system therefore achieves a UAS of 5/6.

Beyond attachment scores, we may also be interested in how well a system is performing on a particular kind of dependency relation, for example NSUBJ, across a development corpus. Here we can make use of the notions of precision and recall introduced in Chapter 17, measuring the percentage of relations labeled NSUBJ by the system that were correct (precision), and the percentage of the NSUBJ relations present in the development set that were in fact discovered by the system (recall). We can employ a confusion matrix to keep track of how often each dependency type was confused for another.

![## Image Analysis: e3eaec49642e8dba1430af4b144cec8f4f02e72693c67febd6f2738352e1b9cc.jpg

**Conceptual Understanding:**
The image conceptually represents the task of dependency parsing in natural language processing. Its main purpose is to compare a correct, human-labeled (reference) parse tree with an automatically generated (system) parse tree for the same sentence. This comparison highlights how a parsing system analyzes sentence structure and where its analysis might deviate from the intended or correct interpretation. The key idea communicated is the evaluation of parsing accuracy by visually illustrating the differences in assigned grammatical relationships and head-dependent connections between words.

**Content Interpretation:**
The image shows two distinct dependency parse trees for the same sentence, 'Book me the flight through Houston.' Each tree maps out the grammatical relationships between the words. The first tree, labeled '(a) Reference', represents a human-annotated or 'gold standard' parse. The second tree, labeled '(b) System', represents the parse generated by an automated parsing system. The purpose of presenting both is to visually compare the output of an automated system against a correct reference, allowing for the identification of differences in assigned dependency relations. These diagrams illustrate the process of dependency parsing, where words are connected by directed arcs labeled with the type of grammatical relationship (e.g., object, subject, nominal modifier).

**Key Insights:**
The main takeaway from this image is the visual comparison of a 'gold standard' dependency parse against a system-generated one. It demonstrates how automated parsing can differ from human judgment, even for relatively simple sentences. Specifically: 1. **Differences in Argument Structure:** The 'Reference' parse correctly identifies 'me' as an indirect object ('iobj') and 'flight' as a direct object ('obj') of 'Book'. The 'System' parse incorrectly assigns 'flight' as a clausal complement ('xcomp') of 'Book' and 'me' as the nominal subject ('nsubj') of 'flight'. This highlights a common challenge in parsing: correctly identifying argument structure. 2. **Consistent Dependencies:** Both parses correctly identify 'the' as a determiner ('det') of 'flight' and 'Houston' as a nominal modifier ('nmod') of 'flight', with 'through' acting as the 'case' marker for 'Houston' in this relationship. This indicates areas where the system performs accurately. These specific textual differences in dependency labels and their attachments are the basis for calculating metrics like LAS and UAS, where matching arcs and labels contribute to higher scores, and discrepancies lower them. The image provides concrete evidence for how a parsing error (e.g., 'iobj' vs 'nsubj'/'xcomp') impacts evaluation.

**Document Context:**
This image is presented in the context of 'Section: 19.4 Evaluation,' directly preceding text that states, 'Figure 19.15 Reference and system parses for Book me the flight through Houston, resulting in an LAS of 2/3 and an UAS of 5/6.' This context indicates that the image serves as a visual example for evaluating the performance of a parsing system. The two dependency trees visually demonstrate what 'Reference' and 'System' parses look like for a specific sentence, thereby illustrating the source of the Link Accuracy Score (LAS) and Unlabeled Accuracy Score (UAS) metrics mentioned in the surrounding text. The differences between the 'Reference' and 'System' parses are crucial for understanding how these evaluation metrics are derived.

**Summary:**
The image displays two dependency parse trees for the sentence "Book me the flight through Houston." Each tree illustrates grammatical relationships between words, with labeled nodes representing dependency types and arrows indicating head-dependent relationships. The two diagrams are presented side-by-side for comparison: (a) Reference and (b) System. The 'Reference' parse serves as a gold standard, while the 'System' parse represents an automated system's output. Differences in the assigned dependency labels highlight the discrepancies between the two parses. For instance, in the 'Reference' parse, 'me' is an 'iobj' of 'Book' and 'flight' is an 'obj' of 'Book'. In contrast, the 'System' parse analyzes 'flight' as an 'xcomp' of 'Book' and 'me' as an 'nsubj' of 'flight'. Both diagrams show 'the' as a 'det' of 'flight', and 'Houston' as an 'nmod' of 'flight' with 'through' serving as the 'case' for 'Houston'. The explicit labeling of each dependency allows for a detailed comparison of syntactic structures.](images/e3eaec49642e8dba1430af4b144cec8f4f02e72693c67febd6f2738352e1b9cc.jpg)
Figure 19.15 Reference and system parses for Book me the flight through Houston, resulting in an LAS of 2/3 and an UAS of $5 / 6$ .

# 19.5 Summary

This chapter has introduced the concept of dependency grammars and dependency parsing. Here’s a summary of the main points that we covered:

• In dependency-based approaches to syntax, the structure of a sentence is described in terms of a set of binary relations that hold between the words in a sentence. Larger notions of constituency are not directly encoded in dependency analyses.   
• The relations in a dependency structure capture the head-dependent relationship among the words in a sentence.   
• Dependency-based analysis provides information directly useful in further language processing tasks including information extraction, semantic parsing and question answering.   
• Transition-based parsing systems employ a greedy stack-based algorithm to create dependency structures.   
• Graph-based methods for creating dependency structures are based on the use of maximum spanning tree methods from graph theory.   
• Both transition-based and graph-based approaches are developed using supervised machine learning techniques.   
• Treebanks provide the data needed to train these systems. Dependency treebanks can be created directly by human annotators or via automatic transformation from phrase-structure treebanks.   
• Evaluation of dependency parsers is based on labeled and unlabeled accuracy scores as measured against withheld development and test corpora.

# Bibliographical and Historical Notes

The dependency-based approach to grammar is much older than the relatively recent phrase-structure or constituency grammars, which date only to the 20th century. Dependency grammar dates back to the Indian grammarian Pan¯ . ini sometime between the 7th and 4th centuries BCE, as well as the ancient Greek linguistic traditions. Contemporary theories of dependency grammar all draw heavily on the 20th century work of Tesniere \` (1959).

Automatic parsing using dependency grammars was first introduced into computational linguistics by early work on machine translation at the RAND Corporation led by David Hays. This work on dependency parsing closely paralleled work on constituent parsing and made explicit use of grammars to guide the parsing process. After this early period, computational work on dependency parsing remained intermittent over the following decades. Notable implementations of dependency parsers for English during this period include Link Grammar (Sleator and Temperley, 1993), Constraint Grammar (Karlsson et al., 1995), and MINIPAR (Lin, 2003).

Dependency parsing saw a major resurgence in the late 1990’s with the appearance of large dependency-based treebanks and the associated advent of data driven approaches described in this chapter. Eisner (1996) developed an efficient dynamic programming approach to dependency parsing based on bilexical grammars derived from the Penn Treebank. Covington (2001) introduced the deterministic word by word approach underlying current transition-based approaches. Yamada and Matsumoto (2003) and Kudo and Matsumoto (2002) introduced both the shift-reduce paradigm and the use of supervised machine learning in the form of support vector machines to dependency parsing.

Transition-based parsing is based on the shift-reduce parsing algorithm originally developed for analyzing programming languages (Aho and Ullman, 1972). Shift-reduce parsing also makes use of a context-free grammar. Input tokens are successively shifted onto the stack and the top two elements of the stack are matched against the right-hand side of the rules in the grammar; when a match is found the matched elements are replaced on the stack (reduced) by the non-terminal from the left-hand side of the rule being matched. In transition-based dependency parsing we skip the grammar, and alter the reduce operation to add a dependency relation between a word and its head.

Nivre (2003) defined the modern, deterministic, transition-based approach to dependency parsing. Subsequent work by Nivre and his colleagues formalized and analyzed the performance of numerous transition systems, training methods, and methods for dealing with non-projective language (Nivre and Scholz 2004, Nivre 2006, Nivre and Nilsson 2005, Nivre et al. 2007b, Nivre 2007). The neural approach was pioneered by Chen and Manning (2014) and extended by Kiperwasser and Goldberg (2016); Kulmizev et al. (2019).

The graph-based maximum spanning tree approach to dependency parsing was introduced by McDonald et al. 2005a, McDonald et al. 2005b. The neural classifier was introduced by (Kiperwasser and Goldberg, 2016).

The long-running Prague Dependency Treebank project (Hajicˇ, 1998) is the most significant effort to directly annotate a corpus with multiple layers of morphological, syntactic and semantic information. PDT 3.0 contains over $1 . 5 \ \mathrm { M }$ tokens (Bejcek ˇ et al., 2013).

Universal Dependencies (UD) (de Marneffe et al., 2021) is an open community project to create a framework for dependency treebank annotation, with nearly 200 treebanks in over 100 languages. The UD annotation scheme evolved out of several distinct efforts including Stanford dependencies (de Marneffe et al. 2006, de Marneffe and Manning 2008, de Marneffe et al. 2014), Google’s universal part-of-speech tags (Petrov et al., 2012), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008).

The Conference on Natural Language Learning (CoNLL) has conducted an influential series of shared tasks related to dependency parsing over the years (Buchholz and Marsi 2006, Nivre et al. 2007a, Surdeanu et al. 2008, Hajic et al. ˇ 2009). More recent evaluations have focused on parser robustness with respect to morphologically rich languages (Seddah et al., 2013), and non-canonical language forms such as social media, texts, and spoken language (Petrov and McDonald, 2012). Choi et al. (2015) presents a performance analysis of 10 dependency parsers across a range of metrics, as well as DEPENDABLE, a robust parser evaluation tool.

# Exercises

# CHAPTER 20 InformationRelations, E Extraction: vents, and Time

Time will explain. Jane Austen, Persuasion

information extraction

relation extraction knowledge graphs

event extraction

Imagine that you are an analyst with an investment firm that tracks airline stocks. You’re given the task of determining the relationship (if any) between airline announcements of fare increases and the behavior of their stocks the next day. Historical data about stock prices is easy to come by, but what about the airline announcements? You will need to know at least the name of the airline, the nature of the proposed fare hike, the dates of the announcement, and possibly the response of other airlines. Fortunately, these can be all found in news articles like this one:

Citing high fuel prices, United Airlines said Friday it has increased fares by $\$ 6$ per round trip on flights to some cities also served by lowercost carriers. American Airlines, a unit of AMR Corp., immediately matched the move, spokesman Tim Wagner said. United, a unit of UAL Corp., said the increase took effect Thursday and applies to most routes where it competes against discount carriers, such as Chicago to Dallas and Denver to San Francisco.

This chapter presents techniques for extracting limited kinds of semantic content from text. This process of information extraction (IE) turns the unstructured information embedded in texts into structured data, for example for populating a relational database to enable further processing.

We begin with the task of relation extraction: finding and classifying semantic relations among entities mentioned in a text, like child-of (X is the child-of Y), or part-whole or geospatial relations. Relation extraction has close links to populating a relational database, and knowledge graphs, datasets of structured relational knowledge, are a useful way for search engines to present information to users.

Next, we discuss event extraction, the task of finding events in which these entities participate, like, in our sample text, the fare increases by United and American and the reporting events said and cite. Events are also situated in time, occurring at a particular date or time, and events can be related temporally, happening before or after or simultaneously with each other. We’ll need to recognize temporal expressions like Friday, Thursday or two days from now and times such as $3 { : } 3 0 P . M .$ , and normalize them onto specific calendar dates or times. We’ll need to link Friday to the time of United’s announcement, Thursday to the previous day’s fare increase, and we’ll need to produce a timeline in which United’s announcement follows the fare increase and American’s announcement follows both of those events.

The related task of template filling is to find recurring stereotypical events or situations in documents and fill in the template slots. These slot-fillers may consist of text segments extracted directly from the text, or concepts like times, amounts, or ontology entities that have been inferred through additional processing. Our airline text presents such a stereotypical situation since airlines often raise fares and then wait to see if competitors follow along. Here we can identify United as a lead airline that initially raised its fares, $\$ 6$ as the amount, Thursday as the increase date, and American as an airline that followed along, leading to a filled template like the following:

![## Image Analysis: 4893ca14630c831424b9a81fe8b5fdb0537a215b962523d1a613ed49f6293957.jpg

**Conceptual Understanding:**
This image conceptually represents a hierarchical taxonomy or classification system for 17 distinct semantic relations. These relations are specifically identified as being 'used in the ACE relation extraction task'.

The main purpose of the image is to visually define and categorize these 17 relations, providing a structured overview of the types of connections or associations that exist between entities (e.g., persons, organizations, locations, artifacts) in a dataset. It serves as a legend or a schema, illustrating the specific vocabulary of relationships that an automated system (like one for ACE) is designed to detect and extract from text. The diagram systematically breaks down broad categories into more granular, specific relation types.

**Content Interpretation:**
The image presents a comprehensive hierarchical taxonomy of 17 relations specifically designed for the ACE (Automatic Content Extraction) relation extraction task. The relations are categorized into six primary types: PERSON-SOCIAL, PHYSICAL, GENERAL AFFILIATION, PART-WHOLE, ORG AFFILIATION, and ARTIFACT. Each primary type branches into several specific sub-types, illustrating a granular classification system.

The significance of this structured classification lies in its ability to standardize and define the types of semantic relationships that can exist between entities within text. For instance, the 'PERSON-SOCIAL' category signifies relationships between people or a person's close social ties. 'PHYSICAL' deals with locational relationships. 'GENERAL AFFILIATION' covers broader societal or organizational ties. 'PART-WHOLE' describes hierarchical or compositional relationships. 'ORG AFFILIATION' details specific roles and connections within organizations, and 'ARTIFACT' focuses on associations with manufactured items.

All extracted text elements support these interpretations by precisely labeling each category and sub-category. For example, 'Family', 'Business', and 'Lasting Personal' under 'PERSON-SOCIAL' clearly define the types of social bonds being categorized. Similarly, 'Founder', 'Ownership', 'Membership', 'Sports-Affiliation', 'Employment', 'Student-Alum', and 'Investor' under 'ORG AFFILIATION' explicitly enumerate the diverse ways an entity can be affiliated with an organization. The specific and detailed naming of each sub-type leaves no ambiguity in the definition of the 17 distinct relations, enabling precise identification and extraction in NLP tasks.

**Key Insights:**
The main takeaway from this image is the exhaustive and hierarchical categorization of 17 specific semantic relations, which are crucial for the ACE relation extraction task. The diagram demonstrates a systematic approach to defining diverse relationships between entities.

Key insights supported by the diagram include:
1.  **Comprehensive Scope:** The image illustrates that relation extraction encompasses a wide array of relationship types, from personal and social ties ('PERSON-SOCIAL': 'Family', 'Business', 'Lasting Personal') to physical locations ('PHYSICAL': 'Located', 'Near'), general affiliations ('GENERAL AFFILIATION': 'Citizen-Resident-Ethnicity-Religion', 'Org-Location-Origin'), part-whole structures ('PART-WHOLE': 'Subsidiary', 'Geographical'), organizational roles ('ORG AFFILIATION': 'Founder', 'Ownership', 'Membership', 'Sports-Affiliation', 'Employment', 'Student-Alum', 'Investor'), and connections to artifacts ('ARTIFACT': 'User-Owner-Inventor-Manufacturer'). Each extracted text element defines a unique facet of these relationships, providing a detailed understanding of the types of information an ACE system is designed to identify.
2.  **Hierarchical Organization:** The relations are not merely a flat list but are organized under broader conceptual categories, indicating a structured ontology. This hierarchy helps in understanding the conceptual grouping of similar relations (e.g., all types of 'ORG AFFILIATION' are grouped together).
3.  **Specificity for NLP Tasks:** The detailed nature of each relation (e.g., 'Citizen-Resident-Ethnicity-Religion' as a single, combined relation type) highlights the precision required for automated information extraction. The explicit listing of all 17 relations provides the exact vocabulary and targets for an NLP system working on ACE data.

These insights, directly derived from the verbatim transcription of all 17 relation types and their parent categories, confirm the diagram's role as a definitive guide to the relationship taxonomy used in ACE relation extraction.

**Document Context:**
The image, titled 'Figure 20.1 The 17 relations used in the ACE relation extraction task.', directly serves as a foundational reference for the document's discussion on 'Information Relations, Events, and Time' within 'CHAPTER 20'. It provides a crucial visual vocabulary for understanding the types of relationships that are the focus of automated content extraction, particularly in the context of the ACE program.

This diagram is highly relevant as it explicitly defines the categories and sub-categories of relations that the subsequent text will likely refer to, analyze, or build upon. By visually mapping out these 17 relations, the document establishes the scope and granularity of relation extraction, which is a core component of information extraction systems. It acts as a legend or a schema, allowing readers to grasp the fundamental building blocks of relation identification before delving into the methodologies and challenges of extracting these relations from text.

**Summary:**
The image presents a hierarchical classification of 17 distinct relations used in the ACE (Automatic Content Extraction) relation extraction task. It categorizes these relations into six primary types, represented by blue oval shapes, each of which branches out into more specific sub-types, shown as brown oval shapes. 

Starting from the top left, the 'PERSON-SOCIAL' category encompasses relations such as 'Family', 'Business', and 'Lasting Personal'. This category focuses on relationships between individuals or a person's significant connections.

Next, the 'PHYSICAL' category describes spatial or positional relationships, further divided into 'Located' and 'Near'. This covers where entities are situated in relation to each other.

The 'GENERAL AFFILIATION' category groups broad affiliation types, specifically 'Citizen-Resident-Ethnicity-Religion' (referring to demographic and cultural ties) and 'Org-Location-Origin' (indicating an entity's association with an organization or place of origin).

Moving to 'PART-WHOLE', this category details compositional or containment relationships, with sub-types 'Subsidiary' (indicating a subordinate part of a larger entity) and 'Geographical' (describing a part-whole relationship in a spatial context, like a city within a country).

In the bottom row, the 'ORG AFFILIATION' category is more extensive, covering various relationships an entity can have with an organization. These include 'Founder', 'Ownership', 'Membership', 'Sports-Affiliation', 'Employment', 'Student-Alum', and 'Investor'. These sub-types specify different roles or connections within organizational structures.

Finally, the 'ARTIFACT' category addresses relationships related to created objects, with the single comprehensive sub-type 'User-Owner-Inventor-Manufacturer'. This captures the various roles entities can have concerning an artifact, from its creation to its use and possession.

In essence, the diagram systematically breaks down complex relationships into manageable, defined categories and sub-categories, providing a structured vocabulary for relation extraction tasks in natural language processing.](images/4893ca14630c831424b9a81fe8b5fdb0537a215b962523d1a613ed49f6293957.jpg)
Figure 20.1 The 17 relations used in the ACE relation extraction task.

<table><tr><td>FARE-RAISE ATTEMPT:</td><td>LEAD AIRLINE: AMOUNT:</td><td>UNITED AIRLINES $6</td></tr><tr><td></td><td>EFFECTIVE DATE:</td><td>2006-10-26</td></tr><tr><td></td><td>FOLLOWER:</td><td>AMERICAN AIRLINES</td></tr></table>

# 20.1 Relation Extraction

Let’s assume that we have detected the named entities in our sample text (perhaps using the techniques of Chapter 17), and would like to discern the relationships that exist among the detected entities:

Citing high fuel prices, [ORG United Airlines] said $\mathrm { [ _ { T I M E } }$ Friday] it has increased fares by [MONEY $\$ 6]$ per round trip on flights to some cities also served by lower-cost carriers. [ORG American Airlines], a unit of [ORG AMR Corp.], immediately matched the move, spokesman [PER Tim Wagner] said. [ORG United], a unit of [ORG UAL Corp.], said the increase took effect [TIME Thursday] and applies to most routes where it competes against discount carriers, such as $\operatorname { I } _ { \mathrm { L O C } }$ Chicago] to $\operatorname { I } _ { \mathrm { L O C } }$ Dallas] and [LOC Denver] to $\operatorname { I } _ { \mathrm { L O C } }$ San Francisco].

The text tells us, for example, that Tim Wagner is a spokesman for American Airlines, that United is a unit of UAL Corp., and that American is a unit of AMR. These binary relations are instances of more generic relations such as part-of or employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample relations. We might also extract more domain-specific relations such as the notion of an airline route. For example from this text we can conclude that United has routes to Chicago, Dallas, Denver, and San Francisco.

<table><tr><td>Relations</td><td>Types</td><td>Examples</td></tr><tr><td>Physical-Located</td><td>PER-GPE</td><td>He was in Tennessee</td></tr><tr><td>Part- Whole-Subsidiary</td><td>ORG-ORG</td><td> XYZ, the parent company of ABC</td></tr><tr><td>Person-Social-Family</td><td>PER-PER</td><td>Yoko&#x27;s husband John</td></tr><tr><td>Org-AFF-Founder</td><td>PER-ORG</td><td> Steve Jobs, co-founder of Aple...</td></tr></table>

Figure 20.2 Semantic relations with examples and the named entity types they involve.

Sets of relations have been defined for many other domains as well. For example UMLS, the Unified Medical Language System from the US National Library of Medicine has a network that defines 134 broad subject categories, entity types, and 54 relations between the entities, such as the following:

<table><tr><td>Entity</td><td>Relation</td><td>Entity</td></tr><tr><td>Injury</td><td>disrupts</td><td>Physiological Function</td></tr><tr><td>Bodily Location</td><td></td><td>location-of Biologic Function</td></tr><tr><td> Anatomical Structure</td><td> part-of</td><td>Organism</td></tr><tr><td> Pharmacologic Substance causes</td><td></td><td> Pathological Function</td></tr><tr><td>Pharmacologic Substance</td><td>treats</td><td> Pathologic Function</td></tr></table>

Given a medical sentence like this one:

(20.1) Doppler echocardiography can be used to diagnose left anterior descending artery stenosis in patients with type 2 diabetes

# infoboxes

We could thus extract the UMLS relation:

Echocardiography, Doppler Diagnoses Acquired stenosis

Wikipedia also offers a large supply of relations, drawn from infoboxes, structured tables associated with certain Wikipedia articles. For example, the Wikipedia infobox for Stanford includes structured facts like state $=$ "California" or president $=$ "Marc Tessier-Lavigne". These facts can be turned into relations like president-of or located-in. or into relations in a metalanguage called RDF (Resource Description Framework). An RDF triple is a tuple of entity-relationentity, called a subject-predicate-object expression. Here’s a sample RDF triple:

subject predicate object Golden Gate Park location San Francisco

# Freebase

For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology derived from Wikipedia containing over 2 billion RDF triples. Another dataset from Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrandeciˇ c´ and Krotzsch ¨ , 2014), has relations between people and their nationality, or locations, and other locations they are contained in.

is-a hypernym

WordNet or other ontologies offer useful ontological relations that express hierarchical relations between words or concepts. For example WordNet has the is-a or hypernym relation between classes,

Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...

WordNet also has Instance-of relation between individuals and classes, so that for example San Francisco is in the Instance-of relation with city. Extracting these relations is an important step in extending or building ontologies.

Finally, there are large datasets that contain sentences hand-labeled with their relations, designed for training and testing relation extractors. The TACRED dataset (Zhang et al., 2017) contains 106,264 examples of relation triples about particular people or organizations, labeled in sentences from news and web text drawn from the annual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains 41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse), plus a no relation tag; examples are shown in Fig. 20.3. About $80 \%$ of all examples are annotated as no relation; having sufficient negative data is important for training supervised classifiers.

<table><tr><td>Example</td><td>Entity Types &amp; Label</td></tr><tr><td>Carey will succeed Cathleen P. Black, who held the position for 15</td><td>PERSON/TITLE</td></tr><tr><td>years and will take on a new role as chairwoman of Hearst Maga- zines, the company said.</td><td>Relation: per:title</td></tr><tr><td> Irene Morgan Kirkaldy, who was born and reared in Baltimore, lived</td><td>PERSON/CITY</td></tr><tr><td>on Long Island and ran a child-care center in Queens with her second husband, Stanley Kirkaldy.</td><td>Relation: per:city_of_birth</td></tr><tr><td>Baldwin declined further comment, and said JetBlue chief executive Dave Barger was unavailable.</td><td>Types: PERSON/TITLE</td></tr></table>

Figure 20.3 Example sentences and labels from the TACRED dataset (Zhang et al., 2017).

A standard dataset was also produced for the SemEval 2010 Task 8, detecting relations between nominals (Hendrickx et al., 2009). The dataset has 10,717 examples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed relations like product-producer ( a factory manufactures suits) or component-whole (my apartment has a large kitchen).

# 20.2 Relation Extraction Algorithms

There are five main classes of algorithms for relation extraction: handwritten patterns, supervised machine learning, semi-supervised (via bootstrapping or distant supervision), and unsupervised. We’ll introduce each of these in the next sections.

# 20.2.1 Using Patterns to Extract Relations

The earliest and still common algorithm for relation extraction is lexico-syntactic patterns, first developed by Hearst (1992a), and therefore often called Hearst patterns. Consider the following sentence:

Agar is a substance prepared from a mixture of red algae, such as Gelidium, for laboratory or industrial use.

Hearst points out that most human readers will not know what Gelidium is, but that they can readily infer that it is a kind of (a hyponym of) red algae, whatever that is. She suggests that the following lexico-syntactic pattern

$$
N P _ { 0 } s u c h a s N P _ { 1 } \{ , N P _ { 2 } . . . , ( a n d | o r ) N P _ { i } \} , i \geq 1
$$

implies the following semantics

$$
\forall N P _ { i } , i \ge 1 , \mathrm { h y p o n y m } ( N P _ { i } , N P _ { 0 } )
$$

allowing us to infer

$$
\mathrm { h y p o n y m ( G e l i d i u m , r e d a l g a e ) }
$$

<table><tr><td>NP {, NP}* {,} (and|or) other NPH NPH such as {NP,}* {(or|and)} NP such NPH as {NP,} * {(or|and)} NP NPH {,} including {NP,}* {(or|and)} NP NPH {,} especially {NP} * {(or|and)} NP</td><td>temples, treasuries, and other important civic buildings red algae such as Gelidium such authors as Herrick, Goldsmith, and Shakespeare common-law countries, including Canada and England</td></tr></table>

Figure 20.4 Hand-built lexico-syntactic patterns for finding hypernyms, using $\{ \}$ to mark optionality (Hearst 1992a, Hearst 1998).

Figure 20.4 shows five patterns Hearst (1992a, 1998) suggested for inferring the hyponym relation; we’ve shown $\mathrm { N P _ { H } }$ as the parent/hyponym. Modern versions of the pattern-based approach extend it by adding named entity constraints. For example if our goal is to answer questions about “Who holds what office in which organization?”, we can use patterns like the following:

PER, POSITION of ORG: George Marshall, Secretary of State of the United States

PER (named|appointed|chose|etc.) PER Prep? POSITION Truman appointed Marshall Secretary of State

PER [be]? (named appointed etc.) Prep? ORG POSITION George Marshall was named US Secretary of State

Hand-built patterns have the advantage of high-precision and they can be tailored to specific domains. On the other hand, they are often low-recall, and it’s a lot of work to create them for all possible patterns.

# 20.2.2 Relation Extraction via Supervised Learning

Supervised machine learning approaches to relation extraction follow a scheme that should be familiar by now. A fixed set of relations and entities is chosen, a training corpus is hand-annotated with the relations and entities, and the annotated texts are then used to train classifiers to annotate an unseen test set.

The most straightforward approach, illustrated in Fig. 20.5 is: (1) Find pairs of named entities (usually in the same sentence). (2): Apply a relation-classification on each pair. The classifier can use any supervised technique (logistic regression, RNN, Transformer, random forest, etc.).

An optional intermediate filtering classifier can be used to speed up the processing by making a binary decision on whether a given pair of named entities are related (by any relation). It’s trained on positive examples extracted directly from all relations in the annotated corpus, and negative examples generated from within-sentence entity pairs that are not annotated with a relation.

Feature-based supervised relation classifiers. Let’s consider sample features for a feature-based classifier (like logistic regression or random forests), classifying the relationship between American Airlines (Mention 1, or M1) and Tim Wagner (Mention 2, M2) from this sentence:

(20.5) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said

These include word features (as embeddings, or 1-hot, stemmed or not):

• The headwords of M1 and M2 and their concatenation Airlines Wagner Airlines-Wagner

function FINDRELATIONS(words) returns relations

relations $\gets n i l$   
entities FINDENTITIES(words)   
forall entity pairs $\langle e l , e 2 \rangle$ in entities do if RELATED? $\wr ( e I , e 2 )$ relations relations+CLASSIFYRELATION(e1, e2)

Figure 20.5 Finding and classifying the relations among entities in a text.

• Bag-of-words and bigrams in M1 and M2 American, Airlines, Tim, Wagner, American Airlines, Tim Wagner

• Words or bigrams in particular positions M2: -1 spokesman M2: $+ 1$ said   
• Bag of words or bigrams between M1 and M2: a, AMR, of, immediately, matched, move, spokesman, the, unit

# Named entity features:

• Named-entity types and their concatenation (M1: ORG, M2: PER, M1M2: ORG-PER)   
• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN) M1: NAME [it or he would be PRONOUN] M2: NAME [the company would be NOMINAL]   
• Number of entities between the arguments (in this case 1, for AMR)

Syntactic structure is a useful signal, often represented as the dependency or constituency syntactic path traversed through the tree between the entities.

• Constituent paths between M1 and M2 $N P \uparrow N P \uparrow S \uparrow S \downarrow N P$   
• Dependency-tree paths $A i r l i n e s \gets _ { s u b j } m a t c h e d \gets _ { c o m p } s a i d \gets _ { s u b j } W a g n e r$

Neural supervised relation classifiers Neural models for relation extraction similarly treat the task as supervised classification. Let’s consider a typical system applied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In TACRED we are given a sentence and two spans within it: a subject, which is a person or organization, and an object, which is any other entity. The task is to assign a relation from the 42 TAC relations, or no relation.

A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a pretrained encoder like BERT and adds a linear layer on top of the sentence representation (for example the BERT [CLS] token), a linear layer that is finetuned as a 1-of-N classifier to assign one of the 43 labels. The input to the BERT encoder is partially de-lexified; the subject and object entities are replaced in the input by their NER tags. This helps keep the system from overfitting to the individual lexical items (Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi et al., 2020) that don’t have two sequences separated by a [SEP] token, but instead form the input from a single long sequence of sentences.

In general, if the test set is similar enough to the training set, and if there is enough hand-labeled data, supervised relation extraction systems can get high accuracies. But labeling a large training set is extremely expensive and supervised models are brittle: they don’t generalize well to different text genres. For this reason, much research in relation extraction has focused on the semi-supervised and unsupervised approaches we turn to next.

![## Image Analysis: 2186d6da8789f941e13fd8f46a8cfa64ea1fb652fb71988ea9b4b961ea0bd713.jpg

**Conceptual Understanding:**
The image conceptually illustrates a deep learning pipeline for relation extraction, a natural language processing task. Its main purpose is to show how a sequence containing placeholder tokens for a subject and an object (based on their NER types) is processed by an encoder and subsequently classified to determine the probability of a specific relation existing between those entities. The key idea is that by replacing specific entities with their NER tags, the model can generalize better to different instances of the same entity types.

**Content Interpretation:**
The image depicts a neural network architecture for relation extraction. It shows how an input sentence is processed to determine the relationship between a subject (SUBJ) and an object (OBJ). The core components are an 'ENCODER', which likely transforms the input text into a numerical representation, and a 'Linear Classifier', which uses this representation to predict the relation. The use of Named Entity Recognition (NER) tags ([SUBJ_PERSON], [OBJ_LOC]) in place of the actual entities is a key aspect, indicating that the model leverages entity type information.

**Key Insights:**
The main takeaway is that relation extraction can be performed by replacing named entities with their generic NER tags in the input sequence, which is then processed by an encoder and classified by a linear layer. This approach, as indicated by the input '[CLS] [SUBJ_PERSON] was born in [OBJ_LOC] , Michigan' and the output 'p(relation|SUBJ,OBJ)', suggests a method to generalize relation extraction irrespective of specific entity names but based on their types and surrounding context. The model learns to predict the probability of a relation given the subject and object entity types.

**Document Context:**
This image directly illustrates the concept of 'Relation extraction as a linear layer on top of an encoder', as described in the accompanying document context. It shows a specific implementation where Named Entity Recognition (NER) tags are used to replace the actual entities in the input, a technique attributed to Zhang et al. (2017) and Joshi et al. (2020). This provides a visual representation of how named entity features are incorporated into a relation extraction model, linking to the broader section on 'Named entity features'.

**Summary:**
The image illustrates a method for relation extraction using an encoder-based model, likely a BERT-like architecture given the context. The process begins with an input sequence where subject and object entities have been replaced by their Named Entity Recognition (NER) tags. This sequence, starting with a [CLS] token, is fed into an 'ENCODER'. The encoder processes this input to generate a rich representation. This representation is then passed to a 'Linear Classifier'. Finally, the linear classifier outputs the probability of a specific relation existing between the identified subject and object entities, denoted as 'p(relation|SUBJ,OBJ)'. This architecture aims to identify the relationship between two entities in a sentence by leveraging their type information (NER tags) and the contextual understanding provided by the encoder and classifier.](images/2186d6da8789f941e13fd8f46a8cfa64ea1fb652fb71988ea9b4b961ea0bd713.jpg)
Figure 20.6 Relation extraction as a linear layer on top of an encoder (in this case BERT), with the subject and object entities replaced in the input by their NER tags (Zhang et al. 2017, Joshi et al. 2020).   
Figure 20.7 Bootstrapping from seed entity pairs to learn relations.

# 20.2.3 Semisupervised Relation Extraction via Bootstrapping

Supervised machine learning assumes that we have lots of labeled data. Unfortunately, this is expensive. But suppose we just have a few high-precision seed patterns, like those in Section 20.2.1, or perhaps a few seed tuples. That’s enough to bootstrap a classifier! Bootstrapping proceeds by taking the entities in the seed pair, and then finding sentences (on the web, or whatever dataset we are using) that contain both entities. From all such sentences, we extract and generalize the context around the entities to learn new patterns. Fig. 20.7 sketches a basic algorithm.

<table><tr><td>function BooTsTRAP(Relation R) returns new relation tuples</td></tr><tr><td>tuples←Gather a set of seed tuples that have relation R iterate</td></tr><tr><td>sentences←find sentences that contain entities in tuples</td></tr><tr><td>patterns ← generalize the context between and around entities in sentences</td></tr><tr><td>newpairs←use patterns to identify more tuples</td></tr><tr><td>newpairs←newpairs with high confidence</td></tr><tr><td>tuples←tuples +newpairs</td></tr><tr><td>return tuples</td></tr></table>

Suppose, for example, that we need to create a list of airline/hub pairs, and we know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover new patterns by finding other mentions of this relation in our corpus. We search for the terms Ryanair, Charleroi and hub in some proximity. Perhaps we find the following set of sentences:

(20.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all weekend flights out of the airport.   
(20.7) All flights in and out of Ryanair’s hub at Charleroi airport were grounded on Friday...   
(20.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000 passengers had already been affected.

confidence values semantic drift

From these results, we can use the context of words between the entity mentions, the words before mention one, the word after mention two, and the named entity types of the two mentions, and perhaps other features, to extract general patterns such as the following:

/ [ORG], which uses [LOC] as a hub / / [ORG]’s hub at [LOC] / / [LOC], a main hub for [ORG] / These new patterns can then be used to search for additional tuples.

Bootstrapping systems also assign confidence values to new tuples to avoid semantic drift. In semantic drift, an erroneous pattern leads to the introduction of erroneous tuples, which, in turn, lead to the creation of problematic patterns and the meaning of the extracted relations ‘drifts’. Consider the following example:

(20.9) Sydney has a ferry hub at Circular Quay.

If accepted as a positive example, this expression could lead to the incorrect introduction of the tuple $\langle S y d n e y , C i r c u l a r Q u a y \rangle$ . Patterns based on this tuple could propagate further errors into the database.

Confidence values for patterns are based on balancing two factors: the pattern’s performance with respect to the current set of tuples and the pattern’s productivity in terms of the number of matches it produces in the document collection. More formally, given a document collection $\mathrm { \textmathcal { D } }$ , a current set of tuples $T$ , and a proposed pattern $p$ , we need to track two factors:

• $h i t s ( p )$ : the set of tuples in $T$ that $p$ matches while looking in $\mathrm { \textmathcal { D } }$ • $\hbar n d s ( p )$ : The total set of tuples that $p$ finds in $\mathrm { \textmathcal { D } }$

The following equation balances these considerations (Riloff and Jones, 1999).

$$
C o n f _ { R l o g F } ( p ) = \frac { | h i t s ( p ) | } { | f n d s ( p ) | } l o g ( | f i n d s ( p ) | )
$$

This metric is generally normalized to produce a probability.

We can assess the confidence in a proposed new tuple by combining the evidence supporting it from all the patterns $P ^ { \prime }$ that match that tuple in $\mathrm { \textmathcal { D } }$ (Agichtein and Gravano, 2000). One way to combine such evidence is the noisy-or technique. Assume that a given tuple is supported by a subset of the patterns in $P$ , each with its own confidence assessed as above. In the noisy-or model, we make two basic assumptions. First, that for a proposed tuple to be false, all of its supporting patterns must have been in error, and second, that the sources of their individual failures are all independent. If we loosely treat our confidence measures as probabilities, then the probability of any individual pattern $p$ failing is $1 - C o n f ( p )$ ; the probability of all of the supporting patterns for a tuple being wrong is the product of their individual failure probabilities, leaving us with the following equation for our confidence in a new tuple.

$$
C o n f ( t ) = 1 - \prod _ { p \in P ^ { \prime } } \left( 1 - C o n f ( p ) \right)
$$

Setting conservative confidence thresholds for the acceptance of new patterns and tuples during the bootstrapping process helps prevent the system from drifting away from the targeted relation.

# 20.2.4 Distant Supervision for Relation Extraction

Although hand-labeling text with relation labels is expensive to produce, there are ways to find indirect sources of training data. The distant supervision method (Mintz et al., 2009) combines the advantages of bootstrapping with supervised learning. Instead of just a handful of seeds, distant supervision uses a large database to acquire a huge number of seed examples, creates lots of noisy pattern features from all these examples and then combines them in a supervised classifier.

For example suppose we are trying to learn the place-of-birth relationship between people and their birth cities. In the seed-based approach, we might have only 5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase have tens of thousands of examples of many relations; including over 100,000 examples of place-of-birth, (<Edwin Hubble, Marshfield>, <Albert Einstein, Ulm>, etc.,). The next step is to run named entity taggers on large amounts of text— Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences that have two named entities that match the tuple, like the following:

...Hubble was born in Marshfield...   
...Einstein, born (1879), Ulm...   
...Hubble’s birthplace in Marshfield...

Training instances can now be extracted from this data, one training instance for each identical tuple <relation, entity1, entity $2 >$ . Thus there will be one training instance for each of:

<born-in, Edwin Hubble, Marshfield> <born-in, Albert Einstein, Ulm> <born-year, Albert Einstein, $1 8 7 9 >$ and so on.

We can then apply feature-based or neural classification. For feature-based classification, we can use standard supervised relation extraction features like the named entity labels of the two mentions, the words and dependency paths in between the mentions, and neighboring words. Each tuple will have features collected from many training instances; the feature vector for a single training instance like (<born-in,Albert Einstein, Ulm> will have lexical and syntactic features from many different sentences that mention Einstein and Ulm.

Because distant supervision has very large training sets, it is also able to use very rich features that are conjunctions of these individual features. So we will extract thousands of patterns that conjoin the entity types with the intervening words or dependency paths like these:

PER was born in LOC PER, born (XXXX), LOC PER’s birthplace in LOC

To return to our running example, for this sentence:

(20.12) American Airlines, a unit of AMR, immediately matched the move, spokesman Tim Wagner said

we would learn rich conjunction features like this one:

$\mathbf { M } 1 = \mathbf { O } \mathbf { R } \mathbf { G }$ & ${ \bf M } 2 =$ PER & nextword $\mathbf { \Phi } = \mathbf { \dot { \Phi } }$ “said”& path= $: N P \uparrow N P \uparrow S \uparrow S \downarrow N P$

The result is a supervised classifier that has a huge rich set of features to use in detecting relations. Since not every test sentence will have one of the training relations, the classifier will also need to be able to label an example as no-relation. This label is trained by randomly selecting entity pairs that do not appear in any Freebase relation, extracting features for them, and building a feature vector for each such tuple. The final algorithm is sketched in Fig. 20.8.

<table><tr><td>function DISTANT SUPERVISION(Database D,Text T) returns relation classifer C</td></tr><tr><td>foreach relation R</td></tr><tr><td>foreach tuple (el,e2) of entities with relation R in D</td></tr><tr><td>sentences←Sentences in Tthat contain el and e2</td></tr><tr><td>f←Frequent features in sentences</td></tr><tr><td>observations ←observations + new training tuple (el, e2, f, R) C←Train supervised classfier on observations</td></tr><tr><td>return C</td></tr><tr><td></td></tr></table>

Figure 20.8 The distant supervision algorithm for relation extraction. A neural classifier would skip the feature set $f$ .

Distant supervision shares advantages with each of the methods we’ve examined. Like supervised classification, distant supervision uses a classifier with lots of features, and supervised by detailed hand-created knowledge. Like pattern-based classifiers, it can make use of high-precision evidence for the relation between entities. Indeed, distance supervision systems learn patterns just like the hand-built patterns of early relation extractors. For example the is-a or hypernym extraction system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as distant supervision, and then learned new patterns from large amounts of text. Their system induced exactly the original 5 template patterns of Hearst (1992a), but also 70,000 additional patterns including these four:

$\mathrm { N P } _ { H }$ like NP Many hormones like leptin... $\mathrm { N P } _ { H }$ called NP ...using a markup language called XHTML NP is a $\mathrm { N P } _ { H }$ Ruby is a programming language... NP, a $\mathrm { N P } _ { H }$ IBM, a company with a long...

This ability to use a large number of features simultaneously means that, unlike the iterative expansion of patterns in seed-based systems, there’s no semantic drift. Like unsupervised classification, it doesn’t use a labeled training corpus of texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very large amounts of unlabeled data. Distant supervision also has the advantage that it can create training tuples to be used with neural classifiers, where features are not required.

The main problem with distant supervision is that it tends to produce low-precision results, and so current research focuses on ways to improve precision. Furthermore, distant supervision can only help in extracting relations for which a large enough database already exists. To extract new relations without datasets, or relations for new domains, purely unsupervised methods must be used.

# 20.2.5 Unsupervised Relation Extraction

The goal of unsupervised relation extraction is to extract relations from the web when we have no labeled training data, and not even any list of relations. This task is often called open information extraction or Open IE. In Open IE, the relations are simply strings of words (usually beginning with a verb).

For example, the ReVerb system (Fader et al., 2011) extracts a relation from a sentence $s$ in 4 steps:

1. Run a part-of-speech tagger and entity chunker over $s$   
2. For each verb in $s ,$ , find the longest sequence of words $w$ that start with a verb and satisfy syntactic and lexical constraints, merging adjacent matches.   
3. For each phrase $w$ , find the nearest noun phrase $x$ to the left which is not a relative pronoun, wh-word or existential “there”. Find the nearest noun phrase $y$ to the right.   
4. Assign confidence $c$ to the relation ${ r } = ( x , w , y )$ using a confidence classifier and return it.

A relation is only accepted if it meets syntactic and lexical constraints. The syntactic constraints ensure that it is a verb-initial sequence that might also include nouns (relations that begin with light verbs like make, have, or do often express the core of the relation with a noun, like have a hub in):

$$
{ \begin{array} { r l } & { \mathrm { V ~ | ~ V P | ~ V W ^ { * } P } } \\ & { \mathrm { V = v e r b ~ p a r t i c l e ? ~ a d v ? } } \\ & { \mathrm { W = ( n o u n ~ | ~ a d j ~ | ~ a d v ~ | ~ p r o n ~ | ~ d e t ~ ) } } \\ & { \mathrm { P = ( p r e p ~ | ~ p a r t i c l e ~ | ~ i n f i n i t i v e ~ ^ { * } t o " } ) } \end{array} }
$$

The lexical constraints are based on a dictionary $D$ that is used to prune very rare, long relation strings. The intuition is to eliminate candidate relations that don’t occur with sufficient number of distinct argument types and so are likely to be bad examples. The system first runs the above relation extraction algorithm offline on 500 million web sentences and extracts a list of all the relations that occur after normalizing them (removing inflection, auxiliary verbs, adjectives, and adverbs). Each relation $r$ is added to the dictionary if it occurs with at least 20 different arguments. Fader et al. (2011) used a dictionary of 1.7 million normalized relations.

Finally, a confidence value is computed for each relation using a logistic regression classifier. The classifier is trained by taking 1000 random web sentences, running the extractor, and hand labeling each extracted relation as correct or incorrect. A confidence classifier is then trained on this hand-labeled data, using features of the relation and the surrounding words. Fig. 20.9 shows some sample features used in the classification.

<table><tr><td>(x,r,y) covers all words in s</td></tr><tr><td>the last preposition in r is for the last preposition in r is on</td></tr><tr><td>len(s)≤10</td></tr><tr><td>there is a coordinating conjunction to the left of r in s r matches a lone V in the syntactic constraints</td></tr></table>

Figure 20.9 Features for the classifier that assigns confidence to relations extracted by the Open Information Extraction system REVERB (Fader et al., 2011).

For example the following sentence:

(20.13) United has a hub in Chicago, which is the headquarters of United Continental Holdings.

has the relation phrases has a hub in and is the headquarters of (it also has has and is, but longer phrases are preferred). Step 3 finds United to the left and Chicago to the right of has a hub in, and skips over which to find Chicago to the left of is the headquarters of. The final output is:

r1: <United, has a hub in, Chicago> r2: <Chicago, is the headquarters of, United Continental Holdings>

The great advantage of unsupervised relation extraction is its ability to handle a huge number of relations without having to specify them in advance. The disadvantage is the need to map all the strings into some canonical form for adding to databases or knowledge graphs. Current methods focus heavily on relations expressed with verbs, and so will miss many relations that are expressed nominally.

# 20.2.6 Evaluation of Relation Extraction

Supervised relation extraction systems are evaluated by using test sets with humanannotated, gold-standard relations and computing precision, recall, and F-measure. Labeled precision and recall require the system to classify the relation correctly, whereas unlabeled methods simply measure a system’s ability to detect entities that are related.

Semi-supervised and unsupervised methods are much more difficult to evaluate, since they extract totally new relations from the web or a large text. Because these methods use very large amounts of text, it is generally not possible to run them solely on a small labeled test set, and as a result it’s not possible to pre-annotate a gold set of correct instances of relations.

For these methods it’s possible to approximate (only) precision by drawing a random sample of relations from the output, and having a human check the accuracy of each of these relations. Usually this approach focuses on the tuples to be extracted from a body of text rather than on the relation mentions; systems need not detect every mention of a relation to be scored correctly. Instead, the evaluation is based on the set of tuples occupying the database when the system is finished. That is, we want to know if the system can discover that Ryanair has a hub at Charleroi; we don’t really care how many times it discovers it. The estimated precision $\hat { P }$ is then

$$
\hat { P } = \frac { \# \mathrm { o f ~ c o r r e c t l y ~ e x t r a c t e d ~ r e l a t i o n ~ t u p l e s ~ i n ~ t h e ~ s a m p l e } } { \mathrm { t o t a l ~ } \# \mathrm { o f ~ e x t r a c t e d ~ r e l a t i o n ~ t u p l e s ~ i n ~ t h e ~ s a m p l e } . }
$$

Another approach that gives us a little bit of information about recall is to compute precision at different levels of recall. Assuming that our system is able to rank the relations it produces (by probability, or confidence) we can separately compute precision for the top 1000 new relations, the top 10,000 new relations, the top 100,000, and so on. In each case we take a random sample of that set. This will show us how the precision curve behaves as we extract more and more tuples. But there is no way to directly evaluate recall.

# 20.3 Extracting Events

The task of event extraction is to identify mentions of events in texts. For the purposes of this task, an event mention is any expression denoting an event or state that can be assigned to a particular point, or interval, in time. The following markup of the sample text on page 435 shows all the events in this text.

[EVENT Citing] high fuel prices, United Airlines [EVENT said] Friday it has [EVENT increased] fares by $\$ 6$ per round trip on flights to some cities also served by lower-cost carriers. American Airlines, a unit of AMR Corp., immediately [EVENT matched] [EVENT the move], spokesman Tim Wagner [EVENT said]. United, a unit of UAL Corp., [EVENT said] [EVENT the increase] took effect Thursday and [EVENT applies] to most routes where it [EVENT competes] against discount carriers, such as Chicago to Dallas and Denver to San Francisco.

# light verbs

In English, most event mentions correspond to verbs, and most verbs introduce events. However, as we can see from our example, this is not always the case. Events can be introduced by noun phrases, as in the move and the increase, and some verbs fail to introduce events, as in the phrasal verb took effect, which refers to when the event began rather than to the event itself. Similarly, light verbs such as make, take, and have often fail to denote events. A light verb is a verb that has very little meaning itself, and the associated event is instead expressed by its direct object noun. In light verb examples like took a flight, it’s the word flight that defines the event; these light verbs just provide a syntactic structure for the noun’s arguments.

# reporting events

Various versions of the event extraction task exist, depending on the goal. For example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract events and aspects like their aspectual and temporal properties. Events are to be classified as actions, states, reporting events (say, report, tell, explain), perception events, and so on. The aspect, tense, and modality of each event also needs to be extracted. Thus for example the various said events in the sample text would be annotated as (clas $\mathrm {  ~ s } = \mathrm {  ~ \partial ~ }$ REPORTING, tens PAST, aspect=PERFECTIVE).

Event extraction is generally modeled via supervised learning, detecting events via IOB sequence models and assigning event classes and attributes with multi-class classifiers. The input can be neural models starting from encoders; or classic featurebased models using features like those in Fig. 20.10.

<table><tr><td>Feature</td><td>Explanation</td></tr><tr><td>Character affixes</td><td>Character-level prefixes and suffixes of target word</td></tr><tr><td>Nominalization suffix</td><td>Character-level suffixes for nominalizations (e.g., -tion)</td></tr><tr><td>Part of speech</td><td> Part of speech of the target word</td></tr><tr><td>Light verb</td><td> Binary feature indicating that the target is governed by a light verb</td></tr><tr><td>Subject syntactic category</td><td> Syntactic category of the subject of the sentence</td></tr><tr><td>Morphological stem Verb root</td><td> Stemmed version of the target word</td></tr><tr><td></td><td> Root form of the verb basis for a nominalization</td></tr><tr><td>WordNet hypernyms</td><td>Hypernym set for the target</td></tr></table>

Figure 20.10 Features commonly used in classic feature-based approaches to event detection.

# 20.4 Representing Time

# temporal logic

Let’s begin by introducing the basics of temporal logic and how human languages convey temporal information. The most straightforward theory of time holds that it flows inexorably forward and that events are associated with either points or intervals in time, as on a timeline. We can order distinct events by situating them on the timeline; one event precedes another if the flow of time leads from the first event

interval algebra

to the second. Accompanying these notions in most theories is the idea of the current moment in time. Combining this notion with the idea of a temporal ordering relationship yields the familiar notions of past, present, and future.

Various kinds of temporal representation systems can be used to talk about temporal ordering relationship. One of the most commonly used in computational modeling is the interval algebra of Allen (1984). Allen models all events and time expressions as intervals there is no representation for points (although intervals can be very short). In order to deal with intervals without points, he identifies 13 primitive relations that can hold between these temporal intervals. Fig. 20.11 shows these 13 Allen relations.

# Allen relations

![## Image Analysis: ec256bbabb604d6945e9688a5f2851b9cda90adfc21a3971f2cf682533d47c40.jpg

**Conceptual Understanding:**
The image conceptually represents Allen's Interval Relations, a seminal framework for describing qualitative temporal relationships between two time intervals. Its main purpose is to visually and textually define the 13 possible, mutually exclusive, and exhaustive relationships that can exist between any two intervals, A and B. The key idea communicated is a systematic and complete taxonomy of how temporal events or periods can relate to each other in terms of their starting and ending points, without requiring precise numerical time values. This includes primary relations and their explicit inverses, all ordered along a single 'Time' axis.

**Content Interpretation:**
The image systematically presents the 13 fundamental temporal relations between two intervals, A and B, as defined by Allen (1984). It shows seven distinct configurations of two temporal intervals (represented by rectangles) along a time axis, each paired with its textual description. Each configuration explicitly includes both the primary relation and its inverse, with the exception of the 'equals' relation which is its own inverse. The 'Time' axis provides the context for these temporal orderings. The visual layout clearly demonstrates the relative starting and ending points of interval A and interval B for each defined relationship.

**Key Insights:**
**Key Takeaways:**
1.  **Comprehensive Set of Relations:** The image demonstrates that there are 13 unique and exhaustive qualitative ways any two time intervals can relate to each other. These are 'before', 'after', 'meets', 'meets'', 'overlaps', 'overlaps'', 'starts', 'starts'', 'during', 'during'', 'finishes', 'finishes'', and 'equals'.
2.  **Interval-Based Reasoning:** The relations focus on the relationships between time intervals (periods with a start and end) rather than discrete points in time. This is evident from the rectangular representation of 'A' and 'B'.
3.  **Inverse Relations:** For most relationships (12 out of 13), an explicit inverse relation exists. For example, 'A before B' has the inverse 'B after A'. The diagram clearly shows these inverse pairs, using a prime symbol (e.g., 'meets' A') to denote the inverse for several relations.
4.  **Foundation for Temporal Logic:** These specific textual descriptions and their visual representations form the basis for Allen's interval algebra, a powerful framework for qualitative temporal reasoning in artificial intelligence and related fields. The precise wording ('A before B', 'A meets B', 'A overlaps B', etc.) is the standard terminology used.

**Document Context:**
This image directly illustrates the 'Allen relations', as mentioned in the document's section title and the caption 'Figure 20.11 The 13 temporal relations from Allen (1984)'. It provides the foundational visual and textual definitions for understanding qualitative temporal reasoning, which is a critical concept in fields such as artificial intelligence, natural language processing, and temporal databases. The image is essential for grasping how temporal intervals are conceptually modeled and how their relationships are formally defined, forming a key component of the broader discussion in the document.

**Summary:**
The image meticulously illustrates the 13 temporal relations proposed by James F. Allen (1984), defining all possible qualitative relationships between two time intervals, A and B. Each relationship is visually represented by two light blue rectangular blocks, labeled 'A' and 'B', positioned horizontally along an implied 'Time' axis indicated at the bottom. Accompanying each visual depiction is a precise textual description of the relationship, often including both the primary relation and its inverse.

Here are the 7 primary visual configurations and their associated textual descriptions:

1.  **A before B / B after A:** Interval A is positioned entirely to the left of Interval B, with a clear temporal gap separating the end of A from the beginning of B. The accompanying text states: 'A before B' and 'B after A'.
2.  **A meets B / B meets' A:** Interval A is positioned immediately adjacent to Interval B, with the end point of A coinciding exactly with the start point of B, and no gap between them. The accompanying text states: 'A meets B' and 'B meets' A'.
3.  **A overlaps B / B overlaps' A:** Interval A starts before Interval B. They share a common duration where B begins before A ends, but A finishes before B. The accompanying text states: 'A overlaps B' and 'B overlaps' A'.
4.  **A starts B / B starts' A:** Both Interval A and Interval B commence at the exact same point in time. However, Interval A is depicted as shorter than B and concludes before B ends, effectively being contained within the initial segment of B. The accompanying text states: 'A starts B' and 'B starts' A'.
5.  **A during B / B during' A:** Interval A is entirely encompassed within Interval B, meaning A begins after B has started and ends before B has finished. The accompanying text states: 'A during B' and 'B during' A'.
6.  **A finishes B / B finishes' A:** Both Interval A and Interval B conclude at the exact same point in time. However, Interval B is depicted as longer than A and begins before A starts, effectively containing the entire duration of A within its ending segment. The accompanying text states: 'A finishes B' and 'B finishes' A'.
7.  **A equals B / (B equals A):** Interval A and Interval B are depicted as having identical start and end points, meaning they occupy the exact same temporal span. The accompanying text states: 'A equals B' and '(B equals A)'.

The horizontal line at the very bottom, labeled 'Time' with a right-pointing arrow, serves as the temporal axis for all these relations.](images/ec256bbabb604d6945e9688a5f2851b9cda90adfc21a3971f2cf682533d47c40.jpg)
Figure 20.11 The 13 temporal relations from Allen (1984).

# 20.4.1 Reichenbach’s reference point

The relation between simple verb tenses and points in time is by no means straightforward. The present tense can be used to refer to a future event, as in this example: (20.15) Ok, we fly from San Francisco to Boston at 10.

Or consider the following examples: (20.16) Flight 1902 arrived late. (20.17) Flight 1902 had arrived late.

Although both refer to events in the past, representing them in the same way seems wrong. The second example seems to have another unnamed event lurking in the background (e.g., Flight 1902 had already arrived late when something else happened).

To account for this phenomena, Reichenbach (1947) introduced the notion of a reference point. In our simple temporal scheme, the current moment in time is equated with the time of the utterance and is used as a reference point for when the event occurred (before, at, or after). In Reichenbach’s approach, the notion of the reference point is separated from the utterance time and the event time. The following examples illustrate the basics of this approach:

(20.18) When Mary’s flight departed, I ate lunch.   
(20.19) When Mary’s flight departed, I had eaten lunch.

In both of these examples, the eating event has happened in the past, that is, prior to the utterance. However, the verb tense in the first example indicates that the eating event began when the flight departed, while the second example indicates that the eating was accomplished prior to the flight’s departure. Therefore, in Reichenbach’s terms the departure event specifies the reference point. These facts can be accommodated by additional constraints relating the eating and departure events. In the first example, the reference point precedes the eating event, and in the second example, the eating precedes the reference point. Figure 20.12 illustrates Reichenbach’s approach with the primary English tenses. Exercise 20.4 asks you to represent these examples in FOL.

![## Image Analysis: bdc4a0c81b1f23313ef66158dd4ccb016e4b92c79368073b6560a81307b4c9d4.jpg

**Conceptual Understanding:**
This image conceptually represents and illustrates Reichenbach's theory of tense. Its main purpose is to visually demonstrate how the temporal relationships between the time of an event (E), the reference time (R), and the time of an utterance (U) define various English tenses. The image conveys the idea that different tenses are characterized by distinct orderings or coincidences of these three temporal points on a linear timeline, where time progresses from left to right.

**Content Interpretation:**
The image systematically illustrates Reichenbach's theory of tense, showing the temporal relationships between the event time (E), reference time (R), and utterance time (U) for six different English tenses: Past Perfect, Simple Past, Present Perfect, Present, Simple Future, and Future Perfect. Each tense is represented by a horizontal timeline with an arrow indicating the flow of time from left to right. The relative positions of E, R, and U on each timeline define the specific tense. For instance, in 'Past Perfect', E is before R, and R is before U. In 'Simple Past', R and E coincide and are both before U. In 'Present Perfect', E is before R and U, which coincide. For 'Present', U, R, and E all coincide. For 'Simple Future', U and R coincide and are both before E. Lastly, for 'Future Perfect', U is before E, and E is before R. This visual representation clearly shows how the three temporal points can be ordered and coincide to distinguish between these tenses.

**Key Insights:**
The main takeaway from this image is that Reichenbach's theory provides a precise and systematic method for analyzing and differentiating English tenses based on the relative ordering and coincidence of three fundamental temporal points: the event time (E), the reference time (R), and the utterance time (U). The specific arrangements of 'E', 'R', and 'U' for each of the six tenses ('Past Perfect', 'Simple Past', 'Present Perfect', 'Present', 'Simple Future', 'Future Perfect') are the key insights. For example, the diagram for 'Past Perfect' showing 'E R U' explicitly illustrates that the event occurred before the reference point, which itself occurred before the act of uttering. Conversely, for 'Present', the 'U,R,E' notation demonstrates that all three temporal points coincide. This highlights that each tense has a unique temporal signature defined by these three points, allowing for a structured understanding of their meaning and usage.

**Document Context:**
This image directly supports section 20.4.1, 'Reichenbach's reference point', by providing a visual application of his approach to various English tenses. It serves as a concrete illustration of the theoretical framework discussed in the surrounding text, specifically detailing how the abstract concepts of event time (E), reference time (R), and utterance time (U) manifest in the temporal structure of different grammatical tenses. The image, described as 'Figure 20.12 Reichenbach's approach applied to various English tenses', is crucial for understanding the practical implications of Reichenbach's theory in linguistic analysis.

**Summary:**
The image presents a series of six horizontal timelines, each illustrating Reichenbach's theory of tense by depicting the temporal relationship between three key points: Event time (E), Reference time (R), and Utterance time (U). Time consistently flows from left to right on all timelines, indicated by an arrow at the right end. The diagrams are arranged in two rows of three, with each diagram representing a different English tense. 

The top row begins with 'Past Perfect', where the event (E) occurs first, followed by the reference time (R), and then the utterance time (U). The next diagram in the top row, 'Simple Past', shows the reference time (R) and event time (E) coinciding, both occurring before the utterance time (U). The third diagram in the top row, 'Present Perfect', illustrates the event time (E) occurring before the reference time (R) and utterance time (U), which coincide. 

The bottom row starts with 'Present', where the utterance time (U), reference time (R), and event time (E) all coincide at a single point on the timeline. Following this is 'Simple Future', which shows the utterance time (U) and reference time (R) coinciding, both occurring before the event time (E). Finally, 'Future Perfect' depicts the utterance time (U) occurring first, followed by the event time (E), and then the reference time (R). This comprehensive visual representation allows for a clear understanding of the distinct temporal configurations for each specified tense.](images/bdc4a0c81b1f23313ef66158dd4ccb016e4b92c79368073b6560a81307b4c9d4.jpg)
Figure 20.12 Reichenbach’s approach applied to various English tenses. In these diagrams, time flows from left to right, E denotes the time of the event, $\mathbf { R }$ denotes the reference time, and U denotes the time of the utterance.

Languages have many other ways to convey temporal information besides tense. Most useful for our purposes will be temporal expressions like in the morning or 6:45 or afterwards.

(20.20) I’d like to go at 6:45 in the morning.   
(20.21) Somewhere around noon, please.   
(20.22) I want to take the train back afterwards.

Incidentally, temporal expressions display a fascinating metaphorical conceptual organization. Temporal expressions in English are frequently expressed in spatial terms, as is illustrated by the various uses of at, in, somewhere, and near in these examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations such as these, in which one domain is systematically expressed in terms of another, are very common in languages of the world.

# 20.5 Representing Aspect

# aspect

aktionsart events states stative

A related notion to time is aspect, which is what we call the way events can be categorized by their internal temporal structure or temporal contour. By this we mean questions like whether events are ongoing or have ended, or whether they are conceptualized as happening at a point in time or over some interval. Such notions of temporal contour have been used to divide event expressions into classes since Aristotle, although the set of four classes we’ll introduce here is due to Vendler (1967) (you may also see the German term aktionsart used to refer to these classes).

The most basic aspectual distinction is between events (which involve change) and states (which do not involve change). Stative expressions represent the notion of an event participant being in a state, or having a particular property, at a given point in time. Stative expressions capture aspects of the world at a single point in time, and conceptualize the participant as unchanging and continuous. Consider the following ATIS examples.

activity

(20.23) I like express trains.   
(20.24) I need the cheapest fare.   
(20.25) I want to go first class.

In examples like these, the event participant denoted by the subject can be seen as experiencing something at a specific point in time, and don’t involve any kind of internal change over time (the liking or needing is conceptualized as continuous and unchanging).

Non-states (which we’ll refer to as events) are divided into subclasses; we’ll introduce three here. Activity expressions describe events undertaken by a participant that occur over a span of time (rather than being conceptualized as a single point in time like stative expressions), and have no particular end point. Of course in practice all things end, but the meaning of the expression doesn’t represent this fact. Consider the following examples:

telic accomplishment expressions

(20.26) She drove a Mazda.   
(20.27) I live in Brooklyn.

These examples both specify that the subject is engaged in, or has engaged in, the activity specified by the verb for some period of time, but doesn’t specify when the driving or living might have stopped.

Two more classes of expressions, achievement expressions and accomplishment expressions, describe events that take place over time, but also conceptualize the event as having a particular kind of endpoint or goal. The Greek word telos means ‘end’ or ’goal’ and so the events described by these kinds of expressions are often called telic events.

Accomplishment expressions describe events that have a natural end point and result in a particular state. Consider the following examples:

(20.28) He booked me a reservation.   
(20.29) The 7:00 train got me to New York City.

In these examples, an event is seen as occurring over some period of time that ends when the intended state is accomplished (i.e., the state of me having a reservation, or me being in New York City).

The final aspectual class, achievement expressions, is only subtly different than accomplishments. Consider the following:

(20.30) She found her gate.   
(20.31) I reached New York.

Like accomplishment expressions, achievement expressions result in a state. But unlike accomplishments, achievement events are ‘punctual’: they are thought of as happening in an instant and the verb doesn’t conceptualize the process or activity leading up the state. Thus the events in these examples may in fact have been preceded by extended searching or traveling events, but the verb doesn’t conceptualize these preceding processes, but rather conceptualizes the events corresponding to finding and reaching as points, not intervals.

In summary, a standard way of categorizing event expressions by their temporal contours is via these four general classes:

Stative: I know my departure gate.

Activity: John is flying.

Accomplishment: Sally booked her flight.

Achievement: She found her gate.

Before moving on, note that event expressions can easily be shifted from one class to another. Consider the following examples:

(20.32) I flew.   
(20.33) I flew to New York.

The first example is a simple activity; it has no natural end point. The second example is clearly an accomplishment event since it has an end point, and results in a particular state. Clearly, the classification of an event is not solely governed by the verb, but by the semantics of the entire expression in context.

# 20.6 Temporally Annotated Datasets: TimeBank

# TimeBank

The TimeBank corpus consists of American English text annotated with temporal information (Pustejovsky et al., 2003). The annotations use TimeML (Saur´ı et al., 2006), a markup language for time based on Allen’s interval algebra discussed above (Allen, 1984). There are three types of TimeML objects: an EVENT represent events and states, a TIME represents time expressions like dates, and a LINK represents various relationships between events and times (event-event, event-time, and timetime). The links include temporal links (TLINK) for the 13 Allen relations, aspectual links (ALINK) for aspectual relationships between events and subevents, and SLINKS which mark factuality.

Consider the following sample sentence and its corresponding markup shown in Fig. 20.13, selected from one of the TimeBank documents.

(20.34) Delta Air Lines earnings soared $33 \%$ to a record in the fiscal first quarter, bucking the industry trend toward declining profits.

This text has three events and two temporal expressions (including the creation time of the article, which serves as the document time), and four temporal links that capture the using the Allen relations:

• $\operatorname { S o a r i n g } _ { e 1 }$ is included in the fiscal first quartert58 • $\operatorname { S o a r i n g } _ { e 1 }$ is before $1 9 8 9 - 1 0 - 2 6 _ { t 5 7 }$ • $\operatorname { S o a r i n g } _ { e 1 }$ is simultaneous with the buckinge3

<TIMEX3 tid="t57" type $\mathbf { \Psi } = \mathbf { \Psi }$ "DATE" value $=$ "1989-10-26" functionInDocument $=$ "CREATION_TIME"> 10/26/89 </TIMEX3>   
Delta Air Lines earnings <EVENT eid="e1" clas $: =$ "OCCURRENCE"> soared </EVENT> $3 3 \%$ to a record in <TIMEX3 tid="t58" type $\mathrel { \mathop : } \stackrel { \prime } { = }$ "DATE" value $\equiv$ "1989-Q1" anchorTimeID="t57"> the fiscal first quarter </TIMEX3>, <EVENT eid="e3" class $: = ^ { \dag }$ "OCCURRENCE">bucking</EVENT> the industry trend toward <EVENT eid="e4" class="OCCURRENCE">declining</EVENT> profits.

Figure 20.13 Example from the TimeBank corpus.

# • Declininge4 includes soaringe1

We can also visualize the links as a graph. The TimeBank snippet in Eq. 20.35 would be represented with a graph like Fig. 20.14.

(20.35) $\mathbf { [ D C T : 1 1 / 0 2 / 8 9 1 ] _ { 1 } }$ : Pacific First Financial Corp. $\mathbf { s a i d } _ { 2 }$ shareholders approved3 its acquisition4 by Royal Trustco Ltd. of Toronto for $\$ 27$ a share, or $\$ 212$ million. The thrift holding company $\mathbf { s a i d } _ { 5 }$ it expects6 to obtain7 regulatory approval8 and complete9 the transaction $^ { 1 0 }$ by year-end11.

![## Image Analysis: 5889e69d764a637568a1038bd4ae3e56f9bd14c21d6b97d7eeb065d294c6b084.jpg

**Conceptual Understanding:**
The image conceptually represents a network of directed semantic relationships between eleven discrete entities or events. Its main purpose is to visualize and categorize these relationships, demonstrating how different types of links—temporal, evidential, modal, factive, and culminating—connect these entities. The diagram communicates the idea of structured dependency and influence between components of a system or narrative, where the nature of the connection is explicitly labeled and visually distinguished (e.g., by color and line style for TLINKS, ALINKS, and SLINKS). It allows for the understanding of complex interactions beyond simple sequential ordering, incorporating aspects like causality, conditionality, and sentiment.

**Content Interpretation:**
The image illustrates a directed graph or network diagram representing relationships between eleven distinct entities or events, labeled 1 through 11. The relationships are categorized by various semantic types, including temporal ('BEFORE', 'AFTER', 'SIMULTANEOUS', 'ENDS'), evidential ('EVIDENTIAL'), modal ('MODAL'), factive ('FACTIVE'), and a culminating relationship ('CULMINATES'). The use of different line colors and styles (blue solid, green dotted, red solid) indicates different categories of links, which are explicitly identified in the document context as TLINKS (Temporal Links), ALINKS (Aspectual Links), and SLINKS (Sentiment Links). This allows for the precise representation of complex interdependencies where, for example, a temporal sequence can be simultaneously supported by an evidential relationship, or a modal condition can lead to a factive outcome. The diagram comprehensively maps out these structured connections.

**Key Insights:**
The main takeaways from this image are: 1. It visually encodes various semantic relationships between eleven distinct nodes, which likely represent events, states, or concepts in a system. 2. The relationships are multifaceted, encompassing temporal (BEFORE, AFTER, SIMULTANEOUS, ENDS), evidential (EVIDENTIAL), modal (MODAL), factive (FACTIVE), and culminating (CULMINATES) aspects. 3. The diagram uses a color-coding and line-style system to differentiate between link types: blue solid lines for Temporal Links (TLINKS), green dotted lines for Aspectual Links (ALINKS), and red solid lines for Sentiment Links (SLINKS), as stated in the document context. 4. The diagram demonstrates complex dependencies, showing how a single node can be involved in multiple types of relationships (e.g., node 2 is 'BEFORE' node 1, 'BEFORE' node 3, and 'SIMULTANEOUS' with node 5). 5. It illustrates that relationships can flow in multiple directions (e.g., 2 'BEFORE' 1, but 3 'AFTER' 2). The textual evidence from the arrow labels directly supports the understanding of each specific relationship type and its direction.

**Document Context:**
This diagram is presented as 'Figure 20.14 A graph of the text in Eq. 20.35, adapted from (Ocal et al., 2022).' This indicates that the image serves as a visual representation or model derived from a specific equation or linguistic analysis, likely pertaining to the 'Declininge4 includes soaringe1' section. It visually explains the relationships between various elements (nodes) that are quantitatively or qualitatively described by Equation 20.35 in the original text. The different types of links (TLINKS, ALINKS, SLINKS) shown in blue, green, and red, respectively, further connect the visual representation to specific theoretical constructs or analytical categories discussed in the academic paper by Ocal et al., 2022. It provides a detailed, structured view of how different concepts or events relate, enriching the reader's understanding of the textual content.

**Summary:**
This image is a network diagram illustrating various relationships between eleven numbered nodes, representing discrete events, states, or concepts. The nodes are numbered 1 through 11. Different types of links (Temporal, Aspectual, and Sentiment) are shown using distinct colors and line styles: blue solid lines for Temporal Links (TLINKS), green dotted lines for Aspectual Links (ALINKS), and a red solid line for a Sentiment Link (SLINK). The diagram maps out a complex interplay of temporal sequencing, evidential support, modality, factivity, and culmination. For instance, node 2 occurs 'BEFORE' node 1 and 'BEFORE' node 3, while node 5 is 'SIMULTANEOUS' with node 2. Node 5 also provides 'EVIDENTIAL' support for node 6, which in turn establishes a 'MODAL' relationship with node 7. The diagram provides a comprehensive visual representation of how these various relationships intertwine to describe a process or system, as indicated by the interconnected nodes and their specific link labels. The systematic mapping details the intricate dependencies, allowing for a thorough understanding of the represented workflow or conceptual structure. It shows that actions or states can be antecedent ('BEFORE'), consequent ('AFTER'), concurrent ('SIMULTANEOUS'), supportive ('EVIDENTIAL'), conditional ('MODAL'), factual ('FACTIVE'), concluding ('ENDS'), or climactic ('CULMINATES'), thereby elucidating a rich semantic network.](images/5889e69d764a637568a1038bd4ae3e56f9bd14c21d6b97d7eeb065d294c6b084.jpg)
Figure 20.14 A graph of the text in Eq. 20.35, adapted from (Ocal et al., 2022). TLINKS are shown in blue, ALINKS in red, and SLINKS in green.

# 20.7 Automatic Temporal Analysis

Here we introduce the three common steps used in analyzing time in text:

1. Extracting temporal expressions   
2. Normalizing these expressions, by converting them to a standard format.   
3. Linking events to times and extracting time graphs and timelines

# 20.7.1 Extracting Temporal Expressions

absolute relative duration

Temporal expressions are phrases that refer to absolute points in time, relative times, durations, and sets of these. Absolute temporal expressions are those that can be mapped directly to calendar dates, times of day, or both. Relative temporal expressions map to particular times through some other reference point (as in a week from last Tuesday). Finally, durations denote spans of time at varying levels of granularity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample temporal expressions in each of these categories.

Temporal expressions are grammatical constructions that often have temporal lexical triggers as their heads, making them easy to find. Lexical triggers might be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist of their phrasal projections: noun phrases, adjective phrases, and adverbial phrases (Figure 20.16).

<table><tr><td>Absolute</td><td>Relative</td><td>Durations</td></tr><tr><td>April 24, 1916</td><td>yesterday</td><td>four hours</td></tr><tr><td>The summer of &#x27;77</td><td>next semester</td><td> three weeks</td></tr><tr><td>10:15 AM</td><td> two weeks from yesterday</td><td> six days</td></tr><tr><td>The 3rd quarter of 2006</td><td> last quarter</td><td> the last three quarters</td></tr></table>

Figure 20.15 Examples of absolute, relational and durational temporal expressions.

<table><tr><td>Category</td><td>Examples</td></tr><tr><td>Noun</td><td>morning,noon, night, winter, dusk,dawn</td></tr><tr><td>Proper Noun Adjective</td><td>January, Monday, Ides,Easter, Rosh Hashana,Ramadan, Tet</td></tr><tr><td>Adverb</td><td>recent, past, annual, former hourly, daily, monthly,yearly</td></tr></table>

Figure 20.16 Examples of temporal lexical triggers.

The task is to detect temporal expressions in running text, like this examples, shown with TIMEX3 tags (Pustejovsky et al. 2005, Ferro et al. 2005).

A fare increase initiated ${ \bf \mathrm { < T I M E X 3 > } }$ last week ${ < } / \mathrm { T I M E X } 3 { > }$ by UAL Corp’s United Airlines was matched by competitors over ${ < } \mathrm { T I M E X } 3 { > }$ the weekend ${ < } / \mathrm { T I M E X } 3 { > }$ , marking the second successful fare increase in <TIMEX3 $>$ two week $\scriptstyle \sum / \mathrm { T I M E X } 3 >$ .

Rule-based approaches use cascades of regular expressions to recognize larger and larger chunks from previous stages, based on patterns containing parts of speech, trigger words (e.g., February) or classes (e.g., MONTH) (Chang and Manning, 2012; Strotgen and Gertz ¨ , 2013; Chambers, 2013). Here’s a rule from SUTime (Chang and Manning, 2012) for detecting expressions like 3 years old:

$$
/ ( \setminus \mathrm { d } + ) \left[ - \setminus \mathsf { s } \right] ( \ S \mathrm { T E U n i t s } ) ( \mathsf { s } ) ? ( [ - \setminus \mathsf { s } ] \circ 1 \mathrm { d } ) ? /
$$

Sequence-labeling approaches use the standard IOB scheme, marking words that are either (I)nside, (O)utside or at the (B)eginning of a temporal expression:

A fare increase initiated last week by UAL Corp’s...

O O O O B I O O O

A statistical sequence labeler is trained, using either embeddings or a fine-tuned encoder, or classic features extracted from the token and context including words, lexical triggers, and POS.

Temporal expression recognizers are evaluated with the usual recall, precision, and $F$ -measures. A major difficulty for all of these very lexicalized approaches is avoiding expressions that trigger false positives:

(20.36) 1984 tells the story of Winston Smith... (20.37) ...U2’s classic Sunday Bloody Sunday

# 20.7.2 Temporal Normalization

Temporal normalization is the task of mapping a temporal expression to a point in time or to a duration. Points in time correspond to calendar dates, to times of day, or both. Durations primarily consist of lengths of time. Normalized times

<table><tr><td>&lt;TIMEX3 id=&quot;t1</td><td>type=&quot;DATE&quot; July 2，2007 &lt;/TIMEX3&gt; A fare increase</td><td>value=&quot;2007-07-02”</td><td></td><td>functionInDocument=&quot;CREATION_TIME&#x27;</td></tr><tr><td>value=&quot;2007-W26” anchorTimeID=&quot;t1&quot;&gt;last week&lt;/TIMEX3&gt; by United Airlines</td><td></td><td></td><td>initiated&lt;TIMEX3 id=&quot;t2”</td><td>type=&quot;DATE&quot;</td></tr><tr><td></td><td></td><td></td><td></td><td></td></tr><tr><td>matched by competitors over &lt;TIMEX3 id=&quot;t3&quot;</td><td></td><td></td><td> ty pe=&quot;DURATION”value=&quot;P1WE&quot;</td><td></td></tr><tr><td>anchorTimeID=&quot;t1&quot;&gt; the weekend &lt;/TIMEX3&gt;</td><td></td><td></td><td>marking the second successful fare</td><td></td></tr><tr><td>increase in &lt;TIMEX3 id=&quot;t4” type=&quot;DURATION” weeks &lt;/TIMEX3&gt;.</td><td></td><td></td><td>value=&quot;P2W&quot;</td><td>anchorTimeID=&quot;t1&quot;&gt; two</td></tr></table>

Figure 20.17 TimeML markup including normalized values for temporal expressions.

are represented via the ISO 8601 standard for encoding temporal values (ISO8601, 2004). Fig. 20.17 reproduces our earlier example with these value attributes.

The dateline, or document date, for this text was July 2, 2007. The ISO representation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02. The encodings for the temporal expressions in our sample text all follow from this date, and are shown here as values for the VALUE attribute.

The first temporal expression in the text proper refers to a particular week of the year. In the ISO standard, weeks are numbered from 01 to 53, with the first week of the year being the one that has the first Thursday of the year. These weeks are represented with the template YYYY-Wnn. The ISO week for our document date is week 27; thus the value for last week is represented as “2007-W26”.

The next temporal expression is the weekend. ISO weeks begin on Monday; thus, weekends occur at the end of a week and are fully contained within a single week. Weekends are treated as durations, so the value of the VALUE attribute has to be a length. Durations are represented according to the pattern $ { \mathrm { P } } n x$ , where $n$ is an integer denoting the length and $x$ represents the unit, as in P3Y for three years or P2D for two days. In this example, one weekend is captured as P1WE. In this case, there is also sufficient information to anchor this particular weekend as part of a particular week. Such information is encoded in the ANCHORTIMEID attribute. Finally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18 give some more examples, but there is a lot more to the various temporal annotation standards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005) for more details.

<table><tr><td>Unit</td><td>Pattern</td><td>Sample Value</td></tr><tr><td>Fully specified dates</td><td>YYYY-MM-DD</td><td>1991-09-28</td></tr><tr><td>Weeks</td><td>YYYY-Wnn</td><td>2007-W27</td></tr><tr><td>Weekends</td><td>PnWE</td><td>P1WE</td></tr><tr><td>24-hour clock times</td><td>HH:MM:SS</td><td>11:13:45</td></tr><tr><td>Dates and times</td><td>YYYY-MM-DDTHH:MM:SS</td><td>1991-09-28T11:00:00</td></tr><tr><td>Financial quarters</td><td>Qn</td><td>1999-Q3</td></tr></table>

Figure 20.18 Sample ISO patterns for representing various times and durations.

Most current approaches to temporal normalization are rule-based (Chang and Manning 2012, Strotgen and Gertz ¨ 2013). Patterns that match temporal expressions are associated with semantic analysis procedures. For example, the pattern above for recognizing phrases like 3 years old can be associated with the predicate Duration that takes two arguments, the length and the unit of time:

pattern: $/ ( \setminus \mathrm { d } + ) [ - \setminus \mathsf { s } ]$ (\$TEUnits)(s)?([-\s]old)?/ result: Duration(\$1, $\$ 2$ )

The task is difficult because fully qualified temporal expressions are fairly rare in real texts. Most temporal expressions in news articles are incomplete and are only implicitly anchored, often with respect to the dateline of the article, which we refer to as the document’s temporal anchor. The values of temporal expressions such as today, yesterday, or tomorrow can all be computed with respect to this temporal anchor. The semantic procedure for today simply assigns the anchor, and the attachments for tomorrow and yesterday add a day and subtract a day from the anchor, respectively. Of course, given the cyclic nature of our representations for months, weeks, days, and times of day, our temporal arithmetic procedures must use modulo arithmetic appropriate to the time unit being used.

Unfortunately, even simple expressions such as the weekend or Wednesday introduce a fair amount of complexity. In our current example, the weekend clearly refers to the weekend of the week that immediately precedes the document date. But this won’t always be the case, as is illustrated in the following example.

(20.38) Random security checks that began yesterday at Sky Harbor will continue at least through the weekend.

In this case, the expression the weekend refers to the weekend of the week that the anchoring date is part of (i.e., the coming weekend). The information that signals this meaning comes from the tense of continue, the verb governing the weekend.

Relative temporal expressions are handled with temporal arithmetic similar to that used for today and yesterday. The document date indicates that our example article is ISO week 27, so the expression last week normalizes to the current week minus 1. To resolve ambiguous next and last expressions we consider the distance from the anchoring date to the nearest unit. Next Friday can refer either to the immediately next Friday or to the Friday following that, but the closer the document date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such ambiguities are handled by encoding language and domain-specific heuristics into the temporal attachments.

# 20.7.3 Temporal Ordering of Events

The goal of temporal analysis, is to link times to events and then fit all these events into a complete timeline. This ambitious task is the subject of considerable current research but solving it with a high level of accuracy is beyond the capabilities of current systems. A somewhat simpler, but still useful, task is to impose a partial ordering on the events and temporal expressions mentioned in a text. Such an ordering can provide many of the same benefits as a true timeline. An example of such a partial ordering is the determination that the fare increase by American Airlines came after the fare increase by United in our sample text. Determining such an ordering can be viewed as a binary relation detection and classification task.

Even this partial ordering task assumes that in addition to the detecting and normalizing time expressions steps described above, we have already detected all the events in the text. Indeed, many temporal expressions are anchored to events mentioned in a text and not directly to other temporal expressions. Consider the following example:

(20.39) One week after the storm, JetBlue issued its customer bill of rights.

To determine when JetBlue issued its customer bill of rights we need to determine the time of the storm event, and then we need to modify that time by the temporal expression one week after.

Thus once the events and times have been detected, our goal next is to assert links between all the times and events: i.e. creating event-event, event-time, time-time, DCT-event, and DCT-time TimeML TLINKS. This can be done by training time relation classifiers to predict the correct T:INK between each pair of times/events, supervised by the gold labels in the TimeBank corpus with features like words/embeddings, parse paths, tense and aspect The sieve-based architecture using precisionranked sets of classifiers, which we’ll introduce in Chapter 23, is also commonly used.

Systems that perform all 4 tasks (time extraction creation and normalization, event extraction, and time/event linking) include TARSQI (Verhagen et al., 2005) CLEARTK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza and Tonelli, 2016).

# 20.8 Template Filling

# scripts

templates template filling

Many texts contain reports of events, and possibly sequences of events, that often correspond to fairly common, stereotypical situations in the world. These abstract situations or stories, related to what have been called scripts (Schank and Abelson, 1977), consist of prototypical sequences of sub-events, participants, and their roles. The strong expectations provided by these scripts can facilitate the proper classification of entities, the assignment of entities into roles and relations, and most critically, the drawing of inferences that fill in things that have been left unsaid. In their simplest form, such scripts can be represented as templates consisting of fixed sets of slots that take as values slot-fillers belonging to particular classes. The task of template filling is to find documents that invoke particular scripts and then fill the slots in the associated templates with fillers extracted from the text. These slot-fillers may consist of text segments extracted directly from the text, or they may consist of concepts that have been inferred from text elements through some additional processing.

A filled template from our original airline story might look like the following.

<table><tr><td>FARE-RAISE ATTEMPT:</td><td>LEAD AIRLINE: AMOUNT:</td><td>UNITED AIRLINES $6</td></tr><tr><td></td><td>EFFECTIVE DATE:</td><td>2006-10-26</td></tr><tr><td></td><td>FOLLOWER:</td><td>AMERICAN AIRLINES</td></tr></table>

This template has four slots (LEAD AIRLINE, AMOUNT, EFFECTIVE DATE, FOLLOWER). The next section describes a standard sequence-labeling approach to filling slots. Section 20.8.2 then describes an older system based on the use of cascades of finite-state transducers and designed to address a more complex template-filling task that current learning-based systems don’t yet address.

# 20.8.1 Machine Learning Approaches to Template Filling

In the standard paradigm for template filling, we are given training documents with text spans annotated with predefined templates and their slot fillers. Our goal is to create one template for each event in the input, filling in the slots with text spans.

The task is generally modeled by training two separate supervised systems. The first system decides whether the template is present in a particular sentence. This task is called template recognition or sometimes, in a perhaps confusing bit of terminology, event recognition. Template recognition can be treated as a text classification task, with features extracted from every sequence of words that was labeled in training documents as filling any slot from the template being detected. The usual set of features can be used: tokens, embeddings, word shapes, part-of-speech tags, syntactic chunk tags, and named entity tags.

The second system has the job of role-filler extraction. A separate classifier is trained to detect each role (LEAD-AIRLINE, AMOUNT, and so on). This can be a binary classifier that is run on every noun-phrase in the parsed input sentence, or a sequence model run over sequences of words. Each role classifier is trained on the labeled data in the training set. Again, the usual set of features can be used, but now trained only on an individual noun phrase or the fillers of a single slot.

Multiple non-identical text segments might be labeled with the same slot label. For example in our sample text, the strings United or United Airlines might be labeled as the LEAD AIRLINE. These are not incompatible choices and the coreference resolution techniques introduced in Chapter 23 can provide a path to a solution.

A variety of annotated collections have been used to evaluate this style of approach to template filling, including sets of job announcements, conference calls for papers, restaurant guides, and biological texts. A key open question is extracting templates in cases where there is no training data or even predefined templates, by inducing templates as sets of linked events (Chambers and Jurafsky, 2011).

# 20.8.2 Earlier Finite-State Template-Filling Systems

The templates above are relatively simple. But consider the task of producing a template that contained all the information in a text like this one (Grishman and Sundheim, 1995):

Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan with a local concern and a Japanese trading house to produce golf clubs to be shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capitalized at 20 million new Taiwan dollars, will start production in January 1990 with production of 20,000 iron and “metal wood” clubs a month.

The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were a series of U.S. government-organized information-extraction evaluations) was to produce hierarchically linked templates describing joint ventures. Figure 20.19 shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how the filler of the ACTIVITY slot of the TIE-UP template is itself a template with slots.

<table><tr><td colspan="2">Tie-up-1</td><td colspan="2">Activity-1:</td></tr><tr><td>RELATIONSHIP</td><td>tie-up</td><td>COMPANY</td><td>Bridgestone Sports Taiwan Co.</td></tr><tr><td rowspan="2">ENTITIES</td><td>Bridgestone Sports Co.</td><td>PRODUCT</td><td> iron and “metal wood&quot; clubs</td></tr><tr><td>a local concern a Japanese trading house</td><td> START DATE</td><td> DURING: January 1990</td></tr><tr><td> JOINT VENTURE</td><td colspan="3"> Bridgestone Sports Taiwan Co.</td></tr><tr><td>ACTIVITY</td><td colspan="3">Activity-1</td></tr><tr><td>AMOUNT</td><td colspan="3">NT$20000000</td></tr></table>

Figure 20.19 The templates produced by FASTUS given the input text on page 457.

Early systems for dealing with these complex templates were based on cascades of transducers based on handwritten rules, as sketched in Fig. 20.20.

The first four stages use handwritten regular expression and grammar rules to do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and events with a recognizer based on finite-state transducers (FSTs), and inserts the recognized objects into the appropriate slots in templates. This FST recognizer is based on hand-built regular expressions like the following (NG indicates Noun-Group and VG Verb-Group), which matches the first sentence of the news story above.

<table><tr><td>No. Step</td><td></td><td>Description</td></tr><tr><td>1</td><td>Tokens</td><td> Tokenize input stream of characters</td></tr><tr><td>2</td><td>Complex Words</td><td> Multiword phrases, numbers, and proper names.</td></tr><tr><td>3</td><td>Basic phrases</td><td> Segment sentences into noun and verb groups</td></tr><tr><td>4</td><td>Complex phrases</td><td> Identify complex noun groups and verb groups</td></tr><tr><td>5</td><td>Semantic Patterns</td><td> Identify entities and events, insert into templates.</td></tr><tr><td>6</td><td>Merging</td><td> Merge references to the same entity or event</td></tr></table>

Figure 20.20 Levels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a specific type of information which is then passed on to the next higher level.

NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies) VG(Produce) NG(Product)

The result of processing these two sentences is the five draft templates (Fig. 20.21) that must then be merged into the single hierarchical structure shown in Fig. 20.19. The merging algorithm, after performing coreference resolution, merges two activities that are likely to be describing the same events.

<table><tr><td># Template/Slot</td><td>Value</td><td></td></tr><tr><td>1</td><td>RELATIONSHIP:</td><td>TIE-UP</td><td rowspan="5"></td></tr><tr><td>2</td><td>ENTITIES:</td><td>Bridgestone Co., a local concern, a Japanese trading house</td></tr><tr><td></td><td>ACTIVITY:</td><td>PRODUCTION</td></tr><tr><td>3</td><td>PRODUCT:</td><td>&quot;golf clubs&quot;</td></tr><tr><td></td><td>RELATIONSHIP:</td><td>TIE-UP</td></tr><tr><td></td><td>JOINT VENTURE:</td><td>“Bridgestone Sports Taiwan Co.&quot;</td></tr><tr><td></td><td>AMOUNT:</td><td>NT$20000000</td></tr><tr><td>4</td><td>ACTIVITY:</td><td>PRODUCTION</td></tr><tr><td></td><td>COMPANY:</td><td>&quot;Bridgestone Sports Taiwan Co.&quot;</td></tr><tr><td>5</td><td>STARTDATE:</td><td>DURING: January 1990</td></tr><tr><td>PRODUCT:</td><td>ACTIVITY:</td><td>PRODUCTION &quot;iron and “metal wood’ clubs&quot;</td></tr></table>

Figure 20.21 The five partial templates produced by stage 5 of FASTUS. These templates are merged in stage 6 to produce the final template shown in Fig. 20.19 on page 457.

# 20.9 Summary

This chapter has explored techniques for extracting limited forms of semantic content from texts.

• Relations among entities can be extracted by pattern-based approaches, supervised learning methods when annotated training data is available, lightly supervised bootstrapping methods when small numbers of seed tuples or seed patterns are available, distant supervision when a database of relations is available, and unsupervised or Open IE methods.   
• Reasoning about time can be facilitated by detection and normalization of temporal expressions.

• Events can be ordered in time using sequence models and classifiers trained on temporally- and event-labeled data like the TimeBank corpus. • Template-filling applications can recognize stereotypical situations in texts and assign elements from the text to roles represented as fixed sets of slots.

# Bibliographical and Historical Notes

The earliest work on information extraction addressed the template-filling task in the context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S. government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sundheim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert et al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite influential and inspired later systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the MUC evaluation techniques.

Due to the difficulty of porting systems from one domain to another, attention shifted to machine learning approaches. Early supervised learning approaches to IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996) focused on automating the knowledge acquisition process, mainly for finite-state rule-based systems. Their success, and the earlier success of HMM-based speech recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of features (Zhou et al., 2005). Neural approaches followed from the pioneering results of Collobert et al. (2011), who applied a CRF on top of a convolutional net.

Progress in this area continues to be stimulated by formal evaluations with shared benchmark datasets, including the Automatic Content Extraction (ACE) evaluations of 2000-2007 on named entity recognition, relation extraction, and temporal expressions1 , the KBP (Knowledge Base Population) evaluations (Ji et al. 2010, Surdeanu 2013) of relation extraction tasks like slot filling (extracting attributes (‘slots’) like age, birthplace, and spouse for a given entity) and a series of SemEval workshops (Hendrickx et al., 2009).

Semisupervised relation extraction was first proposed by Hearst (1992b), and extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOWBALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant supervision algorithm we describe was drawn from Mintz et al. (2009), who first used the term ‘distant supervision’ (which was suggested to them by Chris Manning) but similar ideas had occurred in earlier systems like Craven and Kumlien (1999) and Morgan et al. (2004) under the name weakly labeled data, as well as in Snow et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include KNOWITALL Etzioni et al. (2005), TextRunner (Banko et al., 2007), and REVERB (Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines the advantages of distant supervision and Open IE.

# Exercises

20.1 Acronym expansion, the process of associating a phrase with an acronym, can be accomplished by a simple form of relational analysis. Develop a system based on the relation analysis approaches described in this chapter to populate a database of acronym expansions. If you focus on English Three Letter Acronyms (TLAs) you can evaluate your system’s performance by comparing it to Wikipedia’s TLA page.

20.2 Acquire the CMU seminar corpus and develop a template-filling system by using any of the techniques mentioned in Section 20.8. Analyze how well your system performs as compared with state-of-the-art results on this corpus.

20.3 A useful functionality in newer email and calendar applications is the ability to associate temporal expressions connected with events in email (doctor’s appointments, meeting planning, party invitations, etc.) with specific calendar entries. Collect a corpus of email containing temporal expressions related to event planning. How do these expressions compare to the kinds of expressions commonly found in news text that we’ve been discussing in this chapter?

20.4 For the following sentences, give FOL translations that capture the temporal relationships between the events.

1. When Mary’s flight departed, I ate lunch.   
2. When Mary’s flight departed, I had eaten lunch.

# 21 Semantic Role Labeling

“Who, What, Where, When, With what, Why, How” The seven circumstances, associated with Hermagoras and Aristotle (Sloan, 2010)

Sometime between the 7th and 4th centuries BCE, the Indian grammarian Pan¯ . ini1 wrote a famous treatise on Sanskrit grammar, the As.t.adhy ¯ ay¯ ¯ı (‘8 books’), a treatise that has been called “one of the greatest monuments of human intelligence” (Bloomfield, 1933, 11). The work describes the linguistics of the Sanskrit language in the form of 3959 sutras, each very efficiently (since it had to be memorized!) expressing part of a formal rule system that brilliantly prefigured modern mechanisms of formal language theory (Penn and Kiparsky, 2012). One set of rules describes the karakas ¯ , semantic relationships between a verb and noun arguments, roles like agent, instrument, or destination. Pan¯ . ini’s work was the earliest we know of that modeled the linguistic realization of events and their participants. This task of understanding how participants relate to events—being able to answer the question “Who did what to whom” (and perhaps also “when and where”)—is a central question of natural language processing.

![## Image Analysis: a2912ad6eca3c47c31a60c9876f80c2a50fecc1518b7a1fc8b5bdb04198ea03f.jpg

**Conceptual Understanding:**
The image conceptually represents a historical document, specifically a page from an ancient manuscript. Its main purpose is to showcase an example of ancient textual records, thereby conveying the tangible reality of historical linguistic artifacts and the physical medium through which knowledge was transmitted in pre-modern societies. The key ideas communicated include the enduring nature of written traditions, the historical depth of human language, the challenges and methods of knowledge preservation over centuries, and the inherent value of cultural heritage contained within such artifacts. It speaks to the continuous human effort to record, share, and understand information across generations.

**Content Interpretation:**
The image displays a page from an ancient manuscript, representing a system of knowledge recording and transmission through handwritten script in an ancient civilization. It highlights the physical characteristics of such historical documents, including the aged medium (likely palm leaf or a similar ancient material), the dense, uniform style of the script (visually identified as possibly an Indic script), and the overall state of preservation. The sheer volume and density of the text suggest a continuous narrative, exposition, or collection of teachings, which is characteristic of philosophical, religious, or academic works from ancient periods. The uniformity of the script further indicates a skilled scribe or an established scribal tradition. The significance of this 'data' (the script itself) lies in its immense historical and cultural value, serving as a direct artifact of past intellectual and literary activity. The visual evidence of 'dense, uniform script,' 'handwritten characters,' and 'faded, aged material' unequivocally supports the interpretation of this being an ancient manuscript. The absence of modern formatting elements such as distinct headings, paragraphs, or bullet points reinforces its antiquity, underscoring how knowledge was structured and conveyed in earlier times.

**Key Insights:**
The primary takeaways from this image include the profound tradition of written scholarship and knowledge preservation that existed in ancient cultures. It visually demonstrates the physical form that texts took before the advent of modern printing, emphasizing the labor-intensive process of creating and disseminating written works. The dense packing of text on the page implicitly suggests the value placed on the writing material and the effort to maximize information per page. Furthermore, the existence of such a document underscores the critical need for specialized expertise—such as paleography and historical linguistics—to decipher, translate, and preserve these invaluable historical records. The image supports the conclusion that ancient civilizations developed sophisticated systems for documenting and transmitting complex information. The visual presence of an extensive, ancient handwritten script on an aged medium serves as compelling evidence for these insights, highlighting the dedication to knowledge and the sophisticated scribal practices of the past.

**Document Context:**
The image is presented within a document section titled '21 Semantic Role Labeling.' While the image itself does not depict modern computational Semantic Role Labeling (SRL) techniques, it serves as a foundational historical context for the study of language and meaning. SRL aims to understand the 'who, what, where, when, why, and how' of events described in text. This ancient manuscript, likely containing narratives, philosophical discussions, or religious doctrines, would have been the subject of implicit semantic analysis by its contemporary readers, who sought to understand the roles played by different entities within the linguistic structures. Thus, the image represents the raw linguistic data—the fundamental building blocks of human communication—from which semantic understanding is derived. It connects the highly technical and modern field of computational linguistics back to its ancient roots, reminding us that the human endeavor to extract meaning from language is an age-old pursuit, now being modeled and automated with advanced techniques. This historical artifact contextualizes the enduring importance of language analysis.

**Summary:**
The image displays a complete page from an ancient manuscript, likely originating from India. The page material is aged and appears to be a light brown, possibly treated palm leaf or parchment, showing a slightly uneven texture and some discoloration, particularly along the right edge and bottom. The entire surface of the page is densely covered with handwritten text in a uniform, small, black script. The script strongly resembles an ancient Indic language, characterized by many characters connected at the top, forming continuous lines across the page. There are approximately 35 to 40 lines of text visible from top to bottom. The characters are tightly packed, occupying almost the entire width of the page, with only very thin, irregular margins on all four sides. There are no discernible paragraph breaks, headings, or modern punctuation, suggesting a continuous flow of content typical of ancient scholarly or religious texts. No illustrations, diagrams, or other visual elements are present on the page other than the script itself. The overall impression is that of a valuable historical artifact, a testament to ancient literacy and knowledge preservation. Due to the specialized nature of the ancient, handwritten script and the limitations of general vision analysis, a verbatim transcription of the individual characters and words is not feasible. However, its presence signifies a deep historical tradition of written communication and scholarship.](images/a2912ad6eca3c47c31a60c9876f80c2a50fecc1518b7a1fc8b5bdb04198ea03f.jpg)

Let’s move forward 2.5 millennia to the present and consider the very mundane goal of understanding text about a purchase of stock by XYZ Corporation. This purchasing event and its participants can be described by a wide variety of surface forms. The event can be described by a verb (sold, bought) or a noun (purchase), and XYZ Corp can be the syntactic subject (of bought), the indirect object (of sold), or in a genitive or noun compound relation (with the noun purchase) despite having notionally the same role in all of them:

• XYZ corporation bought the stock.   
• They sold the stock to XYZ corporation.   
• The stock was bought by XYZ corporation.   
• The purchase of the stock by XYZ corporation...   
• The stock purchase by XYZ corporation...

In this chapter we introduce a level of representation that captures the commonality between these sentences: there was a purchase event, the participants were XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic representations , semantic roles, express the role that arguments of a predicate take in the event, codified in databases like PropBank and FrameNet. We’ll introduce semantic role labeling, the task of assigning roles to spans in sentences, and selectional restrictions, the preferences that predicates express about their arguments, such as the fact that the theme of eat is generally something edible.

# 21.1 Semantic Roles

thematic roles agents

theme

Consider the meanings of the arguments Sasha, Pat, the window, and the door in these two sentences.

(21.1) Sasha broke the window.   
(21.2) Pat opened the door.

The subjects Sasha and Pat, what we might call the breaker of the windowbreaking event and the opener of the door-opening event, have something in common. They are both volitional actors, often animate, and they have direct causal responsibility for their events.

Thematic roles are a way to capture this semantic commonality between breakers and openers. We say that the subjects of both these verbs are agents. Thus, AGENT is the thematic role that represents an abstract idea such as volitional causation. Similarly, the direct objects of both these verbs, the BrokenThing and OpenedThing, are both prototypically inanimate objects that are affected in some way by the action. The semantic role for these participants is theme.

<table><tr><td>Thematic Role</td><td>Definition</td></tr><tr><td>AGENT</td><td>The volitional causer of an event</td></tr><tr><td>EXPERIENCER</td><td> The experiencer of an event</td></tr><tr><td>FORCE</td><td> The non-volitional causer of the event</td></tr><tr><td>THEME</td><td> The participant most directly affected by an event</td></tr><tr><td>RESULT</td><td> The end product of an event</td></tr><tr><td>CONTENT</td><td> The proposition or content of a propositional event</td></tr><tr><td>INSTRUMENT</td><td>An instrument used in an event</td></tr><tr><td>BENEFICIARY</td><td> The beneficiary of an event</td></tr><tr><td>SOURCE</td><td>The origin of the object of a transfer event</td></tr><tr><td>GOAL</td><td> The destination of an object of a transfer event</td></tr></table>

Figure 21.1 Some commonly used thematic roles with their definitions.

Although thematic roles are one of the oldest linguistic models, as we saw above, their modern formulation is due to Fillmore (1968) and Gruber (1965). Although there is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some thematic roles that have been used in various computational papers, together with rough definitions and examples. Most thematic role sets have about a dozen roles, but we’ll see sets with smaller numbers of roles with even more abstract meanings, and sets with very large numbers of roles that are specific to situations. We’ll use the general term semantic roles for all sets of roles, whether small or large.

# 21.2 Diathesis Alternations

The main reason computational systems use semantic roles is to act as a shallow meaning representation that can let us make simple inferences that aren’t possible from the pure surface string of words, or even from the parse tree. To extend the earlier examples, if a document says that Company A acquired Company $B$ , we’d like to know that this answers the query Was Company B acquired? despite the fact that the two sentences have very different surface syntax. Similarly, this shallow semantics might act as a useful intermediate language in machine translation.

<table><tr><td>Thematic Role</td><td>Example</td></tr><tr><td>AGENT</td><td>The waiter spilled the soup.</td></tr><tr><td>EXPERIENCER</td><td>John has a headache.</td></tr><tr><td>FORCE</td><td> The wind blows debris from the mall into our yards.</td></tr><tr><td>THEME</td><td>Only after Benjamin Franklin broke the ice...</td></tr><tr><td>RESULT</td><td>The city built a regulation-size baseball diamond...</td></tr><tr><td>CONTENT</td><td> Mona asked “You met Mary Ann at a supermarket?”</td></tr><tr><td>INSTRUMENT</td><td>He poached catfish, stunning them with a shocking device...</td></tr><tr><td>BENEFICIARY</td><td>Whenever Ann Callahan makes hotel reservations for her boss..</td></tr><tr><td>SOURCE</td><td>I flew in from Boston.</td></tr><tr><td>GOAL</td><td>I drove to Portland.</td></tr></table>

Figure 21.2 Some prototypical examples of various thematic roles.

Semantic roles thus help generalize over different surface realizations of predicate arguments. For example, while the AGENT is often realized as the subject of the sentence, in other cases the THEME can be the subject. Consider these possible realizations of the thematic arguments of the verb break:

(21.3) John broke the window. AGENT THEME   
(21.4) John broke the window with a rock. AGENT THEME INSTRUMENT   
(21.5) The rock broke the window. INSTRUMENT THEME   
(21.6) The window broke. THEME   
(21.7) The window was broken by John. THEME AGENT

These examples suggest that break has (at least) the possible arguments AGENT, THEME, and INSTRUMENT. The set of thematic role arguments taken by a verb is often called the thematic grid, $\theta$ -grid, or case frame. We can see that there are (among others) the following possibilities for the realization of these arguments of break:

AGENT/Subject, THEME/Object   
AGENT/Subject, THEME/Object, INSTRUMENT/PPwith   
INSTRUMENT/Subject, THEME/Object   
THEME/Subject

It turns out that many verbs allow their thematic roles to be realized in various syntactic positions. For example, verbs like give can realize the THEME and GOAL arguments in two different ways:

a. Doris gave the book to Cary. AGENT THEME GOAL   
b. Doris gave Cary the book. AGENT GOAL THEME verb   
alternation dative   
alternation

These multiple argument structure realizations (the fact that break can take AGENT, INSTRUMENT, or THEME as subject, and give can realize its THEME and GOAL in either order) are called verb alternations or diathesis alternations. The alternation we showed above for give, the dative alternation, seems to occur with particular semantic classes of verbs, including “verbs of future having” (advance, allocate, offer, owe), “send verbs” (forward, hand, mail), “verbs of throwing” (kick, pass, throw), and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which they belong (47 high-level classes, divided into 193 more specific classes) and the various alternations in which they participate. These lists of verb classes have been incorporated into the online resource VerbNet (Kipper et al., 2000), which links each verb to both WordNet and FrameNet entries.

# 21.3 Semantic Roles: Problems with Thematic Roles

semantic role

proto-agent proto-patient

Representing meaning at the thematic role level seems like it should be useful in dealing with complications like diathesis alternations. Yet it has proved quite difficult to come up with a standard set of roles, and equally difficult to produce a formal definition of roles like AGENT, THEME, or INSTRUMENT.

For example, researchers attempting to define role sets often find they need to fragment a role like AGENT or THEME into many specific roles. Levin and Rappaport Hovav (2005) summarize a number of such cases, such as the fact there seem to be at least two kinds of INSTRUMENTS, intermediary instruments that can appear as subjects and enabling instruments that cannot:

a. Shelly cut the banana with a knife.   
b. The knife cut the banana.   
a. Shelly ate the sliced banana with a fork.   
b. \*The fork ate the sliced banana.

In addition to the fragmentation problem, there are cases in which we’d like to reason about and generalize across semantic roles, but the finite discrete lists of roles don’t let us do this.

Finally, it has proved difficult to formally define the thematic roles. Consider the AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any individual noun phrase might not exhibit all of these properties.

These problems have led to alternative semantic role models that use either many fewer or many more roles.

The first of these options is to define generalized semantic roles that abstract over the specific thematic roles. For example, PROTO-AGENT and PROTO-PATIENT are generalized roles that express roughly agent-like and roughly patient-like meanings. These roles are defined, not by necessary and sufficient conditions, but rather by a set of heuristic features that accompany more agent-like or more patient-like meanings. Thus, the more an argument displays agent-like properties (being volitionally involved in the event, causing an event or a change of state in another participant, being sentient or intentionally involved, moving) the greater the likelihood that the argument can be labeled a PROTO-AGENT. The more patient-like the properties (undergoing change of state, causally affected by another participant, stationary relative to other participants, etc.), the greater the likelihood that the argument can be labeled a PROTO-PATIENT.

The second direction is instead to define semantic roles that are specific to a particular verb or a particular group of semantically related verbs or nouns.

In the next two sections we describe two commonly used lexical resources that make use of these alternative versions of semantic roles. PropBank uses both protoroles and verb-specific semantic roles. FrameNet uses semantic roles that are specific to a general semantic idea called a frame.

# 21.4 The Proposition Bank

# PropBank

The Proposition Bank, generally referred to as PropBank, is a resource of sentences annotated with semantic roles. The English PropBank labels all the sentences in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese TreeBank. Because of the difficulty of defining a universal set of thematic roles, the semantic roles in PropBank are defined with respect to an individual verb sense. Each sense of each verb thus has a specific set of roles, which are given only numbers rather than names: Arg0, Arg1, Arg2, and so on. In general, $\mathbf { A r 8 0 }$ represents the PROTO-AGENT, and Arg1, the PROTO-PATIENT. The semantics of the other roles are less consistent, often being defined specifically for each verb. Nonetheless there are some generalization; the Arg2 is often the benefactive, instrument, attribute, or end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4 the end point.

Here are some slightly simplified PropBank entries for one sense each of the verbs agree and fall. Such PropBank entries are called frame files; note that the definitions in the frame file for each role (“Other entity agreeing”, “Extent, amount fallen”) are informal glosses intended to be read by humans, rather than being formal definitions.

(21.11) agree.01 Arg0: Agreer Arg1: Proposition Arg2: Other entity agreeing

Ex1: $\mathrm { [ _ { A r g 0 } }$ The group] agreed $\mathrm { [ _ { A r g 1 } }$ it wouldn’t make an offer]. $\mathrm { E x } 2$ : [ArgM-TMP Usually] $\mathrm { [ _ { A r g 0 } }$ John] agrees $\operatorname { I } _ { \mathbf { A r g } 2 }$ with Mary] [Arg1 on everything].

(21.12) fall.01

Arg1: Logical subject, patient, thing falling   
Arg2: Extent, amount fallen   
Arg3: start point   
Arg4: end point, end state of arg1   
Ex1: $\mathrm { [ _ { A r g 1 } }$ Sales] fell $\mathrm { [ _ { A r g 4 } }$ to $\$ 25$ million] $\mathrm { [ _ { A r g 3 } }$ from $\$ 27$ million]. $\mathrm { E x } 2$ : $\mathrm { \Delta I _ { A r g 1 } }$ The average junk bond] fell $\operatorname { I } _ { \mathbf { A r g } 2 }$ by $4 . 2 \%$ ].

Note that there is no $\mathrm { \ A r g { 0 } }$ role for fall, because the normal subject of fall is a PROTO-PATIENT.

The PropBank semantic roles can be useful in recovering shallow semantic information about verbal arguments. Consider the verb increase:

(21.13) increase.01 “go up incrementally”

Arg0: causer of increase   
Arg1: thing increasing   
Arg2: amount increased by, EXT, or MNR   
Arg3: start point   
Arg4: end point

A PropBank semantic role labeling would allow us to infer the commonality in the event structures of the following three examples, that is, that in each case Big Fruit Co. is the AGENT and the price of bananas is the THEME, despite the differing surface forms.

(21.14) [Arg0 Big Fruit Co. ] increased $\mathrm { [ _ { A r g 1 } }$ the price of bananas]. (21.15) $\mathrm { [ _ { A r g 1 } }$ The price of bananas] was increased again $\mathrm { [ _ { A r g 0 } }$ by Big Fruit Co. ] (21.16) $\mathrm { [ _ { A r g 1 } }$ The price of bananas] increased $\left[ \mathrm { A r g } 2 \ ^ { 5 \% } \right]$ .

PropBank also has a number of non-numbered arguments called ArgMs, (ArgMTMP, ArgM-LOC, etc.) which represent modification or adjunct meanings. These are relatively stable across predicates, so aren’t listed with each frame file. Data labeled with these modifiers can be helpful in training systems to detect temporal, location, or directional modification across predicates. Some of the ArgM’s include:

TMP when? yesterday evening, now LOC where? at the museum, in San Francisco DIR where to/from? down, to Bangkok MNR how? clearly, with much enthusiasm PRP/CAU why? because ... , in response to the ruling REC themselves, each other ADV miscellaneous PRD secondary predication ...ate the meat raw

# NomBank

While PropBank focuses on verbs, a related project, NomBank (Meyers et al., 2004) adds annotations to noun predicates. For example the noun agreement in Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as the Arg2. This allows semantic role labelers to assign labels to arguments of both verbal and nominal predicates.

# 21.5 FrameNet

While making inferences about the semantic commonalities across different sentences with increase is useful, it would be even more useful if we could make such inferences in many more situations, across different verbs, and also between verbs and nouns. For example, we’d like to extract the similarity among these three sentences:

(21.17) $\mathrm { [ _ { A r g 1 } }$ The price of bananas] increased $\mathrm { [ } _ { \mathrm { A r g } 2 } 5 \% ]$ .   
(21.18) $\mathrm { [ _ { A r g 1 } }$ The price of bananas] rose $\left[ _ { \mathrm { A r g } 2 } 5 \% \right]$ .   
(21.19) There has been a $\mathrm { [ } _ { \mathrm { A r g } 2 } 5 \% ]$ rise $\mathrm { [ _ { A r g 1 } }$ in the price of bananas].

Note that the second example uses the different verb rise, and the third example uses the noun rather than the verb rise. We’d like a system to recognize that the price of bananas is what went up, and that $5 \%$ is the amount it went up, no matter whether the $5 \%$ appears as the object of the verb increased or as a nominal modifier of the noun rise.

The FrameNet project is another semantic-role-labeling project that attempts to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003, Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank project are specific to an individual verb, roles in the FrameNet project are specific to a frame.

What is a frame? Consider the following set of words:

reservation, flight, travel, buy, price, cost, fare, rates, meal, plane

There are many individual lexical relations of hyponymy, synonymy, and so on between many of the words in this list. The resulting set of relations does not, however, add up to a complete account of how these words are related. They are clearly all defined with respect to a coherent chunk of common-sense background information concerning air travel.

model script

frame elements

We call the holistic background knowledge that unites these words a frame (Fillmore, 1985). The idea that groups of words are defined with respect to some background information is widespread in artificial intelligence and cognitive science, where besides frame we see related works like a model (Johnson-Laird, 1983), or even script (Schank and Abelson, 1977).

A frame in FrameNet is a background knowledge structure that defines a set of frame-specific semantic roles, called frame elements, and includes a set of predicates that use these roles. Each word evokes a frame and profiles some aspect of the frame and its elements. The FrameNet dataset includes a set of frames and frame elements, the lexical units associated with each frame, and a set of labeled example sentences. For example, the change position on a scale frame is defined as follows:

This frame consists of words that indicate the change of an Item’s position on a scale (the Attribute) from a starting point (Initial value) to an end point (Final value).

core roles non-core roles   
Some of the semantic roles (frame elements) in the frame are defined as in Fig. 21.3. Note that these are separated into core roles, which are frame specific, and non-core roles, which are more like the Arg-M arguments in PropBank, expressing more general properties of time, location, and so on.   

<table><tr><td colspan="2">Core Roles</td></tr><tr><td>ATTRIBUTE</td><td> The ATTRIBUTE is a scalar property that the ITEM possesses.</td></tr><tr><td>DIFFERENCE FINAL_STATE</td><td>The distance by which an ITEM changes its position on the scale. A description that presents the ITEM&#x27;s state after the change in the ATTRIBUTE&#x27;s</td></tr><tr><td>FINAL_VALUE</td><td>value as an independent predication. The position on the scale where the ITEM ends up. A description that presents the ITEM&#x27;s state before the change in the AT-</td></tr><tr><td>INITIAL_STATE ITEM</td><td>TRIBUTE&#x27;s value as an independent predication. INITIAL-VALUE The initial position on the scale from which the ITEM moves away. The entity that has a position on the scale. VALUE_RANGE A portion of the scale, typically identified by its end points, along which the values of the ATTRIBUTE fluctuate.</td></tr><tr><td colspan="2">Some Non-Core Roles DURATION The length of time over which the change takes place. SPEED The rate of change of the VALUE. GROUP The GROUP in which an ITEM changes the value of an ATTRIBUTE in a specified way.</td></tr></table>

Figure 21.3 The frame elements in the change position on a scale frame from the FrameNet Labelers Guide (Ruppenhofer et al., 2016).

Here are some example sentences:

(21.20) [ITEM Oil] rose [ATTRIBUTE in price] [DIFFERENCE by $2 \%$ ].   
(21.21) [ITEM It] has increased [FINAL STATE to having them 1 day a month].   
(21.22) [ITEM Microsoft shares] fell [FINAL VALUE to 7 5/8].   
(21.23) [ITEM Colon cancer incidence] fell [DIFFERENCE by $5 0 \%$ ] [GROUP among men].   
(21.24) a steady increase [INITIAL VALUE from 9.5] [FINAL VALUE to 14.3] [ITEM in dividends]   
(21.25) a [DIFFERENCE $5 \%$ ] [ITEM dividend] increase...

Note from these example sentences that the frame includes target words like rise, fall, and increase. In fact, the complete frame consists of the following words:

<table><tr><td>VERBS: dwindle advance climb decline decrease</td><td>edge explode fall fluctuate</td><td>move mushroom plummet reach rise</td><td>soar swell swing triple tumble</td><td>escalation explosion fall fluctuation gain growth</td><td>shift tumble ADVERBS: increasingly</td></tr><tr><td>diminish dip double drop</td><td>gain grow increase jump</td><td>rocket shift skyrocket slide</td><td>NOUNS: decline decrease</td><td>hike increase rise</td><td></td></tr></table>

FrameNet also codes relationships between frames, allowing frames to inherit from each other, or representing relations between frames like causation (and generalizations among frame elements in different frames can be represented by inheritance as well). Thus, there is a Cause change of position on a scale frame that is linked to the Change of position on a scale frame by the cause relation, but that adds an AGENT role and is used for causative examples such as the following:

(21.26) [AGENT They] raised $\operatorname { \ I { \mathrm { I T E M } } }$ the price of their soda] [DIFFERENCE by $2 \%$

Together, these two frames would allow an understanding system to extract the common event semantics of all the verbal and nominal causative and non-causative usages.

FrameNets have also been developed for many other languages including Spanish, German, Japanese, Portuguese, Italian, and Chinese.

# 21.6 Semantic Role Labeling

# semantic role labeling

Semantic role labeling (sometimes shortened as SRL) is the task of automatically finding the semantic roles of each argument of each predicate in a sentence. Current approaches to semantic role labeling are based on supervised machine learning, often using the FrameNet and PropBank resources to specify what counts as a predicate, define the set of roles used in the task, and provide training and test sets.

Recall that the difference between these two models of semantic roles is that FrameNet (21.27) employs many frame-specific frame elements as roles, while PropBank (21.28) uses a smaller number of numbered argument labels that can be interpreted as verb-specific labels, along with the more general ARGM labels. Some examples:

(21.27) [You]COGNIZER can’t [blame] [the program] [for being unable to identify it] (21.28) [The San Francisco Examiner] issued [a special edition] [yesterday]0 1 -

# 21.6.1 A Feature-based Algorithm for Semantic Role Labeling

A simplified feature-based semantic role labeling algorithm is sketched in Fig. 21.4. Feature-based algorithms—from the very earliest systems like (Simmons, 1973)— begin by parsing, using broad-coverage parsers to assign a parse to the input string.

Figure 21.5 shows a parse of (21.28) above. The parse is then traversed to find all words that are predicates.

For each of these predicates, the algorithm examines each node in the parse tree and uses supervised classification to decide the semantic role (if any) it plays for this predicate. Given a labeled training set such as PropBank or FrameNet, a feature vector is extracted for each node, using feature templates described in the next subsection. A 1-of-N classifier is then trained to predict a semantic role for each constituent given these features, where N is the number of potential semantic roles plus an extra NONE role for non-role constituents. Any standard classification algorithms can be used. Finally, for each test sentence to be labeled, the classifier is run on each relevant constituent.

<table><tr><td>function SEMANTICROLELABEL(words) returns labeled tree</td></tr><tr><td>parse←PARSE(words)</td></tr><tr><td>for each predicate in parse do</td></tr><tr><td>for each node in parse do</td></tr><tr><td>featurevector←EXTRACTFEATUREs(node,predicate,parse)</td></tr><tr><td>CLASSIFYNoDE(node,featurevector, parse)</td></tr><tr><td></td></tr></table>

![## Image Analysis: e4c3c2c61ff29383a191ed3e41aded9d5a1f094c6efa92a283ec3967a5dee2e3.jpg

**Conceptual Understanding:**
The image represents a **parse tree (also known as a syntax tree or constituent tree)**, a hierarchical structure that shows the syntactic relationships between words and phrases in a sentence. Conceptually, it illustrates how a sentence can be broken down into its constituent parts (e.g., noun phrases, verb phrases, prepositions) and how those parts are organized according to grammatical rules. The main purpose of this specific parse tree is to demonstrate **Semantic Role Labeling (SRL)**, which involves identifying the predicate (usually a verb) in a sentence and then determining the semantic roles (e.g., agent, patient, time, location) played by its arguments and adjuncts. It visually maps the syntactic structure of the sentence "The San Francisco Examiner issued a special edition around noon yesterday" to its corresponding semantic roles, using a system similar to PropBank. Key ideas being communicated include: 1. The **syntactic decomposition** of a sentence. 2. The **assignment of semantic roles** to syntactic constituents. 3. The identification of a **'TARGET' predicate** (the verb 'issued'). 4. The use of **path features** within the parse tree to link arguments to their predicates for computational analysis, as indicated by the dashed line.

**Content Interpretation:**
The image displays a parse tree, which is a hierarchical representation of the syntactic structure of a sentence. It shows how words in a sentence are grouped into constituents (phrases) and how these constituents relate to each other. The core components shown are:### 1. **Syntactic Constituent Labels (Phrase Types and Part-of-Speech Tags):**
*   **S:** Sentence (Root node).
*   **NP-SBJ:** Noun Phrase - Subject.
*   **VP:** Verb Phrase.
*   **DT:** Determiner.
*   **NNP:** Proper Noun, Singular.
*   **VBD:** Verb, Past Tense.
*   **NP:** Noun Phrase.
*   **JJ:** Adjective.
*   **NN:** Noun, Singular.
*   **PP-TMP:** Prepositional Phrase - Temporal.
*   **IN:** Preposition/Subordinating Conjunction.
*   **NP-TMP:** Noun Phrase - Temporal.

### 2. **Lexical Items (Words):**
*   The
*   San
*   Francisco
*   Examiner
*   issued
*   a
*   special
*   edition
*   around
*   noon
*   yesterday

### 3. **Semantic Role Labels:**
These labels are assigned to specific syntactic constituents, indicating their semantic function relative to the 'TARGET' verb.
*   **ARG0:** Agent or subject (assigned to 'NP-SBJ').
*   **TARGET:** The predicate or main verb (assigned to 'VBD' 'issued').
*   **ARG1:** Patient or theme (assigned to 'NP').
*   **ARGM-TMP:** Adjunct - Temporal (assigned to 'PP-TMP').

### 4. **Path Feature Indication:**
*   A dashed line visually traces a path through the tree. This path starts at 'VBD = TARGET' ('issued'), goes upwards to 'VP', then upwards to 'S', and finally downwards to 'NP-SBJ = ARG0' ('The San Francisco Examiner'). This dashed line, as stated in the document context, represents a 'path feature NP↑S↓VP↓VBD for ARG0', which is crucial for feature-based semantic role labeling algorithms. It shows the syntactic relationship between the predicate and its argument.

### Supporting Evidence from Extracted Text:
*   The presence of labels like 'NP-SBJ = ARG0', 'VBD = TARGET', 'NP = ARG1', and 'PP-TMP = ARGM-TMP' directly indicates the assignment of semantic roles to syntactic constituents. This clearly demonstrates the process of Semantic Role Labeling.
*   The hierarchical structure with nodes like 'S', 'VP', 'NP', 'DT', 'NNP', 'JJ', 'NN', 'IN' showcases the syntactic parsing of the sentence into its grammatical components.
*   The specific words at the leaf nodes ("The", "San", "Francisco", "Examiner", "issued", "a", "special", "edition", "around", "noon", "yesterday") confirm the sentence being analyzed.
*   The dashed line's traversal from 'VBD = TARGET' through 'VP' and 'S' to 'NP-SBJ = ARG0' explicitly visualizes the path feature extraction described in the surrounding text, demonstrating how contextual syntactic information is gathered to identify arguments.

**Key Insights:**
The main takeaways from this image are:### 1. **Parse Trees as a Foundation for Semantic Analysis:** The image demonstrates that syntactic parse trees are a foundational representation for deeper semantic analysis in Natural Language Processing. The hierarchical structure of phrases and words provides the necessary framework for identifying argument-predicate relationships.

### 2. **Semantic Role Labeling (SRL) in Action:** The tree clearly illustrates the concept of Semantic Role Labeling by assigning specific semantic roles (ARG0, ARG1, ARGM-TMP) to different syntactic constituents relative to a 'TARGET' verb ('issued'). This shows how sentences can be understood beyond their surface grammatical structure to uncover 'who did what to whom, when, where, why'.

### 3. **PropBank-style Annotation:** The labels 'ARG0', 'ARG1', 'ARGM-TMP', and 'TARGET' are characteristic of the PropBank annotation scheme, indicating that the image is a practical example of how PropBank frames are applied to sentences to mark arguments and adjuncts of a predicate.

### 4. **Feature Extraction for SRL Algorithms:** The dashed line, representing a 'path feature' (NP↑S↓VP↓VBD), provides a key insight into how machine learning algorithms for SRL might work. It shows that specific structural relationships (paths) between a target verb and its arguments within the parse tree are extracted as features to help classify the semantic role of a constituent.

### 5. **Interplay of Syntax and Semantics:** The image highlights the close relationship between syntax and semantics. The syntactic structure (NP-SBJ, VP, PP-TMP) dictates the potential locations for semantic arguments, which are then explicitly labeled with their roles.

### Textual Evidence for Insights:
*   **"NP-SBJ = ARG0"**, **"VBD = TARGET"**, **"NP = ARG1"**, **"PP-TMP = ARGM-TMP"**: These explicit labels directly demonstrate the assignment of semantic roles based on syntactic categories, confirming the process of SRL and the use of a PropBank-like scheme.
*   The entire hierarchical arrangement of **"S"**, **"NP-SBJ"**, **"VP"**, **"DT"**, **"NNP"**, etc., leading down to the actual words, serves as evidence for parse trees being a fundamental input.
*   The **dashed line** visually connecting **"VBD = TARGET"** to **"NP-SBJ = ARG0"** via **"VP"** and **"S"** strongly supports the concept of extracting path features for SRL algorithms, illustrating how contextual information is derived from the tree structure.

**Document Context:**
This image is highly relevant to the section "21.6.1 A Feature-based Algorithm for Semantic Role Labeling" as it provides a concrete example of the input and output of such an algorithm. The parse tree explicitly shows the syntactic structure of a sentence and how semantic roles (like ARG0, ARG1, ARGM-TMP, and TARGET) are assigned to constituents within that structure, which is the core task of Semantic Role Labeling (SRL). The dashed line highlights a "path feature" (NP↑S↓VP↓VBD) as described in the text accompanying the image, which is a critical piece of information for feature-based SRL algorithms. The image serves as a visual aid to understand the "labeled data such as FrameNet or PropBank" that is used to train such classifiers, as mentioned in Figure 21.4's description, and specifically shows PropBank argument labels as indicated in Figure 21.5's description. It helps readers visualize the process of identifying and categorizing sentence parts based on their semantic contributions, which is fundamental to understanding the computational approach to SRL being discussed in the document.

**Summary:**
The image is a parse tree illustrating the syntactic structure and semantic role labeling for the sentence "The San Francisco Examiner issued a special edition around noon yesterday". It is organized hierarchically, starting from the root 'S' (Sentence) node. The tree branches down to assign grammatical categories and then individual words, along with specific semantic roles identified using a scheme like PropBank. The main purpose is to visually represent how a sentence is broken down into its constituent parts and how these parts are assigned roles relative to a central verb or 'TARGET'. The structure moves from abstract syntactic categories to concrete lexical items. The dashed line specifically highlights a path feature that connects the TARGET verb to one of its arguments, providing a visual cue for how features are extracted for semantic role labeling. All textual elements are meticulously transcribed to show the complete linguistic analysis presented in the diagram.](images/e4c3c2c61ff29383a191ed3e41aded9d5a1f094c6efa92a283ec3967a5dee2e3.jpg)
Figure 21.4 A generic semantic-role-labeling algorithm. CLASSIFYNODE is a 1-of- $. N$ classifier that assigns a semantic role (or NONE for non-role constituents), trained on labeled data such as FrameNet or PropBank.   
Figure 21.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line shows the path feature $\mathrm { N P \uparrow S \downarrow V P \downarrow V B D }$ for ARG0, the NP-SBJ constituent The San Francisco Examiner.

Instead of training a single-stage classifier as in Fig. 21.5, the node-level classification task can be broken down into multiple steps:

1. Pruning: Since only a small number of the constituents in a sentence are arguments of any given predicate, many systems use simple heuristics to prune unlikely constituents.   
2. Identification: a binary classification of each node as an argument to be labeled or a NONE.   
3. Classification: a 1-of- ${ \mathbf { } } \cdot N$ classification of all the constituents that were labeled as arguments by the previous stage

The separation of identification and classification may lead to better use of features (different features may be useful for the two tasks) or to computational efficiency.

# Global Optimization

The classification algorithm of Fig. 21.5 classifies each argument separately (‘locally’), making the simplifying assumption that each argument of a predicate can be labeled independently. This assumption is false; there are interactions between arguments that require a more ‘global’ assignment of labels to constituents. For example, constituents in FrameNet and PropBank are required to be non-overlapping. More significantly, the semantic roles of constituents are not independent. For example PropBank does not allow multiple identical arguments; two constituents of the same verb cannot both be labeled ARG0 .

Role labeling systems thus often add a fourth step to deal with global consistency across the labels in a sentence. For example, the local classifiers can return a list of possible labels associated with probabilities for each constituent, and a second-pass Viterbi decoding or re-ranking approach can be used to choose the best consensus label. Integer linear programming (ILP) is another common way to choose a solution that conforms best to multiple constraints.

# Features for Semantic Role Labeling

Most systems use some generalization of the core set of features introduced by Gildea and Jurafsky (2000). Common basic features templates (demonstrated on the NP-SBJ constituent The San Francisco Examiner in Fig. 21.5) include:

• The governing predicate, in this case the verb issued. The predicate is a crucial feature since labels are defined only with respect to a particular predicate.   
• The phrase type of the constituent, in this case, $N P$ (or NP-SBJ). Some semantic roles tend to appear as NPs, others as $s$ or $P P$ , and so on.   
• The headword of the constituent, Examiner. The headword of a constituent can be computed with standard head rules, such as those given in Appendix D in Fig. 18.17. Certain headwords (e.g., pronouns) place strong constraints on the possible semantic roles they are likely to fill.   
• The headword part of speech of the constituent, NNP.   
• The path in the parse tree from the constituent to the predicate. This path is marked by the dotted line in Fig. 21.5. Following Gildea and Jurafsky (2000), we can use a simple linear representation of the path, $\mathrm { N P \uparrow S \downarrow V P \downarrow V B D }$ . $\uparrow$ and $\downarrow$ represent upward and downward movement in the tree, respectively. The path is very useful as a compact representation of many kinds of grammatical function relationships between the constituent and the predicate.   
• The voice of the clause in which the constituent appears, in this case, active (as contrasted with passive). Passive sentences tend to have strongly different linkings of semantic roles to surface form than do active ones.   
• The binary linear position of the constituent with respect to the predicate, either before or after.   
• The subcategorization of the predicate, the set of expected arguments that appear in the verb phrase. We can extract this information by using the phrasestructure rule that expands the immediate parent of the predicate; $\mathrm { V P }  \mathrm { V B D }$ NP PP for the predicate in Fig. 21.5.   
• The named entity type of the constituent.

• The first words and the last word of the constituent.

The following feature vector thus represents the first NP in our example (recall that most observations will have the value NONE rather than, for example, ARG0, since most constituents in the parse tree will not bear a semantic role):

ARG0: [issued, NP, Examiner, NNP, NP↑S↓VP↓VBD, active, before, $\mathrm { V P }  \mathrm { N P }$ PP, ORG, The, Examiner]

Other features are often used in addition, such as sets of n-grams inside the constituent, or more complex versions of the path features (the upward or downward halves, or whether particular nodes occur in the path).

It’s also possible to use dependency parses instead of constituency parses as the basis of features, for example using dependency parse paths instead of constituency paths.

# 21.6.2 A Neural Algorithm for Semantic Role Labeling

A simple neural approach to SRL is to treat it as a sequence labeling task like namedentity recognition, using the BIO approach. Let’s assume that we are given the predicate and the task is just detecting and labeling spans. Recall that with BIO tagging, we have a begin and end tag for each possible role (B-ARG0, I-ARG0; BARG1, I-ARG1, and so on), plus an outside tag O.

![## Image Analysis: f62902caf25ac4a91842ea78dd422897b2665a61cf37e03b9af8f57deb11acb5.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified neural network architecture designed for **Semantic Role Labeling (SRL)**. Its main purpose is to illustrate how a neural model can take an input sentence and a target predicate, then process each word in the sentence to assign a semantic role (like agent, patient, etc.) relative to that predicate. The key idea being communicated is a step-by-step process of encoding, incorporating predicate-specific information, and then classifying each word's role using feed-forward networks and a Softmax layer.

**Content Interpretation:**
The image depicts a neural system for identifying semantic roles.Processes Being Shown:*   **Text Encoding:** The `ENCODER` block takes the raw input text and converts it into a rich, contextualized numerical representation. This is fundamental in modern NLP models.*   **Predicate-Aware Feature Generation:** The "concatenate with predicate" step is crucial. It explicitly integrates information about the target predicate (`love`) with the representation of each word in the sentence. This allows the model to determine a word's role *relative* to that specific predicate, as roles are predicate-specific. The multiple dark red/orange circles show the resulting features after this concatenation.*   **Feature Transformation:** The `FFN` (Feed-Forward Network) layers further process these concatenated features, extracting more abstract or task-specific representations. Each word's feature set passes through its own FFN, implying individual processing for role prediction.*   **Classification:** The `Softmax` layer at the top performs a multi-class classification. It takes the output of the FFN and converts it into a probability distribution over a set of predefined semantic roles. The bar chart icons visually indicate this probability distribution.Concepts and Relationships:*   **Semantic Role Labeling (SRL):** The overall system addresses SRL, which involves identifying the arguments of a predicate (verb, noun, etc.) and classifying them into semantic roles such as `Agent` (who performed the action, often `ARG0`), `Patient` (who received the action, often `ARG1`), `Predicate` (`PRED`).*   **BIO Tagging Scheme:** The output labels `B-ARG0`, `I-ARG0`, `B-PRED`, `B-ARG1` strongly indicate the use of a BIO (Beginning, Inside, Outside) tagging scheme. `B-` (Begin) marks the start of a semantic argument, `I-` (Inside) marks subsequent tokens within the same argument, and `PRED` specifically tags the predicate.*   **Neural Network Architecture:** The sequence of `ENCODER`, feature concatenation, `FFN`, and `Softmax` layers clearly shows a neural network approach, implying learning through training to map input sentences to semantic role predictions.Supporting Text Evidence:*   The input `[CLS] the cats love hats [SEP] love [SEP]` provides the exact format of the input sentence and the explicitly provided predicate.*   The `ENCODER` label identifies the initial processing component.*   The "concatenate with predicate" label precisely describes the feature engineering step.*   The `FFN` labels within the trapezoidal blocks indicate the use of feed-forward networks for non-linear transformations.*   The `Softmax` label indicates the final classification layer.*   The output labels `B-ARG0`, `I-ARG0`, `B-PRED`, `B-ARG1` directly specify the target semantic roles and the BIO tagging scheme used for SRL.

**Key Insights:**
The image provides several key takeaways regarding neural approaches to Semantic Role Labeling:1.  **Explicit Predicate Information is Crucial:** A core insight is that explicit predicate information is vital for SRL. The step "concatenate with predicate" and the repeated `love` token in the input sequence demonstrate that the model is designed to make predictions for each word *in the context of a specific predicate*. This is a direct architectural choice to improve SRL performance, as roles are defined relative to a predicate.2.  **Modular Neural Architecture:** The model uses a modular, layered neural architecture. It separates concerns: `ENCODER` for contextualized representations, "concatenate with predicate" for feature engineering, `FFN`s for local transformations, and `Softmax` for classification. This is evidenced by the distinct labels and visual separation of these components.3.  **BIO Tagging for SRL Output:** The use of `B-ARG0`, `I-ARG0`, `B-PRED`, `B-ARG1` as output labels indicates that SRL is framed as a sequence tagging problem using the BIO (Begin, Inside, Outside) scheme. This allows the model to identify not just the role but also the span of words that constitute an argument.4.  **Word-level Prediction:** The repeated application of the `FFN` and `Softmax` layers for each word (or at least for each relevant word, as implied by the four parallel processing paths) suggests that the model performs word-level prediction of semantic roles. Each word in the sentence gets its own set of features processed to determine its role.These takeaways are directly supported by the verbatim textual elements in the image. For instance, "concatenate with predicate" directly shows the predicate's role; `ENCODER`, `FFN`, `Softmax` describe the architecture; and `B-ARG0`, `I-ARG0`, `B-PRED`, `B-ARG1` define the output format and task.

**Document Context:**
This image, titled "Figure 21.6 A simple neural approach to semantic role labeling," fits directly into a document section discussing "21.6.2 A Neural Algorithm for Semantic Role Labeling." It serves as a visual explanation of a specific type of neural network architecture that can perform SRL. It concretely illustrates the concepts discussed in the surrounding text, showing how an input sentence and a target predicate are processed through various neural layers to ultimately assign semantic roles to words. It highlights key design choices, such as explicit predicate integration, that are common in effective SRL models. The "After He et al. (2017) and Shi and Lin (2019)" notation further grounds it in contemporary research, indicating it's an example derived from established methods.

**Summary:**
This diagram illustrates a straightforward neural network method for **Semantic Role Labeling (SRL)**, which is the process of identifying "who did what to whom, when, and where."Let's break down the process step-by-step:1.  **Input Preparation:** The system starts by taking an input sentence. In this example, the sentence is "[CLS] the cats love hats". To this, a special separator token `[SEP]` is added, followed by the specific **predicate** (the verb or other word whose arguments we want to identify) that the model should focus on, which is "love" (highlighted in red). Another `[SEP]` token marks the end of this combined input. This ensures the model explicitly knows which predicate to analyze.2.  **Encoding the Input:** The entire combined sequence (`[CLS]` through the second `[SEP]`) is fed into an `ENCODER` block. This `ENCODER` is a neural network component that reads the input tokens and transforms them into rich, contextual numerical representations (embeddings) for each word. These representations capture the meaning of each word considering its surroundings in the sentence.3.  **Integrating Predicate Information:** After the `ENCODER` processes the input, the next crucial step is labeled "concatenate with predicate". For each word in the original sentence (e.g., "the", "cats", "love", "hats"), its numerical representation from the `ENCODER` is combined (concatenated) with specific information related to the target predicate, "love". This creates a new, enhanced representation for each word that now explicitly includes context about the predicate it's being evaluated against. The diagram shows four sets of small, colored circles representing these concatenated features for four words.4.  **Feature Transformation (FFN):** Each of these predicate-aware representations (the sets of circles) is then passed through its own independent `FFN` (Feed-Forward Network). An `FFN` is a small neural network that applies non-linear transformations to its input, helping to extract more discriminative features that are useful for classifying semantic roles. There's a separate `FFN` for each word being analyzed.5.  **Role Classification (Softmax):** The output of each `FFN` is then fed into a `Softmax` layer. The `Softmax` layer is a common final step in classification tasks. It converts the `FFN`'s output into a probability distribution over a set of predefined semantic roles. Essentially, it tells us how likely each word is to be a particular role. The small bar charts visually represent these probability distributions.6.  **Semantic Role Output:** Finally, the `Softmax` layer outputs the predicted semantic role for each word. The possible roles shown are `B-ARG0`, `I-ARG0`, `B-PRED`, and `B-ARG1`. These are tags from the BIO (Begin, Inside, Outside) scheme:    *   `B-ARG0`: Beginning of Argument 0 (often the agent or experiencer).    *   `I-ARG0`: ARG0 is typically the AGENT (the one performing the action).    *   `I-ARG0`: Inside Argument 0 (a token within an `ARG0` span).    *   `B-PRED`: Beginning of the Predicate itself (the `love` in this case).    *   `B-ARG1`: Beginning of Argument 1 (often the patient or theme).This entire process allows the neural algorithm to systematically identify and label the semantic roles of words in a sentence, informed by both the sentence context and the specific predicate under consideration.](images/f62902caf25ac4a91842ea78dd422897b2665a61cf37e03b9af8f57deb11acb5.jpg)
Figure 21.6 A simple neural approach to semantic role labeling. The input sentence is followed by [SEP] and an extra input for the predicate, in this case love. The encoder outputs are concatenated to an indicator variable which is 1 for the predicate and 0 for all other words After He et al. (2017) and Shi and Lin (2019).

As with all the taggers, the goal is to compute the highest probability tag sequence $\hat { y }$ , given the input sequence of words $w$ :

$$
{ \hat { y } } \ = \ \operatorname * { a r g m a x } _ { y \in T } P ( \mathbf { y } | \mathbf { w } )
$$

Fig. 21.6 shows a sketch of a standard algorithm from He et al. (2017). Here each input word is mapped to pretrained embeddings, and then each token is concatenated with the predicate embedding and then passed through a feedforward network with a softmax which outputs a distribution over each SRL label. For decoding, a CRF layer can be used instead of the MLP layer on top of the biLSTM output to do global inference, but in practice this doesn’t seem to provide much benefit.

# 21.6.3 Evaluation of Semantic Role Labeling

The standard evaluation for semantic role labeling is to require that each argument label must be assigned to the exactly correct word sequence or parse constituent, and then compute precision, recall, and $F$ -measure. Identification and classification can also be evaluated separately. Two common datasets used for evaluation are CoNLL2005 (Carreras and Marquez\` , 2005) and CoNLL-2012 (Pradhan et al., 2013).

# 21.7 Selectional Restrictions

# selectional restriction

We turn in this section to another way to represent facts about the relationship between predicates and arguments. A selectional restriction is a semantic type constraint that a verb imposes on the kind of concepts that are allowed to fill its argument roles. Consider the two meanings associated with the following example:

(21.29) I want to eat someplace nearby.

There are two possible parses and semantic interpretations for this sentence. In the sensible interpretation, eat is intransitive and the phrase someplace nearby is an adjunct that gives the location of the eating event. In the nonsensical speaker-asGodzilla interpretation, eat is transitive and the phrase someplace nearby is the direct object and the THEME of the eating, like the NP Malaysian food in the following sentences:

# (21.30) I want to eat Malaysian food.

How do we know that someplace nearby isn’t the direct object in this sentence? One useful cue is the semantic fact that the THEME of EATING events tends to be something that is edible. This restriction placed by the verb eat on the filler of its THEME argument is a selectional restriction.

Selectional restrictions are associated with senses, not entire lexemes. We can see this in the following examples of the lexeme serve:

(21.31) The restaurant serves green-lipped mussels. (21.32) Which airlines serve Denver?

Example (21.31) illustrates the offering-food sense of serve, which ordinarily restricts its THEME to be some kind of food Example (21.32) illustrates the provides a commercial service to sense of serve, which constrains its THEME to be some type of appropriate location.

Selectional restrictions vary widely in their specificity. The verb imagine, for example, imposes strict requirements on its AGENT role (restricting it to humans and other animate entities) but places very few semantic requirements on its THEME role. A verb like diagonalize, on the other hand, places a very specific constraint on the filler of its THEME role: it has to be a matrix, while the arguments of the adjective odorless are restricted to concepts that could possess an odor:

(21.33) In rehearsal, I often ask the musicians to imagine a tennis game.   
(21.34) Radon is an odorless gas that can’t be detected by human senses.   
(21.35) To diagonalize a matrix is to find its eigenvalues.

These examples illustrate that the set of concepts we need to represent selectional restrictions (being a matrix, being able to possess an odor, etc) is quite open ended. This distinguishes selectional restrictions from other features for representing lexical knowledge, like parts-of-speech, which are quite limited in number.

# 21.7.1 Representing Selectional Restrictions

One way to capture the semantics of selectional restrictions is to use and extend the event representation of Appendix F. Recall that the neo-Davidsonian representation of an event consists of a single variable that stands for the event, a predicate denoting the kind of event, and variables and relations for the event roles. Ignoring the issue of the $\lambda$ -structures and using thematic roles rather than deep event roles, the semantic contribution of a verb like eat might look like the following:

$$
\exists e , x , y E a t i n g ( e ) \land A g e n t ( e , x ) \land T h e m e ( e , y )
$$

With this representation, all we know about $y$ , the filler of the THEME role, is that it is associated with an Eating event through the Theme relation. To stipulate the selectional restriction that $y$ must be something edible, we simply add a new term to that effect:

$$
\exists e , x , y E a t i n g ( e ) \land A g e n t ( e , x ) \land T h e m e ( e , y ) \land E d i b l e T h i n g ( y )
$$

When a phrase like ate a hamburger is encountered, a semantic analyzer can form the following kind of representation:

$$
\exists e , x , y E a t i n g ( e ) \land E a t e r ( e , x ) \land T h e m e ( e , y ) \land E d i b l e T h i n g ( y ) \land H a m b u r g e r ( y )
$$

This representation is perfectly reasonable since the membership of $y$ in the category Hamburger is consistent with its membership in the category EdibleThing, assuming a reasonable set of facts in the knowledge base. Correspondingly, the representation for a phrase such as ate a takeoff would be ill-formed because membership in an event-like category such as Takeoff would be inconsistent with membership in the category EdibleThing.

While this approach adequately captures the semantics of selectional restrictions, there are two problems with its direct use. First, using FOL to perform the simple task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms can do the job with far less computational cost. The second problem is that this approach presupposes a large, logical knowledge base of facts about the concepts that make up selectional restrictions. Unfortunately, although such common-sense knowledge bases are being developed, none currently have the kind of coverage necessary to the task.

A more practical approach is to state selectional restrictions in terms of WordNet synsets rather than as logical concepts. Each predicate simply specifies a WordNet synset as the selectional restriction on each of its arguments. A meaning representation is well-formed if the role filler word is a hyponym (subordinate) of this synset.

For our ate a hamburger example, for instance, we could set the selectional restriction on the THEME role of the verb eat to the synset {food, nutrient}, glossed as any substance that can be metabolized by an animal to give energy and build tissue. Luckily, the chain of hypernyms for hamburger shown in Fig. 21.7 reveals that hamburgers are indeed food. Again, the filler of a role need not match the restriction synset exactly; it just needs to have the synset as one of its superordinates.

We can apply this approach to the THEME roles of the verbs imagine, lift, and diagonalize, discussed earlier. Let us restrict imagine’s THEME to the synset {entity}, lift’s THEME to {physical entity}, and diagonalize to $\{ \mathrm { m a t r i x } \}$ . This arrangement correctly permits imagine a hamburger and lift a hamburger, while also correctly ruling out diagonalize a hamburger.

<table><tr><td>Sense1 hamburger， beefburger -- (a fried cake of minced beef served on a bun) =&gt; sandwich</td></tr><tr><td>=&gt; snack food =&gt;dish =&gt; nutriment，nourishment，nutrition... =&gt; food，nutrient =&gt; substance =&gt;matter</td></tr></table>

# 21.7.2 Selectional Preferences

In the earliest implementations, selectional restrictions were considered strict constraints on the kind of arguments a predicate could take (Katz and Fodor 1963, Hirst 1987). For example, the verb eat might require that its THEME argument be $[ + \mathtt { F } 0 0 \mathsf { D } ]$ . Early word sense disambiguation systems used this idea to rule out senses that violated the selectional restrictions of their governing predicates.

Very quickly, however, it became clear that these selectional restrictions were better represented as preferences rather than strict constraints (Wilks 1975b, Wilks 1975a). For example, selectional restriction violations (like inedible arguments of eat) often occur in well-formed sentences, for example because they are negated (21.36), or because selectional restrictions are overstated (21.37):

(21.36) But it fell apart in 1931, perhaps because people realized you can’t eat gold for lunch if you’re hungry.

(21.37) In his two championship trials, Mr. Kulkarni ate glass on an empty stomach, accompanied only by water and tea.

selectional preference strength

Modern systems for selectional preferences therefore specify the relation between a predicate and its possible arguments with soft constraints of some kind.

# Selectional Association

One of the most influential has been the selectional association model of Resnik (1993). Resnik defines the idea of selectional preference strength as the general amount of information that a predicate tells us about the semantic class of its arguments. For example, the verb eat tells us a lot about the semantic class of its direct objects, since they tend to be edible. The verb be, by contrast, tells us less about its direct objects. The selectional preference strength can be defined by the difference in information between two distributions: the distribution of expected semantic classes $P ( c )$ (how likely is it that a direct object will fall into class $c$ ) and the distribution of expected semantic classes for the particular verb $P ( c | \nu )$ (how likely is it that the direct object of the specific verb $\nu$ will fall into semantic class $c$ ). The greater the difference between these distributions, the more information the verb is giving us about possible objects. The difference between these two distributions can be quantified by relative entropy, or the Kullback-Leibler divergence (Kullback and Leibler, 1951). The Kullback-Leibler or KL divergence $D ( P | | Q )$ expresses the difference between two probability distributions $P$ and $Q$

$$
D ( P | | Q ) = \sum _ { x } P ( x ) \log \frac { P ( x ) } { Q ( x ) } 
$$

The selectional preference $S _ { R } ( \nu )$ uses the $\mathrm { K L }$ divergence to express how much information, in bits, the verb $\nu$ expresses about the possible semantic class of its argument.

$$
\begin{array}{c} \begin{array} { l } { { S _ { R } ( \nu ) } } \end{array} = \ D ( P ( c | \nu ) | | P ( c ) ) \ ~  \\ { = \ \sum _ { c } P ( c | \nu ) \log \frac { P ( c | \nu ) } { P ( c ) } } \end{array}
$$

Resnik then defines the selectional association of a particular class and verb as the relative contribution of that class to the general selectional preference of the verb:

$$
A _ { R } ( \nu , c ) ~ = ~ \frac { 1 } { S _ { R } ( \nu ) } P ( c | \nu ) \log \frac { P ( c | \nu ) } { P ( c ) }
$$

The selectional association is thus a probabilistic measure of the strength of association between a predicate and a class dominating the argument to the predicate. Resnik estimates the probabilities for these associations by parsing a corpus, counting all the times each predicate occurs with each argument word, and assuming that each word is a partial observation of all the WordNet concepts containing the word. The following table from Resnik (1996) shows some sample high and low selectional associations for verbs and some WordNet semantic classes of their direct objects.

<table><tr><td>Verb</td><td>Direct Object Semantic Class</td><td>Assoc</td><td>Direct Object Semantic Class</td><td>Assoc</td></tr><tr><td>read</td><td>WRITING</td><td>6.80</td><td>ACTIVITY</td><td>-.20</td></tr><tr><td>write</td><td>WRITING</td><td>7.26</td><td>COMMERCE</td><td>0</td></tr><tr><td>see</td><td>ENTITY</td><td>5.79</td><td>METHOD</td><td>-0.01</td></tr></table>

# Selectional Preference via Conditional Probability

An alternative to using selectional association between a verb and the WordNet class of its arguments is to use the conditional probability of an argument word given a predicate verb, directly modeling the strength of association of one verb (predicate) with one noun (argument).

The conditional probability model can be computed by parsing a very large corpus (billions of words), and computing co-occurrence counts: how often a given verb occurs with a given noun in a given relation. The conditional probability of an argument noun given a verb for a particular relation $P ( n | \nu , r )$ can then be used as a selectional preference metric for that pair of words (Brockmann and Lapata 2003, Keller and Lapata 2003):

$$
P ( n | \nu , r ) = \left\{ \begin{array} { c l } { { \frac { C ( n , \nu , r ) } { C ( \nu , r ) } } } & { { \mathrm { i f } C ( n , \nu , r ) > 0 } } \\ { { 0 } } & { { \mathrm { o t h e r w i s e } } } \end{array} \right.
$$

The inverse probability $P ( \nu | n , r )$ was found to have better performance in some cases (Brockmann and Lapata, 2003):

$$
P ( \nu | n , r ) = \left\{ \begin{array} { c l } { { \frac { C ( n , \nu , r ) } { C ( n , r ) } } } & { { \mathrm { i f } C ( n , \nu , r ) > 0 } } \\ { { 0 } } & { { \mathrm { o t h e r w i s e } } } \end{array} \right.
$$

An even simpler approach is to use the simple log co-occurrence frequency of the predicate with the argument log count $( \nu , n , r )$ instead of conditional probability; this seems to do better for extracting preferences for syntactic subjects rather than objects (Brockmann and Lapata, 2003).

# Evaluating Selectional Preferences

# pseudowords

One way to evaluate models of selectional preferences is to use pseudowords (Gale et al. 1992b, Schutze ¨ 1992a). A pseudoword is an artificial word created by concatenating a test word in some context (say banana) with a confounder word (say door) to create banana-door). The task of the system is to identify which of the two words is the original word. To evaluate a selectional preference model (for example on the relationship between a verb and a direct object) we take a test corpus and select all verb tokens. For each verb token (say drive) we select the direct object (e.g., car), concatenated with a confounder word that is its nearest neighbor, the noun with the frequency closest to the original (say house), to make car/house). We then use the selectional preference model to choose which of car and house are more preferred objects of drive, and compute how often the model chooses the correct original object (e.g., car) (Chambers and Jurafsky, 2010).

Another evaluation metric is to get human preferences for a test set of verbargument pairs, and have them rate their degree of plausibility. This is usually done by using magnitude estimation, a technique from psychophysics, in which subjects rate the plausibility of an argument proportional to a modulus item. A selectional preference model can then be evaluated by its correlation with the human preferences (Keller and Lapata, 2003).

# 21.8 Primitive Decomposition of Predicates

One way of thinking about the semantic roles we have discussed through the chapter is that they help us define the roles that arguments play in a decompositional way, based on finite lists of thematic roles (agent, patient, instrument, proto-agent, protopatient, etc.). This idea of decomposing meaning into sets of primitive semantic elements or features, called primitive decomposition or componential analysis, has been taken even further, and focused particularly on predicates.

Consider these examples of the verb kill:

(21.41) Jim killed his philodendron.   
(21.42) Jim did something to cause his philodendron to become not alive.

There is a truth-conditional (‘propositional semantics’) perspective from which these two sentences have the same meaning. Assuming this equivalence, we could represent the meaning of kill as:

$$
\operatorname { K I L L } ( \mathbf { x } , \mathbf { y } ) \Leftrightarrow \operatorname { C A U S E } ( \mathbf { x } , \operatorname { B E C O M E } ( \operatorname { N O T } ( \operatorname { A L I V E } ( \mathbf { y } ) ) ) )
$$

thus using semantic primitives like $d o$ , cause, become not, and alive.

Indeed, one such set of potential semantic primitives has been used to account for some of the verbal alternations discussed in Section 21.2 (Lakoff 1965, Dowty 1979). Consider the following examples.

(21.44) John opened the door. $\Rightarrow$ CAUSE(John, BECOME(OPEN(door))) (21.45) The door opened. $\Rightarrow$ BECOME(OPEN(door))

# (21.46) The door is open. $\Rightarrow$ OPEN(door)

The decompositional approach asserts that a single state-like predicate associated with open underlies all of these examples. The differences among the meanings of these examples arises from the combination of this single predicate with the primitives CAUSE and BECOME.

While this approach to primitive decomposition can explain the similarity between states and actions or causative and non-causative predicates, it still relies on having a large number of predicates like open. More radical approaches choose to break down these predicates as well. One such approach to verbal predicate decomposition that played a role in early natural language systems is conceptual dependency (CD), a set of ten primitive predicates, shown in Fig. 21.8.

conceptual dependency

<table><tr><td>Primitive</td><td>Definition</td></tr><tr><td>ATRANS</td><td>The abstract transfer of possession or control from one entity to another</td></tr><tr><td>PTRANS</td><td> The physical transfer of an object from one location to another</td></tr><tr><td>MTRANS</td><td>The transfer of mental concepts between entities or within an entity</td></tr><tr><td>MBUILD PROPEL</td><td> The creation of new information within an entity</td></tr><tr><td>MovE</td><td> The application of physical force to move an object</td></tr><tr><td>INGEST</td><td>The integral movement of a body part by an animal</td></tr><tr><td>EXPEL</td><td>The taking in of a substance by an animal</td></tr><tr><td>SPEAK</td><td> The expulsion of something from an animal</td></tr><tr><td></td><td> The action of producing a sound</td></tr><tr><td>ATTEND</td><td>The action of focusing a sense organ</td></tr></table>

Figure 21.8 A set of conceptual dependency primitives.

Below is an example sentence along with its CD representation. The verb brought is translated into the two primitives ATRANS and PTRANS to indicate that the waiter both physically conveyed the check to Mary and passed control of it to her. Note that CD also associates a fixed set of thematic roles with each primitive to represent the various participants in the action.

(21.47) The waiter brought Mary the check.

∃x, y Atrans(x) ∧ Actor(x,Waiter) ∧ Ob ject(x,Check) ∧ To(x, Mary) ∧Ptrans(y) ∧ Actor(y,Waiter) ∧ Ob ject(y,Check) ∧ To(y, Mary)

# 21.9 Summary

• Semantic roles are abstract models of the role an argument plays in the event described by the predicate.   
• Thematic roles are a model of semantic roles based on a single finite list of roles. Other semantic role models include per-verb semantic role lists and proto-agent/proto-patient, both of which are implemented in PropBank, and per-frame role lists, implemented in FrameNet.   
• Semantic role labeling is the task of assigning semantic role labels to the constituents of a sentence. The task is generally treated as a supervised machine learning task, with models trained on PropBank or FrameNet. Algorithms generally start by parsing a sentence and then automatically tag each parse tree node with a semantic role. Neural models map straight from words end-to-end.   
• Semantic selectional restrictions allow words (particularly predicates) to post constraints on the semantic properties of their argument words. Selectional preference models (like selectional association or simple conditional probability) allow a weight or probability to be assigned to the association between a predicate and an argument word or class.

# Bibliographical and Historical Notes

Although the idea of semantic roles dates back to Pan¯ . ini, they were re-introduced into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fillmore had become interested in argument structure by studying Lucien Tesniere’s\` groundbreaking El´ ements de Syntaxe Structurale ´ (Tesniere \` , 1959) in which the term ‘dependency’ was introduced and the foundations were laid for dependency grammar. Following Tesniere’s terminology, Fillmore first referred to argument roles as \` actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003)) and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument, etc.), that could be taken on by the arguments of predicates. Verbs would be listed in the lexicon with their case frame, the list of obligatory (or optional) case arguments.

The idea that semantic roles could provide an intermediate level of semantic representation that could help map from syntactic parse structures to deeper, more fully-specified representations of meaning was quickly adopted in natural language processing, and systems for extracting case frames were created for machine translation (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language processing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). Generalpurpose semantic role labelers were developed. The earliest ones (Simmons, 1973) first parsed a sentence by means of an ATN (Augmented Transition Network) parser. Each verb then had a set of rules specifying how the parse should be mapped to semantic roles. These rules mainly made reference to grammatical functions (subject, object, complement of specific prepositions) but also checked constituent internal features such as the animacy of head nouns. Later systems assigned roles from prebuilt parse trees, again by using dictionaries with verb-specific case frames (Levin 1977, Marcus 1980).

By 1977 case representation was widely used and taught in AI and NLP courses, and was described as a standard of natural language processing in the first edition of Winston’s 1977 textbook Artificial Intelligence.

In the 1980s Fillmore proposed his model of frame semantics, later describing the intuition as follows:

“The idea behind frame semantics is that speakers are aware of possibly quite complex situation types, packages of connected expectations, that go by various names—frames, schemas, scenarios, scripts, cultural narratives, memes—and the words in our language are understood with such frames as their presupposed background.” (Fillmore, 2012, p. 712)

The word frame seemed to be in the air for a suite of related notions proposed at about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as well as related notions with other names like scripts (Schank and Abelson, 1975) and schemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison). Fillmore was also influenced by the semantic field theorists and by a visit to the Yale AI lab where he took notice of the lists of slots and fillers used by early information extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s Fillmore drew on these insights to begin the FrameNet corpus annotation project.

At the same time, Beth Levin drew on her early case frame dictionaries (Levin, 1977) to develop her book which summarized sets of verb classes defined by shared argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus created by Martha Palmer and colleagues (Palmer et al., 2005).

The combination of rich linguistic annotation and corpus-based approach instantiated in FrameNet and PropBank led to a revival of automatic approaches to semantic role labeling, first on FrameNet (Gildea and Jurafsky, 2000) and then on PropBank data (Gildea and Palmer, 2002, inter alia). The problem first addressed in the 1970s by handwritten rules was thus now generally recast as one of supervised machine learning enabled by large and consistent databases. Many popular features used for role labeling are defined in Gildea and Jurafsky (2002), Surdeanu et al. (2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao et al. (2009). The use of dependency rather than constituency parses was introduced in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer et al. (2010) and Marquez et al. \` (2008).

The use of neural approaches to semantic role labeling was pioneered by Collobert et al. (2011), who applied a CRF on top of a convolutional net. Early work like Foland, Jr. and Martin (2015) focused on using dependency features. Later work eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to augment the biLSTM architecture with highway networks and also replace the CRF with $\mathbf { A } ^ { * }$ decoding that make it possible to apply a wide variety of global constraints in SRL decoding.

Most semantic role labeling schemes only work within a single sentence, focusing on the object of the verbal (or nominal, in the case of NomBank) predicate. However, in many cases, a verbal or nominal predicate may have an implicit argument: one that appears only in a contextual sentence, or perhaps not at all and must be inferred. In the two sentences This house has a new owner. The sale was finalized 10 days ago. the sale in the second sentence has no ARG1, but a reasonable reader would infer that the Arg1 should be the house mentioned in the prior sentence. Finding these arguments, implicit argument detection (sometimes shortened as iSRL) was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do et al. (2017) for more recent neural models.

To avoid the need for huge labeled training sets, unsupervised approaches for semantic role labeling attempt to induce the set of semantic roles by clustering over arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev (2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khoddam (2014).

Recent innovations in frame labeling include connotation frames, which mark richer information about the argument of predicates. Connotation frames mark the sentiment of the writer or reader toward the arguments (for example using the verb survive in he survived a bombing expresses the writer’s sympathy toward the subject he and negative sentiment toward the bombing. See Chapter 22 for more details.

Selectional preference has been widely studied beyond the selectional association models of Resnik (1993) and Resnik (1996). Methods have included clustering (Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic models (Seaghdha ´ 2010, Ritter et al. 2010b), and constraints can be expressed at the level of words or classes (Agirre and Martinez, 2001). Selectional preferences have also been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al. 2013, Do et al. 2017).

# Exercises

# CHAPTER 22 Lexicons for Sentiment, Affect,and Connotation

Some day we’ll be able to measure the power of words Maya Angelou

In this chapter we turn to tools for interpreting affective meaning, extending our study of sentiment analysis in Chapter 4. We use the word ‘affective’, following the tradition in affective computing (Picard, 1995) to mean emotion, sentiment, personality, mood, and attitudes. Affective meaning is closely related to subjectivity, the study of a speaker or writer’s evaluations, opinions, emotions, and speculations (Wiebe et al., 1999).

How should affective meaning be defined? One influential typology of affective states comes from Scherer (2000), who defines each class of affective states by factors like its cognitive realization and time course (Fig. 22.1).

Emotion: Relatively brief episode of response to the evaluation of an external or internal event as being of major significance. (angry, sad, joyful, fearful, ashamed, proud, elated, desperate)   
Mood: Diffuse affect state, most pronounced as change in subjective feeling, of low intensity but relatively long duration, often without apparent cause. (cheerful, gloomy, irritable, listless, depressed, buoyant)   
Interpersonal stance: Affective stance taken toward another person in a specific interaction, coloring the interpersonal exchange in that situation. (distant, cold, warm, supportive, contemptuous, friendly)   
Attitude: Relatively enduring, affectively colored beliefs, preferences, and predispositions towards objects or persons. (liking, loving, hating, valuing, desiring)   
Personality traits: Emotionally laden, stable personality dispositions and behavior tendencies, typical for a person. (nervous, anxious, reckless, morose, hostile, jealous)

We can design extractors for each of these kinds of affective states. Chapter 4 already introduced sentiment analysis, the task of extracting the positive or negative orientation that a writer expresses in a text. This corresponds in Scherer’s typology to the extraction of attitudes: figuring out what people like or dislike, from affectrich texts like consumer reviews of books or movies, newspaper editorials, or public sentiment in blogs or tweets.

Detecting emotion and moods is useful for detecting whether a student is confused, engaged, or certain when interacting with a tutorial system, whether a caller to a help line is frustrated, whether someone’s blog posts or tweets indicated depression. Detecting emotions like fear in novels, for example, could help us trace what groups or situations are feared and how that changes over time.

Detecting different interpersonal stances can be useful when extracting information from human-human conversations. The goal here is to detect stances like friendliness or awkwardness in interviews or friendly conversations, for example for summarizing meetings or finding parts of a conversation where people are especially excited or engaged, conversational hot spots that can help in meeting summarization. Detecting the personality of a user—such as whether the user is an extrovert or the extent to which they are open to experience— can help improve conversational agents, which seem to work better if they match users’ personality expectations (Mairesse and Walker, 2008). And affect is important for generation as well as recognition; synthesizing affect is important for conversational agents in various domains, including literacy tutors such as children’s storybooks, or computer games.

In Chapter 4 we introduced the use of naive Bayes classification to classify a document’s sentiment. Various classifiers have been successfully applied to many of these tasks, using all the words in the training set as input to a classifier which then determines the affect status of the text.

connotations

In this chapter we focus on an alternative model, in which instead of using every word as a feature, we focus only on certain words, ones that carry particularly strong cues to affect or sentiment. We call these lists of words affective lexicons or sentiment lexicons. These lexicons presuppose a fact about semantics: that words have affective meanings or connotations. The word connotation has different meanings in different fields, but here we use it to mean the aspects of a word’s meaning that are related to a writer or reader’s emotions, sentiment, opinions, or evaluations. In addition to their ability to help determine the affective status of a text, connotation lexicons can be useful features for other kinds of affective tasks, and for computational social science analysis.

In the next sections we introduce basic theories of emotion, show how sentiment lexicons are a special case of emotion lexicons, and mention some useful lexicons. We then survey three ways for building lexicons: human labeling, semi-supervised, and supervised. Finally, we talk about how to detect affect toward a particular entity, and introduce connotation frames.

# 22.1 Defining Emotion

# emotion

One of the most important affective classes is emotion, which Scherer (2000) defines as a “relatively brief episode of response to the evaluation of an external or internal event as being of major significance”.

Detecting emotion has the potential to improve a number of language processing tasks. Emotion recognition could help dialogue systems like tutoring systems detect that a student was unhappy, bored, hesitant, confident, and so on. Automatically detecting emotions in reviews or customer responses (anger, dissatisfaction, trust) could help businesses recognize specific problem areas or ones that are going well. Emotion can play a role in medical NLP tasks like helping diagnose depression or suicidal intent. Detecting emotions expressed toward characters in novels might play a role in understanding how different social groups were viewed by society at different times.

Computational models of emotion in NLP have mainly been based on two families of theories of emotion (out of the many studied in the field of affective science). In one of these families, emotions are viewed as fixed atomic units, limited in number, and from which others are generated, often called basic emotions (Tomkins

1962, Plutchik 1962), a model dating back to Darwin. Perhaps the most well-known of this family of theories are the 6 emotions proposed by Ekman (e.g., Ekman 1999) to be universally present in all cultures: surprise, happiness, anger, fear, disgust, sadness. Another atomic theory is the Plutchik (1980) wheel of emotion, consisting of 8 basic emotions in four opposing pairs: joy–sadness, anger–fear, trust–disgust, and anticipation–surprise, together with the emotions derived from them, shown in Fig. 22.2.

![## Image Analysis: ac53f63752fa05475de01863e697df2e69d4f0a9a99af0393452be3abe6b6194.jpg

**Conceptual Understanding:**
The image conceptually represents the Plutchik Wheel of Emotion, a model designed to illustrate the relationships between various emotions. Its main purpose is to categorize human emotions, demonstrate their varying degrees of intensity, and show how primary emotions can combine to form more complex, secondary emotions. It conveys the idea that emotions are interlinked and can be understood in terms of their fundamental components and their mixtures, providing a systematic framework for emotional understanding.

**Content Interpretation:**
The image, titled 'Plutchik wheel of emotion', illustrates Robert Plutchik's psychoevolutionary theory of emotion. It depicts eight basic emotions, their varying intensities, and how they can combine to form secondary emotions. The structure shows a circular arrangement with 8 'petals', each representing a primary emotion. Within each petal, the emotion is shown at three levels of intensity, moving from high intensity (innermost) to low intensity (outermost). The labels 'rage', 'vigilance', 'ecstasy', 'admiration', 'terror', 'amazement', 'grief', and 'loathing' represent the highest intensity forms of the eight primary emotions, located in the central core of the wheel. Moving outwards, the intensity decreases to 'anger', 'anticipation', 'joy', 'trust', 'fear', 'surprise', 'sadness', and 'disgust'. The lowest intensity forms are 'annoyance', 'interest', 'serenity', 'acceptance', 'apprehension', 'distraction', 'pensiveness', and 'boredom'. The image also presents eight 'mixed emotions' or 'dyads' on a dashed outer circle, positioned between the primary emotion petals. These include 'aggressiveness' (between annoyance and interest), 'optimism' (between interest and serenity), 'love' (between serenity and acceptance), 'submission' (between acceptance and apprehension), 'awe' (between apprehension and distraction), 'disapproval' (between distraction and pensiveness), 'remorse' (between pensiveness and boredom), and 'contempt' (between boredom and annoyance). The visual design with color gradients further emphasizes the continuous nature and relatedness of emotions.

**Key Insights:**
The main takeaways from the Plutchik wheel of emotion are: 1. There are eight primary emotions, each represented by a distinct 'petal' and color: Rage/Anger/Annoyance (red), Vigilance/Anticipation/Interest (orange), Ecstasy/Joy/Serenity (yellow), Admiration/Trust/Acceptance (green), Terror/Fear/Apprehension (dark green), Amazement/Surprise/Distraction (light blue), Grief/Sadness/Pensiveness (dark blue/purple), and Loathing/Disgust/Boredom (magenta/pink). 2. Each primary emotion exists on a spectrum of intensity, from highest ('rage', 'ecstasy') to lowest ('annoyance', 'serenity'). This is shown by the concentric segments within each petal. 3. More complex or 'mixed' emotions (dyads) can be formed by combining adjacent primary emotions. Examples include 'love' (combination of serenity and acceptance), 'optimism' (interest and serenity), 'aggressiveness' (annoyance and interest), 'contempt' (boredom and annoyance), 'remorse' (pensiveness and boredom), 'disapproval' (distraction and pensiveness), 'awe' (apprehension and distraction), and 'submission' (acceptance and apprehension). The visual arrangement supports the idea that emotions are not discrete but exist in relationships of intensity and combination.

**Document Context:**
This image directly relates to the 'emotion' section of the document, as indicated by the context. It serves as Figure 22.2, providing a visual and structured representation of emotion theory, specifically Plutchik's wheel. It would likely be used to introduce or explain fundamental concepts of emotional classification, intensity, and combination, offering a foundational model for understanding human affective states within the broader discussion of emotion in the document.

**Summary:**
The image displays the Plutchik wheel of emotion, a psychoevolutionary theory of emotion. It is structured as a colorful, eight-petaled flower-like diagram. Each petal represents a primary emotion, showing three levels of intensity, with the most intense level at the center and decreasing intensity towards the outer edge of the petal. The center of the wheel is composed of the highest intensity forms of the eight primary emotions. Between each of the main petals, on a dashed outer circle, are labels for secondary emotions, which are conceptualized as combinations of adjacent primary emotions. The wheel illustrates how emotions can be categorized, their varying intensities, and how they can combine to form more complex emotional states. The colors smoothly transition from one primary emotion to the next, visually reinforcing the relationships between them. This comprehensive visual model helps in understanding the spectrum and dynamics of human emotions.](images/ac53f63752fa05475de01863e697df2e69d4f0a9a99af0393452be3abe6b6194.jpg)
Figure 22.2 Plutchik wheel of emotion.

The second class of emotion theories widely used in NLP views emotion as a space in 2 or 3 dimensions (Russell, 1980). Most models include the two dimensions valence and arousal, and many add a third, dominance. These can be defined as:

valence: the pleasantness of the stimulus   
arousal: the level of alertness, activeness, or energy provoked by the stimulus   
dominance: the degree of control or dominance exerted by the stimulus or the emotion

Sentiment can be viewed as a special case of this second view of emotions as points in space. In particular, the valence dimension, measuring how pleasant or unpleasant a word is, is often used directly as a measure of sentiment.

In these lexicon-based models of affect, the affective meaning of a word is generally fixed, irrespective of the linguistic context in which a word is used, or the dialect or culture of the speaker. By contrast, other models in affective science represent emotions as much richer processes involving cognition (Barrett et al., 2007). In appraisal theory, for example, emotions are complex processes, in which a person considers how an event is congruent with their goals, taking into account variables like the agency, certainty, urgency, novelty and control associated with the event (Moors et al., 2013). Computational models in NLP taking into account these richer theories of emotion will likely play an important role in future work.

# 22.2 Available Sentiment and Affect Lexicons

# General Inquirer

A wide variety of affect lexicons have been created and released. The most basic lexicons label words along one dimension of semantic variability, generally called “sentiment” or “valence”.

In the simplest lexicons this dimension is represented in a binary fashion, with a wordlist for positive words and a wordlist for negative words. The oldest is the General Inquirer (Stone et al., 1966), which drew on content analysis and on early work in the cognitive psychology of word meaning (Osgood et al., 1957). The General Inquirer has a lexicon of 1915 positive words and a lexicon of 2291 negative words (as well as other lexicons discussed below). The MPQA Subjectivity lexicon (Wilson et al., 2005) has 2718 positive and 4912 negative words drawn from prior lexicons plus a bootstrapped list of subjective words and phrases (Riloff and Wiebe, 2003). Each entry in the lexicon is hand-labeled for sentiment and also labeled for reliability (strongly subjective or weakly subjective). The polarity lexicon of $\mathrm { H u }$ and Liu (2004b) gives 2006 positive and 4783 negative words, drawn from product reviews, labeled using a bootstrapping method from WordNet.

<table><tr><td>Positive</td><td>admire, amazing, assure, celebration, charm, eager, enthusiastic, excellent, fancy, fan- tastic, frolic, graceful, happy, joy, luck, majesty, mercy, nice, patience, perfect, proud, rejoice, relief, respect, satisfactorily, sensational, super, terrific, thank, vivid, wise, won- derful, zest</td></tr><tr><td></td><td>Negative abominable, anger, anxious, bad, catastrophe, cheap, complaint, condescending, deceit, defective, disappointment, embarrass,fake, fear, filthy, fool, guilt, hate, idiot, inflict,lazy, miserable, mourn, nervous, objection, pest, plot, reject, scream, silly, terble, unfriendly, vile, wicked</td></tr></table>

Figure 22.3 Some words with consistent sentiment across the General Inquirer (Stone et al., 1966), the MPQA Subjectivity lexicon (Wilson et al., 2005), and the polarity lexicon of Hu and Liu (2004b).

Slightly more general than these sentiment lexicons are lexicons that assign each word a value on all three affective dimensions. The NRC Valence, Arousal, and Dominance (VAD) lexicon (Mohammad, 2018a) assigns valence, arousal, and dominance scores to 20,000 words. Some examples are shown in Fig. 22.4.

<table><tr><td colspan="2">Valence</td><td colspan="2">Arousal</td><td colspan="2">Dominance</td></tr><tr><td>vacation</td><td>.840</td><td>enraged</td><td>.962</td><td> powerful</td><td>.991</td></tr><tr><td>delightful</td><td>.918</td><td> party</td><td>.840</td><td> authority</td><td>.935</td></tr><tr><td>whistle</td><td>.653</td><td> organized</td><td>.337</td><td> saxophone</td><td>.482</td></tr><tr><td>consolation</td><td>.408</td><td>effortless</td><td>.120</td><td> discouraged</td><td>.0090</td></tr><tr><td>torture</td><td>.115</td><td> napping</td><td>.046</td><td>weak</td><td>.045</td></tr></table>

Figure 22.4 Values of sample words on the emotional dimensions of Mohammad (2018a).

# EmoLex

The NRC Word-Emotion Association Lexicon, also called EmoLex (Mohammad and Turney, 2013), uses the Plutchik (1980) 8 basic emotions defined above. The lexicon includes around 14,000 words including words from prior lexicons as well as frequent nouns, verbs, adverbs and adjectives. Values from the lexicon for some sample words:

<table><tr><td>Word</td><td>ssauses 1ssssp 三</td><td> 1sn.1</td><td></td><td>0</td></tr><tr><td>reward 0 1 0 worry 0 1 0 tenderness 0 0 0 sweetheart 0 1 0 suddenly 0 0 0 thirst 0</td><td>0 1 0 1 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 1 0 0 0 1 1 0 0 1 0 0 0 0</td><td>1 1 0 0 0 1 1 1 0 0 0 0 0 0</td><td></td><td>1 0 0 0 0 1</td></tr></table>

For a smaller set of 5,814 words, the NRC Emotion/Affect Intensity Lexicon (Mohammad, 2018b) contains real-valued scores of association for anger, fear, joy, and sadness; Fig. 22.5 shows examples.

<table><tr><td colspan="2">Anger</td><td colspan="2">Fear</td><td colspan="2">Joy</td><td colspan="2">Sadness</td></tr><tr><td>outraged</td><td>0.964</td><td>horror</td><td>0.923</td><td> superb</td><td>0.864</td><td> sad</td><td>0.844</td></tr><tr><td>violence</td><td>0.742</td><td> anguish</td><td>0.703</td><td>cheered</td><td>0.773</td><td> guilt</td><td>0.750</td></tr><tr><td>coup</td><td>0.578</td><td> pestilence</td><td>0.625</td><td>rainbow</td><td>0.531</td><td> unkind</td><td>0.547</td></tr><tr><td>oust</td><td>0.484</td><td> stressed</td><td>0.531</td><td> gesture</td><td>0.387</td><td> difficulties</td><td>0.421</td></tr><tr><td>suspicious</td><td>0.484</td><td>failing</td><td>0.531</td><td>warms</td><td>0.391</td><td> beggar</td><td>0.422</td></tr><tr><td>nurture</td><td>0.059</td><td>confident</td><td>0.094</td><td> hardship</td><td>.031</td><td>sing</td><td>0.017</td></tr></table>

Figure 22.5 Sample emotional intensities for words for anger, fear, joy, and sadness from Mohammad (2018b).

# LIWC

LIWC, Linguistic Inquiry and Word Count, is a widely used set of 73 lexicons containing over 2300 words (Pennebaker et al., 2007), designed to capture aspects of lexical meaning relevant for social psychological tasks. In addition to sentiment-related lexicons like ones for negative emotion (bad, weird, hate, problem, tough) and positive emotion (love, nice, sweet), LIWC includes lexicons for categories like anger, sadness, cognitive mechanisms, perception, tentative, and inhibition, shown in Fig. 22.6.

There are various other hand-built affective lexicons. The General Inquirer includes additional lexicons for dimensions like strong vs. weak, active vs. passive, overstated vs. understated, as well as lexicons for categories like pleasure, pain, virtue, vice, motivation, and cognitive orientation.

Another useful feature for various tasks is the distinction between concrete words like banana or bathrobe and abstract words like belief and although. The lexicon in Brysbaert et al. (2014) used crowdsourcing to assign a rating from 1 to 5 of the concreteness of 40,000 words, thus assigning banana, bathrobe, and bagel 5, belief 1.19, although 1.07, and in between words like brisk a 2.5.

# 22.3 Creating Affect Lexicons by Human Labeling

The earliest method used to build affect lexicons, and still in common use, is to have humans label each word. This is now most commonly done via crowdsourcing: breaking the task into small pieces and distributing them to a large number of annotators. Let’s take a look at some of the methodological choices for two crowdsourced emotion lexicons.

<table><tr><td>Positive Emotion</td><td>Negative Emotion</td><td>Insight</td><td>Inhibition</td><td> Family</td><td>Negate</td></tr><tr><td>appreciat*</td><td> anger*</td><td>aware*</td><td>avoid*</td><td>brother*</td><td>aren&#x27;t</td></tr><tr><td>comfort*</td><td>bore*</td><td>believe</td><td> careful*</td><td> cousin*</td><td>cannot</td></tr><tr><td>great</td><td>cry</td><td>decid*</td><td>hesitat*</td><td> daughter*</td><td>didn&#x27;t</td></tr><tr><td>happy</td><td> despair*</td><td>feel</td><td> limit*</td><td>family</td><td> neither</td></tr><tr><td>interest</td><td>fail*</td><td> figur*</td><td> oppos*</td><td>father*</td><td>never</td></tr><tr><td>joy*</td><td>fear</td><td>know</td><td> prevent*</td><td> grandf*</td><td>no</td></tr><tr><td>perfect*</td><td>griev*</td><td>knew</td><td> reluctan*</td><td> grandm*</td><td>nobod*</td></tr><tr><td>please*</td><td>hate*</td><td> means</td><td> safe*</td><td> husband</td><td>none</td></tr><tr><td>safe*</td><td> panic*</td><td>notice*</td><td> stop</td><td> mom</td><td>nor</td></tr><tr><td>terrific</td><td> suffers</td><td> recogni*</td><td> stubborn*</td><td> mother</td><td> nothing</td></tr><tr><td>value</td><td>terrify</td><td>sense</td><td>wait</td><td>niece*</td><td> nowhere</td></tr><tr><td>wow*</td><td>violent*</td><td>think</td><td>wary</td><td>wife</td><td>without</td></tr></table>

Figure 22.6 Samples from 5 of the 73 lexical categories in LIWC (Pennebaker et al., 2007). The \* means the previous letters are a word prefix and all words with that prefix are included in the category.

The NRC Emotion Lexicon (EmoLex) (Mohammad and Turney, 2013), labeled emotions in two steps. To ensure that the annotators were judging the correct sense of the word, they first answered a multiple-choice synonym question that primed the correct sense of the word (without requiring the annotator to read a potentially confusing sense definition). These were created automatically using the headwords associated with the thesaurus category of the sense in question in the Macquarie dictionary and the headwords of 3 random distractor categories. An example:

Which word is closest in meaning (most related) to startle?

• automobile • shake • honesty • entertain

For each word (e.g. startle), the annotator was then asked to rate how associated that word is with each of the 8 emotions (joy, fear, anger, etc.). The associations were rated on a scale of not, weakly, moderately, and strongly associated. Outlier ratings were removed, and then each term was assigned the class chosen by the majority of the annotators, with ties broken by choosing the stronger intensity, and then the 4 levels were mapped into a binary label for each word (no and weak mapped to 0, moderate and strong mapped to 1).

The NRC VAD Lexicon (Mohammad, 2018a) was built by selecting words and emoticons from prior lexicons and annotating them with crowd-sourcing using bestworst scaling (Louviere et al. 2015, Kiritchenko and Mohammad 2017). In bestworst scaling, annotators are given N items (usually 4) and are asked which item is the best (highest) and which is the worst (lowest) in terms of some property. The set of words used to describe the ends of the scales are taken from prior literature. For valence, for example, the raters were asked:

Q1. Which of the four words below is associated with the MOST happiness / pleasure / positiveness / satisfaction / contentedness / hopefulness OR LEAST unhappiness / annoyance / negativeness / dissatisfaction / melancholy / despair? (Four words listed as options.) Q2. Which of the four words below is associated with the LEAST happiness / pleasure / positiveness / satisfaction / contentedness / hopefulness OR MOST unhappiness / annoyance / negativeness / dissatisfaction / melancholy / despair? (Four words listed as options.)

The score for each word in the lexicon is the proportion of times the item was chosen as the best (highest V/A/D) minus the proportion of times the item was chosen as the worst (lowest V/A/D). The agreement between annotations are evaluated by splithalf reliability: split the corpus in half and compute the correlations between the annotations in the two halves.

# 22.4 Semi-supervised Induction of Affect Lexicons

Another common way to learn sentiment lexicons is to start from a set of seed words that define two poles of a semantic axis (words like good or bad), and then find ways to label each word $w$ by its similarity to the two seed sets. Here we summarize two families of seed-based semi-supervised lexicon induction algorithms, axis-based and graph-based.

# 22.4.1 Semantic Axis Methods

One of the most well-known lexicon induction methods, the Turney and Littman (2003) algorithm, is given seed words like good or bad, and then for each word $w$ to be labeled, measures both how similar it is to good and how different it is from bad. Here we describe a slight extension of the algorithm due to An et al. (2018), which is based on computing a semantic axis.

In the first step, we choose seed words by hand. There are two methods for dealing with the fact that the affect of a word is different in different contexts: (1) start with a single large seed lexicon and rely on the induction algorithm to fine-tune it to the domain, or (2) choose different seed words for different genres. Hellrich et al. (2019) suggests that for modeling affect across different historical time periods, starting with a large modern affect dictionary is better than small seedsets tuned to be stable across time. As an example of the second approach, Hamilton et al. (2016a) define one set of seed words for general sentiment analysis, a different set for Twitter, and yet another set for sentiment in financial text:

<table><tr><td>Domain</td><td>Positive seeds</td><td>Negative seeds</td></tr><tr><td>General</td><td>good,lovely, excellent, fortunate, pleas- ant， delightful， perfect， loved, love, happy</td><td>bad， horrible, poor, unfortunate, un- pleasant, disgusting, evil, hated, hate, unhappy</td></tr><tr><td>Twitter</td><td>love, loved， loves， awesome， nice, amazing, best, fantastic, correct, happy</td><td>hate, hated, hates, terrible, nasty, awful, worst, horrible, wrong, sad</td></tr><tr><td>Finance</td><td>successful, excellent, profit, beneficial, improving， improved, success， gains, positive</td><td>negligent, loss, volatile, wrong, losses, damages, bad, litigation, failure, down, negative</td></tr></table>

In the second step, we compute embeddings for each of the pole words. These embeddings can be off-the-shelf word2vec embeddings, or can be computed directly

on a specific corpus (for example using a financial corpus if a finance lexicon is the goal), or we can fine-tune off-the-shelf embeddings to a corpus. Fine-tuning is especially important if we have a very specific genre of text but don’t have enough data to train good embeddings. In fine-tuning, we begin with off-the-shelf embeddings like word2vec, and continue training them on the small target corpus.

Once we have embeddings for each pole word, we create an embedding that represents each pole by taking the centroid of the embeddings of each of the seed words; recall that the centroid is the multidimensional version of the mean. Given a set of embeddings for the positive seed words ${ \cal S } ^ { + } = \{ { \cal E } ( w _ { 1 } ^ { + } ) , { \cal E } ( w _ { 2 } ^ { + } ) , . . . , { \cal E } ( w _ { n } ^ { + } ) \}$ , and embeddings for the negative seed words $S ^ { - } = \{ E ( w _ { 1 } ^ { - } ) , E ( w _ { 2 } ^ { - } ) , . . . , E ( w _ { m } ^ { - } ) \}$ , the pole centroids are:

$$
\begin{array} { l } { { \displaystyle { \mathsf { V } } ^ { + } = \frac { 1 } { n } \sum _ { 1 } ^ { n } E ( w _ { i } ^ { + } ) } } \\ { { \displaystyle { \mathsf { V } } ^ { - } = \frac { 1 } { m } \sum _ { 1 } ^ { m } E ( w _ { i } ^ { - } ) } } \end{array}
$$

The semantic axis defined by the poles is computed just by subtracting the two vectors:

$$
\pmb { \mathsf { V } } _ { a x i s } = \pmb { \mathsf { V } } ^ { + } - \pmb { \mathsf { V } } ^ { - }
$$

$\pmb { \mathsf { v } } _ { a x i s }$ , the semantic axis, is a vector in the direction of positive sentiment. Finally, we compute (via cosine similarity) the angle between the vector in the direction of positive sentiment and the direction of $w$ ’s embedding. A higher cosine means that $w$ is more aligned with $S ^ { + }$ than $S ^ { - }$ .

$$
\begin{array} { r } { \mathrm { s c o r e } ( w ) ~ = ~ \cos { \left( E ( w ) , \pmb { v } _ { \mathrm { a x i s } } \right) } } \\ { ~ = ~ \frac { E ( w ) \cdot \pmb { v } _ { \mathrm { a x i s } } } { \| E ( w ) \| \| \pmb { \operatorname* { v } } _ { \mathrm { a x i s } } \| } } \end{array}
$$

If a dictionary of words with sentiment scores is sufficient, we’re done! Or if we need to group words into a positive and a negative lexicon, we can use a threshold or other method to give us discrete lexicons.

# 22.4.2 Label Propagation

An alternative family of methods defines lexicons by propagating sentiment labels on graphs, an idea suggested in early work by Hatzivassiloglou and McKeown (1997). We’ll describe the simple SentProp (Sentiment Propagation) algorithm of Hamilton et al. (2016a), which has four steps:

1. Define a graph: Given word embeddings, build a weighted lexical graph by connecting each word with its $k$ nearest neighbors (according to cosine similarity). The weights of the edge between words $w _ { i }$ and $w _ { j }$ are set as:

$$
\mathbf { E } _ { i , j } = \operatorname { a r c c o s } \left( - \frac { \mathbf { w _ { i } } ^ { \top } \mathbf { w _ { j } } } { \| \mathbf { w _ { i } } \| \| \mathbf { w _ { j } } \| } \right) .
$$

2. Define a seed set: Choose positive and negative seed words.

3. Propagate polarities from the seed set: Now we perform a random walk on this graph, starting at the seed set. In a random walk, we start at a node and

then choose a node to move to with probability proportional to the edge probability. A word’s polarity score for a seed set is proportional to the probability of a random walk from the seed set landing on that word (Fig. 22.7).

4. Create word scores: We walk from both positive and negative seed sets, resulting in positive (rawscore $^ + ( w _ { i } ) )$ ) and negative (rawscore $^ - \left( w _ { i } \right) .$ ) raw label scores. We then combine these values into a positive-polarity score as:

$$
\operatorname { s c o r e } ^ { + } ( w _ { i } ) = { \frac { \operatorname { r a w s c o r e } ^ { + } ( w _ { i } ) } { \operatorname { r a w s c o r e } ^ { + } ( w _ { i } ) + \operatorname { r a w s c o r e } ^ { - } ( w _ { i } ) } }
$$

It’s often helpful to standardize the scores to have zero mean and unit variance within a corpus.

5. Assign confidence to each score: Because sentiment scores are influenced by the seed set, we’d like to know how much the score of a word would change if a different seed set is used. We can use bootstrap sampling to get confidence regions, by computing the propagation $B$ times over random subsets of the positive and negative seed sets (for example using $B = 5 0$ and choosing 7 of the 10 seed words each time). The standard deviation of the bootstrap sampled polarity scores gives a confidence measure.

![## Image Analysis: 9291a28bca4677a799518a72552dbecae0823cd189735cf92df4737abcbf4a0f.jpg

**Conceptual Understanding:**
This image represents the conceptual intuition behind the SENTPROP algorithm, which is a method for sentiment analysis based on label propagation in a lexical network. Its main purpose is to visually explain how sentiment (positive or negative polarity) is spread from a small set of initially labeled "seed words" to other unlabelled words within a graph structure.

Key ideas being communicated include:
1.  **Seed-based Initialization:** The process starts with a few words whose sentiment is known (e.g., "love" is positive, "hate" is negative).
2.  **Network Representation:** Words and their relationships are modeled as a graph, where sentiment can "flow" along the connections.
3.  **Random Walks as Propagation Mechanism:** Sentiment propagates through the network via simulated random walks, where the frequency of visits to a node influences its eventual sentiment.
4.  **Polarity Assignment:** Based on the aggregated influence from the random walks, words are assigned a sentiment polarity (positive, negative, or neutral), which is visually represented by color coding. Words closer to positive seeds become positive, and those closer to negative seeds become negative, with varying degrees of intensity.

**Content Interpretation:**
The image illustrates the process of the SENTPROP algorithm for sentiment analysis. It shows:

*   **Lexical Network Construction:** A graph depicting words as nodes and their relationships as connecting lines.
*   **Seed Node Initialization:** "love" (green) is a positive seed word, and "hate" (red) is a negative seed word. All other words are initially neutral (gray).
*   **Random Walk Simulation (Diagram a):**
    *   **Positive Propagation:** Thick green arrows from "love" to "adore," "adore" to "appreciate," and "appreciate" to "idolize," and a thinner green arrow from "idolize" to "like," indicate the spread of positive sentiment. The thickness suggests the "frequency of random walk visits" or strength of influence.
    *   **Negative Propagation:** Thick red arrows from "hate" to "dislike," "hate" to "abhor," and "dislike" to "abhor," and a thinner red arrow from "abhor" to "loathe," show the spread of negative sentiment. Arrow thickness again indicates strength of influence.
*   **Polarity Score Assignment (Diagram b):** The final state where words are colored based on acquired sentiment.
    *   **Strong Positive (Green):** "love".
    *   **Positive (Light Green):** "idolize", "adore", "appreciate", "like", showing acquired positive sentiment.
    *   **Strong Negative (Red):** "hate", "loathe", "abhor", "dislike", "despise", indicating strong acquired negative sentiment.
    *   **Weak Negative/Mixed (Light Red/Orange):** "disapprove", suggesting a less intense negative sentiment.
    *   **Neutral (Gray):** "find", "see", "uncover", "notice", indicating no significant sentiment acquired.

The significance is to visually demonstrate how an algorithm can infer sentiment for many words from a few labeled examples by leveraging their relationships in a network, with the color changes in (b) serving as direct evidence for the assigned "polarity scores" and the arrows in (a) showing the "random walks" that lead to these scores.

**Key Insights:**
The main takeaways from this image are:

*   **Sentiment Spreads Through Connections:** The SENTPROP algorithm effectively propagates sentiment labels from known seed words to other related words in a network. For example, "love" (green seed) propagates to "adore," "appreciate," "idolize," and "like" (all turning light green), while "hate" (red seed) propagates to "dislike," "abhor," "loathe," and "despise" (all turning red).
*   **Seed Words are Crucial for Initialization:** The process relies entirely on a few pre-labeled seed words ("love" and "hate" in this case) to initiate the sentiment diffusion. These seed words are shown as bright green and red boxes, respectively, in both (a) and (b), serving as the origins of sentiment.
*   **Proximity and Frequency Influence Sentiment Strength:** Words that are more closely connected to or frequently visited by random walks from a sentiment seed acquire a stronger or clearer sentiment. This is evidenced by the thicker arrows in (a) indicating stronger propagation and the intensity of colors in (b) reflecting the assigned polarity (e.g., bright green/red for seeds, lighter green/orange for influenced words).
*   **Graded Sentiment Assignment:** Sentiment is not always binary. The algorithm can assign nuanced polarities, as shown by "disapprove" turning light red/orange, indicating a weaker negative sentiment compared to the strongly red words like "loathe" or "despise."
*   **Neutrality for Unconnected/Uninfluenced Words:** Words not sufficiently influenced by either positive or negative random walks remain neutral. "find," "see," "uncover," and "notice" stay gray in (b), demonstrating that the algorithm intelligently distinguishes between sentiment-laden and neutral terms.

**Document Context:**
This image is directly relevant to Section 22.4.2, titled "Label Propagation," as it visually illustrates the core mechanism of the SENTPROP algorithm, which is a specific type of label propagation applied to sentiment analysis. It serves as a concrete example to explain the abstract concept of propagating labels (sentiments in this case) across a graph structure. The accompanying text "Figure 22.7 Intuition of the SENTPROP algorithm. (a) Run random walks from the seed words. (b) Assign polarity scores (shown here as colors green or red) based on the frequency of random walk visits." further clarifies its role as an explanatory figure for the algorithm.

**Summary:**
This figure, "Intuition of the SENTPROP algorithm," visually demonstrates how sentiment labels are propagated across a network of words. It is divided into two parts: (a) showing the process of random walks, and (b) illustrating the resulting sentiment assignments.

**Part (a): Running Random Walks from Seed Words**
Initially, the network contains a set of words represented by rectangular boxes. Two "seed" words are explicitly labeled with sentiment: "love" (colored bright green) indicating positive sentiment, and "hate" (colored bright red) indicating negative sentiment. All other words, such as "idolize," "adore," "appreciate," "like," "find," "see," "uncover," "notice," "loathe," "abhor," "dislike," "disapprove," and "despise," are initially neutral, depicted in gray boxes.

The diagram shows sentiment propagating from these seed words through "random walks," visually represented by colored arrows.
*   **Positive Sentiment Propagation:** From the "love" seed, thick green arrows show a strong flow of positive sentiment. It moves from "love" to "adore," then from "adore" to "appreciate." From "appreciate," a strong green arrow points towards "idolize," suggesting a cyclical or reinforcing path. A thinner green arrow from "idolize" points to "like," indicating a less intense or more distant propagation.
*   **Negative Sentiment Propagation:** Similarly, from the "hate" seed, thick red arrows illustrate a strong flow of negative sentiment. These arrows move from "hate" to "dislike" and from "hate" to "abhor." An additional thick red arrow shows sentiment moving from "dislike" to "abhor," reinforcing the negative influence. A thinner red arrow from "abhor" points to "loathe," indicating further negative spread.
The thickness of these arrows signifies the "frequency of random walk visits" – thicker arrows imply more frequent or stronger connections and thus greater influence.

**Part (b): Assigning Polarity Scores**
Following the random walks, part (b) displays the outcome where each word in the network has been assigned a polarity score, visualized through its box color.
*   **Strong Positive:** "love" remains bright green.
*   **Positive:** Words like "idolize," "adore," "appreciate," and "like" have turned light green, indicating they have acquired a positive sentiment.
*   **Strong Negative:** "hate" remains bright red. Words like "loathe," "abhor," "dislike," and "despise" have turned red, signifying a strong negative sentiment.
*   **Weak Negative/Mixed:** The word "disapprove" is colored light red or orange, suggesting a weaker negative sentiment or a mixed influence, possibly due to fewer negative random walk visits compared to the strongly red words.
*   **Neutral:** Words such as "find," "see," "uncover," and "notice" remain in gray boxes, indicating they did not receive sufficient influence from either positive or negative walks to be assigned a specific sentiment polarity, thus retaining their neutral status.

In summary, the image effectively demonstrates how the SENTPROP algorithm leverages seed words and a word network to propagate sentiment, ultimately assigning polarity scores to a broader vocabulary based on the strength and direction of these propagation paths.](images/9291a28bca4677a799518a72552dbecae0823cd189735cf92df4737abcbf4a0f.jpg)
Figure 22.7 Intuition of the SENTPROP algorithm. (a) Run random walks from the seed words. (b) Assign polarity scores (shown here as colors green or red) based on the frequency of random walk visits.

# 22.4.3 Other Methods

The core of semisupervised algorithms is the metric for measuring similarity with the seed words. The Turney and Littman (2003) and Hamilton et al. (2016a) approaches above used embedding cosine as the distance metric: words were labeled as positive basically if their embeddings had high cosines with positive seeds and low cosines with negative seeds. Other methods have chosen other kinds of distance metrics besides embedding cosine.

For example the Hatzivassiloglou and McKeown (1997) algorithm uses syntactic cues; two adjectives are considered similar if they were frequently conjoined by and and rarely conjoined by but. This is based on the intuition that adjectives conjoined by the words and tend to have the same polarity; positive adjectives are generally coordinated with positive, negative with negative:

fair and legitimate, corrupt and brutal but less often positive adjectives coordinated with negative:

\*fair and brutal, \*corrupt and legitimate By contrast, adjectives conjoined by but are likely to be of opposite polarity:

fair but brutal

Another cue to opposite polarity comes from morphological negation (un-, im-, -less). Adjectives with the same root but differing in a morphological negative (adequate/inadequate, thoughtful/thoughtless) tend to be of opposite polarity.

Yet another method for finding words that have a similar polarity to seed words is to make use of a thesaurus like WordNet (Kim and Hovy 2004, Hu and Liu 2004b). A word’s synonyms presumably share its polarity while a word’s antonyms probably have the opposite polarity. After a seed lexicon is built, each lexicon is updated as follows, possibly iterated.

$\operatorname { L e x } ^ { + }$ : Add synonyms of positive words (well) and antonyms (like fine) of negative words   
Lex−: Add synonyms of negative words (awful) and antonyms (like evil) of positive words

An extension of this algorithm assigns polarity to WordNet senses, called SentiWordNet (Baccianella et al., 2010). Fig. 22.8 shows some examples.

# SentiWordNet

<table><tr><td>Synset</td><td>Pos</td><td>Neg</td><td>Obj</td></tr><tr><td>good#6 ‘agreeable or pleasing&#x27;</td><td>1</td><td>0</td><td>0</td></tr><tr><td>respectable#2 honorable#4 good#4 estimable#2‘deserving of esteem&#x27;</td><td>0.75</td><td>0</td><td>0.25</td></tr><tr><td>estimable#3 computable#1 ‘may be computed or estimated&#x27;</td><td>0</td><td>0</td><td>1</td></tr><tr><td>sting#1 burn#4 bite#2 ‘cause a sharp or stinging pain&#x27;</td><td>0</td><td>0.875 .125</td><td></td></tr><tr><td>acute#6 &quot;of critical importance and consequence&#x27;</td><td></td><td>0.625 0.125 .250</td><td></td></tr><tr><td>acute#4 ‘of an angle; less than 90 degrees&#x27;</td><td>0</td><td>0</td><td>1</td></tr><tr><td>acute#1 ‘having or experiencing a rapid onset and short but severe course’</td><td>0</td><td>0.5</td><td>0.5</td></tr></table>

Figure 22.8 Examples from SentiWordNet 3.0 (Baccianella et al., 2010). Note the differences between senses of homonymous words: estimable#3 is purely objective, while estimable#2 is positive; acute can be positive (acute#6), negative (acute#1), or neutral (acute #4).

In this algorithm, polarity is assigned to entire synsets rather than words. A positive lexicon is built from all the synsets associated with 7 positive words, and a negative lexicon from synsets associated with 7 negative words. A classifier is then trained from this data to take a WordNet gloss and decide if the sense being defined is positive, negative or neutral. A further step (involving a random-walk algorithm) assigns a score to each WordNet synset for its degree of positivity, negativity, and neutrality.

In summary, semisupervised algorithms use a human-defined set of seed words for the two poles of a dimension, and use similarity metrics like embedding cosine, coordination, morphology, or thesaurus structure to score words by how similar they are to the positive seeds and how dissimilar to the negative seeds.

# 22.5 Supervised Learning of Word Sentiment

Semi-supervised methods require only minimal human supervision (in the form of seed sets). But sometimes a supervision signal exists in the world and can be made use of. One such signal is the scores associated with online reviews.

The web contains an enormous number of online reviews for restaurants, movies, books, or other products, each of which have the text of the review along with an associated review score: a value that may range from 1 star to 5 stars, or scoring 1 to 10. Fig. 22.9 shows samples extracted from restaurant, book, and movie reviews.

<table><tr><td></td><td>Movie review excerpts (IMDb) 10 A great movie. This film is just a wonderful experience. It&#x27;s surreal, zany, witty and slapstick</td></tr><tr><td>1</td><td>all at the same time. And terrific performances too. This was probably the worst movie I have ever seen. The story went nowhere even though they could have done some interesting stuff with it.</td></tr><tr><td>5</td><td>Restaurant review excerpts (Yelp) The service was impeccable. The food was cooked and seasoned perfectly.. The watermelon</td></tr><tr><td>2</td><td>was perfectly square ... The grilled octopus was... mouthwatering... ...it tok a while to get our waters, we got our entree before our starter, and we never received silverware or napkins until we requested them... Book review excerpts (GoodReads)</td></tr><tr><td>1</td><td>I am going to try and stop being deceived by eye-catching titles. I so wanted to like this book and was so disappointed by it. This book is hilarious. I would recommend it to anyone looking for a satirical read with a</td></tr><tr><td>5</td><td>romantic twist and a narrator that keeps butting in Product review excerpts (Amazon)</td></tr><tr><td>5 1</td><td>The lid on this blender though is probably what I like the best about it.. enables you to pour into something without even taking the lid off! .. the perfect pitcher! ... works fantastic. I hate this blender.. It is nearly impossible to get frozen fruit and ice to turn into a smoothie.. You have to add a TON of liquid. I also wish it had a spout ...</td></tr></table>

Figure 22.9 Excerpts from some reviews from various review websites, all on a scale of 1 to 5 stars except IMDb, which is on a scale of 1 to 10 stars.

We can use this review score as supervision: positive words are more likely to appear in 5-star reviews; negative words in 1-star reviews. And instead of just a binary polarity, this kind of supervision allows us to assign a word a more complex representation of its polarity: its distribution over stars (or other scores).

Thus in a ten-star system we could represent the sentiment of each word as a 10-tuple, each number a score representing the word’s association with that polarity level. This association can be a raw count, or a likelihood $P ( w | c )$ , or some other function of the count, for each class $c$ from 1 to 10.

For example, we could compute the IMDb likelihood of a word like disappoint(ed/ing) occurring in a 1 star review by dividing the number of times disappoint(ed/ing) occurs in 1-star reviews in the IMDb dataset (8,557) by the total number of words occurring in 1-star reviews (25,395,214), so the IMDb estimate of P(disappointing|1) is .0003.

A slight modification of this weighting, the normalized likelihood, can be used as an illuminating visualization (Potts, 2011)1

$$
\begin{array} { c } { P ( w | c ) ~ = ~ \frac { c o u n t ( w , c ) } { \sum _ { w \in C } c o u n t ( w , c ) } } \\ { P o t t s S c o r e ( w ) ~ = ~ \frac { P ( w | c ) } { \sum _ { c } P ( w | c ) } } \end{array}
$$

Dividing the IMDb estimate $P ( d i s a p p o i n t i n g | 1 )$ of .0003 by the sum of the likelihood $P ( w | c )$ over all categories gives a Potts score of 0.10. The word disappointing thus is associated with the vector [.10, .12, .14, .14, .13, .11, .08, .06, .06, .05]. The

# Potts diagram

Potts diagram (Potts, 2011) is a visualization of these word scores, representing the prior sentiment of a word as a distribution over the rating categories.

Fig. 22.10 shows the Potts diagrams for 3 positive and 3 negative scalar adjectives. Note that the curve for strongly positive scalars have the shape of the letter J, while strongly negative scalars look like a reverse J. By contrast, weakly positive and negative scalars have a hump-shape, with the maximum either below the mean (weakly negative words like disappointing) or above the mean (weakly positive words like good). These shapes offer an illuminating typology of affective meaning.

![## Image Analysis: c41c8ae5f04bf907f60a65b6cc74ba083a65524249bed2b7175836ef9f3c890e.jpg

**Conceptual Understanding:**
This image conceptually represents 'Potts diagrams' for various scalar adjectives, categorizing them as either positive or negative. It illustrates how the perceived intensity or applicability of these adjectives distributes across a numerical rating scale (1-10). The main purpose of the image is to visually demonstrate the characteristic 'shapes' (specifically, the J-shape, reverse J-shape, and hump-shape as mentioned in the document context) that emerge from these distributions, linking these shapes to the adjective's polarity (positive/negative) and its degree of polarization (strong/weak). The key ideas being communicated are that the semantic properties of scalar adjectives have quantifiable and predictable distributional patterns when mapped onto a rating continuum, and that these patterns systematically vary based on the adjective's specific meaning and strength. It shows how empirical data can reflect subtle linguistic distinctions.

**Content Interpretation:**
The image displays a visualization of scalar adjective semantics, showing the relationship between an adjective's polarity (positive or negative) and its perceived strength, and its statistical distribution across a 10-point rating scale. The 'system' is essentially a mapping from specific linguistic items (adjectives) to numerical data distributions. The data points (black dots) and fitted curves (solid red or blue, and dashed lines) illustrate how the likelihood or frequency of using a particular adjective changes across different ratings.

Specifically:
*   **Positive Scalars Column:**
    *   **'good'**: Exhibits a 'hump-shape' distribution (red curve), peaking around the middle ratings (5-6) with y-axis values reaching up to approximately 0.12. This indicates 'good' is a weakly polarized positive adjective, applicable across a moderate range of positive evaluations.
    *   **'great'**: Shows a positive 'J-shape' distribution (blue curve), where the likelihood of use increases with higher ratings (from ~0.05 at rating 1 to ~0.17 at rating 10). This identifies 'great' as a strongly positive adjective, primarily associated with higher ratings.
    *   **'excellent'**: Also displays a positive 'J-shape' distribution (blue curve), but with a steeper increase than 'great', rising from ~0.03 at rating 1 to ~0.2 at rating 10. This suggests 'excellent' is an even stronger positive adjective, reserved for the highest ratings.

*   **Negative Scalars Column:**
    *   **'disappointing'**: Shows a 'hump-shape' distribution (red curve), peaking around ratings 4-6 with y-axis values up to ~0.17. Similar to 'good', this suggests 'disappointing' is a weakly polarized negative adjective, applicable to a moderate range of negative evaluations.
    *   **'bad'**: Exhibits a 'reverse J-shape' distribution (blue curve), where the likelihood of use decreases as ratings increase (from ~0.2 at rating 1 to ~0.04 at rating 10). This indicates 'bad' is a strongly negative adjective, strongly associated with lower ratings.
    *   **'terrible'**: Also displays a 'reverse J-shape' distribution (red curve), but with a steeper decline than 'bad', starting at ~0.28 at rating 1 and dropping to ~0.03 at rating 10. This signifies 'terrible' as an even stronger negative adjective, predominantly used for the lowest ratings.

All extracted text elements, including the column titles ('Positive scalars', 'Negative scalars'), subplot titles ('good', 'great', 'excellent', 'disappointing', 'bad', 'terrible'), x-axis label ('rating' with values 1-10), and y-axis numerical labels (e.g., '0.12-', '0.17', '0.09', '0.05', '0.02', '0.2', '0.04', '0.28', '0.14', '0.08', '0.03'), provide the quantitative and qualitative evidence for these interpretations of the adjectives' semantic distributions.

**Key Insights:**
The main takeaways and lessons from this image, supported by the textual evidence, are:

1.  **Scalar adjectives have distinct and characteristic distributional patterns:** The plots unequivocally demonstrate that adjectives are not uniformly distributed across a rating scale (1-10) but exhibit specific tendencies. For example, 'excellent' is heavily skewed towards higher ratings, while 'terrible' is skewed towards lower ratings.
2.  **Polarity and Strength determine the distribution shape:** The image provides strong evidence that both the positive/negative polarity and the inherent strength (or polarization) of an adjective directly correlate with the shape of its 'Potts diagram' distribution.
    *   **'Hump-shapes'**: Adjectives like 'good' and 'disappointing' show a 'hump-shape' distribution, peaking in the middle ratings (e.g., 'good' peaking around 5-6, 'disappointing' peaking around 4-6), and declining towards the extremes. This pattern, as supported by the document's context, signifies adjectives that are 'more weakly polarized'.
    *   **'J-shapes'**: Strongly positive adjectives such as 'great' and 'excellent' exhibit an upward 'J-shape'. The text states these are for 'strongly positive adjectives', which is visually confirmed by the curves steadily increasing with higher ratings, indicating they are predominantly used for very positive evaluations. 'excellent' shows a steeper J-shape than 'great', suggesting an even stronger positive polarization.
    *   **'Reverse J-shapes'**: Strongly negative adjectives like 'bad' and 'terrible' display a downward 'reverse J-shape'. The context confirms these are for 'strongly negative adjectives', and the graphs show a clear decline in likelihood as ratings increase, meaning they are primarily associated with very low ratings. 'terrible' exhibits a steeper reverse J-shape than 'bad', indicating an even stronger negative polarization.

These insights demonstrate how linguistic properties translate into quantifiable and visually distinct patterns of usage on a continuous rating scale, providing empirical support for theories about scalar adjective semantics.

**Document Context:**
This image directly serves to visually exemplify and clarify the concept of "Potts diagrams" as introduced in the "Potts diagram" section of the document. It provides concrete instances of the "J-shape," "reverse J-shape," and "hump-shape" patterns, associating them with "strongly positive," "strongly negative," and "more weakly polarized adjectives," respectively, as stated in the text after the image: "Figure 22.10 Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, showing the J-shape and reverse J-shape for strongly positive and negative adjectives, and the hump-shape for more weakly polarized adjectives." This visual evidence supports the theoretical framework or linguistic analysis being presented, allowing readers to see the empirical manifestation of these semantic distinctions and understand how different scalar adjectives behave on a rating scale.

**Summary:**
This image displays six "Potts diagrams," arranged in two vertical columns, illustrating the distribution of various scalar adjectives across a 1-10 rating scale. The left column, titled "Positive scalars," features diagrams for the adjectives "good," "great," and "excellent." The right column, titled "Negative scalars," presents diagrams for "disappointing," "bad," and "terrible."

Each of the six individual graphs shares a common horizontal axis labeled "rating," ranging from 1 to 10. The vertical axes have numerical labels (e.g., "0.12-", "0.17", "0.09", "0.05", "0.02", "0.2", "0.04", "0.28", "0.14", "0.08", "0.03"), representing an implied frequency or likelihood associated with each rating. Black dots mark the individual data points, while a solid colored line (either red or blue) and a dashed line represent the fitted curves modeling these distributions.

For the "Positive scalars":
*   The diagram for "good" shows a red "hump-shape" curve, peaking roughly in the middle of the rating scale (around 5-6) and declining towards both ends, with a peak y-value around 0.12. This indicates "good" is a more weakly polarized positive adjective, applicable across a moderate range of positive evaluations.
*   The diagram for "great" displays a blue "J-shape" curve, steadily increasing from a y-value of approximately 0.05 at rating 1 to 0.17 at rating 10. This signifies "great" as a strongly positive adjective, predominantly used for higher ratings.
*   The diagram for "excellent" also exhibits a blue "J-shape" curve, but with an even steeper upward slope, rising from about 0.03 at rating 1 to 0.2 at rating 10. This suggests "excellent" is an even stronger positive adjective, almost exclusively associated with the highest ratings.

For the "Negative scalars":
*   The diagram for "disappointing" features a red "hump-shape" curve, peaking around ratings 4-6 and declining towards the extreme low and high ratings, with a peak y-value around 0.17. Similar to "good," this indicates "disappointing" is a more weakly polarized negative adjective, applying to a moderate range of negative evaluations.
*   The diagram for "bad" shows a blue "reverse J-shape" curve, starting at a higher y-value around 0.2 at rating 1 and decreasing to 0.04 at rating 10. This indicates "bad" is a strongly negative adjective, strongly associated with lower ratings.
*   The diagram for "terrible" also displays a red "reverse J-shape" curve, with an even steeper downward trend, beginning at approximately 0.28 at rating 1 and decreasing sharply to 0.03 at rating 10. This signifies "terrible" as an even stronger negative adjective, predominantly reserved for the lowest ratings.

In summary, these "Potts diagrams" visually demonstrate how the semantic properties of scalar adjectives, specifically their polarity (positive/negative) and strength, are reflected in their characteristic distributional shapes across a rating continuum. Weaker adjectives ("good," "disappointing") exhibit a "hump-shape," while stronger ones ("great," "excellent" for positive; "bad," "terrible" for negative) show distinct "J-shape" or "reverse J-shape" distributions, respectively.](images/c41c8ae5f04bf907f60a65b6cc74ba083a65524249bed2b7175836ef9f3c890e.jpg)
Figure 22.10 Potts diagrams (Potts, 2011) for positive and negative scalar adjectives, showing the J-shape and reverse J-shape for strongly positive and negative adjectives, and the hump-shape for more weakly polarized adjectives.

Fig. 22.11 shows the Potts diagrams for emphasizing and attenuating adverbs. Note that emphatics tend to have a J-shape (most likely to occur in the most positive reviews) or a U-shape (most likely to occur in the strongly positive and negative). Attenuators all have the hump-shape, emphasizing the middle of the scale and downplaying both extremes. The diagrams can be used both as a typology of lexical sentiment, and also play a role in modeling sentiment compositionality.

In addition to functions like posterior $P ( c | w )$ , likelihood $P ( w | c )$ , or normalized likelihood (Eq. 22.6) many other functions of the count of a word occurring with a sentiment label have been used. We’ll introduce some of these on page 496, including ideas like normalizing the counts per writer in Eq. 22.14.

# 22.5.1 Log Odds Ratio Informative Dirichlet Prior

One thing we often want to do with word polarity is to distinguish between words that are more likely to be used in one category of texts than in another. We may, for example, want to know the words most associated with 1 star reviews versus those associated with 5 star reviews. These differences may not be just related to sentiment. We might want to find words used more often by Democratic than Republican members of Congress, or words used more often in menus of expensive restaurants

![## Image Analysis: a6bf4a10b049a59debe3badfd6de2e760016d78ab8ca7ff8cce3743e3ca57c0c.jpg

**Conceptual Understanding:**
The image conceptually illustrates the empirical distributions of how different adverbs are used or perceived across a 1-10 rating scale. It clearly delineates two categories of adverbs: 'Emphatics' and 'Attenuators,' highlighting a fundamental difference in their semantic function. The main purpose of these 'Potts diagrams' is to provide a visual representation of these distinct usage patterns, likely to inform the construction of an 'informative Dirichlet prior' in a statistical model, as mentioned in the accompanying document section. The key idea communicated is that emphatic adverbs (e.g., 'totally') are strongly associated with the extreme ends of a scale, while attenuating adverbs (e.g., 'somewhat') are primarily associated with the middle of the scale. This demonstrates a clear quantitative distinction in their semantic impact.

**Content Interpretation:**
The image displays six individual line plots, referred to as Potts diagrams, illustrating the distributional patterns of different adverbs across a 1-10 rating scale. These distributions are categorized into two groups: 'Emphatics' and 'Attenuators'. Each plot features observed data points (black dots) and two fitted curves (solid red and dashed black lines), which likely represent model estimates or comparisons, given the context of statistical priors. The 'Emphatics' adverbs ('totally', 'absolutely', 'utterly') consistently show U-shaped distributions, indicating a higher probability of use with extreme ratings (1 or 10). For example, 'totally' shows peaks around 0.14 at ratings 1 and 10, dipping to around 0.07 in the middle. Conversely, the 'Attenuators' adverbs ('somewhat', 'fairly', 'pretty') exhibit bell-shaped (inverse U) distributions, with higher probabilities in the middle ratings and lower probabilities at the extremes. For instance, 'somewhat' peaks around 0.17 in the middle ratings and drops to around 0.04 at the ends. The 'somewhat' plot also includes an additional 'Category' X-axis ranging from -0.50 to 0.50, which could represent an underlying log odds ratio scale. These plots collectively demonstrate the distinct semantic profiles of these two adverb categories based on their empirical usage distributions.

**Key Insights:**
The key insight from these diagrams is that emphatic and attenuating adverbs possess clearly distinguishable statistical usage patterns along a rating scale. Emphatic adverbs ('totally', 'absolutely', 'utterly') are predominantly used to express extreme sentiments or values, as evidenced by their U-shaped distributions (e.g., 'utterly' having probabilities around 0.18 at extremes and 0.05 in the middle). Attenuating adverbs ('somewhat', 'fairly', 'pretty'), in contrast, are associated with moderate or central values, which is reflected in their bell-shaped distributions (e.g., 'fairly' peaking around 0.17 in the middle and decreasing to 0.04 at the ends). These distinct patterns provide strong empirical evidence for their differing semantic roles in language. The numerical values on the axes (e.g., Y-axis probabilities, X-axis ratings from 1-10) quantify these differences, offering concrete data points for understanding their distributions. The inclusion of two fitted lines suggests that these are model-based representations of these patterns, likely used to derive informative priors for statistical analysis, as indicated by the document context.

**Document Context:**
This image is highly relevant to Section 22.5.1, 'Log Odds Ratio Informative Dirichlet Prior,' as it visually represents the empirical distributions that would inform a statistical prior for modeling the log odds ratios of emphatic and attenuating adverbs. The 'Potts diagrams' illustrate the observed usage patterns for these adverbs, demonstrating how 'Emphatics' tend to be associated with extreme values and 'Attenuators' with moderate values. This empirical evidence is crucial for constructing an 'informative Dirichlet prior,' which helps guide a statistical model to converge on more realistic and semantically meaningful results by incorporating prior knowledge about adverb usage. The distinct shapes of the distributions directly provide the information needed to define such a prior for different adverb categories, enhancing the model's ability to differentiate their linguistic functions.

**Summary:**
This image presents six Potts diagrams, organized into two main categories: "Emphatics" on the left side and "Attenuators" on the right side. Each category contains three individual line plots, each representing a specific adverb and its distribution across a 1-10 rating scale. The plots show black data points, a solid red line, and a dashed black line, likely representing observed data and fitted statistical curves.

Under the "Emphatics" category:
1. The top-left plot is for the adverb "totally." Its Y-axis shows values 0.14 and 0.07. The X-axis is labeled "rating" and ranges from 1 to 10. The curve is U-shaped, indicating higher probabilities at extreme ratings (1 and 10) and lower probabilities in the middle.
2. The middle-left plot is for the adverb "absolutely." Its Y-axis shows values 0.16 and 0.06. The X-axis is labeled "rating" and ranges from 1 to 10. This curve also displays a U-shaped distribution, similar to "totally."
3. The bottom-left plot is for the adverb "utterly." Its Y-axis shows values 0.18, 0.1, and 0.05. The X-axis is labeled "rating" and ranges from 1 to 10. This plot also shows a U-shaped distribution, reinforcing the pattern for emphatic adverbs.

Under the "Attenuators" category:
1. The top-right plot is for the adverb "somewhat." It has two Y-axis scales: an upper one with values 0.17, 0.09, 0.04, and a lower one with values 0.15, 0.09, 0.05. It also has two X-axis labels: "rating" from 1 to 10, and an additional "Category" axis with values -0.50, -0.39, -0.28, -0.17, -0.06, 0.06, 0.17, 0.28, 0.39, 0.50. This curve is bell-shaped (inverse U), showing higher probabilities in the middle ratings and lower probabilities at the extremes.
2. The middle-right plot is for the adverb "fairly." Its Y-axis shows values 0.17, 0.09, and 0.04. The X-axis is labeled "rating" and ranges from 1 to 10. This curve also displays a bell-shaped distribution, peaking in the middle ratings.
3. The bottom-right plot is for the adverb "pretty." Its Y-axis shows values 0.13, 0.09, and 0.05. The X-axis is labeled "rating" and ranges from 1 to 10. This plot similarly shows a bell-shaped distribution, consistent with attenuating adverbs.

In essence, the diagrams illustrate how emphatic adverbs are associated with extreme ends of a rating scale, while attenuating adverbs are associated with the middle or moderate points of the scale. The specific numerical labels on the axes and the adverb names provide precise details for understanding these distinct patterns.](images/a6bf4a10b049a59debe3badfd6de2e760016d78ab8ca7ff8cce3743e3ca57c0c.jpg)
Figure 22.11 Potts diagrams (Potts, 2011) for emphatic and attenuating adverbs.

than cheap restaurants.

Given two classes of documents, to find words more associated with one category than another, we could measure the difference in frequencies (is a word $w$ more frequent in class $A$ or class $B ?$ ). Or instead of the difference in frequencies we could compute the ratio of frequencies, or compute the log odds ratio (the log of the ratio between the odds of the two words). We could then sort words by whichever association measure we pick, ranging from words overrepresented in category $A$ to words overrepresented in category $B$ .

The problem with simple log-likelihood or log odds methods is that they overemphasize differences in very rare words, and often also in very frequent words. Very rare words will seem to occur very differently in the two corpora since with tiny counts there may be statistical fluctations, or even zero occurrences in one corpus compared to non-zero occurrences in the other. Very frequent words will also seem different since all counts are large.

In this section we walk through the details of one solution to this problem: the “log odds ratio informative Dirichlet prior” method of Monroe et al. (2008) that is a particularly useful method for finding words that are statistically overrepresented in one particular category of texts compared to another. It’s based on the idea of using another large corpus to get a prior estimate of what we expect the frequency of each word to be.

Let’s start with the goal: assume we want to know whether the word horrible occurs more in corpus $i$ or corpus $j$ . We could compute the log likelihood ratio, using $f ^ { i } ( w )$ to mean the frequency of word $w$ in corpus $i$ , and $n ^ { i }$ to mean the total number of words in corpus $i$ :

$$
\begin{array} { r l } { { 1 } \mathrm { { l r } } ( h o r r i b l e ) ~ = ~ \log \frac { P ^ { i } ( h o r r i b l e ) } { P ^ { j } ( h o r r i b l e ) } ~ } & { } \\ { ~ = ~ \log P ^ { i } ( h o r r i b l e ) - \log P ^ { j } ( h o r r i b l e ) } & { } \\ { ~ = ~ \log \frac { \mathrm { { f ^ { i } } } ( h o r r i b l e ) } { n ^ { i } } - \log \frac { \mathrm { { f ^ { j } } } ( h o r r i b l e ) } { n ^ { j } } } \end{array}
$$

Instead, let’s compute the log odds ratio: does horrible have higher odds in $i$ or in

$$
{ \begin{array} { r l } { \log ( h o r r i b l e ) } & { = \log \left( { \frac { P ^ { i } \left( h o r r i b l e \right) } { 1 - P ^ { i } \left( h o r r i b l e \right) } } \right) - \log \left( { \frac { P ^ { j } \left( h o r r i b l e \right) } { 1 - P ^ { j } \left( h o r r i b l e \right) } } \right) } \\ & { = \ \log \left( { \frac { \frac { \mathbf { f } ^ { i } \left( h o r r i b l e \right) } { n ^ { i } } } { 1 - { \frac { \mathbf { f } ^ { i } \left( h o r r i b l e \right) } { n ^ { i } } } } } \right) - \log \left( { \frac { \frac { \mathbf { f } ^ { j } \left( h o r r i b l e \right) } { n ^ { j } } } { 1 - { \frac { \mathbf { f } ^ { j } \left( h o r r i b l e \right) } { n ^ { j } } } } } \right) } \\ & { = \ \log \left( { \frac { \mathbf { f } ^ { i } \left( h o r r i b l e \right) } { n ^ { i } - { \mathbf { f } ^ { i } \left( h o r r i b l e \right) } } } \right) - \log \left( { \frac { \mathbf { f } ^ { j } \left( h o r r i b l e \right) } { n ^ { j } - \mathbf { f } ^ { j } \left( h o r r i b l e \right) } } \right) } \end{array} }
$$

The Dirichlet intuition is to use a large background corpus to get a prior estimate of what we expect the frequency of each word $w$ to be. We’ll do this very simply by adding the counts from that corpus to the numerator and denominator, so that we’re essentially shrinking the counts toward that prior. It’s like asking how large are the differences between $i$ and $j$ given what we would expect given their frequencies in a well-estimated large background corpus.

The method estimates the difference between the frequency of word $w$ in two corpora $i$ and $j$ via the prior-modified log odds ratio for $w$ , $\delta _ { w } ^ { ( i - j ) }$ , which is estimated as:

$$
\delta _ { w } ^ { ( i - j ) } = \log \left( \frac { f _ { w } ^ { i } + \alpha _ { w } } { n ^ { i } + \alpha _ { 0 } - \left( f _ { w } ^ { i } + \alpha _ { w } \right) } \right) - \log \left( \frac { f _ { w } ^ { j } + \alpha _ { w } } { n ^ { j } + \alpha _ { 0 } - \left( f _ { w } ^ { j } + \alpha _ { w } \right) } \right)
$$

(where $n ^ { i }$ is the size of corpus $i$ , $n ^ { j }$ is the size of corpus $j , f _ { w } ^ { i }$ is the count of word $w$ in corpus $i$ , $f _ { w } ^ { j }$ is the count of word $w$ in corpus $j , \alpha _ { 0 }$ is the scaled size of the background corpus, and $\alpha _ { w }$ is the scaled count of word $w$ in the background corpus.)

In addition, Monroe et al. (2008) make use of an estimate for the variance of the log–odds–ratio:

$$
\sigma ^ { 2 } \left( \hat { \delta } _ { w } ^ { ( i - j ) } \right) \approx \frac { 1 } { f _ { w } ^ { i } + \alpha _ { w } } + \frac { 1 } { f _ { w } ^ { j } + \alpha _ { w } }
$$

The final statistic for a word is then the $\mathbf { Z }$ –score of its log–odds–ratio:

$$
\frac { \hat { \delta } _ { w } ^ { ( i - j ) } } { \sqrt { \sigma ^ { 2 } \left( \hat { \delta } _ { w } ^ { ( i - j ) } \right) } }
$$

The Monroe et al. (2008) method thus modifies the commonly used log odds ratio in two ways: it uses the $\mathbf { Z }$ -scores of the log odds ratio, which controls for the amount of variance in a word’s frequency, and it uses counts from a background corpus to provide a prior count for words.

Fig. 22.12 shows the method applied to a dataset of restaurant reviews from Yelp, comparing the words used in 1-star reviews to the words used in 5-star reviews (Jurafsky et al., 2014). The largest difference is in obvious sentiment words, with the 1-star reviews using negative sentiment words like worse, bad, awful and the 5-star reviews using positive sentiment words like great, best, amazing. But there are other illuminating differences. 1-star reviews use logical negation (no, not), while 5-star reviews use emphatics and emphasize universality (very, highly, every, always). 1- star reviews use first person plurals (we, us, our) while 5 star reviews use the second person. 1-star reviews talk about people (manager, waiter, customer) while 5-star reviews talk about dessert and properties of expensive restaurants like courses and atmosphere. See Jurafsky et al. (2014) for more details.

<table><tr><td>Class</td><td>Words in 1-star reviews</td><td>Class</td><td>Words in 5-star reviews</td></tr><tr><td>Negative</td><td>worst，rude， terrible，horrible,bad, awful， disgusting， bland, tasteless, gross,mediocre, overpriced, worse,</td><td>Positive</td><td>great, best, love(d), delicious, amazing, favorite, perfect, excellent, awesome, friendly,fantastic,fresh,wonderful,in-</td></tr><tr><td>Negation</td><td>poor no, not</td><td>Emphatics/ universals</td><td>credible, sweet, yum(my) very, highly, perfectly, definitely,abso lutely, everything, every, always</td></tr><tr><td> 1Pl pro</td><td>we, us, our</td><td>2 pro</td><td>you</td></tr><tr><td>3 pro</td><td>she, he,her, him</td><td>Articles</td><td>a, the</td></tr><tr><td> Past verb</td><td>was, were， asked, told, said, did, charged,waited,left,took</td><td>Advice</td><td>try, recommend</td></tr><tr><td>Sequencers after, then</td><td></td><td>Conjunct</td><td>also, as, well, with, and</td></tr><tr><td>Nouns</td><td>manager, waitress, waiter, customer, customers, attitude, waste, poisoning, money, bill,minutes</td><td>Nouns</td><td>atmosphere, dessert, chocolate, wine, course, menu</td></tr><tr><td>Irrealis modals</td><td>would, should</td><td> Auxiliaries</td><td> is/&#x27;s, can,&#x27;ve, are</td></tr><tr><td>Comp</td><td>to, that</td><td>Prep,otherin, of, die,city, mouth</td><td></td></tr></table>

Figure 22.12 The top 50 words associated with one–star and five-star restaurant reviews in a Yelp dataset of 900,000 reviews, using the Monroe et al. (2008) method (Jurafsky et al., 2014).

# 22.6 Using Lexicons for Sentiment Recognition

In Chapter 4 we introduced the naive Bayes algorithm for sentiment analysis. The lexicons we have focused on throughout the chapter so far can be used in a number of ways to improve sentiment detection.

In the simplest case, lexicons can be used when we don’t have sufficient training data to build a supervised sentiment analyzer; it can often be expensive to have a human assign sentiment to each document to train the supervised classifier.

In such situations, lexicons can be used in a rule-based algorithm for classification. The simplest version is just to use the ratio of positive to negative words: if a document has more positive than negative words (using the lexicon to decide the polarity of each word in the document), it is classified as positive. Often a threshold $\lambda$ is used, in which a document is classified as positive only if the ratio is greater than $\lambda$ . If the sentiment lexicon includes positive and negative weights for each word, $\theta _ { w } ^ { + }$ and $\theta _ { w } ^ { - }$ , these can be used as well. Here’s a simple such sentiment algorithm:

$$
\begin{array} { r l r } { f ^ { + } = } & { \displaystyle \sum _ { w \ : s , t \ : w \in p o s i t i v e l e x i c o n } \theta _ { w } ^ { + } c o u n t ( w ) } & \\ { f ^ { - } = } & { \displaystyle \sum _ { w \ : s , t \ : w \in n e g a t i v e l e x i c o n } \theta _ { w } ^ { - } c o u n t ( w ) } \\ { s e n t i m e n t \ } & { = } & { \left\{ \begin{array} { l l } { + } & { \mathrm { i f ~ } \frac { f ^ { + } } { f } > \lambda } \\ { - } & { \mathrm { i f ~ } \frac { f ^ { - } } { f ^ { + } } > \lambda } \\ { 0 } & { \mathrm { o t h e r w i s e } . } \end{array} \right. } \end{array}
$$

If supervised training data is available, these counts computed from sentiment lexicons, sometimes weighted or normalized in various ways, can also be used as features in a classifier along with other lexical or non-lexical features. We return to such algorithms in Section 22.7.

# 22.7 Using Lexicons for Affect Recognition

Detection of emotion (and the other kinds of affective meaning described by Scherer (2000)) can be done by generalizing the algorithms described above for detecting sentiment.

The most common algorithms involve supervised classification: a training set is labeled for the affective meaning to be detected, and a classifier is built using features extracted from the training set. As with sentiment analysis, if the training set is large enough, and the test set is sufficiently similar to the training set, simply using all the words or all the bigrams as features in a powerful classifier like SVM or logistic regression, as described in Fig. 4.2 in Chapter 4, is an excellent algorithm whose performance is hard to beat. Thus we can treat affective meaning classification of a text sample as simple document classification.

Some modifications are nonetheless often necessary for very large datasets. For example, the Schwartz et al. (2013) study of personality, gender, and age using 700 million words of Facebook posts used only a subset of the n-grams of lengths 1- 3. Only words and phrases used by at least $1 \%$ of the subjects were included as features, and 2-grams and 3-grams were only kept if they had sufficiently high PMI (PMI greater than $^ { 2 \ast }$ length, where length is the number of words):

$$
\mathrm { p m i } ( p h r a s e ) = \log \frac { p ( p h r a s e ) } { \prod _ { w \in p h r a s e } p ( w ) }
$$

Various weights can be used for the features, including the raw count in the training set, or some normalized probability or log probability. Schwartz et al. (2013), for example, turn feature counts into phrase likelihoods by normalizing them by each subject’s total word use.

$$
p ( p h r a s e | s u b j e c t ) = \frac { \mathrm { f r e q } ( p h r a s e , s u b j e c t ) } { \displaystyle \sum _ { p h r a s e ^ { \prime } \in \mathrm { v o c a b } ( s u b j e c t ) } \mathrm { f r e q } ( p h r a s e ^ { \prime } , s u b j e c t ) }
$$

If the training data is sparser, or not as similar to the test set, any of the lexicons we’ve discussed can play a helpful role, either alone or in combination with all the words and $\mathbf { n }$ -grams.

Many possible values can be used for lexicon features. The simplest is just an indicator function, in which the value of a feature $f _ { L }$ takes the value 1 if a particular text has any word from the relevant lexicon $L$ . Using the notation of Chapter 4, in which a feature value is defined for a particular output class $c$ and document $x$ .

$$
f _ { L } ( c , x ) ~ = ~ \left\{ \begin{array} { l l } { { 1 \mathrm { i f } \exists w : w \in L \ \& \ w \in x \ \& \ c l a s s = c } } \\ { { 0 \mathrm { o t h e r w i s e } } } \end{array} \right.
$$

Alternatively the value of a feature $f _ { L }$ for a particular lexicon $L$ can be the total number of word tokens in the document that occur in $L$ :

$$
f _ { L } = \sum _ { w \in L } c o u n t ( w )
$$

For lexica in which each word is associated with a score or weight, the count can be multiplied by a weight $\theta _ { w } ^ { L }$ :

$$
f _ { L } = \sum _ { w \in L } \theta _ { w } ^ { L } c o u n t ( w )
$$

Counts can alternatively be logged or normalized per writer as in Eq. 22.14.

However they are defined, these lexicon features are then used in a supervised classifier to predict the desired affective category for the text or document. Once a classifier is trained, we can examine which lexicon features are associated with which classes. For a classifier like logistic regression the feature weight gives an indication of how associated the feature is with the class.

# 22.8 Lexicon-based methods for Entity-Centric Affect

What if we want to get an affect score not for an entire document, but for a particular entity in the text? The entity-centric method of Field and Tsvetkov (2019) combines affect lexicons with contextual embeddings to assign an affect score to an entity in text. In the context of affect about people, they relabel the Valence/Arousal/Dominance dimension as Sentiment/Agency/Power. The algorithm first trains classifiers to map embeddings to scores:

1. For each word $w$ in the training corpus:

(a) Use off-the-shelf pretrained encoders (like BERT) to extract a contextual embedding e for each instance of the word. No additional fine-tuning is done.   
(b) Average over the e embeddings of each instance of $w$ to obtain a single embedding vector for one training point $w$ .   
(c) Use the NRC VAD Lexicon to get S, A, and $\mathrm { \bf P }$ scores for $w$ .

2. Train (three) regression models on all words $w$ to predict V, A, D scores from a word’s average embedding.

Now given an entity mention $m$ in a text, we assign affect scores as follows:

1. Use the same pretrained LM to get contextual embeddings for $m$ in context. 2. Feed this embedding through the 3 regression models to get S, A, $\mathrm { \bf P }$ scores for the entity.

This results in a (S,A,P) tuple for a given entity mention; To get scores for the representation of an entity in a complete document, we can run coreference resolution and average the (S,A,P) scores for all the mentions. Fig. 22.13 shows the scores from their algorithm for characters from the movie The Dark Knight when run on Wikipedia plot summary texts with gold coreference.

# 22.9 Connotation Frames

# connotation frame

The lexicons we’ve described so far define a word as a point in affective space. A connotation frame, by contrast, is a lexicon that incorporates a richer kind of grammatical structure, by combining affective lexicons with the frame semantic lexicons of Chapter 21. The basic insight of connotation frame lexicons is that a predicate like a verb expresses connotations about the verb’s arguments (Rashkin et al. 2016, Rashkin et al. 2017).

Consider sentences like:

(22.15) Country A violated the sovereignty of Country B (22.16) the teenager ... survived the Boston Marathon bombing”

![## Image Analysis: e1a21fb6310e8f68ba42db6de49b148a31d270bb63a7d7117e2156d1a113631d.jpg

**Conceptual Understanding:**
This image conceptually represents a character attribute analysis from the movie "The Dark Knight". It aims to quantify and visualize the perceived 'Power', 'Sentiment', and 'Agency' of key characters. The main purpose is to show, at a glance, the relative strengths and emotional profiles of each character across these three dimensions. It communicates the idea that even seemingly qualitative character traits can be analyzed and mapped onto a quantifiable spectrum, highlighting their narrative roles and impact.

**Content Interpretation:**
The image illustrates a character profiling system where characters from "The Dark Knight" are rated across three distinct dimensions: Power (dominance), Sentiment (emotional valence), and Agency (arousal/impact).

*   **Power Score (dominance):** This scale measures how influential or dominant a character is, ranging from "weakly" to "powerfully".
    *   *Evidence:* "Rachel" is at the "weakly" end, while "Joker" and "Batman" are at the "powerfully" end, indicating they are the most dominant characters. "Dent" and "Gordan" are in the middle, with "Dent" slightly less powerful than "Gordan".
*   **Sentiment Score (valence):** This scale measures the emotional tone associated with a character, from "negative" to "positive".
    *   *Evidence:* "Joker" is firmly on the "negative" side, while "Batman" and "Rachel" are on the "positive" side. "Dent" is more negative than "Gordan", with both in the middle-to-negative range.
*   **Agency Score (arousal):** This scale measures how impactful, stimulating, or "scary" a character is, ranging from "dull" to "scary".
    *   *Evidence:* "Dent" is near "dull", while "Joker" is explicitly labeled "scary" and "Batman" is also high in agency. "Gordan" and "Rachel" are in the middle, with "Gordan" slightly less agentic than "Rachel".

**Significance of Data:**
*   **Protagonist (Batman) and Antagonist (Joker):** Both characters show high "Power Score" and high "Agency Score", indicating significant influence and impact. However, they are starkly differentiated by "Sentiment Score", with "Joker" being negative and "Batman" positive. This highlights their opposing moral alignments despite similar levels of influence and impactful presence.
*   **Love Interest (Rachel):** Has low "Power Score" and low "Agency Score" (compared to Batman and Joker), but a high "Sentiment Score". This positions her as less dominant and impactful, but associated with positive emotions, fitting a typical romantic interest role.
*   **Supporting Characters (Dent, Gordan):** Generally fall in the middle to lower ranges for Power and Agency, and closer to neutral/slightly positive for Sentiment, reflecting their more grounded roles compared to the main hero/villain.

**Key Insights:**
The main takeaways and insights from this image are:

1.  **Opposing Extremes in Sentiment Define Hero/Villain:** The Joker and Batman, while both powerful and agentic, occupy opposite ends of the sentiment spectrum, clearly defining their hero/villain dichotomy.
    *   *Evidence:* "Joker" is on the "negative" side of the "Sentiment Score" scale, while "Batman" is on the "positive" side.
2.  **Shared High Power and Agency for Central Figures:** The primary hero and villain (Batman and Joker) are depicted as having the highest levels of both power and agency within the character set.
    *   *Evidence:* "Batman" and "Joker" are positioned furthest to the right (high end) on both the "Power Score" ("powerfully") and "Agency Score" ("scary") scales.
3.  **Rachel's Role as Positive but Less Dominant:** Rachel's character is consistently associated with positive sentiment but comparatively less power and agency than the main hero/villain.
    *   *Evidence:* "Rachel" is on the "positive" side of "Sentiment Score" but towards the "weakly" end of "Power Score" and closer to "dull" on "Agency Score" than Batman or Joker.
4.  **Supporting Characters as Moderate:** Supporting characters like Dent and Gordan generally show moderate levels across all three attributes, not reaching the extremes of the main hero and villain, which aligns with their more grounded narrative roles.
    *   *Evidence:* "Dent" and "Gordan" are positioned in the middle ranges for all three scores.

**Document Context:**
This image directly supports the "connotation frame" section of the document, providing a visual representation of character analysis discussed in the surrounding text. It graphically illustrates the computed power (dominance), sentiment (valence), and agency (arousal) scores for characters in "The Dark Knight". The diagram specifically validates the findings mentioned in the accompanying text, which states that "the protagonist (Batman) and the antagonist (the Joker) have high power and agency scores but differ in sentiment, while the love interest Rachel has low power and agency but high sentiment." The visual data reinforces the analytical insights derived from embeddings trained on the NRC VAD Lexicon and ELMo embeddings.

**Summary:**
This image presents three separate linear scales, each representing a different character attribute, to visualize the profiles of key characters from the movie "The Dark Knight".

1.  **Power Score (Dominance):** This top scale measures a character's influence, ranging from "weakly" on the left to "powerfully" on the right.
    *   "Rachel" is positioned at the lowest end, indicating low power.
    *   "Dent" and "Gordan" are in the middle-low range, with "Dent" slightly less powerful than "Gordan".
    *   "Batman" and "Joker" are at the highest end, demonstrating significant power, with "Joker" slightly more powerful than "Batman".

2.  **Sentiment Score (Valence):** The middle scale assesses the emotional tone associated with each character, spanning from "negative" on the left to "positive" on the right.
    *   "Joker" is firmly on the "negative" side, indicating his malevolent nature.
    *   "Dent" is in the middle-negative range, followed by "Gordan" in the middle-positive range.
    *   "Rachel" is positioned strongly on the "positive" side, followed closely by "Batman" who is even more "positive".

3.  **Agency Score (Arousal):** The bottom scale reflects a character's impact or ability to arouse strong reactions, ranging from "dull" on the left to "scary" on the right.
    *   "Dent" is near the "dull" end, suggesting low agency.
    *   "Gordan" and "Rachel" are in the middle range, with "Gordan" slightly less agentic than "Rachel".
    *   "Batman" and "Joker" are at the highest end, signifying high agency and impact, with "Joker" being explicitly labeled "scary".

In summary, the diagram effectively visualizes the distinct profiles of the main characters: "Batman" and "Joker" exhibit high power and agency, but "Joker" is strongly negative in sentiment while "Batman" is positive. "Rachel" stands out with high positive sentiment but comparatively lower power and agency. "Dent" and "Gordan" generally occupy the moderate ranges across all three attributes. This provides a clear, data-driven understanding of their narrative roles and emotional impact.](images/e1a21fb6310e8f68ba42db6de49b148a31d270bb63a7d7117e2156d1a113631d.jpg)
Figure 22.13 Figure 2:Power (dominance), sentiment (valence) and agency (arousal) for characters acters inin the movie The Dark Knight computed from embeddings trained on the NRC VAD Lexicon. acters in The Dark Night as learned through the regres- ELMo eNote the protagonist (Batman) and the antagonist (the Joker) have high power and agency sion model with ELMo embeddings. Scores generallyscores but differ in sentiment, while the love interest Rachel has low power and agency but high sentiment.

vey DenBy using the verb violate in (22.15), the author is expressing their sympathies with RachelCountry B, portraying Country B as a victim, and expressing antagonism toward the agent Country A. By contrast, in using the verb survive, the author of (22.16) is movement portray men like Weinstein as unpow-expressing that the bombing is a negative experience, and the subject of the sentence, erful, we can speculate that the corpora used tothe teenager, is a sympathetic character. These aspects of connotation are inherent train ELMo and BERT portray them as powerful.in the meaning of the verbs violate and survive, as shown in Fig. 22.14.

![## Image Analysis: 21c174de8678168c3c5c6050b59ecec33d98b180e69ef632e1a32c615d48d6d9.jpg

**Conceptual Understanding:**
The image conceptually represents 'connotation frames,' which are structured models of the semantic and emotional relationships between key roles (Role1, Role2) and perspectives (Writer, Reader) when a specific action (e.g., 'survives', 'violates') occurs. The main purpose is to illustrate how changing a verb fundamentally alters the sentiment assignments and characterizations of the involved roles, demonstrating the impact of linguistic framing on narrative interpretation. The key ideas communicated are the dynamic nature of role perceptions, the alignment of sentiment between an author and audience, and the existence of underlying structural relationships within these frames that persist across different verbs.

**Content Interpretation:**
The image displays two structured representations, called 'connotation frames,' that map sentiment relationships between abstract roles ('Role1', 'Role2') and external perspectives ('Writer', 'Reader') for two different verbs: 'survives' and 'violates'.

**For 'Role1 survives Role2' (Diagram a):**
*   **Roles:** Role1 is defined as a 'sympathetic victim', and Role2 is associated with 'some type of hardship'.
*   **Sentiment from Writer:** The Writer holds a positive sentiment towards Role1 (labeled 'S(writer→role1) +') and a negative sentiment towards Role2 (labeled 'S(writer→role2) -').
*   **Sentiment from Reader:** The Reader also holds a positive sentiment towards Role1 ('+') and a negative sentiment towards Role2 ('-').
*   **Internal Role Relationship:** There is a negative sentiment from Role1 to Role2 (labeled 'S(role1→role2) -') and a positive sentiment from Role2 to Role1 ('+'). This suggests Role1's survival involves a negative interaction with Role2, but Role2's role in this interaction ultimately leads to a positive outcome or aspect for Role1.

**For 'Role1 violates Role2' (Diagram b):**
*   **Roles:** Role1 is explicitly defined as 'the antagonist', and Role2 is defined as a 'sympathetic victim'.
*   **Sentiment from Writer:** The Writer holds a negative sentiment towards Role1 ('-') and a positive sentiment towards Role2 (labeled 'S(writer→role2) +').
*   **Sentiment from Reader:** The Reader also holds a negative sentiment towards Role1 ('-') and a positive sentiment towards Role2 ('+').
*   **Internal Role Relationship:** The internal relationship is identical to frame (a), with a negative sentiment from Role1 to Role2 (labeled 'S(role1→role2) -') and a positive sentiment from Role2 to Role1 ('+'). This indicates that Role1's violation is a negative action towards Role2, while Role2 maintains a sympathetic position or creates a positive (perhaps through consequence or moral standing) for Role1, despite the violation.

The significance is in demonstrating how the same abstract roles take on distinct moral and emotional valences depending on the action, and how these valences are reflected in consistent sentiment patterns from both the writer and the reader.

**Key Insights:**
The main takeaways from this image are:

1.  **Verb-Dependent Role Connotation:** The perceived nature and sentiment towards abstract roles (Role1 and Role2) are highly contingent on the verb that connects them. As evidenced by:
    *   In frame (a) 'survives', Role1 is explicitly labeled 'Role1 is a sympathetic victim'.
    *   In frame (b) 'violates', Role1 is explicitly labeled 'Role1 is the antagonist'.
    *   Similarly, Role2 is associated with 'There is some type of hardship' in (a) and becomes 'Role2 is a sympathetic victim' in (b).

2.  **Alignment of Writer and Reader Sentiment:** The sentiment of both the 'Writer' and 'Reader' consistently aligns towards Role1 and Role2 within each connotation frame. This suggests a shared, intended, or commonly understood emotional response dictated by the narrative framing. This is supported by:
    *   In frame (a): Writer sentiment S(writer→role1) is '+' and Reader sentiment towards Role1 is '+'. Writer sentiment S(writer→role2) is '-' and Reader sentiment towards Role2 is '-'.
    *   In frame (b): Writer sentiment towards Role1 is '-' and Reader sentiment towards Role1 is '-'. Writer sentiment S(writer→role2) is '+' and Reader sentiment towards Role2 is '+'.

3.  **Core Relational Dynamic:** An underlying core relational dynamic exists between Role1 and Role2, where Role1 consistently exerts a negative influence on Role2 (S(role1→role2) -), while Role2 reciprocates with a positive sentiment towards Role1 ('+'). This loop remains constant across both frames, highlighting a fundamental interaction that persists despite the changing overall connotation of the event. The consistent labels 'S(role1→role2) -' from Role1 to Role2, and the '+' from Role2 to Role1, in both diagrams (a) and (b), provide direct textual evidence for this stable internal dynamic.

These insights demonstrate how connotation frames systematically capture and illustrate the nuanced emotional and moral judgments embedded within language and narrative structures.

**Document Context:**
The image is presented within a section likely discussing 'connotation frames,' serving as a critical visual aid to illustrate how these frames function. The accompanying document text, which mentions 'connotation frames for survive and violate' and describes the 'writer and reader have positive sentiment toward Role1, the subject, and negative sentiment toward Role2' for 'survive', and 'positive sentiment instead toward Role2, the direct object' for 'violate', directly explains and confirms the relationships depicted in the diagrams. This image visually grounds the abstract concept of connotation frames by providing concrete examples of sentiment assignment and role definition, essential for understanding how linguistic choices shape narrative perception and emotional responses. It directly supports the document's argument on how verbs like 'survive' and 'violate' structure emotional understanding.

**Summary:**
This image presents two "connotation frames," which are conceptual diagrams illustrating how sentiment is structured around the roles involved in specific actions. Diagram (a) depicts the "Connotation Frame for 'Role1 survives Role2'." In this scenario, Role1 is characterized as a "sympathetic victim," while Role2 is associated with "some type of hardship." Both the Writer and the Reader perceive Role1 positively, as indicated by the label "S(writer→role1) +" on the arrow from Writer to Role1 and a general "+" on the arrow from Reader to Role1. Conversely, both the Writer (indicated by "S(writer→role2) -") and the Reader (indicated by "-") feel negatively towards Role2. Internally, there is a negative sentiment from Role1 towards Role2 (labeled "S(role1→role2) -"), which could represent Role1 acting against Role2 to survive, or Role2 being the source of adversity. Simultaneously, there's a positive sentiment from Role2 back to Role1 (indicated by "+"), suggesting that Role1 benefits or gains a positive outcome from its interaction with Role2, enabling its survival. Diagram (b) illustrates the "Connotation Frame for 'Role1 violates Role2'." Here, the roles are dramatically different. Role1 is now "the antagonist," and Role2 is designated as a "sympathetic victim." The sentiment from both the Writer (indicated by "-") and the Reader (indicated by "-") towards Role1 is negative. In contrast, both the Writer (indicated by "S(writer→role2) +") and the Reader (indicated by "+") express positive sentiment towards Role2. The internal relationship between the roles maintains a negative sentiment from Role1 to Role2 (labeled "S(role1→role2) -"), explicitly showing Role1's harmful action. Similar to the "survives" frame, there is a positive sentiment from Role2 back to Role1 (indicated by "+"), which might imply that despite being violated, Role2's victim status elicits sympathy, or perhaps there's an enduring aspect of Role2 that makes it significant to Role1, even if negatively. In essence, these diagrams demonstrate how the choice of a single verb ("survives" versus "violates") profoundly alters the moral valence and emotional perception of the main characters (Role1 and Role2) for both the creator (Writer) and the interpreter (Reader) of a narrative. The consistent loop of S(role1->role2) negative and Role2->Role1 positive across both frames suggests a fundamental dynamic between the roles, where Role1 is the active agent, and Role2 is the recipient, yet Role2 maintains a certain enduring "positive" aspect in relation to Role1 or in its overall characterization as a victim.](images/21c174de8678168c3c5c6050b59ecec33d98b180e69ef632e1a32c615d48d6d9.jpg)
Figure 22.14 task, and even outperform the frequency baseline Connotation frames for survive and violate. (a) For survive, the writer and reader have positive in one setting. Nevertheless, they do not outper- terns. Bsentiment toward Role1, the subject, and negative sentiment toward Role2, the direct object. (b) For violate, the form Field et al. (2019), likely becausewriter and reader have positive sentiment instead toward Role2, the direct object.

est agenThe connotation frame lexicons of Rashkin et al. (2016) and Rashkin et al. (2017) also express other connotative aspects of the predicate toward each argument, including the effect (something bad happened to x) value: ( $\mathbf { \dot { x } }$ is valuable), and mental state: ( $\mathbf { \dot { x } }$ is distressed by the event). Connotation frames can also mark the Finally, we qualitatively analyze how well our power differential between the arguments (using the verb implore means that the method captures affect dimensions by analyzing profile stheme argument has greater power than the agent), and the agency of each argument low-(waited is low agency). Fig. 22.15 shows a visualization from Sap et al. (2017).

driver. IConnotation frames can be built by hand (Sap et al., 2017), or they can be learned betweenby supervised learning (Rashkin et al., 2016), for example using hand-labeled traina position of less power than the theme (“the tri- Figure 3: Sampling data to supervise classifiers for each of the individual relations, e.g., whether S(writer $\mathrm { \Phi }  \mathrm { R o l e } 1 $ bun) is $^ +$ ). In contrast, “He demanded the tribunal with high annot or -, and then improving accuracy via global constraints across all relations.

![## Image Analysis: 8e7d287238f262b5aed21aba888650ea6a520278919c62386cbf9c0eda4fd505.jpg

**Conceptual Understanding:**
This image represents or illustrates conceptual "connotation frames" associated with verbs in natural language. Its main purpose is to demonstrate how specific verbs (like "implore" and "wait") implicitly convey information about the relative power dynamics between an agent and a theme, or the level of agency possessed by an agent. The key ideas communicated are the concepts of "power" and "agency" as inherent semantic properties of verbs and their arguments.

**Content Interpretation:**
The image demonstrates two distinct linguistic relationships: power dynamics and agent agency. The first example, "He implored the tribunal to show mercy," illustrates how the verb "implore" implies a power imbalance. The extracted text "power(AG < TH)" explicitly states that the Agent ("He") has less power than the Theme ("the tribunal"). This is significant because it highlights how verbs are not merely actions but also carry implicit social and relational information. The second example, "The princess waited for her prince," demonstrates the concept of agency. The extracted text "agency(AG) = -" indicates that the Agent ("The princess") has low or negative agency in the context of the verb "wait." This shows that some verbs inherently describe situations where the subject is not actively exercising control or initiation. The blue oval labeled "VERB implore" and the green oval labeled "VERB wait" visually center the specific verbs under analysis. The labels "AGENT" and "THEME" clearly define the roles of the noun phrases in relation to the verb. The curved arrows visually link these elements, with the textual labels "power(AG < TH)" and "agency(AG) = -" providing the precise semantic relationships being conveyed by the connotation frames.

**Key Insights:**
The main takeaways and insights from this image are: Verbs convey implicit semantic information: Verbs carry more than just their literal action; they embed connotations about the relationships between participants. The use of "implored" (with "power(AG < TH)") and "waited" (with "agency(AG) = -") explicitly demonstrates this. Connotation frames can formalize these implicit meanings: The diagram shows a method to represent these semantic connotations formally. The notation "power(AG < TH)" and "agency(AG) = -" are direct textual evidence for this formalization. Power and agency are key aspects of verb semantics: The specific focus on "power" and "agency" in these examples highlights their importance in understanding the deeper meaning of sentences. The verb "implore" directly implies a lower power status of the implorer relative to the one being implored, as shown by "power(AG < TH)". The verb "wait" implies a passive state with low agency for the waiting subject, as shown by "agency(AG) = -".

**Document Context:**
This image directly relates to the concept of "connotation frames," as mentioned in the "connotation frame" section and the surrounding text "Figure 22.15 The connotation frames of Sap et al. (2017), showing that the verb implore [...] and showing the low level of agency of the subject of waited." It provides visual examples to illustrate the formal notation and the practical application of these frames to specific verbs, thereby enhancing the understanding of how verbs encode power and agency.

**Summary:**
This diagram illustrates two distinct "connotation frames," which are conceptual models used to analyze the implicit meanings of verbs in sentences, specifically focusing on "power" and "agency." The first frame analyzes the sentence: "He implored the tribunal to show mercy." In this frame, "He" is identified as the AGENT, and "the tribunal" as the THEME. The central action is the VERB "implore." A curved arrow connects the AGENT and THEME, passing through the verb, and is labeled with "power(AG < TH)." This notation formally indicates that the AGENT ("He") has less power than the THEME ("the tribunal"), which is a key semantic implication of the verb "implore." The second frame analyzes the sentence: "The princess waited for her prince." Here, "The princess" is identified as the AGENT, and "her prince" as the THEME. The central action is the VERB "wait." A curved arrow originates from and returns to the AGENT, labeled with "agency(AG) = -." This notation formally signifies that the AGENT ("The princess") possesses a low or negative level of agency, meaning they are in a passive state rather than actively initiating an action. Together, these two examples visually clarify how connotation frames can be used to explicitly represent the subtle but significant social and relational meanings (like relative power and degree of agency) that are embedded within the choice of a particular verb in a sentence.](images/8e7d287238f262b5aed21aba888650ea6a520278919c62386cbf9c0eda4fd505.jpg)
Figure 22.15 The connotation frames of Sap et al. (2017), showing that the verb implore Figure 2: The formal notation of the connotationimplies the agent has lower power than the theme (in contrast, say, with a verb like demanded), frames of power and agency. The first exampleand showing the low level of agency of the subject of waited. Figure from Sap et al. (2017).

# 22.10 Summary

frames offer new insights that complement and de- ors” the theme (• Many kinds of affective states can be distinguished, including emotions, moods, viate from the well-known Bechdel test (Bechdel, writer implieattitudes (which include sentiment), interpersonal stance, and personality.   
1986). In particular, we find that high-agency • Emotion can be represented by fixed atomic units often called basic emowomen through the lens of connotation frames are  tions, or as points in space defined by dimensions like valence and arousal.   
movies (e.g., Snow White) accidentally pass the agreement is 0.3• Words have connotational aspects related to these affective states, and this Bechdel test and also because even movies withconnotational aspect of word meaning can be represented in lexicons.   
verb denotes w• Affective lexicons can be built by hand, using crowd sourcing to label the affective content of each word.   
2 Connotation Frames of Power and For example, a• Lexicons can be built with semi-supervised, bootstrapping from seed words Agencyusing similarity metrics like embedding cosine.   
We create two new connotation relations, power cisive as someo• Lexicons can be learned in a fully supervised manner, when a convenient and agency (examples in Figure 3), as an expan- ing” things. Atraining signal can be found in the world, such as ratings assigned by users on a review site.   
with placeholders to avoid gender bias in the con- high agency as • Words can be assigned weights in a lexicon by using various functions of word text (e.g., X rescued Y; an example task is shown as agency(AG)counts in training texts, and ratio metrics like log odds ratio informative in tDirichlet prior.   
56% and 51%• Affect can be detected, just like sentiment, by using standard supervised text Power Differentials Many verbs imply the au- tively. Despiteclassification techniques, using all the words or bigrams in a text as features. thority levels of the agent and theme relative to 94% Additional features can be drawn from counts of words in lexicons.   
2 http://homes.cs.washington.edu/˜msap/ notators rarely s• Lexicons can also be used to detect affect in a rule-based classifier by picking movie-bias/. 3 Some contthe simple majority sentiment based on counts of words in each lexicon.   
homes.cs.washington.edu/˜msap/movie-bias/. include the sub• Connotation frames express richer relations of affective meaning that a predicate encodes about its arguments.

# Bibliographical and Historical Notes

The idea of formally representing the subjective meaning of words began with Osgood et al. (1957), the same pioneering study that first proposed the vector space model of meaning described in Chapter 6. Osgood et al. (1957) had participants rate words on various scales, and ran factor analysis on the ratings. The most significant factor they uncovered was the evaluative dimension, which distinguished between pairs like good/bad, valuable/worthless, pleasant/unpleasant. This work influenced the development of early dictionaries of sentiment and affective meaning in the field of content analysis (Stone et al., 1966).

Wiebe (1994) began an influential line of work on detecting subjectivity in text, beginning with the task of identifying subjective sentences and the subjective characters who are described in the text as holding private states, beliefs or attitudes. Learned sentiment lexicons such as the polarity lexicons of Hatzivassiloglou and McKeown (1997) were shown to be a useful feature in subjectivity detection (Hatzivassiloglou and Wiebe 2000, Wiebe 2000).

The term sentiment seems to have been introduced in 2001 by Das and Chen (2001), to describe the task of measuring market sentiment by looking at the words in stock trading message boards. In the same paper Das and Chen (2001) also proposed the use of a sentiment lexicon. The list of words in the lexicon was created by hand, but each word was assigned weights according to how much it discriminated a particular class (say buy versus sell) by maximizing across-class variation and minimizing within-class variation. The term sentiment, and the use of lexicons, caught on quite quickly (e.g., inter alia, Turney 2002). Pang et al. (2002) first showed the power of using all the words without a sentiment lexicon; see also Wang and Manning (2012).

Most of the semi-supervised methods we describe for extending sentiment dictionaries drew on the early idea that synonyms and antonyms tend to co-occur in the same sentence (Miller and Charles 1991, Justeson and Katz 1991, Riloff and Shepherd 1997). Other semi-supervised methods for learning cues to affective meaning rely on information extraction techniques, like the AutoSlog pattern extractors (Riloff and Wiebe, 2003). Graph based algorithms for sentiment were first suggested by Hatzivassiloglou and McKeown (1997), and graph propagation became a standard method (Zhu and Ghahramani 2002, Zhu et al. 2003, Zhou et al. 2004a, Velikovich et al. 2010). Crowdsourcing can also be used to improve precision by filtering the result of semi-supervised lexicon learning (Riloff and Shepherd 1997, Fast et al. 2016).

Much recent work focuses on ways to learn embeddings that directly encode sentiment or other properties, such as the DENSIFIER algorithm of Rothe et al. (2016) that learns to transform the embedding space to focus on sentiment (or other) information.

# Exercises

22.1 Show that the relationship between a word $w$ and a category $c$ in the Potts Score in Eq. 22.6 is a variant of the pointwise mutual information $\mathrm { p m i } ( w , c )$ without the log term.

# 23 Coreference Resolution and Entity Linking

and even Stigand, the patriotic archbishop of Canterbury, found it advisable–”’ ‘Found WHAT?’ said the Duck. ‘Found IT,’ the Mouse replied rather crossly: ‘of course you know what “it”means.’ ‘I know what “it”means well enough, when I find a thing,’ said the Duck: ‘it’s generally a frog or a worm. The question is, what did the archbishop find?’

Lewis Carroll, Alice in Wonderland

An important component of language processing is knowing who is being talked about in a text. Consider the following passage:

(23.1) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $\$ 2.3$ million, as the 38-year-old became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks.

Each of the underlined phrases in this passage is used by the writer to refer to a person named Victoria Chen. We call linguistic expressions like her or Victoria Chen mentions or referring expressions, and the discourse entity that is referred to (Victoria Chen) the referent. (To distinguish between referring expressions and their referents, we italicize the former.)1 Two or more referring expressions that are used to refer to the same discourse entity are said to corefer; thus, Victoria Chen and she corefer in (23.1).

Coreference is an important component of natural language processing. A dialogue system that has just told the user “There is a 2pm flight on United and a 4pm one on Cathay Pacific” must know which flight the user means by “I’ll take the second one”. A question answering system that uses Wikipedia to answer a question about Marie Curie must know who she was in the sentence “She was born in Warsaw”. And a machine translation system translating from a language like Spanish, in which pronouns can be dropped, must use coreference from the previous sentence to decide whether the Spanish sentence ‘“Me encanta el conocimiento”, dice.’ should be translated as ‘“I love knowledge”, he says’, or ‘“I love knowledge”, she says’. Indeed, this example comes from an actual news article in El Pa´ıs about a female professor and was mistranslated as “he” in machine translation because of inaccurate coreference resolution (Schiebinger, 2013).

Natural language processing systems (and humans) interpret linguistic expressions with respect to a discourse model (Karttunen, 1969). A discourse model (Fig. 23.1) is a mental model that the understander builds incrementally when interpreting a text, containing representations of the entities referred to in the text, as well as properties of the entities and relations among them. When a referent is first mentioned in a discourse, we say that a representation for it is evoked into the model. Upon subsequent mention, this representation is accessed from the model.

![## Image Analysis: 34dae23d1155d90246ee8e9055316ad2a2531975146586f7802a99b1cc6890a0.jpg

**Conceptual Understanding:**
The image conceptually represents the process of coreference resolution and entity linking within a 'Discourse Model'. It illustrates how linguistic mentions relate to the entities they refer to, distinguishing between introducing new entities and referring to existing ones. The main purpose of the image is to visually explain the core concepts of 'referring (evoke)', 'referring (access)', and 'coreference' as fundamental mechanisms in understanding how language creates and manages referential links in discourse. It conveys the idea that our mental model of a conversation (the 'Discourse Model') is populated and accessed by the words we use.

**Content Interpretation:**
The image depicts the conceptual processes involved in coreference resolution and entity linking within discourse analysis. It shows how linguistic expressions, referred to as 'mentions', relate to 'discourse entities' that are part of a 'Discourse Model'.

**Processes Shown:**
*   **Evoking an Entity:** The mention '"Victoria"' initiates the creation or introduction of the 'human figure' entity (labeled 'V') into the 'Discourse Model'. This is explicitly labeled by the arrow 'refer (evoke)'. This process signifies that a proper noun or a new definite description can introduce a new participant into the ongoing discourse.
*   **Accessing an Entity:** The mention '"she"' subsequently refers to an *already established* 'human figure' entity (labeled 'V') within the 'Discourse Model'. This is explicitly labeled by the arrow 'refer (access)'. This process illustrates how pronouns or subsequent mentions typically refer back to entities already present in the discourse context.
*   **Coreference:** The bidirectional arrow labeled 'corefer' between '"Victoria"' and '"she"' signifies that these two distinct linguistic mentions refer to the *same* underlying discourse entity (the human figure labeled 'V'). This is the core concept of coreference resolution.

**Concepts/Systems Shown:**
*   **Discourse Model:** This is a conceptual representation of the mental model or representation of entities and their relationships that participants in a discourse maintain. It is the repository for the entities introduced and referred to during communication.
*   **Discourse Entities:** These are the abstract or concrete referents within the discourse model. The image shows several types: 'Megabucks' and 'Lotsabucks' (representing organizations or locations), '$ pay' (representing a concept like salary or payment), and a specific individual, inferred to be 'Victoria' (represented by the human figure with 'V').
*   **Mentions:** These are the actual linguistic expressions (words or phrases) used in a text or speech to refer to an entity. Here, '"Victoria"' is a proper noun mention, and '"she"' is a pronominal mention.

**Significance of Information Presented:**
*   The diagram clearly differentiates between the act of introducing an entity ('evoke') and referring to an already existing one ('access'), which is fundamental to understanding how discourse models are built and maintained. The specific labels 'refer (evoke)' and 'refer (access)' provide precise terminology for these distinct operations.
*   The concept of 'corefer' is visually and textually highlighted as the relationship between different mentions pointing to the same entity, which is the ultimate goal of coreference resolution tasks in NLP.

**Key Insights:**
The main takeaways and insights from this image are: 

1.  **Dynamic Nature of Discourse Models:** The 'Discourse Model' is a dynamic mental representation that is built and updated as linguistic mentions are encountered. It contains various entities (e.g., organizations, individuals, concepts like pay). The existence of entities like 'Megabucks', 'Lotsabucks', '$ pay', and the person 'V' inside the 'Discourse Model' cloud demonstrates this.
2.  **Two Modes of Reference:** Mentions interact with the discourse model in at least two fundamental ways: 
    *   **Evocation:** A mention can *evoke* (introduce) a new entity into the discourse model. This is evidenced by the arrow labeled 'refer (evoke)' originating from the mention '"Victoria"' and pointing to the human figure entity (V).
    *   **Access:** A mention can *access* (refer to) an already established entity within the discourse model. This is evidenced by the arrow labeled 'refer (access)' originating from the mention '"she"' and pointing to the *same* human figure entity (V).
3.  **Coreference Defined:** When two or more distinct linguistic mentions refer to the same underlying entity in the discourse model, they are 'coreferential'. This is explicitly demonstrated by the bidirectional arrow labeled 'corefer' connecting the mentions '"Victoria"' and '"she"'. This signifies that despite being different linguistic forms, they both point to the same conceptual individual 'Victoria' within the discourse. 
4.  **Types of Mentions:** The image implicitly shows different types of mentions, a proper noun ('"Victoria"') and a pronoun ('"she"'), demonstrating how different grammatical forms play distinct roles in referring to entities, aligning with the concept of coreference resolution in natural language processing.

**Document Context:**
This image serves as a foundational visual aid for understanding the core concepts within Section 23, 'Coreference Resolution and Entity Linking'. It directly illustrates the mechanisms by which linguistic mentions establish and maintain connections to entities within a 'Discourse Model'. The diagram clarifies the distinction between 'evoking' (introducing a new entity) and 'accessing' (referring to an existing entity) and visually defines 'coreference' as the relationship between mentions referring to the same entity. This visual explanation is crucial for readers to grasp the theoretical underpinnings before delving into the computational methods and challenges of coreference resolution, making the abstract concepts tangible and easier to follow within the broader document.

**Summary:**
The image illustrates how linguistic mentions, such as names and pronouns, interact with a 'Discourse Model' to evoke or access discourse entities, forming coreferential relationships. The central element is a large light-blue cloud labeled 'Discourse Model', representing a mental model of entities in a conversation or text. Inside this cloud are several distinct entities: two bank-like building icons labeled 'Megabucks' and 'Lotsabucks', a dollar sign symbol labeled 'pay' (with a subscript 'pay'), and a stylized human figure icon representing a female, which has a small 'V' inscribed on its chest, likely standing for 'Victoria'. Outside the 'Discourse Model' cloud, two specific linguistic mentions are shown. The mention '"Victoria"' is connected by an arrow to the human figure entity within the 'Discourse Model' cloud. This arrow is labeled 'refer (evoke)', indicating that the mention '"Victoria"' introduces or creates this entity in the discourse model. Another mention, '"she"', is also connected by an arrow to the *same* human figure entity within the 'Discourse Model' cloud. This arrow is labeled 'refer (access)', signifying that the mention '"she"' refers to an already existing entity in the discourse model. Finally, there is a bidirectional arrow positioned between the mentions '"Victoria"' and '"she"'. This arrow is labeled 'corefer', clearly indicating that both mentions refer to the same underlying entity in the discourse. Below the main 'Discourse Model' cloud, two smaller, isolated light-blue clouds suggest that other entities or conceptual spaces might exist but are not detailed in this specific diagram.](images/34dae23d1155d90246ee8e9055316ad2a2531975146586f7802a99b1cc6890a0.jpg)
Figure 23.1 How mentions evoke and access discourse entities in a discourse model.

anaphora anaphor antecedent

singleton coreference resolution coreference chain cluster

Reference in a text to an entity that has been previously introduced into the discourse is called anaphora, and the referring expression used is said to be an anaphor, or anaphoric.2 In passage (23.1), the pronouns she and her and the definite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior mention (in this case Victoria Chen) that is called the antecedent. Not every referring expression is an antecedent. An entity that has only a single mention in a text (like Lotsabucks in (23.1)) is called a singleton.

In this chapter we focus on the task of coreference resolution. Coreference resolution is the task of determining whether two mentions corefer, by which we mean they refer to the same entity in the discourse model (the same discourse entity). The set of coreferring expressions is often called a coreference chain or a cluster. For example, in processing (23.1), a coreference resolution algorithm would need to find at least four coreference chains, corresponding to the four entities in the discourse model in Fig. 23.1.

1. {Victoria Chen, her, the 38-year-old, She}   
2. {Megabucks Banking, the company, Megabucks}   
3. {her pay}   
4. {Lotsabucks}

Note that mentions can be nested; for example the mention her is syntactically part of another mention, her pay, referring to a completely different discourse entity.

Coreference resolution thus comprises two tasks (although they are often performed jointly): (1) identifying the mentions, and (2) clustering them into coreference chains/discourse entities.

We said that two mentions corefered if they are associated with the same discourse entity. But often we’d like to go further, deciding which real world entity is associated with this discourse entity. For example, the mention Washington might refer to the US state, or the capital city, or the person George Washington; the interpretation of the sentence will of course be very different for each of these. The task of entity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping a discourse entity to some real-world individual.3 We usually operationalize entity linking or resolution by mapping to an ontology: a list of entities in the world, like a gazeteer (Appendix F). Perhaps the most common ontology used for this task is Wikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus the entity linking task of wikification (Mihalcea and Csomai, 2007) is the task of deciding which Wikipedia page corresponding to an individual is being referred to by a mention. But entity linking can be done with any ontology; for example if we have an ontology of genes, we can link mentions of genes in text to the disambiguated gene name in the ontology.

In the next sections we introduce the task of coreference resolution in more detail, and survey a variety of architectures for resolution. We also introduce two architectures for the task of entity linking.

Before turning to algorithms, however, we mention some important tasks we will only touch on briefly at the end of this chapter. First are the famous Winograd Schema problems (so-called because they were first pointed out by Terry Winograd in his dissertation). These entity coreference resolution problems are designed to be too difficult to be solved by the resolution methods we describe in this chapter, and the kind of real-world knowledge they require has made them a kind of challenge task for natural language processing. For example, consider the task of determining the correct antecedent of the pronoun they in the following example:

(23.2) The city council denied the demonstrators a permit because

a. they feared violence.   
b. they advocated violence.

Determining the correct antecedent for the pronoun they requires understanding that the second clause is intended as an explanation of the first clause, and also that city councils are perhaps more likely than demonstrators to fear violence and that demonstrators might be more likely to advocate violence. Solving Winograd Schema problems requires finding way to represent or discover the necessary real world knowledge.

A problem we won’t discuss in this chapter is the related task of event coreference, deciding whether two event mentions (such as the buy and the acquisition in these two sentences from the $\mathrm { E C B + }$ corpus) refer to the same event:

(23.3) AMD agreed to [buy] Markham, Ontario-based ATI for around $\$ 5.4$ billion in cash and stock, the companies announced Monday.   
(23.4) The [acquisition] would turn AMD into one of the world’s largest providers of graphics chips.

Event mentions are much harder to detect than entity mentions, since they can be verbal as well as nominal. Once detected, the same mention-pair and mention-ranking models used for entities are often applied to events.

discourse deixis

An even more complex kind of coreference is discourse deixis (Webber, 1988), in which an anaphor refers back to a discourse segment, which can be quite hard to delimit or categorize, like the examples in (23.5) adapted from Webber (1991):

(23.5) According to Soleil, Beau just opened a restaurant

a. But that turned out to be a lie.   
b. But that was false.   
c. That struck me as a funny way to describe the situation.

The referent of that is a speech act (see Chapter 15) in (23.5a), a proposition in (23.5b), and a manner of description in (23.5c). We don’t give algorithms in this chapter for these difficult types of non-nominal antecedents, but see Kolhatkar et al. (2018) for a survey.

# 23.1 Coreference Phenomena: Linguistic Background

We now offer some linguistic background on reference phenomena. We introduce the four types of referring expressions (definite and indefinite NPs, pronouns, and names), describe how these are used to evoke and access entities in the discourse model, and talk about linguistic features of the anaphor/antecedent relation (like number/gender agreement, or properties of verb semantics).

# 23.1.1 Types of Referring Expressions

Indefinite Noun Phrases: The most common form of indefinite reference in English is marked with the determiner $a$ (or an), but it can also be marked by a quantifier such as some or even the determiner this. Indefinite reference generally introduces into the discourse context entities that are new to the hearer.

(23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose. b. He had gone round one day to bring her some walnuts. c. I saw this beautiful cauliflower today.

Definite Noun Phrases: Definite reference, such as via NPs that use the English article the, refers to an entity that is identifiable to the hearer. An entity can be identifiable to the hearer because it has been mentioned previously in the text and thus is already represented in the discourse model:

(23.7) It concerns a white stallion which I have sold to an officer. But the pedigree of the white stallion was not fully established.

Alternatively, an entity can be identifiable because it is contained in the hearer’s set of beliefs about the world, or the uniqueness of the object is implied by the description itself, in which case it evokes a representation of the referent into the discourse model, as in (23.9):

(23.8) I read about it in the New York Times. (23.9) Have you seen the car keys?

These last uses are quite common; more than half of definite NPs in newswire texts are non-anaphoric, often because they are the first time an entity is mentioned (Poesio and Vieira 1998, Bean and Riloff 1999).

Pronouns: Another form of definite reference is pronominalization, used for entities that are extremely salient in the discourse, (as we discuss below):

(23.10) Emma smiled and chatted as cheerfully as she could,

Pronouns can also participate in cataphora, in which they are mentioned before their referents are, as in (23.11).

(23.11) Even before she saw it, Dorothy had been thinking about the Emerald City every day.

# bound

Here, the pronouns she and it both occur before their referents are introduced.

Pronouns also appear in quantified contexts in which they are considered to be bound, as in (23.12).

(23.12) Every dancer brought her left arm forward.

Under the relevant reading, her does not refer to some woman in context, but instead behaves like a variable bound to the quantified expression every dancer. We are not concerned with the bound interpretation of pronouns in this chapter.

In some languages, pronouns can appear as clitics attached to a word, like lo (‘it’) in this Spanish example from AnCora (Recasens and Mart´ı, 2010):

(23.13) La intencion es reconocer el gran prestigio que tiene la marat ´ on y unir ´ lo con esta gran carrera. ‘The aim is to recognize the great prestige that the Marathon has and join|it with this great race.”

Demonstrative Pronouns: Demonstrative pronouns this and that can appear either alone or as determiners, for instance, this ingredient, that spice:

(23.14) I just bought a copy of Thoreau’s Walden. I had bought one five years ago. That one had been very tattered; this one was in much better condition.

Note that this NP is ambiguous; in colloquial spoken English, it can be indefinite, as in (23.6), or definite, as in (23.14).

Zero Anaphora: Instead of using a pronoun, in some languages (including Chinese, Japanese, and Italian) it is possible to have an anaphor that has no lexical realization at all, called a zero anaphor or zero pronoun, as in the following Italian and Japanese examples from Poesio et al. (2016):

(23.15) EN $[ \mathrm { J o h n } ] _ { i }$ went to visit some friends. On the way $[ \mathrm { h e l } _ { i }$ bought some wine. IT [Giovanni]i ando a far visita a degli amici. Per via \` $\phi _ { i }$ compro del vino. \` JA [John]i-wa yujin-o houmon-sita. Tochu-de $\phi _ { i }$ wain-o ka-tta.

or this Chinese example:

(23.16) [ ]前一会精神上 紧 。[0]现在 较平静了 我 太 张 比[I] was too nervous a while ago. ... [0] am now calmer.

Zero anaphors complicate the task of mention detection in these languages.

Names: Names (such as of people, locations, or organizations) can be used to refer to both new and old entities in the discourse:

a. Miss Woodhouse certainly had not done him justice. b. International Business Machines sought patent compensation from Amazon; IBM had previously sued other companies.

information status discourse-new discourse-old

# 23.1.2 Information Status

The way referring expressions are used to evoke new referents into the discourse (introducing new information), or access old entities from the model (old information), is called their information status or information structure. Entities can be discourse-new or discourse-old, and indeed it is common to distinguish at least three kinds of entities informationally (Prince, 1981):

# new NPs:

brand new NPs: these introduce entities that are discourse-new and hearernew like a fruit or some walnuts. unused NPs: these introduce entities that are discourse-new but hearer-old (like Hong Kong, Marie Curie, or the New York Times. old NPs: also called evoked NPs, these introduce entities that already in the discourse model, hence are both discourse-old and hearer-old, like it in $^ { * } I$ went to a new restaurant. It was...”.

inferrables: these introduce entities that are neither hearer-old nor discourse-old, but the hearer can infer their existence by reasoning based on other entities that are in the discourse. Consider the following examples:

(23.18) I went to a superb restaurant yesterday. The chef had just opened it.   
(23.19) Mix flour, butter and water. Knead the dough until shiny.

Neither the chef nor the dough were in the discourse model based on the first sentence of either example, but the reader can make a bridging inference that these entities should be added to the discourse model and associated with the restaurant and the ingredients, based on world knowledge that restaurants have chefs and dough is the result of mixing flour and liquid (Haviland and Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).

The form of an NP gives strong clues to its information status. We often talk about an entity’s position on the given-new dimension, the extent to which the referent is given (salient in the discourse, easier for the hearer to call to mind, predictable by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976, Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) i.e., very salient in the hearer’s mind or easy to call to mind, can be referred to with less linguistic material. For example pronouns are used only when the referent has a high degree of activation or salience in the discourse model.4 By contrast, less salient entities, like a new referent being introduced to the discourse, will need to be introduced with a longer and more explicit referring expression to help the hearer recover the referent.

# salience

Thus when an entity is first introduced into a discourse its mentions are likely to have full names, titles or roles, or appositive or restrictive relative clauses, as in the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks Banking. As an entity is discussed over a discourse, it becomes more salient to the hearer and its mentions on average typically becomes shorter and less informative, for example with a shortened name (for example Ms. Chen), a definite description (the 38-year-old), or a pronoun (she or her) (Hawkins 1978). However, this change in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b, Reichman 1985, Fox 1993).

# 23.1.3 Complications: Non-Referring Expressions

Many noun phrases or other nominals are not referring expressions, although they may bear a confusing superficial resemblance. For example in some of the earliest computational work on reference resolution, Karttunen (1969) pointed out that the NP a car in the following example does not create a discourse referent:

(23.20) Janet doesn’t have a car.

and cannot be referred back to by anaphoric it or the car:

(23.21) $^ { * } I t$ is a Toyota.   
(23.22) \*The car is red.

We summarize here four common types of structures that are not counted as mentions in coreference tasks and hence complicate the task of mention-detection:

Appositives: An appositional structure is a noun phrase that appears next to a head noun phrase, describing the head. In English they often appear in commas, like “a unit of UAL” appearing in apposition to the NP United, or CFO of Megabucks Banking in apposition to Victoria Chen.

(23.23) Victoria Chen, CFO of Megabucks Banking, saw ...   
(23.24) United, a unit of UAL, matched the fares.

Appositional NPs are not referring expressions, instead functioning as a kind of supplementary parenthetical description of the head NP. Nonetheless, sometimes it is useful to link these phrases to an entity they describe, and so some datasets like OntoNotes mark appositional relationships.

Predicative and Prenominal NPs: Predicative or attributive NPs describe properties of the head noun. In United is a unit of UAL, the NP a unit of UAL describes a property of United, rather than referring to a distinct entity. Thus they are not marked as mentions in coreference tasks; in our example the NPs $\$ 2.3$ million and the company’s president, are attributive, describing properties of her pay and the 38-year-old; Example (23.27) shows a Chinese example in which the predicate NP (中国最 的城市; China’s biggest city) is not a mention.

(23.25) her pay jumped to $\$ 2.3$ million (23.26) the 38-year-old became the company’s president (23.27) 上海是[中国最 的城市] [Shanghai is China’s biggest city]

Expletives: Many uses of pronouns like $i t$ in English and corresponding pronouns in other languages are not referential. Such expletive or pleonastic cases include it is raining, in idioms like hit it off, or in particular syntactic situations like clefts (23.28a) or extraposition (23.28b):

a. It was Emma Goldman who founded Mother Earth b. It surprised me that there was a herring hanging on her wall.

Generics: Another kind of expression that does not refer back to an entity explicitly evoked in the text is generic reference. Consider (23.29).

(23.29) I love mangos. They are very tasty.

Here, they refers, not to a particular mango or set of mangos, but instead to the class of mangos in general. The pronoun you can also be used generically:

(23.30) In July in San Francisco you have to wear a jacket.

# 23.1.4 Linguistic Properties of the Coreference Relation

Now that we have seen the linguistic properties of individual referring expressions we turn to properties of the antecedent/anaphor pair. Understanding these properties is helpful both in designing novel features and performing error analyses.

Number Agreement: Referring expressions and their referents must generally agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plural, and you is unspecified for number. So a plural antecedent like the chefs cannot generally corefer with a singular anaphor like she. However, algorithms cannot enforce number agreement too strictly. First, semantically plural entities can be referred to by either $i t$ or they:

(23.31) IBM announced a new machine translation product yesterday. They have been working on it for 20 years.

# singular they

Second, singular they has become much more common, in which they is used to describe singular individuals, often useful because they is gender neutral. Although recently increasing, singular they is quite old, part of English for many centuries.5

Person Agreement: English distinguishes between first, second, and third person, and a pronoun’s antecedent must agree with the pronoun in person. Thus a third person pronoun (he, she, they, him, her, them, his, her, their) must have a third person antecedent (one of the above or any other noun phrase). However, phenomena like quotation can cause exceptions; in this example I, my, and she are coreferent:

(23.32) “I voted for Nader because he was most aligned with my values,” she said.

Gender or Noun Class Agreement: In many languages, all nouns have grammatical gender or noun class6 and pronouns generally agree with the grammatical gender of their antecedent. In English this occurs only with third-person singular pronouns, which distinguish between male (he, him, his), female (she, her), and nonpersonal $( i t )$ grammatical genders. Non-binary pronouns like ze or hir may also occur in more recent texts. Knowing which gender to associate with a name in text can be complex, and may require world knowledge about the individual. Some examples:

(23.33) Maryam has a theorem. She is exciting. (she $\ c =$ Maryam, not the theorem) (23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)

Binding Theory Constraints: The binding theory is a name for syntactic constraints on the relations between a mention and an antecedent in the same sentence (Chomsky, 1981). Oversimplifying a bit, reflexive pronouns like himself and herself corefer with the subject of the most immediate clause that contains them (23.35), whereas nonreflexives cannot corefer with this subject (23.36).

(23.35) Janet bought herself a bottle of fish sauce. [herself $\sqsupseteq$ Janet] (23.36) Janet bought her a bottle of fish sauce. [her6=Janet]

Recency: Entities introduced in recent utterances tend to be more salient than those introduced from utterances further back. Thus, in (23.37), the pronoun $i t$ is more likely to refer to Jim’s map than the doctor’s map.

(23.37) The doctor found an old map in the captain’s chest. Jim found an even older map hidden on the shelf. It described an island.

Grammatical Role: Entities mentioned in subject position are more salient than those in object position, which are in turn more salient than those mentioned in oblique positions. Thus although the first sentence in (23.38) and (23.39) expresses roughly the same propositional content, the preferred referent for the pronoun he varies with the subject—John in (23.38) and Bill in (23.39).

(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of rum. [ he $=$ Billy ]

(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of rum. $[ \mathrm { h e } = \mathrm { J i m }$ ]

Verb Semantics: Some verbs semantically emphasize one of their arguments, biasing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).

(23.40) John telephoned Bill. He lost the laptop.   
(23.41) John criticized Bill. He lost the laptop.

These examples differ only in the verb used in the first sentence, yet “he” in (23.40) is typically resolved to John, whereas “he” in (23.41) is resolved to Bill. This may be partly due to the link between implicit causality and saliency: the implicit cause of a “criticizing” event is its object, whereas the implicit cause of a “telephoning” event is its subject. In such verbs, the entity which is the implicit cause may be more salient.

Selectional Restrictions: Many other kinds of semantic knowledge can play a role in referent preference. For example, the selectional restrictions that a verb places on its arguments (Chapter 21) can help eliminate referents, as in (23.42).

(23.42) I ate the soup in my new bowl after cooking it for hours

There are two possible referents for it, the soup and the bowl. The verb eat, however, requires that its direct object denote something edible, and this constraint can rule out bowl as a possible referent.

# 23.2 Coreference Tasks and Datasets

We can formulate the task of coreference resolution as follows: Given a text $T$ , find all entities and the coreference links between them. We evaluate our task by comparing the links our system creates with those in human-created gold coreference annotations on $T$ .

Let’s return to our coreference example, now using superscript numbers for each coreference chain (cluster), and subscript letters for individual mentions in the cluster:

(23.43) [Victoria $\mathrm { C h e n } ] _ { a } ^ { 1 }$ , CFO of [Megabucks Banking]2a, saw $[ [ \mathrm { h e r } ] _ { b } ^ { 1 }$ pay $] _ { a } ^ { 3 }$ jump to $\$ 2.3$ million, as [the 38-year-old ${ \sf l } _ { c } ^ { 1 }$ also became [[the company $| _ { b } ^ { 2 }$ ’s president. It is widely known that [she $] _ { d } ^ { 1 }$ came to [Megabucks] $\mathrm { l } _ { c } ^ { 2 }$ from rival [Lotsabucks $] _ { a } ^ { 4 }$ .

Assuming example (23.43) was the entirety of the article, the chains for her pay and Lotsabucks are singleton mentions:

1. {Victoria Chen, her, the 38-year-old, She}   
2. {Megabucks Banking, the company, Megabucks}   
3. { her pay}   
4. { Lotsabucks}

For most coreference evaluation campaigns, the input to the system is the raw text of articles, and systems must detect mentions and then link them into clusters. Solving this task requires dealing with pronominal anaphora (figuring out that her refers to Victoria Chen), filtering out non-referential pronouns like the pleonastic $I t$ in It has been ten years), dealing with definite noun phrases to figure out that the 38-year-old is coreferent with Victoria Chen, and that the company is the same as Megabucks. And we need to deal with names, to realize that Megabucks is the same as Megabucks Banking.

Exactly what counts as a mention and what links are annotated differs from task to task and dataset to dataset. For example some coreference datasets do not label singletons, making the task much simpler. Resolvers can achieve much higher scores on corpora without singletons, since singletons constitute the majority of mentions in running text, and they are often hard to distinguish from non-referential NPs. Some tasks use gold mention-detection (i.e. the system is given human-labeled mention boundaries and the task is just to cluster these gold mentions), which eliminates the need to detect and segment mentions from running text.

Coreference is usually evaluated by the CoNLL F1 score, which combines three metrics: MUC, $B ^ { 3 }$ , and $C E A F _ { e }$ ; Section 23.8 gives the details.

Let’s mention a few characteristics of one popular coreference dataset, OntoNotes (Pradhan et al. $2 0 0 7 \mathrm { c }$ , Pradhan et al. 2007a), and the CoNLL 2012 Shared Task based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese and English coreference datasets of roughly one million words each, consisting of newswire, magazine articles, broadcast news, broadcast conversations, web data and conversational speech data, as well as about 300,000 words of annotated Arabic newswire. The most important distinguishing characteristic of OntoNotes is that it does not label singletons, simplifying the coreference task, since singletons represent $60 \% \text{‰}$ of all entities. In other ways, it is similar to other coreference datasets. Referring expression NPs that are coreferent are marked as mentions, but generics and pleonastic pronouns are not marked. Appositive clauses are not marked as separate mentions, but they are included in the mention. Thus in the NP, “Richard Godown, president of the Industrial Biotechnology Association” the mention is the entire phrase. Prenominal modifiers are annotated as separate entities only if they are proper nouns. Thus wheat is not an entity in wheat fields, but UN is an entity in UN policy (but not adjectives like American in American policy).

A number of corpora mark richer discourse phenomena. The ISNotes corpus annotates a portion of OntoNotes for information status, include bridging examples (Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains coreference annotations for 210,532 tokens from 100 different literary novels, including singletons and quantified and negated noun phrases. The AnCora-CO coreference corpus (Recasens and Mart´ı, 2010) contains 400,000 words each of Spanish (AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for complex phenomena like discourse deixis in both languages. The ARRAU corpus (Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which means singleton clusters are available. ARRAU includes diverse genres like dialog (the TRAINS data) and fiction (the Pear Stories), and has labels for bridging references, discourse deixis, generics, and ambiguous anaphoric relations.

# 23.3 Mention Detection

# mention detection

The first stage of coreference is mention detection: finding the spans of text that constitute each mention. Mention detection algorithms are usually very liberal in proposing candidate mentions (i.e., emphasizing recall), and only filtering later. For example many systems run parsers and named entity taggers on the text and extract every span that is either an NP, a possessive pronoun, or a named entity.

Doing so from our sample text repeated in (23.44):

(23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $\$ 2.3$ million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks.

might result in the following list of 13 potential mentions:

<table><tr><td>Victoria Chen</td><td> $2.3 million</td><td>she Megabucks</td></tr><tr><td>CFO of Megabucks Banking Megabucks Banking</td><td>the 38-year-old the company</td><td>Lotsabucks</td></tr><tr><td>her</td><td> the company&#x27;s president</td><td></td></tr><tr><td> her pay</td><td>It</td><td></td></tr></table>

More recent mention detection systems are even more generous; the span-based algorithm we will describe in Section 23.6 first extracts literally all n-gram spans of words up to ${ \Nu } { = } 1 0$ . Of course recall from Section 23.1.3 that many NPs—and the overwhelming majority of random n-gram spans—are not referring expressions. Therefore all such mention detection systems need to eventually filter out pleonastic/expletive pronouns like $I t$ above, appositives like CFO of Megabucks Banking Inc, or predicate nominals like the company’s president or $\$ 2.3$ million.

Some of this filtering can be done by rules. Early rule-based systems designed regular expressions to deal with pleonastic $i t$ , like the following rules from Lappin and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe, know, anticipate) to capture pleonastic $i t$ in “It is thought that ketchup...”, or modal adjectives (e.g., necessary, possible, certain, important), for, e.g., “It is likely that I...”. Such rules are sometimes used as part of modern systems:

It is Modaladjective that S It is Modaladjective (for NP) to VP It is Cogv-ed that S It seems/appears/means/follows (that) S

Mention-detection rules are sometimes designed specifically for particular evaluation campaigns. For OntoNotes, for example, mentions are not embedded within larger mentions, and while numeric quantities are annotated, they are rarely coreferential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a common first pass rule-based mention detection algorithm (Lee et al., 2013) is:

1. Take all NPs, possessive pronouns, and named entities. 2. Remove numeric quantities (100 dollars, $8 \%$ ), mentions embedded in larger mentions, adjectival forms of nations, and stop words (like there). 3. Remove pleonastic it based on regular expression patterns.

Rule-based systems, however, are generally insufficient to deal with mentiondetection, and so modern systems incorporate some sort of learned mention detection component, such as a referentiality classifier, an anaphoricity classifier— detecting whether an NP is an anaphor—or a discourse-new classifier— detecting whether a mention is discourse-new and a potential antecedent for a future anaphor.

An anaphoricity detector, for example, can draw its positive training examples from any span that is labeled as an anaphoric referring expression in hand-labeled datasets like OntoNotes, ARRAU, or AnCora. Any other NP or named entity can be marked as a negative training example. Anaphoricity classifiers use features of the candidate mention such as its head word, surrounding words, definiteness, animacy, length, position in the sentence/discourse, many of which were first proposed in early work by $\mathrm { N g }$ and Cardie (2002a); see Section 23.5 for more on features.

Referentiality or anaphoricity detectors can be run as filters, in which only mentions that are classified as anaphoric or referential are passed on to the coreference system. The end result of such a filtering mention detection system on our example above might be the following filtered set of 9 potential mentions:

<table><tr><td>Victoria Chen Megabucks Bank the 38-year-old Megabucks her the company</td><td>her pay she</td></tr></table>

It turns out, however, that hard filtering of mentions based on an anaphoricity or referentiality classifier leads to poor performance. If the anaphoricity classifier threshold is set too high, too many mentions are filtered out and recall suffers. If the classifier threshold is set too low, too many pleonastic or non-referential mentions are included and precision suffers.

The modern approach is instead to perform mention detection, anaphoricity, and coreference jointly in a single end-to-end model $\mathrm { N g } ~ 2 0 0 5 \mathrm { b }$ , Denis and Baldridge 2007, Rahman and $\mathrm { N g } ~ 2 0 0 9 .$ ). For example mention detection in the Lee et al. (2017b),2018 system is based on a single end-to-end neural network that computes a score for each mention being referential, a score for two mentions being coreference, and combines them to make a decision, training all these scores with a single end-to-end loss. We’ll describe this method in detail in Section 23.6.

Despite these advances, correctly detecting referential mentions seems to still be an unsolved problem, since systems incorrectly marking pleonastic pronouns like $i t$ and other non-referential NPs as coreferent is a large source of errors of modern coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube 2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).

Mention, referentiality, or anaphoricity detection is thus an important open area of investigation. Other sources of knowledge may turn out to be helpful, especially in combination with unsupervised and semisupervised algorithms, which also mitigate the expense of labeled datasets. In early work, for example Bean and Riloff (1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by extracting and generalizing over the first NPs in a text, which are guaranteed to be non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in the training data but never appear as gold mentions to help find non-referential NPs. Bergsma et al. (2008b) use web counts as a semisupervised way to augment standard features for anaphoricity detection for English $i t$ , an important task because $i t$ is both common and ambiguous; between a quarter and half it examples are non-anaphoric. Consider the following two examples:

(23.45) You can make [it] in advance. [anaphoric] (23.46) You can make [it] in Hollywood. [non-anaphoric]

The it in make it is non-anaphoric, part of the idiom make it. Bergsma et al. (2008b) turn the context around each example into patterns, like “make \* in advance” from (23.45), and “make \* in Hollywood” from (23.46). They then use Google n-grams to enumerate all the words that can replace it in the patterns. Non-anaphoric contexts tend to only have it in the wildcard positions, while anaphoric contexts occur with many other NPs (for example make them in advance is just as frequent in their data as make it in advance, but make them in Hollywood did not occur at all). These n-gram contexts can be used as features in a supervised anaphoricity classifier.

# 23.4 Architectures for Coreference Algorithms

Modern systems for coreference are based on supervised neural machine learning, supervised from hand-labeled datasets like OntoNotes. In this section we overview the various architecture of modern systems, using the categorization of $\mathrm { N g }$ (2010), which distinguishes algorithms based on whether they make each coreference decision in a way that is entity-based—representing each entity in the discourse model— or only mention-based—considering each mention independently, and whether they use ranking models to directly compare potential antecedents. Afterwards, we go into more detail on one state-of-the-art algorithm in Section 23.6.

# 23.4.1 The Mention-Pair Architecture

mention-pair

We begin with the mention-pair architecture, the simplest and most influential coreference architecture, which introduces many of the features of more complex algorithms, even though other architectures perform better. The mention-pair architecture is based around a classifier that— as its name suggests—is given a pair of mentions, a candidate anaphor and a candidate antecedent, and makes a binary classification decision: coreferring or not.

Let’s consider the task of this classifier for the pronoun she in our example, and assume the slightly simplified set of potential antecedents in Fig. 23.2.

![## Image Analysis: 294f7f1a0666e522c746167436afd52ce4f66ae7544444ad5110e9ccd389cadc.jpg

**Conceptual Understanding:**
This image conceptually represents the task of **coreference resolution** in natural language processing, specifically illustrating how a **mention-pair architecture** approaches this problem. The main purpose of the image is to demonstrate that for a given anaphoric mention (like a pronoun or a descriptive phrase), there can be multiple potential antecedents (previous mentions) in the text, and a system must assign a probability to the coreference link between each potential antecedent and the current mention.

The key ideas being communicated are:
1.  **Identification of Mentions:** The individual words and phrases like "Victoria Chen", "Megabucks Banking", "her", "her pay", "the 37-year-old", and "she" are identified as mentions.
2.  **Antecedent-Anaphor Pairs:** The diagram forms pairs where one mention is a potential antecedent and another is the anaphor (the mention whose referent needs to be resolved).
3.  **Probabilistic Coreference:** The notation `p(coref|Antecedent, Anaphor)` indicates that a probability is calculated for each potential coreference link, rather than a simple true/false classification.
4.  **Competing Antecedents:** The diagram shows that a single anaphor, such as "she", can potentially refer to different entities (e.g., "Victoria Chen" or "Megabucks Banking"), highlighting the challenge and decision-making process in coreference resolution.

**Content Interpretation:**
The image primarily illustrates the concept of coreference resolution in natural language processing. It depicts how a system might identify and associate different textual expressions that refer to the same real-world entity. Specifically, it shows a mention-pair architecture where a target mention (like "she") is evaluated against potential antecedent mentions. The diagram highlights two distinct sets of potential coreference links:

1.  **"Victoria Chen" as an Antecedent:** This is associated with solid arrows pointing to "her", "her pay", "the 37-year-old", and "she". The explicit label "p(coref|"Victoria Chen", "she")" signifies that the system calculates the probability that "Victoria Chen" is the antecedent for "she". By extension, the other mentions ("her", "her pay", "the 37-year-old") are also considered likely coreferents to "Victoria Chen", given their grammatical and semantic relationship.

2.  **"Megabucks Banking" as an Antecedent:** This is associated with dotted arrows pointing to "her pay" and "she". The explicit label "p(coref|"Megabucks Banking", "she")" indicates the probability that "Megabucks Banking" is the antecedent for "she". The link to "her pay" suggests a potential, though less direct, coreference where "her pay" might be indirectly associated with the institution "Megabucks Banking" if "her" refers to someone employed there, or if "her pay" is a descriptive phrase related to the bank's operations.

The significance is that the system must weigh the likelihood of "she" referring to "Victoria Chen" versus "Megabucks Banking" (or any other potential antecedent). The visual representation clearly shows competing possibilities for coreference, with the 'p(coref|..., ...)' notation emphasizing the probabilistic nature of this decision. The different line styles (solid vs. dotted) serve to visually group the coreference candidates under their respective antecedents.

**Key Insights:**
The main takeaways from this image are:

1.  **Coreference Resolution is Probabilistic:** The notation "p(coref|antecedent, mention)" explicitly demonstrates that coreference is not a binary (yes/no) decision but rather a probabilistic assessment. The system assigns a likelihood score to each potential coreference link.
2.  **Multiple Antecedents for a Single Mention:** A single anaphoric mention (e.g., "she") can have multiple potential antecedents ("Victoria Chen", "Megabucks Banking"). The system must evaluate each potential pair to determine the most likely coreference.
3.  **Mention-Pair Architecture Focus:** The diagram visually represents the fundamental idea of the mention-pair architecture, which is to consider pairs of mentions (an antecedent and a subsequent mention) and classify whether they are coreferent. This is evident from the explicit listing of potential antecedents and the various mentions they link to.
4.  **Types of Mentions:** The image illustrates different types of mentions that can be involved in coreference: proper nouns ("Victoria Chen", "Megabucks Banking"), common nouns with definite articles ("the 37-year-old"), and pronouns ("her", "she"). All these can participate in coreference relationships.

These insights are directly supported by the text elements:
*   The phrases "Victoria Chen", "Megabucks Banking", "her", "her pay", "the 37-year-old", and "she" are the specific mentions being evaluated.
*   The labels "p(coref|"Victoria Chen", "she")" and "p(coref|"Megabucks Banking", "she")" are explicit textual evidence for the probabilistic nature and the pairing of an antecedent with a target mention. These labels show the coreference function 'p' taking an antecedent and a mention as arguments.

**Document Context:**
This image is directly relevant to Section 23.4.1, titled "The Mention-Pair Architecture," as indicated by the document context. It visually explains the core concept of this architecture: for a given mention, the system considers potential antecedents and assigns a probability of a coreference link. The text after the image, "Figure 23.2 For each pair of a mention (like she), and a potential antecedent mention (like Victoria Chen or her), the mention-pair classifier assigns a probability of a coreference link," perfectly aligns with and describes the visual information presented in the diagram. The image serves as a concrete example, demonstrating how a mention like "she" can have multiple potential antecedents ("Victoria Chen", "Megabucks Banking"), and how a classifier would evaluate the probability of a coreference link for each pair. It illustrates the input (mention pairs) and the output (probability of coreference) of such a classifier within the mention-pair architecture.

**Summary:**
The image illustrates a coreference resolution scenario within a mention-pair architecture. It presents several textual mentions and shows potential coreference links between a primary mention (antecedent) and subsequent mentions. The diagram explicitly shows two main antecedents: "Victoria Chen" and "Megabucks Banking". Each antecedent is evaluated for its coreference probability with other mentions, particularly "she".

For the antecedent "Victoria Chen", solid curved arrows indicate potential coreference relationships with the mentions "her", "her pay", "the 37-year-old", and "she". The label "p(coref|"Victoria Chen", "she")" is explicitly associated with these solid arrows, indicating the probability of a coreference link where "Victoria Chen" is the antecedent and "she" is the target mention, while implicitly extending to the other personal pronouns and descriptive phrases related to "Victoria Chen".

Similarly, for the antecedent "Megabucks Banking", dotted curved arrows indicate potential coreference relationships with the mentions "her pay" and "she". The label "p(coref|"Megabucks Banking", "she")" is explicitly associated with these dotted arrows, indicating the probability of a coreference link where "Megabucks Banking" is the antecedent and "she" is the target mention, while implicitly extending to "her pay" which could refer to an entity related to "Megabucks Banking".

The diagram demonstrates that multiple antecedents can be considered for a single target mention (like "she"), and a coreference classifier would assign a probability to each potential link, as indicated by the 'p(coref|antecedent, mention)' notation. The visual distinction between solid and dotted arrows helps differentiate the source antecedent for these probabilistic links.](images/294f7f1a0666e522c746167436afd52ce4f66ae7544444ad5110e9ccd389cadc.jpg)
Figure 23.2 For each pair of a mention (like she), and a potential antecedent mention (like Victoria Chen or her), the mention-pair classifier assigns a probability of a coreference link.

For each prior mention (Victoria Chen, Megabucks Banking, her, etc.), the binary classifier computes a probability: whether or not the mention is the antecedent of she. We want this probability to be high for actual antecedents (Victoria Chen, her, the 38-year-old) and low for non-antecedents (Megabucks Banking, her pay).

Early classifiers used hand-built features (Section 23.5); more recent classifiers use neural representation learning (Section 23.6)

For training, we need a heuristic for selecting training samples; since most pairs of mentions in a document are not coreferent, selecting every pair would lead to a massive overabundance of negative samples. The most common heuristic, from (Soon et al., 2001), is to choose the closest antecedent as a positive example, and all pairs in between as the negative examples. More formally, for each anaphor mention $m _ { i }$ we create

• one positive instance $( m _ { i } , m _ { j } )$ where $m _ { j }$ is the closest antecedent to $m _ { i }$ , and • a negative instance $( m _ { i } , m _ { k } )$ for each $m _ { k }$ between $m _ { j }$ and $m _ { i }$

Thus for the anaphor she, we would choose (she, her) as the positive example and no negative examples. Similarly, for the anaphor the company we would choose (the company, Megabucks) as the positive example and (the company, she) (the company, the 38-year-old) (the company, her pay) and (the company, her) as negative examples.

Once the classifier is trained, it is applied to each test sentence in a clustering step. For each mention $i$ in a document, the classifier considers each of the prior $i - 1$ mentions. In closest-first clustering (Soon et al., 2001), the classifier is run right to left (from mention $i - 1$ down to mention 1) and the first antecedent with probability $> . 5$ is linked to $i$ . If no antecedent has probably $> 0 . 5$ , no antecedent is selected for $i$ . In best-first clustering, the classifier is run on all $i - 1$ antecedents and the most probable preceding mention is chosen as the antecedent for i. The transitive closure of the pairwise relation is taken as the cluster.

While the mention-pair model has the advantage of simplicity, it has two main problems. First, the classifier doesn’t directly compare candidate antecedents to each other, so it’s not trained to decide, between two likely antecedents, which one is in fact better. Second, it ignores the discourse model, looking only at mentions, not entities. Each classifier decision is made completely locally to the pair, without being able to take into account other mentions of the same entity. The next two models each address one of these two flaws.

# 23.4.2 The Mention-Rank Architecture

The mention ranking model directly compares candidate antecedents to each other, choosing the highest-scoring antecedent for each anaphor.

In early formulations, for mention $i$ , the classifier decides which of the $\{ 1 , . . . , i -$ $1 \}$ prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose $i$ is in fact not anaphoric, and none of the antecedents should be chosen? Such a model would need to run a separate anaphoricity classifier on $i$ . Instead, it turns out to be better to jointly learn anaphoricity detection and coreference together with a single loss (Rahman and $\mathrm { N g }$ , 2009).

So in modern mention-ranking systems, for the ith mention (anaphor), we have an associated random variable $y _ { i }$ ranging over the values $Y ( i ) = \{ 1 , . . . , i - 1 , \epsilon \}$ . The value $\epsilon$ is a special dummy mention meaning that $i$ does not have an antecedent (i.e., is either discourse-new and starts a new coref chain, or is non-anaphoric).

![## Image Analysis: 0cd371e66c5cbc75b9dea2a39b48876df7f713782082d7f7de15801012e89797.jpg

**Conceptual Understanding:**
This image conceptually illustrates the core mechanism of a 'mention-rank architecture' for coreference resolution, a natural language processing task. It visualizes how a system assesses the likelihood of a pronoun, specifically 'she', referring to various previously mentioned entities. The main purpose is to demonstrate the probabilistic assignment of anaphor-antecedent links and the desired outcomes of these assignments in distinguishing plausible from implausible referents. It communicates the key idea that effective coreference resolution involves assigning high probabilities to correct antecedents and low probabilities to incorrect or non-referential (dummy) options, thereby establishing a ranked list of potential referents.

**Content Interpretation:**
The image depicts a coreference resolution mention-ranking system in action. It shows how an anaphoric mention, 'she', is evaluated against a set of potential antecedent phrases and a special dummy mention. The system assigns a conditional probability 'p(antecedent | "she")' to each candidate, quantifying the likelihood of a coreference link. The diagram clearly segregates candidates into two groups based on the expected outcome of these probabilities: those that should ideally be high (plausible antecedents) and those that should ideally be low (implausible or dummy antecedents).

The specific processes shown include:
*   **Candidate Generation:** Implicitly, the system identifies 'Victoria Chen', 'Megabucks Banking', 'her', 'her pay', 'the 37-year-old', and 'ϵ' as potential antecedents for 'she'.
*   **Probability Assignment/Scoring:** The expressions like 'p("Victoria Chen"|"she")' represent the core scoring mechanism, where each candidate is given a probability of being the referent of 'she'.
*   **Categorization of Antecedents:** The blue and red color coding, along with the solid and dotted lines, visually categorizes antecedents based on their expected probability values, which directly reflects their semantic and syntactic compatibility with 'she'.

The significance of the information is the clear distinction between well-formed, plausible coreference links and incorrect or non-referential situations:
*   The blue lines and probabilities for 'Victoria Chen', 'her', and 'the 37-year-old' highlight that these are plausible human entities that 'she' could refer to. The textual evidence 'One or more of these should be high' confirms that the system aims to correctly identify one of these as the primary referent.
*   The red dotted lines and probabilities for 'ϵ', 'Megabucks Banking', and 'her pay' signify implausible or non-referential links. 'Megabucks Banking' (an organization) and 'her pay' (an abstract concept) are semantically incompatible with 'she' (a personal pronoun). The dummy mention 'ϵ' is for cases where 'she' has no explicit antecedent among the candidates. The textual evidence 'All of these should be low' reinforces that the system expects these to be correctly discounted.

**Key Insights:**
The main takeaway from this image is a clear demonstration of how a mention-ranking system within coreference resolution operates by assigning probabilities to candidate antecedents for an anaphoric mention.

Key conclusions and insights supported by the diagram and its textual elements include:

1.  **Probabilistic Evaluation:** Coreference resolution can be framed as a probabilistic task, where the system calculates 'p(candidate | anaphor)' to determine the most likely referent. The presence of explicit probability notations like 'p("Victoria Chen"|"she")' serves as direct evidence.
2.  **Semantic Compatibility:** The system must discern semantically compatible candidates from incompatible ones. 'Victoria Chen', 'her', and 'the 37-year-old' are identified as plausible human referents for 'she', evidenced by their blue color coding and the instruction 'One or more of these should be high'. Conversely, 'Megabucks Banking' (an organization) and 'her pay' (an abstract concept) are deemed implausible, indicated by their red color and the instruction 'All of these should be low'.
3.  **Handling Non-Referential Cases:** The inclusion of the 'ϵ' (epsilon) dummy mention highlights the system's ability to account for instances where an anaphor might not refer to any explicit prior mention in the text. The expectation that 'p("ϵ"|"she")' should be low (part of 'All of these should be low') signifies that if an explicit antecedent is found, the dummy option is correctly de-prioritized.
4.  **Ranking Principle:** The core idea is to rank candidates by their assigned probabilities, aiming for high probabilities for correct antecedents and low probabilities for incorrect ones. This is explicitly stated by 'One or more of these should be high' for the blue candidates and 'All of these should be low' for the red candidates, demonstrating the desired outcome of the ranking process.

**Document Context:**
This image is highly relevant to the document's Section '23.4.2 The Mention-Rank Architecture' and directly illustrates the core concept described. The text immediately following the image, 'Figure 23.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a probability distribution over all previous mentions plus the special dummy mention ϵ .', perfectly aligns with the diagram's content. The image visually breaks down how this 'probability distribution' is constructed and what the ideal outcomes are for different types of candidate mentions.

It provides a concrete example of how the 'mention-ranking system' evaluates the likelihood of an anaphoric pronoun ('she') referring to various preceding linguistic expressions, including proper nouns, other pronouns, descriptive phrases, and even non-referential (dummy) cases. This visual explanation is crucial for understanding the computational mechanics of how such systems make coreference decisions based on assigned probabilities, linking directly to the theoretical framework discussed in the surrounding text.

**Summary:**
This image, Figure 23.3, illustrates the core mechanism of a mention-ranking architecture used in coreference resolution, as described in Section 23.4.2 of the document. It details how a system assigns a probability distribution over potential antecedent mentions for a given anaphoric mention, specifically the pronoun 'she'. The diagram focuses on evaluating various candidate antecedents and the expected probability outcomes for correctly identifying the referent of 'she'.

The central element is the anaphoric mention, 'she', enclosed in a box. To the left of 'she' are several candidate antecedent mentions: 'Victoria Chen', 'Megabucks Banking', 'her', 'her pay', 'the 37-year-old', and a special dummy mention 'ϵ'.

For each candidate, the system calculates a conditional probability 'p(candidate | "she")', indicating the likelihood that 'she' refers to that candidate. These probabilities are visually represented with arrows leading from the probability expressions to the 'she' box.

There are two main categories of probability expectations:

1.  **High Probability Candidates (Blue):** These are 'p("Victoria Chen"|"she")', 'p("her"|"she")', and 'p("the 37-year-old"|"she")'. These expressions are connected to 'she' by solid blue arrows. A blue curly brace to the right of these paths explicitly states: 'One or more of these should be high'. This indicates that 'Victoria Chen', 'her', and 'the 37-year-old' are plausible human referents for 'she', and a correct coreference system should assign a high probability to at least one of them.

2.  **Low Probability Candidates (Red):** These include 'p("ϵ"|"she")', 'p("Megabucks Banking"|"she")', and 'p("her pay"|"she")'. These expressions are connected to 'she' by dotted red arrows. A red curly brace to the right of these paths states: 'All of these should be low'. This signifies that 'Megabucks Banking' (a company) and 'her pay' (an abstract concept) are semantically incompatible as referents for 'she' and should thus have low probabilities. Similarly, if 'she' refers to an explicit prior mention, the dummy mention 'ϵ' (which implies no explicit referent) should also have a low probability. The diagram demonstrates that the system distinguishes between plausible and implausible antecedents, including the dummy mention, by assigning appropriate probabilities.](images/0cd371e66c5cbc75b9dea2a39b48876df7f713782082d7f7de15801012e89797.jpg)
Figure 23.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a probability distribution over all previous mentions plus the special dummy mention .

At test time, for a given mention $i$ the model computes one softmax over all the antecedents (plus $\epsilon$ ) giving a probability for each candidate antecedent (or none).

Fig. 23.3 shows an example of the computation for the single candidate anaphor she.

Once the antecedent is classified for each anaphor, transitive closure can be run over the pairwise decisions to get a complete clustering.

Training is trickier in the mention-ranking model than the mention-pair model, because for each anaphor we don’t know which of all the possible gold antecedents to use for training. Instead, the best antecedent for each mention is latent; that is, for each mention we have a whole cluster of legal gold antecedents to choose from. Early work used heuristics to choose an antecedent, for example choosing the closest antecedent as the gold antecedent and all non-antecedents in a window of two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013, Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent by summing over all of them, with a loss function that optimizes the likelihood of all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the details in Section 23.6.

Mention-ranking models can be implemented with hand-build features or with neural representation learning (which might also incorporate some hand-built features). we’ll explore both directions in Section 23.5 and Section 23.6.

# 23.4.3 Entity-based Models

Both the mention-pair and mention-ranking models make their decisions about mentions. By contrast, entity-based models link each mention not to a previous mention but to a previous discourse entity (cluster of mentions).

A mention-ranking model can be turned into an entity-ranking model simply by having the classifier make its decisions over clusters of mentions rather than individual mentions (Rahman and Ng, 2009).

For traditional feature-based models, this can be done by extracting features over clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper, (D)efinite, (I)ndefinite, $( \mathrm { P r } )$ onoun, so that a cluster composed of Victoria, her, the 38-year-old} would have the shape $P  – P r – D$ (Bjorkelund and Kuhn ¨ , 2014). An entitybased model that includes a mention-pair classifier can use as features aggregates of mention-pair probabilities, for example computing the average probability of coreference over all mention-pairs in the two clusters (Clark and Manning 2015).

Neural models can learn representations of clusters automatically, for example by using an RNN over the sequence of cluster mentions to encode a state corresponding to a cluster representation (Wiseman et al., 2016), or by learning distributed representations for pairs of clusters by pooling over learned representations of mention pairs (Clark and Manning, 2016b).

However, although entity-based models are more expressive, the use of clusterlevel information in practice has not led to large gains in performance, so mentionranking models are still more commonly used.

# 23.5 Classifiers using hand-built features

Feature-based classifiers, use hand-designed features in logistic regression, SVM, or random forest classifiers for coreference resolution. These classifiers don’t perform as well as neural ones. Nonetheless, they are still sometimes useful to build lightweight systems when compute or data are sparse, and the features themselves are useful for error analysis even in neural systems.

Given an anaphor mention and a potential antecedent mention, feature based classifiers make use of three types of features: (i) features of the anaphor, (ii) features of the candidate antecedent, and (iii) features of the relationship between the pair. Entity-based models can make additional use of two additional classes: (iv) feature of all mentions from the antecedent’s entity cluster, and (v) features of the relation between the anaphor and the mentions in the antecedent entity cluster.

Figure 23.4 shows a selection of commonly used features, and shows the value that would be computed for the potential anaphor “she” and potential antecedent “Victoria Chen” in our example sentence, repeated below:   

<table><tr><td colspan="3"> Features of the Anaphor or Antecedent Mention</td></tr><tr><td>First (last) word Head word Attributes</td><td>Victoria/she Victoria/she</td><td>First or last word (or embedding) of antecedent/anaphor Head word (or head embedding) of antecedent/anaphor Sg-F-A-3-PER/ The number, gender, animacy, person, named entity type</td></tr><tr><td>Length Mention type</td><td>Sg-F-A-3-PER attributes of (antecedent/anaphor) 2/1 P/Pr</td><td>length in words of (antecedent/anaphor) Type: (P)roper, (D)efinite, (I)ndefinite, (Pr)onoun) of an-</td></tr><tr><td colspan="3">tecedent/anaphor Features of the Antecedent Entity</td></tr><tr><td>Entity shape</td><td>P-Pr-D</td><td>The‘shape’ or list of types of the mentions in the antecedent entity (cluster)， i.e.， sequences of (P)roper,</td></tr><tr><td>Entity attributes Ant. cluster size</td><td></td><td>(D)efinite, (I)ndefinite, (Pr)onoun. Sg-F-A-3-PER The number, gender, animacy, person, named entity type attributes of the antecedent entity</td></tr><tr><td colspan="3">3 Number of mentions in the antecedent cluster Features of the Pair of Mentions</td></tr><tr><td>Sentence distance Mention distance i-within-i</td><td>1 4 F</td><td>The number of sentences between antecedent and anaphor The number of mentions between antecedent and anaphor Anaphor has i-within-i relation with antecedent</td></tr><tr><td colspan="3">Cosine Cosine between antecedent and anaphor embeddings Features of the Pair of Entities</td></tr><tr><td>Exact String Match F</td><td></td><td>True if the strings of any two mentions from the antecedent and anaphor clusters are identical.</td></tr><tr><td>Head Word Match</td><td>F</td><td>True if any mentions from antecedent cluster has same headword as any mention in anaphor cluster</td></tr><tr><td>Word Inclusion</td><td>F</td><td>All words in anaphor cluster included in antecedent cluster</td></tr></table>

Figure 23.4 Feature-based coreference: sample feature values for anaphor “she” and potential antecedent “Victoria Chen”.

(23.47) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $\$ 2.3$ million, as the 38-year-old also became the company’s president. It is widely known that she came to Megabucks from rival Lotsabucks.

Features that prior work has found to be particularly useful are exact string match, entity headword agreement, mention distance, as well as (for pronouns) exact attribute match and i-within-i, and (for nominals and proper names) word inclusion and cosine. For lexical features (like head words) it is common to only use words that appear enough times ${ \it > } 2 0 { \it \Delta }$ times).

It is crucial in feature-based systems to use conjunctions of features; one experiment suggested that moving from individual features in a classifier to conjunctions of multiple features increased F1 by 4 points (Lee et al., 2017a). Specific conjunctions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using decision tree or random forest classifiers $\mathrm { N g }$ and Cardie 2002a, Lee et al. 2017a).

Features can also be used in neural models as well. Neural systems use contextual word embeddings so don’t benefit from shallow features like string match or or mention types. However features like mention length, distance between mentions, or genre can complement neural contextual embedding models.

# 23.6 A neural mention-ranking algorithm

In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b) (simplified and extended a bit, drawing on Joshi et al. (2019) and others). This is a mention-ranking algorithm that considers all possible spans of text in the document, assigns a mention-score to each span, prunes the mentions based on this score, then assigns coreference links to the remaining mentions.

More formally, given a document $D$ with $T$ words, the model considers all of the $\frac { T ( T + 1 ) } { 2 }$ text spans in $D$ (unigrams, bigrams, trigrams, 4-grams, etc; in practice we only consider spans up a maximum length around 10). The task is to assign to each span $i$ an antecedent $y _ { i }$ , a random variable ranging over the values $Y ( i ) =$ $\{ 1 , . . . , i - 1 , \epsilon \}$ ; each previous span and a special dummy token $\epsilon$ . Choosing the dummy token means that $i$ does not have an antecedent, either because $i$ is discoursenew and starts a new coreference chain, or because $i$ is non-anaphoric.

For each pair of spans $i$ and $j$ , the system assigns a score $s ( i , j )$ for the coreference link between span $i$ and span $j$ . The system then learns a distribution $P ( y _ { i } )$ over the antecedents for span $i$ :

$$
P ( y _ { i } ) = \frac { \exp ( s ( i , y _ { i } ) ) } { \sum _ { y ^ { \prime } \in Y ( i ) } \exp ( s ( i , y ^ { \prime } ) ) }
$$

This score $s ( i , j )$ includes three factors that we’ll define below: $m ( i )$ ; whether span $i$ is a mention; $m ( j )$ ; whether span $j$ is a mention; and $c ( i , j )$ ; whether $j$ is the antecedent of $i$ :

$$
s ( i , j ) = m ( i ) + m ( j ) + c ( i , j )
$$

For the dummy antecedent $\epsilon$ , the score $s ( i , \epsilon )$ is fixed to 0. This way if any nondummy scores are positive, the model predicts the highest-scoring antecedent, but if all the scores are negative it abstains.

# 23.6.1 Computing span representations

To compute the two functions $m ( i )$ and $c ( i , j )$ which score a span $i$ or a pair of spans $( i , j )$ , we’ll need a way to represent a span. The e2e-coref family of algorithms represents each span by trying to capture 3 words/tokens: the first word, the last word, and the most important word. We first run each paragraph or subdocument through an encoder (like BERT) to generate embeddings $h _ { i }$ for each token $i$ . The span $i$ is then represented by a vector ${ \bf g } _ { i }$ that is a concatenation of the encoder output embedding for the first (start) token of the span, the encoder output for the last (end) token of the span, and a third vector which is an attention-based representation:

$$
\mathbf { g } _ { i } = [ \mathsf { h } _ { \mathrm { S T A R T } ( i ) } , \mathsf { h } _ { \mathrm { E N D } ( i ) } , \mathsf { h } _ { \mathrm { A T T } ( i ) } ]
$$

The goal of the attention vector is to represent which word/token is the likely syntactic head-word of the span; we saw in the prior section that head-words are a useful feature; a matching head-word is a good indicator of coreference. The attention representation is computed as usual; the system learns a weight vector ${ \pmb w } _ { \alpha }$ , and computes its dot product with the hidden state $\mathbf { h } _ { t }$ transformed by a FFN:

$$
\alpha _ { t } = \mathbf { w } _ { \alpha } \cdot \mathrm { F F N } _ { \alpha } ( \mathbf { h } _ { t } )
$$

The attention score is normalized into a distribution via a softmax:

$$
a _ { i , t } ~ = ~ \frac { \exp ( \alpha _ { t } ) } { \sum _ { k = \mathrm { S T A R T } ( i ) } ^ { \mathrm { E N D } ( i ) } \exp ( \alpha _ { k } ) }
$$

And then the attention distribution is used to create a vector $\mathbf { h } _ { \mathrm { A T T } ( i ) }$ which is an attention-weighted sum of the embeddings $\mathbf { e } _ { t }$ of each of the words in span $i$ :

$$
\mathsf { \mathbf { h } } _ { \mathrm { A T T } ( i ) } \ = \ \sum _ { t = \mathrm { S T A R T } ( i ) } ^ { \mathrm { E N D } ( i ) } a _ { i , t } \cdot \mathbf { e } _ { t }
$$

![## Image Analysis: 893d481b97ec08b55f6f0c8177285196203ecf43d984bffc67fd589a66bb6617.jpg

**Conceptual Understanding:**
This image conceptually illustrates a critical component of a natural language processing (NLP) system, specifically within the domain of coreference resolution. It depicts the multi-stage computational process by which a model derives a "span representation" (g) and a "mention score" (m) for candidate text spans.

The core idea is to transform raw word encodings into a more abstract and informative representation of multi-word expressions (spans), ultimately assigning a score that indicates their likelihood of being a referential mention. The main purpose is to show the architectural flow for computing these span-level features, highlighting how individual word encodings (h) are combined via an intermediate "span head" (h_ATT) to create a comprehensive "span representation" (g), which then leads to a "mention score" (m). It visualizes the hierarchical aggregation of information from words to phrases/spans.

**Content Interpretation:**
The image displays a layered neural network architecture responsible for processing text spans within a BERT-version of the e2e-coref model. The processes shown are:

1.  **Input Layer (Encoder):** The blue "Encoder" box represents a pre-trained language model (like BERT) that takes a sequence of words ("... General Electric said the Postal Service contacted the company") and produces contextualized embeddings.
2.  **Encodings (h) Layer:** Each set of three red/orange circles at this level represents the vector embedding (encoding) for an individual word. The label "Encodings (h)" confirms these are the contextualized word representations output by the Encoder.
3.  **Span Head (h_ATT) Layer:** The pink square with a '+' symbol indicates an operation that aggregates the encodings of the words within a *span* to create a "span head." The label "Span head (h_ATT)" suggests this might involve an attention mechanism or summation to weigh the importance of different words within the span, summarizing its main idea. For example, for "General Electric", the encodings of "General" and "Electric" are combined to form its span head.
4.  **Span Representation (g) Layer:** This layer computes a "Span representation (g)" for each span. The arrows show that this representation is a combination of the individual "Encodings (h)" of words within the span *and* the derived "Span head (h_ATT)". This comprehensive representation aims to capture both the local context of each word and the overall semantic meaning of the entire span.
5.  **Mention Score (m) Layer:** The final layer calculates a "Mention score (m)" for each span representation. This is a scalar value (represented by a single red/orange circle) that likely indicates the probability or confidence that the given span is a linguistic "mention" (i.e., a noun phrase referring to an entity). The labels above this layer ("General Electric", "Electric said the", etc.) explicitly identify the spans for which these scores are computed.

The detailed connections demonstrate a hierarchical composition where individual word features (h) are first condensed into a span head (h_ATT) using a summation/attention-like operation (+). Then, the full span representation (g) is formed by combining the individual word features (h) with this distilled span head (h_ATT). Finally, this rich span representation (g) is used to predict a mention score (m). This process is repeated for multiple candidate spans, as exemplified by the five different spans shown in the diagram.

**Key Insights:**
1.  **Hierarchical Representation Learning:** The diagram explicitly shows that span representations are built hierarchically. Word-level encodings ("Encodings (h)") form the base, which are then used to compute an intermediate "Span head (h_ATT)", and subsequently combined with original encodings to form a final "Span representation (g)". This structure is crucial for capturing both local word information and aggregated span-level semantics.
2.  **Modular Design for Span Processing:** The distinct layers labeled "Span head (h_ATT)" and "Span representation (g)" highlight a modular approach to feature engineering for text spans. This suggests that different aspects of a span's meaning are captured and processed at successive stages of the computation.
3.  **Integration of BERT-like Models:** The presence of the "Encoder" block at the bottom, combined with the document context, indicates that powerful contextualized embeddings (likely from a transformer model like BERT) are fundamental. These initial "Encodings (h)" provide rich, context-aware features that are essential for subsequent span-level tasks.
4.  **Relevance to Coreference Resolution:** The ultimate output, "Mention score (m)", directly links this computational graph to coreference resolution. Identifying valid mentions is a foundational step in coreference, and this process is designed to assign a scalar score indicating a span's likelihood of being a referential mention.
5.  **Multi-Input Feature Combination:** The "Span representation (g)" is notably formed by combining multiple types of inputs: the individual word encodings and the span head. This demonstrates that a robust span representation benefits from integrating both the fine-grained word-level details and a coarser, aggregated summary of the span's central idea. The numerous arrows converging to the "Span representation (g)" nodes visually confirm this multi-input combination.
6.  **Processing Varied Span Lengths:** The labels for the spans (e.g., "General Electric" - a bigram, "Electric said the" - a trigram) along with the accompanying text (referencing "bigram and trigram spans" and a "maximum width of say 10") imply the model's capability to process and derive representations for spans of different lengths.

**Document Context:**
This image is highly relevant to the "23.6.1 Computing span representations" section of the document, as it visually illustrates the exact computational process described in the text. It shows how the "span representation (g)" and "mention score (m)" are derived within a BERT-based e2e-coref model. The figure provides a concrete diagram for the abstract concepts of span head and span representation, which are critical for understanding how coreference resolution models process textual units larger than individual words. The text after the image explicitly references "Fig. 23.5" and its role in showing "the computation of the span representation $\mathbf { g }$ (and the mention score $\mathbf { m }$ )", directly linking the visual to the surrounding discussion and enhancing comprehension of the technical details.

**Summary:**
This diagram, titled "Computation of the span representation $\mathbf { g }$ (and the mention score $\mathbf { m }$ ) in a BERT version of the e2e-coref model", illustrates a step-by-step process for analyzing segments of text called "spans" to determine their significance, especially for tasks like identifying mentions of entities in a document.

The process begins at the bottom with a blue rectangular box labeled "Encoder". This "Encoder" takes a sequence of input words, such as "... General Electric said the Postal Service contacted the company". It processes these words to generate rich, contextualized numerical representations.

Above the "Encoder" is the "Encodings (h)" layer. Each word from the input sentence (e.g., "General", "Electric", "said", "the", "Postal", "Service", "contacted", "the", "company") is transformed into a set of three red/dark orange circles, which visually represent its unique vector embedding or "encoding (h)". These encodings capture the semantic and syntactic properties of each word in its specific context.

The next layer up is "Span head (h_ATT)". For each identified span of text (which could be one word, two words, three words, etc., up to a maximum width, e.g., "General Electric" or "Electric said the"), a "span head (h_ATT)" is computed. This computation is represented by a pink square box containing a white plus (+) symbol. This plus symbol signifies an aggregation operation, where the "Encodings (h)" of the individual words within that span are combined to create a summary representation or "head" for that span. For instance, the span head for "General Electric" is derived by combining the encodings of "General" and "Electric".

Following this, the "Span representation (g)" layer computes a comprehensive representation for each span. For a given span, this representation (shown as another set of three red/dark orange circles) is formed by combining multiple pieces of information: the individual "Encodings (h)" of all words within that span AND the "Span head (h_ATT)" that was just computed for that same span. This multi-input approach allows the "span representation (g)" to capture both the fine-grained details of the words and the overall summary meaning of the span.

Finally, at the top, the "Mention score (m)" is calculated. For each "Span representation (g)", an arrow points upwards to a single red/dark orange circle, which represents the "Mention score (m)". This score is a numerical value that indicates the likelihood or confidence that the corresponding text span is a "mention" – that is, a reference to an entity (like a person, organization, or location) in the text. The labels directly above this layer, such as "General Electric", "Electric said the", "the Postal Service", "Service contacted the", and "the company", explicitly name the specific spans for which these mention scores are being computed.

In essence, the diagram illustrates a sophisticated way to convert raw text into meaningful span-level features and scores, which are foundational for advanced NLP tasks such as coreference resolution.](images/893d481b97ec08b55f6f0c8177285196203ecf43d984bffc67fd589a66bb6617.jpg)
Fig. 23.5 shows the computation of the span representation and the mention score.   
Figure 23.5 Computation of the span representation $\mathbf { g }$ (and the mention score $\mathbf { m }$ ) in a BERT version of the e2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of say 10; the figure shows a small subset of the bigram and trigram spans.

# 23.6.2 Computing the mention and antecedent scores m and c

Now that we know how to compute the vector ${ \bf g } _ { i }$ for representing span $i$ , we can see the details of the two scoring functions $m ( i )$ and $c ( i , j )$ . Both are computed by feedforward networks:

$$
\begin{array} { r } { m ( i ) \ = \ w _ { m } \cdot \mathrm { F F N } _ { m } ( \mathbf { g } _ { i } ) \qquad } \\ { c ( i , j ) \ = \ w _ { c } \cdot \mathrm { F F N } _ { c } \big ( [ \mathbf { g } _ { i } , \mathbf { g } _ { j } , \mathbf { g } _ { i } \circ \mathbf { g } _ { j } , ] \big ) \qquad } \end{array}
$$

At inference time, this mention score $m$ is used as a filter to keep only the best few mentions.

We then compute the antecedent score for high-scoring mentions. The antecedent score $c ( i , j )$ takes as input a representation of the spans $i$ and $j$ , but also the elementwise similarity of the two spans to each other $\mathbf { g } _ { i } \circ \mathbf { g } _ { j }$ (here $\circ$ is element-wise multiplication). Fig. 23.6 shows the computation of the score $s$ for the three possible antecedents of the company in the example sentence from Fig. 23.5.

![## Image Analysis: 788327c8d2926692c7e1c6123e3aa5c2a8c5ee4d807c68467b44102f48292294.jpg

**Conceptual Understanding:**
This image conceptually represents a computational model, likely a neural network, for coreference resolution. Its main purpose is to illustrate the detailed step-by-step process of how 'coreference scores' (s) are calculated for a given 'mention' with respect to several 'potential antecedents', and how these scores are then transformed into a probability distribution over the possible antecedents. The image communicates the key idea that coreference resolution involves combining different types of information – representations of spans, scores for individual mentions, and scores for antecedent-mention pairs – in a structured, hierarchical manner to make a final decision about an entity's referent.

**Content Interpretation:**
The image depicts a computational model, likely a neural network, designed for coreference resolution in natural language processing. It illustrates the steps involved in calculating coreference scores (s) for a target mention, 'the company', against two potential antecedents, 'General Electric' and 'the Postal Service', as well as a dummy antecedent (ε). The model progresses through several layers: span representation (g), mention score (m), antecedent score (c), and finally, coreference score (s), which then feeds into a Softmax layer to yield a probability distribution. The diagram shows how individual word spans are represented, how scores for individual mentions are derived, how scores for antecedent-mention pairs are computed, and how these components are combined to determine a final coreference score for each potential antecedent. The presence of 's(the company, ε) = 0' indicates a mechanism to account for cases where the mention has no prior antecedent in the text.

**Key Insights:**
The main takeaway is the hierarchical and compositional nature of coreference score computation. The coreference score (s) for a mention-antecedent pair is not a single value but is derived from lower-level scores: individual span representations (g), mention scores (m) for the individual spans, and an antecedent score (c) for the pair. Specifically, 's(the company, General Electric)' is formed from inputs related to 'General Electric' and 'the company', and the 'c' score derived from them. The inclusion of 's(the company, ε) = 0' highlights the model's ability to handle cases where a mention does not refer to any previous entity, assigning a baseline score of zero for this 'null' antecedent. The final Softmax layer converts these raw scores into a probability distribution, indicating the model's confidence in each antecedent's likelihood. This provides a clear understanding of the components and their integration in a modern coreference resolution system.

**Document Context:**
This image is highly relevant to the document's section '23.6.2 Computing the mention and antecedent scores m and c'. It visually explains the intricate process of how a coreference resolution model calculates the `m` (mention score) and `c` (antecedent score) as components leading to the `s` (coreference score). The figure directly supports the textual explanation by illustrating the architecture and data flow for these score computations, using specific examples ('the company', 'General Electric', 'the Postal Service') as mentioned in the context. It shows the three possible antecedents for 'the company' and how their scores are computed to determine the final probability of coreference. The reference to Lee et al. (2017b) further grounds the model in current research.

**Summary:**
The image illustrates a neural network architecture for computing coreference scores and probabilities for a given mention. It details the hierarchical computation from span representations to final softmax probabilities. The process starts with individual span representations for potential antecedents and the target mention. These representations feed into mention scores, which then combine to form antecedent scores for specific antecedent-mention pairs. These antecedent scores, along with individual mention scores, contribute to the final coreference scores. A special coreference score is also calculated for a dummy antecedent. Finally, these coreference scores, along with context, are fed into a softmax layer to output the probability distribution over possible antecedents for the target mention. The flow demonstrates how the model evaluates the likelihood of a specific mention, 'the company', referring to 'General Electric', 'the Postal Service', or a null antecedent.](images/788327c8d2926692c7e1c6123e3aa5c2a8c5ee4d807c68467b44102f48292294.jpg)
Figure 23.6 The computation of the score $s$ for the three possible antecedents of the company in the example sentence from Fig. 23.5. Figure after Lee et al. (2017b).

Given the set of mentions, the joint distribution of antecedents for each document is computed in a forward pass, and we can then do transitive closure on the antecedents to create a final clustering for the document.

Fig. 23.7 shows example predictions from the model, showing the attention weights, which Lee et al. (2017b) find correlate with traditional semantic heads. Note that the model gets the second example wrong, presumably because attendants and pilot likely have nearby word embeddings.

<table><tr><td>We are looking for (a region of central Italy bordering the Adriatic Sea). (The area) is mostly mountainous and includes Mt.Corno,the highest peak of the Apennines.(It) also includes a lot of sheep,good clean-living,healthy sheep,and an Italian entrepreneur has an idea about how to make a little money of them.</td></tr><tr><td>(The flight attendants) have until 6:OO today to ratify labor concessions. (The pilots&#x27;) union and ground crew did so yesterday.</td></tr></table>

Figure 23.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per example, showing one correct example and one mistake. Bold, parenthesized spans are mentions in the predicted cluster. The amount of red color on a word indicates the head-finding attention weight $a _ { i , t }$ in Eq. 23.52. Figure adapted from Lee et al. (2017b).

# 23.6.3 Learning

For training, we don’t have a single gold antecedent for each mention; instead the coreference labeling only gives us each entire cluster of coreferent mentions; so a mention only has a latent antecedent. We therefore use a loss function that maximizes the sum of the coreference probability of any of the legal antecedents. For a given mention $i$ with possible antecedents $Y ( i )$ , let $\mathrm { G O L D } ( i )$ be the set of mentions in the gold cluster containing $i .$ . Since the set of mentions occurring before $i$ is $Y ( i )$ , the set of mentions in that gold cluster that also occur before $i$ is $Y ( i ) \cap \mathrm { G O L D } ( i )$ . We

therefore want to maximize:

$$
\sum _ { \hat { y } \in Y ( i ) \cap \mathrm { G O L D } ( i ) } P ( \hat { y } )
$$

If a mention $i$ is not in a gold cluster $\mathrm { \ G O L D } ( i ) = \epsilon$

To turn this probability into a loss function, we’ll use the cross-entropy loss function we defined in Eq. 5.23 in Chapter 5, by taking the $- \log$ of the probability. If we then sum over all mentions, we get the final loss function for training:

$$
L = \sum _ { i = 2 } ^ { N } - \log \sum _ { \hat { y } \in Y ( i ) \cap \mathrm { { G O L D } } ( i ) } P ( \hat { y } )
$$

# 23.7 Entity Linking

# entity linking

wikification

Entity linking is the task of associating a mention in text with the representation of some real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It is the natural follow-on to coreference resolution; coreference resolution is the task of associating textual mentions that corefer to the same entity. Entity linking takes the further step of identifying who that entity is. It is especially important for any NLP task that links to a knowledge base.

While there are all sorts of potential knowledge-bases, we’ll focus in this section on Wikipedia, since it’s widely used as an ontology for NLP tasks. In this usage, each unique Wikipedia page acts as the unique id for a particular entity. This task of deciding which Wikipedia page corresponding to an individual is being referred to by a text mention has its own name: wikification (Mihalcea and Csomai, 2007).

Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008), entity linking is done in (roughly) two stages: mention detection and mention disambiguation. We’ll give two algorithms, one simple classic baseline that uses anchor dictionaries and information from the Wikipedia graph structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al., 2020). We’ll focus here mainly on the application of entity linking to questions, since a lot of the literature has been in that context.

# 23.7.1 Linking based on Anchor Dictionaries and Web Graph

As a simple baseline we introduce the TAGME linker (Ferragina and Scaiella, 2011) for Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007, Cucerzan 2007, Milne and Witten 2008). Wikification algorithms define the set of entities as the set of Wikipedia pages, so we’ll refer to each Wikipedia page as a unique entity $e$ . TAGME first creates a catalog of all entities (i.e. all Wikipedia pages, removing some disambiguation and other meta-pages) and indexes them in a standard IR engine like Lucene. For each page $e$ , the algorithm computes an in-link count $\operatorname { i n } ( e )$ : the total number of in-links from other Wikipedia pages that point to $e$ . These counts can be derived from Wikipedia dumps.

Finally, the algorithm requires an anchor dictionary. An anchor dictionary lists for each Wikipedia page, its anchor texts: the hyperlinked spans of text on other pages that point to it. For example, the web page for Stanford University, http://www.stanford.edu, might be pointed to from another page using anchor texts like Stanford or Stanford University:

<a href="http://www.stanford.edu" $\mathrm { \Delta } >$ Stanford University $< / \mathsf { a } >$

We compute a Wikipedia anchor dictionary by including, for each Wikipedia page $e$ , $e$ ’s title as well as all the anchor texts from all Wikipedia pages that point to $e$ . For each anchor string $a$ we’ll also compute its total frequency freq $( a )$ in Wikipedia (including non-anchor uses), the number of times $a$ occurs as a link (which we’ll call $l i n k ( a ) )$ , and its link probability linkprob $\cdot ( a ) = \operatorname* { l i n k } ( a ) / \operatorname* { f r e q } ( a )$ . Some cleanup of the final anchor dictionary is required, for example removing anchor strings composed only of numbers or single characters, that are very rare, or that are very unlikely to be useful entities because they have a very low linkprob.

Mention Detection Given a question (or other text we are trying to link), TAGME detects mentions by querying the anchor dictionary for each token sequence up to 6 words. This large set of sequences is pruned with some simple heuristics (for example pruning substrings if they have small linkprobs). The question:

When was Ada Lovelace born?

might give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans like Lovelace might be pruned as having too low a linkprob, and but spans like born have such a low linkprob that they would not be in the anchor dictionary at all.

Mention Disambiguation If a mention span is unambiguous (points to only one entity/Wikipedia page), we are done with entity linking! However, many spans are ambiguous, matching anchors for multiple Wikipedia entities/pages. The TAGME algorithm uses two factors for disambiguating ambiguous spans, which have been referred to as prior probability and relatedness/coherence. The first factor is $p ( e | a )$ , the probability with which the span refers to a particular entity. For each page $e \in$ $\mathcal { E } \left( a \right)$ , the probability $p ( e | a )$ that anchor $a$ points to $e$ , is the ratio of the number of links into $e$ with anchor text $a$ to the total number of occurrences of $a$ as an anchor:

$$
\operatorname { p r i o r } ( a \to e ) ~ = ~ p ( e | a ) = { \frac { \operatorname { c o u n t } ( a \to e ) } { \operatorname { l i n k } ( a ) } }
$$

Let’s see how that factor works in linking entities in the following question:

What Chinese Dynasty came before the Yuan?

The most common association for the span Yuan in the anchor dictionary is the name of the Chinese currency, i.e., the probability $p$ (Yuan currency| yuan) is very high. Rarer Wikipedia associations for Yuan include the common Chinese last name, a language spoken in Thailand, and the correct entity in this case, the name of the Chinese dynasty. So if we chose based only on $p ( e | a )$ , we would make the wrong disambiguation and miss the correct link, Yuan dynasty.

To help in just this sort of case, TAGME uses a second factor, the relatedness of this entity to other entities in the input question. In our example, the fact that the question also contains the span Chinese Dynasty, which has a high probability link to the page Dynasties in Chinese history, ought to help match Yuan dynasty.

Let’s see how this works. Given a question $q$ , for each candidate anchors span $a$ detected in $q$ , we assign a relatedness score to each possible entity $e \in \mathcal { E } ( a )$ of $a$ . The relatedness score of the link $a \to e$ is the weighted average relatedness between $e$ and all other entities in $q$ . Two entities are considered related to the extent their Wikipedia pages share many in-links. More formally, the relatedness between two entities $A$ and $B$ is computed as

$$
\operatorname { r e l } ( A , B ) \ = \ { \frac { \log ( \operatorname* { m a x } ( \left| \operatorname { i n } ( A ) \right| , \left| \operatorname { i n } ( B ) \right| ) ) - \log ( \left| \operatorname { i n } ( A ) \cap \operatorname { i n } ( B ) \right| ) } { \log ( \left| W \right| ) - \log ( \operatorname* { m i n } ( \left| \operatorname { i n } ( A ) \right| , \left| \operatorname { i n } ( B ) \right| ) ) } }
$$

where $\mathrm { i n } ( x )$ is the set of Wikipedia pages pointing to $x$ and $W$ is the set of all Wikipedia pages in the collection.

The vote given by anchor $^ b$ to the candidate annotation $a \to X$ is the average, over all the possible entities of $^ b$ , of their relatedness to $X$ , weighted by their prior probability:

$$
\operatorname { v o t e } ( b , X ) = { \frac { 1 } { | { \mathcal { E } } ( b ) | } } \sum _ { Y \in { \mathcal { E } } ( b ) } \operatorname { r e l } ( X , Y ) p ( Y | b )
$$

The total relatedness score for $a \to X$ is the sum of the votes of all the other anchors detected in $q$ :

$$
\operatorname { r e l a t e d n e s s } ( a \to X ) = \sum _ { b \in { \mathcal { X } } _ { q } \setminus a } \operatorname { v o t e } ( b , X )
$$

To score $a \to X$ , we combine relatedness and prior by choosing the entity $X$ that has the highest relatedness $( a \to X )$ , finding other entities within a small $\epsilon$ of this value, and from this set, choosing the entity with the highest prior $P ( X | a )$ . The result of this step is a single entity assigned to each span in $q$ .

The TAGME algorithm has one further step of pruning spurious anchor/entity pairs, assigning a score averaging link probability with the coherence.

$$
{ \begin{array} { r c h e r e n c e } { ( a  X ) = { \frac { 1 } { | S | - 1 } } \displaystyle \sum _ { B \in { \mathcal { S } } \backslash X } \mathrm { r e l } ( B , X ) } \\ { \operatorname { s c o r e } ( a  X ) = { \frac { \operatorname { c o h e r e n c e } ( a  X ) + \operatorname { l i n k p r o b } ( a ) } { 2 } } } \end{array} }
$$

Finally, pairs are pruned if score $( a  X ) < \lambda$ , where the threshold $\lambda$ is set on a held-out set.

# 23.7.2 Neural Graph-based linking

More recent entity linking models are based on bi-encoders, encoding a candidate mention span, encoding an entity, and computing the dot product between the encodings. This allows embeddings for all the entities in the knowledge base to be precomputed and cached (Wu et al., 2020). Let’s sketch the ELQ linking algorithm of Li et al. (2020), which is given a question $q$ and a set of candidate entities from Wikipedia with associated Wikipedia text, and outputs tuples $\left( e , m _ { s } , m _ { e } \right)$ of entity id, mention start, and mention end. As Fig. 23.8 shows, it does this by encoding each Wikipedia entity using text from Wikipedia, encoding each mention span using text from the question, and computing their similarity, as we describe below.

Entity Mention Detection To get an $h$ -dimensional embedding for each question token, the algorithm runs the question through BERT in the normal way:

$$
[ \mathbf { q } _ { 1 } \cdots \mathbf { q } _ { n } ] = \mathrm { B E R T } ( [ \mathbf { C L S } ] q _ { 1 } \cdots q _ { n } [ \mathbf { S E P } ] )
$$

It then computes the likelihood of each span $[ i , j ]$ in $q$ being an entity mention, in a way similar to the span-based algorithm we saw for the reader above. First we compute the score for $i / j$ being the start/end of a mention:

$$
s _ { \mathrm { s t a r t } } ( i ) = { \bf w } _ { \mathrm { s t a r t } } \cdot { \bf q } _ { i } , s _ { \mathrm { e n d } } ( j ) = { \bf w } _ { \mathrm { e n d } } \cdot { \bf q } _ { j } ,
$$

![## Image Analysis: 8c2b88d27918732404c2cbd704504521e6f2048ebf90d499c7f1a2e264bcdb7a.jpg

**Conceptual Understanding:**
This image conceptually represents a simplified inference process within an entity linking algorithm, specifically the ELQ algorithm, for linking entities found in natural language questions to knowledge base entities. The main purpose is to illustrate how a given question and a candidate entity description are processed to determine two key probabilities: first, whether a specific span in the question is an entity mention (e.g., 'shaq' in 'When did shaq come to the nba?'), and second, whether a candidate entity (e.g., 'Shaquille O'Neal') is the correct link for that mention, given the question. It communicates the idea of encoding textual inputs (question and entity description) into vector representations and then using classifiers and similarity measures (like an inner product) to make these crucial linking decisions in a neural network framework.

**Content Interpretation:**
The image depicts the core components and data flow for entity linking. It shows two main encoders: a "Question Encoder" and an "Entity Encoder," which transform the input question and a candidate entity description into vector embeddings. The specific textual elements such as "Q : When did shaq come to the nba?" and "[CLS] Shaquille O'Neal is an former American professional basketball player ..." serve as explicit examples of the inputs. The light blue bars visually represent these vector embeddings, with highlighted bars indicating specific token embeddings being used in subsequent steps.

The process branches into two main tasks:
1.  **Mention Detection:** The "Mention classifier + scorer" takes the embedding of a potential mention ("shaq" from the question) to compute "P('shaq' is a mention|Q)". This signifies the algorithm's ability to identify relevant entity spans within the input question.
2.  **Entity Disambiguation/Linking:** An "Inner product" operation calculates the similarity between the mention's embedding ("shaq") and the candidate entity's contextual embedding (the "[CLS]" token's embedding from "Shaquille O'Neal"). This similarity score is then used to determine "P("Shaquille O'Neal"|Q, 'shaq' is a mention)", representing the probability of linking the identified mention to the specific candidate entity. The inner product is a standard method for measuring the relatedness of two learned representations in embedding spaces.

**Key Insights:**
The main takeaways from this image regarding the ELQ algorithm's inference process are:
1.  **Dual Encoding:** Both the question and the candidate entity's descriptive text are separately encoded into rich vector representations using dedicated "Question Encoder" and "Entity Encoder" components. This is evident from the two distinct encoder blocks and their respective inputs, which are tokenized into elements like "[CLS]", "When", "did", "shaq", and "[CLS]", "Shaquille", "O'Neal", etc.
2.  **Two-Stage Probabilistic Prediction:** The algorithm makes two types of probabilistic predictions:
    *   **Mention Probability:** It first determines the likelihood that a specific span in the question (e.g., 'shaq') is a mention, indicated by "P('shaq' is a mention|Q)", which is derived from the "Mention classifier + scorer" processing the 'shaq' embedding.
    *   **Entity Linking Probability:** It then calculates the probability of linking a given mention to a specific candidate entity (e.g., "Shaquille O'Neal") in the context of the question, represented by "P("Shaquille O'Neal"|Q, 'shaq' is a mention)". This linking is achieved through a similarity calculation, specifically an "Inner product" between the mention's embedding and the entity's embedding.
3.  **Embedding-based Similarity:** The explicit use of an "Inner product" between the highlighted 'shaq' embedding from the question and the '[CLS]' embedding from the entity description implies that the core of the linking process relies on computing the similarity between their learned vector representations. A higher similarity indicates a higher probability of a correct entity link.

**Document Context:**
This image is directly relevant to Section 23.7.2, titled "Neural Graph-based linking," and specifically illustrates the "inference process in the ELQ algorithm for entity linking in questions (Li et al., 2020)," as indicated by the document context. It provides a detailed visual breakdown of how the ELQ algorithm identifies and links entities within natural language questions, supporting the broader discussion on neural graph-based entity linking methods. The diagram clarifies the computational steps involved in mapping textual mentions to knowledge base entities, which is a fundamental task in natural language understanding.

**Summary:**
The diagram illustrates the core inference steps of the ELQ (Entity Linking in Questions) algorithm. The process begins with two main inputs: a question, exemplified as "Q : When did shaq come to the nba?", and a descriptive text for a candidate entity, such as "[CLS] Shaquille O'Neal is an former American professional basketball player ...".

First, both the question and the entity description undergo an encoding process. The "Question Encoder" takes the question, tokenizes it into individual words or sub-word units (e.g., "[CLS]", "When", "did", "shaq", "come", "to", "the", "nba", "?"), and converts each into a numerical vector representation (embedding), shown as light blue bars. Similarly, the "Entity Encoder" processes the candidate entity's description, tokenizing it (e.g., "[CLS]", "Shaquille", "O'Neal", "is", "an", "former", "American", "professional", "basketball", "player", "...") and generating corresponding embeddings. The "[CLS]" token embedding typically represents the overall context or meaning of the entire input sequence.

From the encoded question, the embedding corresponding to a potential entity mention, in this case, "shaq", is extracted and highlighted. This "shaq" embedding follows two parallel paths:

1.  **Mention Classification:** It is fed into a "Mention classifier + scorer". This component evaluates the likelihood that 'shaq' is indeed an entity mention within the question, outputting a probability labeled as "P('shaq' is a mention|Q)". This step aims to identify which parts of the question are referring to entities.

2.  **Entity Linking (Similarity Calculation):** Simultaneously, the "shaq" embedding from the question is combined with the "[CLS]" embedding from the "Entity Encoder" (representing the overall concept of "Shaquille O'Neal"). These two embeddings are passed to an "Inner product" module. The inner product is a mathematical operation that measures the similarity between two vectors. The result of this similarity calculation then contributes to determining the probability "P("Shaquille O'Neal"|Q, 'shaq' is a mention)". This final probability indicates how likely it is that "Shaquille O'Neal" is the correct entity to be linked to the mention 'shaq' in the context of the given question.

In essence, the ELQ algorithm first identifies potential entity mentions within a question and then, for each potential mention, evaluates its similarity to various candidate entities to determine the most probable link, all through sophisticated neural network encoders and scoring mechanisms.](images/8c2b88d27918732404c2cbd704504521e6f2048ebf90d499c7f1a2e264bcdb7a.jpg)
Figure 23.8 A sketch of the inference process in the ELQ algorithm for entity linking in questions (Li et al., 2020). Each candidate question mention span and candidate entity are separately encoded, and then scored by the entity/span dot product.

where ${ \pmb w } _ { \mathrm { s t a r t } }$ and $\boldsymbol { \mathsf { w } } _ { \mathrm { e n d } }$ are vectors learned during training. Next, another trainable embedding, $\pmb { \mathsf { w } } _ { \mathrm { m e n t i o n } }$ is used to compute a score for each token being part of a mention:

$$
s _ { \mathrm { m e n t i o n } } ( t ) = { \bf w } _ { \mathrm { m e n t i o n } } \cdot { \bf q } _ { t }
$$

Mention probabilities are then computed by combining these three scores:

$$
p ( [ i , j ] ) = \sigma \left( s _ { \mathrm { s t a r t } } ( i ) + s _ { \mathrm { e n d } } ( j ) + \sum _ { t = i } ^ { j } s _ { \mathrm { m e n t i o n } } ( t ) \right)
$$

Entity Linking To link mentions to entities, we next compute embeddings for each entity in the set $\mathcal { E } = e _ { 1 } , \cdots , e _ { i } , \cdots , e _ { w }$ of all Wikipedia entities. For each entity $e _ { i }$ we’ll get text from the entity’s Wikipedia page, the title $t ( e _ { i } )$ and the first 128 tokens of the Wikipedia page which we’ll call the description $d ( e _ { i } )$ . This is again run through BERT, taking the output of the CLS token BERT[CLS] as the entity representation:

$$
\begin{array} { r } { \mathbf { x } _ { e _ { i } } = \mathrm { B E R T } _ { [ \mathrm { C L S } ] } ( [ \mathrm { C L S } ] t ( e _ { i } ) [ \mathrm { E N T } ] d ( e _ { i } ) [ \mathrm { S E P } ] ) } \end{array}
$$

Mention spans can be linked to entities by computing, for each entity $e$ and span $[ i , j ]$ , the dot product similarity between the span encoding (the average of the token embeddings) and the entity encoding.

$$
\begin{array} { r } { \mathbf { y } _ { i , j } = \displaystyle \frac { 1 } { \left( j - i + 1 \right) } \sum _ { t = i } ^ { j } \mathbf { q } _ { t } } \\ { s ( e , [ i , j ] ) = \mathbf { x } _ { e } \mathbf { y } _ { i , j } } \end{array}
$$

Finally, we take a softmax to get a distribution over entities for each span:

$$
p ( e | [ i , j ] ) = \frac { \exp ( s ( e , [ i , j ] ) ) } { \sum _ { e ^ { \prime } \in \mathcal { E } } \exp ( s ( e ^ { \prime } , [ i , j ] ) ) }
$$

Training The ELQ mention detection and entity linking algorithm is fully supervised. This means, unlike the anchor dictionary algorithms from Section 23.7.1, it requires datasets with entity boundaries marked and linked. Two such labeled datasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions (Berant et al., 2013) dataset derived from Google search questions, and GraphQuestions (Su et al., 2016). Both have had entity spans in the questions marked and linked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled versions ${ \mathrm { W e b Q S P } } _ { \mathrm { E L } }$ and GraphQEL (Li et al., 2020).

Given a training set, the ELQ mention detection and entity linking phases are trained jointly, optimizing the sum of their losses. The mention detection loss is a binary cross-entropy loss, with $L$ the length of the passage and $N$ the number of candidates:

$$
{ \mathcal { L } } _ { \mathrm { M D } } = - { \frac { 1 } { N } } \sum _ { \substack { 1 \leq i \leq j \leq m i n ( i + L - 1 , n ) } } \left( y _ { [ i , j ] } \log p ( [ i , j ] ) + ( 1 - y _ { [ i , j ] } ) \log ( 1 - p ( [ i , j ] ) ) \right)
$$

with $y _ { [ i , j ] } = 1$ if $[ i , j ]$ is a gold mention span, else 0. The entity linking loss is:

$$
\mathcal { L } _ { \mathrm { E D } } = - l o g p ( e _ { g } | [ i , j ] )
$$

where $e _ { g }$ is the gold entity for mention $[ i , j ]$ .

# 23.8 Evaluation of Coreference Resolution

We evaluate coreference algorithms model-theoretically, comparing a set of hypothesis chains or clusters $H$ produced by the system against a set of gold or reference chains or clusters $R$ from a human labeling, and reporting precision and recall.

However, there are a wide variety of methods for doing this comparison. In fact, there are 5 common metrics used to evaluate coreference algorithms: the link based MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014) metrics, the mention based $B ^ { 3 }$ metric (Bagga and Baldwin, 1998), the entity based CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and Strube, 2016).

Let’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995) is based on the number of coreference links (pairs of mentions) common to $H$ and $R$ . Precision is the number of common links divided by the number of links in $H$ . Recall is the number of common links divided by the number of links in $R$ ; This makes MUC biased toward systems that produce large chains (and fewer entities), and it ignores singletons, since they don’t involve links.

$\mathbf { B } ^ { 3 }$ is mention-based rather than link-based. For each mention in the reference chain, we compute a precision and recall, and then we take a weighted sum over all $N$ mentions in the document to compute a precision and recall for the entire task. For a given mention $i$ , let $R$ be the reference chain that includes $i .$ , and $H$ the hypothesis chain that has $i$ . The set of correct mentions in $H$ is $H \cap R$ . Precision for mention i is thus $\frac { | H \cap R | } { | H | }$ , and recall for mention $i$ thus $\frac { | H \cap R | } { | R | }$ . The total precision is the weighted sum of the precision for mention $i$ , weighted by a weight $w _ { i }$ . The total recall is the weighted sum of the recall for mention $i$ , weighted by a weight $w _ { i }$ . Equivalently:

$$
\begin{array} { r l } { \mathrm { P r e c i s i o n ~ } = } & { \sum _ { i = 1 } ^ { N } w _ { i } \frac { \mathcal { H } \ o f c o r r e c t m e n t i o n s ~ i n ~ h y p o t h e s i s ~ c h a i n ~ c o n t a i n i n g ~ e n t i t y _ { i } } { \mathcal { H } o f m e n t i o n s ~ i n ~ h y p o t h e s i s ~ c h a i n ~ c o n t a i n i n g ~ e n t i t y _ { i } } } \\ { \mathrm { R e c a l l } } & { = \sum _ { i = 1 } ^ { N } w _ { i } \frac { \mathcal { H } \ o f c o r r e c t m e n t i o n s ~ i n ~ h y p o t h e s i s ~ c h a i n ~ c o n t a i n i n g ~ e n t i t y _ { i } } { \mathcal { H } o f m e n t i o n s ~ i n ~ r e f e r e n c e ~ c h a i n ~ c o n t a i n i n g ~ e n t i t y _ { i } } } \end{array}
$$

The weight $w _ { i }$ for each entity can be set to different values to produce different versions of the algorithm.

Following a proposal from Denis and Baldridge (2009), the CoNLL coreference competitions were scored based on the average of MUC, CEAF-e, and ${ \mathbf B } ^ { 3 }$ (Pradhan et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed description of the entire set of metrics; reference implementations of these should be used rather than attempting to reimplement from scratch (Pradhan et al., 2014).

Alternative metrics have been proposed that deal with particular coreference domains or tasks. For example, consider the task of resolving mentions to named entities (persons, organizations, geopolitical entities), which might be useful for information extraction or knowledge base completion. A hypothesis chain that correctly contains all the pronouns referring to an entity, but has no version of the name itself, or is linked with a wrong name, is not useful for this task. We might instead want a metric that weights each mention by how informative it is (with names being most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to match a gold chain only if it contains at least one variant of a name (the NEC F1 metric of Agarwal et al. (2019)).

# 23.9 Winograd Schema problems

From early on in the field, researchers have noted that some cases of coreference are quite difficult, seeming to require world knowledge or sophisticated reasoning to solve. The problem was most famously pointed out by Winograd (1972) with the following example:

(23.72) The city council denied the demonstrators a permit because

a. they feared violence.   
b. they advocated violence.

Winograd noticed that the antecedent that most readers preferred for the pronoun they in continuation (a) was the city council, but in (b) was the demonstrators. He suggested that this requires understanding that the second clause is intended as an explanation of the first clause, and also that our cultural frames suggest that city councils are perhaps more likely than demonstrators to fear violence and that demonstrators might be more likely to advocate violence.

In an attempt to get the field of NLP to focus more on methods involving world knowledge and common-sense reasoning, Levesque (2011) proposed a challenge task called the Winograd Schema Challenge.8 The problems in the challenge task are coreference problems designed to be easily disambiguated by the human reader, but hopefully not solvable by simple techniques such as selectional restrictions, or other basic word association methods.

The problems are framed as a pair of statements that differ in a single word or phrase, and a coreference question:

(23.73) The trophy didn’t fit into the suitcase because it was too large. Question: What was too large? Answer: The trophy

(23.74) The trophy didn’t fit into the suitcase because it was too small.

Question: What was too small? Answer: The suitcase

The problems have the following characteristics:

1. The problems each have two parties   
2. A pronoun preferentially refers to one of the parties, but could grammatically also refer to the other   
3. A question asks which party the pronoun refers to   
4. If one word in the question is changed, the human-preferred answer changes to the other party

The kind of world knowledge that might be needed to solve the problems can vary. In the trophy/suitcase example, it is knowledge about the physical world; that a bigger object cannot fit into a smaller object. In the original Winograd sentence, it is stereotypes about social actors like politicians and protesters. In examples like the following, it is knowledge about human actions like turn-taking or thanking.

(23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose turn was [over/next]? Answers: Bill/John   
(23.76) Joan made sure to thank Susan for all the help she had [given/received]. Who had [given/received] help? Answers: Susan/Joan.

Although the Winograd Schema was designed to require common-sense reasoning, a large percentage of the original set of problems can be solved by pretrained language models, fine-tuned on Winograd Schema sentences (Kocijan et al., 2019). Large pretrained language models encode an enormous amount of world or common-sense knowledge! The current trend is therefore to propose new datasets with increasingly difficult Winograd-like coreference resolution problems like KNOWREF (Emami et al., 2019), with examples like:

(23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the gap wasn’t all that big.

In the end, it seems likely that some combination of language modeling and knowledge will prove fruitful; indeed, it seems that knowledge-based models overfit less to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),

# 23.10 Gender Bias in Coreference

As with other aspects of language processing, coreference models exhibit gender and other biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018). For example the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Winograd Schema paradigm to test the extent to which coreference algorithms are biased toward linking gendered pronouns with antecedents consistent with cultural stereotypes. As we summarized in Chapter 6, embeddings replicate societal biases in their training test, such as associating men with historically sterotypical male occupations like doctors, and women with stereotypical female occupations like secretaries (Caliskan et al. 2017, Garg et al. 2018).

A WinoBias sentence contain two mentions corresponding to stereotypicallymale and stereotypically-female occupations and a gendered pronoun that must be linked to one of them. The sentence cannot be disambiguated by the gender of the pronoun, but a biased model might be distracted by this cue. Here is an example sentence:

(23.78) The secretary called the physiciani and told $\mathrm { h i m } _ { i }$ about a new patient [pro-stereotypical]   
(23.79) The secretary called the physiciani and told heri about a new patient [anti-stereotypical]

Zhao et al. (2018a) consider a coreference system to be biased if it is more accurate at linking pronouns consistent with gender stereotypical occupations (e.g., him with physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical occupations (e.g., her with physician in (23.79)). They show that coreference systems of all architectures (rule-based, feature-based machine learned, and end-toend-neural) all show significant bias, performing on average $2 1 \ \mathrm { F } _ { 1 }$ points worse in the anti-stereotypical cases.

One possible source of this bias is that female entities are significantly underrepresented in the OntoNotes dataset, used to train most coreference systems. Zhao et al. (2018a) propose a way to overcome this bias: they generate a second gender-swapped dataset in which all male entities in OntoNotes are replaced with female ones and vice versa, and retrain coreference systems on the combined original and swapped OntoNotes data, also using debiased GloVE embeddings (Bolukbasi et al., 2016). The resulting coreference systems no longer exhibit bias on the WinoBias dataset, without significantly impacting OntoNotes coreference accuracy. In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo contextualized word vector representations and coref systems that use them. They showed that retraining ELMo with data augmentation again reduces or removes bias in coreference systems on WinoBias.

Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered Pronoun Resolution as a tool for developing improved coreference algorithms for gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences with gendered ambiguous pronouns (by contrast, only $20 \%$ of the gendered pronouns in the English OntoNotes training data are feminine). The examples were created by drawing on naturally occurring sentences from Wikipedia pages to create hard to resolve cases with two named entities of the same gender and an ambiguous pronoun that may refer to either person (or neither), like the following:

(23.80) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving back from Karuizawa to Kitami where she had spent her junior days.

Webster et al. (2018) show that modern coreference algorithms perform significantly worse on resolving feminine pronouns than masculine pronouns in GAP. Kurita et al. (2019) shows that a system based on BERT contextualized word representations shows similar bias.

# 23.11 Summary

This chapter introduced the task of coreference resolution.

• This is the task of linking together mentions in text which corefer, i.e. refer to the same discourse entity in the discourse model, resulting in a set of coreference chains (also called clusters or entities).   
• Mentions can be definite NPs or indefinite NPs, pronouns (including zero pronouns) or names.   
• The surface form of an entity mention is linked to its information status (new, old, or inferrable), and how accessible or salient the entity is.   
• Some NPs are not referring expressions, such as pleonastic $i t$ in It is raining.   
• Many corpora have human-labeled coreference annotations that can be used for supervised learning, including OntoNotes for English, Chinese, and Arabic, ARRAU for English, and AnCora for Spanish and Catalan.   
• Mention detection can start with all nouns and named entities and then use anaphoricity classifiers or referentiality classifiers to filter out non-mentions.   
• Three common architectures for coreference are mention-pair, mention-rank, and entity-based, each of which can make use of feature-based or neural classifiers.   
• Modern coreference systems tend to be end-to-end, performing mention detection and coreference in a single end-to-end architecture.   
• Algorithms learn representations for text spans and heads, and learn to compare anaphor spans with candidate antecedent spans.   
• Entity linking is the task of associating a mention in text with the representation of some real-world entity in an ontology .   
• Coreference systems are evaluated by comparing with gold entity labels using precision/recall metrics like MUC, $\mathbf { B } ^ { 3 }$ , CEAF, BLANC, or LEA.   
• The Winograd Schema Challenge problems are difficult coreference problems that seem to require world knowledge or sophisticated reasoning to solve.   
• Coreference systems exhibit gender bias which can be evaluated using datasets like Winobias and GAP.

# Bibliographical and Historical Notes

Coreference has been part of natural language processing since the 1970s (Woods et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation of coreference was formulated by Karttunen (1969) (at the 3rd COLING conference), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But it was Bonnie Webber’s 1978 dissertation and following work (Webber 1983) that explored the model’s computational aspects, providing fundamental insights into how entities are represented in the discourse model and the ways in which they can license subsequent reference. Many of the examples she provided continue to challenge theories of reference to this day.

The Hobbs algorithm9 is a tree-search algorithm that was the first in a long series of syntax-based methods for identifying reference robustly in naturally occurring text. The input to the Hobbs algorithm is a pronoun to be resolved, together with a syntactic (constituency) parse of the sentences up to and including the current sentence. The details of the algorithm depend on the grammar used, but can be understood from a simplified version due to Kehler et al. (2004) that just searches through the list of NPs in the current and prior sentences. This simplified Hobbs algorithm searches NPs in the following order: “(i) in the current sentence from right-to-left, starting with the first NP to the left of the pronoun, (ii) in the previous sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in the current sentence from left-to-right, starting with the first noun group to the right of the pronoun (for cataphora). The first noun group that agrees with the pronoun with respect to number, gender, and person is chosen as the antecedent” (Kehler et al., 2004).

Lappin and Leass (1994) was an influential entity-based system that used weights to combine syntactic and other features, extended soon after by Kennedy and Boguraev (1996) whose system avoids the need for full syntactic parses.

Approximately contemporaneously centering (Grosz et al., 1995) was applied to pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of work followed focused on centering’s use in coreference (Kameyama 1986, Di Eugenio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler 1997a, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how centering can be integrated with coherence-driven theories of pronoun interpretation. See Chapter 24 for the use of centering in measuring discourse coherence.

Coreference competitions as part of the US DARPA-sponsored MUC conferences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC7 corpora), and set the tone for much later work, choosing to focus exclusively on the simplest cases of identity coreference (ignoring difficult cases like bridging, metonymy, and part-whole) and drawing the community toward supervised machine learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE evaluations produced labeled coreference corpora in English, Chinese, and Arabic that were widely used for model training and evaluation.

This DARPA work influenced the community toward supervised learning beginning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by $\mathrm { N g }$ and Cardie (2002b), and a series of machine learning models followed over the next 15 years. These often focused separately on pronominal anaphora resolution (Kehler et al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999, Ng and Cardie 2002b, $\mathrm { N g } 2 0 0 5 \mathrm { a }$ ) and definite NP reference (Poesio and Vieira 1998, Vieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff 1999, Bean and Riloff 2004, $\mathrm { N g }$ and Cardie 2002a, $\mathrm { N g } 2 0 0 4 ,$ ), or singleton detection (de Marneffe et al., 2015).

The move from mention-pair to mention-ranking approaches was pioneered by Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods, then extended by Denis and Baldridge (2008) who proposed to do ranking via a softmax over all prior mentions. The idea of doing mention detection, anaphoricity, and coreference jointly in a single end-to-end model grew out of the early proposal of $\mathrm { N g }$ (2005b) to use a dummy antecedent for mention-ranking, allowing ‘non-referential’ to be a choice for coreference classifiers, Denis and Baldridge’s 2007 joint system combining anaphoricity classifier probabilities with coreference probabilities, the Denis and Baldridge (2008) ranking model, and the Rahman and $\mathrm { N g }$ (2009) proposal to train the two models jointly with a single objective.

Simple rule-based systems for coreference returned to prominence in the 2010s, partly because of their ability to encode entity-based features in a high-precision way (Zhou et al. 2004b, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al. 2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an inability to deal with the semantics necessary to correctly handle cases of common noun coreference.

A return to supervised learning led to a number of advances in mention-ranking models which were also extended into neural architectures, for example using reinforcement learning to directly optimize coreference evaluation models Clark and Manning (2016a), doing end-to-end coreference all the way from span extraction (Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take advantage of global entity-level information (Clark and Manning 2016b, Wiseman et al. 2016, Lee et al. 2018).

Coreference is also related to the task of entity linking discussed in Chapter 14. Coreference can help entity linking by giving more possible surface forms to help link to the right Wikipedia page, and conversely entity linking can help improve coreference resolution. Consider this example from Hajishirzi et al. (2013):

(23.81) [Michael Eisner]1 and [Donald Tsang]2 announced the grand opening of [[Hong Kong]3 Disneyland $\mathrm { l } _ { 4 }$ yesterday. [Eisner]1 thanked [the President]2 and welcomed [fans]5 to [the park]4.

Integrating entity linking into coreference can help draw encyclopedic knowledge (like the fact that Donald Tsang is a president) to help disambiguate the mention the President. Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012) showed that such attributes extracted from Wikipedia pages could be used to build richer models of entity mentions in coreference. More recent research shows how to do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even jointly with named entity tagging as well (Durrett and Klein 2014).

The coreference task as we introduced it involves a simplifying assumption that the relationship between an anaphor and its antecedent is one of identity: the two coreferring mentions refer to the identical discourse referent. In real texts, the relationship can be more complex, where different aspects of a discourse referent can be neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an example of metonymy, in which the capital city Washington is used metonymically to refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):

(23.82) a strict interpretation of a policy requires The U.S. to notify foreign dictators of certain coup plots ... Washington rejected the bid ...   
(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New Year. In the South, everyone was celebrating New Year; to the North, it was a regular day.   
(23.84) In France, the president is elected for a term of seven years, while in the United States he is elected for a term of four years.

For further linguistic discussions of these complications of coreference see Pustejovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and Turner (2008), Versley (2008), and Barker (2010).

$\mathrm { N g } \left( 2 0 1 7 \right)$ offers a useful compact history of machine learning models in coreference resolution. There are three excellent book-length surveys of anaphora/coreference resolution, covering different time periods: Hirst (1981) (early work until about 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).

Andy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there are some remnants of Andy’s lovely prose still in this third-edition coreference chapter.

# CHAPTER 24 Discourse Coherence

And even in our wildest and most wandering reveries, nay in our very dreams, we shall find, if we reflect, that the imagination ran not altogether at adventures, but that there was still a connection upheld among the different ideas, which succeeded each other. Were the loosest and freest conversation to be transcribed, there would immediately be transcribed, there would immediately be observed something which connected it in all its transitions.

David Hume, An enquiry concerning human understanding, 1748

local global

Orson Welles’ movie Citizen Kane was groundbreaking in many ways, perhaps most notably in its structure. The story of the life of fictional media magnate Charles Foster Kane, the movie does not proceed in chronological order through Kane’s life. Instead, the film begins with Kane’s death (famously murmuring “Rosebud”) and is structured around flashbacks to his life inserted among scenes of a reporter investigating his death. The novel idea that the structure of a movie does not have to linearly follow the structure of the real timeline made apparent for 20th century cinematography the infinite possibilities and impact of different kinds of coherent narrative structures.

But coherent structure is not just a fact about movies or works of art. Like movies, language does not normally consist of isolated, unrelated sentences, but instead of collocated, structured, coherent groups of sentences. We refer to such a coherent structured group of sentences as a discourse, and we use the word coherence to refer to the relationship between sentences that makes real discourses different than just random assemblages of sentences. The chapter you are now reading is an example of a discourse, as is a news article, a conversation, a thread on social media, a Wikipedia page, and your favorite novel.

What makes a discourse coherent? If you created a text by taking random sentences each from many different sources and pasted them together, would that be a coherent discourse? Almost certainly not. Real discourses exhibit both local coherence and global coherence. Let’s consider three ways in which real discourses are locally coherent;

First, sentences or clauses in real discourses are related to nearby sentences in systematic ways. Consider this example from Hobbs (1979):

(24.1) John took a train from Paris to Istanbul. He likes spinach.

This sequence is incoherent because it is unclear to a reader why the second sentence follows the first; what does liking spinach have to do with train trips? In fact, a reader might go to some effort to try to figure out how the discourse could be coherent; perhaps there is a French spinach shortage? The very fact that hearers try to identify such connections suggests that human discourse comprehension involves the need to establish this kind of coherence.

By contrast, in the following coherent example:

(24.2) Jane took a train from Paris to Istanbul. She had to attend a conference.

the second sentence gives a REASON for Jane’s action in the first sentence. Structured relationships like REASON that hold between text units are called coherence relations, and coherent discourses are structured by many such coherence relations. Coherence relations are introduced in Section 24.1.

A second way a discourse can be locally coherent is by virtue of being “about” someone or something. In a coherent discourse some entities are salient, and the discourse focuses on them and doesn’t go back and forth between multiple entities. This is called entity-based coherence. Consider the following incoherent passage, in which the salient entity seems to wildly swing from John to Jenny to the piano store to the living room, back to Jenny, then the piano again:

(24.3) John wanted to buy a piano for his living room. Jenny also wanted to buy a piano. He went to the piano store. It was nearby. The living room was on the second floor. She didn’t find anything she liked. The piano he bought was hard to get up to that floor.

# Centering Theory

Entity-based coherence models measure this kind of coherence by tracking salient entities across a discourse. For example Centering Theory (Grosz et al., 1995), the most influential theory of entity-based coherence, keeps track of which entities in the discourse model are salient at any point (salient entities are more likely to be pronominalized or to appear in prominent syntactic positions like subject or object). In Centering Theory, transitions between sentences that maintain the same salient entity are considered more coherent than ones that repeatedly shift between entities. The entity grid model of coherence (Barzilay and Lapata, 2008) is a commonly used model that realizes some of the intuitions of the Centering Theory framework. Entity-based coherence is introduced in Section 24.3.

entity grid

Finally, discourses can be locally coherent by being topically coherent: nearby sentences are generally about the same topic and use the same or similar vocabulary to discuss these topics. Because topically coherent discourses draw from a single semantic field or topic, they tend to exhibit the surface property known as lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semantically related words in nearby sentences. For example, the fact that the words house, chimney, garret, closet, and window— all of which belong to the same semantic field— appear in the two sentences in (24.4), or that they share the identical word shingled, is a cue that the two are tied together as a discourse:

(24.4) Before winter I built a chimney, and shingled the sides of my house... I have thus a tight shingled and plastered house... with a garret and a closet, a large window on each side....

In addition to the local coherence between adjacent or nearby sentences, discourses also exhibit global coherence. Many genres of text are associated with particular conventional discourse structures. Academic articles might have sections describing the Methodology or Results. Stories might follow conventional plotlines or motifs. Persuasive essays have a particular claim they are trying to argue for, and an essay might express this claim together with a structured set of premises that support the argument and demolish potential counterarguments. We’ll introduce versions of each of these kinds of global coherence.

Why do we care about the local or global coherence of a discourse? Since coherence is a property of a well-written text, coherence detection plays a part in any task that requires measuring the quality of a text. For example coherence can help in pedagogical tasks like essay grading or essay quality measurement that are trying to grade how well-written a human essay is (Somasundaran et al. 2014, Feng et al. 2014, Lai and Tetreault 2018). Coherence can also help for summarization; knowing the coherence relationship between sentences can help know how to select information from them. Finally, detecting incoherent text may even play a role in mental health tasks like measuring symptoms of schizophrenia or other kinds of disordered language (Ditman and Kuperberg 2010, Elvevag et al. ˚ 2007, Bedi et al. 2015, Iter et al. 2018).

# 24.1 Coherence Relations

RST nucleus satellite

Recall from the introduction the difference between passages (24.5) and (24.6).

(24.5) Jane took a train from Paris to Istanbul. She likes spinach.   
(24.6) Jane took a train from Paris to Istanbul. She had to attend a conference.

The reason (24.6) is more coherent is that the reader can form a connection between the two sentences, in which the second sentence provides a potential REASON for the first sentences. This link is harder to form for (24.5). These connections between text spans in a discourse can be specified as a set of coherence relations. The next two sections describe two commonly used models of coherence relations and associated corpora: Rhetorical Structure Theory (RST), and the Penn Discourse TreeBank (PDTB).

# 24.1.1 Rhetorical Structure Theory

The most commonly used model of discourse organization is Rhetorical Structure Theory (RST) (Mann and Thompson, 1987). In RST relations are defined between two spans of text, generally a nucleus and a satellite. The nucleus is the unit that is more central to the writer’s purpose and that is interpretable independently; the satellite is less central and generally is only interpretable with respect to the nucleus. Some symmetric relations, however, hold between two nuclei.

Below are a few examples of RST coherence relations, with definitions adapted from the RST Treebank Manual (Carlson and Marcu, 2001).

Reason: The nucleus is an action carried out by an animate agent and the satellite is the reason for the nucleus.

(24.7) [NUC Jane took a train from Paris to Istanbul.] [SAT She had to attend a conference.]

Elaboration: The satellite gives additional information or detail about the situation presented in the nucleus.

(24.8) [NUC Dorothy was from Kansas.] [SAT She lived in the midst of the great Kansas prairies.]

Evidence: The satellite gives additional information or detail about the situation presented in the nucleus. The information is presented with the goal of convince the reader to accept the information presented in the nucleus.

(24.9) [NUC Kevin must be here.] [SAT His car is parked outside.]

Attribution: The satellite gives the source of attribution for an instance of reported speech in the nucleus.

(24.10) [SAT Analysts estimated] [NUC that sales at U.S. stores declined in the quarter, too]

List: In this multinuclear relation, a series of nuclei is given, without contrast or explicit comparison:

(24.11) [NUC Billy Bones was the mate; ] [NUC Long John, he was quartermaster]

RST relations are traditionally represented graphically; the asymmetric NucleusSatellite relation is represented with an arrow from the satellite to the nucleus:

![## Image Analysis: 59dada1a85056830bd5b326c491b20a40649b6d9d00f7c325eaa901ac884c337.jpg

**Conceptual Understanding:**
This image conceptually represents a rhetorical relationship between two propositions within the framework of Rhetorical Structure Theory (RST). Its main purpose is to visually illustrate how one statement (the satellite) can provide 'evidence' or justification for another statement (the nucleus). The image conveys the idea of an inferential link, where a conclusion is drawn based on a given piece of information or observation. It communicates the key concept of how coherence in text is built upon specific rhetorical relations that connect different parts of a discourse.

**Content Interpretation:**
This image illustrates a fundamental concept in Rhetorical Structure Theory (RST): the 'evidence' rhetorical relation. It demonstrates how one segment of text, the satellite, provides justification or grounds for believing another segment, the nucleus. In this specific diagram, the statement 'His car is parked outside' acts as the evidence (satellite) that supports the claim or conclusion 'Kevin must be here.' (nucleus). The curved arrow labeled 'evidence' explicitly connects the supporting statement to the statement being supported, indicating the direction of the rhetorical function.

**Key Insights:**
The main takeaway from this image is the clear demonstration of the 'evidence' rhetorical relation in action. It teaches that claims are often supported by specific pieces of information, and this support can be formally analyzed within a textual structure. The text 'His car is parked outside' provides the reason or 'evidence' for the deduction 'Kevin must be here.' This highlights how inferential reasoning is encoded in language and how RST identifies these logical connections. The diagram emphasizes that understanding these rhetorical links is essential for a deeper analysis of text meaning and coherence.

**Document Context:**
Given the document context 'Section: 24.1.1 Rhetorical Structure Theory', this image serves as a direct and concise example of a rhetorical relation. It helps readers understand the core mechanism of RST by presenting a practical instance of how two sentences can be rhetorically linked. Specifically, it illustrates the 'evidence' relation, a common type of rhetorical connection where one part of the text provides justification for another, thereby enhancing the coherence and persuasive force of a message. This diagram is crucial for comprehending how RST analyzes the hierarchical and functional organization of text.

**Summary:**
The image displays a diagram illustrating a rhetorical relationship between two text segments, commonly seen in Rhetorical Structure Theory (RST). On the left, a horizontal line segment has the text "Kevin must be here." positioned directly beneath it. On the right, another horizontal line segment has the text "His car is parked outside" beneath it. A curved arrow originates from the right line segment (specifically, above "His car is parked outside") and points to the left line segment (above "Kevin must be here."). This arrow is labeled with the word "evidence" positioned above its curve. A vertical line extends upwards from the left line segment, indicating that "Kevin must be here." is the nucleus or primary proposition being supported. The diagram visually represents that the statement "His car is parked outside" serves as evidence for the conclusion "Kevin must be here."](images/59dada1a85056830bd5b326c491b20a40649b6d9d00f7c325eaa901ac884c337.jpg)

We can also talk about the coherence of a larger text by considering the hierarchical structure between coherence relations. Figure 24.1 shows the rhetorical structure of a paragraph from Marcu (2000a) for the text in (24.12) from the Scientific American magazine.

(24.12) With its distant orbit–50 percent farther from the sun than Earth–and slim atmospheric blanket, Mars experiences frigid weather conditions. Surface temperatures typically average about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator and can dip to -123 degrees C near the poles. Only the midday sun at tropical latitudes is warm enough to thaw ice on occasion, but any liquid water formed in this way would evaporate almost instantly because of the low atmospheric pressure.

![## Image Analysis: daca74b45d5d0c0f0f6302fd832ad41593340c7c5f610ebfabcfd10ab445b74a.jpg

**Conceptual Understanding:**
This image represents a Discourse Tree, specifically a Rhetorical Structure Theory (RST) analysis, of a short scientific text about Mars. Conceptually, it illustrates how the meaning and coherence of a document are built through a hierarchical arrangement of text segments connected by specific rhetorical relations.

The main purpose of this image is to visually deconstruct the logical and persuasive organization of the given text. It shows how individual sentences or clauses (segments) contribute to a larger message, revealing the functional roles they play (e.g., providing background, offering evidence, elaborating, contrasting, or explaining).

Key ideas communicated include:
*   The systematic method of analyzing text structure using RST.
*   The interdependencies between different parts of a text.
*   The specific scientific facts about Mars's orbit, atmosphere, temperatures, and the behavior of water, and how these facts are structured to form a coherent explanation or argument.

**Content Interpretation:**
The image illustrates the rhetorical structure of a scientific text, detailing the causes and effects of Mars's frigid conditions, specific temperature ranges, and the transient nature of liquid water on its surface. It demonstrates how different text segments are logically connected through rhetorical relations (e.g., background, evidence, elaboration, contrast, purpose, explanation-argumentative) to form a coherent and structured argument.

*   **Title (1) "Mars"**: The overarching subject of the discourse.
*   **Evidence (2-9):** The entire block of text from (2) to (9) collectively serves as "evidence" for a broader claim about Mars's environment.
    *   **Background (2-3):** Segment (2) "With its distant orbit <p> -- 50 percent farther from the sun than Earth -- </p> and slim atmospheric blanket," provides the "background" for the statement in (3) "Mars experiences frigid weather conditions."
    *   **Elaboration-Additional (4-9):** Segments (4) through (9) offer "elaboration-additional" details related to Mars's conditions.
        *   **List (4-5):** Segments (4) "Surface temperatures typically average about -60 degrees Celsius <p> (-76 degrees Fahrenheit)</p> at the equator" and (5) "and can dip to -123 degrees C near the poles." form a "List," providing specific temperature data.
        *   **Contrast (6-9):** Segments (6) through (9) establish a "Contrast" regarding the presence of liquid water.
            *   **Purpose (6-7):** Segment (7) "to thaw ice on occasion," states the "purpose" or outcome of the condition in (6) "Only the midday sun at tropical latitudes is warm enough."
            *   **Explanation-Argumentative (8-9):** Segment (9) "because of the low atmospheric pressure." provides an "explanation-argumentative" reason for the phenomenon described in (8) "but any liquid water formed in this way would evaporate almost instantly."

The diagram systematically deconstructs the text, revealing the author's intended organization and argumentative flow. It shows how specific facts (e.g., distant orbit, temperature ranges) are used to build arguments (e.g., Mars is frigid), and how details are added or contrasted to elaborate on these arguments. For instance, the fact that some thawing can occur is immediately contrasted with its impermanence, which is then explained.

**Key Insights:**
The discourse tree provides several key insights into both the structure of scientific writing and the characteristics of Mars:

*   **Rhetorical Structure of Scientific Text:** Scientific documents, even short passages, possess a discernible rhetorical structure that can be analyzed to understand the author's logical progression and argumentative strategy. (Evidenced by the entire diagram's existence and its labeled relations).
*   **Causes of Mars's Frigid Conditions:** Mars's distant orbit and slim atmospheric blanket are the fundamental reasons for its extremely cold weather. (Evidenced by Segment (2): "With its distant orbit <p> -- 50 percent farther from the sun than Earth -- </p> and slim atmospheric blanket," serving as "background" for Segment (3): "Mars experiences frigid weather conditions.").
*   **Extreme Martian Temperatures:** Surface temperatures on Mars are severely low, averaging about -60 degrees Celsius (-76 degrees Fahrenheit) at the equator and dropping to -123 degrees Celsius near the poles. (Evidenced by Segments (4) and (5), linked by the "List" relation).
*   **Transient Nature of Liquid Water on Mars:** Despite the possibility of ice thawing at tropical latitudes during midday, any resulting liquid water would evaporate almost immediately. (Evidenced by Segments (6), (7), and (8), connected by "purpose" and "Contrast" relations).
*   **Reason for Water Evaporation:** The primary cause for the rapid evaporation of liquid water on Mars is its low atmospheric pressure. (Evidenced by Segment (9): "because of the low atmospheric pressure." acting as an "explanation-argumentative" for Segment (8): "but any liquid water formed in this way would evaporate almost instantly").

These insights demonstrate how the text systematically builds an understanding of Mars's environment by presenting conditions, providing detailed data, introducing a contrasting observation, and then offering a scientific explanation.

**Document Context:**
This image is explicitly linked to Section 24.1.1, "Rhetorical Structure Theory," within the document. It serves as a practical, concrete example of a discourse tree, demonstrating the application of RST principles to analyze the rhetorical organization of a scientific text from 'Scientific American'. The analysis from Marcu (2000a) helps illustrate how text segments are identified and how rhetorical relations between them are mapped to understand the hierarchical structure of a document. The note about asymmetric relations and curved arrows directly connects to the theoretical concepts discussed in the surrounding text, making the abstract theory tangible.

**Summary:**
This diagram, titled "Mars" (1), visually represents the rhetorical structure of a scientific text, breaking it down into nine distinct segments and showing their logical connections. This analysis uses Rhetorical Structure Theory (RST) to explain how different parts of the text function together to convey a cohesive message.

At a high level, the entire set of statements from (2) to (9) provides "evidence" to support a central idea about Mars. This evidence begins by establishing "background" (2) for a key fact (3):
*   **Segment (2):** "With its distant orbit <p> -- 50 percent farther from the sun than Earth -- </p> and slim atmospheric blanket," describes Mars's physical characteristics.
*   **Segment (3):** "Mars experiences frigid weather conditions." This is the main point for which (2) is the background.

Following this, an "elaboration-additional" section (segments 4-9) provides further details. This elaboration first uses a "List" (4-5) to describe the frigid temperatures:
*   **Segment (4):** "Surface temperatures typically average about -60 degrees Celsius <p> (-76 degrees Fahrenheit)</p> at the equator" specifies average temperatures.
*   **Segment (5):** "and can dip to -123 degrees C near the poles." This adds information about temperature extremes.

The elaboration then shifts to a "Contrast" (6-9) concerning water on Mars. This contrast highlights a specific "purpose" (6-7):
*   **Segment (6):** "Only the midday sun at tropical latitudes is warm enough" describes a condition.
*   **Segment (7):** "to thaw ice on occasion," states the purpose or consequence of that condition.

However, this potential for thawing is immediately contrasted by an underlying reality, which is then explained by an "explanation-argumentative" relationship (8-9):
*   **Segment (8):** "but any liquid water formed in this way would evaporate almost instantly" presents the counter-argument or reality.
*   **Segment (9):** "because of the low atmospheric pressure." This provides the argumentative reason, explaining *why* the liquid water evaporates instantly.

In essence, the diagram shows how the text progresses from setting the scene for Mars's cold environment, to detailing its extreme temperatures, and finally to explaining the transient nature of any liquid water due to atmospheric pressure, all through a series of rhetorically linked statements. The curved arrows, as noted in the context, indicate asymmetric relations where one segment (the satellite) supports or elaborates on another (the nucleus), thereby building a comprehensive and structured understanding of the text's message.](images/daca74b45d5d0c0f0f6302fd832ad41593340c7c5f610ebfabcfd10ab445b74a.jpg)
Figure 24.1 A discourse tree for the Scientific American text in (24.12), from Marcu (2000a). Note that asymmetric relations are represented with a curved arrow from the satellite to the nucleus.

The leaves in the Fig. 24.1 tree correspond to text spans of a sentence, clause or U phrase that are called elementary discourse units or EDUs in RST; these units can also be referred to as discourse segments. Because these units may correspond to arbitrary spans of text, determining the boundaries of an EDU is an important task for extracting coherence relations. Roughly speaking, one can think of discourse segments as being analogous to constituents in sentence syntax, and indeed as we’ll see in Section 24.2 we generally draw on parsing algorithms to infer discourse structure.

There are corpora for many discourse coherence models; the RST Discourse TreeBank (Carlson et al., 2001) is the largest available discourse corpus. It consists of 385 English language documents selected from the Penn Treebank, with full RST parses for each one, using a large set of 78 distinct relations, grouped into 16 classes. RST treebanks exist also for Spanish, German, Basque, Dutch and Brazilian Portuguese (Braud et al., 2017).

Now that we’ve seen examples of coherence, we can see more clearly how a coherence relation can play a role in summarization or information extraction. For example, the nuclei of a text presumably express more important information than the satellites, which might be dropped in a summary.

# 24.1.2 Penn Discourse TreeBank (PDTB)

# PDTB

The Penn Discourse TreeBank (PDTB) is a second commonly used dataset that embodies another model of coherence relations (Miltsakaki et al. 2004, Prasad et al. 2008, Prasad et al. 2014). PDTB labeling is lexically grounded. Instead of asking annotators to directly tag the coherence relation between text spans, they were given a list of discourse connectives, words that signal discourse relations, like because, although, when, since, or as a result. In a part of a text where these words marked a coherence relation between two text spans, the connective and the spans were then annotated, as in Fig. 24.13, where the phrase as a result signals a causal relationship between what PDTB calls Arg1 (the first two sentences, here in italics) and Arg2 (the third sentence, here in bold).

(24.13) Jewelry displays in department stores were often cluttered and uninspired. And the merchandise was, well, fake. As a result, marketers of faux gems steadily lost space in department stores to more fashionable rivals—cosmetics makers.   
(24.14) In July, the Environmental Protection Agency imposed a gradual ban on virtually all uses of asbestos. (implicit=as a result) By 1997, almost all remaining uses of cancer-causing asbestos will be outlawed.

Not all coherence relations are marked by an explicit discourse connective, and so the PDTB also annotates pairs of neighboring sentences with no explicit signal, like (24.14). The annotator first chooses the word or phrase that could have been its signal (in this case as a result), and then labels its sense. For example for the ambiguous discourse connective since annotators marked whether it is using a CAUSAL or a TEMPORAL sense.

The final dataset contains roughly 18,000 explicit relations and 16,000 implicit relations. Fig. 24.2 shows examples from each of the 4 major semantic classes, while Fig. 24.3 shows the full tagset.

Unlike the RST Discourse Treebank, which integrates these pairwise coherence relations into a global tree structure spanning an entire discourse, the PDTB does not annotate anything above the span-pair level, making no commitment with respect to higher-level discourse structure.

There are also treebanks using similar methods for other languages; (24.15) shows an example from the Chinese Discourse TreeBank (Zhou and Xue, 2015). Because Chinese has a smaller percentage of explicit discourse connectives than English (only $22 \%$ of all discourse relations are marked with explicit connectives, compared to $47 \%$ in English), annotators labeled this corpus by directly mapping pairs of sentences to 11 sense tags, without starting with a lexical discourse connector.

Figure 24.2 The four high-level semantic distinctions in the PDTB sense hierarchy   

<table><tr><td>Class</td><td>Type</td><td>Example</td></tr><tr><td>TEMPORAL</td><td></td><td>S YNCHRONoUs The parishioners of St. Michael and All Angels stop to chat at the church door, as members here always have. (Implicit while) In the tower, five men and women pull rhythmically on ropes attached to the same five bells that first sounded here in 1614.</td></tr><tr><td>CONTINGENCY REASON</td><td></td><td>Also unlike Mr. Ruder, Mr. Breeden appears to be in a position to get somewhere with his agenda. (implicit=because) As a for- mer White House aide who worked closely with Congress,</td></tr><tr><td>COMPARISON</td><td>CONTRAST</td><td>he is savvy in the ways of Washington. The U.S. wants the removal of what it perceives as barriers to investment; Japan denies there are real barriers.</td></tr><tr><td>EXPANSION</td><td>CONJUNCTION</td><td>Not only do the actors stand outside their characters and make it clear they are at odds with them, but they often literally stand on their heads.</td></tr></table>

<table><tr><td>Temporal</td><td>Comparison</td></tr><tr><td>· Asynchronous</td><td>· Contrast (Juxtaposition, Opposition)</td></tr><tr><td rowspan="3">·Synchronous (Precedence, Succession)</td><td>Pragmatic Contrast (Juxtaposition, Opposition)</td></tr><tr><td>· Concession (Expectation, Contra-expectation)</td></tr><tr><td>·Pragmatic Concession</td></tr><tr><td>Contingency</td><td>Expansion</td></tr><tr><td>· Cause (Reason, Result)</td><td>·Exception</td></tr><tr><td>· Pragmatic Cause (Justification)</td><td>· Instantiation</td></tr><tr><td>Present/Past, Factual Present/Past)</td><td>· Condition (Hypothetical, General, Unreal·Restatement (Specification,Equivalence,Generalization)</td></tr><tr><td>·Pragmatic Condition (Relevance,Implicit As-</td><td>· Alternative (Conjunction, Disjunction, Chosen Alterna-</td></tr><tr><td rowspan="2">sertion)</td><td>tive)</td></tr><tr><td>·List</td></tr></table>

Figure 24.3 The PDTB sense hierarchy. There are four top-level classes, 16 types, and 23 subtypes (not all types have subtypes). 11 of the 16 types are commonly used for implicit argument classification; the 5 types in italics are too rare in implicit labeling to be used.

(24.15) [Conn 为] [Arg2 推动图们江地区 发] ，[Arg1 韩国捐 一百万美元 设 了图们江发 基 ] 立 展 金“[In order to] [Arg2 promote the development of the Tumen River region], [Arg1 South Korea donated one million dollars to establish the Tumen River Development Fund].”

These discourse treebanks have been used for shared tasks on multilingual discourse parsing (Xue et al., 2016).

# 24.2 Discourse Structure Parsing

Given a sequence of sentences, how can we automatically determine the coherence relations between them? This task is often called discourse parsing (even though for PDTB we are only assigning labels to leaf spans and not building a full parse

tree as we do for RST).

# 24.2.1 EDU segmentation for RST parsing

RST parsing is generally done in two stages. The first stage, EDU segmentation, extracts the start and end of each EDU. The output of this stage would be a labeling like the following:

(24.16) [Mr. Rambo says]e1 [that a 3.2-acre proper $\mathrm { t y l } _ { \mathrm { e } 2 }$ [overlooking the San Fernando Valley]e3 [is priced at $\$ 4\mathrm { m i l l i o n l } _ { \mathrm { e } 4 }$ [because the late actor Erroll Flynn once lived there.]e5

Since EDUs roughly correspond to clauses, early models of EDU segmentation first ran a syntactic parser, and then post-processed the output. Modern systems generally use neural sequence models supervised by the gold EDU segmentation in datasets like the RST Discourse Treebank. Fig. 24.4 shows an example architecture simplified from the algorithm of Lukasik et al. (2020) that predicts for each token whether or not it is a break. Here the input sentence is passed through an encoder and then passed through a linear layer and a softmax to produce a sequence of 0s and 1, where 1 indicates the start of an EDU.

![## Image Analysis: 9d5beea87c2173bd52a341fd2614e723a0f5c65022348603bd457ab3a83910dc.jpg

**Conceptual Understanding:**
The image conceptually represents a deep learning model's architecture designed for the task of Elementary Discourse Unit (EDU) segmentation in natural language processing. Its main purpose is to illustrate the process by which a sequence of input words is processed through various neural network layers to predict, for each word, whether it marks the beginning of a new EDU. The key idea being communicated is the application of an encoder-based neural network (with linear and softmax layers) for token-level binary classification to identify discourse unit boundaries in text, which is fundamental for higher-level discourse analysis like Rhetorical Structure Theory parsing. The diagram shows how each word contributes to a decision about a discourse break.

**Content Interpretation:**
The image displays a neural network architecture designed for Elementary Discourse Unit (EDU) segmentation. It illustrates a sequential processing pipeline where individual words (tokens) are passed through an "ENCODER" to generate contextual representations. These representations are then processed by a "linear layer" and subsequently a "softmax" layer to produce a binary classification for each token, indicating the presence or absence of an "EDU break". The visual bar charts within the "softmax" layer suggest a probability distribution for the classification output. The output values "0" or "1" represent the final prediction for an EDU break at each token position. The specific example "Mr. Rambo says that ..." with a '1' output for "that" highlights the model's capability to identify discourse boundaries based on linguistic cues.

**Key Insights:**
The image conveys several key insights into EDU segmentation using neural networks:

1.  **Token-level Classification:** The model predicts EDU breaks at the individual word (token) level, as evidenced by the separate inputs ("Mr.", "Rambo", "says", "that", "...") and corresponding individual outputs ("0", "0", "0", "1") for "EDU break". This implies fine-grained segmentation.
2.  **Encoder for Contextual Understanding:** The presence of an "ENCODER" highlights that the model leverages contextual information from the input text to make its predictions. This is crucial for discourse unit identification, which often depends on the relationships between words and clauses rather than isolated tokens.
3.  **Multi-stage Prediction Pipeline:** The process involves a sequence of neural network layers: an "ENCODER", a "linear layer", and a "softmax" layer, indicating a standard deep learning architecture for classification tasks. Each layer contributes to refining the representation and making the final probabilistic decision.
4.  **Binary Output for Break Detection:** The "EDU break" output is binary (0 or 1), indicating a clear decision for each token position: 0 for no break, and 1 for a break. The example shows the model correctly identifying a common EDU boundary after "that", demonstrating its ability to learn linguistic patterns relevant to discourse segmentation.

**Document Context:**
This image directly supports Section 24.2.1, titled "EDU segmentation for RST parsing," by visually detailing a computational method for identifying Elementary Discourse Units. Accurate EDU segmentation is a foundational step for Rhetorical Structure Theory (RST) parsing, as RST builds rhetorical trees upon these basic units. The diagram demonstrates a neural network approach to automate this critical initial phase, providing a concrete example of how the abstract concept of EDU segmentation is implemented in a machine learning context. The prediction of '1' for "that" aligns with typical linguistic patterns where 'that'-clauses often initiate new discourse units, making this a relevant illustration for RST. The diagram explains how text input is transformed through various layers to yield EDU break predictions.

**Summary:**
This diagram illustrates a neural network architecture designed to predict the beginnings of Elementary Discourse Units (EDUs) from encoded text, a critical step in Rhetorical Structure Theory (RST) parsing. 

The process begins at the bottom with individual words, or tokens, from a sentence, such as "Mr.", "Rambo", "says", "that", and continues for subsequent words (indicated by "...").

1.  **ENCODER:** These input tokens are first fed into a central "ENCODER" component. The encoder processes these words, generating rich, contextualized numerical representations (embeddings) for each token, capturing their meaning within the sentence.
2.  **Linear Layer:** The output from the "ENCODER" for each individual token is then passed through a "linear layer". This layer performs a mathematical transformation on the encoded representations, preparing them for the final classification.
3.  **Softmax Layer:** Following the linear layer, the processed features for each token are fed into a "softmax" layer. This layer takes the raw scores and converts them into a probability distribution, which is visually suggested by the bar charts within the oval shapes. This distribution represents the likelihood of different outcomes (e.g., whether an EDU break occurs or not).
4.  **EDU Break Prediction:** Finally, based on the softmax output, a binary prediction is made for each token at the "EDU break" level. A "0" indicates that no EDU break is predicted after that word, while a "1" indicates that an EDU break is predicted, signifying the beginning of a new discourse unit.

In the provided example:
*   After "Mr.", the prediction is "0".
*   After "Rambo", the prediction is "0".
*   After "says", the prediction is "0".
*   However, after "that", the prediction is "1". This '1' indicates that the model identifies an EDU break after the word "that", suggesting that "Mr. Rambo says" forms one EDU, and the clause starting with "that" initiates a new EDU. This demonstrates the model's ability to identify grammatical and discourse boundaries crucial for text understanding and RST parsing. The overall diagram therefore provides a clear, step-by-step view of how a neural model can automatically segment text into EDUs.](images/9d5beea87c2173bd52a341fd2614e723a0f5c65022348603bd457ab3a83910dc.jpg)
Figure 24.4 Predicting EDU segment beginnings from encoded text.

# 24.2.2 RST parsing

Tools for building RST coherence structure for a discourse have long been based on syntactic parsing algorithms like shift-reduce parsing (Marcu, 1999). Many modern RST parsers since Ji and Eisenstein (2014) draw on the neural syntactic parsers we saw in Chapter 20, using representation learning to build representations for each span, and training a parser to choose the correct shift and reduce actions based on the gold parses in the training set.

We’ll describe the shift-reduce parser of $\mathrm { Y u }$ et al. (2018). The parser state consists of a stack and a queue, and produces this structure by taking a series of actions on the states. Actions include:

• shift: pushes the first EDU in the queue onto the stack creating a single-node subtree. • reduce(l,d): merges the top two subtrees on the stack, where $l$ is the coherence relation label, and $d$ is the nuclearity direction, $d \in \{ N N , N S , S N \}$ .

As well as the pop root operation, to remove the final tree from the stack.

Fig. 24.6 shows the actions the parser takes to build the structure in Fig. 24.5.

![## Image Analysis: aa8bc44b0852cdb06c8a9dd297a2012174e98d2696eedf7e2a99d2dc6c1aecc1.jpg

**Conceptual Understanding:**
This image conceptually represents a fragment of a discourse tree or a dependency graph as utilized in Rhetorical Structure Theory (RST) parsing. The main purpose of the diagram is to illustrate how elementary discourse units (EDUs) or text spans are segmented and connected through specific rhetorical relations to form a coherent discourse structure. It communicates the key ideas of hierarchical organization of text, the classification of functional relations between text segments, and the directed nature of these relationships (e.g., nucleus-satellite relationships where one segment elaborates or attributes information to another).

**Content Interpretation:**
The image displays a hierarchical and potentially non-projective representation of rhetorical relations between elementary discourse units (EDUs) or text spans, labeled as e1, e2, e3, and e4. These units are connected by directed arcs representing specific rhetorical relations: 'attr' and 'elab'. The 'attr' relation connects e1 to e2, suggesting that e1 serves an attributive role for e2. The 'elab' relation connects e3 to e4, indicating that e3 elaborates on e4. Additionally, a higher-level 'elab' relation connects a span ending at e2 to a span starting at e3, implying that the first span elaborates on the second. The horizontal lines at the top indicate broader discourse segments or spans within which these elementary units and their relations are nested, demonstrating how RST can represent complex textual structures.

**Key Insights:**
The main takeaway from this image is the visual representation of rhetorical relations in the context of RST parsing. It demonstrates that: 

1.  **Elementary Discourse Units (EDUs) or Spans:** Text is segmented into fundamental units (e.g., 'e1', 'e2', 'e3', 'e4'). 
2.  **Rhetorical Relations:** These units are linked by specific, labeled relations, such as 'attr' (attribution) and 'elab' (elaboration), which describe their functional connection. 
3.  **Hierarchical Structure:** Relations can exist at different levels, connecting individual EDUs (e.g., 'e1' to 'e2') or larger spans (e.g., the span ending at 'e2' to the span starting at 'e3'). This implies a hierarchical organization of discourse. 
4.  **Directionality:** The arrows indicate the direction of the relation, typically from a nucleus to a satellite, where the satellite provides additional information (like elaboration or attribution) for the nucleus. 

The specific text elements 'e1', 'e2', 'e3', 'e4', 'attr', and 'elab' explicitly provide evidence for these points by showing concrete examples of units and their connections.

**Document Context:**
This image is directly relevant to the section '24.2.2 RST parsing' as it provides a visual example of how Rhetorical Structure Theory (RST) represents discourse structure. It illustrates the fundamental components of RST parsing, namely elementary discourse units (e.g., e1, e2, e3, e4) and the rhetorical relations (e.g., 'attr', 'elab') that connect them. The diagram helps to explain the hierarchical and directed nature of these relations, showing how smaller units combine into larger spans and how different types of semantic connections are represented graphically in an RST tree or diagram. This visual aid is crucial for understanding the practical application and structural output of an RST parser.

**Summary:**
This diagram illustrates rhetorical relations between elementary discourse units (EDUs) or text spans, which is typical in Rhetorical Structure Theory (RST) parsing. It visually represents how different textual segments are connected through specific rhetorical relations. The diagram shows four elementary units, e1, e2, e3, and e4, which are connected by two distinct types of relations: 'attr' (attribution) and 'elab' (elaboration). 

Specifically, e1 is connected to e2 by an 'attr' relation, indicating that e1 attributes something to e2. Concurrently, e3 is connected to e4 by an 'elab' relation, suggesting that e3 elaborates on e4. A broader 'elab' relation spans across e2 and e3, connecting a segment that implicitly includes e2 (or a span ending at e2) to a segment that implicitly includes e3 (or a span starting at e3), indicating that the first segment elaborates on the second. The horizontal lines at the top represent larger spans, and the vertical lines extending downwards to e2 and e3 indicate that these units are part of larger, encompassing segments.](images/aa8bc44b0852cdb06c8a9dd297a2012174e98d2696eedf7e2a99d2dc6c1aecc1.jpg)

$e _ { 1 }$ : American Telephone & Telegraph Co. said it $e _ { 2 }$ : will lay off 75 to 85 technicians here , effective Nov. 1. $e _ { 3 }$ : The workers install , maintain and repair its private branch exchanges, $e _ { 4 }$ : which are large intracompany telephone networks.

ure 1: An eFigure 24.5 attrExample RST discourse tree, showing four EDUs. Figure from Yu et al. (2018).   

<table><tr><td>Step</td><td>Stack</td><td>Queue</td><td>Action</td><td>Relation</td></tr><tr><td>1</td><td>Q</td><td>e1,e2,e3,e4</td><td>SH</td><td>Q</td></tr><tr><td>2</td><td>e1</td><td>e2,e3,e4</td><td>SH</td><td>Q</td></tr><tr><td>3</td><td>e1,e2</td><td>e3,e4</td><td>RD(attr,SN)</td><td>Q</td></tr><tr><td>4</td><td>e1:2</td><td>e3,e4</td><td>SH</td><td>e1e2</td></tr><tr><td>5</td><td>e1:2, e3</td><td>e4</td><td>SH</td><td>e1e2</td></tr><tr><td>6</td><td>e1:2,e3,e4</td><td>Q</td><td>RD(elab,NS)</td><td>e1e2</td></tr><tr><td>7</td><td>e1:2, e3:4</td><td>Q</td><td>RD(elab,SN)</td><td>e1e,ee4</td></tr><tr><td>8</td><td>e1:4</td><td>Q</td><td>PR</td><td>e1e,ee4,e1:2e3:4</td></tr></table>

Figure 24.6 Parsing the example of Fig. 24.5 using a shift-reduce parser. Figure from Yu et al. (2018).

estigThe $\mathrm { Y u }$ the implicit syntax feature extraction approach for RST parsing. In a et al. (2018) uses an encoder-decoder architecture, where the encoder a transition-based neural model for this task, which is able to incorporate vario is an empty state, and the final state represents a full result. There arepresents the input span of words and EDUs using a hierarchical biLSTM. The e exploit hierarchical bi-directional LSTMs (Bi-LSTMs) to encode texts, and furthnsition system:first biLSTM layer represents the words inside an EDU, and the second represents ion-based model with dynamic oracle. Basethe EDU sequence. Given an input sentence $w _ { 1 } , w _ { 2 } , . . . , w _ { m }$ osed model, we study t, the words can be repreproposed implicit syntax features. We conduct experiments on a standard RST diwhich removes the first EDU in the queue onto the stack, forming a sinsented as usual (by static embeddings, combinations with character embeddings or Carlson et al., 2003). First, we evaluate the performance of our proposed transitiotags, or contextual embeddings) resulting in an input word representation sequence $\pmb { \times } _ { 1 } ^ { w } , \pmb { \times } _ { 2 } ^ { w } , . . . , \pmb { \times } _ { m } ^ { w }$ model is able to achieve strong performances after applyich merges the top two subtrees on the stack, whe. The result of the word-level biLSTM is then a sequence of $\mathbf { h } ^ { \boldsymbol { w } }$ dynaml is a dvalues:

$$
\begin{array} { r } { \mathbf { h } _ { 1 } ^ { w } , \mathbf { h } _ { 2 } ^ { w } , . . . , \mathbf { h } _ { m } ^ { w } \ = \ \mathrm { b i L S T M } ( \mathbf { x } _ { 1 } ^ { w } , \mathbf { x } _ { 2 } ^ { w } , . . . , \mathbf { x } _ { m } ^ { w } ) } \end{array}
$$

(Li et al., 2015  An EDU of span $w _ { s } , w _ { s + 1 } , . . . , w _ { t }$ will be released for public under the A      then has biLSTM output representation $\mathbf { h } _ { s } ^ { w } , \mathbf { h } _ { s + 1 } ^ { w } , . . . , \mathbf { h } _ { t } ^ { w }$ ack holds only one subtree and tand is represented by average pooling:

$$
\pmb { \times } ^ { e } = \frac { 1 } { t - s + 1 } \sum _ { k = s } ^ { t } \mathbf { h } _ { k } ^ { w }
$$

ls including the transition-based neural model, the dynamic oracle strategy and t. By this way, we naturally convert RST discourse parsing into predictiThe second layer uses this input to compute a final representation of the sequence of ure extraction approas, where each lineEDU representations $\mathbf { h } ^ { e }$ .c:

$$
\mathbf { h } _ { 1 } ^ { e } , \mathbf { h } _ { 2 } ^ { e } , . . . , \mathbf { h } _ { n } ^ { e } \ = \ \mathrm { b i L S T M } ( \mathbf { x } _ { 1 } ^ { e } , \mathbf { x } _ { 2 } ^ { e } , . . . , \mathbf { x } _ { n } ^ { e } )
$$

sed Discourse ParsingThe decoder is then a feedforward network $\boldsymbol { \mathsf { W } }$ that outputs an action $o$ based on a concatenation of the top three subtrees on the stack $( s _ { o } , s _ { 1 } , s _ { 2 } )$ plus the first EDU in isenstein (e featurethe queue $\left( q _ { 0 } \right)$ ),a:

$$
\mathbf { 0 } ~ = ~ \mathbf { W } ( \mathsf { h } _ { \mathrm { s 0 } } ^ { \mathrm { t } } , \mathsf { h } _ { \mathrm { s 1 } } ^ { \mathrm { t } } , \mathsf { h } _ { \mathrm { s } 2 } ^ { \mathrm { t } } , \mathsf { h } _ { \mathrm { q 0 } } ^ { \mathrm { e } } )
$$

..., hn}, and the decoder predicts next step acwhere the representation of the EDU on the queue ${ \bf h } _ { \mathrm { q 0 } } ^ { \mathrm { e } }$ s conditioned on the ecomes directly from the encoder, and the three hidden vectors representing partial trees are computed by average pooling over the encoder output for the EDUs in those trees:

$$
{ \sf h } _ { \mathrm { S } } ^ { \mathrm { t } } = \frac { 1 } { j - i + 1 } \sum _ { k = i } ^ { j } { \sf h } _ { k } ^ { e }
$$

Training first maps each RST gold parse tree into a sequence of oracle actions, and then uses the standard cross-entropy loss (with $l _ { 2 }$ regularization) to train the system to take such actions. Give a state $s$ and oracle action $a$ , we first compute the decoder output using Eq. 24.20, apply a softmax to get probabilities:

$$
p _ { a } ~ = ~ { \frac { \exp ( \mathbf { o } _ { a } ) } { \sum _ { a ^ { \prime } \in A } \exp ( \mathbf { o } _ { a ^ { \prime } } ) } }
$$

and then computing the cross-entropy loss:

$$
L _ { \mathrm { C E } } ( \ v r ) \ = \ - \log ( p _ { a } ) + \frac { \lambda } { 2 } | | \Theta | | ^ { 2 }
$$

RST discourse parsers are evaluated on the test section of the RST Discourse Treebank, either with gold EDUs or end-to-end, using the RST-Pareval metrics (Marcu, 2000b). It is standard to first transform the gold RST trees into right-branching binary trees, and to report four metrics: trees with no labels (S for Span), labeled with nuclei (N), with relations (R), or both (F for Full), for each metric computing micro-averaged $\mathrm { F } _ { 1 }$ over all spans from all documents (Marcu 2000b, Morey et al. 2017).

# 24.2.3 PDTB discourse parsing

PDTB discourse parsing, the task of detecting PDTB coherence relations between spans, is sometimes called shallow discourse parsing because the task just involves flat relationships between text spans, rather than the full trees of RST parsing.

The set of four subtasks for PDTB discourse parsing was laid out by Lin et al. (2014) in the first complete system, with separate tasks for explicit (tasks 1-3) and implicit (task 4) connectives:

1. Find the discourse connectives (disambiguating them from non-discourse uses)   
2. Find the two spans for each connective   
3. Label the relationship between these spans   
4. Assign a relation between every adjacent pair of sentences

Many systems have been proposed for Task 4: taking a pair of adjacent sentences as input and assign a coherence relation sense label as output. The setup often follows Lin et al. (2009) in assuming gold sentence span boundaries and assigning each adjacent span one of the 11 second-level PDTB tags or none (removing the 5 very rare tags of the 16 shown in italics in Fig. 24.3).

A simple but very strong algorithm for Task 4 is to represent each of the two spans by BERT embeddings and take the last layer hidden state corresponding to the position of the [CLS] token, pass this through a single layer tanh feedforward network and then a softmax for sense classification (Nie et al., 2019).

Each of the other tasks also have been addressed. Task 1 is to disambiguating discourse connectives from their non-discourse use. For example as Pitler and Nenkova (2009) point out, the word and is a discourse connective linking the two clauses by an elaboration/expansion relation in (24.24) while it’s a non-discourse NP conjunction in (24.25):

(24.24) Selling picked up as previous buyers bailed out of their positions and aggressive short sellers—anticipating further declines—moved in. (24.25) My favorite colors are blue and green.

Similarly, once is a discourse connective indicating a temporal relation in (24.26), but simply a non-discourse adverb meaning ‘formerly’ and modifying used in (24.27):

(24.26) The asbestos fiber, crocidolite, is unusually resilient once it enters the lungs, with even brief exposures to it causing symptoms that show up decades later, researchers said.   
(24.27) A form of asbestos once used to make Kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than 30 years ago, researchers reported.

Determining whether a word is a discourse connective is thus a special case of word sense disambiguation. Early work on disambiguation showed that the 4 PDTB high-level sense classes could be disambiguated with high $( 9 4 \% )$ accuracy used syntactic features from gold parse trees (Pitler and Nenkova, 2009). Recent work performs the task end-to-end from word inputs using a biLSTM-CRF with BIO outputs (B-CONN, I-CONN, O) (Yu et al., 2019).

For task 2, PDTB spans can be identified with the same sequence models used to find RST EDUs: a biLSTM sequence model with pretrained contextual embedding (BERT) inputs (Muller et al., 2019). Simple heuristics also do pretty well as a baseline at finding spans, since $93 \%$ of relations are either completely within a single sentence or span two adjacent sentences, with one argument in each sentence (Biran and McKeown, 2015).

# 24.3 Centering and Entity-Based Coherence

# Centering Theory

A second way a discourse can be coherent is by virtue of being “about” some entity. This idea that at each point in the discourse some entity is salient, and a discourse is coherent by continuing to discuss the same entity, appears early in functional linguistics and the psychology of discourse (Chafe 1976, Kintsch and Van Dijk 1978), and soon made its way to computational models. In this section we introduce two models of this kind of entity-based coherence: Centering Theory (Grosz et al., 1995), and the entity grid model of Barzilay and Lapata (2008).

# 24.3.1 Centering

Centering Theory (Grosz et al., 1995) is a theory of both discourse salience and discourse coherence. As a model of discourse salience, Centering proposes that at any given point in the discourse one of the entities in the discourse model is salient: it is being “centered” on. As a model of discourse coherence, Centering proposes that discourses in which adjacent sentences CONTINUE to maintain the same salient entity are more coherent than those which SHIFT back and forth between multiple entities (we will see that CONTINUE and SHIFT are technical terms in the theory).

The following two texts from Grosz et al. (1995) which have exactly the same propositional content but different saliences, can help in understanding the main Centering intuition.

(24.28) a. John went to his favorite music store to buy a piano. b. He had frequented the store for many years. c. He was excited that he could finally buy a piano. d. He arrived just as the store was closing for the day.

(24.29) a. John went to his favorite music store to buy a piano. b. It was a store John had frequented for many years. c. He was excited that he could finally buy a piano. d. It was closing just as John arrived.

forward-looking center

While these two texts differ only in how the two entities (John and the store) are realized in the sentences, the discourse in (24.28) is intuitively more coherent than the one in (24.29). As Grosz et al. (1995) point out, this is because the discourse in (24.28) is clearly about one individual, John, describing his actions and feelings. The discourse in (24.29), by contrast, focuses first on John, then the store, then back to John, then to the store again. It lacks the “aboutness” of the first discourse.

Centering Theory realizes this intuition by maintaining two representations for each utterance $U _ { n }$ . The backward-looking center of $U _ { n }$ , denoted as $C _ { b } ( U _ { n } )$ , represents the current salient entity, the one being focused on in the discourse after $U _ { n }$ is interpreted. The forward-looking centers of $U _ { n }$ , denoted as $C _ { f } ( U _ { n } )$ , are a set of potential future salient entities, the discourse entities evoked by $U _ { n }$ any of which could serve as $C _ { b }$ (the salient entity) of the following utterance, i.e. $C _ { b } ( U _ { n + 1 } )$ .

The set of forward-looking centers $C _ { f } ( U _ { n } )$ are ranked according to factors like discourse salience and grammatical role (for example subjects are higher ranked than objects, which are higher ranked than all other grammatical roles). We call the highest-ranked forward-looking center $C _ { p }$ (for “preferred center”). $C _ { p }$ is a kind of prediction about what entity will be talked about next. Sometimes the next utterance indeed talks about this entity, but sometimes another entity becomes salient instead.

We’ll use here the algorithm for centering presented in Brennan et al. (1987), which defines four intersentential relationships between a pair of utterances $U _ { n }$ and $U _ { n + 1 }$ that depend on the relationship between $C _ { b } ( U _ { n + 1 } )$ , $C _ { b } ( U _ { n } )$ , and $C _ { p } ( U _ { n + 1 } )$ ; these are shown in Fig. 24.7.

<table><tr><td></td><td>Cb(Un+1) =Cb(Un) or undefined Cb(Un)</td><td>Cb(Un+1)≠Cb(Un)</td></tr><tr><td>Cb(Un+1)=Cp(Un+1)</td><td>Continue</td><td>Smooth-Shift</td></tr><tr><td>Cb(Un+1) ≠Cp(Un+1)</td><td>Retain</td><td>Rough-Shift</td></tr></table>

Figure 24.7 Centering Transitions for Rule 2 from Brennan et al. (1987).

The following rules are used by the algorithm:

Rule 1: If any element of $C _ { f } ( U _ { n } )$ is realized by a pronoun in utterance $U _ { n + 1 }$ , then $C _ { b } ( U _ { n + 1 } )$ must be realized as a pronoun also.   
Rule 2: Transition states are ordered. Continue is preferred to Retain is preferred to Smooth-Shift is preferred to Rough-Shift.

Rule 1 captures the intuition that pronominalization (including zero-anaphora) is a common way to mark discourse salience. If there are multiple pronouns in an utterance realizing entities from the previous utterance, one of these pronouns must realize the backward center $C _ { b }$ ; if there is only one pronoun, it must be $C _ { b }$ .

Rule 2 captures the intuition that discourses that continue to center the same entity are more coherent than ones that repeatedly shift to other centers. The transition table is based on two factors: whether the backward-looking center $C _ { b }$ is the same from $U _ { n }$ to $U _ { n + 1 }$ and whether this discourse entity is the one that is preferred $( C _ { p } )$ in the new utterance $U _ { n + 1 }$ . If both of these hold, a CONTINUE relation, the speaker has been talking about the same entity and is going to continue talking about that entity. In a RETAIN relation, the speaker intends to SHIFT to a new entity in a future utterance and meanwhile places the current entity in a lower rank $C _ { f }$ . In a SHIFT relation, the speaker is shifting to a new salient entity.

Let’s walk though the start of (24.28) again, repeated as (24.30), showing the representations after each utterance is processed.

(24.30) John went to his favorite music store to buy a piano. $( U _ { 1 } )$ He was excited that he could finally buy a piano. $( U _ { 2 } )$ ) He arrived just as the store was closing for the day. $( U _ { 3 } )$ It was closing just as John arrived $( U _ { 4 } )$ )

Using the grammatical role hierarchy to order the $\mathrm { C } _ { f }$ , for sentence $U _ { 1 }$ we get:

$C _ { f } ( U _ { 1 } )$ : {John, music store, piano}   
$C _ { p } ( U _ { 1 } )$ : John   
$C _ { b } ( U _ { 1 } )$ : undefined

and then for sentence $U _ { 2 }$ :

$C _ { f } ( U _ { 2 } )$ : {John, piano}   
$C _ { p } ( U _ { 2 } )$ : John   
$C _ { b } ( U _ { 2 } )$ : John   
Result: Continue $( C _ { p } ( U _ { 2 } ) { = } C _ { b } ( U _ { 2 } ) ; C _ { b } ( U _ { 1 } )$ undefined)

The transition from $U _ { 1 }$ to $U _ { 2 }$ is thus a CONTINUE. Completing this example is left as exercise (1) for the reader

# 24.3.2 Entity Grid model

Centering embodies a particular theory of how entity mentioning leads to coherence: that salient entities appear in subject position or are pronominalized, and that discourses are salient by means of continuing to mention the same entity in such ways.

The entity grid model of Barzilay and Lapata (2008) is an alternative way to capture entity-based coherence: instead of having a top-down theory, the entity-grid model using machine learning to induce the patterns of entity mentioning that make a discourse more coherent.

The model is based around an entity grid, a two-dimensional array that represents the distribution of entity mentions across sentences. The rows represent sentences, and the columns represent discourse entities (most versions of the entity grid model focus just on nominal mentions). Each cell represents the possible appearance of an entity in a sentence, and the values represent whether the entity appears and its grammatical role. Grammatical roles are subject (S), object (O), neither (X), or absent $( - )$ ; in the implementation of Barzilay and Lapata (2008), subjects of passives are represented with O, leading to a representation with some of the characteristics of thematic roles.

Fig. 24.8 from Barzilay and Lapata (2008) shows a grid for the text shown in Fig. 24.9. There is one row for each of the six sentences. The second column, for the entity ‘trial’, is O – – – X, showing that the trial appears in the first sentence as direct object, in the last sentence as an oblique, and does not appear in the middle sentences. The third column, for the entity Microsoft, shows that it appears as subject in sentence 1 (it also appears as the object of the preposition against, but entities that appear multiple times are recorded with their highest-ranked grammatical function). Computing the entity grids requires extracting entities and doing coreference ranked higher than objects, which in turn are ranked higher than the rest. For exampresolution to cluster them into discourse entities (Chapter 23) as well as parsing theet al. 2004; Poesio et al. 2004), but this is not an option for our model. An obvi sentences to get grammatical roles.solution for identifying entity clas

<table><tr><td>一 １２３456 1 -- os -o 1 1 s- o- X 1 s -- 一</td><td>s8u)ueg-ii 1ns 一 １２3456 一 ---0- 一 -0</td></tr></table>

<table><tr><td></td><td>1 [The Justice Department]s is conducting an [anti-trust triall against [Microsoft Corp.lx with [evidence]x that [the companyls is increasingly attempting to crush [competitorslo. 2 [Microsoft] is accused of trying to forcefully buy into [markets]x where [its own</td></tr></table>

Figure 24.9 A discourse with the entities marked and annotated with grammatical funcWhen a noun is attested more than otions. Figure from Barzilay and Lapata (2008).

In the resulting grid, columns that are dense (like the column for Microsoft) in-ool that determines which noun phrases refer to the same entity in a document. dicate entities that are mentioned often in the texts; sparse columns (like the column for earnings) indicate entities that are mentioned rarely.

In the entity grid model, coherence is measured by patterns of local entity tran-ontradictory pairwise classifications and constructs a partition on the set of NPs. sition. For example, Department is a subject in sentence 1, and then not men-our experiments, we employ Ng and Cardie’s (2002) coreference resolution syste in coherent texts exhibits certain regularities reflected in grid topology. Some of thtioned in sentence 2; this is the transition [S –]. The transitions are thus sequencesThe system decides whether two NPs are coreferent by exploiting a wealth of lexic $\{ \mathbf { s } , 0 ~ \mathrm { X } , - \} ^ { n }$ s are formalized in Centering Theory as constraints on transitions of twhich can be extracted as continuous cells from each column. Eachal, semantic, and positional features. It is trained on the MUC (6–7) data s local focus in adjacent sentences. Grids of coherent texts are likely to have some detransition has a probability; the probability of [S –] in the grid from Fig. 24.8 is 0.08and yields state-of-the-art performance (70.4 F-measure on MUC-6 and 63.4 on MUCcolumns (i.e., columns with just a few gaps, such as Microsoft in Table 1) and ma(it occurs 6 times out of the 75 total transitions of length two). Fig. 24.10 shows the sparse columns which will consist mostly of gaps (see markets and earnings in Tabledistribution over transitions of length 2 for the text of Fig. 24.9 (shown as the first rowTa $d _ { 1 }$ ), and 2 other documents. 3

<table><tr><td>Ss sO sx s- 0s oo 0x 0- xs xO xx X- -s -O -x --</td><td colspan="13"></td><td rowspan="7"></td><td colspan="7"></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td>d</td><td>.01</td><td></td><td>.010</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>.08.0100.09000.03.05.07.03.59</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td>.02</td><td></td><td>.01.01.020</td><td></td><td></td><td></td><td>.070</td><td></td><td></td><td></td><td></td><td>.02.14.14.06.04.03</td><td></td><td></td><td></td><td></td><td></td><td>.070.1.36</td></tr><tr><td>dd</td><td>.020</td><td></td><td>0</td><td></td><td></td><td>.03.090°</td><td></td><td></td><td></td><td>.09 .06000</td><td></td><td></td><td>.05</td><td></td><td>.03</td><td></td><td></td><td>.07.17 .39</td></tr></table>

[i.e., six] diFigure 24.10 ided by the total number of transitions of length two [i.e., 75]). Each tA feature vector for representing documents using all transitions of length 2. can thusDocument $d _ { 1 }$ e viewed as a distribution defined over transition types.is the text in Fig. 24.9. Figure from Barzilay and Lapata (2008).

j   The transitions and their probabilities can then be used as features for a machine i       ij  1 ij 2 ij   m ij    learning model. This model can be a text classifier trained to produce human-labeled t ij    coherence scores (for example from humans labeling each text as coherent or incoalgorithms (see our experiments in Sections 4–6). Furthermore, it allows the consherent). But such data is expensive to gather. Barzilay and Lapata (2005) introduced eration of large numbers of transitions which could potentially uncover novel enta simplifying innovation: coherence models can be trained by self-supervision: distribution patterns relevant for coherence assessment or other coherence-related tastrained to distinguish the natural original order of sentences in a discourse from a modified order (such as a randomized order). We turn to these evaluations in the next section.

# 24.3.3 Evaluating Neural and Entity-based coherence

Entity-based coherence models, as well as the neural models we introduce in the next section, are generally evaluated in one of two ways.

First, we can have humans rate the coherence of a document and train a classifier to predict these human ratings, which can be categorial (high/low, or high/mid/low) or continuous. This is the best evaluation to use if we have some end task in mind, like essay grading, where human raters are the correct definition of the final label.

Alternatively, since it’s very expensive to get human labels, and we might not yet have an end-task in mind, we can use natural texts to do self-supervision. In self-supervision we pair up a natural discourse with a pseudo-document created by changing the ordering. Since naturally-ordered discourses are more coherent than random permutation (Lin et al., 2011), a successful coherence algorithm should prefer the original ordering.

Self-supervision has been implemented in 3 ways. In the sentence order discrimination task (Barzilay and Lapata, 2005), we compare a document to a random permutation of its sentences. A model is considered correct for an (original, permuted) test pair if it ranks the original document higher. Given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing.

In the sentence insertion task (Chen et al., 2007) we take a document, remove one of the $n$ sentences $s$ , and create $n - 1$ copies of the document with $s$ inserted into each position. The task is to decide which of the $n$ documents is the one with the original ordering, distinguishing the original position for $s$ from all other positions. Insertion is harder than discrimination since we are comparing documents that differ by only one sentence.

Finally, in the sentence order reconstruction task (Lapata, 2003), we take a document, randomize the sentences, and train the model to put them back in the correct order. Again given $k$ documents, we can compute $n$ permutations, resulting in kn pairs each with one original document and one permutation, to use in training and testing. Reordering is of course a much harder task than simple classification.

# 24.4 Representation learning models for local coherence

# lexical cohesion

The third kind of local coherence is topical or semantic field coherence. Discourses cohere by talking about the same topics and subtopics, and drawing on the same semantic fields in doing so.

# TextTiling

The field was pioneered by a series of unsupervised models in the 1990s of this kind of coherence that made use of lexical cohesion (Halliday and Hasan, 1976): the sharing of identical or semantically related words in nearby sentences. Morris and Hirst (1991) computed lexical chains of words (like pine, bush trees, trunk) that occurred through a discourse and that were related in Roget’s Thesaurus (by being in the same category, or linked categories). They showed that the number and density of chain correlated with the topic structure. The TextTiling algorithm of Hearst (1997) computed the cosine between neighboring text spans (the normalized dot product of vectors of raw word counts), again showing that sentences or paragraph in a subtopic have high cosine with each other, but not with sentences in a neighboring subtopic.

A third early model, the LSA Coherence method of Foltz et al. (1998) was the first to use embeddings, modeling the coherence between two sentences as the cosine between their LSA sentence embedding vectors1, computing embeddings for a sentence $s$ by summing the embeddings of its words $w$ :

$$
\begin{array} { l } { \displaystyle \sin ( s , t ) = \cos ( \mathbf { s } , \mathbf { t } ) } \\ { \displaystyle = \cos ( \sum _ { w \in s } \mathbf { w } , \sum _ { w \in t } \mathbf { w } ) } \end{array}
$$

and defining the overall coherence of a text as the average similarity over all pairs of adjacent sentences $s _ { i }$ and $s _ { i + 1 }$ :

$$
\operatorname { c o h e r e n c e } ( T ) ~ = ~ { \frac { 1 } { n - 1 } } { \sum _ { i = 1 } ^ { n - 1 } \cos ( s _ { i } , s _ { i + 1 } ) }
$$

Modern neural representation-learning coherence models, beginning with Li et al. (2014), draw on the intuitions of these early unsupervised models for learning sentence representations and measuring how they change between neighboring sentences. But the new models also draw on the idea pioneered by Barzilay and Lapata (2005) of self-supervision. That is, unlike say coherence relation models, which train on hand-labeled representations for RST or PDTB, these models are trained to distinguish natural discourses from unnatural discourses formed by scrambling the order of sentences, thus using representation learning to discover the features that matter for at least the ordering aspect of coherence.

Here we present one such model, the local coherence discriminator (LCD) $\mathrm { { X u } }$ et al., 2019). Like early models, LCD computes the coherence of a text as the average of coherence scores between consecutive pairs of sentences. But unlike the early unsupervised models, LCD is a self-supervised model trained to discriminate consecutive sentence pairs $\left( { { s _ { i } } , { s _ { i + 1 } } } \right)$ in the training documents (assumed to be coherent) from (constructed) incoherent pairs $\left( s _ { i } , s ^ { \prime } \right)$ . All consecutive pairs are positive examples, and the negative (incoherent) partner for a sentence $s _ { i }$ is another sentence uniformly sampled from the same document as $s _ { i }$ .

Fig. 24.11 describes the architecture of the model $f _ { \theta }$ , which takes a sentence pair and returns a score, higher scores for more coherent pairs. Given an input sentence pair $s$ and $t$ , the model computes sentence embeddings s and t (using any sentence embeddings algorithm), and then concatenates four features of the pair: (1) the concatenation of the two vectors (2) their difference $\mathsf { \pmb { s } } - \mathsf { \pmb { t } }$ ; (3) the absolute value of their difference $\left| \mathsf { s } - \mathsf { t } \right|$ ; (4) their element-wise product s $\odot$ t. These are passed through a one-layer feedforward network to output the coherence score.

The model is trained to make this coherence score higher for real pairs than for negative pairs. More formally, the training objective for a corpus $C$ of documents $d$ each of which consists of a list of sentences $s _ { i }$ , is:

$$
L _ { \theta } = \sum _ { d \in C } \sum _ { s _ { i } \in d } \operatorname { \mathbb { E } } _ { p ( s ^ { \prime } | s _ { i } ) } [ L ( f _ { \theta } ( s _ { i } , s _ { i + 1 } ) , f _ { \theta } ( s _ { i } , s ^ { \prime } ) ) ]
$$

$\mathbb { E } _ { p ( s ^ { \prime } \mid s _ { i } ) }$ is the expectation with respect to the negative sampling distribution conditioned on $s _ { i }$ : given a sentence $s _ { i }$ the algorithms samples a negative sentence $s ^ { \prime }$ ment touniformly over the other sentences in the same document. $L$ is a loss function that 4.2 Pre-trained Generative Model as thetakes two scores, one for a positive pair and one for a negative pair, with the goal of encouraging $f ^ { + } = f _ { \theta } ( s _ { i } , s _ { i + 1 } )$ Encoder to be high and $f ^ { - } = f _ { \theta } ( s _ { i } , s ^ { \prime } ) )$ to be low. Fig. 24.11 nk, thenuse the margin loss $l ( f ^ { + } , f ^ { - } ) = \operatorname* { m a x } ( 0 , \eta - f ^ { + } + f ^ { - } )$ where $\eta$ is the margin hypere nk   1parameter.

![## Image Analysis: 2e1dfa61be1fdbdf4ca7158daa2d66a35ca271746a47649ce8f015d097da7105.jpg

**Conceptual Understanding:**
This image conceptually represents a neural network model designed to assess the semantic coherence or relatedness between two given sentences. Its main purpose is to illustrate the architectural flow of how two input sentences are processed through encoding, feature combination, and a neural layer to ultimately output a quantitative "Coherence Score." The key ideas communicated are the use of sentence embeddings (S, T), the importance of deriving relational features (S-T, |S-T|, S*T) from these embeddings, and the application of a simple neural network (one-layer MLP) to predict the coherence value.

**Content Interpretation:**
The image depicts a neural network architecture, specifically a Siamese-like network structure, for calculating the coherence between two sentences. The core process involves encoding two separate sentences into vector representations, performing element-wise operations and concatenation on these representations, and then using a shallow neural network (one-layer MLP) to predict a coherence score. The elements S and T represent the embedded vector representations of the first and second sentences, respectively. The intermediate feature vector (S, T, S - T, |S - T|, S * T) signifies a concatenation of the individual embeddings, their element-wise difference, the absolute element-wise difference, and their element-wise product. This combination aims to capture various relational aspects between the two sentence embeddings, which are then used by the MLP to determine the coherence. All extracted text elements (e.g., "sentence encoder with first sentence," "S," "T," "(S, T, S - T, |S - T|, S * T)," "one-layer MLP," "Coherence Score") directly represent the functional components and data flow within this architecture.

**Key Insights:**
**Main Takeaways and Insights:**
1.  **Dual Input Processing:** The model processes two sentences independently initially, using separate encoders for each. This suggests a design focused on comparing or contrasting two distinct inputs.
2.  **Feature Engineering for Relationship Capture:** The intermediate step "(S, T, S - T, |S - T|, S * T)" is critical. It indicates that coherence is not merely based on individual sentence representations (S, T) but also on their differences (S - T, |S - T|) and interactions (S * T). This implies that understanding the relationship between sentence embeddings is key to determining coherence.
3.  **Neural Network for Score Prediction:** A "one-layer MLP" is used to map these engineered features to a single "Coherence Score." This highlights that the relationship features are linearly (or non-linearly, if activation is present) combined and transformed to yield the final score, suggesting a learned function for coherence.
4.  **Modular Architecture:** The design is modular, separating sentence encoding from feature combination and final prediction, which is common in neural network architectures for complex tasks.

**Evidence from Text Elements:**
*   "sentence encoder with first sentence" and "sentence encoder with second sentence" confirm the dual input and independent initial processing.
*   "S" and "T" are the encoded representations, forming the basis for comparison.
*   "(S, T, S - T, |S - T|, S * T)" explicitly lists the features derived, demonstrating the sophisticated feature engineering aimed at capturing various aspects of the relationship between S and T.
*   "one-layer MLP" indicates the neural network component responsible for learning the mapping from features to coherence.
*   "Coherence Score" is the final output, clearly stating the objective of the model.

**Document Context:**
This image, described in the document context as "Figure 24.11 The architecture of the LCD model of document coherence, showing the computation of the score for a pair of sentences s for and t our proposed model. Figure from Xu et al. (2019)," directly explains the computational model for determining document coherence. It provides a visual representation of how the coherence score is derived from two input sentences, which is a fundamental component of the TextTiling section. The detailed steps, from sentence encoding to feature combination and final score prediction, align with the technical discussion of how such a model functions in natural language processing research.

**Summary:**
This image illustrates the architecture of a model designed to compute a "Coherence Score" between two sentences. The process begins with two distinct inputs: a "first sentence" and a "second sentence." Each sentence is processed independently by its respective "sentence encoder." The "sentence encoder with first sentence" generates a representation denoted as "S." Similarly, the "sentence encoder with second sentence" generates a representation denoted as "T." These two representations, "S" and "T," are then combined and undergo feature engineering to create a composite representation: "(S, T, S - T, |S - T|, S * T)." This combined feature set is then fed into a "one-layer MLP" (Multi-Layer Perceptron). Finally, the output of the "one-layer MLP" is the "Coherence Score," which quantifies the coherence between the initial two sentences. The flow is strictly sequential from the sentence encoders up to the final coherence score.](images/2e1dfa61be1fdbdf4ca7158daa2d66a35ca271746a47649ce8f015d097da7105.jpg)
Figure 24.11 The architecture of the LCD model of document coherence, showing the odel to Figure 1: Generic architectucomputation of the score for a pair of sentences $s$ forand $t$ our proposed model.. Figure from Xu et al. (2019).

with tence encoder, ranging from the most simplisticXu et al. (2019) also give a useful baseline algorithm that itself has quite high irs, and average GloVe (Pennington et al., 2014) embed-performance in measuring perplexity: train an RNN language model on the data, out that dings to more sophisticaand compute the log likelihood of sentence $s _ { i }$ d supervised or unsu-in two ways, once given the preceding context (conditional log likelihood) and once with no context (marginal log likelihood). The difference between these values tells us how much the preceding context time, is et al., 2017).improved the predictability of $s _ { i }$ s mentioned in the introduction,, a predictability measure of coherence.

cov- since generative models can often be turned intoTraining models to predict longer contexts than just consecutive pairs of sentice, we sentence encoder, generative coherence model cantences can result in even stronger discourse representations. For example a Transwe see be leveraged by our model to benefit from theformer language model trained with a contrastive sentence objective to predict text up to a distance of $\pm 2$ sentences improves performance on various discourse coherence tasks (Iter et al., 2020).

Language-model style models are generally evaluated by the methods of Secetails on 2018). After initialization, we freeze the genera-tion 24.3.3, although they can also be evaluated on the RST and PDTB coherence trelation tasks.

# while we do24.5 Global Coherence

h is dis- the choice of sentence encoder.A discourse must also cohere globally rather than just at the level of pairs of sentences. Consider stories, for example. The narrative structure of stories is one of 5 Experimentsthe oldest kinds of global coherence to be studied. In his influential Morphology of the Folktale, Propp (1968) models the discourse structure of Russian folktales via ectors Sa kind of plot grammar. His model includes a set of character categories he called n of thedramatis personae, like Hero, Villain, Donor, or Helper, and a set of events he the two Following Nguyen and Joty (2017) and other pre-called functions (like “Villain commits kidnapping”, “Donor tests Hero”, or “Hero e S T ; vious work, we evaluate our models on the dis-is pursued”) that have to occur in particular order, along with other components. crimination and insertion tasks. Additionally, wePropp shows that the plots of each of the fairy tales he studies can be represented as a sequence of these functions, different tales choosing different subsets of functions, but always in the same order. Indeed Lakoff (1972) showed that Propp’s model amounted to a discourse grammar of stories, and in recent computational work Finlayson (2016) demonstrates that some of these Proppian functions could be induced from corpora of folktale texts by detecting events that have similar actions across stories. Bamman et al. (2013) showed that generalizations over dramatis personae could be induced from movie plot summaries on Wikipedia. Their model induced latent personae from features like the actions the character takes (e.g., Villains strangle), the actions done to them (e.g., Villains are foiled and arrested) or the descriptive words used of them (Villains are evil).

In this section we introduce two kinds of such global discourse structure that have been widely studied computationally. The first is the structure of arguments: the way people attempt to convince each other in persuasive essays by offering claims and supporting premises. The second is somewhat related: the structure of scientific papers, and the way authors present their goals, results, and relationship to prior work in their papers.

# 24.5.1 Argumentation Structure

argumentation mining

pathos ethos logos

The first type of global discourse structure is the structure of arguments. Analyzing people’s argumentation computationally is often called argumentation mining.

The study of arguments dates back to Aristotle, who in his Rhetorics described three components of a good argument: pathos (appealing to the emotions of the listener), ethos (appealing to the speaker’s personal character), and logos (the logical structure of the argument).

claims premises

Most of the discourse structure studies of argumentation have focused on logos, particularly via building and training on annotated datasets of persuasive essays or other arguments (Reed et al. 2008, Stab and Gurevych 2014a, Peldszus and Stede 2016, Habernal and Gurevych 2017, Musi et al. 2018). Such corpora, for example, often include annotations of argumentative components like claims (the central component of the argument that is controversial and needs support) and premises (the reasons given by the author to persuade the reader by supporting or attacking the claim or other premises), as well as the argumentative relations between them like SUPPORT and ATTACK.

Consider the following example of a persuasive essay from Stab and Gurevych (2014b). The first sentence (1) presents a claim (in bold). (2) and (3) present two premises supporting the claim. (4) gives a premise supporting premise (3).

“(1) Museums and art galleries provide a better understanding about arts than Internet. (2) In most museums and art galleries, detailed descriptions in terms of the background, history and author are provided. (3) Seeing an artwork online is not the same as watching it with our own eyes, as (4) the picture online does not show the texture or three-dimensional structure of the art, which is important to study.”

Thus this example has three argumentative relations: SUPPORT(2,1), SUPPORT(3,1) and SUPPORT(4,3). Fig. 24.12 shows the structure of a much more complex argument.

While argumentation mining is clearly related to rhetorical structure and other kinds of coherence relations, arguments tend to be much less local; often a persuasive essay will have only a single main claim, with premises spread throughout the text, without the local coherence we see in coherence relations.

![## Image Analysis: 0a9b6f246f0dfc63c78a24a47deb315bdc4b565046c6731d1e7a1978e1e0a510.jpg

**Conceptual Understanding:**
This image conceptually represents the **argumentation structure of a persuasive essay**. It illustrates how a central "Major Claim" (or set of claims) is built up or challenged by various supporting "Claims" and underlying "Premises" throughout different sections of an essay (Introduction, Body Paragraphs, Conclusion).

The main purpose of this diagram is to **visualize the logical relationships between different argumentative components** within an essay. It differentiates between arguments that *support* the main point and those that *attack* it (or attack counter-arguments), using specific arrow types. It highlights the hierarchical nature of argumentation, where premises build up to claims, and claims contribute to the major thesis.

Key ideas communicated include:
*   The modular structure of an essay into distinct sections.
*   The concept of a "Major Claim" as the central thesis.
*   The distinction between supporting arguments ("for") and counter-arguments or attacks ("against").
*   The role of "Premises" (P) as foundational evidence or reasoning for claims.
*   The direct (solid lines) versus potentially indirect or weaker (dashed lines) argumentative relations.
*   The hierarchical nature of arguments, where lower-level premises or claims feed into higher-level claims.

**Content Interpretation:**
The image depicts a detailed model of argumentative relationships, categorized into different parts of an essay.

*   **Processes Shown:** The diagram illustrates the intellectual process of constructing and dissecting arguments within a persuasive essay. It models how claims are made, supported, and sometimes attacked, ultimately in relation to a broader "Major Claim." It shows a multi-layered reasoning process, where specific premises (P1-P11) support or attack more general claims (Claim 1-7), which in turn support or attack the overarching "Major Claim 1 & 2."

*   **Concepts/Relationships Shown:**
    *   **Hierarchy of Argumentation:** "P 2" supports "P 1," which supports "Claim 2 (for)," which then supports "Major Claim 1 & 2." This demonstrates a clear chain of reasoning from specific evidence to broader conclusions.
    *   **Types of Claims:** Claims are explicitly labeled as either "(for)" or "(against)," indicating their stance relative to the "Major Claim." For example, "Claim 1 (for)" and "Claim 2 (for)" provide positive support, while "Claim 5 (against)" and "Claim 6 (against)" present opposition.
    *   **Support and Attack Relations:** The arrows are crucial. As per the context provided, arrowheads signify "SUPPORT," and circleheads signify "ATTACK."
        *   **Direct Support:** "P 6" directly supports "Claim 3 (for)" (arrowhead). "Claim 3 (for)" directly supports "Major Claim 1 & 2" (arrowhead).
        *   **Direct Attack:** "P 9" directly attacks "Claim 5 (against)" (circlehead). "Claim 5 (against)" directly attacks "Major Claim 1 & 2" (circlehead).
        *   **Indirect Argumentation:** The dashed lines (e.g., from "Claim 1 (for)" to "Major Claim 1 & 2") might suggest a less direct or perhaps more introductory/concluding support/attack compared to the solid lines in the body paragraphs.
    *   **Counter-Argument Refutation:** The section "Body Paragraph 3" with "Claim 5 (against)" is particularly interesting. "Claim 5 (against)" attacks the "Major Claim." However, "P 9" and "P 11" *attack* "Claim 5 (against)." This implies that the premises in Body Paragraph 3 are being used to refute a counter-argument ("Claim 5 (against)"), thereby indirectly strengthening the "Major Claim." "P 10" further supports "P 11," making the attack on "Claim 5 (against)" stronger.

*   **Significance of Information:**
    *   The numbering of "P" (P1 through P11) indicates distinct pieces of evidence or logical steps.
    *   The grouping into "Body Paragraph 1," "Body Paragraph 2," and "Body Paragraph 3" suggests how these arguments are structured within the essay's physical layout, showing that different paragraphs might focus on different claims and their supporting premises.
    *   The "Introduction" sets the stage with an initial supporting claim, and the "Conclusion" summarizes with both potential attacks and final supports.

*   **Supporting Evidence from Transcribed Text:**
    *   "Major Claim 1 & 2" establishes the central focus.
    *   Labels like "Claim 1 (for)", "Claim 2 (for)", "Claim 3 (for)", "Claim 4 (for)", "Claim 7 (for)" explicitly state arguments in favor.
    *   Labels like "Claim 5 (against)" and "Claim 6 (against)" explicitly state counter-arguments.
    *   "P 1" through "P 11" denote specific premises.
    *   The "Introduction," "Body Paragraph 1," "Body Paragraph 2," "Body Paragraph 3," and "Conclusion" labels show the essay's structural progression.
    *   The consistent use of arrowheads for support and circleheads for attack (as interpreted from the document context) graphically reinforces the argumentative relations. For example, the arrow from "P 2" to "P 1" with an arrowhead shows P2 supports P1. The arrow from "P 9" to "Claim 5 (against)" with a circlehead shows P9 attacks Claim 5.

**Key Insights:**
**Main Takeaways/Lessons:**

1.  **Persuasive essays have a clear, hierarchical argumentative structure:** The diagram demonstrates that major claims are not just asserted but are built upon a foundation of supporting claims and premises. "Major Claim 1 & 2" sits at the top, receiving support from multiple "Claims" which are themselves often supported by "Premises."
2.  **Arguments can be layered:** Premises can support other premises, which then support claims, as seen with "P 2" supporting "P 1," which in turn supports "Claim 2 (for)." This highlights the depth of reasoning required.
3.  **Essays must address counter-arguments:** The inclusion of "Claim 5 (against)" and "Claim 6 (against)" explicitly shows the importance of acknowledging and potentially refuting opposing viewpoints to strengthen the overall argument.
4.  **Refuting counter-arguments indirectly strengthens the main claim:** In "Body Paragraph 3," "P 9" and "P 11" attack "Claim 5 (against)." Since "Claim 5 (against)" originally attacked the "Major Claim," by attacking the counter-argument, the essay indirectly reinforces its "Major Claim." The presence of "P 10" supporting "P 11" further illustrates strengthening the refutation.
5.  **Different essay sections play distinct roles in argumentation:**
    *   **Introduction:** Can introduce initial supporting claims (e.g., "Claim 1 (for)").
    *   **Body Paragraphs:** Form the core of the argument, developing claims with premises (e.g., "Body Paragraph 1," "Body Paragraph 2") and addressing counter-arguments (e.g., "Body Paragraph 3").
    *   **Conclusion:** Can reiterate or summarize both attacks and supports (e.g., "Claim 6 (against)" and "Claim 7 (for)").

**Conclusions/Insights with Textual Evidence:**

*   **Strong arguments require multiple levels of support:** The chain "P 2" -> "P 1" -> "Claim 2 (for)" -> "Major Claim 1 & 2" illustrates the detailed evidence (premises) needed to substantiate a claim and ultimately a major thesis.
*   **A persuasive essay is a dynamic interplay of support and attack:** The coexistence of "for" claims (e.g., "Claim 2 (for)") and "against" claims (e.g., "Claim 5 (against)") in relation to the "Major Claim 1 & 2" highlights the complex argumentative landscape an essay navigates.
*   **The structure allows for systematic analysis of argumentative strength:** By mapping out each claim and premise and their relationships (support or attack), one can assess the logical coherence and persuasive power of an essay. The diagram acts as a blueprint for evaluating arguments.
*   **Premises can be independent or interconnected:** In "Body Paragraph 1," "P 1" and "P 3" independently support "Claim 2," but "P 1" is further supported by "P 2," and "P 3" is further supported by "P 4" and "P 5." This shows both parallel and sequential reasoning.

**Document Context:**
This image, titled "Figure 24.12 Argumentation structure of a persuasive essay," fits directly into a section discussing argumentation structure, likely within an academic, technical, or research document focused on rhetoric, essay writing, or argument analysis. The accompanying text ("Arrows indicate argumentation relations, either of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises.") explicitly defines the symbols used, making the diagram a critical visual aid for understanding the theoretical framework presented in the chapter. It provides a concrete, graphical example of the abstract concepts of claims, premises, support, and attack in the context of a persuasive essay. It visually reinforces the principles discussed in "Section: 24.5.1 Argumentation Structure."

**Summary:**
This diagram provides a comprehensive, visual blueprint of how arguments are structured within a persuasive essay, all leading back to a central “Major Claim 1 & 2.” Imagine the “Major Claim” as the main thesis statement of the essay.

The essay is broken down into five distinct sections: “Introduction,” “Body Paragraph 1,” “Body Paragraph 2,” “Body Paragraph 3,” and “Conclusion.”

In the **Introduction**, an initial argument, “Claim 1 (for),” is presented to support the “Major Claim 1 & 2.” This support is indicated by a dashed arrow with an arrowhead pointing upwards.

The **Body Paragraphs** form the core of the essay's argumentation:

*   **Body Paragraph 1** focuses on “Claim 2 (for),” which directly supports “Major Claim 1 & 2.” To make “Claim 2 (for)” convincing, it is supported by “P 1” and “P 3” (where 'P' stands for premise or supporting evidence). These premises are also built upon: “P 1” is supported by “P 2,” and “P 3” is supported by both “P 4” and “P 5.” This illustrates how detailed evidence forms a hierarchical structure to bolster a claim.

*   **Body Paragraph 2** introduces two more distinct arguments: “Claim 3 (for)” and “Claim 4 (for),” both providing independent support for “Major Claim 1 & 2.” “Claim 3 (for)” is backed by “P 6,” while “Claim 4 (for)” is supported by “P 7” and “P 8.”

*   **Body Paragraph 3** addresses a potential counter-argument. Here, “Claim 5 (against)” is presented, which challenges “Major Claim 1 & 2” (indicated by a dashed arrow with a circlehead, signifying an attack). To neutralize or weaken this counter-argument, the essay presents “P 9” and “P 11,” which *attack* “Claim 5 (against)” (solid arrows with circleheads). Furthermore, “P 11” itself is strengthened by “P 10” (arrowhead from P10 to P11), meaning that the attack on the counter-argument is well-supported.

Finally, the **Conclusion** wraps up the essay's argumentation. It may briefly acknowledge a final counter-argument, “Claim 6 (against),” which attacks “Major Claim 1 & 2,” but then re-emphasizes a final supporting argument, “Claim 7 (for),” which reinforces “Major Claim 1 & 2.” Both of these concluding points use dashed lines, suggesting their role in summarizing or providing final thoughts rather than introducing new detailed evidence.

In summary, the diagram systematically maps out how an essay's arguments are constructed and connected, using premises to build claims, and claims to establish or defend a major thesis, while also strategically addressing opposing views. The distinct arrow types (arrowheads for support, circleheads for attack) are key to understanding the argumentative relationships.](images/0a9b6f246f0dfc63c78a24a47deb315bdc4b565046c6731d1e7a1978e1e0a510.jpg)
Figure 2Figure 24.12 Argumentation structure of a persuasive essay. Arrows indicate argumentation relations, eiArgumentation structure of the example essay. Arrows indicate argumentative relations.ther of SUPPORT (with arrowheads) or ATTACK (with circleheads); P denotes premises. Figure from Stab and Gurevych (2017).

# argumentation schemes

Algorithms for detecting argumentation structure often include classifiers for 629distinguishing claims, premises, or non-argumentation, together with relation classifiers for deciding if two spans have the SUPPORT, ATTACK, or neither relation (Peldszus and Stede, 2013). While these are the main focus of much computational work, there is also preliminary efforts on annotating and detecting richer semantic relationships (Park and Cardie 2014, Hidey et al. 2017) such as detecting argumentation schemes, larger-scale structures for argument like argument from example, or argument from cause to effect, or argument from consequences (Feng and Hirst, 2011).

Another important line of research is studying how these argument structure (or other features) are associated with the success or persuasiveness of an argument (Habernal and Gurevych 2016, Tan et al. 2016, Hidey et al. 2017. Indeed, while it is Aristotle’s logos that is most related to discourse structure, Aristotle’s ethos and pathos techniques are particularly relevant in the detection of mechanisms of this sort of persuasion. For example scholars have investigated the linguistic realization of features studied by social scientists like reciprocity (people return favors), social proof (people follow others’ choices), authority (people are influenced by those with power), and scarcity (people value things that are scarce), all of which can be brought up in a persuasive argument (Cialdini, 1984). Rosenthal and McKeown (2017) showed that these features could be combined with argumentation structure to predict who influences whom on social media, Althoff et al. (2014) found that linguistic models of reciprocity and authority predicted success in online requests, while the semisupervised model of Yang et al. (2019) detected mentions of scarcity, commitment, and social identity to predict the success of peer-to-peer lending platforms.

See Stede and Schneider (2018) for a comprehensive survey of argument mining.

# 24.5.2 The structure of scientific discourse

Scientific papers have a very specific global structure: somewhere in the course of the paper the authors must indicate a scientific goal, develop a method for a solution, provide evidence for the solution, and compare to prior work. One popular annotation scheme for modeling these rhetorical goals is the argumentative zoning model of Teufel et al. (1999) and Teufel et al. (2009), which is informed by the idea that each scientific paper tries to make a knowledge claim about a new piece of knowledge being added to the repository of the field (Myers, 1992). Sentences in a scientific paper can be assigned one of 15 tags; Fig. 24.13 shows 7 (shortened) examples of labeled sentences.

<table><tr><td>Category</td><td>Description</td><td>Example</td></tr><tr><td>AIM</td><td> hypothesis of current paper</td><td>Statement of specific research goal, or “The aim of this process is to examine the role that training plays in the tagging process&quot;</td></tr><tr><td></td><td>methods</td><td>OWN_METHOD New Knowledge claim, own work: “In order forit to be useful for our purposes, the following extensions must be made:&quot;</td></tr><tr><td></td><td>work</td><td>OWN_REsULTs Measurable/objective outcome of own “Allthe curves have a generally upward trend but always lie far below backoff (51% error rate)&quot;</td></tr><tr><td>USE</td><td>Other work is used in own work</td><td>“We use the framework for the allocation and transfer of control of Whittaker...&quot; </td></tr><tr><td>GAP_WEAK</td><td> other solutions</td><td>Lack of solution in field, problem with “Here, we will produce experimental evidence suggesting that this simple model leads to serious overestimates&quot;</td></tr><tr><td> SUPPORT</td><td> supported by current work</td><td>Other work supports current work or is “Work similar to that described here has been car- ried out by Merialdo (1994), with broadly similar conclusions.&quot;</td></tr><tr><td>ANTISUPPORT</td><td>Clash with other&#x27;s results or theory; su- “This result challenges the claims of..&quot; periority of own work</td><td></td></tr></table>

Figure 24.13 Examples for 7 of the 15 labels from the Argumentative Zoning labelset (Teufel et al., 2009).

Teufel et al. (1999) and Teufel et al. (2009) develop labeled corpora of scientific articles from computational linguistics and chemistry, which can be used as supervision for training standard sentence-classification architecture to assign the 15 labels.

# 24.6 Summary

In this chapter we introduced local and global models for discourse coherence.

• Discourses are not arbitrary collections of sentences; they must be coherent. Among the factors that make a discourse coherent are coherence relations between the sentences, entity-based coherence, and topical coherence. • Various sets of coherence relations and rhetorical relations have been proposed. The relations in Rhetorical Structure Theory (RST) hold between spans of text and are structured into a tree. Because of this, shift-reduce and other parsing algorithms are generally used to assign these structures. The Penn Discourse Treebank (PDTB) labels only relations between pairs of spans, and the labels are generally assigned by sequence models. • Entity-based coherence captures the intuition that discourses are about an entity, and continue mentioning the entity from sentence to sentence. Centering Theory is a family of models describing how salience is modeled for discourse entities, and hence how coherence is achieved by virtue of keeping the same discourse entities salient over the discourse. The entity grid model gives a more bottom-up way to compute which entity realization transitions lead to coherence.

• Many different genres have different types of global coherence. Persuasive essays have claims and premises that are extracted in the field of argument mining, scientific articles have structure related to aims, methods, results, and comparisons.

# Bibliographical and Historical Notes

Coherence relations arose from the independent development of a number of scholars, including Hobbs (1979) idea that coherence relations play an inferential role for the hearer, and the investigations by Mann and Thompson (1987) of the discourse structure of large texts. Other approaches to coherence relations and their extraction include Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides 2003, Baldridge et al. 2007) and the Linguistic Discourse Model (Polanyi 1988, Scha and Polanyi 1988, Polanyi et al. 2004). Wolf and Gibson (2005) argue that coherence structure includes crossed bracketings, which make it impossible to represent as a tree, and propose a graph representation instead. A compendium of over 350 relations that have been proposed in the literature can be found in Hovy (1990).

RST parsing was first proposed by Marcu (1997), and early work was rule-based, focused on discourse markers (Marcu, 2000a). The creation of the RST Discourse TreeBank (Carlson et al. 2001, Carlson and Marcu 2001) enabled a wide variety of machine learning algorithms, beginning with the shift-reduce parser of Marcu (1999) that used decision trees to choose actions, and continuing with a wide variety of machine learned parsing methods (Soricut and Marcu 2003, Sagae 2009, Hernault et al. 2010, Feng and Hirst 2014, Surdeanu et al. 2015, Joty et al. 2015) and chunkers (Sporleder and Lapata, 2005). Subba and Di Eugenio (2009) integrated sophisticated semantic information into RST parsing. Ji and Eisenstein (2014) first applied neural models to RST parsing neural models, leading to the modern set of neural RST models (Li et al. 2014, Li et al. 2016b, Braud et al. 2017, Yu et al. 2018, inter alia) as well as neural segmenters (Wang et al. 2018b). and neural PDTB parsing models (Ji and Eisenstein 2015, Qin et al. 2016, Qin et al. 2017).

Barzilay and Lapata (2005) pioneered the idea of self-supervision for coherence: training a coherence model to distinguish true orderings of sentences from random permutations. Li et al. (2014) first applied this paradigm to neural sentencerepresentation, and many neural self-supervised models followed (Li and Jurafsky 2017, Logeswaran et al. 2018, Lai and Tetreault 2018, Xu et al. 2019, Iter et al. 2020)

Another aspect of global coherence is the global topic structure of a text, the way the topics shift over the course of the document. Barzilay and Lee (2004) introduced an HMM model for capturing topics for coherence, and later work expanded this intuition (Soricut and Marcu 2006, Elsner et al. 2007, Louis and Nenkova 2012, Li and Jurafsky 2017).

The relationship between explicit and implicit discourse connectives has been a fruitful one for research. Marcu and Echihabi (2002) first proposed to use sentences with explicit relations to help provide training data for implicit relations, by removing the explicit relations and trying to re-predict them as a way of improving performance on implicit connectives; this idea was refined by Sporleder and Lascarides (2005), (Pitler et al., 2009), and Rutherford and Xue (2015). This relationship can also be used as a way to create discourse-aware representations. The DisSent algorithm (Nie et al., 2019) creates the task of predicting explicit discourse markers between two sentences. They show that representations learned to be good at this task also function as powerful sentence representations for other discourse tasks.

The idea of entity-based coherence seems to have arisen in multiple fields in the mid-1970s, in functional linguistics (Chafe, 1976), in the psychology of discourse processing (Kintsch and Van Dijk, 1978), and in the roughly contemporaneous work of Grosz, Sidner, Joshi, and their colleagues. Grosz (1977a) addressed the focus of attention that conversational participants maintain as the discourse unfolds. She defined two levels of focus; entities relevant to the entire discourse were said to be in global focus, whereas entities that are locally in focus (i.e., most central to a particular utterance) were said to be in immediate focus. Sidner 1979; 1983 described a method for tracking (immediate) discourse foci and their use in resolving pronouns and demonstrative noun phrases. She made a distinction between the current discourse focus and potential foci, which are the predecessors to the backwardand forward-looking centers of Centering theory, respectively. The name and further roots of the centering approach lie in papers by Joshi and Kuhn (1979) and Joshi and Weinstein (1981), who addressed the relationship between immediate focus and the inferences required to integrate the current utterance into the discourse model. Grosz et al. (1983) integrated this work with the prior work of Sidner and Grosz. This led to a manuscript on centering which, while widely circulated since 1986, remained unpublished until Grosz et al. (1995). A collection of centering papers appears in Walker et al. (1998b). See Karamanis et al. (2004) and Poesio et al. (2004) for a deeper exploration of centering and its parameterizations, and the History section of Chapter 23 for more on the use of centering on coreference.

The grid model of entity-based coherence was first proposed by Barzilay and Lapata (2005) drawing on earlier work by Lapata (2003) and Barzilay, and then extended by them Barzilay and Lapata (2008) and others with additional features (Elsner and Charniak 2008, 2011, Feng et al. 2014, Lin et al. 2011) a model that projects entities into a global graph for the discourse (Guinaudeau and Strube 2013, Mesgar and Strube 2016), and a convolutional model to capture longer-range entity dependencies (Nguyen and Joty, 2017).

Theories of discourse coherence have also been used in algorithms for interpreting discourse-level linguistic phenomena, including verb phrase ellipsis and gapping (Asher 1993, Kehler 1993), and tense interpretation (Lascarides and Asher 1993, Kehler 1994, Kehler 2000). An extensive investigation into the relationship between coherence relations and discourse connectives can be found in Knott and Dale (1994).

Useful surveys of discourse processing and structure include Stede (2011) and Webber et al. (2012).

Andy Kehler wrote the Discourse chapter for the 2000 first edition of this textbook, which we used as the starting point for the second-edition chapter, and there are some remnants of Andy’s lovely prose still in this third-edition coherence chapter.

# Exercises

24.1 Finish the Centering Theory processing of the last two utterances of (24.30), and show how (24.29) would be processed. Does the algorithm indeed mark (24.29) as less coherent?   
24.2 Select an editorial column from your favorite newspaper, and determine the discourse structure for a 10–20 sentence portion. What problems did you encounter? Were you helped by superficial cues the speaker included (e.g., discourse connectives) in any places?

# Bibliography

Abadi, M., A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G. Irving, M. Isard, Y. Jia, R. Jozefowicz, L. Kaiser, M. Kudlur, J. Levenberg, D. Mane,´ R. Monga, S. Moore, D. Murray, C. Olah, M. Schuster, J. Shlens, B. Steiner, I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O. Vinyals, P. War- ´ den, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng. 2015. TensorFlow: Large-scale machine learning on heterogeneous systems. Software available from tensorflow.org.   
Abney, S. P., R. E. Schapire, and Y. Singer. 1999. Boosting applied to tagging and PP attachment. EMNLP/VLC.   
Agarwal, O., S. Subramanian, A. Nenkova, and D. Roth. 2019. Evaluation of named entity coreference. Workshop on Computational Models of Reference, Anaphora and Coreference.   
Aggarwal, C. C. and C. Zhai. 2012. A survey of text classification algorithms. In C. C. Aggarwal and C. Zhai, eds, Mining text data, 163– 222. Springer.   
Agichtein, E. and L. Gravano. 2000. Snowball: Extracting relations from large plain-text collections. Proceedings of the 5th ACM International Conference on Digital Libraries.   
Agirre, E., C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre, W. Guo, I. Lopez-Gazpio, M. Maritxalar, R. Mihalcea, G. Rigau, L. Uria, and J. Wiebe. 2015. SemEval-2015 task 2: Semantic textual similarity, English, Spanish and pilot on interpretability. SemEval-15.   
Agirre, E., M. Diab, D. Cer, and A. Gonzalez-Agirre. 2012. SemEval-2012 task 6: A pilot on semantic textual similarity. SemEval12.   
Agirre, E. and D. Martinez. 2001. Learning class-to-class selectional preferences. CoNLL.   
Aho, A. V. and J. D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume 1. Prentice Hall.   
Algoet, P. H. and T. M. Cover. 1988. A sandwich proof of the ShannonMcMillan-Breiman theorem. The Annals of Probability, 16(2):899– 909.   
Allen, J. 1984. Towards a general theory of action and time. Artificial Intelligence, 23(2):123–154.   
Allen, J. and C. R. Perrault. 1980. Analyzing intention in utterances. Artificial Intelligence, 15:143–178.   
Allen, J., M. S. Hunnicut, and D. H. Klatt. 1987. From Text to Speech: The MITalk system. Cambridge University Press.   
Althoff, T., C. Danescu-NiculescuMizil, and D. Jurafsky. 2014. How to ask for a favor: A case study on the success of altruistic requests. ICWSM 2014.   
An, J., H. Kwak, and Y.-Y. Ahn. 2018. SemAxis: A lightweight framework to characterize domainspecific word semantics beyond sentiment. ACL.   
Anastasopoulos, A. and G. Neubig. 2020. Should all cross-lingual embeddings speak English? ACL.   
Antoniak, M. and D. Mimno. 2018. Evaluating the stability of embedding-based word similarities. TACL, 6:107–119.   
Aone, C. and S. W. Bennett. 1995. Evaluating automated and manual acquisition of anaphora resolution strategies. ACL.   
Ariel, M. 2001. Accessibility theory: An overview. In T. Sanders, J. Schilperoord, and W. Spooren, eds, Text Representation: Linguistic and Psycholinguistic Aspects, 29– 87. Benjamins.   
Arora, S., P. Lewis, A. Fan, J. Kahn, and C. Re. 2023. ´ Reasoning over public and private data in retrieval-based systems. TACL, 11:902–921.   
Artetxe, M. and H. Schwenk. 2019. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. TACL, 7:597– 610.   
Artstein, R., S. Gandhe, J. Gerten, A. Leuski, and D. Traum. 2009. Semi-formal evaluation of conversational characters. In Languages: From Formal to Natural, 22–35. Springer.   
Asher, N. 1993. Reference to Abstract Objects in Discourse. Studies in Linguistics and Philosophy (SLAP) 50, Kluwer.   
Asher, N. and A. Lascarides. 2003. Logics of Conversation. Cambridge University Press.   
Atal, B. S. and S. Hanauer. 1971. Speech analysis and synthesis by prediction of the speech wave. JASA,   
Austin, J. L. 1962. How to Do Things with Words. Harvard University Press.   
Awadallah, A. H., R. G. Kulkarni, U. Ozertem, and R. Jones. 2015. Charaterizing and predicting voice query reformulation. CIKM-15.   
Ba, J. L., J. R. Kiros, and G. E. Hinton. 2016. Layer normalization. NeurIPS workshop.   
Baayen, R. H. 2001. Word frequency distributions. Springer.   
Baccianella, S., A. Esuli, and F. Sebastiani. 2010. Sentiwordnet 3.0: An enhanced lexical resource for sentiment analysis and opinion mining. LREC.   
Bach, K. and R. Harnish. 1979. Linguistic communication and speech acts. MIT Press.   
Backus, J. W. 1959. The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference. Information Processing: Proceedings of the International Conference on Information Processing, Paris. UNESCO.   
Backus, J. W. 1996. Transcript of question and answer session. In R. L. Wexelblat, ed., History of Programming Languages, page 162. Academic Press.   
Bada, M., M. Eckert, D. Evans, K. Garcia, K. Shipley, D. Sitnikov, W. A. Baumgartner, K. B. Cohen, K. Verspoor, J. A. Blake, and L. E. Hunter. 2012. Concept annotation in the craft corpus. BMC bioinformatics, 13(1):161.   
Bagga, A. and B. Baldwin. 1998. Algorithms for scoring coreference chains. LREC Workshop on Linguistic Coreference.   
Bahdanau, D., K. H. Cho, and Y. Bengio. 2015. Neural machine translation by jointly learning to align and translate. ICLR 2015.   
Bahdanau, D., J. Chorowski, D. Serdyuk, P. Brakel, and Y. Bengio. 2016. End-to-end attentionbased large vocabulary speech recognition. ICASSP.   
Bahl, L. R. and R. L. Mercer. 1976. Part of speech assignment by a statistical decision algorithm. Proceedings IEEE International Symposium on Information Theory.   
Bahl, L. R., F. Jelinek, and R. L. Mercer. 1983. A maximum likelihood approach to continuous speech recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence , 5(2):179–190.   
Bajaj, P., D. Campos, N. Craswell, L. Deng, J. G. ando Xiaodong Liu, R. Majumder, A. McNamara, B. Mitra, T. Nguye, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang. 2016. MS MARCO: A human generated MAchine Reading COmprehension dataset. NeurIPS.   
Baker, C. F., C. J. Fillmore, and J. B. Lowe. 1998. The Berkeley FrameNet project. COLING/ACL.   
Baker, J. K. 1975a. The DRAGON system – An overview. IEEE Transactions on ASSP, ASSP-23(1):24–29.   
Baker, J. K. 1975b. Stochastic modeling for automatic speech understanding. In D. R. Reddy, ed., Speech Recognition. Academic Press.   
Baldridge, J., N. Asher, and J. Hunter. 2007. Annotation for and robust parsing of discourse structure on unrestricted texts. Zeitschrift fur¨ Sprachwissenschaft, 26:213–239.   
Bamman, D., O. Lewke, and A. Mansoor. 2020. An annotated dataset of coreference in English literature. LREC.   
Bamman, D., B. O’Connor, and N. A. Smith. 2013. Learning latent personas of film characters. ACL.   
Bamman, D., S. Popat, and S. Shen. 2019. An annotated dataset of literary entities. NAACL HLT.   
Banerjee, S. and A. Lavie. 2005. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments. Proceedings of ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MT and/or Summarization.   
Banko, M., M. Cafarella, S. Soderland, M. Broadhead, and O. Etzioni. 2007. Open information extraction for the web. IJCAI.   
Ban˜on, M., P. Chen, B. Haddow,´ K. Heafield, H. Hoang, M. Espla-\` Gomis, M. L. Forcada, A. Kamran, F. Kirefu, P. Koehn, S. Ortiz Rojas, L. Pla Sempere, G. Ram´ırezSanchez, E. Sarr ´ ´ıas, M. Strelec, B. Thompson, W. Waites, D. Wiggins, and J. Zaragoza. 2020. ParaCrawl: Web-scale acquisition of parallel corpora. ACL.   
Bar-Hillel, Y. 1960. The present status of automatic translation of languages. In F. Alt, ed., Advances in Computers 1, 91–163. Academic Press.   
Barker, C. 2010. Nominals don’t provide criteria of identity. In M. Rathert and A. Alexiadou, eds, The Semantics of Nominalizations across Languages and Frameworks, 9–24. Mouton.   
Barrett, L. F., B. Mesquita, K. N. Ochsner, and J. J. Gross. 2007. The experience of emotion. Annual Review of Psychology, 58:373–403.   
Barzilay, R. and M. Lapata. 2005. Modeling local coherence: An entitybased approach. ACL.   
Barzilay, R. and M. Lapata. 2008. Modeling local coherence: An entitybased approach. Computational Linguistics, 34(1):1–34.   
Barzilay, R. and L. Lee. 2004. Catching the drift: Probabilistic content models, with applications to generation and summarization. HLT-NAACL.   
Baum, L. E. and J. A. Eagon. 1967. An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology. Bulletin of the American Mathematical Society, 73(3):360–363.   
Baum, L. E. and T. Petrie. 1966. Statistical inference for probabilistic functions of finite-state Markov chains. Annals of Mathematical Statistics, 37(6):1554–1563.   
Baum, L. F. 1900. The Wizard of Oz. Available at Project Gutenberg.   
Bayes, T. 1763. An Essay Toward Solving a Problem in the Doctrine of Chances, volume 53. Reprinted in Facsimiles of Two Papers by Bayes, Hafner Publishing, 1963.   
Bazell, C. E. 1952/1966. The correspondence fallacy in structural linguistics. In E. P. Hamp, F. W. Householder, and R. Austerlitz, eds, Studies by Members of the English Department, Istanbul University (3), reprinted in Readings in Linguistics II (1966), 271–298. University of Chicago Press.   
Bean, D. and E. Riloff. 1999. Corpus-based identification of nonanaphoric noun phrases. ACL.   
Bean, D. and E. Riloff. 2004. Unsupervised learning of contextual role knowledge for coreference resolution. HLT-NAACL.   
Bedi, G., F. Carrillo, G. A. Cecchi, D. F. Slezak, M. Sigman, N. B. Mota, S. Ribeiro, D. C. Javitt, M. Copelli, and C. M. Corcoran. 2015. Automated analysis of free speech predicts psychosis onset in high-risk youths. npj Schizophrenia, 1.   
Bejcek, E., E. Haji ˇ cov ˇ a, J. Haji ´ c,ˇ P. J´ınova,´ V. Kettnerov a,´ V. Kola´ˇrova,´ M. Mikulov a,´ J. M´ırovsky,´ A. Nedoluzhko, J. Panevova,´ L. Pol akov ´ a,´ M. Sev ˇ cˇ´ıkova, J. ´ Stˇ epˇ anek, and ´ S. Zik ˇ anov ´ a. 2013. ´ Prague dependency treebank 3.0. Technical report, Institute of Formal and Applied Linguistics, Charles University aig ital library at Institute of Formal and Applied Linguistics, Charles University in Prague.   
Bellegarda, J. R. 1997. A latent semantic analysis framework for largespan language modeling. EUROSPEECH.   
Bellegarda, J. R. 2000. Exploiting latent semantic information in statistical language modeling. Proceedings of the IEEE, 89(8):1279–1296.   
Bellegarda, J. R. 2013. Natural language technology in mobile devices: Two grounding frameworks. In Mobile Speech and Advanced Natural Language Solutions, 185–196. Springer.   
Bellman, R. 1957. Dynamic Programming. Princeton University Press.   
Bellman, R. 1984. Eye of the Hurricane: an autobiography. World Scientific Singapore.   
Bender, E. M. 2019. The #BenderRule: On naming the languages we study and why it matters. Blog post.   
Bender, E. M., B. Friedman, and A. McMillan-Major. 2021. A guide for writing data statements for natural language processing. http://techpolicylab.uw. edu/data-statements/.   
Bender, E. M. and A. Koller. 2020. Climbing towards NLU: On meaning, form, and understanding in the age of data. ACL.   
Bengio, Y., A. Courville, and P. Vincent. 2013. Representation learning: A review and new perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8):1798–1828.   
Bengio, Y., R. Ducharme, and P. Vincent. 2000. A neural probabilistic language model. NeurIPS.   
Bengio, Y., R. Ducharme, P. Vincent, and C. Jauvin. 2003. A neural probabilistic language model. JMLR, 3:1137–1155.   
Bengio, Y., P. Lamblin, D. Popovici, and H. Larochelle. 2007. Greedy layer-wise training of deep networks. NeurIPS.   
Bengio, Y., H. Schwenk, J.-S. Senecal,´ F. Morin, and J.-L. Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, 137–186. Springer.   
Bengtson, E. and D. Roth. 2008. Understanding the value of features for coreference resolution. EMNLP.   
Bentivogli, L., M. Cettolo, M. Federico, and C. Federmann. 2018. Machine translation human evaluation: an investigation of evaluation based on post-editing and its relation with direct assessment. ICSLT. P. Liang. 2013. Semantic parsing on freebase from question-answer pairs. EMNLP.   
Berg-Kirkpatrick, T., D. Burkett, and D. Klein. 2012. An empirical investigation of statistical significance in NLP. EMNLP.   
Berger, A., S. A. Della Pietra, and V. J. Della Pietra. 1996. A maximum entropy approach to natural language processing. Computational Linguistics, 22(1):39–71.   
Bergsma, S. and D. Lin. 2006. Bootstrapping path-based pronoun resolution. COLING/ACL.   
Bergsma, S., D. Lin, and R. Goebel. 2008a. Discriminative learning of selectional preference from unlabeled text. EMNLP.   
Bergsma, S., D. Lin, and R. Goebel. 2008b. Distributional identification of non-referential pronouns. ACL.   
Bethard, S. 2013. ClearTK-TimeML: A minimalist approach to TempEval 2013. SemEval-13.   
Bhat, I., R. A. Bhat, M. Shrivastava, and D. Sharma. 2017. Joining hands: Exploiting monolingual treebanks for parsing of code-mixing data. EACL.   
Bianchi, F., M. Suzgun, G. Attanasio, P. Rottger, D. Jurafsky, T. Hashimoto, and J. Zou. 2024. Safety-tuned LLaMAs: Lessons from improving the safety of large language models that follow instructions. ICLR.   
Bickel, B. 2003. Referential density in discourse and syntactic typology. Language, 79(2):708–736.   
Bickmore, T. W., H. Trinh, S. Olafsson, T. K. O’Leary, R. Asadi, N. M. Rickles, and R. Cruz. 2018. Patient and consumer safety risks when using conversational assistants for medical information: An observational study of Siri, Alexa, and Google Assistant. Journal of Medical Internet Research, 20(9):e11510.   
Bikel, D. M., S. Miller, R. Schwartz, and R. Weischedel. 1997. Nymble: A high-performance learning namefinder. ANLP.   
Biran, O. and K. McKeown. 2015. PDTB discourse parsing as a tagging task: The two taggers approach. SIGDIAL.   
Bird, S., E. Klein, and E. Loper. 2009. Natural Language Processing with Python. O’Reilly.   
Bisani, M. and H. Ney. 2004. Bootstrap estimates for confidence intervals in ASR performance evaluation. ICASSP.   
Bishop, C. M. 2006. Pattern recognition and machine learning. Springer.   
Bisk, Y., A. Holtzman, J. Thomason, J. Andreas, Y. Bengio, J. Chai, M. Lapata, A. Lazaridou, J. May, A. Nisnevich, N. Pinto, and J. Turian. 2020. Experience grounds language. EMNLP.   
Bizer, C., J. Lehmann, G. Kobilarov, S. Auer, C. Becker, R. Cyganiak, and S. Hellmann. 2009. DBpedia— A crystallization point for the Web of Data. Web Semantics: science, services and agents on the world wide web, 7(3):154–165.   
Bjorkelund, A. and J. Kuhn. 2014. ¨ Learning structured perceptrons for coreference resolution with latent antecedents and non-local features. ACL.   
Black, A. W. and P. Taylor. 1994. CHATR: A generic speech synthesis system. COLING.   
Black, E., S. P. Abney, D. Flickinger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. L. Klavans, M. Y. Liberman, M. P. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. Speech and Natural Language Workshop.   
Blei, D. M., A. Y. Ng, and M. I. Jordan. 2003. Latent Dirichlet allocation. JMLR, 3(5):993–1022.   
Blodgett, S. L., S. Barocas, H. Daume III, and H. Wallach. 2020. ´ Language (technology) is power: A critical survey of “bias” in NLP. ACL.   
Blodgett, S. L., L. Green, and B. O’Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. EMNLP.   
Blodgett, S. L. and B. O’Connor. 2017. Racial disparity in natural language processing: A case study of social media African-American English. FAT/ML Workshop, KDD.   
Bloomfield, L. 1914. An Introduction to the Study of Language. Henry Holt and Company.   
Bloomfield, L. 1933. Language. University of Chicago Press.   
Bobrow, D. G., R. M. Kaplan, M. Kay, D. A. Norman, H. Thompson, and T. Winograd. 1977. GUS, A frame driven dialog system. Artificial Intelligence, 8:155–173.   
Bobrow, D. G. and D. A. Norman. 1975. Some principles of memory schemata. In D. G. Bobrow and A. Collins, eds, Representation and Understanding. Academic Press.   
Bojanowski, P., E. Grave, A. Joulin, and T. Mikolov. 2017. Enriching word vectors with subword information. TACL, 5:135–146. ollacker, K., C. Evans, P. Paritosh, T. Sturge, and J. Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. SIGMOD 2008. olukbasi, T., K.-W. Chang, J. Zou, V. Saligrama, and A. T. Kalai. 2016. Man is to computer programmer as woman is to homemaker? Debiasing word embeddings. NeurIPS.   
ommasani, R., D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, E. Brynjolfsson, S. Buch, D. Card, R. Castellon, N. S. Chatterji, A. S. Chen, K. A. Creel, J. Davis, D. Demszky, C. Donahue, M. Doumbouya, E. Durmus, S. Ermon, J. Etchemendy, K. Ethayarajh, L. Fei-Fei, C. Finn, T. Gale, L. E. Gillespie, K. Goel, N. D. Goodman, S. Grossman, N. Guha, T. Hashimoto, P. Henderson, J. Hewitt, D. E. Ho, J. Hong, K. Hsu, J. Huang, T. F. Icard, S. Jain, D. Jurafsky, P. Kalluri, S. Karamcheti, G. Keeling, F. Khani, O. Khattab, P. W. Koh, M. S. Krass, R. Krishna, R. Kuditipudi, A. Kumar, F. Ladhak, M. Lee, T. Lee, J. Leskovec, I. Levent, X. L. Li, X. Li, T. Ma, A. Malik, C. D. Manning, S. P. Mirchandani, E. Mitchell, Z. Munyikwa, S. Nair, A. Narayan, D. Narayanan, B. Newman, A. Nie, J. C. Niebles, H. Nilforoshan, J. F. Nyarko, G. Ogut, L. Orr, I. Papadimitriou, J. S. Park, C. Piech, E. Portelance, C. Potts, A. Raghunathan, R. Reich, H. Ren, F. Rong, Y. H. Roohani, C. Ruiz, J. Ryan, C. R’e, D. Sadigh, S. Sagawa, K. Santhanam, A. Shih, K. P. Srinivasan, A. Tamkin, R. Taori, A. W. Thomas, F. Tramer, R. E. Wang, W. Wang, \` B. Wu, J. Wu, Y. Wu, S. M. Xie, M. Yasunaga, J. You, M. A. Zaharia, M. Zhang, T. Zhang, X. Zhang, Y. Zhang, L. Zheng, K. Zhou, and P. Liang. 2021. On the opportunities and risks of foundation models. ArXiv.   
ooth, T. L. 1969. Probabilistic representation of formal languages. IEEE Conference Record of the 1969 Tenth Annual Symposium on Switching and Automata Theory.   
orges, J. L. 1964. The analytical language of john wilkins. In Other inquisitions 1937–1952. University of Texas Press. Trans. Ruth L. C. Simms.   
ostrom, K. and G. Durrett. 2020. Byte pair encoding is suboptimal for language model pretraining. EMNLP. ourlard, H. and N. Morgan. 1994. Connectionist Speech Recognition: A Hybrid Approach. Kluwer.   
Brants, T. 2000. TnT: A statistical partof-speech tagger. ANLP.   
Brants, T., A. C. Popat, P. Xu, F. J. Och, and J. Dean. 2007. Large language models in machine translation. EMNLP/CoNLL.   
Braud, C., M. Coavoux, and A. Søgaard. 2017. Cross-lingual RST discourse parsing. EACL.   
Breal, M. 1897. ´ Essai de Semantique: ´ Science des significations. Hachette.   
Brennan, S. E., M. W. Friedman, and C. Pollard. 1987. A centering approach to pronouns. ACL.   
Brin, S. 1998. Extracting patterns and relations from the World Wide Web. Proceedings World Wide Web and Databases International Workshop, Number 1590 in LNCS. Springer.   
Brockmann, C. and M. Lapata. 2003. Evaluating and combining approaches to selectional preference acquisition. EACL.   
Broschart, J. 1997. Why Tongan does it differently. Linguistic Typology, 1:123–165.   
Brown, P. F., J. Cocke, S. A. Della Pietra, V. J. Della Pietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin. 1990. A statistical approach to machine translation. Computational Linguistics, 16(2):79–85.   
Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.   
Brown, T., B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. 2020. Language models are few-shot learners. NeurIPS, volume 33.   
Bruce, B. C. 1975. Generation as a social action. Proceedings of TINLAP1 (Theoretical Issues in Natural Language Processing).   
Brysbaert, M., A. B. Warriner, and V. Kuperman. 2014. Concreteness ratings for 40 thousand generally known English word lemmas. Behavior Research Methods, 46(3):904–911.   
Bu, H., J. Du, X. Na, B. Wu, and H. Zheng. 2017. AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline. O-COCOSDA Proceedings.   
Buchholz, S. and E. Marsi. 2006. Conllx shared task on multilingual dependency parsing. CoNLL.   
Budanitsky, A. and G. Hirst. 2006. Evaluating WordNet-based measures of lexical semantic relatedness. Computational Linguistics, 32(1):13–47.   
Budzianowski, P., T.-H. Wen, B.- H. Tseng, I. Casanueva, S. Ultes, O. Ramadan, and M. Gasiˇ c. 2018. ´ MultiWOZ - a large-scale multidomain wizard-of-Oz dataset for task-oriented dialogue modelling. EMNLP.   
Bullinaria, J. A. and J. P. Levy. 2007. Extracting semantic representations from word co-occurrence statistics: A computational study. Behavior research methods, 39(3):510–526.   
Bullinaria, J. A. and J. P. Levy. 2012. Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and SVD. Behavior research methods, 44(3):890–907.   
Bulyko, I., K. Kirchhoff, M. Ostendorf, and J. Goldberg. 2005. Errorsensitive response generation in a spoken language dialogue system. Speech Communication, 45(3):271– 288.   
Caliskan, A., J. J. Bryson, and A. Narayanan. 2017. Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334):183–186.   
Callison-Burch, C., M. Osborne, and P. Koehn. 2006. Re-evaluating the role of BLEU in machine translation research. EACL.   
Canavan, A., D. Graff, and G. Zipperlen. 1997. CALLHOME American English speech LDC97S42. Linguistic Data Consortium.   
Carbonell, J. R. 1970. AI in CAI: An artificial-intelligence approach to computer-assisted instruction. IEEE transactions on manmachine systems, 11(4):190–202.   
Cardie, C. 1993. A case-based approach to knowledge acquisition for domain specific sentence analysis. AAAI.   
Cardie, C. 1994. Domain-Specific Knowledge Acquisition for Conceptual Sentence Analysis. Ph.D. thesis, University of Massachusetts, Amherst, MA. Available as CMPSCI Technical Report 94-74.   
Cardie, C. and K. Wagstaff. 1999. Noun phrase coreference as clustering. EMNLP/VLC.   
Carlini, N., F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss, K. Lee, A. Roberts, T. Brown, D. Song, U. Erlingsson, et al. 2021. Fv language models. 30th USENIX Security Symposium (USENIX Security 21).   
Carlson, G. N. 1977. Reference to kinds in English. Ph.D. thesis, University of Massachusetts, Amherst. Forward.   
Carlson, L. and D. Marcu. 2001. Discourse tagging manual. Technical Report ISI-TR-545, ISI.   
Carlson, L., D. Marcu, and M. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. SIGDIAL.   
Carreras, X. and L. Marquez. 2005. \` Introduction to the CoNLL-2005 shared task: Semantic role labeling. CoNLL.   
Chafe, W. L. 1976. Givenness, contrastiveness, definiteness, subjects, topics, and point of view. In C. N. Li, ed., Subject and Topic, 25–55. Academic Press.   
Chambers, N. 2013. NavyTime: Event and time ordering from raw text. SemEval-13.   
Chambers, N., T. Cassidy, B. McDowell, and S. Bethard. 2014. Dense event ordering with a multi-pass architecture. TACL, 2:273–284.   
Chambers, N. and D. Jurafsky. 2010. Improving the use of pseudo-words for evaluating selectional preferences. ACL.   
Chambers, N. and D. Jurafsky. 2011. Template-based information extraction without the templates. ACL.   
Chan, W., N. Jaitly, Q. Le, and O. Vinyals. 2016. Listen, attend and spell: A neural network for large vocabulary conversational speech recognition. ICASSP.   
Chandioux, J. 1976. MET´ EO´ : un systeme op \` erationnel pour la tra- ´ duction automatique des bulletins met´ eorologiques destin ´ es au grand ´ public. Meta, 21:127–133.   
Chang, A. X. and C. D. Manning. 2012. SUTime: A library for recognizing and normalizing time expressions. LREC.   
Chang, K.-W., R. Samdani, and D. Roth. 2013. A constrained latent variable model for coreference resolution. EMNLP.   
Chang, K.-W., R. Samdani, A. Rozovskaya, M. Sammons, and D. Roth. 2012. Illinois-Coref: The UI system in the CoNLL-2012 shared task. CoNLL.   
Chaplot, D. S. and R. Salakhutdinov. 2018. Knowledge-based word sense disambiguation using topic models. AAAI. ing with a context-free grammar and word statistics. AAAI.   
Charniak, E., C. Hendrickson, N. Jacobson, and M. Perkowitz. 1993. Equations for part-of-speech tagging. AAAI.   
Che, W., Z. Li, Y. Li, Y. Guo, B. Qin, and T. Liu. 2009. Multilingual dependency-based syntactic and semantic parsing. CoNLL.   
Chen, C. and V. Ng. 2013. Linguistically aware coreference evaluation metrics. IJCNLP.   
Chen, D., A. Fisch, J. Weston, and A. Bordes. 2017a. Reading Wikipedia to answer open-domain questions. ACL.   
Chen, D. and C. Manning. 2014. A fast and accurate dependency parser using neural networks. EMNLP.   
Chen, E., B. Snyder, and R. Barzilay. 2007. Incremental text structuring with online hierarchical ranking. EMNLP/CoNLL.   
Chen, S. F. and J. Goodman. 1999. An empirical study of smoothing techniques for language modeling. Computer Speech and Language, 13:359–394.   
Chen, X., Z. Shi, X. Qiu, and X. Huang. 2017b. Adversarial multi-criteria learning for Chinese word segmentation. ACL.   
Cheng, J., L. Dong, and M. Lapata. 2016. Long short-term memory-networks for machine reading. EMNLP.   
Cheng, M., E. Durmus, and D. Jurafsky. 2023. Marked personas: Using natural language prompts to measure stereotypes in language models. ACL.   
Chiang, D. 2005. A hierarchical phrasebased model for statistical machine translation. ACL.   
Chinchor, N., L. Hirschman, and D. L. Lewis. 1993. Evaluating Message Understanding systems: An analysis of the third Message Understanding Conference. Computational Linguistics, 19(3):409–449.   
Chiticariu, L., M. Danilevsky, Y. Li, F. Reiss, and H. Zhu. 2018. SystemT: Declarative text understanding for enterprise. NAACL HLT, volume 3.   
Chiticariu, L., Y. Li, and F. R. Reiss. 2013. Rule-Based Information Extraction is Dead! Long Live RuleBased Information Extraction Sys tems! EMNLP.   
Chiu, J. P. C. and E. Nichols. 2016. Named entity recognition with bidirectional LSTM-CNNs. TACL, 4:357–370.   
Cho, K., B. van Merrienboer, C. Gul- ¨ cehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio. 2014. Learning phrase representations using RNN encoder–decoder for statistical machine translation. EMNLP.   
Choe, D. K. and E. Charniak. 2016. Parsing as language modeling. EMNLP.   
Choi, J. D. and M. Palmer. 2011a. Getting the most out of transition-based dependency parsing. ACL.   
Choi, J. D. and M. Palmer. 2011b. Transition-based semantic role labeling using predicate argument clustering. Proceedings of the ACL 2011 Workshop on Relational Models of Semantics.   
Choi, J. D., J. Tetreault, and A. Stent. 2015. It depends: Dependency parser comparison using a webbased evaluation tool. ACL.   
Chomsky, N. 1956. Three models for the description of language. IRE Transactions on Information Theory, 2(3):113–124.   
Chomsky, N. 1956/1975. The Logical Structure of Linguistic Theory. Plenum.   
Chomsky, N. 1957. Syntactic Structures. Mouton.   
Chomsky, N. 1963. Formal properties of grammars. In R. D. Luce, R. Bush, and E. Galanter, eds, Handbook of Mathematical Psychology, volume 2, 323–418. Wiley.   
Chomsky, N. 1981. Lectures on Government and Binding. Foris.   
Chorowski, J., D. Bahdanau, K. Cho, and Y. Bengio. 2014. End-to-end continuous speech recognition using attention-based recurrent NN: First results. NeurIPS Deep Learning and Representation Learning Workshop.   
Chou, W., C.-H. Lee, and B. H. Juang. 1993. Minimum error rate training based on $n$ -best string models. ICASSP.   
Christiano, P. F., J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. 2017. Deep reinforcement learning from human preferences. NeurIPS, volume 30.   
Christodoulopoulos, C., S. Goldwater, and M. Steedman. 2010. Two decades of unsupervised POS induction: How far have we come? EMNLP.   
Chu, Y.-J. and T.-H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400.   
Chu-Carroll, J. 1998. A statistical model for discourse act recognition in dialogue interactions. Applying Machine Learning to Discourse Processing. Papers from the 1998 AAAI Spring Symposium. Tech. rep. SS98-01. AAAI Press.   
Chu-Carroll, J. and S. Carberry. 1998. Collaborative response generation in planning dialogues. Computational Linguistics, 24(3):355–400.   
Church, K. W. 1988. A stochastic parts program and noun phrase parser for unrestricted text. ANLP.   
Church, K. W. 1989. A stochastic parts program and noun phrase parser for unrestricted text. ICASSP.   
Church, K. W. 1994. Unix for Poets. Slides from 2nd ELSNET Summer School and unpublished paper ms.   
Church, K. W. and W. A. Gale. 1991. A comparison of the enhanced GoodTuring and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, 5:19–54.   
Church, K. W. and P. Hanks. 1989. Word association norms, mutual information, and lexicography. ACL.   
Church, K. W. and P. Hanks. 1990. Word association norms, mutual information, and lexicography. Computational Linguistics, 16(1):22–29.   
Cialdini, R. B. 1984. Influence: The psychology of persuasion. Morrow.   
Cieri, C., D. Miller, and K. Walker. 2004. The Fisher corpus: A resource for the next generations of speechto-text. LREC.   
Clark, E. 1987. The principle of contrast: A constraint on language acquisition. In B. MacWhinney, ed., Mechanisms of language acquisition, 1–33. LEA.   
Clark, H. H. 1996. Using Language. Cambridge University Press.   
Clark, H. H. and J. E. Fox Tree. 2002. Using uh and um in spontaneous speaking. Cognition, 84:73–111.   
Clark, H. H. and C. Marshall. 1981. Definite reference and mutual knowledge. In A. K. Joshi, B. L. Webber, and I. A. Sag, eds, Elements of Discourse Understanding, 10–63. Cambridge.   
Clark, H. H. and D. Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1–39.   
Clark, J. H., E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Nikolaev, and J. Palomaki. 2020a. TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. TACL, 8:454–470.   
Clark, K., M.-T. Luong, Q. V. Le, and C. D. Manning. 2020b. Electra: Pretraining text encoders as discriminators rather than generators. ICLR.   
Clark, K. and C. D. Manning. 2015. Entity-centric coreference resolution with model stacking. ACL.   
Clark, K. and C. D. Manning. 2016a. Deep reinforcement learning for mention-ranking coreference models. EMNLP.   
Clark, K. and C. D. Manning. 2016b. Improving coreference resolution by learning entity-level distributed representations. ACL.   
Clark, S., J. R. Curran, and M. Osborne. 2003. Bootstrapping POS-taggers using unlabelled data. CoNLL.   
Cobbe, K., V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton, R. Nakano, C. Hesse, and J. Schulman. 2021. Training verifiers to solve math word problems. ArXiv preprint.   
Coccaro, N. and D. Jurafsky. 1998. Towards better integration of semantic predictors in statistical language modeling. ICSLP.   
Coenen, A., E. Reif, A. Yuan, B. Kim, A. Pearce, F. Viegas, and M. Watten-´ berg. 2019. Visualizing and measuring the geometry of bert. NeurIPS.   
Cohen, A. D., A. Roberts, A. Molina, A. Butryna, A. Jin, A. Kulshreshtha, B. Hutchinson, B. Zevenbergen, B. H. Aguera-Arcas, C. ching Chang, C. Cui, C. Du, D. D. F. Adiwardana, D. Chen, D. D. Lepikhin, E. H. Chi, E. Hoffman-John, H.-T. Cheng, H. Lee, I. Krivokon, J. Qin, J. Hall, J. Fenton, J. Soraker, K. Meier-Hellstern, K. Olson, L. M. Aroyo, M. P. Bosma, M. J. Pickett, M. A. Menegali, M. Croak, M. D´ıaz, M. Lamm, M. Krikun, M. R. Morris, N. Shazeer, Q. V. Le, R. Bernstein, R. Rajakumar, R. Kurzweil, R. Thoppilan, S. Zheng, T. Bos, T. Duke, T. Doshi, V. Y. Zhao, V. Prabhakaran, W. Rusch, Y. Li, Y. Huang, Y. Zhou, Y. Xu, and Z. Chen. 2022. Lamda: Language models for dialog applications. ArXiv preprint.   
Cohen, M. H., J. P. Giangola, and J. Balogh. 2004. Voice User Interface Design. Addison-Wesley.   
Cohen, P. R. and C. R. Perrault. 1979. Elements of a plan-based theory of speech acts. Cognitive Science, 3(3):177–212.   
Colby, K. M., S. Weber, and F. D. Hilf. 1971. Artificial paranoia. Artificial Intelligence, 2(1):1–25.   
Cole, R. A., D. G. Novick, P. J. E. Vermeulen, S. Sutton, M. Fanty, L. F. A. Wessels, J. H. de Villiers, J. Schalkwyk, B. Hansen, and D. Burnett. 1997. Experiments with a spoken dialogue system for taking the US census. Speech Communication, 23:243–260.   
Collins, M. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania, Philadelphia.   
Collobert, R. and J. Weston. 2007. Fast semantic extraction using a novel neural network architecture. ACL.   
Collobert, R. and J. Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. ICML.   
Collobert, R., J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa. 2011. Natural language processing (almost) from scratch. JMLR, 12:2493–2537.   
Comrie, B. 1989. Language Universals and Linguistic Typology, 2nd edition. Blackwell.   
Conneau, A., K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzman, E. Grave, M. Ott, ´ L. Zettlemoyer, and V. Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. ACL.   
Connolly, D., J. D. Burger, and D. S. Day. 1994. A machine learning approach to anaphoric reference. Proceedings of the International Conference on New Methods in Language Processing (NeMLaP).   
Cooley, J. W. and J. W. Tukey. 1965. An algorithm for the machine calculation of complex Fourier series. Mathematics of Computation, 19(90):297–301.   
Cooper, F. S., A. M. Liberman, and J. M. Borst. 1951. The interconversion of audible and visible patterns as a basis for research in the perception of speech. Proceedings of the National Academy of Sciences, 37(5):318–325.   
Cordier, B. 1965. Factor-analysis of correspondences. COLING 1965.   
Costa-jussa, M. R., J. Cross, O. C¸ elebi, \` M. Elbayad, K. Heafield, K. Heffernan, E. Kalbassi, J. Lam, D. Licht, J. Maillard, A. Sun, S. Wang, G. Wenzek, A. Youngblood, B. Akula, L. Barrault, G. M. Gonzalez, P. Hansanti, J. Hoffman, S. Jarrett, K. R. Sadagopan, D. Rowe, S. Spruit, C. Tran, P. Andrews, N. F. Ayan, S. Bhosale, S. Edunov, A. Fan, C. Gao, V. Goswami, F. Guzman, ´ P. Koehn, A. Mourachko, C. Ropers, S. Saleem, H. Schwenk, J. Wang, and NLLB Team. 2022. No language left behind: Scaling humancentered machine translation. ArXiv.   
Cover, T. M. and J. A. Thomas. 1991. Elements of Information Theory. Wiley.   
Covington, M. 2001. A fundamental algorithm for dependency parsing. Proceedings of the 39th Annual ACM Southeast Conference.   
Cox, D. 1969. Analysis of Binary Data. Chapman and Hall, London.   
Craven, M. and J. Kumlien. 1999. Constructing biological knowledge bases by extracting information from text sources. ISMB-99.   
Crawford, K. 2017. The trouble with bias. Keynote at NeurIPS.   
Croft, W. 1990. Typology and Universals. Cambridge University Press.   
Crosbie, J. and E. Shutova. 2022. Induction heads as an essential mechanism for pattern matching in incontext learning. ArXiv preprint.   
Cross, J. and L. Huang. 2016. Spanbased constituency parsing with a structure-label system and provably optimal dynamic oracles. EMNLP.   
Cruse, D. A. 2004. Meaning in Language: an Introduction to Semantics and Pragmatics. Oxford University Press. Second edition.   
Cucerzan, S. 2007. Large-scale named entity disambiguation based on Wikipedia data. EMNLP/CoNLL.   
Dagan, I., S. Marcus, and S. Markovitch. 1993. Contextual word similarity and estimation from sparse data. ACL.   
Dahl, G. E., T. N. Sainath, and G. E. Hinton. 2013. Improving deep neural networks for LVCSR using rectified linear units and dropout. ICASSP.   
Dahl, G. E., D. Yu, L. Deng, and A. Acero. 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on audio, speech, and language processing, 20(1):30–42.   
Dahl, M., V. Magesh, M. Suzgun, and D. E. Ho. 2024. Large legal fictions: Profiling legal hallucinations in large language models. Journal of Legal Analysis, 16:64–93.   
Dai, A. M. and Q. V. Le. 2015. Semi-supervised sequence learning. NeurIPS.   
Danieli, M. and E. Gerbino. 1995. Metrics for evaluating dialogue strategies in a spoken language system. AAAI Spring Symposium on Empirical Methods in Discourse Interpretation and Generation.   
Das, S. R. and M. Y. Chen. 2001. Yahoo! for Amazon: Sentiment parsing from small talk on the web. EFA 2001 Barcelona Meetings. http:// ssrn.com/abstract=276189.   
David, Jr., E. E. and O. G. Selfridge. 1962. Eyes and ears for computers. Proceedings of the IRE (Institute of Radio Engineers), 50:1093–1101.   
Davidson, T., D. Bhattacharya, and I. Weber. 2019. Racial bias in hate speech and abusive language detection datasets. Third Workshop on Abusive Language Online.   
Davies, M. 2012. Expanding horizons in historical linguistics with the 400-million word Corpus of Historical American English. Corpora, 7(2):121–157.   
Davies, M. 2015. The Wikipedia Corpus: 4.6 million articles, 1.9 billion words. Adapted from Wikipedia. https://www. english-corpora.org/wiki/.   
Davies, M. 2020. The Corpus of Contemporary American English (COCA): One billion words, 1990-2019. https://www. english-corpora.org/coca/.   
Davis, E., L. Morgenstern, and C. L. Ortiz. 2017. The first Winograd schema challenge at IJCAI-16. AI Magazine, 38(3):97–98.   
Davis, K. H., R. Biddulph, and S. Balashek. 1952. Automatic recognition of spoken digits. JASA, 24(6):637– 642.   
Davis, S. and P. Mermelstein. 1980. Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. IEEE Transactions on ASSP, 28(4):357–366.   
Deerwester, S. C., S. T. Dumais, G. W. Furnas, R. A. Harshman, T. K. Landauer, K. E. Lochbaum, and L. Streeter. 1988. Computer information retrieval using latent semantic structure: US Patent 4,839,853.   
Deerwester, S. C., S. T. Dumais, T. K. Landauer, G. W. Furnas, and R. A. Harshman. 1990. Indexing by latent semantics analysis. JASIS, 41(6):391–407.   
Deibel, D. and R. Evanhoe. 2021. Conversations with Things: UX Design for Chat and Voice. Rosenfeld.   
DeJong, G. F. 1982. An overview of the FRUMP system. In W. G. Lehnert and M. H. Ringle, eds, Strategies for Natural Language Processing, 149– 176. LEA.   
Demberg, V. 2006. Letter-to-phoneme conversion for a German text-tospeech system. Diplomarbeit Nr. 47, Universitat Stuttgart. ¨   
Denes, P. 1959. The design and operation of the mechanical speech recognizer at University College London. Journal of the British Institution of Radio Engineers, 19(4):219– 234. Appears together with companion paper (Fry 1959).   
Deng, L., G. Hinton, and B. Kingsbury. 2013. New types of deep neural network learning for speech recognition and related applications: An overview. ICASSP.   
Deng, Y. and W. Byrne. 2005. HMM word and phrase alignment for statistical machine translation. HLTEMNLP.   
Denis, P. and J. Baldridge. 2007. Joint determination of anaphoricity and coreference resolution using integer programming. NAACL-HLT.   
Denis, P. and J. Baldridge. 2008. Specialized models and ranking for coreference resolution. EMNLP.   
Denis, P. and J. Baldridge. 2009. Global joint models for coreference resolution and named entity classification. Procesamiento del Lenguaje Natural, 42.   
DeRose, S. J. 1988. Grammatical category disambiguation by statistical optimization. Computational Linguistics, 14:31–39.   
Devlin, J., M.-W. Chang, K. Lee, and K. Toutanova. 2019. BERT: Pretraining of deep bidirectional transformers for language understanding. NAACL HLT.   
Di Eugenio, B. 1990. Centering theory and the Italian pronominal system. COLING.   
Di Eugenio, B. 1996. The discourse functions of Italian subjects: A centering approach. COLING.   
Dias Oliva, T., D. Antonialli, and A. Gomes. 2021. Fighting hate speech, silencing drag queens? artificial intelligence in content moderation and risks to lgbtq voices online. Sexuality & Culture, 25:700–732.   
Dinan, E., G. Abercrombie, A. S. Bergman, S. Spruit, D. Hovy, Y.-L. Boureau, and V. Rieser. 2021. Anticipating safety issues in e2e conversational ai: Framework and tooling. ArXiv.   
Dinan, E., A. Fan, A. Williams, J. Urbanek, D. Kiela, and J. Weston. 2020. Queens are powerful too: Mitigating gender bias in dialogue generation. EMNLP.   
Ditman, T. and G. R. Kuperberg. 2010. Building coherence: A framework for exploring the breakdown of links across clause boundaries in schizophrenia. Journal of neurolinguistics, 23(3):254–269.   
Dixon, L., J. Li, J. Sorensen, N. Thain, and L. Vasserman. 2018. Measuring and mitigating unintended bias in text classification. 2018 AAAI/ACM Conference on AI, Ethics, and Society.   
Dixon, N. and H. Maxey. 1968. Terminal analog synthesis of continuous speech using the diphone method of segment assembly. IEEE Transactions on Audio and Electroacoustics, 16(1):40–50.   
Do, Q. N. T., S. Bethard, and M.-F. Moens. 2017. Improving implicit semantic role labeling by predicting semantic frame arguments. IJCNLP.   
Doddington, G. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. HLT.   
Dodge, J., S. Gururangan, D. Card, R. Schwartz, and N. A. Smith. 2019. Show your work: Improved reporting of experimental results. EMNLP.   
Dodge, J., M. Sap, A. Marasovic,´ W. Agnew, G. Ilharco, D. Groeneveld, M. Mitchell, and M. Gardner. 2021. Documenting large webtext corpora: A case study on the colossal clean crawled corpus. EMNLP.   
Dong, L. and M. Lapata. 2016. Language to logical form with neural attention. ACL.   
Dorr, B. 1994. Machine translation divergences: A formal description and proposed solution. Computational Linguistics, 20(4):597–633.   
Dostert, L. 1955. The GeorgetownI.B.M. experiment. In Machine Translation of Languages: Fourteen Essays, 124–135. MIT Press.   
Dowty, D. R. 1979. Word Meaning and Montague Grammar. D. Reidel.   
Dozat, T. and C. D. Manning. 2017. Deep biaffine attention for neural dependency parsing. ICLR.   
Dozat, T. and C. D. Manning. 2018. Simpler but more accurate semantic dependency parsing. ACL.   
Dozat, T., P. Qi, and C. D. Manning. 2017. Stanford’s graph-based neural dependency parser at the CoNLL 2017 shared task. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.   
Dror, R., G. Baumer, M. Bogomolov, and R. Reichart. 2017. Replicability analysis for natural language processing: Testing significance with multiple datasets. TACL, 5:471– –486.   
Dror, R., L. Peled-Cohen, S. Shlomov, and R. Reichart. 2020. Statistical Significance Testing for Natural Language Processing, volume 45 of Synthesis Lectures on Human Language Technologies. Morgan & Claypool.   
Dryer, M. S. and M. Haspelmath, eds. 2013. The World Atlas of Language Structures Online. Max Planck Institute for Evolutionary Anthropology, Leipzig. Available online at http://wals.info.   
Du Bois, J. W., W. L. Chafe, C. Meyer, S. A. Thompson, R. Englebretson, and N. Martey. 2005. Santa Barbara corpus of spoken American English, Parts 1-4. Philadelphia: Linguistic Data Consortium.   
Durrett, G. and D. Klein. 2013. Easy victories and uphill battles in coreference resolution. EMNLP.   
Durrett, G. and D. Klein. 2014. A joint model for entity analysis: Coreference, typing, and linking. TACL, 2:477–490.   
Earley, J. 1968. An Efficient ContextFree Parsing Algorithm. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.   
Earley, J. 1970. An efficient contextfree parsing algorithm. CACM, 6(8):451–455.   
Ebden, P. and R. Sproat. 2015. The Kestrel TTS text normalization system. Natural Language Engineering, 21(3):333.   
Edmonds, J. 1967. Optimum branchings. Journal of Research of the National Bureau of Standards B, 71(4):233–240.   
Edunov, S., M. Ott, M. Auli, and D. Grangier. 2018. Understanding back-translation at scale. EMNLP.   
Efron, B. and R. J. Tibshirani. 1993. An introduction to the bootstrap. CRC press.   
Egghe, L. 2007. Untangling Herdan’s law and Heaps’ law: Mathematical and informetric arguments. JASIST, 58(5):702–709.   
Eisner, J. 1996. Three new probabilistic models for dependency parsing: An exploration. COLING.   
Ekman, P. 1999. Basic emotions. In T. Dalgleish and M. J. Power, eds, Handbook of Cognition and Emotion, 45–60. Wiley.   
Elhage, N., N. Nanda, C. Olsson, T. Henighan, N. Joseph, B. Mann, A. Askell, Y. Bai, A. Chen, T. Conerly, N. DasSarma, D. Drain, D. Ganguli, Z. Hatfield-Dodds, D. Hernandez, A. Jones, J. Kernion, L. Lovitt, K. Ndousse, D. Amodei, T. Brown, J. Clark, J. Kaplan, S. McCandlish, and C. Olah. 2021. A mathematical framework for transformer circuits. White paper.   
Elman, J. L. 1990. Finding structure in time. Cognitive science, 14(2):179– 211.   
Elsner, M., J. Austerweil, and E. Charniak. 2007. A unified local and global model for discourse coherence. NAACL-HLT.   
Elsner, M. and E. Charniak. 2008. Coreference-inspired coherence modeling. ACL.   
Elsner, M. and E. Charniak. 2011. Extending the entity grid with entityspecific features. ACL.   
Elvevag, B., P. W. Foltz, D. R. ˚ Weinberger, and T. E. Goldberg. 2007. Quantifying incoherence in speech: an automated methodology and novel application to schizophrenia. Schizophrenia research, 93(1- 3):304–316.   
Emami, A. and F. Jelinek. 2005. A neural syntactic language model. Machine learning, 60(1):195–227.   
Emami, A., P. Trichelair, A. Trischler, K. Suleman, H. Schulz, and J. C. K. Cheung. 2019. The KNOWREF coreference corpus: Removing gender and number cues for difficult pronominal anaphora resolution. ACL.   
Erk, K. 2007. A simple, similaritybased model for selectional preferences. ACL.   
van Esch, D. and R. Sproat. 2018. An expanded taxonomy of semiotic classes for text normalization. INTERSPEECH.   
Ethayarajh, K. 2019. How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings. EMNLP.   
Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019a. Towards understanding linear word analogies. ACL.   
Ethayarajh, K., D. Duvenaud, and G. Hirst. 2019b. Understanding undesirable word embedding associations. ACL.   
Ethayarajh, K. and D. Jurafsky. 2020. Utility is in the eye of the user: A critique of NLP leaderboards. EMNLP.   
Etzioni, O., M. Cafarella, D. Downey, A.-M. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: An experimental study. Artificial Intelligence, 165(1):91–134.   
Evans, N. 2000. Word classes in the world’s languages. In G. Booij, C. Lehmann, and J. Mugdan, eds, Morphology: A Handbook on Inflection and Word Formation, 708–732. Mouton.   
Fader, A., S. Soderland, and O. Etzioni. 2011. Identifying relations for open information extraction. EMNLP.   
Fan, A., S. Bhosale, H. Schwenk, Z. Ma, A. El-Kishky, S. Goyal, M. Baines, O. Celebi, G. Wenzek, V. Chaudhary, N. Goyal, T. Birch, V. Liptchinsky, S. Edunov, M. Auli, and A. Joulin. 2021. Beyond english-centric multilingual machine translation. JMLR, 22(107):1– 48.   
Fano, R. M. 1961. Transmission of Information: A Statistical Theory of Communications. MIT Press.   
Fant, G. M. 1951. Speech communication research. Ing. Vetenskaps Akad. Stockholm, Sweden, 24:331–337.   
Fant, G. M. 1986. Glottal flow: Models and interaction. Journal of Phonetics, 14:393–399.   
Fast, E., B. Chen, and M. S. Bernstein. 2016. Empath: Understanding Topic Signals in Large-Scale Text. CHI.   
Fauconnier, G. and M. Turner. 2008. The way we think: Conceptual blending and the mind’s hidden complexities. Basic Books.   
Feldman, J. A. and D. H. Ballard. 1982. Connectionist models and their properties. Cognitive Science, 6:205–254.   
Fellbaum, C., ed. 1998. WordNet: An Electronic Lexical Database. MIT Press.   
Feng, V. W. and G. Hirst. 2011. Classifying arguments by scheme. ACL.   
Feng, V. W. and G. Hirst. 2014. A linear-time bottom-up discourse parser with constraints and postediting. ACL.   
Feng, V. W., Z. Lin, and G. Hirst. 2014. The impact of deep hierarchical discourse structures in the evaluation of text coherence. COLING.   
Fernandes, E. R., C. N. dos Santos, and R. L. Milidiu. 2012. ´ Latent structure perceptron with feature induction for unrestricted coreference resolution. CoNLL.   
Ferragina, P. and U. Scaiella. 2011. Fast and accurate annotation of short texts with wikipedia pages. IEEE Software, 29(1):70–75.   
Ferro, L., L. Gerber, I. Mani, B. Sundheim, and G. Wilson. 2005. Tides 2005 standard for the annotation of temporal expressions. Technical report, MITRE.   
Ferrucci, D. A. 2012. Introduction to “This is Watson”. IBM Journal of Research and Development, 56(3/4):1:1–1:15.   
Fessler, L. 2017. We tested bots like Siri and Alexa to see who would stand up to sexual harassment. Quartz. Feb 22, 2017. https://qz.com/ 911681/.   
Field, A. and Y. Tsvetkov. 2019. Entitycentric contextual affective analysis. ACL.   
Fikes, R. E. and N. J. Nilsson. 1971. STRIPS: A new approach to the application of theorem proving to problem solving. Artificial Intelligence, 2:189–208.   
Fillmore, C. J. 1966. A proposal concerning English prepositions. In F. P. Dinneen, ed., 17th annual Round Table, volume 17 of Monograph Series on Language and Linguistics, 19– 34. Georgetown University Press.   
Fillmore, C. J. 1968. The case for case. In E. W. Bach and R. T. Harms, eds, Universals in Linguistic Theory, 1– 88. Holt, Rinehart & Winston.   
Fillmore, C. J. 1985. Frames and the semantics of understanding. Quaderni di Semantica, VI(2):222–254.   
Fillmore, C. J. 2003. Valency and semantic roles: the concept of deep structure case. In V. Agel, L. M. Eichinger, H. W. Eroms, P. Hellwig, H. J. Heringer, and H. Lobin, eds, Dependenz und Valenz: Ein internationales Handbuch der zeitgenossischen Forschung ¨ , chapter 36, 457–475. Walter de Gruyter.   
Fillmore, C. J. 2012. ACL lifetime achievement award: Encounters with language. Computational Linguistics, 38(4):701–718.   
Fillmore, C. J. and C. F. Baker. 2009. A frames approach to semantic analysis. In B. Heine and H. Narrog, eds, The Oxford Handbook of Linguistic Analysis, 313–340. Oxford University Press.   
Fillmore, C. J., C. R. Johnson, and M. R. L. Petruck. 2003. Background to FrameNet. International journal of lexicography, 16(3):235–250.   
Finkelstein, L., E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman, and E. Ruppin. 2002. Placing search in context: The concept revisited. ACM Transactions on Information Systems, 20(1):116—-131.   
Finlayson, M. A. 2016. Inferring Propp’s functions from semantically annotated text. The Journal of American Folklore, 129(511):55–77.   
Firth, J. R. 1935. The technique of semantics. Transactions of the philological society, 34(1):36–73.   
Firth, J. R. 1957. A synopsis of linguistic theory 1930–1955. In Studies in Linguistic Analysis. Philological Society. Reprinted in Palmer, F. (ed.) 1968. Selected Papers of J. R. Firth. Longman, Harlow.   
Flanagan, J. L. 1972. Speech Analysis, Synthesis, and Perception. Springer.   
Flanagan, J. L., K. Ishizaka, and K. L. Shipley. 1975. Synthesis of speech from a dynamic model of the vocal cords and vocal tract. The Bell System Technical Journal, 54(3):485– 506.   
Foland, W. and J. H. Martin. 2016. CU-NLP at SemEval-2016 task 8: AMR parsing using LSTM-based recurrent neural networks. SemEval2016.   
Foland, Jr., W. R. and J. H. Martin. 2015. Dependency-based semantic role labeling using convolutional neural networks. \*SEM 2015.   
Foltz, P. W., W. Kintsch, and T. K. Landauer. 1998. The measurement of textual coherence with latent semantic analysis. Discourse processes, 25(2-3):285–307.   
∀, W. Nekoto, V. Marivate, T. Matsila, T. Fasubaa, T. Kolawole, T. Fagbohungbe, S. O. Akinola, S. H. Muhammad, S. Kabongo, S. Osei, S. Freshia, R. A. Niyongabo, R. M. P. Ogayo, O. Ahia, M. Meressa, M. Adeyemi, M. MokgesiSelinga, L. Okegbemi, L. J. Martinus, K. Tajudeen, K. Degila, K. Ogueji, K. Siminyu, J. Kreutzer, J. Webster, J. T. Ali, J. A. I. Orife, I. Ezeani, I. A. Dangana, H. Kamper, H. Elsahar, G. Duru, G. Kioko, E. Murhabazi, E. van Biljon, D. Whitenack, C. Onyefuluchi, C. Emezue, B. Dossou, B. Sibanda, B. I. Bassey, A. Olabiyi, A. Ramkilowan, A. Oktem, A. Akin- ¨ faderin, and A. Bashir. 2020. Participatory research for low-resourced machine translation: A case study in African languages. Findings of EMNLP.   
Fox, B. A. 1993. Discourse Structure and Anaphora: Written and Conversational English. Cambridge.   
Francis, W. N. and H. Kucera. 1982. ˇ Frequency Analysis of English Usage. Houghton Mifflin, Boston.   
Franz, A. and T. Brants. 2006. All our n-gram are belong to you. https: //research.google/blog/ all-our-n-gram-are-belong-to   
Fraser, N. M. and G. N. Gilbert. 1991. Simulating speech systems. Computer Speech and Language, 5:81– 99.   
Friedman, B. and D. G. Hendry. 2019. Value Sensitive Design: Shaping Technology with Moral Imagination. MIT Press.   
Friedman, B., D. G. Hendry, and A. Borning. 2017. A survey of value sensitive design methods. Foundations and Trends in HumanComputer Interaction, 11(2):63– 125.   
Fry, D. B. 1959. Theoretical aspects of mechanical speech recognition. Journal of the British Institution of Radio Engineers, 19(4):211– 218. Appears together with companion paper (Denes 1959).   
Furnas, G. W., T. K. Landauer, L. M. Gomez, and S. T. Dumais. 1987. The vocabulary problem in humansystem communication. Communications of the ACM, 30(11):964– 971.   
Gabow, H. N., Z. Galil, T. Spencer, and R. E. Tarjan. 1986. Efficient algorithms for finding minimum spanning trees in undirected and directed graphs. Combinatorica, 6(2):109– 122.   
Gaddy, D., M. Stern, and D. Klein. 2018. What’s going on in neural constituency parsers? an analysis. NAACL HLT.   
Gale, W. A. and K. W. Church. 1994. What is wrong with adding one? In N. Oostdijk and P. de Haan, eds, Corpus-Based Research into Language, 189–198. Rodopi.   
Gale, W. A. and K. W. Church. 1991. A program for aligning sentences in bilingual corpora. ACL.   
Gale, W. A. and K. W. Church. 1993. A program for aligning sentences in bilingual corpora. Computational Linguistics, 19:75–102.   
Gale, W. A., K. W. Church, and D. Yarowsky. 1992a. One sense per discourse. HLT.   
Gale, W. A., K. W. Church, and D. Yarowsky. 1992b. Work on statistical methods for word sense disambiguation. AAAI Fall Symposium on Probabilistic Approaches to Natural Language.   
Gao, L., T. Hoppe, A. Thite, S. Biderman, C. Foster, N. Nabeshima, S. Black, J. Phang, S. Presser, L. Golding, H. He, and C. Leahy. 2020. The Pile: An 800GB dataset of diverse text for language modeling. ArXiv preprint.   
Garg, N., L. Schiebinger, D. Jurafsky, and J. Zou. 2018. Word embeddings   
-you/.quantify 100 years of gender and ethnic stereotypes. Proceedings of the National Academy of Sciences, 115(16):E3635–E3644.   
Garside, R. 1987. The CLAWS wordtagging system. In R. Garside, G. Leech, and G. Sampson, eds, The Computational Analysis of English, 30–41. Longman.   
Garside, R., G. Leech, and A. McEnery. 1997. Corpus Annotation. Longman.   
Gebru, T., J. Morgenstern, B. Vecchione, J. W. Vaughan, H. Wallach, H. Daume III, and K. Craw-´ ford. 2020. Datasheets for datasets. ArXiv.   
Gehman, S., S. Gururangan, M. Sap, Y. Choi, and N. A. Smith. 2020. RealToxicityPrompts: Evaluating neural toxic degeneration in language models. Findings of EMNLP.   
Gerber, M. and J. Y. Chai. 2010. Beyond nombank: A study of implicit arguments for nominal predicates. ACL.   
Gers, F. A., J. Schmidhuber, and F. Cummins. 2000. Learning to forget: Continual prediction with lstm. Neural computation, 12(10):2451– 2471.   
Gil, D. 2000. Syntactic categories, cross-linguistic variation and universal grammar. In P. M. Vogel and B. Comrie, eds, Approaches to the Typology of Word Classes, 173–216. Mouton.   
Gildea, D. and D. Jurafsky. 2000. Automatic labeling of semantic roles. ACL.   
Gildea, D. and D. Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.   
Gildea, D. and M. Palmer. 2002. The necessity of syntactic parsing for predicate argument recognition. ACL.   
Giles, C. L., G. M. Kuhn, and R. J. Williams. 1994. Dynamic recurrent neural networks: Theory and applications. IEEE Trans. Neural Netw. Learning Syst., 5(2):153–156.   
Gillick, L. and S. J. Cox. 1989. Some statistical issues in the comparison of speech recognition algorithms. ICASSP.   
Girard, G. 1718. La justesse de la langue franc¸oise: ou les differentes ´ significations des mots qui passent pour synonimes. Laurent d’Houry, Paris.   
Giuliano, V. E. 1965. The interpretation of word associations. Statistical Association Methods For Mechanized Documentation. Symposium Proceedings. Washington, D.C., USA, March 17, 1964. https://nvlpubs.nist. gov/nistpubs/Legacy/MP/ nbsmiscellaneouspub269.pdf.   
Gladkova, A., A. Drozd, and S. Matsuoka. 2016. Analogy-based detection of morphological and semantic relations with word embeddings: what works and what doesn’t. NAACL Student Research Workshop.   
Glaese, A., N. McAleese, M. Trebacz, J. Aslanides, V. Firoiu, T. Ewalds, M. Rauh, L. Weidinger, M. Chadwick, P. Thacker, L. CampbellGillingham, J. Uesato, P.-S. Huang, R. Comanescu, F. Yang, A. See, S. Dathathri, R. Greig, C. Chen, D. Fritz, J. Sanchez Elias, R. Green, S. Mokra, N. Fernando, B. Wu, ´ R. Foley, S. Young, I. Gabriel, W. Isaac, J. Mellor, D. Hassabis, K. Kavukcuoglu, L. A. Hendricks, and G. Irving. 2022. Improving alignment of dialogue agents via targeted human judgements. ArXiv preprint.   
Glenberg, A. M. and D. A. Robertson. 2000. Symbol grounding and meaning: A comparison of highdimensional and embodied theories of meaning. Journal of memory and language, 43(3):379–401.   
Godfrey, J., E. Holliman, and J. McDaniel. 1992. SWITCHBOARD: Telephone speech corpus for research and development. ICASSP.   
Goel, V. and W. Byrne. 2000. Minimum bayes-risk automatic speech recognition. Computer Speech & Language, 14(2):115–135.   
Goffman, E. 1974. Frame analysis: An essay on the organization of experience. Harvard University Press.   
Goldberg, J., M. Ostendorf, and K. Kirchhoff. 2003. The impact of response wording in error correction subdialogs. ISCA Tutorial and Research Workshop on Error Handling in Spoken Dialogue Systems.   
Goldberg, Y. 2017. Neural Network Methods for Natural Language Processing, volume 10 of Synthesis Lectures on Human Language Technologies. Morgan & Claypool.   
Gonen, H. and Y. Goldberg. 2019. Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them. NAACL HLT.   
Good, M. D., J. A. Whiteside, D. R. Wixon, and S. J. Jones. 1984. Building a user-derived interface. CACM, 27(10):1032–1043.   
Goodfellow, I., Y. Bengio, and A. Courville. 2016. Deep Learning. MIT Press.   
Goodman, J. 2006. A bit of progress in language modeling: Extended version. Technical Report MSRTR-2001-72, Machine Learning and Applied Statistics Group, Microsoft Research, Redmond, WA.   
Goodwin, C. 1996. Transparent vision. In E. Ochs, E. A. Schegloff, and S. A. Thompson, eds, Interaction and Grammar, 370–404. Cambridge University Press.   
Gopalakrishnan, K., B. Hedayatnia, Q. Chen, A. Gottardi, S. Kwatra, A. Venkatesh, R. Gabriel, and D. Hakkani-Tur. 2019. ¨ Topicalchat: Towards knowledge-grounded open-domain conversations. INTERSPEECH.   
Gould, J. D., J. Conti, and T. Hovanyecz. 1983. Composing letters with a simulated listening typewriter. CACM, 26(4):295–308.   
Gould, J. D. and C. Lewis. 1985. Designing for usability: Key principles and what designers think. CACM, 28(3):300–311.   
Gould, S. J. 1980. The Panda’s Thumb. Penguin Group.   
Graff, D. 1997. The 1996 Broadcast News speech and language-model corpus. Proceedings DARPA Speech Recognition Workshop.   
Gravano, A., J. Hirschberg, and S. Be ˇ nuˇ s. 2012. ˇ Affirmative cue words in task-oriented dialogue. Computational Linguistics, 38(1):1– 39.   
Graves, A. 2012. Sequence transduction with recurrent neural networks. ICASSP.   
Graves, A. 2013. Generating sequences with recurrent neural networks. ArXiv.   
Graves, A., S. Fernandez, F. Gomez, ´ and J. Schmidhuber. 2006. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. ICML.   
Graves, A., S. Fernandez, M. Li-´ wicki, H. Bunke, and J. Schmidhuber. 2007. Unconstrained on-line handwriting recognition with recurrent neural networks. NeurIPS.   
Graves, A. and N. Jaitly. 2014. Towards end-to-end speech recognition with recurrent neural networks. ICML.   
Graves, A., A.-r. Mohamed, and G. Hinton. 2013. Speech recognition with deep recurrent neural networks. ICASSP.   
Graves, A. and J. Schmidhuber. 2005. Framewise phoneme classification with bidirectional LSTM and other neural network architectures. Neural Networks, 18(5-6):602–610.   
Graves, A., G. Wayne, and I. Danihelka. 2014. Neural Turing machines. ArXiv.   
Green, B. F., A. K. Wolf, C. Chomsky, and K. Laughery. 1961. Baseball: An automatic question answerer. Proceedings of the Western Joint Computer Conference 19.   
Greene, B. B. and G. M. Rubin. 1971. Automatic grammatical tagging of English. Department of Linguistics, Brown University, Providence, Rhode Island.   
Greenwald, A. G., D. E. McGhee, and J. L. K. Schwartz. 1998. Measuring individual differences in implicit cognition: the implicit association test. Journal of personality and social psychology, 74(6):1464–1480.   
Grenager, T. and C. D. Manning. 2006. Unsupervised discovery of a statistical verb lexicon. EMNLP.   
Grice, H. P. 1975. Logic and conversation. In P. Cole and J. L. Morgan, eds, Speech Acts: Syntax and Semantics Volume 3, 41–58. Academic Press.   
Grice, H. P. 1978. Further notes on logic and conversation. In P. Cole, ed., Pragmatics: Syntax and Semantics Volume 9, 113–127. Academic Press.   
Grishman, R. and B. Sundheim. 1995. Design of the MUC-6 evaluation. MUC-6.   
Grosz, B. J. 1977a. The representation and use of focus in a system for understanding dialogs. IJCAI-77. Morgan Kaufmann.   
Grosz, B. J. 1977b. The Representation and Use of Focus in Dialogue Understanding. Ph.D. thesis, University of California, Berkeley.   
Grosz, B. J., A. K. Joshi, and S. Weinstein. 1983. Providing a unified account of definite noun phrases in English. ACL.   
Grosz, B. J., A. K. Joshi, and S. Weinstein. 1995. Centering: A framework for modeling the local coherence of discourse. Computational Linguistics, 21(2):203–225.   
Grosz, B. J. and C. L. Sidner. 1980. Plans for discourse. In P. R. Cohen, J. Morgan, and M. E. Pollack, eds, Intentions in Communication, 417– 444. MIT Press.   
Gruber, J. S. 1965. Studies in Lexical Relations. Ph.D. thesis, MIT.   
Grunewald, S., A. Friedrich, and ¨ J. Kuhn. 2021. Applying Occam’s razor to transformer-based dependency parsing: What works, what doesn’t, and what is really necessary. IWPT.   
Guinaudeau, C. and M. Strube. 2013. Graph-based local coherence modeling. ACL.   
Guindon, R. 1988. A multidisciplinary perspective on dialogue structure in user-advisor dialogues. In R. Guindon, ed., Cognitive Science and Its Applications for Human-Computer Interaction, 163–200. Lawrence Erlbaum.   
Gundel, J. K., N. Hedberg, and R. Zacharski. 1993. Cognitive status and the form of referring expressions in discourse. Language, 69(2):274– 307.   
Gururangan, S., A. Marasovic,´ S. Swayamdipta, K. Lo, I. Beltagy, D. Downey, and N. A. Smith. 2020. Don’t stop pretraining: Adapt language models to domains and tasks. ACL.   
Gusfield, D. 1997. Algorithms on Strings, Trees, and Sequences. Cambridge University Press   
Guyon, I. and A. Elisseeff. 2003. An introduction to variable and feature selection. JMLR, 3:1157–1182.   
Haber, J. and M. Poesio. 2020. Assessing polyseme sense similarity through co-predication acceptability and contextualised embedding distance. \*SEM.   
Habernal, I. and I. Gurevych. 2016. Which argument is more convincing? Analyzing and predicting convincingness of Web arguments using bidirectional LSTM. ACL.   
Habernal, I. and I. Gurevych. 2017. Argumentation mining in usergenerated web discourse. Computational Linguistics, 43(1):125–179.   
Haghighi, A. and D. Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. EMNLP.   
Hajishirzi, H., L. Zilles, D. S. Weld, and L. Zettlemoyer. 2013. Joint coreference resolution and namedentity linking with multi-pass sieves. EMNLP.   
Hajic, J. 1998. ˇ Building a Syntactically Annotated Corpus: The Prague Dependency Treebank, 106– 132. Karolinum.   
Hajic, J. 2000. ˇ Morphological tagging: Data vs. dictionaries. NAACL.   
Hajic, J., M. Ciaramita, R. Johans- ˇ son, D. Kawahara, M. A. Mart´ı, L. Marquez, A. Meyers, J. Nivre,\` S. Pado, J. ´ Stˇ epˇ anek, P. Stran ´ aˇk,´ M. Surdeanu, N. Xue, and Y. Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. CoNLL.   
Hakkani-Tur, D., K. Oflazer, and ¨ G. Tur. 2002. Statistical morpholog- ¨ ical disambiguation for agglutinative languages. Journal of Computers and Humanities, 36(4):381–410.   
Halliday, M. A. K. and R. Hasan. 1976. Cohesion in English. Longman. English Language Series, Title No. 9.   
Hamilton, W. L., K. Clark, J. Leskovec, and D. Jurafsky. 2016a. Inducing domain-specific sentiment lexicons from unlabeled corpora. EMNLP.   
Hamilton, W. L., J. Leskovec, and D. Jurafsky. 2016b. Diachronic word embeddings reveal statistical laws of semantic change. ACL.   
Hannun, A. 2017. Sequence modeling with CTC. Distill, 2(11).   
Hannun, A. Y., A. L. Maas, D. Jurafsky, and A. Y. Ng. 2014. First-pass large vocabulary continuous speech recognition using bi-directional recurrent DNNs. ArXiv preprint arXiv:1408.2873.   
Harris, C. M. 1953. A study of the building blocks in speech. JASA, 25(5):962–969.   
Harris, R. A. 2005. Voice Interaction Design: Crafting the New Conversational Speech Systems. Morgan Kaufmann.   
Harris, Z. S. 1946. From morpheme to utterance. Language, 22(3):161– 183.   
Harris, Z. S. 1954. Distributional structure. Word, 10:146–162.   
Harris, Z. S. 1962. String Analysis of Sentence Structure. Mouton, The Hague.   
Hashimoto, T., M. Srivastava, H. Namkoong, and P. Liang. 2018. Fairness without demographics in repeated loss minimization. ICML.   
Hastie, T., R. J. Tibshirani, and J. H. Friedman. 2001. The Elements of Statistical Learning. Springer.   
Hatzivassiloglou, V. and K. McKeown. 1997. Predicting the semantic orientation of adjectives. ACL.   
Hatzivassiloglou, V. and J. Wiebe. 2000. Effects of adjective orientation and gradability on sentence subjectivity. COLING.   
Haviland, S. E. and H. H. Clark. 1974. What’s new? Acquiring new information as a process in comprehension. Journal of Verbal Learning and Verbal Behaviour, 13:512–521.   
Hawkins, J. A. 1978. Definiteness and indefiniteness: a study in reference and grammaticality prediction. Croom Helm Ltd.   
Hayashi, T., R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan. 2020. ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-tospeech toolkit. ICASSP.   
He, L., K. Lee, M. Lewis, and L. Zettlemoyer. 2017. Deep semantic role labeling: What works and what’s next. ACL.   
He, W., K. Liu, J. Liu, Y. Lyu, S. Zhao, X. Xiao, Y. Liu, Y. Wang, H. Wu, Q. She, X. Liu, T. Wu, and H. Wang. 2018. DuReader: a Chinese machine reading comprehension dataset from real-world applications. Workshop on Machine Reading for Question Answering.   
Heafield, K. 2011. KenLM: Faster and smaller language model queries. Workshop on Statistical Machine Translation.   
Heafield, K., I. Pouzyrevsky, J. H. Clark, and P. Koehn. 2013. Scalable modified Kneser-Ney language model estimation. ACL.   
Heaps, H. S. 1978. Information retrieval. Computational and theoretical aspects. Academic Press.   
Hearst, M. A. 1992a. Automatic acquisition of hyponyms from large text corpora. COLING.   
Hearst, M. A. 1992b. Automatic acquisition of hyponyms from large text corpora. COLING.   
Hearst, M. A. 1997. Texttiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23:33–64.   
Hearst, M. A. 1998. Automatic discov ery of WordNet relations. In C. Fellbaum, ed., WordNet: An Electronic Lexical Database. MIT Press.   
Heckerman, D., E. Horvitz, M. Sahami, and S. T. Dumais. 1998. A bayesian approach to filtering junk e-mail. AAAI-98 Workshop on Learning for Text Categorization.   
Heim, I. 1982. The semantics of definite and indefinite noun phrases. Ph.D. thesis, University of Massachusetts at Amherst.   
Hellrich, J., S. Buechel, and U. Hahn. 2019. Modeling word emotion in historical language: Quantity beats supposed stability in seed word selection. 3rd Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature.   
Hellrich, J. and U. Hahn. 2016. Bad company—Neighborhoods in neural embedding spaces considered harmful. COLING.   
Henderson, J. 1994. Description Based Parsing in a Connectionist Network. Ph.D. thesis, University of Pennsylvania, Philadelphia, PA.   
Henderson, J. 2003. Inducing history representations for broad coverage statistical parsing. HLT-NAACL-03.   
Henderson, J. 2004. Discriminative training of a neural network statistical parser. ACL.   
Henderson, P., J. Hu, J. Romoff, E. Brunskill, D. Jurafsky, and J. Pineau. 2020. Towards the systematic reporting of the energy and carbon footprints of machine learning. Journal of Machine Learning Research, 21(248):1–43.   
Henderson, P., X. Li, D. Jurafsky, T. Hashimoto, M. A. Lemley, and P. Liang. 2023. Foundation models and fair use. JMLR, 24(400):1–79.   
Henderson, P., K. Sinha, N. AngelardGontier, N. R. Ke, G. Fried, R. Lowe, and J. Pineau. 2017. Ethical challenges in data-driven dialogue systems. AAAI/ACM AI Ethics and Society Conference.   
Hendrickx, I., S. N. Kim, Z. Kozareva, P. Nakov, D. O S ´ eaghdha, S. Pad ´ o, ´ M. Pennacchiotti, L. Romano, and S. Szpakowicz. 2009. Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. 5th International Workshop on Semantic Evaluation.   
Hendrix, G. G., C. W. Thompson, and J. Slocum. 1973. Language processing via canonical verbs and semantic models. Proceedings of IJCAI-73.   
Herdan, G. 1960. Type-token mathematics. Mouton.   
Hermann, K. M., T. Kocisky, E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. 2015a. Teaching machines to read and comprehend. NeurIPS.   
Hermann, K. M., T. Kocisk ˇ y,´ E. Grefenstette, L. Espeholt, W. Kay, M. Suleyman, and P. Blunsom. 2015b. Teaching machines to read and comprehend. NeurIPS.   
Hernault, H., H. Prendinger, D. A. duVerle, and M. Ishizuka. 2010. Hilda: A discourse parser using support vector machine classification. Dialogue & Discourse, 1(3).   
Hidey, C., E. Musi, A. Hwang, S. Muresan, and K. McKeown. 2017. Analyzing the semantic types of claims and premises in an online persuasive forum. 4th Workshop on Argument Mining.   
Hill, F., R. Reichart, and A. Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. Computational Linguistics, 41(4):665–695.   
Hinkelman, E. A. and J. Allen. 1989. Two constraints on speech act ambiguity. ACL.   
Hinton, G. E. 1986. Learning distributed representations of concepts. COGSCI.   
Hinton, G. E., S. Osindero, and Y.-W. Teh. 2006. A fast learning algorithm for deep belief nets. Neural computation, 18(7):1527–1554.   
Hinton, G. E., N. Srivastava, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. 2012. Improving neural networks by preventing co-adaptation of feature detectors. ArXiv preprint arXiv:1207.0580.   
Hirschberg, J., D. J. Litman, and M. Swerts. 2001. Identifying user corrections automatically in spoken dialogue systems. NAACL.   
Hirschman, L., M. Light, E. Breck, and J. D. Burger. 1999. Deep Read: A reading comprehension system. ACL.   
Hirst, G. 1981. Anaphora in Natural Language Understanding: A survey. Number 119 in Lecture notes in computer science. Springer-Verlag.   
Hirst, G. 1987. Semantic Interpretation and the Resolution of Ambiguity. Cambridge University Press.   
Hjelmslev, L. 1969. Prologomena to a Theory of Language. University of Wisconsin Press. Translated by Francis J. Whitfield; original Danish edition 1943.   
Hobbs, J. R. 1978. Resolving pronoun references. Lingua, 44:311–338.   
Hobbs, J. R. 1979. Coherence and coreference. Cognitive Science, 3:67–90.   
Hobbs, J. R., D. E. Appelt, J. Bear, D. Israel, M. Kameyama, M. E. Stickel, and M. Tyson. 1997. FASTUS: A cascaded finite-state transducer for extracting information from natural-language text. In E. Roche and Y. Schabes, eds, Finite-State Language Processing, 383–406. MIT Press.   
Hochreiter, S. and J. Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735– 1780.   
Hofmann, T. 1999. Probabilistic latent semantic indexing. SIGIR-99.   
Holtzman, A., J. Buys, L. Du, M. Forbes, and Y. Choi. 2020. The curious case of neural text degeneration. ICLR.   
Honovich, O., U. Shaham, S. R. Bowman, and O. Levy. 2023. Instruction induction: From few examples to natural language task descriptions. ACL.   
Hopcroft, J. E. and J. D. Ullman. 1979. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley.   
Hou, Y., K. Markert, and M. Strube. 2018. Unrestricted bridging resolution. Computational Linguistics, 44(2):237–284.   
Householder, F. W. 1995. Dionysius Thrax, the technai, and Sextus Empiricus. In E. F. K. Koerner and R. E. Asher, eds, Concise History of the Language Sciences, 99–103. Elsevier Science.   
Hovy, E. H. 1990. Parsimonious and profligate approaches to the question of discourse structure relations. Proceedings of the 5th International Workshop on Natural Language Generation.   
Hovy, E. H., M. P. Marcus, M. Palmer, L. A. Ramshaw, and R. Weischedel. 2006. OntoNotes: The $90 \%$ solution. HLT-NAACL.   
Hu, M. and B. Liu. $2 0 0 4 \mathrm { a }$ . Mining and summarizing customer reviews. KDD.   
Hu, M. and B. Liu. 2004b. Mining and summarizing customer reviews. SIGKDD-04.   
Huang, E. H., R. Socher, C. D. Manning, and A. Y. Ng. 2012. Improving word representations via global context and multiple word prototypes. ACL.   
Huang, Z., W. Xu, and K. Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging. arXiv preprint arXiv:1508.01991.   
Huffman, S. 1996. Learning information extraction patterns from examples. In S. Wertmer, E. Riloff, and G. Scheller, eds, Connectionist, Statistical, and Symbolic Approaches to Learning Natural Language Processing, 246–260. Springer.   
Hunt, A. J. and A. W. Black. 1996. Unit selection in a concatenative speech synthesis system using a large speech database. ICASSP.   
Hutchins, W. J. 1986. Machine Translation: Past, Present, Future. Ellis Horwood, Chichester, England.   
Hutchins, W. J. 1997. From first conception to first demonstration: The nascent years of machine translation, 1947–1954. A chronology. Machine Translation, 12:192–252.   
Hutchins, W. J. and H. L. Somers. 1992. An Introduction to Machine Translation. Academic Press.   
Hutchinson, B., V. Prabhakaran, E. Denton, K. Webster, Y. Zhong, and S. Denuyl. 2020. Social biases in NLP models as barriers for persons with disabilities. ACL.   
Hymes, D. 1974. Ways of speaking. In R. Bauman and J. Sherzer, eds, Explorations in the ethnography of speaking, 433–451. Cambridge University Press.   
Iida, R., K. Inui, H. Takamura, and Y. Matsumoto. 2003. Incorporating contextual cues in trainable models for coreference resolution. EACL Workshop on The Computational Treatment of Anaphora.   
Irsoy, O. and C. Cardie. 2014. Opinion mining with deep recurrent neural networks. EMNLP.   
Ischen, C., T. Araujo, H. Voorveld, G. van Noort, and E. Smit. 2019. Privacy concerns in chatbot interactions. International Workshop on Chatbot Research and Design.   
ISO8601. 2004. Data elements and interchange formats—information interchange—representation o f dates and times. Technical report, International Organization for Standards (ISO).   
Itakura, F. 1975. Minimum prediction residual principle applied to speech recognition. IEEE Transactions on ASSP, ASSP-32:67–72.   
Iter, D., K. Guu, L. Lansing, and D. Jurafsky. 2020. Pretraining with contrastive sentence objectives improves discourse performance of language models. ACL.   
Iter, D., J. Yoon, and D. Jurafsky. 2018. Automatic detection of incoherent speech for diagnosing schizophrenia. Fifth Workshop on Computational Linguistics and Clinical Psychology.   
Ito, K. and L. Johnson. 2017. The LJ speech dataset. https://keithito.com/ LJ-Speech-Dataset/.   
Iyer, S., I. Konstas, A. Cheung, J. Krishnamurthy, and L. Zettlemoyer. 2017. Learning a neural semantic parser from user feedback. ACL.   
Iyer, S., X. V. Lin, R. Pasunuru, T. Mihaylov, D. Simig, P. Yu, K. Shuster, T. Wang, Q. Liu, P. S. Koura, X. Li, B. O’Horo, G. Pereyra, J. Wang, C. Dewan, A. Celikyilmaz, L. Zettlemoyer, and V. Stoyanov. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. ArXiv preprint.   
Izacard, G., P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. Dwivedi-Yu, A. Joulin, S. Riedel, and E. Grave. 2022. Few-shot learning with retrieval augmented language models. ArXiv preprint.   
Jackendoff, R. 1983. Semantics and Cognition. MIT Press.   
Jacobs, P. S. and L. F. Rau. 1990. SCISOR: A system for extracting information from on-line news. CACM, 33(11):88–97.   
Jaech, A., G. Mulcaire, S. Hathi, M. Ostendorf, and N. A. Smith. 2016. Hierarchical character-word models for language identification. ACL Workshop on NLP for Social Media.   
Jaitly, N., P. Nguyen, A. Senior, and V. Vanhoucke. 2012. Application of pretrained deep neural networks to large vocabulary speech recognition. INTERSPEECH.   
Jauhiainen, T., M. Lui, M. Zampieri, T. Baldwin, and K. Linden. 2019. ´ Automatic language identification in texts: A survey. JAIR, 65(1):675– 682.   
Jefferson, G. 1972. Side sequences. In D. Sudnow, ed., Studies in social interaction, 294–333. Free Press, New York.   
Jeffreys, H. 1948. Theory of Probability, 2nd edition. Clarendon Press. Section 3.23.   
Jelinek, F. 1969. A fast sequential decoding algorithm using a stack. IBM Journal of Research and Development, 13:675–685.   
Jelinek, F. 1990. Self-organized language modeling for speech recognition. In A. Waibel and K.-F. Lee, eds, Readings in Speech Recognition, 450–506. Morgan Kaufmann. Originally distributed as IBM technical report in 1985.   
Jelinek, F. and R. L. Mercer. 1980. Interpolated estimation of Markov source parameters from sparse data. In E. S. Gelsema and L. N. Kanal, eds, Proceedings, Workshop on Pattern Recognition in Practice, 381– 397. North Holland.   
Jelinek, F., R. L. Mercer, and L. R. Bahl. 1975. Design of a linguistic statistical decoder for the recognition of continuous speech. IEEE Transactions on Information Theory, IT-21(3):250–256.   
Ji, H. and R. Grishman. 2011. Knowledge base population: Successful approaches and challenges. ACL.   
Ji, H., R. Grishman, and H. T. Dang. 2010. Overview of the tac 2011 knowledge base population track. TAC-11.   
Ji, Y. and J. Eisenstein. 2014. Representation learning for text-level discourse parsing. ACL.   
Ji, Y. and J. Eisenstein. 2015. One vector is not enough: Entity-augmented distributed semantics for discourse relations. TACL, 3:329–344.   
Jia, R. and P. Liang. 2016. Data recombination for neural semantic parsing. ACL.   
Jia, S., T. Meng, J. Zhao, and K.-W. Chang. 2020. Mitigating gender bias amplification in distribution by posterior regularization. ACL.   
Johnson, J., M. Douze, and H. Jegou. ´ 2017. Billion-scale similarity search with GPUs. ArXiv preprint arXiv:1702.08734.   
Johnson, W. E. 1932. Probability: deductive and inductive problems (appendix to). Mind, 41(164):421–423.   
Johnson-Laird, P. N. 1983. Mental Models. Harvard University Press, Cambridge, MA.   
Jones, M. P. and J. H. Martin. 1997. Contextual spelling correction using latent semantic analysis. ANLP.   
Jones, R., A. McCallum, K. Nigam, and E. Riloff. 1999. Bootstrapping for text learning tasks. IJCAI-99 Workshop on Text Mining: Foundations, Techniques and Applications.   
Jones, T. 2015. Toward a description of African American Vernacular English dialect regions using “Black Twitter”. American Speech, 90(4):403–440.   
Joos, M. 1950. Description of language design. JASA, 22:701–708.   
Jordan, M. 1986. Serial order: A parallel distributed processing approach. Technical Report ICS Report 8604, parser from antiquity. In A. Kornai, ed., Extended Finite State Models of Language, 6–15. Cambridge University Press.   
Joshi, A. K. and S. Kuhn. 1979. Centered logic: The role of entity centered sentence representation in natural language inferencing. IJCAI-79.   
Joshi, A. K. and S. Weinstein. 1981. Control of inference: Role of some aspects of discourse structure – centering. IJCAI-81.   
Joshi, M., D. Chen, Y. Liu, D. S. Weld, L. Zettlemoyer, and O. Levy. 2020. SpanBERT: Improving pretraining by representing and predicting spans. TACL, 8:64–77.   
Joshi, M., O. Levy, D. S. Weld, and L. Zettlemoyer. 2019. BERT for coreference resolution: Baselines and analysis. EMNLP.   
Joty, S., G. Carenini, and R. T. Ng. 2015. CODRA: A novel discrimi native framework for rhetorical analysis. Computational Linguistics, 41(3):385–435.   
Jurafsky, D. 2014. The Language of Food. W. W. Norton, New York.   
Jurafsky, D., V. Chahuneau, B. R. Routledge, and N. A. Smith. 2014. Narrative framing of consumer sentiment in online restaurant reviews. First Monday, 19(4).   
Jurafsky, D., C. Wooters, G. Tajchman, J. Segal, A. Stolcke, E. Fosler, and N. Morgan. 1994. The Berkeley restaurant project. ICSLP.   
Jurgens, D., S. M. Mohammad, P. Turney, and K. Holyoak. 2012. SemEval-2012 task 2: Measuring degrees of relational similarity. \*SEM 2012.   
Jurgens, D., Y. Tsvetkov, and D. Jurafsky. 2017. Incorporating dialectal variability for socially equitable language identification. ACL.   
Justeson, J. S. and S. M. Katz. 1991. Co-occurrences of antonymous adjectives and their contexts. Computational linguistics, 17(1):1–19.   
Kalchbrenner, N. and P. Blunsom. 2013. Recurrent continuous translation models. EMNLP.   
Kameyama, M. 1986. A propertysharing constraint in centering. ACL.   
Kamp, H. 1981. A theory of truth and semantic representation. In J. Groenendijk, T. Janssen, and M. Stokhof, eds, Formal Methods in the Study of Language, 189–222. Mathematical Centre, Amsterdam.   
Kamphuis, C., A. P. de Vries, L. Boytsov, and J. Lin. 2020. Which BM25 do you mean? a large-scale ducibility study of Information Retrieval.   
Kane, S. K., M. R. Morris, A. Paradiso, and J. Campbell. 2017. “at times avuncular and cantankerous, with the reflexes of a mongoose”: Understanding self-expression through augmentative and alternative communication devices. CSCW.   
Kaplan, J., S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford, J. Wu, and D. Amodei. 2020. Scaling laws for neural language models. ArXiv preprint.   
Kaplan, R. M. 1973. A general syntactic processor. In R. Rustin, ed., Natural Language Processing, 193–241. Algorithmics Press.   
Karamanis, N., M. Poesio, C. Mellish, and J. Oberlander. 2004. Evaluating centering-based metrics of coherence for text structuring using a reliably annotated corpus. ACL.   
Karita, S., N. Chen, T. Hayashi, T. Hori, H. Inaguma, Z. Jiang, M. Someki, N. E. Y. Soplin, R. Yamamoto, X. Wang, S. Watanabe, T. Yoshimura, and W. Zhang. 2019. A comparative study on transformer vs RNN in speech applications. IEEE ASRU-19.   
Karlsson, F., A. Voutilainen, J. Heikkila, and A. Anttila, eds. ¨ 1995. Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text. Mouton de Gruyter.   
Karpukhin, V., B. Oguz, S. Min, ˘ P. Lewis, L. Wu, S. Edunov, D. Chen, and W.-t. Yih. 2020. Dense passage retrieval for open-domain question answering. EMNLP.   
Karttunen, L. 1969. Discourse referents. COLING. Preprint No. 70.   
Karttunen, L. 1999. Comments on Joshi. In A. Kornai, ed., Extended Finite State Models of Language, 16–18. Cambridge University Press.   
Kasami, T. 1965. An efficient recognition and syntax analysis algorithm for context-free languages. Technical Report AFCRL-65-758, Air Force Cambridge Research Laboratory, Bedford, MA.   
Katz, J. J. and J. A. Fodor. 1963. The structure of a semantic theory. Language, 39:170–210.   
Kay, M. 1967. Experiments with a powerful parser. COLING.   
Kay, M. 1973. The MIND system. In R. Rustin, ed., Natural Language Processing, 155–188. Algorithmics Press.   
Kay, M. 1982. Algorithm schemata and data structures in syntactic processing. In S. Allen, ed., ´ Text Processing: Text Analysis and Generation, 358. Almqvist and Wiksell, Stockholm.   
Kay, M. and M. Roscheisen. 1988. ¨ Text-translation alignment. Technical Report P90-00143, Xerox Palo Alto Research Center, Palo Alto, CA.   
Kay, M. and M. Roscheisen. 1993. ¨ Text-translation alignment. Computational Linguistics, 19:121–142.   
Kehler, A. 1993. The effect of establishing coherence in ellipsis and anaphora resolution. ACL.   
Kehler, A. 1994. Temporal relations: Reference or discourse coherence? ACL.   
Kehler, A. 1997a. Current theories of centering for pronoun interpretation: A critical evaluation. Computational Linguistics, 23(3):467–475.   
Kehler, A. 1997b. Probabilistic coreference in information extraction. EMNLP.   
Kehler, A. 2000. Coherence, Reference, and the Theory of Grammar. CSLI Publications.   
Kehler, A., D. E. Appelt, L. Taylor, and A. Simma. 2004. The (non)utility of predicate-argument frequencies for pronoun interpretation. HLTNAACL.   
Kehler, A. and H. Rohde. 2013. A probabilistic reconciliation of coherencedriven and centering-driven theories of pronoun interpretation. Theoretical Linguistics, 39(1-2):1–37.   
Keller, F. and M. Lapata. 2003. Using the web to obtain frequencies for unseen bigrams. Computational Linguistics, 29:459–484.   
Kendall, T. and C. Farrington. 2020. The Corpus of Regional African American Language. Version 2020.05. Eugene, OR: The Online Resources for African American Language Project. http: //oraal.uoregon.edu/coraal.   
Kennedy, C. and B. K. Boguraev. 1996. Anaphora for everyone: Pronominal anaphora resolution without a parser. COLING.   
Khandelwal, U., O. Levy, D. Jurafsky, L. Zettlemoyer, and M. Lewis. 2019. Generalization through memorization: Nearest neighbor language models. ICLR.   
Khattab, O., C. Potts, and M. Zaharia. 2021. Relevance-guided supervision for OpenQA with ColBERT. TACL, 9:929–944.   
Khattab, O., A. Singhvi, P. Maheshwari, Z. Zhang, K. Santhanam, S. Haq, A. Sharma, T. T. Joshi, H. Moazam, H. Miller, M. Zaharia, and C. Potts. 2024. DSPy: Compiling declarative language model calls into self-im ing pipelines. ICLR.   
Khattab, O. and M. Zaharia. 2020. ColBERT: Efficient and effective passage search via contextualized late interaction over BERT. SIGIR.   
Kiela, D., M. Bartolo, Y. Nie, D. Kaushik, A. Geiger, Z. Wu, B. Vidgen, G. Prasad, A. Singh, P. Ringshia, et al. 2021. Dynabench: Rethinking benchmarking in nlp. NAACL HLT.   
Kiela, D. and S. Clark. 2014. A systematic study of semantic vector space model parameters. EACL 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC).   
Kim, E. 2019. Optimize computational efficiency of skipgram with negative sampling. https://aegis4048.github. io/optimize_computational efficiency_of_skip-gram_ with_negative_sampling.   
Kim, S. M. and E. H. Hovy. 2004. Determining the sentiment of opinions. COLING.   
King, S. 2020. From African American Vernacular English to African American Language: Rethinking the study of race and language in African Americans’ speech. Annual Review of Linguistics, 6:285–300.   
Kingma, D. and J. Ba. 2015. Adam: A method for stochastic optimization. ICLR 2015.   
Kintsch, W. and T. A. Van Dijk. 1978. Toward a model of text comprehension and production. Psychological review, 85(5):363–394.   
Kiperwasser, E. and Y. Goldberg. 2016. Simple and accurate dependency parsing using bidirectional LSTM feature representations. TACL, 4:313–327.   
Kipper, K., H. T. Dang, and M. Palmer. 2000. Class-based construction of a verb lexicon. AAAI.   
Kiritchenko, S. and S. M. Mohammad. 2017. Best-worst scaling more reliable than rating scales: A case study on sentiment intensity annotation. ACL.   
Kiritchenko, S. and S. M. Mohammad. 2018. Examining gender and race bias in two hundred sentiment analysis systems. $\ast s E M$ .   
Kiss, T. and J. Strunk. 2006. Unsupervised multilingual sentence boundary detection. Computational Linguistics, 32(4):485–525.   
Kitaev, N., S. Cao, and D. Klein. 2019. Multilingual constituency parsing with self-attention and pretraining. ACL.   
Kitaev, N. and D. Klein. 2018. Constituency parsing with a selfattentive encoder. ACL.   
Klatt, D. H. 1975. Voice onset time, friction, and aspiration in wordinitial consonant clusters. Journal of Speech and Hearing Research, 18:686–706.   
Klatt, D. H. 1977. Review of the ARPA speech understanding project. JASA, 62(6):1345–1366.   
Klatt, D. H. 1982. The Klattalk text-tospeech conversion system. ICASSP.   
Kleene, S. C. 1951. Representation of events in nerve nets and finite automata. Technical Report RM-704, RAND Corporation. RAND Research Memorandum.   
Kleene, S. C. 1956. Representation of events in nerve nets and finite automata. In C. Shannon and J. McCarthy, eds, Automata Studies, 3–41. Princeton University Press.   
Klein, S. and R. F. Simmons. 1963. A computational approach to grammatical coding of English words. Journal of the ACM, 10(3):334–347.   
Knott, A. and R. Dale. 1994. Using linguistic phenomena to motivate a set of coherence relations. Discourse Processes, 18(1):35–62.   
Kocijan, V., A.-M. Cretu, O.-M. Camburu, Y. Yordanov, and T. Lukasiewicz. 2019. A surprisingly robust trick for the Winograd Schema Challenge. ACL.   
Kocmi, T., C. Federmann, R. Grundkiewicz, M. Junczys-Dowmunt, H. Matsushita, and A. Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. ArXiv.   
Koehn, P. 2005. Europarl: A parallel corpus for statistical machine translation. MT summit, vol. 5.   
Koehn, P., H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin, and E. Herbst. 2006. Moses: Open source toolkit for statistical machine translation. ACL.   
Koehn, P., F. J. Och, and D. Marcu. 2003. Statistical phrase-based translation. HLT-NAACL.   
Kolhatkar, V., A. Roussel, S. Dipper, and H. Zinsmeister. 2018. Anaphora with non-nominal antecedents in computational linguistics: A survey. Computational Linguistics, 44(3):547–612.   
Kreutzer, J., I. Caswell, L. Wang, A. Wahab, D. van Esch, N. UlziiOrshikh, A. Tapo, N. Subramani, A. Sokolov, C. Sikasote, M. Setyawan, S. Sarin, S. Samb, B. Sagot, C. Rivera, A. Rios, I. Paadimitriou, S. Osei, P. O. Sua I. Orife, K. Ogueji, A. N. Rubungo, T. Q. Nguyen, M. Muller, A. M ¨ uller, ¨ S. H. Muhammad, N. Muhammad, A. Mnyakeni, J. Mirzakhalov, T. Matangira, C. Leong, N. Lawson, S. Kudugunta, Y. Jernite, M. Jenny, O. Firat, B. F. P. Dossou, S. Dlamini, N. de Silva, S. C¸ abuk Ballı, S. Biderman, A. Battisti, A. Baruwa, A. Bapna, P. Baljekar, I. A. Azime, A. Awokoya, D. Ataman, O. Ahia, O. Ahia, S. Agrawal, and M. Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. TACL, 10:50– 72.   
Krovetz, R. 1993. Viewing morphology as an inference process. SIGIR-93.   
Kruskal, J. B. 1983. An overview of sequence comparison. In D. Sankoff and J. B. Kruskal, eds, Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison, 1–44. Addison-Wesley.   
Kudo, T. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. ACL.   
Kudo, T. and Y. Matsumoto. 2002. Japanese dependency analysis using cascaded chunking. CoNLL.   
Kudo, T. and J. Richardson. 2018a. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. EMNLP.   
Kudo, T. and J. Richardson. 2018b. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. EMNLP.   
Kullback, S. and R. A. Leibler. 1951. On information and sufficiency. Annals of Mathematical Statistics, 22:79–86.   
Kulmizev, A., M. de Lhoneux, J. Gontrum, E. Fano, and J. Nivre. 2019. Deep contextualized word embeddings in transition-based and graph-based dependency parsing - a tale of two parsers revisited. EMNLP.   
Kumar, S. and W. Byrne. 2004. Minimum Bayes-risk decoding for statistical machine translation. HLTNAACL.   
Kummerfeld, J. K. and D. Klein. 2013. Error-driven analysis of challenges in coreference resolution. EMNLP.   
Kuno, S. 1965. The predictive analyzer and a path elimination technique. CACM, 8(7):453–462.   
Kupiec, J. 1992. Robust part-of-speech tagging using a hidden Markov model. Computer Speech and Language, 6:225–242.   
Kurita, K., N. Vyas, A. Pareek, A. W. Black, and Y. Tsvetkov. 2019. Quantifying social biases in contextual word representations. 1st ACL Workshop on Gender Bias for Natural Language Processing.   
Kucera, H. and W. N. Francis. 1967. ˇ Computational Analysis of PresentDay American English. Brown Univ. Press.   
Kwiatkowski, T., J. Palomaki, O. Redfield, M. Collins, A. Parikh, C. Alberti, D. Epstein, I. Polosukhin, J. Devlin, K. Lee, K. Toutanova, L. Jones, M. Kelcey, M.-W. Chang, A. M. Dai, J. Uszkoreit, Q. Le, and S. Petrov. 2019. Natural questions: A benchmark for question answering research. TACL, 7:452–466.   
Lafferty, J. D., A. McCallum, and F. C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. ICML.   
Lai, A. and J. Tetreault. 2018. Discourse coherence in the wild: A dataset, evaluation and methods. SIGDIAL.   
Lake, B. M. and G. L. Murphy. 2021. Word meaning in minds and machines. Psychological Review. In press.   
Lakoff, G. 1965. On the Nature of Syntactic Irregularity. Ph.D. thesis, Indiana University. Published as Irregularity in Syntax. Holt, Rinehart, and Winston, New York, 1970.   
Lakoff, G. 1972. Structural complexity in fairy tales. In The Study of Man, 128–50. School of Social Sciences, University of California, Irvine, CA.   
Lakoff, G. and M. Johnson. 1980. Metaphors We Live By. University of Chicago Press, Chicago, IL.   
Lample, G., M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer. 2016. Neural architectures for named entity recognition. NAACL HLT.   
Lample, G. and A. Conneau. 2019. Cross-lingual language model pretraining. NeurIPS, volume 32.   
Lan, Z., M. Chen, S. Goodman, K. Gimpel, P. Sharma, and R. Soricut. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. ICLR.   
Landauer, T. K., ed. 1995. The Trouble with Computers: Usefulness, Usability, and Productivity. MIT Press.   
Landauer, T. K. and S. T. Dumais. 1997. A solution to Plato’s problem: The Latent Semantic Analysis theory of acquisition, induction, and representation of knowledge. Psychological Review, 104:211–240.   
Landauer, T. K., D. Laham, B. Rehder, and M. E. Schreiner. 1997. How well can passage meaning be derived without using word order? A comparison of Latent Semantic Analysis and humans. COGSCI.   
Lang, J. and M. Lapata. 2014. Similarity-driven semantic role induction via graph partitioning. Computational Linguistics, 40(3):633– 669.   
Lang, K. J., A. H. Waibel, and G. E. Hinton. 1990. A time-delay neural network architecture for isolated word recognition. Neural networks, 3(1):23–43.   
Lapata, M. 2003. Probabilistic text structuring: Experiments with sentence ordering. ACL.   
Lapesa, G. and S. Evert. 2014. A large scale evaluation of distributional semantic models: Parameters, interactions and model selection. TACL, 2:531–545.   
Lappin, S. and H. Leass. 1994. An algorithm for pronominal anaphora resolution. Computational Linguistics, 20(4):535–561.   
Larsson, S. and D. R. Traum. 2000. Information state and dialogue management in the trindi dialogue move engine toolkit. Natural Language Engineering, 6(323-340):97–114.   
Lascarides, A. and N. Asher. 1993. Temporal interpretation, discourse relations, and common sense entailment. Linguistics and Philosophy, 16(5):437–493.   
Lawrence, W. 1953. The synthesis of speech from signals which have a low information rate. In W. Jackson, ed., Communication Theory, 460– 469. Butterworth.   
LDC. 1998. LDC Catalog: Hub4 project. University of Pennsylvania. www.ldc.upenn.edu/ Catalog/LDC98S71.html.   
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541–551.   
Lee, D. D. and H. S. Seung. 1999. Learning the parts of objects by nonnegative matrix factorization. Nature, 401(6755):788–791.   
Lee, H., A. Chang, Y. Peirsman, N. Chambers, M. Surdeanu, and D. Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics, 39(4):885–916.   
Lee, H., Y. Peirsman, A. Chang, N. Chambers, M. Surdeanu, and D. Jurafsky. 2011. Stanford’s multipass sieve coreference resolution system at the CoNLL-2011 shared task. CoNLL.   
Lee, H., M. Surdeanu, and D. Jurafsky. 2017a. A scaffolding approach to coreference resolution integrating statistical and rule-based models. Natural Language Engineering, 23(5):733–762.   
Lee, K., M.-W. Chang, and K. Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. ACL.   
Lee, K., L. He, M. Lewis, and L. Zettlemoyer. 2017b. End-to-end neural coreference resolution. EMNLP.   
Lee, K., L. He, and L. Zettlemoyer. 2018. Higher-order coreference resolution with coarse-to-fine inference. NAACL HLT.   
Lehnert, W. G., C. Cardie, D. Fisher, E. Riloff, and R. Williams. 1991. Description of the CIRCUS system as used for MUC-3. MUC-3.   
Lemon, O., K. Georgila, J. Henderson, and M. Stuttle. 2006. An ISU dialogue system exhibiting reinforcement learning of dialogue policies: Generic slot-filling in the TALK incar system. EACL.   
Levenshtein, V. I. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Cybernetics and Control Theory, 10(8):707–710. Original in Doklady Akademii Nauk SSSR 163(4): 845–848 (1965).   
Levesque, H. 2011. The Winograd Schema Challenge. Logical Formalizations of Commonsense Reasoning — Papers from the AAAI 2011 Spring Symposium (SS-11-06).   
Levesque, H., E. Davis, and L. Morgenstern. 2012. The Winograd Schema Challenge. KR-12.   
Levesque, H. J., P. R. Cohen, and J. H. T. Nunes. 1990. On acting together. AAAI. Morgan Kaufmann.   
Levin, B. 1977. Mapping sentences to case frames. Technical Report 167, MIT AI Laboratory. AI Working Paper 143.   
Levin, B. 1993. English Verb Classes and Alternations: A Preliminary Investigation. University of Chicago Press.   
Levin, B. and M. Rappaport Hovav. 2005. Argument Realization. Cambridge University Press.   
Levin, E., R. Pieraccini, and W. Eckert. 2000. A stochastic model of humanmachine interaction for learning dialog strategies. IEEE Transactions on Speech and Audio Processing, 8:11– 23.   
Levinson, S. C. 1983. Conversational Analysis, chapter 6. Cambridge University Press.   
Levow, G.-A. 1998. Characterizing and recognizing spoken corrections in human-computer dialogue. COLING/ACL.   
Levy, O. and Y. Goldberg. 2014a. Dependency-based word embeddings. ACL.   
Levy, O. and Y. Goldberg. 2014b. Linguistic regularities in sparse and explicit word representations. CoNLL.   
Levy, O. and Y. Goldberg. 2014c. Neural word embedding as implicit matrix factorization. NeurIPS.   
Levy, O., Y. Goldberg, and I. Dagan. 2015. Improving distributional similarity with lessons learned from word embeddings. TACL, 3:211– 225.   
Li, B. Z., S. Min, S. Iyer, Y. Mehdad, and W.-t. Yih. 2020. Efficient onepass end-to-end entity linking for questions. EMNLP.   
Li, J., X. Chen, E. H. Hovy, and D. Jurafsky. 2015. Visualizing and understanding neural models in NLP. NAACL HLT.   
Li, J. and D. Jurafsky. 2017. Neu ral net models of open-domain discourse coherence. EMNLP.   
Li, J., R. Li, and E. H. Hovy. 2014. Recursive deep models for discourse parsing. EMNLP.   
Li, J., W. Monroe, A. Ritter, D. Jurafsky, M. Galley, and J. Gao. 2016a. Deep reinforcement learning for dialogue generation. EMNLP.   
Li, M., J. Weston, and S. Roller. 2019a. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. NeurIPS19 Workshop on Conversational AI.   
Li, Q., T. Li, and B. Chang. 2016b. Discourse parsing with attentionbased hierarchical neural networks. EMNLP.   
Li, X., Y. Meng, X. Sun, Q. Han, A. Yuan, and J. Li. 2019b. Is word segmentation necessary for deep learning of Chinese representations? ACL.   
Liang, P., R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan, B. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Re, D. Acosta- ´ Navas, D. A. Hudson, E. Zelikman, E. Durmus, F. Ladhak, F. Rong, H. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuksekgonul, M. Suzgun, N. Kim, N. Guha, N. Chatterji, O. Khattab, P. Henderson, Q. Huang, R. Chi, S. M. Xie, S. Santurkar, S. Ganguli, T. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li, Y. Mai, Y. Zhang, and Y. Koreeda. 2023. Holistic evaluation of language models. Transactions on Machine Learning Research.   
Lin, C.-Y. 2004. ROUGE: A package for automatic evaluation of summaries. ACL 2004 Workshop on Text Summarization Branches Out.   
Lin, D. 2003. Dependency-based evaluation of minipar. Workshop on the Evaluation of Parsing Systems.   
Lin, Y., J.-B. Michel, E. Aiden Lieberman, J. Orwant, W. Brockman, and S. Petrov. 2012a. Syntactic annotations for the Google books NGram corpus. ACL.   
Lin, Y., J.-B. Michel, E. Lieberman Aiden, J. Orwant, W. Brockman, and S. Petrov. 2012b. Syntactic annotations for the Google Books NGram corpus. ACL.   
Lin, Z., A. Madotto, J. Shin, P. Xu, and P. Fung. 2019. MoEL: Mixture of empathetic listeners. EMNLP.   
Lin, Z., M.-Y. Kan, and H. T. Ng. 2009. Recognizing implicit discourse relations in the Penn Discourse Treebank. EMNLP.   
Lin, Z., H. T. Ng, and M.-Y. Kan. 2011. Automatically evaluating text coherence using discourse relations. ACL.   
Lin, Z., H. T. Ng, and M.-Y. Kan. 2014. A pdtb-styled end-to-end discourse parser. Natural Language Engineering, 20(2):151–184.   
Ling, W., C. Dyer, A. W. Black, I. Trancoso, R. Fermandez, S. Amir, L. Marujo, and T. Lu´ıs. 2015. Finding function in form: Compositional character models for open vocabulary word representation. EMNLP.   
Linzen, T. 2016. Issues in evaluating semantic spaces using word analogies. 1st Workshop on Evaluating VectorSpace Representations for NLP.   
Lison, P. and J. Tiedemann. 2016. Opensubtitles2016: Extracting large parallel corpora from movie and tv subtitles. LREC.   
Litman, D. J. 1985. Plan Recognition and Discourse Analysis: An Integrated Approach for Understanding Dialogues. Ph.D. thesis, University of Rochester, Rochester, NY.   
Litman, D. J. and J. Allen. 1987. A plan recognition model for subdialogues in conversation. Cognitive Science, 11:163–200.   
Litman, D. J., M. A. Walker, and M. Kearns. 1999. Automatic detection of poor speech recognition at the dialogue level. ACL.   
Liu, B. and L. Zhang. 2012. A survey of opinion mining and sentiment analysis. In C. C. Aggarwal and C. Zhai, eds, Mining text data, 415– 464. Springer.   
Liu, H., J. Dacon, W. Fan, H. Liu, Z. Liu, and J. Tang. 2020. Does gender matter? Towards fairness in dialogue systems. COLING.   
Liu, J., S. Min, L. Zettlemoyer, Y. Choi, and H. Hajishirzi. 2024. Infini-gram: Scaling unbounded n-gram language models to a trillion tokens. ArXiv preprint.   
Liu, Y., C. Sun, L. Lin, and X. Wang. 2016. Learning natural language inference using bidirectional LSTM model and inner-attention. ArXiv.   
Liu, Y., P. Fung, Y. Yang, C. Cieri, S. Huang, and D. Graff. 2006. HKUST/MTS: A very large scale Mandarin telephone speech corpus. International Conference on Chinese Spoken Language Processing.   
Liu, Y., M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. 2019. RoBERTa: A robustly optimized BERT pretraining approach. ArXiv preprint arXiv:1907.11692.   
Llama Team. 2024. The llama 3 herd of models.   
Lochbaum, K. E., B. J. Grosz, and C. L. Sidner. 2000. Discourse structure and intention recognition. In R. Dale, H. Moisl, and H. L. Somers, eds, Handbook of Natural Language Processing. Marcel Dekker.   
Logeswaran, L., H. Lee, and D. Radev. 2018. Sentence ordering and coherence modeling using recurrent neural networks. AAAI.   
Longpre, S., L. Hou, T. Vu, A. Webson, H. W. Chung, Y. Tay, D. Zhou, Q. V. Le, B. Zoph, J. Wei, and A. Roberts. 2023. The Flan collection: Designing data and methods for effective instruction tuning. ICML.   
Longpre, S., R. Mahari, A. Lee, C. Lund, H. Oderinwale, W. Brannon, N. Saxena, N. Obeng-Marnu, T. South, C. Hunter, et al. 2024a. Consent in crisis: The rapid decline of the ai data commons. ArXiv preprint.   
Longpre, S., G. Yauney, E. Reif, K. Lee, A. Roberts, B. Zoph, D. Zhou, J. Wei, K. Robinson, D. Mimno, and D. Ippolito. 2024b. A pretrainer’s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. NAACL HLT.   
Louis, A. and A. Nenkova. 2012. A coherence model based on syntactic patterns. EMNLP.   
Loureiro, D. and A. Jorge. 2019. Language modelling makes sense: Propagating representations through WordNet for full-coverage word sense disambiguation. ACL.   
Louviere, J. J., T. N. Flynn, and A. A. J. Marley. 2015. Best-worst scaling: Theory, methods and applications. Cambridge University Press.   
Lovins, J. B. 1968. Development of a stemming algorithm. Mechanical Translation and Computational Linguistics, 11(1–2):9–13.   
Lowerre, B. T. 1976. The Harpy Speech Recognition System. Ph.D. thesis, Carnegie Mellon University, Pittsburgh, PA.   
Luhn, H. P. 1957. A statistical approach to the mechanized encoding and searching of literary information. IBM Journal of Research and Development, 1(4):309–317.   
Lui, M. and T. Baldwin. 2011. Crossdomain feature selection for language identification. IJCNLP.   
Lui, M. and T. Baldwin. 2012. langid.py: An off-the-shelf language identification tool. ACL.   
Lukasik, M., B. Dadachev, K. Papineni, and G. Simoes. 2020. ˜ Text segmentation by cross segment attention. EMNLP.   
Luo, X. 2005. On coreference resolution performance metrics. EMNLP.   
Luo, X. and S. Pradhan. 2016. Evaluation metrics. In M. Poesio, R. Stuckardt, and Y. Versley, eds, Anaphora resolution: Algorithms, resources, and applications, 141– 163. Springer.   
Luo, X., S. Pradhan, M. Recasens, and E. H. Hovy. 2014. An extension of BLANC to system mentions. ACL.   
Ma, X. and E. H. Hovy. 2016. Endto-end sequence labeling via bidirectional LSTM-CNNs-CRF. ACL.   
Maas, A., Z. Xie, D. Jurafsky, and A. Y. Ng. 2015. Lexicon-free conversational speech recognition with neural networks. NAACL HLT.   
Maas, A. L., A. Y. Hannun, and A. Y. Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. ICML.   
Maas, A. L., P. Qi, Z. Xie, A. Y. Hannun, C. T. Lengerich, D. Jurafsky, and A. Y. Ng. 2017. Building dnn acoustic models for large vocabulary speech recognition. Computer Speech & Language, 41:195–213.   
Magerman, D. M. 1995. Statistical decision-tree models for parsing. ACL.   
Mairesse, F. and M. A. Walker. 2008. Trainable generation of big-five personality styles through data-driven parameter estimation. ACL.   
Mann, W. C. and S. A. Thompson. 1987. Rhetorical structure theory: A theory of text organization. Technical Report RS-87-190, Information Sciences Institute.   
Manning, C. D. 2011. Part-of-speech tagging from $9 7 \%$ to $100 \%$ : Is it time for some linguistics? CICLing 2011.   
Manning, C. D., P. Raghavan, and H. Schutze. 2008. ¨ Introduction to Information Retrieval. Cambridge.   
Manning, C. D., M. Surdeanu, J. Bauer, J. Finkel, S. Bethard, and D. McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. ACL.   
Marcu, D. 1997. The rhetorical parsing of natural language texts. ACL.   
Marcu, D. 1999. A decision-based approach to rhetorical parsing. ACL.   
Marcu, D. 2000a. The rhetorical parsing of unrestricted texts: A surfacebased approach. Computational Linguistics, 26(3):395–448.   
Marcu, D., ed. 2000b. The Theory and Practice of Discourse Parsing and Summarization. MIT Press.   
Marcu, D. and A. Echihabi. 2002. An unsupervised approach to recognizing discourse relations. ACL.   
Marcu, D. and W. Wong. 2002. A phrase-based, joint probability model for statistical machine translation. EMNLP.   
Marcus, M. P. 1980. A Theory of Syntactic Recognition for Natural Language. MIT Press.   
Marcus, M. P., B. Santorini, and M. A. Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn treebank. Computational Linguistics, 19(2):313–330.   
Marie, B., A. Fujita, and R. Rubino. 2021. Scientific credibility of machine translation research: A metaevaluation of 769 papers. ACL.   
Markov, A. A. 1913. Essai d’une recherche statistique sur le texte du roman “Eugene Onegin” illustrant la liaison des epreuve en chain (‘Example of a statistical investigation of the text of “Eugene Onegin” illustrating the dependence between samples in chain’). Izvistia Imperatorskoi Akademii Nauk (Bulletin de l’Academie Imp ´ eriale des Sciences ´ de St.-Petersbourg) ´ , 7:153–162.   
de Marneffe, M.-C., T. Dozat, N. Silveira, K. Haverinen, F. Ginter, J. Nivre, and C. D. Manning. 2014. Universal Stanford dependencies: A cross-linguistic typology. LREC.   
de Marneffe, M.-C., B. MacCartney, and C. D. Manning. 2006. Generating typed dependency parses from phrase structure parses. LREC.   
de Marneffe, M.-C. and C. D. Manning. 2008. The Stanford typed dependencies representation. COLING Workshop on Cross-Framework and Cross-Domain Parser Evaluation.   
de Marneffe, M.-C., C. D. Manning, J. Nivre, and D. Zeman. 2021. Universal Dependencies. Computational Linguistics, 47(2):255–308.   
de Marneffe, M.-C., M. Recasens, and C. Potts. 2015. Modeling the lifespan of discourse entities with application to coreference resolution. JAIR, 52:445–475.   
Maron, M. E. 1961. Automatic indexing: an experimental inquiry. Journal of the ACM, 8(3):404–417.   
Marquez, L., X. Carreras, K. C.\` Litkowski, and S. Stevenson. 2008. Semantic role labeling: An introduction to the special issue. Computational linguistics, 34(2):145–159.   
Marshall, I. 1983. Choice of grammatical word-class without global syntactic analysis: Tagging words in the LOB corpus. Computers and the Humanities, 17:139–150.   
Marshall, I. 1987. Tag selection using probabilistic methods. In R. Garside, G. Leech, and G. Sampson, eds, The Computational Analysis of English, 42–56. Longman.   
Martschat, S. and M. Strube. 2014. Recall error analysis for coreference resolution. EMNLP.   
Martschat, S. and M. Strube. 2015. Latent structures for coreference resolution. TACL, 3:405–418.   
Mathis, D. A. and M. C. Mozer. 1995. On the computational utility of consciousness. NeurIPS. MIT Press.   
McCallum, A., D. Freitag, and F. C. N. Pereira. 2000. Maximum entropy Markov models for information extraction and segmentation. ICML.   
McCallum, A. and W. Li. 2003. Early results for named entity recogni tion with conditional random fields, feature induction and web-enhanced lexicons. CoNLL.   
McCallum, A. and K. Nigam. 1998. A comparison of event models for naive bayes text classification. AAAI/ICML-98 Workshop on Learning for Text Categorization.   
McCarthy, J. F. and W. G. Lehnert. 1995. Using decision trees for coreference resolution. IJCAI-95.   
McClelland, J. L. and J. L. Elman. 1986. The TRACE model of speech perception. Cognitive Psychology, 18:1–86.   
McClelland, J. L. and D. E. Rumelhart, eds. 1986. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 2: Psychological and Biological Models. MIT Press.   
McCulloch, W. S. and W. Pitts. 1943. A logical calculus of ideas immanent in nervous activity. Bulletin of Mathematical Biophysics, 5:115–133.   
McDonald, R., K. Crammer, and F. C. N. Pereira. 2005a. Online large-margin training of dependency parsers. ACL.   
McDonald, R. and J. Nivre. 2011. Analyzing and integrating dependency parsers. Computational Linguistics, 37(1):197–230.   
McDonald, R., F. C. N. Pereira, K. Ribarov, and J. Hajic. 2005b. ˇ Nonprojective dependency parsing using spanning tree algorithms. HLTEMNLP.   
McGuffie, K. and A. Newhouse. 2020. The radicalization risks of GPT-3 and advanced neural language models. ArXiv preprint arXiv:2009.06807.   
McLuhan, M. 1964. Understanding Media: The Extensions of Man. New American Library.   
Melamud, O., J. Goldberger, and I. Dagan. 2016. context2vec: Learning generic context embedding with bidirectional LSTM. CoNLL.   
Merialdo, B. 1994. Tagging English text with a probabilistic model. Computational Linguistics, 20(2):155–172.   
Mesgar, M. and M. Strube. 2016. Lexical coherence graph modeling using word embeddings. ACL.   
Metsis, V., I. Androutsopoulos, and G. Paliouras. 2006. Spam filtering with naive bayes-which naive bayes? CEAS.   
Meyers, A., R. Reeves, C. Macleod, R. Szekely, V. Zielinska, B. Young, and R. Grishman. 2004. The nombank project: An interim report. NAACL/HLT Workshop: Frontiers in Corpus Annotation.   
Mihalcea, R. and A. Csomai. 2007. Wikify!: Linking documents to encyclopedic knowledge. CIKM 2007.   
Mikheev, A., M. Moens, and C. Grover. 1999. Named entity recognition without gazetteers. EACL.   
Mikolov, T. 2012. Statistical language models based on neural networks. Ph.D. thesis, Brno University of Technology.   
Mikolov, T., K. Chen, G. S. Corrado, and J. Dean. 2013a. Efficient estimation of word representations in vector space. ICLR 2013.   
Mikolov, T., M. Karafiat, L. Bur- ´ get, J. Cernock ˇ y, and S. Khudan- \` pur. 2010. Recurrent neural network based language model. INTERSPEECH.   
Mikolov, T., S. Kombrink, L. Burget, J. H. Cernock ˇ y, and S. Khudanpur. \` 2011. Extensions of recurrent neural network language model. ICASSP.   
Mikolov, T., I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. 2013b. Distributed representations of words and phrases and their compositionality. NeurIPS.   
Mikolov, T., W.-t. Yih, and G. Zweig. 2013c. Linguistic regularities in continuous space word representations. NAACL HLT.   
Miller, G. A. and J. G. Beebe-Center. 1956. Some psychological methods for evaluating the quality of translations. Mechanical Translation, 3:73–80.   
Miller, G. A. and W. G. Charles. 1991. Contextual correlates of semantics similarity. Language and Cognitive Processes, 6(1):1–28.   
Miller, G. A. and N. Chomsky. 1963. Finitary models of language users. In R. D. Luce, R. R. Bush, and E. Galanter, eds, Handbook of Mathematical Psychology, volume II, 419–491. John Wiley.   
Miller, G. A. and J. A. Selfridge. 1950. Verbal context and the recall of meaningful material. American Journal of Psychology, 63:176–185.   
Miller, S., R. J. Bobrow, R. Ingria, and R. Schwartz. 1994. Hidden understanding models of natural language. ACL.   
Milne, D. and I. H. Witten. 2008. Learning to link with wikipedia. CIKM 2008.   
Miltsakaki, E., R. Prasad, A. K. Joshi, and B. L. Webber. 2004. The Penn Discourse Treebank. LREC.   
Min, S., X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? EMNLP.   
Minsky, M. 1961. Steps toward artificial intelligence. Proceedings of the IRE, 49(1):8–30.   
Minsky, M. 1974. A framework for representing knowledge. Technical Report 306, MIT AI Laboratory. Memo 306.   
Minsky, M. and S. Papert. 1969. Perceptrons. MIT Press.   
Mintz, M., S. Bills, R. Snow, and D. Jurafsky. 2009. Distant supervision for relation extraction without labeled data. ACL IJCNLP.   
Mirza, P. and S. Tonelli. 2016. CATENA: CAusal and TEmporal relation extraction from NAtural

language texts. COLING.

Mishra, S., D. Khashabi, C. Baral, and H. Hajishirzi. 2022. Cross-task generalization via natural language crowdsourcing instructions. ACL.   
Mitchell, M., S. Wu, A. Zaldivar, P. Barnes, L. Vasserman, B. Hutchinson, E. Spitzer, I. D. Raji, and T. Gebru. 2019. Model cards for model reporting. ACM FAccT.   
Mitkov, R. 2002. Anaphora Resolution. Longman.   
Mohamed, A., G. E. Dahl, and G. E. Hinton. 2009. Deep Belief Networks for phone recognition. NIPS Workshop on Deep Learning for Speech Recognition and Related Applications.   
Mohammad, S. M. 2018a. Obtaining reliable human ratings of valence, arousal, and dominance for 20,000 English words. ACL.   
Mohammad, S. M. 2018b. Word affect intensities. LREC.   
Mohammad, S. M. and P. D. Turney. 2013. Crowdsourcing a wordemotion association lexicon. Computational Intelligence, 29(3):436– 465.   
Monroe, B. L., M. P. Colaresi, and K. M. Quinn. 2008. Fightin’words: Lexical feature selection and evaluation for identifying the content of political conflict. Political Analysis, 16(4):372–403.   
Moors, A., P. C. Ellsworth, K. R. Scherer, and N. H. Frijda. 2013. Appraisal theories of emotion: State of the art and future development. Emotion Review, 5(2):119–124.   
Moosavi, N. S. and M. Strube. 2016. Which coreference evaluation metric do you trust? A proposal for a link-based entity aware metric. ACL.   
Morey, M., P. Muller, and N. Asher. 2017. How much progress have we made on RST discourse parsing? a replication study of recent results on the rst-dt. EMNLP.   
Morgan, A. A., L. Hirschman, M. Colosimo, A. S. Yeh, and J. B. Colombe. 2004. Gene name identification and normalization using a model organism database. Journal of Biomedical Informatics, 37(6):396– 410.   
Morgan, N. and H. Bourlard. 1990. Continuous speech recognition using multilayer perceptrons with hidden markov models. ICASSP.   
Morgan, N. and H. A. Bourlard. 1995. Neural networks for statistical recognition of continuous speech. Proceedings of the IEEE,

83(5):742–772.

Morris, J. and G. Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48.   
Mosteller, F. and D. L. Wallace. 1963. Inference in an authorship problem: A comparative study of discrimination methods applied to the authorship of the disputed federalist papers. Journal of the American Statistical Association, 58(302):275–309.   
Mosteller, F. and D. L. Wallace. 1964. Inference and Disputed Authorship: The Federalist. Springer-Verlag. 1984 2nd edition: Applied Bayesian and Classical Inference.   
Mrksiˇ c, N., D. ´ O S ´ eaghdha, T.-H. Wen, ´ B. Thomson, and S. Young. 2017. Neural belief tracker: Data-driven dialogue state tracking. ACL.   
Muller, P., C. Braud, and M. Morey. 2019. ToNy: Contextual embeddings for accurate multilingual discourse segmentation of full documents. Workshop on Discourse Relation Parsing and Treebanking.   
Murphy, K. P. 2012. Machine learning: A probabilistic perspective. MIT Press.   
Musi, E., M. Stede, L. Kriese, S. Muresan, and A. Rocci. 2018. A multilayer annotated corpus of argumentative text: From argument schemes to discourse relations. LREC.   
Myers, G. 1992. “In this paper we report...”: Speech acts and scientific facts. Journal of Pragmatics, 17(4):295–313.   
Nadas, A. 1984. Estimation of prob- ´ abilities in the language model of the IBM speech recognition system. IEEE Transactions on ASSP, 32(4):859–861.   
Nadeem, M., A. Bethke, and S. Reddy. 2021. StereoSet: Measuring stereotypical bias in pretrained language models. ACL.   
Nagata, M. and T. Morimoto. 1994. First steps toward statistical modeling of dialogue to predict the speech act type of the next utterance. Speech Communication, 15:193–203.   
Nallapati, R., B. Zhou, C. dos Santos, C¸ . Gulc¸ehre, and B. Xiang. 2016. Abstractive text summarization using sequence-to-sequence RNNs and beyond. CoNLL.   
Nash-Webber, B. L. 1975. The role of semantics in automatic speech understanding. In D. G. Bobrow and A. Collins, eds, Representation and Understanding, 351–382. Academic Press.   
Naur, P., J. W. Backus, F. L. Bauer, J. Green, C. Katz, J. McCarthy, A. J. Perlis, H. Rutishauser, K. Samelson, B. Vauquois, J. H. Wegstein, A. van Wijnagaarden, and M. Woodger. 1960. Report on the algorithmic language ALGOL 60. CACM, 3(5):299–314. Revised in CACM 6:1, 1-17, 1963.   
Nayak, N., D. Hakkani-Tur, M. A. ¨ Walker, and L. P. Heck. 2017. To plan or not to plan? discourse planning in slot-value informed sequence to sequence models for language generation. INTERSPEECH.   
Neff, G. and P. Nagy. 2016. Talking to bots: Symbiotic agency and the case of Tay. International Journal of Communication, 10:4915–4931.   
Ng, A. Y. and M. I. Jordan. 2002. On discriminative vs. generative classifiers: A comparison of logistic regression and naive bayes. NeurIPS.   
Ng, H. T., L. H. Teo, and J. L. P. Kwan. 2000. A machine learning approach to answering questions for reading comprehension tests. EMNLP.   
Ng, V. 2004. Learning noun phrase anaphoricity to improve coreference resolution: Issues in representation and optimization. ACL.   
Ng, V. 2005a. Machine learning for coreference resolution: From local classification to global ranking. ACL.   
Ng, V. 2005b. Supervised ranking for pronoun resolution: Some recent improvements. AAAI.   
Ng, V. 2010. Supervised noun phrase coreference research: The first fifteen years. ACL.   
Ng, V. 2017. Machine learning for entity coreference resolution: A retrospective look at two decades of research. AAAI.   
Ng, V. and C. Cardie. 2002a. Identifying anaphoric and non-anaphoric noun phrases to improve coreference resolution. COLING.   
Ng, V. and C. Cardie. 2002b. Improving machine learning approaches to coreference resolution. ACL.   
Nguyen, D. T. and S. Joty. 2017. A neural local coherence model. ACL.   
Nickerson, R. S. 1976. On conversational interaction with computers. Proceedings of the ACM/SIGGRAPH workshop on User-oriented design of interactive graphics systems.   
Nie, A., E. Bennett, and N. Goodman. 2019. DisSent: Learning sentence representations from explicit discourse relations. ACL.   
Nielsen, J. 1992. The usability engineering life cycle. IEEE Computer, 25(3):12–22.   
Nielsen, M. A. 2015. Neural networks and Deep learning. Determination Press USA.   
Nigam, K., J. D. Lafferty, and A. McCallum. 1999. Using maximum entropy for text classification. IJCAI99 workshop on machine learning for information filtering.   
Nirenburg, S., H. L. Somers, and Y. Wilks, eds. 2002. Readings in Machine Translation. MIT Press.   
Nissim, M., S. Dingare, J. Carletta, and M. Steedman. 2004. An annotation scheme for information status in dialogue. LREC.   
NIST. 2005. Speech recognition scoring toolkit (sctk) version 2.1. http://www.nist.gov/speech/ tools/.   
NIST. 2007. Matched Pairs SentenceSegment Word Error (MAPSSWE) Test.   
Nivre, J. 2007. Incremental nonprojective dependency parsing. NAACL-HLT.   
Nivre, J. 2003. An efficient algorithm for projective dependency parsing. Proceedings of the 8th International Workshop on Parsing Technologies (IWPT).   
Nivre, J. 2006. Inductive Dependency Parsing. Springer.   
Nivre, J. 2009. Non-projective dependency parsing in expected linear time. ACL IJCNLP.   
Nivre, J., J. Hall, S. Kubler, R. Mc-¨ Donald, J. Nilsson, S. Riedel, and D. Yuret. 2007a. The conll 2007 shared task on dependency parsing. EMNLP/CoNLL.   
Nivre, J., J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kubler, S. Mari- ¨ nov, and E. Marsi. 2007b. Maltparser: A language-independent system for data-driven dependency parsing. Natural Language Engineering, 13(02):95–135.   
Nivre, J. and J. Nilsson. 2005. Pseudoprojective dependency parsing. ACL.   
Nivre, J. and M. Scholz. 2004. Deterministic dependency parsing of english text. COLING.   
Niwa, Y. and Y. Nitta. 1994. Cooccurrence vectors from corpora vs. distance vectors from dictionaries. COLING.   
Noreen, E. W. 1989. Computer Intensive Methods for Testing Hypothesis. Wiley.   
Norman, D. A. 1988. The Design of Everyday Things. Basic Books.   
Norvig, P. 1991. Techniques for automatic memoization with applications to context-free parsing. Computational Linguistics, 17(1):91–98.   
Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002a. Harvesting implicit group attitudes and beliefs from a demonstration web site. Group Dynamics: Theory, Research, and Practice, 6(1):101.   
Nosek, B. A., M. R. Banaji, and A. G. Greenwald. 2002b. Math=male, me=female, therefore math $\neq$ me. Journal of personality and social psychology, 83(1):44.   
Nostalgebraist. 2020. Interpreting gpt: the logit lens. White paper.   
Ocal, M., A. Perez, A. Radas, and M. Finlayson. 2022. Holistic evaluation of automatic TimeML annotators. LREC.   
Och, F. J. 1998. Ein beispielsbasierter und statistischer Ansatz zum maschinellen Lernen von naturlichsprachlicher ¨ Ubersetzung ¨ . Ph.D. thesis, Universitat Erlangen- ¨ Nurnberg, Germany. Diplomarbeit ¨ (diploma thesis).   
Och, F. J. 2003. Minimum error rate training in statistical machine translation. ACL.   
Och, F. J. and H. Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. ACL.   
Och, F. J. and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.   
Och, F. J. and H. Ney. 2004. The align ment template approach to statistical machine translation. Computational Linguistics, 30(4):417–449.   
O’Connor, B., M. Krieger, and D. Ahn. 2010. Tweetmotif: Exploratory search and topic summarization for twitter. ICWSM.   
Olive, J. P. 1977. Rule synthesis of speech from dyadic units. ICASSP77.   
Olsson, C., N. Elhage, N. Nanda, N. Joseph, N. DasSarma, T. Henighan, B. Mann, A. Askell, Y. Bai, A. Chen, et al. 2022. Incontext learning and induction heads. ArXiv preprint.   
Olteanu, A., F. Diaz, and G. Kazai. 2020. When are search completion suggestions problematic? CSCW.   
van den Oord, A., S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu. 2016. WaveNet: A Generative Model for Raw Audio. ISCA Workshop on Speech Synthesis Workshop.   
Oppenheim, A. V., R. W. Schafer, and T. G. J. Stockham. 1968. Nonlinear filtering of multiplied and convolved signals. Proceedings of the IEEE, 56(8):1264–1291.   
Oravecz, C. and P. Dienes. 2002. Efficient stochastic part-of-speech tagging for Hungarian. LREC.   
Osgood, C. E., G. J. Suci, and P. H. Tannenbaum. 1957. The Measurement of Meaning. University of Illinois Press.   
Ouyang, L., J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano, J. Leike, and R. Lowe. 2022. Training language models to follow instructions with human feedback. NeurIPS, volume 35.   
Packard, D. W. 1973. Computerassisted morphological analysis of ancient Greek. COLING.   
Palmer, D. 2012. Text preprocessing. In N. Indurkhya and F. J. Damerau, eds, Handbook of Natural Language Processing, 9–30. CRC Press.   
Palmer, M., D. Gildea, and N. Xue. 2010. Semantic role labeling. Synthesis Lectures on Human Language Technologies, 3(1):1–103.   
Palmer, M., P. Kingsbury, and D. Gildea. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.   
Panayotov, V., G. Chen, D. Povey, and S. Khudanpur. 2015. Librispeech: an ASR corpus based on public domain audio books. ICASSP.   
Pang, B. and L. Lee. 2008. Opinion mining and sentiment analysis. Foundations and trends in information retrieval, 2(1-2):1–135.   
Pang, B., L. Lee, and S. Vaithyanathan. 2002. Thumbs up? Sentiment classification using machine learning techniques. EMNLP.   
Paolino, J. 2017. Google Home vs Alexa: Two simple user experience design gestures that delighted a female user. Medium. Jan 4, 2017. https: //medium.com/startup-grind/ google-home-vs-alexa-56e26f6   
Papadimitriou, I., K. Lopez, and D. Jurafsky. 2023. Multilingual BERT has an accent: Evaluating English influences on fluency in multilingual models. EACL Findings.   
Papineni, K., S. Roukos, T. Ward, and W.-J. Zhu. 2002. Bleu: A method for automatic evaluation of machine translation. ACL.   
Park, J. H., J. Shin, and P. Fung. 2018. Reducing gender bias in abusive language detection. EMNLP.   
Park, J. and C. Cardie. 2014. Identifying appropriate support for propositions in online user comments. First workshop on argumentation mining.   
Parrish, A., A. Chen, N. Nangia, V. Padmakumar, J. Phang, J. Thompson, P. M. Htut, and S. Bowman. 2022. BBQ: A hand-built bias benchmark for question answering. Findings of ACL 2022.   
Paszke, A., S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer. 2017. Automatic differentiation in pytorch. NIPS-W.   
Pearl, C. 2017. Designing Voice User Interfaces: Principles of Conversational Experiences. O’Reilly.   
Peldszus, A. and M. Stede. 2013. From argument diagrams to argumentation mining in texts: A survey. International Journal of Cognitive Informatics and Natural Intelligence (IJCINI), 7(1):1–31.   
Peldszus, A. and M. Stede. 2016. An annotated corpus of argumentative microtexts. 1st European Conference on Argumentation.   
Penn, G. and P. Kiparsky. 2012. On Pan¯ . ini and the generative capacity of contextualized replacement systems. COLING.   
Pennebaker, J. W., R. J. Booth, and M. E. Francis. 2007. Linguistic Inquiry and Word Count: LIWC 2007. Austin, TX.   
Pennington, J., R. Socher, and C. D. Manning. 2014. GloVe: Global vectors for word representation. EMNLP.   
Percival, W. K. 1976. On the historical source of immediate constituent analysis. In J. D. McCawley, ed., Syntax and Semantics Volume 7, Notes from the Linguistic Underground, 229–242. Academic Press.   
Perrault, C. R. and J. Allen. 1980. A plan-based analysis of indirect speech acts. American Journal of Computational Linguistics, 6(3- 4):167–182.   
Peters, M., M. Neumann, M. Iyyer,   
9ac77.M. Gardner, C. Clark, K. Lee,and L. Zettlemoyer. 2018. Deep contextualized word representations. NAACL HLT.   
Peterson, G. E., W. S.-Y. Wang, and E. Sivertsen. 1958. Segmentation techniques in speech synthesis. JASA, 30(8):739–742.   
Peterson, J. C., D. Chen, and T. L. Griffiths. 2020. Parallelograms revisited: Exploring the limitations of vector space models for simple analogies. Cognition, 205.   
Petroni, F., T. Rocktaschel, S. Riedel, ¨ P. Lewis, A. Bakhtin, Y. Wu, and A. Miller. 2019. Language models as knowledge bases? EMNLP.   
Petrov, S., D. Das, and R. McDonald. 2012. A universal part-of-speech tagset. LREC.   
Petrov, S. and R. McDonald. 2012. Overview of the 2012 shared task on parsing the web. Notes of the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), volume 59.   
Phillips, A. V. 1960. A questionanswering routine. Technical Report 16, MIT AI Lab.   
Picard, R. W. 1995. Affective computing. Technical Report 321, MIT Media Lab Perceputal Computing Technical Report. Revised November 26, 1995.   
Pieraccini, R., E. Levin, and C.-H. Lee. 1991. Stochastic representation of conceptual structure in the ATIS task. Speech and Natural Language Workshop.   
Pierce, J. R., J. B. Carroll, E. P. Hamp, D. G. Hays, C. F. Hockett, A. G. Oettinger, and A. J. Perlis. 1966. Language and Machines: Computers in Translation and Linguistics. ALPAC report. National Academy of Sciences, National Research Council, Washington, DC.   
Pilehvar, M. T. and J. CamachoCollados. 2019. WiC: the wordin-context dataset for evaluating context-sensitive meaning representations. NAACL HLT.   
Pitler, E., A. Louis, and A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. ACL IJCNLP.   
Pitler, E. and A. Nenkova. 2009. Using syntax to disambiguate explicit discourse connectives in text. ACL IJCNLP.   
Plutchik, R. 1962. The emotions: Facts, theories, and a new model. Random House.   
Plutchik, R. 1980. A general psychoevolutionary theory of emotion. In R. Plutchik and H. Kellerman, eds, Emotion: Theory, Research, and Experience, Volume 1, 3–33. Academic Press.   
Poesio, M., R. Stevenson, B. Di Eugenio, and J. Hitzeman. 2004. Centering: A parametric theory and its instantiations. Computational Linguistics, 30(3):309–363.   
Poesio, M., R. Stuckardt, and Y. Versley. 2016. Anaphora resolution: Algorithms, resources, and applications. Springer.   
Poesio, M., P. Sturt, R. Artstein, and R. Filik. 2006. Underspecification and anaphora: Theoretical issues and preliminary evidence. Discourse processes, 42(2):157–175.   
Poesio, M. and R. Vieira. 1998. A corpus-based investigation of definite description use. Computational Linguistics, 24(2):183–216.   
Polanyi, L. 1988. A formal model of the structure of discourse. Journal of Pragmatics, 12.   
Polanyi, L., C. Culy, M. van den Berg, G. L. Thione, and D. Ahn. 2004. A rule based approach to discourse parsing. Proceedings of SIGDIAL.   
Pollard, C. and I. A. Sag. 1994. HeadDriven Phrase Structure Grammar. University of Chicago Press.   
Ponzetto, S. P. and M. Strube. 2006. Exploiting semantic role labeling, WordNet and Wikipedia for coreference resolution. HLT-NAACL.   
Ponzetto, S. P. and M. Strube. 2007. Knowledge derived from Wikipedia for computing semantic relatedness. JAIR, 30:181–212.   
Popovic, M. 2015. ´ chrF: character n-gram F-score for automatic MT evaluation. Proceedings of the Tenth Workshop on Statistical Machine Translation.   
Popp, D., R. A. Donovan, M. Crawford, K. L. Marsh, and M. Peele. 2003. Gender, race, and speech style stereotypes. Sex Roles, 48(7-8):317– 325.   
Porter, M. F. 1980. An algorithm for suffix stripping. Program, 14(3):130–137.   
Post, M. 2018. A call for clarity in reporting BLEU scores. WMT 2018.   
Potts, C. 2011. On the negativity of negation. In N. Li and D. Lutz, eds, Proceedings of Semantics and Linguistic Theory 20, 636–659. CLC Publications, Ithaca, NY.   
Povey, D., A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz, J. Silovsky,´ G. Stemmer, and K. Vesely. 2011. ´ The Kaldi speech recognition toolkit. ASRU.   
Pradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel. 2007a. OntoNotes: A unified relational semantic representation. Proceedings of ICSC.   
Pradhan, S., E. H. Hovy, M. P. Marcus, M. Palmer, L. A. Ramshaw, and R. M. Weischedel. 2007b. Ontonotes: a unified relational semantic representation. Int. J. Semantic Computing, 1(4):405–419.   
Pradhan, S., X. Luo, M. Recasens, E. H. Hovy, V. Ng, and M. Strube. 2014. Scoring coreference partitions of predicted mentions: A reference implementation. ACL.   
Pradhan, S., A. Moschitti, N. Xue, H. T. Ng, A. Bjorkelund, O. Uryupina, ¨ Y. Zhang, and Z. Zhong. 2013. Towards robust linguistic analysis using OntoNotes. CoNLL.   
Pradhan, S., A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang. 2012a. CoNLL-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. CoNLL.   
Pradhan, S., A. Moschitti, N. Xue, O. Uryupina, and Y. Zhang. 2012b. Conll-2012 shared task: Modeling multilingual unrestricted coreference in OntoNotes. CoNLL.   
Pradhan, S., L. Ramshaw, M. P. Marcus, M. Palmer, R. Weischedel, and N. Xue. 2011. CoNLL-2011 shared task: Modeling unrestricted coreference in OntoNotes. CoNLL.   
Pradhan, S., L. Ramshaw, R. Weischedel, J. MacBride, and L. Micciulla. 2007c. Unrestricted coreference: Identifying entities and events in OntoNotes. Proceedings of ICSC 2007.   
Pradhan, S., W. Ward, K. Hacioglu, J. H. Martin, and D. Jurafsky. 2005. Semantic role labeling using different syntactic views. ACL.   
Prasad, A., P. Hase, X. Zhou, and M. Bansal. 2023. GrIPS: Gradientfree, edit-based instruction search for prompting large language models. EACL.   
Prasad, R., N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. K. Joshi, and B. L. Webber. 2008. The Penn Discourse TreeBank 2.0. LREC.   
Prasad, R., B. L. Webber, and A. Joshi. 2014. Reflections on the Penn Discourse Treebank, comparable corpora, and complementary annotation. Computational Linguistics, 40(4):921–950.   
Prates, M. O. R., P. H. Avelar, and L. C. Lamb. 2019. Assessing gender bias in machine translation: a case study with Google Translate. Neural Computing and Applications, 32:6363– 6381.   
Price, P. J., W. Fisher, J. Bernstein, and D. Pallet. 1988. The DARPA 1000-word resource management database for continuous speech recognition. ICASSP.   
Prince, E. 1981. Toward a taxonomy of given-new information. In P. Cole, ed., Radical Pragmatics, 223–255. Academic Press.   
Propp, V. 1968. Morphology of the Folktale, 2nd edition. University of Texas Press. Original Russian 1928. Translated by Laurence Scott.   
Pryzant, R., D. Iter, J. Li, Y. Lee, C. Zhu, and M. Zeng. 2023. Au

tomatic prompt optimization with

EMNLP.   
Pundak, G. and T. N. Sainath. 2016. Lower frame rate neural network acoustic models. INTERSPEECH.   
Pustejovsky, J. 1991. The generative lexicon. Computational Linguistics, 17(4).   
Pustejovsky, J., P. Hanks, R. Saur´ı, A. See, R. Gaizauskas, A. Setzer, D. Radev, B. Sundheim, D. S. Day, L. Ferro, and M. Lazo. 2003. The TIMEBANK corpus. Proceedings of Corpus Linguistics 2003 Conference. UCREL Technical Paper number 16.   
Pustejovsky, J., R. Ingria, R. Saur´ı, J. Castano, J. Littman, ˜ R. Gaizauskas, A. Setzer, G. Katz, and I. Mani. 2005. The Specification Language TimeML, chapter 27. Oxford.   
Qin, L., Z. Zhang, and H. Zhao. 2016. A stacking gated neural architecture for implicit discourse relation classification. EMNLP.   
Qin, L., Z. Zhang, H. Zhao, Z. Hu, and E. Xing. 2017. Adversarial connective-exploiting networks for implicit discourse relation classification. ACL.   
Radford, A., J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. 2019. Language models are unsupervised multitask learners. OpenAI tech report.   
Rafailov, R., A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, and C. Finn. 2023. Direct preference optimization: Your language model is secretly a reward model. NeurIPS.   
Raffel, C., N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. 2020. Exploring the limits of transfer learning with a unified text-totext transformer. JMLR, 21(140):1– 67.   
Raghunathan, K., H. Lee, S. Rangarajan, N. Chambers, M. Surdeanu, D. Jurafsky, and C. D. Manning. 2010. A multi-pass sieve for coreference resolution. EMNLP.   
Rahman, A. and V. Ng. 2009. Supervised models for coreference resolution. EMNLP.   
Rahman, A. and V. Ng. 2012. Resolving complex cases of definite pronouns: the Winograd Schema challenge. EMNLP.   
Rajpurkar, P., R. Jia, and P. Liang. 2018. Know what you don’t know: Unanswerable questions for SQuAD. ACL.   
Rajpurkar, P., J. Zhang, K. Lopyrev, and P. Liang. 2016. SQuAD: $^ { 1 0 0 , 0 0 0 + }$ questions for machine comprehension of text. EMNLP.   
Ram, O., Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. LeytonBrown, and Y. Shoham. 2023. In-context retrieval-augmented language models. ArXiv preprint.   
Ramshaw, L. A. and M. P. Marcus. 1995. Text chunking using transformation-based learning. Proceedings of the 3rd Annual Workshop on Very Large Corpora.   
Rashkin, H., E. Bell, Y. Choi, and S. Volkova. 2017. Multilingual connotation frames: A case study on social media for targeted sentiment analysis and forecast. ACL.   
Rashkin, H., S. Singh, and Y. Choi. 2016. Connotation frames: A datadriven investigation. ACL.   
Rashkin, H., E. M. Smith, M. Li, and Y.-L. Boureau. 2019. Towards empathetic open-domain conversation models: A new benchmark and dataset. ACL.   
Ratinov, L. and D. Roth. 2012. Learning-based multi-sieve coreference resolution with knowledge. EMNLP.   
Ratnaparkhi, A. 1996. A maximum entropy part-of-speech tagger. EMNLP.   
Ratnaparkhi, A. 1997. A linear observed time statistical parser based on maximum entropy models. EMNLP.   
Rawls, J. 2001. Justice as fairness: A restatement. Harvard University Press.   
Recasens, M. and E. H. Hovy. 2011. BLANC: Implementing the Rand index for coreference evaluation. Natural Language Engineering, 17(4):485–510.   
Recasens, M., E. H. Hovy, and M. A. Mart´ı. 2011. Identity, non-identity, and near-identity: Addressing the complexity of coreference. Lingua, 121(6):1138–1152.   
Recasens, M. and M. A. Mart´ı. 2010. AnCora-CO: Coreferentially annotated corpora for Spanish and Catalan. Language Resources and Evaluation, 44(4):315–345.   
Reed, C., R. Mochales Palau, G. Rowe, and M.-F. Moens. 2008. Language resources for studying argument. LREC.   
Reeves, B. and C. Nass. 1996. The Media Equation: How People Treat Computers, Television, and New Media Like Real People and Places. Cambridge University Press.   
Rehder, B., M. E. Schreiner, M. B. W. Wolfe, D. Laham, T. K. Landauer, and W. Kintsch. 1998. Using Latent Semantic Analysis to assess knowledge: Some technical considerations. Discourse Processes, 25(2- 3):337–354.   
Rei, R., C. Stewart, A. C. Farinha, and A. Lavie. 2020. COMET: A neural framework for MT evaluation. EMNLP.   
Reichenbach, H. 1947. Elements of Symbolic Logic. Macmillan, New York.   
Reichman, R. 1985. Getting Computers to Talk Like You and Me. MIT Press.   
Resnik, P. 1993. Semantic classes and syntactic ambiguity. HLT.   
Resnik, P. 1996. Selectional constraints: An information-theoretic model and its computational realization. Cognition, 61:127–159.   
Reynolds, L. and K. McDonell. 2021. Prompt programming for large language models: Beyond the few-shot paradigm. CHI 2021.   
Riedel, S., L. Yao, and A. McCallum. 2010. Modeling relations and their mentions without labeled text. In Machine Learning and Knowledge Discovery in Databases, 148–163. Springer.   
Riedel, S., L. Yao, A. McCallum, and B. M. Marlin. 2013. Relation extraction with matrix factorization and universal schemas. NAACL HLT.   
Riloff, E. 1993. Automatically constructing a dictionary for information extraction tasks. AAAI.   
Riloff, E. 1996. Automatically generating extraction patterns from untagged text. AAAI.   
Riloff, E. and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. AAAI.   
Riloff, E. and M. Schmelzenbach. 1998. An empirical approach to conceptual case frame acquisition. Proceedings of the Sixth Workshop on Very Large Corpora.   
Riloff, E. and J. Shepherd. 1997. A corpus-based approach for building semantic lexicons. EMNLP.   
Riloff, E. and M. Thelen. 2000. A rulebased question answering system for reading comprehension tests. ANLP/NAACL workshop on reading comprehension tests.   
Riloff, E. and J. Wiebe. 2003. Learning extraction patterns for subjective expressions. EMNLP.   
Ritter, A., C. Cherry, and B. Dolan. 2010a. Unsupervised modeling of twitter conversations. NAACL HLT.   
Ritter, A., O. Etzioni, and Mausam. 2010b. A latent dirichlet allocation method for selectional preferences. ACL.   
Ritter, A., L. Zettlemoyer, Mausam, and O. Etzioni. 2013. Modeling missing data in distant supervision for information extraction. TACL, 1:367– 378.   
Roberts, A., C. Raffel, and N. Shazeer. 2020. How much knowledge can you pack into the parameters of a language model? EMNLP.   
Robertson, S., S. Walker, S. Jones, M. M. Hancock-Beaulieu, and M. Gatford. 1995. Okapi at TREC-3. Overview of the Third Text REtrieval Conference (TREC-3).   
Robinson, T. and F. Fallside. 1991. A recurrent error propagation network speech recognition system. Computer Speech & Language, 5(3):259–274.   
Robinson, T., M. Hochberg, and S. Renals. 1996. The use of recurrent neural networks in continuous speech recognition. In C.-H. Lee, F. K. Soong, and K. K. Paliwal, eds, Automatic speech and speaker recognition, 233–258. Springer.   
Rogers, A., M. Gardner, and I. Augenstein. 2023. QA dataset explosion: A taxonomy of NLP resources for question answering and reading comprehension. ACM Computing Surveys, 55(10):1–45.   
Rohde, D. L. T., L. M. Gonnerman, and D. C. Plaut. 2006. An improved model of semantic similarity based on lexical co-occurrence. CACM, 8:627–633.   
Roller, S., E. Dinan, N. Goyal, D. Ju, M. Williamson, Y. Liu, J. Xu, M. Ott, E. M. Smith, Y.-L. Boureau, and J. Weston. 2021. Recipes for building an open-domain chatbot. EACL.   
Rooth, M., S. Riezler, D. Prescher, G. Carroll, and F. Beil. 1999. Inducing a semantically annotated lexicon via EM-based clustering. ACL.   
Rosenblatt, F. 1958. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386–408.   
Rosenfeld, R. 1992. Adaptive Statistical Language Modeling: A Maximum Entropy Approach. Ph.D. thesis, Carnegie Mellon University.   
Rosenfeld, R. 1996. A maximum entropy approach to adaptive statistical language modeling. Computer Speech and Language, 10:187–228.   
Rosenthal, S. and K. McKeown. 2017. Detecting influencers in multiple online genres. ACM Transactions on Internet Technology (TOIT), 17(2).   
Rothe, S., S. Ebert, and H. Schutze. ¨ 2016. Ultradense Word Embeddings by Orthogonal Transformation. NAACL HLT.   
Roy, N., J. Pineau, and S. Thrun. 2000. Spoken dialogue management using probabilistic reasoning. ACL.   
Rudinger, R., J. Naradowsky, B. Leonard, and B. Van Durme. 2018. Gender bias in coreference resolution. NAACL HLT.   
Rumelhart, D. E., G. E. Hinton, and R. J. Williams. 1986. Learning internal representations by error propagation. In D. E. Rumelhart and J. L. McClelland, eds, Parallel Distributed Processing, volume 2, 318– 362. MIT Press.   
Rumelhart, D. E. and J. L. McClelland. 1986a. On learning the past tense of English verbs. In D. E. Rumelhart and J. L. McClelland, eds, Parallel Distributed Processing, volume 2, 216–271. MIT Press.   
Rumelhart, D. E. and J. L. McClelland, eds. 1986b. Parallel Distributed Processing. MIT Press.   
Rumelhart, D. E. and A. A. Abrahamson. 1973. A model for analogical reasoning. Cognitive Psychology, 5(1):1–28.   
Rumelhart, D. E. and J. L. McClelland, eds. 1986c. Parallel Distributed Processing: Explorations in the Microstructure of Cognition, volume 1: Foundations. MIT Press.   
Ruppenhofer, J., M. Ellsworth, M. R. L. Petruck, C. R. Johnson, C. F. Baker, and J. Scheffczyk. 2016. FrameNet II: Extended theory and practice.   
Ruppenhofer, J., C. Sporleder, R. Morante, C. F. Baker, and M. Palmer. 2010. Semeval-2010 task 10: Linking events and their participants in discourse. 5th International Workshop on Semantic Evaluation.   
Russell, J. A. 1980. A circumplex model of affect. Journal of personality and social psychology, 39(6):1161–1178.   
Russell, S. and P. Norvig. 2002. Artificial Intelligence: A Modern Approach, 2nd edition. Prentice Hall.   
Rutherford, A. and N. Xue. 2015. Improving the inference of implicit discourse relations via classifying explicit discourse connectives. NAACL HLT.   
Sachan, D. S., M. Lewis, D. Yogatama, L. Zettlemoyer, J. Pineau, and M. Zaheer. 2023. Questions are all you need to train a dense passage retriever. TACL, 11:600–616.   
Sacks, H., E. A. Schegloff, and G. Jefferson. 1974. A simplest systematics for the organization of turntaking for conversation. Language, 50(4):696–735.   
Sag, I. A. and M. Y. Liberman. 1975. The intonational disambiguation of indirect speech acts. In CLS-75, 487–498. University of Chicago.   
Sagae, K. 2009. Analysis of discourse structure with syntactic dependencies and data-driven shiftreduce parsing. IWPT-09.   
Sagawa, S., P. W. Koh, T. B. Hashimoto, and P. Liang. 2020. Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization. ICLR.   
Sagisaka, Y. 1988. Speech synthesis by rule using an optimal selection of non-uniform synthesis units. ICASSP.   
Sagisaka, Y., N. Kaiki, N. Iwahashi, and K. Mimura. 1992. Atr – ν -talk speech synthesis system. ICSLP.   
Sahami, M., S. T. Dumais, D. Heckerman, and E. Horvitz. 1998. A Bayesian approach to filtering junk e-mail. AAAI Workshop on Learning for Text Categorization.   
Sakoe, H. and S. Chiba. 1971. A dynamic programming approach to continuous speech recognition. Proceedings of the Seventh International Congress on Acoustics, volume 3. Akademiai Kiad ´ o. ´   
Sakoe, H. and S. Chiba. 1984. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on ASSP, ASSP-26(1):43–49.   
Salomaa, A. 1969. Probabilistic and weighted grammars. Information and Control, 15:529–544.   
Salton, G. 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. Prentice Hall.   
Salvetti, F., J. B. Lowe, and J. H. Martin. 2016. A tangled web: The faint signals of deception in text - boulder lies and truth corpus (BLT-C). LREC.   
Sampson, G. 1987. Alternative grammatical coding systems. In R. Garside, G. Leech, and G. Sampson, eds, The Computational Analysis of English, 165–183. Longman.   
Sankoff, D. and W. Labov. 1979. On the uses of variable rules. Language in society, 8(2-3):189–222.   
Sap, M., D. Card, S. Gabriel, Y. Choi, and N. A. Smith. 2019. The risk of racial bias in hate speech detection. ACL.   
Sap, M., M. C. Prasettio, A. Holtzman, H. Rashkin, and Y. Choi. 2017. Connotation frames of power and agency in modern films. EMNLP.   
Saur´ı, R., J. Littman, B. Knippen, R. Gaizauskas, A. Setzer, and J. Pustejovsky. 2006. TimeML annotation guidelines version 1.2.1. Manuscript.   
Scha, R. and L. Polanyi. 1988. An augmented context free grammar for discourse. COLING.   
Schank, R. C. and R. P. Abelson. 1975. Scripts, plans, and knowledge. Proceedings of IJCAI-75.   
Schank, R. C. and R. P. Abelson. 1977. Scripts, Plans, Goals and Understanding. Lawrence Erlbaum.   
Schegloff, E. A. 1968. Sequencing in conversational openings. American Anthropologist, 70:1075–1095.   
Scherer, K. R. 2000. Psychological models of emotion. In J. C. Borod, ed., The neuropsychology of emotion, 137–162. Oxford.   
Schiebinger, L. 2013. Machine translation: Analyzing gender. http://genderedinnovations. stanford.edu/case-studies/ nlp.html#tabs-2.   
Schiebinger, L. 2014. Scientific research must take gender into account. Nature, 507(7490):9.   
Schluter, N. 2018. The word analogy testing caveat. NAACL HLT.   
Schone, P. and D. Jurafsky. 2000. Knowlege-free induction of morphology using latent semantic analysis. CoNLL.   
Schone, P. and D. Jurafsky. 2001a. Is knowledge-free induction of multiword unit dictionary headwords a solved problem? EMNLP.   
Schone, P. and D. Jurafsky. 2001b. Knowledge-free induction of inflectional morphologies. NAACL.   
Schuster, M. and K. Nakajima. 2012. Japanese and Korean voice search. ICASSP.   
Schuster, M. and K. K. Paliwal. 1997. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45:2673–2681.   
Schutze, H. 1992a. Context space. ¨ AAAI Fall Symposium on Probabilistic Approaches to Natural Language.   
Schutze, H. 1992b. ¨ Dimensions of meaning. Proceedings of Supercomputing ’92. IEEE Press.   
Schutze, H. 1997. ¨ Ambiguity Resolution in Language Learning – Computational and Cognitive Models. CSLI, Stanford, CA.   
Schutze, H., D. A. Hull, and J. Peder- ¨ sen. 1995. A comparison of classifiers and document representations for the routing problem. SIGIR-95.   
Schutze, H. and J. Pedersen. 1993. A ¨ vector model for syntagmatic and paradigmatic relatedness. 9th Annual Conference of the UW Centre for the New OED and Text Research.   
Schutze, H. and Y. Singer. 1994. ¨ Partof-speech tagging using a variable memory Markov model. ACL.   
Schwartz, H. A., J. C. Eichstaedt, M. L. Kern, L. Dziurzynski, S. M. Ramones, M. Agrawal, A. Shah, M. Kosinski, D. Stillwell, M. E. P. Seligman, and L. H. Ungar. 2013. Personality, gender, and age in the language of social media: The openvocabulary approach. PloS one, 8(9):e73791.   
Schwenk, H. 2007. Continuous space language models. Computer Speech & Language, 21(3):492–518.   
Schwenk, H. 2018. Filtering and mining parallel data in a joint multilingual space. ACL.   
Schwenk, H., D. Dechelotte, and J.-L. Gauvain. 2006. Continuous space language models for statistical machine translation. COLING/ACL.   
Schwenk, H., G. Wenzek, S. Edunov, E. Grave, A. Joulin, and A. Fan. 2021. CCMatrix: Mining billions of high-quality parallel sentences on the web. ACL.   
Seaghdha, D. O. 2010. ´ Latent variable models of selectional preference. ACL.   
Seddah, D., R. Tsarfaty, S. Kubler, ¨ M. Candito, J. D. Choi, R. Farkas, J. Foster, I. Goenaga, K. Gojenola, Y. Goldberg, S. Green, N. Habash, M. Kuhlmann, W. Maier, J. Nivre, A. Przepiorkowski, R. Roth, ´ W. Seeker, Y. Versley, V. Vincze, M. Wolinski, A. Wr ´ oblewska, and ´ E. Villemonte de la Clergerie. ´ 2013. Overview of the SPMRL 2013 shared task: cross-framework evaluation of parsing morphologically rich languages. 4th Workshop on Statistical Parsing of Morphologically-Rich Languages.   
See, A., S. Roller, D. Kiela, and J. Weston. 2019. What makes a good conversation? how controllable attributes affect human judgments. NAACL HLT.   
Sekine, S. and M. Collins. 1997. The evalb software. http: //cs.nyu.edu/cs/projects/ proteus/evalb.   
Sellam, T., D. Das, and A. Parikh. 2020. BLEURT: Learning robust metrics for text generation. ACL.   
Sennrich, R., B. Haddow, and A. Birch. 2016. Neural machine translation of rare words with subword units. ACL.   
Seo, M., A. Kembhavi, A. Farhadi, and H. Hajishirzi. 2017. Bidirectional hension. ICLR.   
Shannon, C. E. 1948. A mathematical theory of communication. Bell System Technical Journal, 27(3):379– 423. Continued in the following volume.   
Shannon, C. E. 1951. Prediction and entropy of printed English. Bell System Technical Journal, 30:50–64.   
Sheil, B. A. 1976. Observations on context free parsing. SMIL: Statistical Methods in Linguistics, 1:71–109.   
Shen, J., R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu. 2018. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. ICASSP.   
Sheng, E., K.-W. Chang, P. Natarajan, and N. Peng. 2019. The woman worked as a babysitter: On biases in language generation. EMNLP.   
Shi, P. and J. Lin. 2019. Simple BERT models for relation extraction and semantic role labeling. ArXiv.   
Shi, W., S. Min, M. Yasunaga, M. Seo, R. James, M. Lewis, L. Zettlemoyer, and W.-t. Yih. 2023. REPLUG: Retrieval-augmented black-box language models. ArXiv preprint.   
Shriberg, E., R. Bates, P. Taylor, A. Stolcke, D. Jurafsky, K. Ries, N. Coccaro, R. Martin, M. Meteer, and C. Van Ess-Dykema. 1998. Can prosody aid the automatic classification of dialog acts in conversational speech? Language and Speech (Special Issue on Prosody and Conversation), 41(3-4):439–487.   
Sidner, C. L. 1979. Towards a computational theory of definite anaphora comprehension in English discourse. Technical Report 537, MIT Artificial Intelligence Laboratory, Cambridge, MA.   
Sidner, C. L. 1983. Focusing in the comprehension of definite anaphora. In M. Brady and R. C. Berwick, eds, Computational Models of Discourse, 267–330. MIT Press.   
Simmons, R. F. 1965. Answering English questions by computer: A survey. CACM, 8(1):53–70.   
Simmons, R. F. 1973. Semantic networks: Their computation and use for understanding English sentences. In R. C. Schank and K. M. Colby, eds, Computer Models of Thought and Language, 61–113. W.H. Freeman & Co.   
Simmons, R. F., S. Klein, and K. McConlogue. 1964. Indexing and dependency logic for answering English questions. American Documentation, 15(3):196–204. 2018. Ethnologue: Languages of the world, 21st edition. SIL International.   
Singh, S. P., D. J. Litman, M. Kearns, and M. A. Walker. 2002. Optimizing dialogue management with reinforcement learning: Experiments with the NJFun system. JAIR, 16:105–133.   
Singh, S., F. Vargus, D. D’souza, B. F. Karlsson, A. Mahendiran, W.-Y. Ko, H. Shandilya, J. Patel, D. Mataciunas, L. O’Mahony, M. Zhang, R. Hettiarachchi, J. Wilson, M. Machado, L. S. Moura, D. Krzeminski, H. Fadaei, I. Erg ´ un, ¨ I. Okoh, A. Alaagib, O. Mudannayake, Z. Alyafeai, V. M. Chien, S. Ruder, S. Guthikonda, E. A. Alghamdi, S. Gehrmann, N. Muennighoff, M. Bartolo, J. Kreutzer, A. U¨ Ust ¨ un, M. Fadaee, and ¨ S. Hooker. 2024. Aya dataset: An open-access collection for multilingual instruction tuning. ArXiv preprint.   
Sleator, D. and D. Temperley. 1993. Parsing English with a link grammar. IWPT-93.   
Sloan, M. C. 2010. Aristotle’s Nicomachean Ethics as the original locus for the Septem Circumstantiae. Classical Philology, 105(3):236– 251.   
Slobin, D. I. 1996. Two ways to travel. In M. Shibatani and S. A. Thompson, eds, Grammatical Constructions: Their Form and Meaning, 195–220. Clarendon Press.   
Smith, V. L. and H. H. Clark. 1993. On the course of answering questions. Journal of Memory and Language, 32:25–38.   
Smolensky, P. 1988. On the proper treatment of connectionism. Behavioral and brain sciences, 11(1):1– 23.   
Smolensky, P. 1990. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159–216.   
Snover, M., B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A study of translation edit rate with targeted human annotation. AMTA2006.   
Snow, R., D. Jurafsky, and A. Y. Ng. 2005. Learning syntactic patterns for automatic hypernym discovery. NeurIPS.   
Socher, R., J. Bauer, C. D. Manning, and A. Y. Ng. 2013. Parsing with compositional vector grammars. ACL.   
Socher, R., C. C.-Y. Lin, A. Y. Ng, and C. D. Manning. 2011. Parsing natural scenes and natural language with recursive neural networks. ICML.   
Soderland, S., D. Fisher, J. Aseltine, and W. G. Lehnert. 1995. CRYSTAL: Inducing a conceptual dictionary. IJCAI-95.   
Søgaard, A. 2010. Simple semisupervised training of part-ofspeech taggers. ACL.   
Søgaard, A. and Y. Goldberg. 2016. Deep multi-task learning with low level tasks supervised at lower layers. ACL.   
Søgaard, A., A. Johannsen, B. Plank, D. Hovy, and H. M. Alonso. 2014. What’s in a p-value in NLP? CoNLL.   
Soldaini, L., R. Kinney, A. Bhagia, D. Schwenk, D. Atkinson, R. Authur, B. Bogin, K. Chandu, J. Dumas, Y. Elazar, V. Hofmann, A. H. Jha, S. Kumar, L. Lucy, X. Lyu, N. Lambert, I. Magnusson, J. Morrison, N. Muennighoff, A. Naik, C. Nam, M. E. Peters, A. Ravichander, K. Richardson, Z. Shen, E. Strubell, N. Subramani, O. Tafjord, P. Walsh, L. Zettlemoyer, N. A. Smith, H. Hajishirzi, I. Beltagy, D. Groeneveld, J. Dodge, and K. Lo. 2024. Dolma: An open corpus of three trillion tokens for language model pretraining research. ArXiv preprint.   
Solorio, T., E. Blair, S. Maharjan, S. Bethard, M. Diab, M. Ghoneim, A. Hawwari, F. AlGhamdi, J. Hirschberg, A. Chang, and P. Fung. 2014. Overview for the first shared task on language identification in code-switched data. Workshop on Computational Approaches to Code Switching.   
Somasundaran, S., J. Burstein, and M. Chodorow. 2014. Lexical chaining for measuring discourse coherence quality in test-taker essays. COLING.   
Soon, W. M., H. T. Ng, and D. C. Y. Lim. 2001. A machine learning approach to coreference resolution of noun phrases. Computational Linguistics, 27(4):521–544.   
Soricut, R. and D. Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. HLT-NAACL.   
Soricut, R. and D. Marcu. 2006. Discourse generation using utilitytrained coherence models. COLING/ACL.   
Sorokin, D. and I. Gurevych. 2018. Mixing context granularities for improved entity linking on question answering data across entity categories. \*SEM.   
Sparck Jones, K. 1972. A statistical interpretation of term specificity and its application in retrieval. Journal of Documentation, 28(1):11–21.   
Sparck Jones, K. 1986. Synonymy and Semantic Classification. Edinburgh University Press, Edinburgh. Republication of 1964 PhD Thesis.   
Sporleder, C. and A. Lascarides. 2005. Exploiting linguistic cues to classify rhetorical relations. RANLP-05.   
Sporleder, C. and M. Lapata. 2005. Discourse chunking and its application to sentence compression. EMNLP.   
Sproat, R., A. W. Black, S. F. Chen, S. Kumar, M. Ostendorf, and C. Richards. 2001. Normalization of non-standard words. Computer Speech & Language, 15(3):287– 333.   
Sproat, R. and K. Gorman. 2018. A brief summary of the Kaggle text normalization challenge.   
Srivastava, N., G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. R. Salakhutdinov. 2014. Dropout: a simple way to prevent neural networks from overfitting. JMLR, 15(1):1929–1958.   
Stab, C. and I. Gurevych. 2014a. Annotating argument components and relations in persuasive essays. COLING.   
Stab, C. and I. Gurevych. 2014b. Identifying argumentative discourse structures in persuasive essays. EMNLP.   
Stab, C. and I. Gurevych. 2017. Parsing argumentation structures in persuasive essays. Computational Linguistics, 43(3):619–659.   
Stalnaker, R. C. 1978. Assertion. In P. Cole, ed., Pragmatics: Syntax and Semantics Volume 9, 315–332. Academic Press.   
Stamatatos, E. 2009. A survey of modern authorship attribution methods. JASIST, 60(3):538–556.   
Stanovsky, G., N. A. Smith, and L. Zettlemoyer. 2019. Evaluating gender bias in machine translation. ACL.   
Stede, M. 2011. Discourse processing. Morgan & Claypool.   
Stede, M. and J. Schneider. 2018. Argumentation Mining. Morgan & Claypool.   
Stern, M., J. Andreas, and D. Klein. 2017. A minimal span-based neural constituency parser. ACL.   
Stevens, K. N., S. Kasowski, and G. M. Fant. 1953. An electrical analog of the vocal tract. JASA, 25(4):734– 742.   
Stevens, S. S. and J. Volkmann. 1940. The relation of pitch to frequency: A revised scale. The American Journal of Psychology, 53(3):329–353. Newman. 1937. A scale for the measurement of the psychological magnitude pitch. JASA, 8:185–190.   
Stifelman, L. J., B. Arons, C. Schmandt, and E. A. Hulteen. 1993. VoiceNotes: A speech interface for a hand-held voice notetaker. INTERCHI 1993.   
Stolcke, A. 1998. Entropy-based pruning of backoff language models. Proc. DARPA Broadcast News Transcription and Understanding Workshop.   
Stolcke, A. 2002. SRILM – an extensible language modeling toolkit. ICSLP.   
Stolcke, A., Y. Konig, and M. Weintraub. 1997. Explicit word error minimization in N-best list rescoring. EUROSPEECH, volume 1.   
Stolcke, A., K. Ries, N. Coccaro, E. Shriberg, R. Bates, D. Jurafsky, P. Taylor, R. Martin, M. Meteer, and C. Van Ess-Dykema. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. Computational Linguistics, 26(3):339–371.   
Stolz, W. S., P. H. Tannenbaum, and F. V. Carstensen. 1965. A stochastic approach to the grammatical coding of English. CACM, 8(6):399–405.   
Stone, P., D. Dunphry, M. Smith, and D. Ogilvie. 1966. The General Inquirer: A Computer Approach to Content Analysis. MIT Press.   
Strotgen, J. and M. Gertz. 2013. ¨ Multilingual and cross-domain temporal tagging. Language Resources and Evaluation, 47(2):269–298.   
Strube, M. and U. Hahn. 1996. Functional centering. ACL.   
Strubell, E., A. Ganesh, and A. McCallum. 2019. Energy and policy considerations for deep learning in NLP. ACL.   
Su, Y., H. Sun, B. Sadler, M. Srivatsa, I. Gur, Z. Yan, and X. Yan. 2016. ¨ On generating characteristic-rich question sets for QA evaluation. EMNLP.   
Subba, R. and B. Di Eugenio. 2009. An effective discourse parser that uses rich linguistic information. NAACL HLT.   
Sukhbaatar, S., A. Szlam, J. Weston, and R. Fergus. 2015. End-to-end memory networks. NeurIPS.   
Sundheim, B., ed. 1991. Proceedings of MUC-3.   
Sundheim, B., ed. 1992. Proceedings of MUC-4.   
Sundheim, B., ed. 1993. Proceedings of MUC-5. Baltimore, MD.   
Sundheim, B., ed. 1995. Proceedings of MUC-6.   
Surdeanu, M. 2013. Overview of the TAC2013 Knowledge Base Population evaluation: English slot filling and temporal slot filling. TAC-13.   
Surdeanu, M., S. Harabagiu, J. Williams, and P. Aarseth. 2003. Using predicate-argument structures for information extraction. ACL.   
Surdeanu, M., T. Hicks, and M. A. Valenzuela-Escarcega. 2015. Two practical rhetorical structure theory parsers. NAACL HLT.   
Surdeanu, M., R. Johansson, A. Meyers, L. Marquez, and J. Nivre. 2008. \` The CoNLL 2008 shared task on joint parsing of syntactic and semantic dependencies. CoNLL.   
Sutskever, I., O. Vinyals, and Q. V. Le. 2014. Sequence to sequence learning with neural networks. NeurIPS.   
Suzgun, M., L. Melas-Kyriazi, and D. Jurafsky. 2023a. Follow the wisdom of the crowd: Effective text generation via minimum Bayes risk decoding. Findings of ACL 2023.   
Suzgun, M., N. Scales, N. Scharli, ¨ S. Gehrmann, Y. Tay, H. W. Chung, A. Chowdhery, Q. Le, E. Chi, D. Zhou, and J. Wei. 2023b. Challenging BIG-bench tasks and whether chain-of-thought can solve them. ACL Findings.   
Swerts, M., D. J. Litman, and J. Hirschberg. 2000. Corrections in spoken dialogue systems. ICSLP.   
Swier, R. and S. Stevenson. 2004. Unsupervised semantic role labelling. EMNLP.   
Switzer, P. 1965. Vector images in document retrieval. Statistical Association Methods For Mechanized Documentation. Symposium Proceedings. Washington, D.C., USA, March 17, 1964. https://nvlpubs.nist. gov/nistpubs/Legacy/MP/ nbsmiscellaneouspub269.pdf.   
Syrdal, A. K., C. W. Wightman, A. Conkie, Y. Stylianou, M. Beutnagel, J. Schroeter, V. Strom, and K.-S. Lee. 2000. Corpus-based techniques in the AT&T NEXTGEN synthesis system. ICSLP.   
Talmy, L. 1985. Lexicalization patterns: Semantic structure in lexical forms. In T. Shopen, ed., Language Typology and Syntactic Description, Volume 3. Cambridge University Press. Originally appeared as UC Berkeley Cognitive Science Program Report No. 30, 1980.   
Talmy, L. 1991. Path to realization: A typology of event conflation. BLS91.   
Tan, C., V. Niculae, C. DanescuNiculescu-Mizil, and L. Lee. 2016. Winning arguments: Interaction dynamics and persuasion strategies in good-faith online discussions. WWW-16.   
Tannen, D. 1979. What’s in a frame? Surface evidence for underlying expectations. In R. Freedle, ed., New Directions in Discourse Processing, 137–181. Ablex.   
Taylor, P. 2009. Text-to-Speech Synthesis. Cambridge University Press.   
Taylor, W. L. 1953. Cloze procedure: A new tool for measuring readability. Journalism Quarterly, 30:415–433.   
Teranishi, R. and N. Umeda. 1968. Use of pronouncing dictionary in speech synthesis experiments. 6th International Congress on Acoustics.   
Tesniere, L. 1959.\` El´ ements de Syntaxe´ Structurale. Librairie C. Klincksieck, Paris.   
Tetreault, J. R. 2001. A corpus-based evaluation of centering and pronoun resolution. Computational Linguistics, 27(4):507–520.   
Teufel, S., J. Carletta, and M. Moens. 1999. An annotation scheme for discourse-level argumentation in research articles. EACL.   
Teufel, S., A. Siddharthan, and C. Batchelor. 2009. Towards domain-independent argumentative zoning: Evidence from chemistry and computational linguistics. EMNLP.   
Thede, S. M. and M. P. Harper. 1999. A second-order hidden Markov model for part-of-speech tagging. ACL.   
Thompson, B. and P. Koehn. 2019. Vecalign: Improved sentence alignment in linear time and space. EMNLP.   
Thompson, K. 1968. Regular expression search algorithm. CACM, 11(6):419–422.   
Tian, Y., V. Kulkarni, B. Perozzi, and S. Skiena. 2016. On the convergent properties of word embedding methods. ArXiv preprint arXiv:1605.03956.   
Tibshirani, R. J. 1996. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical Society. Series B (Methodological), 58(1):267–288.   
Timkey, W. and M. van Schijndel. 2021. All bark and no bite: Rogue dimensions in transformer language models obscure representational quality. EMNLP.   
Titov, I. and E. Khoddam. 2014. Unsupervised induction of semantic roles within a reconstruction-error minimization framework. NAACL HLT.   
Titov, I. and A. Klementiev. 2012. A Bayesian approach to unsupervised semantic role induction. EACL.   
Tomkins, S. S. 1962. Affect, imagery, consciousness: Vol. I. The positive affects. Springer.   
Toutanova, K., D. Klein, C. D. Manning, and Y. Singer. 2003. Featurerich part-of-speech tagging with a cyclic dependency network. HLTNAACL.   
Trichelair, P., A. Emami, J. C. K. Cheung, A. Trischler, K. Suleman, and F. Diaz. 2018. On the evaluation of common-sense reasoning in natural language understanding. NeurIPS 2018 Workshop on Critiquing and Correcting Trends in Machine Learning.   
Trnka, K., D. Yarrington, J. McCaw, K. F. McCoy, and C. Pennington. 2007. The effects of word prediction on communication rate for AAC. NAACL-HLT.   
Turian, J. P., L. Shen, and I. D. Melamed. 2003. Evaluation of machine translation and its evaluation. Proceedings of MT Summit IX.   
Turian, J., L. Ratinov, and Y. Bengio. 2010. Word representations: a simple and general method for semisupervised learning. ACL.   
Turney, P. D. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. ACL.   
Turney, P. D. and M. Littman. 2003. Measuring praise and criticism: Inference of semantic orientation from association. ACM Transactions on Information Systems (TOIS), 21:315–346.   
Turney, P. D. and M. L. Littman. 2005. Corpus-based learning of analogies and semantic relations. Machine Learning, 60(1-3):251–278.   
Umeda, N. 1976. Linguistic rules for text-to-speech synthesis. Proceedings of the IEEE, 64(4):443–451.   
Umeda, N., E. Matui, T. Suzuki, and H. Omura. 1968. Synthesis of fairy tale using an analog vocal tract. 6th International Congress on Acoustics.   
Ung, M., J. Xu, and Y.-L. Boureau. 2022. SaFeRDialogues: Taking feedback gracefully after conversational safety failures. ACL.   
Uryupina, O., R. Artstein, A. Bristot, F. Cavicchio, F. Delogu, K. J. Rodriguez, and M. Poesio. 2020. Annotating a broad range of anaphoric phenomena, in a variety of genres: The ARRAU corpus. Natural Language Engineering, 26(1):1–34.   
Uszkoreit, J. 2017. Transformer: A novel neural network architecture for language understanding. Google Research blog post, Thursday August 31, 2017.   
van Deemter, K. and R. Kibble. 2000. On coreferring: coreference in MUC and related annotation schemes. Computational Linguistics, 26(4):629–637.   
van der Maaten, L. and G. E. Hinton. 2008. Visualizing high-dimensional data using t-SNE. JMLR, 9:2579– 2605.   
van Rijsbergen, C. J. 1975. Information Retrieval. Butterworths.   
Vaswani, A., N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. 2017. Attention is all you need. NeurIPS.   
Vauquois, B. 1968. A survey of formal grammars and algorithms for recognition and transformation in machine translation. IFIP Congress 1968.   
Velichko, V. M. and N. G. Zagoruyko. 1970. Automatic recognition of 200 words. International Journal of Man-Machine Studies, 2:223–234.   
Velikovich, L., S. Blair-Goldensohn, K. Hannan, and R. McDonald. 2010. The viability of web-derived polarity lexicons. NAACL HLT.   
Vendler, Z. 1967. Linguistics in Philosophy. Cornell University Press.   
Verhagen, M., R. Gaizauskas, F. Schilder, M. Hepple, J. Moszkowicz, and J. Pustejovsky. 2009. The TempEval challenge: Identifying temporal relations in text. Language Resources and Evaluation, 43(2):161–179.   
Verhagen, M., I. Mani, R. Sauri, R. Knippen, S. B. Jang, J. Littman, A. Rumshisky, J. Phillips, and J. Pustejovsky. 2005. Automating temporal annotation with TARSQI. ACL.   
Versley, Y. 2008. Vagueness and referential ambiguity in a large-scale annotated corpus. Research on Language and Computation, 6(3- 4):333–353.   
Vieira, R. and M. Poesio. 2000. An empirically based system for processing definite descriptions. Computational Linguistics, 26(4):539–593.   
Vilain, M., J. D. Burger, J. Aberdeen, D. Connolly, and L. Hirschman. 1995. A model-theoretic coreference scoring scheme. MUC-6.   
Vintsyuk, T. K. 1968. Speech discrimination by dynamic programming. Cybernetics, 4(1):52–57. Original Russian: Kibernetika 4(1):81- 88. 1968.   
Vinyals, O., Ł. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton. 2015. Grammar as a foreign language. NeurIPS.   
Voorhees, E. M. 1999. TREC-8 question answering track report. Proceedings of the 8th Text Retrieval Conference.   
Voorhees, E. M. and D. K. Harman. 2005. TREC: Experiment and Evaluation in Information Retrieval. MIT Press.   
Voutilainen, A. 1999. Handcrafted rules. In H. van Halteren, ed., Syntactic Wordclass Tagging, 217–246. Kluwer.   
Vrandeciˇ c, D. and M. Kr´ otzsch. 2014.¨ Wikidata: a free collaborative knowledge base. CACM, 57(10):78– 85.   
Wade, E., E. Shriberg, and P. J. Price. 1992. User behaviors affecting speech recognition. ICSLP.   
Wagner, R. A. and M. J. Fischer. 1974. The string-to-string correction problem. Journal of the ACM, 21:168– 173.   
Waibel, A., T. Hanazawa, G. Hinton, K. Shikano, and K. J. Lang. 1989. Phoneme recognition using time-delay neural networks. IEEE Transactions on ASSP, 37(3):328– 339.   
Walker, M. A. 2000. An application of reinforcement learning to dialogue strategy selection in a spoken dialogue system for email. JAIR, 12:387–416.   
Walker, M. A., J. C. Fromer, and S. S. Narayanan. 1998a. Learning optimal dialogue strategies: A case study of a spoken dialogue agent for email. COLING/ACL.   
Walker, M. A., M. Iida, and S. Cote. 1994. Japanese discourse and the process of centering. Computational Linguistics, 20(2):193–232.   
Walker, M. A., A. K. Joshi, and E. Prince, eds. 1998b. Centering in Discourse. Oxford University Press.   
Wang, A., A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman. 2018a. Glue: A multi-task benchmark and analysis platform for natural language understanding. ICLR.   
Wang, S. and C. D. Manning. 2012. Baselines and bigrams: Simple, good sentiment and topic classification. ACL.   
Wang, W. and B. Chang. 2016. Graphbased dependency parsing with bidirectional LSTM. ACL.   
Wang, Y., S. Li, and J. Yang. 2018b. Toward fast and accurate neural discourse segmentation. EMNLP.   
Wang, Y., S. Mishra, P. Alipoormolabashi, Y. Kordi, A. Mirzaei, A. Naik, A. Ashok, A. S. Dhanasekaran, A. Arunkumar, D. Stap, E. Pathak, G. Karamanolakis, H. Lai, I. Purohit, I. Mondal, J. Anderson, K. Kuznia, K. Doshi, K. K. Pal, M. Patel, M. Moradshahi, M. Parmar, M. Purohit, N. Varshney, P. R. Kaza, P. Verma, R. S. Puri, R. Karia, S. Doshi, S. K. Sampat, S. Mishra, S. Reddy A, S. Patro, T. Dixit, and X. Shen. 2022. SuperNaturalInstructions: Generalization via declarative instructions on $1 6 0 0 +$ NLP tasks. EMNLP.   
Wang, Y., R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous. 2017. Tacotron: Towards end-to-end speech synthesis. INTERSPEECH.   
Watanabe, S., T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. E. Y. Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai. 2018. ESPnet: End-to-end speech processing toolkit. INTERSPEECH.   
Weaver, W. 1949/1955. Translation. In W. N. Locke and A. D. Boothe, eds, Machine Translation of Languages, 15–23. MIT Press. Reprinted from a memorandum written by Weaver in 1949.   
Webber, B. L. 1978. A Formal Approach to Discourse Anaphora. Ph.D. thesis, Harvard University.   
Webber, B. L. 1983. So what can we talk about now? In M. Brady and R. C. Berwick, eds, Computational Models of Discourse, 331–371. The MIT Press.   
Webber, B. L. 1991. Structure and ostension in the interpretation of discourse deixis. Language and Cognitive Processes, 6(2):107–135.   
Webber, B. L. and B. Baldwin. 1992. Accommodating context change. ACL.   
Webber, B. L., M. Egg, and V. Kordoni. 2012. Discourse structure and language technology. Natural Language Engineering, 18(4):437–490.   
Webber, B. L. 1988. Discourse deixis: Reference to discourse segments. ACL.   
Webson, A. and E. Pavlick. 2022. Do prompt-based models really understand the meaning of their prompts? NAACL HLT.   
Webster, K., M. Recasens, V. Axelrod, and J. Baldridge. 2018. Mind the GAP: A balanced corpus of gendered ambiguous pronouns. TACL, 6:605–617.   
Wei, J., X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou, et al. 2022. Chain-ofthought prompting elicits reasoning in large language models. NeurIPS, volume 35.   
Weischedel, R., M. Meteer, R. Schwartz, L. A. Ramshaw, and J. Palmucci. 1993. Coping with ambiguity and unknown words through probabilistic models. Computational Linguistics, 19(2):359–382.   
Weizenbaum, J. 1966. ELIZA – A computer program for the study of natural language communication between man and machine. CACM, 9(1):36–45.   
Weizenbaum, J. 1976. Computer Power and Human Reason: From Judgement to Calculation. W.H. Freeman & Co.   
Werbos, P. 1974. Beyond regression: new tools for prediction and analysis in the behavioral sciences. Ph.D. thesis, Harvard University.   
Werbos, P. J. 1990. Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560.   
Weston, J., S. Chopra, and A. Bordes. 2015. Memory networks. ICLR 2015.   
Widrow, B. and M. E. Hoff. 1960. Adaptive switching circuits. IRE WESCON Convention Record, volume 4.   
Wiebe, J. 1994. Tracking point of view in narrative. Computational Linguistics, 20(2):233–287.   
Wiebe, J. 2000. Learning subjective adjectives from corpora. AAAI.   
Wiebe, J., R. F. Bruce, and T. P. O’Hara. 1999. Development and use of a gold-standard data set for subjectivity classifications. ACL.   
Wierzbicka, A. 1992. Semantics, Culture, and Cognition: University Human Concepts in Culture-Specific Configurations. Oxford University Press.   
Wierzbicka, A. 1996. Semantics: Primes and Universals. Oxford University Press.   
Wilensky, R. 1983. Planning and Understanding: A Computational Approach to Human Reasoning. Addison-Wesley.   
Wilks, Y. 1973. An artificial intelligence approach to machine translation. In R. C. Schank and K. M. Colby, eds, Computer Models of Thought and Language, 114–151. W.H. Freeman.   
Wilks, Y. 1975a. Preference semantics. In E. L. Keenan, ed., The Formal Semantics of Natural Language, 329– 350. Cambridge Univ. Press.   
Wilks, Y. 1975b. A preferential, pattern-seeking, semantics for natural language inference. Artificial Intelligence, 6(1):53–74.   
Williams, A., N. Nangia, and S. Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. NAACL HLT.   
Williams, J. D., K. Asadi, and G. Zweig. 2017. Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. ACL.   
Williams, J. D., A. Raux, and M. Henderson. 2016. The dialog state tracking challenge series: A review. Dialogue & Discourse, 7(3):4–33.   
Williams, J. D. and S. J. Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech and Language, 21(1):393–422.   
Wilson, T., J. Wiebe, and P. Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. EMNLP.   
Winograd, T. 1972. Understanding Natural Language. Academic Press.   
Winston, P. H. 1977. Artificial Intelligence. Addison Wesley.   
Wiseman, S., A. M. Rush, and S. M. Shieber. 2016. Learning global features for coreference resolution. NAACL HLT.   
Wiseman, S., A. M. Rush, S. M. Shieber, and J. Weston. 2015. Learning anaphoricity and antecedent ranking features for coreference resolution. ACL.   
Witten, I. H. and T. C. Bell. 1991. The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. IEEE Transactions on Information Theory, 37(4):1085–1094.   
Witten, I. H. and E. Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques, 2nd edition. Morgan Kaufmann.   
Wittgenstein, L. 1953. Philosophical Investigations. (Translated by Anscombe, G.E.M.). Blackwell.   
Wolf, F. and E. Gibson. 2005. Representing discourse coherence: A corpus-based analysis. Computational Linguistics, 31(2):249–287.   
Wolf, M. J., K. W. Miller, and F. S. Grodzinsky. 2017. Why we should have seen that coming: Comments on Microsoft’s Tay “experiment,” and wider implications. The ORBIT Journal, 1(2):1–12.   
Woods, W. A. 1978. Semantics and quantification in natural language question answering. In M. Yovits, ed., Advances in Computers, 2–64. Academic.   
Woods, W. A., R. M. Kaplan, and B. L. Nash-Webber. 1972. The lunar sciences natural language information system: Final report. Technical Report 2378, BBN.   
Woodsend, K. and M. Lapata. 2015. Distributed representations for unsupervised semantic role labeling. EMNLP.   
Wu, D. 1996. A polynomial-time algorithm for statistical machine translation. ACL.   
Wu, F. and D. S. Weld. 2007. Autonomously semantifying Wikipedia. CIKM-07.   
Wu, F. and D. S. Weld. 2010. Open information extraction using Wikipedia. ACL.   
Wu, L., F. Petroni, M. Josifoski, S. Riedel, and L. Zettlemoyer. 2020. Scalable zero-shot entity linking with dense entity retrieval. EMNLP.   
Wu, S. and M. Dredze. 2019. Beto, Bentz, Becas: The surprising crosslingual effectiveness of BERT. EMNLP.   
Wu, Y., M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun, Y. Cao, Q. Gao, K. Macherey, J. Klingner, A. Shah, M. Johnson, X. Liu, Ł. Kaiser, S. Gouws, Y. Kato, T. Kudo, H. Kazawa, K. Stevens, G. Kurian, N. Patil, W. Wang, C. Young, J. Smith, J. Riesa, A. Rudnick, O. Vinyals, G. S. Corrado, M. Hughes, and J. Dean. 2016. Google’s neural machine translation system: Bridging the gap between human and machine translation. ArXiv preprint arXiv:1609.08144.   
Wundt, W. 1900. Volkerpsychologie: ¨ eine Untersuchung der Entwicklungsgesetze von Sprache, Mythus, und Sitte. W. Engelmann, Leipzig. Band II: Die Sprache, Zweiter Teil.   
Xu, A., E. Pathak, E. Wallace, S. Gururangan, M. Sap, and D. Klein. 2021. Detoxifying language models risks marginalizing minority voices. NAACL HLT.   
Xu, J., D. Ju, M. Li, Y.-L. Boureau, J. Weston, and E. Dinan. 2020. Recipes for safety in opendomain chatbots. ArXiv preprint arXiv:2010.07079.   
Xu, P., H. Saghir, J. S. Kang, T. Long, A. J. Bose, Y. Cao, and J. C. K. Cheung. 2019. A cross-domain transferable neural coherence model. ACL.   
Xue, N., H. T. Ng, S. Pradhan, A. Rutherford, B. L. Webber, C. Wang, and H. Wang. 2016. CoNLL 2016 shared task on multilingual shallow discourse parsing. CoNLL-16 shared task.   
Xue, N. and M. Palmer. 2004. Calibrating features for semantic role labeling. EMNLP.   
Yamada, H. and Y. Matsumoto. 2003. Statistical dependency analysis with support vector machines. IWPT-03.   
Yang, D., J. Chen, Z. Yang, D. Jurafsky, and E. H. Hovy. 2019. Let’s make your request more persuasive: Modeling persuasive strategies via semisupervised neural nets on crowdfunding platforms. NAACL HLT.   
Yang, X., G. Zhou, J. Su, and C. L. Tan. 2003. Coreference resolution using competition learning approach. ACL.   
Yang, Y. and J. Pedersen. 1997. A comparative study on feature selection in text categorization. ICML.   
Yankelovich, N., G.-A. Levow, and M. Marx. 1995. Designing SpeechActs: Issues in speech user interfaces. CHI-95.   
Yih, W.-t., M. Richardson, C. Meek, M.-W. Chang, and J. Suh. 2016. The value of semantic parse labeling for knowledge base question answering. ACL.   
Young, S. J., M. Gasiˇ c, S. Keizer, ´ F. Mairesse, J. Schatzmann, B. Thomson, and K. Yu. 2010. The Hidden Information State model: A practical framework for POMDPbased spoken dialogue management. Computer Speech & Language, 24(2):150–174.   
Younger, D. H. 1967. Recognition and parsing of context-free languages in time $n ^ { 3 }$ . Information and Control, 10:189–208.   
Yu, N., M. Zhang, and G. Fu. 2018. Transition-based neural RST parsing with implicit syntax features. COLING.   
Yu, Y., Y. Zhu, Y. Liu, Y. Liu, S. Peng, M. Gong, and A. Zeldes. 2019. GumDrop at the DISRPT2019 shared task: A model stacking approach to discourse unit segmentation and connective detection. Workshop on Discourse Relation Parsing and Treebanking 2019.   
Zapirain, B., E. Agirre, L. Marquez, \` and M. Surdeanu. 2013. Selectional preferences for semantic role classification. Computational Linguistics, 39(3):631–663.   
Zelle, J. M. and R. J. Mooney. 1996. Learning to parse database queries using inductive logic programming. AAAI.   
Zeman, D. 2008. Reusable tagset conversion using tagset drivers. LREC.   
Zens, R. and H. Ney. 2007. Efficient phrase-table representation for maonline MT and speech translation. NAACL-HLT.   
Zettlemoyer, L. and M. Collins. 2005. Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars. Uncertainty in Artificial Intelligence, UAI’05.   
Zettlemoyer, L. and M. Collins. 2007. Online learning of relaxed CCG grammars for parsing to logical form. EMNLP/CoNLL.   
Zhang, H., R. Sproat, A. H. Ng, F. Stahlberg, X. Peng, K. Gorman, and B. Roark. 2019. Neural models of text normalization for speech applications. Computational Linguistics, 45(2):293–337.   
Zhang, R., C. N. dos Santos, M. Yasunaga, B. Xiang, and D. Radev. 2018. Neural coreference resolution with deep biaffine attention by joint mention detection and mention clustering. ACL.   
Zhang, T., V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi. 2020. BERTscore: Evaluating text generation with BERT. ICLR 2020.   
Zhang, Y., V. Zhong, D. Chen, G. Angeli, and C. D. Manning. 2017. Position-aware attention and supervised data improve slot filling. EMNLP.   
Zhao, H., W. Chen, C. Kit, and G. Zhou. 2009. Multilingual dependency learning: A huge feature engineering method to semantic dependency parsing. CoNLL.   
Zhao, J., T. Wang, M. Yatskar, R. Cotterell, V. Ordonez, and K.-W. Chang. 2019. Gender bias in contextualized word embeddings. NAACL HLT.   
Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang. 2017. Men also like shopping: Reducing gender bias amplification using corpus-level constraints. EMNLP.   
Zhao, J., T. Wang, M. Yatskar, V. Ordonez, and K.-W. Chang. 2018a. Gender bias in coreference resolution: Evaluation and debiasing methods. NAACL HLT.   
Zhao, J., Y. Zhou, Z. Li, W. Wang, and K.-W. Chang. 2018b. Learning gender-neutral word embeddings. EMNLP.   
Zheng, J., L. Vilnis, S. Singh, J. D. Choi, and A. McCallum. 2013. Dynamic knowledge-base alignment for coreference resolution. CoNLL.   
Zhou, D., O. Bousquet, T. N. Lal, J. Weston, and B. Scholkopf. 2004a. ¨ Learning with local and global consistency. NeurIPS.   
Zhou, G., J. Su, J. Zhang, and M. Zhang. 2005. Exploring various knowledge in relation extrac  
Zhou, J. and W. Xu. 2015a. End-toend learning of semantic role labeling using recurrent neural networks. ACL.   
Zhou, J. and W. Xu. 2015b. End-toend learning of semantic role labeling using recurrent neural networks. ACL.   
Zhou, K., K. Ethayarajh, D. Card, and D. Jurafsky. 2022. Problems with cosine as a measure of embedding similarity for high frequency words. ACL.   
Zhou, K., J. Hwang, X. Ren, and M. Sap. 2024. Relying on the unreliable: The impact of language models’ reluctance to express uncertainty. ACL.   
Zhou, L., M. Ticrea, and E. H. Hovy. 2004b. Multi-document biography summarization. EMNLP.   
Zhou, Y., A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. 2023. Large language models are human-level prompt engineers. The Eleventh International Conference on Learning Representations.   
Zhou, Y. and N. Xue. 2015. The Chinese Discourse TreeBank: a Chinese corpus annotated with discourse relations. Language Resources and Evaluation, 49(2):397–431.   
Zhu, X. and Z. Ghahramani. 2002. Learning from labeled and unlabeled data with label propagation. Technical Report CMU-CALD-02, CMU.   
Zhu, X., Z. Ghahramani, and J. Lafferty. 2003. Semi-supervised learning using gaussian fields and harmonic functions. ICML.   
Zhu, Y., R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. IEEE International Conference on Computer Vision.   
Ziemski, M., M. Junczys-Dowmunt, and B. Pouliquen. 2016. The United Nations parallel corpus v1.0. LREC.

# Subject Index

\*?, 9   
+?, 9   
.wav format, 336   
10-fold cross-validation, 69   
(derives), 389   
→ˆ, 58   
\* (RE Kleene \*), 7   
$^ +$ (RE Kleene +), 7   
. (RE any character), 7   
\$ (RE end-of-line), 8   
( (RE precedence symbol), 8   
[ (RE character disjunction), 6   
\B (RE non word-boundary), 8   
\b (RE word-boundary), 8   
] (RE character disjunction), 6   
ˆ (RE start-of-line), 8   
[ˆ] (single-char negation), 6   
4-gram, 38   
4-tuple, 392   
5-gram, 38   
A-D conversion, 335   
AAC, 32   
AAE, 15   
AB test, 353   
ablating, 248   
absolute position, 198   
absolute temporal expression, 452   
abstract word, 485   
accessible, 506   
accessing a referent, 501   
accomplishment expressions, 450   
accuracy, 366   
achievement expressions, 450   
acknowledgment speech act, 312   
activation, 133   
activity expressions, 450   
acute-eval, 325   
ad hoc retrieval, 291   
add gate, 172   
add-k, 47   
add-one smoothing, 46   
adequacy, 280   
adjacency pairs, 313   
Adjectives, 364   
adverb, 364 degree, 364 directional, 364 locative, 364 manner, 364 temporal, 364   
Adverbs, 364   
AED, 339   
affective, 481   
affix, 24   
agent, as thematic role, 462   
agglutinative language, 267   
AIFF file, 336   
AISHELL-1, 334   
aktionsart, 450   
ALGOL, 409   
algorithm byte-pair encoding, 22 CKY, 397 minimum edit distance, 28 naive Bayes classifier, 57 pointwise mutual information, 114 semantic role labeling, 469 TextTiling, 544 Viterbi, 373   
aligned, 249   
alignment, 25, 342 in ASR, 346 minimum cost, 27 string, 25 via minimum edit distance, 27   
Allen relations, 448   
allocational harm, 126   
ambiguity amount of part-of-speech in Brown corpus, 366 attachment, 396 coordination, 396 of referring expressions, 503 part-of-speech, 365 resolution of tag, 366   
American Structuralism, 408   
anaphor, 502   
anaphora, 502   
anaphoricity detector, 511   
anchor texts, 520   
anchors in regular expressions, 8, 29   
anisotropy, 234   
antecedent, 502   
Apple AIFF, 336   
approximate randomization, 71   
arc eager, 423   
arc standard, 417   
argmax, 58   
argumentation mining, 547   
argumentation schemes, 548   
argumentative relations, 547   
argumentative zoning, 549   
Aristotle, 362, 450   
ARPA, 355   
article (part-of-speech), 364   
articulatory synthesis, 357   
aspect, 450   
ASR, 331 confidence, 320   
association, 103   
ATIS corpus, 390   
ATN, 478   
ATRANS, 477   
attachment ambiguity, 396   
attention cross-attention, 272 encoder-decoder, 272 history in transformers, 202   
attention head, 188   
attention mechanism, 179   
Attribution (as coherence relation), 534   
augmentative communication, 32   
authorship attribution, 56   
autoregressive generation, 167, 207   
Auxiliary, 365   
${ \mathbf B } ^ { 3 }$ , 524   
Babbage, C., 332   
backoff, 49 in smoothing, 48   
backprop, 147   
backpropagation through time, 161   
backtrace in minimum edit distance, 29   
backtranslation, 279   
Backus-Naur form, 388   
backward-looking center, 541   
bag of words, 58, 59 in IR, 291   
bakeoff, 355 speech recognition competition, 355   
barged in, 326   
base model, 249   
basic emotions, 482   
batch training, 94   
Bayes’ rule, 58 dropping denominator, 59, 372   
Bayesian inference, 58   
BDI, 329   
beam search, 275, 424   
beam width, 275, 424   
Berkeley Restaurant Project, 36   
Bernoulli naive Bayes, 75   
BERT for affect, 497   
best-worst scaling, 486   
bias amplification, 126   
bias term, 79, 133   
bidirectional RNN, 170   
bigram, 34   
binary branching, 394   
binary naive Bayes, 63   
binary tree, 394   
BIO, 238, 368   
BIO tagging, 238 for NER, 238, 368   
BIOES, 238, 368   
bitext, 270   
bits for measuring entropy, 49   
blank in CTC, 342   
BM25, 291, 293   
BNF (Backus-Naur form), 388   
bootstrap, 73   
bootstrap algorithm, 73   
bootstrap test, 71   
bootstrapping, 71 in IE, 441   
bound pronoun, 504   
BPE, 21   
BPE, 22   
bracketed notation, 391   
bridging inference, 506   
broadcast news speech recognition of, 355   
Brown corpus, 13 original tagging of, 384   
byte-pair encoding, 21   
calibrated, 290   
CALLHOME, 333   
Candide, 287   
Cantonese, 267   
capture group, 12   
cascade regular expression in ELIZA, 12   
case sensitivity in regular expression search, 6   
case folding, 23   
case frame, 463, 478   
CAT, 263   
cataphora, 504   
CD (conceptual dependency), 477   
Centering Theory, 532, 540   
centroid, 117   
cepstrum history, 355   
CFG, see context-free grammar   
chain rule, 99, 148   
chain-of-thought, 254   
channels in stored waveforms, 336   
chart parsing, 397   
Chatbots, 309, 321   
chatbots, 4   
CHiME, 333   
Chinese as verb-framed language, 267 words for brother, 266   
Chomsky normal form, 394   
Chomsky-adjunction, 395   
chrF, 281   
CIRCUS, 459   
citation form, 102   
Citizen Kane, 531   
CKY algorithm, 387   
claims, 547   
class-based n-gram, 53   
classifier head, 235   
clefts, 507   
clitic, 19 origin of term, 362   
closed book, 304   
closed class, 363   
cloze task, 226   
cluster, 502   
CNF, see Chomsky normal form   
Cocke-Kasami-Younger algorithm, see CKY   
code switching, 15   
coherence, 531 entity-based, 540 relations, 533   
cohesion lexical, 532, 544   
ColBERT, 300   
cold languages, 268   
collection in IR, 291   
commissive speech act, 312   
common crawl, 211   
common ground, 312, 328   
Common nouns, 363   
complementizers, 364   
componential analysis, 476   
compression, 335   
Computational Grammar Coder (CGC), 384   
concatenation, 5, 29   
conceptual dependency, 477   
concrete word, 485   
conditional generation, 204   
conditional random field, 376   
confidence, 285 ASR, 320 in relation extraction, 442   
confidence values, 442   
configuration, 417   
confusion matrix, 66   
Conjunctions, 364   
connectionist, 157   
connotation frame, 497   
connotation frames, 479   
connotations, 104, 482   
constative speech act, 312   
constituency, 388   
constituent, 388 titles which are not, 387   
Constraint Grammar, 433   
content planning, 319   
context embedding, 122   
context-free grammar, 388, 392, 407

Chomsky normal form, 394 invention of, 409 non-terminal symbol, 389 productions, 388 rules, 388 terminal symbol, 389 weak and strong equivalence, 394 contextual embeddings, 186, 231 continued pretraining, 214 conversation, 309 conversation analysis, 313, 328 conversational implicature, 314 conversational speech, 333 convex, 90 coordination ambiguity, 396 copula, 365 CORAAL, 333 corefer, 501 coreference chain, 502 coreference resolution, 502 gender agreement, 508 Hobbs tree search algorithm, 528 number agreement, 507 person agreement, 508 recency preferences, 508 selectional restrictions, 509 syntactic (“binding”) constraints, 508 verb semantics, 509 corpora, 13 corpus, 13 ATIS, 390 Broadcast news, 355 Brown, 13, 384 fisher, 355 LOB, 384 regular expression searching inside, 5 Switchboard, 13, 333, 335 TimeBank, 451 Wall Street Journal, 355 correction act detection, 319 cosine as a similarity metric, 110 cost function, 88 count nouns, 363 counters, 29 counts treating low as zero, 379 CRF, 376 compared to HMM, 376 inference, 380 Viterbi inference, 380 CRFs learning, 381 cross-attention, 272 cross-brackets, 406

cross-entropy, 51   
cross-entropy loss, 88, 145   
cross-validation, 69 10-fold, 69   
crowdsourcing, 485   
CTC, 341   
datasheet, 16   
dative alternation, 463   
debiasing, 127   
decision boundary, 80, 136   
decoder-only model, 201   
decoding, 207, 372 Viterbi, 372   
deep neural networks, 132   
deep learning, 132   
definite reference, 504   
degree adverb, 364   
delexicalize, 320   
demonstrations, 246   
denoising, 226   
dependency grammar, 411   
dependency tree, 414   
dependent, 412   
derivation direct (in a formal language), 392 syntactic, 389, 389, 392, 392   
Det, 388   
determiner, 364, 388   
Determiners, 364   
development set, 38   
development test set, 69   
development test set (dev-test), 39   
devset, see development test set (dev-test), 69   
DFT, 338   
dialogue, 309   
dialogue act correction, 319   
Dialogue acts, 318   
dialogue policy, 319   
dialogue systems, 309 design, 325   
diathesis alternation, 463   
diff program, 30   
digit recognition, 332   
digital divide, 263   
digitization, 335   
dilated convolutions, 352   
dimension, 107   
diphthong origin of term, 362   
direct derivation (in a formal language), 392   
directional adverb, 364   
directive speech act, 312   
disambiguation in parsing, 403 syntactic, 397   
discount, 47, 49   
discounting, 45   
discourse, 531

segment, 534 discourse connectives, 535 discourse deixis, 503 discourse model, 501 discourse parsing, 536 discourse-new, 505 discourse-old, 505 discovery procedure, 408 discrete Fourier transform, 338 discriminative model, 78 disfluency, 13 disjunction, 29 pipe in regular expressions as, 8 square braces in regular expression as, 6 dispreferred response, 330 distant supervision, 443 distributional hypothesis, 101 distributional similarity, 408 divergences between languages in MT, 265 document in IR, 291 document frequency, 112 document vector, 117 domination in syntax, 389 dot product, 79, 110 dot-product attention, 180 Dragon Systems, 355 dropout, 151 duration temporal expression, 452 dynamic programming, 26 and parsing, 397 Viterbi as, 373 dynamic time warping, 355 edge-factored, 426 edit distance minimum algorithm, 26 EDU, 534 effect size, 70 efficiency costs, 317 Elaboration (as coherence relation), 533 ELIZA, 4 implementation, 12 sample conversation, 12 Elman Networks, 158 ELMo for affect, 497 EM for deleted interpolation, 48 embedding layer, 154 embeddings, 105 cosine for similarity, 110 skip-gram, learning, 120 sparse, 110 tf-idf, 112 word2vec, 117 emission probabilities, 370 EmoLex, 484

emotion, 482   
Encoder-decoder, 175   
encoder-decoder attention, 272   
end-to-end training, 166   
endpointing, 312   
English lexical differences from French, 267 simplified grammar rules, 390 verb-framed, 267   
entity dictionary, 379   
entity grid, 542   
Entity linking, 520   
entity linking, 502   
entity-based coherence, 540   
entropy, 49 and perplexity, 49 cross-entropy, 51 per-word, 50 rate, 50 relative, 474   
error backpropagation, 147   
ESPnet, 356   
ethos, 547   
Euclidean distance in L2 regularization, 96   
Eugene Onegin, 52   
Euler’s formula, 338   
Europarl, 270   
evalb, 406   
evaluating parsers, 405   
evaluation 10-fold cross-validation, 69 AB test, 353 comparing models, 41 cross-validation, 69 development test set, 39, 69 devset, 69 devset or development test set, 39 extrinsic, 38 fluency in MT, 280 Matched-Pair Sentence Segment Word Error (MAPSSWE), 347 mean opinion score, 353 most frequent class baseline, 366 MT, 280 named entity recognition, 240, 381 of n-gram, 38 of n-grams via perplexity, 40 pseudoword, 476 relation extraction, 446 test set, 39 training on the test set, 39 training set, 39 TTS, 353   
event coreference, 503   
event extraction, 435, 446   
events, 450

F (for F-measure), 67   
F-measure, 67   
$F$ -measure   
in NER, 240, 381   
factoid question, 289   
Faiss, 301   
false negatives, 9   
false positives, 9   
Farsi, verb-framed, 267   
fast Fourier transform, 338,   
355   
fasttext, 123   
FASTUS, 457   
feature cutoff, 379   
feature interactions, 82   
feature selection   
information gain, 76   
feature template, 421   
feature templates, 82   
part-of-speech tagging,   
378   
feature vectors, 334   
Federalist papers, 75   
feedforward network, 138   
fenceposts, 398   
few-shot, 246   
FFT, 338, 355   
file format, .wav, 336   
filled pause, 13   
filler, 13   
finetuning, 213, 235   
finetuning;supervsed, 249   
first-order co-occurrence,   
124   
fluency, 280   
in MT, 280   
fold (in cross-validation),   
69   
forget gate, 172   
formal language, 391   
formant synthesis, 357   
forward inference, 153   
forward-looking centers,   
541   
Fosler, E., see   
Fosler-Lussier, E.   
foundation model, 222   
fragment of word, 13   
frame, 336   
semantic, 467   
frame elements, 467   
FrameNet, 466   
frames, 314   
free word order, 411   
Freebase, 437   
freeze, 155, 214   
French, 265   
Frump, 459   
Evidence (as coherence   
relation), 533   
evoking a referent, 501   
execution accuracy, 256   
expansion, 390, 391   
expletive, 507   
explicit confirmation, 319   
extraposition, 507   
extrinsic evaluation, 38

Gaussian prior on weights, 96   
gazetteer, 379   
General Inquirer, 64, 484   
generalize, 95   
generalized semantic role, 464   
generation of sentences to test a CFG grammar, 390   
generative AI, 204   
generative grammar, 391   
generative model, 78   
generative models, 59   
generator, 389   
generics, 507   
German, 265   
given-new, 506   
Godzilla, speaker as, 472   
gold labels, 66   
gradient, 90   
Grammar Constraint, 433 Head-Driven Phrase Structure (HPSG), 406 Link, 433   
grammar binary branching, 394 checking, 387 equivalence, 394 generative, 391 inversion transduction, 287   
grammatical function, 412   
grammatical relation, 412   
grammatical sentences, 391   
greedy decoding, 206   
greedy RE patterns, 9   
grep, 5, 5, 30   
Gricean maxims, 314   
grounding, 312   
GUS, 314   
hallucinate, 290   
hallucination, 219   
Hamilton, Alexander, 75   
Hamming, 337   
Hansard, 287   
hanzi, 19   
harmonic mean, 67   
head, 188, 199, 406, 412 finding, 406   
Head-Driven Phrase Structure Grammar (HPSG), 406   
Heaps’ Law, 14   
Hearst patterns, 438   
held-out, 48   
Herdan’s Law, 14   
hidden, 370   
hidden layer, 138 as representation of input, 139

fully-connected, 138   
function word, 363, 383   
fusion language, 267

hidden units, 138   
Hindi, 265   
Hindi, verb-framed, 267   
HKUST, 334   
HMM, 370 formal definition of, 370 history in speech recognition, 355 initial distribution, 370 observation likelihood, 370 observations, 370 simplifying assumptions for POS tagging, 372 states, 370 transition probabilities, 370   
Hobbs algorithm, 528   
Hobbs tree search algorithm for pronoun resolution, 528   
homonymy, 232   
hot languages, 268   
Hungarian part-of-speech tagging, 382   
hybrid, 356   
hyperarticulation, 319   
hypernym, 437 lexico-syntactic patterns for, 438   
hyperparameter, 92   
hyperparameters, 152   
IBM Models, 287   
IBM Thomas J. Watson Research Center, 53, 355   
idf, 113   
idf term weighting, 113, 292   
immediately dominates, 389   
implicature, 314   
implicit argument, 479   
implicit confirmation, 320   
in-context learning, 247   
indefinite reference, 504   
induction heads, 247   
inference-based learning, 429   
infoboxes, 437   
information structure, 505   
status, 505   
information extraction (IE), 435 bootstrapping, 441   
information gain, 76 for feature selection, 76   
Information retrieval, 108, 290   
information retrieval, 290   
initiative, 313   
inner product, 110   
instance, word, 14   
Institutional Review Board, 327   
Instruction tuning, 249   
intent determination, 316   
intercept, 79   
Interjections, 364   
interpolated precision, 296   
interpolation in smoothing, 48   
interpretable, 98   
interval algebra, 448   
intrinsic evaluation, 38   
inversion transduction grammar (ITG), 287   
inverted index, 295   
IO, 238, 368   
IOB tagging for temporal expressions, 453   
IR, 290 idf term weighting, 113, 292 term weighting, 291 vector space model, 107   
IRB, 327   
is-a, 437   
ISO 8601, 454   
isolating language, 267   
iSRL, 479   
ITG (inversion transduction grammar), 287   
Japanese, 265, 267   
Jay, John, 75   
joint intention, 328   
Kaldi, 356   
KBP, 459   
KenLM, 38, 53   
key, 188   
KL divergence, 474   
Klatt formant synthesizer, 357   
Kleene \*, 7 sneakiness of matching zero things, 7   
Kleene $^ +$ , 7   
knowledge claim, 549   
knowledge graphs, 435   
Kullback-Leibler divergence, 474   
KV cache, 217   
L1 regularization, 96   
L2 regularization, 96   
labeled precision, 405   
labeled recall, 405   
language identification, 354 universal, 264   
language id, 56   
language model, 32   
language model:coined by, 53   
language modeling head, 199   
Laplace smoothing, 45 for PMI, 116   
lasso regression, 96   
latent semantic analysis, 130   
layer norm, 192   
LDC, 19   
learning rate, 91   
lemma, 15, 102 versus wordform, 15   
Lemmatization, 23   
lemmatization, 5   
Levenshtein distance, 25   
lexical category, 389 cohesion, 532, 544 gap, 267 semantics, 102 trigger, in IE, 452   
lexico-syntactic pattern, 438   
lexicon, 388   
LibriSpeech, 333   
light verbs, 447   
likelihood, 59   
linear chain CRF, 376, 377   
linear classifiers, 60   
linear interpolation for n-grams, 48   
linearly separable, 136   
Linguistic Data Consortium, 19   
Linguistic Discourse model, 550   
Link Grammar, 433   
List (as coherence relation), 534   
listen attend and spell, 339   
LIWC, 64, 485   
LM, 32   
LOB corpus, 384   
localization, 263   
location-based attention, 351   
locative, 364   
locative adverb, 364   
log why used for probabilities, 37 why used to compress speech, 336   
log likelihood ratio, 493   
log odds ratio, 493   
log probabilities, 37, 37   
logistic function, 79   
logistic regression, 77 conditional maximum likelihood estimation, 88 Gaussian priors, 96 learning in, 87 regularization, 96 relation to neural networks, 140   
logit, 80, 200   
logit lens, 200   
logos, 547   
long short-term memory, 172   
lookahead in regex, 13   
LoRA, 218   
loss, 88   
low frame rate, 340   
LPC (Linear Predictive Coding), 355   
LSI, see latent semantic analysis   
LSTM, 385   
LUNAR, 307   
machine learning for NER, 382 textbooks, 75, 100   
machine translation, 263   
macroaveraging, 68   
Madison, James, 75   
MAE, 15   
Mandarin, 265   
Manhattan distance in L1 regularization, 96   
manner adverb, 364   
Markov, 34 assumption, 34   
Markov assumption, 369   
Markov chain, 52, 369 formal definition of, 370 initial distribution, 370 n-gram as, 369 states, 370 transition probabilities, 370   
Markov model, 34 formal definition of, 370 history, 53   
Marx, G., 387   
Masked Language Modeling, 226   
mass nouns, 363   
maxent, 100   
maxim, Gricean, 314   
maximum entropy, 100   
maximum spanning tree, 426   
Mayan, 267   
MBR, 277   
McNemar’s test, 348   
mean element-wise, 167   
mean average precision, 297   
mean opinion score, 353   
mean reciprocal rank, 305   
mechanical indexing, 129   
Mechanical Turk, 332   
mel, 338   
memory networks, 202   
mention detection, 510   
mention-pair, 513   
mentions, 501   
MERT, for training in MT, 287   
MeSH (Medical Subject Headings), 57   
Message Understanding Conference, 457   
METEOR, 288   
metonymy, 530   
microaveraging, 68   
Microsoft .wav format, 336   
mini-batch, 94   
Minimum Bayes risk, 277   
minimum edit distance, 25, 25, 373 example of, 28 for speech recognition evaluation, 346   
MINIMUM EDIT DISTANCE, 28   
minimum edit distance algorithm, 26   
Minimum Error Rate Training, 287   
MLE for n-grams, 35 for n-grams, intuition, 36   
MLM, 226   
MLP, 138   
MMLU, 258, 304   
modal verb, 365   
model alignment, 249   
model card, 74   
morpheme, 23   
MOS (mean opinion score), 353   
Moses, Michelangelo statue of, 309   
Moses, MT toolkit, 287   
MRR, 305   
MS MARCO, 303   
MT, 263 divergences, 265 post-editing, 263   
mu-law, 336   
MUC, 457, 459   
MUC F-measure, 524   
multi-head attention, 189   
multi-hop, 303   
multi-layer perceptrons, 138   
multinomial logistic regression, 84   
multinomial naive Bayes, 57   
multinomial naive Bayes classifier, 57   
multiword expressions, 130   
MWE, 130   
n-best list, 341   
n-gram, 32, 34 add-one smoothing, 45 as approximation, 34 as generators, 43 as Markov chain, 369 equation for, 35 example of, 36, 37 for Shakespeare, 43 history of, 53 interpolation, 48 KenLM, 38, 53 logprobs in, 37 normalizing, 36 parameter estimation, 35 sensitivity to corpus, 43 smoothing, 45

test set, 38 training set, 38 naive Bayes multinomial, 57 simplifying assumptions, 59 naive Bayes assumption, 59 naive Bayes classifier use in text categorization, 57 named entity, 237, 362, 367 list of types, 238, 367 named entity recognition, 237, 367 natural language inference, 237 Natural Questions, 303 negative log likelihood loss, 88, 97, 146 NER, 237, 367 neural networks relation to logistic regression, 140 newline character, 10 Next Sentence Prediction, 228 NIST for MT evaluation, 288 noisy-or, 442 NomBank, 466 Nominal, 388 non-capturing group, 12 non-greedy, 9 non-standard words, 349 non-stationary process, 336 non-terminal symbols, 389, 390 normal form, 394, 394 normalization temporal, 453 word, 23 normalization of probabilities, 35 normalize, 83 normalizing, 140 noun abstract, 363 common, 363 count, 363 mass, 363 proper, 363 noun phrase, 388 constituents, 388 Nouns, 363 NP, 388, 390 nucleus, 533 null hypothesis, 70 Nyquist frequency, 335

observation likelihood role in Viterbi, 374 one-hot vector, 153, 197 open book, 304 open class, 363 open information extraction, 444 operation list, 25

operator precedence, 8, 9   
optionality use of ? in regular expressions for, 6   
output gate, 173   
overfitting, 95   
p-value, 71   
Paired, 71   
parallel corpus, 270   
parallel distributed processing, 157   
parallelogram model, 124   
parameter-efficient fine tuning, 217   
parse tree, 389, 391   
PARSEVAL, 405   
parsing ambiguity, 395 CKY, 397 CYK, see CKY evaluation, 405 relation to grammars, 392 syntactic, 387 well-formed substring table, 409   
part of speech as used in CFG, 389   
part-of-speech adjective, 364 adverb, 364 closed class, 363 interjection, 364 noun, 363 open class, 363 particle, 364 subtle distinction between verb and noun, 364 verb, 364   
part-of-speech tagger PARTS, 384 TAGGIT, 384   
Part-of-speech tagging, 365   
part-of-speech tagging ambiguity and, 365 amount of ambiguity in Brown corpus, 366 and morphological analysis, 382 feature templates, 378 history of, 384 Hungarian, 382 Turkish, 382 unknown words, 376   
particle, 364   
PARTS tagger, 384   
parts of speech, 362   
pathos, 547   
pattern, regular expression, 5   
PCM (Pulse Code Modulation), 336   
PDP, 157   
PDTB, 535   
PEFT, 217   
Penn Discourse TreeBank, 535   
Penn Treebank, 393 tagset, 365, 365   
Penn Treebank tokenization, 19   
per-word entropy, 50   
perceptron, 135   
period disambiguation, 82   
perplexity, 40, 52 as weighted average branching factor, 41 defined via cross-entropy, 52   
perplexity:coined by, 53   
personal pronoun, 364   
persuasion, 548   
phrasal verb, 364   
phrase-based translation, 287   
phrase-structure grammar, 388   
PII, 212   
pipe, 8   
planning and speech acts, 329 shared plans, 328   
pleonastic, 507   
Pointwise mutual information, 114   
polysynthetic language, 267   
pooling, 143, 166 max, 167 mean, 166   
Porter stemmer, 24   
POS, 362   
position embeddings relative, 199   
positional embeddings, 198   
possessive pronoun, 364   
post-editing, 263   
post-training, 249   
postings, 295   
postposition, 265   
Potts diagram, 492   
PP, 390   
PP-attachment ambiguity, 396   
PPMI, 115   
precedence, 8   
precedence, operator, 8   
Precision, 67   
precision for MT evaluation, 288 in NER, 240, 381   
precision-recall curve, 296   
premises, 547   
prepositional phrase constituency, 390   
prepositions, 364   
presequences, 313   
pretraining, 145, 203   
primitive decomposition, 476   
principle of contrast, 103   
prior probability, 59   
pro-drop languages, 268   
probabilistic context-free grammars, 409   
productions, 388   
projective, 414   
prompt, 243   
prompt engineering, 243   
pronoun, 364 bound, 504 demonstrative, 505 non-binary, 508 personal, 364 possessive, 364 wh-, 364   
PropBank, 465   
proper noun, 363   
PROTO-AGENT, 464   
PROTO-PATIENT, 464   
pseudoword, 476   
PTRANS, 477   
punctuation for numbers cross-linguistically, 19 for sentence segmentation, 24 tokenization, 19 treated as words, 13 treated as words in LM, 44   
QA, 289   
quantization, 335   
query, 188, 291 in IR, 291   
question factoid, 289   
question answering factoid questions, 289   
Radio Rex, 331   
RAG, 290, 302   
random sampling, 208   
range, regular expression, 6   
ranking, 281   
rarefaction, 335   
RDF, 437   
RDF triple, 437   
Read speech, 333   
reading comprehension, 304   
Reason (as coherence relation), 533   
Recall, 67   
recall for MT evaluation, 288 in NER, 240, 381   
rectangular, 336   
reference bound pronouns, 504 cataphora, 504 definite, 504 generics, 507 indefinite, 504   
reference point, 449   
referent, 501 accessing of, 501 evoking of, 501   
referential density, 268   
reflexive, 508   
regex regular expression, 5   
register in regex, 12   
regression lasso, 96 ridge, 96   
regular expression, 5, 29 substitutions, 11   
regularization, 95   
rejection conversation act, 320   
relatedness, 103   
relation extraction, 435   
relative temporal expression, 452   
relative entropy, 474   
relative frequency, 36   
relevance, 314   
relexicalize, 321   
ReLU, 134   
reporting events, 447   
representation learning, 101   
representational harm, 127   
representational harms, 73   
rescore, 341   
residual stream, 191   
resolve, 366   
Resource Management, 355   
retrieval-augmented generation, 302   
ReVerb, 445   
rewrite, 389   
Rhetorical Structure Theory, see RST   
Riau Indonesian, 364   
ridge regression, 96   
RLHF, 325   
RNN-T, 345   
role-filler extraction, 457   
Rosebud, sled named, 531   
row vector, 108   
RST, 533 TreeBank, 535, 550   
rules context-free, 388 context-free, expansion, 389 context-free, sample, 390   
Russian fusion language, 267 verb-framed, 267   
S as start symbol in CFG, 390   
salience, in discourse model, 506   
Sampling, 42   
sampling of analog waveform, 335 rate, 335   
satellite, 267, 533   
satellite-framed language, 267   
saturated, 135   
scaling laws, 216   
SCISOR, 459   
sclite, 347   
sclite package, 30   
script Schankian, 467   
scripts, 456   
SDRT (Segmented Discourse Representation Theory), 550   
search engine, 290   
search tree, 274   
second-order co-occurrence, 124   
seed pattern in IE, 441   
seed tuples, 441   
segmentation sentence, 24 word, 18   
selectional association, 475   
selectional preference strength, 474   
selectional preferences pseudowords for evaluation, 476   
selectional restriction, 472 representing with events, 473 violations in WSD, 474   
self-supervision, 118, 163, 210   
self-training, 155   
semantic drift in IE, 442   
semantic feature, 130   
semantic field, 103   
semantic frame, 104   
semantic relations in IE, 436 table, 437   
semantic role, 462, 462, 464   
Semantic role labeling, 468   
semantics lexical, 102   
sense word, 232   
sentence error rate, 347 segmentation, 24   
sentence realization, 320   
sentence segmentation, 5   
sentence separation, 176   
SentencePiece, 270   
sentiment, 104 origin of term, 500   
sentiment analysis, 56   
sentiment lexicons, 64   
SentiWordNet, 490   
sequence labeling, 362   
SFT, 249   
SGNS, 117   
Shakespeare n-gram approximations to, 43   
shallow discourse parsing, 539   
shared plans, 328   
side sequence, 313   
sigmoid, 79, 133   
significance test MAPSSWE for ASR, 347 McNemar’s, 348   
similarity, 103 cosine, 110   
singleton, 502   
singular they, 508   
skip-gram, 117   
slot error rate, 317   
slot filling, 316, 459   
slots, 314   
smoothing, 45, 45 add-one, 45 interpolation, 48 Laplace, 45 linear interpolation, 48   
softmax, 85, 140   
SOV language, 265   
spam detection, 56, 64   
span, 403   
Speaker diarization, 353   
speaker identification, 354   
speaker recognition, 354   
speaker verification, 354   
speech telephone bandwidth, 335   
speech acts, 312   
speech recognition architecture, 332, 339 history of, 354   
speech synthesis, 332   
split-half reliability, 487   
SRILM, 53   
SRL, 468   
Stacked RNNs, 169   
standardize, 82   
start symbol, 389   
states, 450   
static embeddings, 118   
stationary process, 336   
stationary stochastic process, 51   
statistical MT, 287   
statistical significance MAPSSWE for ASR, 347 McNemar’s test, 348   
statistically significant, 71   
stative expressions, 450   
stem, 23   
Stemming, 5   
stemming, 24   
stop list, 294   
stop words, 61   
streaming, 345   
stride, 336   
structural ambiguity, 395   
stupid backoff, 49   
subdialogue, 313   
subjectivity, 481, 500   
substitutability, 408   
substitution operator (regular expressions), 11   
subword tokens, 18   
subwords, 21   
supervised finetuning, 249   
TAC KBP, 438   
Tacotron2, 351   
TACRED dataset, 437   
TAGGIT, 384   
tagset Penn Treebank, 365, 365 table of Penn Treebank tags, 365   
Tamil, 267   
tanh, 134   
target embedding, 122   
task error rate, 317   
Tay, 326   
teacher forcing, 164, 178, 210, 274   
technai, 362   
telephone-bandwidth speech, 335   
telic, 450   
temperature sampling, 209   
template, 245   
template filling, 435, 456   
template recognition, 456   
template, in IE, 456   
templates, 244   
temporal adverb, 364   
temporal anchor, 455   
temporal expression absolute, 452 metaphor for, 449 relative, 452   
temporal logic, 447   
temporal normalization, 453   
term in IR, 291 weight in IR, 291   
term frequency, 112   
term weight, 291   
term-document matrix, 106   
term-term matrix, 109   
terminal symbol, 389   
test set, 38 development, 39 how to choose, 39   
text categorization, 56 bag-of-words assumption, 58 naive Bayes approach, 57 unknown words, 61   
text normalization, 4, 16   
text summarization, 205   
text-to-speech, 332   
supervised machine learning, 57   
SVD, 130   
SVO language, 265   
Swedish, verb-framed, 267   
Switchboard, 333   
Switchboard Corpus, 13, 333, 335   
synchronous grammar, 287   
synonyms, 103   
syntactic disambiguation, 397   
syntax, 387 origin of term, 362   
TextTiling, 544   
tf-idf, 113   
The Pile, 211   
thematic grid, 463   
thematic role, 462 and diathesis alternation, 463 examples of, 462 problems, 464   
theme, 462   
theme, as thematic role, 462   
TimeBank, 451   
tokenization, 4 sentence, 24 word, 18   
Top-k sampling, 208   
top-p sampling, 209   
topic models, 104   
toxicity detection, 74   
training oracle, 419   
training set, 38 cross-validation, 69 how to choose, 39   
transcription of speech, 331 reference, 346   
transduction grammars, 287   
transfer learning, 223   
Transformations and Discourse Analysis Project (TDAP), 384   
transition probability role in Viterbi, 374   
transition-based, 416   
translation divergences, 265   
TREC, 308   
treebank, 392   
trigram, 38   
TTS, 332   
Turk, Mechanical, 332   
Turkish agglutinative, 267 part-of-speech tagging, 382   
turns, 311   
TyDi QA, 304   
typed dependency structure, 411   
types word, 14   
typology, 265 linguistic, 265   
unembedding, 200   
ungrammatical sentences, 391   
unigram name of tokenization algorithm, 270   
unit production, 397   
unit vector, 111   
Universal Dependencies, 413   
universal, linguistic, 264   
Unix, 5   
unknown words in part-of-speech tagging, 376 in text categorization, 61   
user-centered design, 325   
utterance, 13   
value, 188

value sensitive design, 326   
vanishing gradient, 135   
vanishing gradients, 172   
Vauquois triangle, 286   
vector, 107, 133   
vector length, 110

Vector semantics, 105   
vector semantics, 101   
vector space, 107   
vector space model, 107   
verb copula, 365 modal, 365 phrasal, 364   
verb alternations, 463   
verb phrase, 390   
verb-framed language, 267   
Verbs, 364   
Vietnamese, 267   
Viterbi and beam search, 275   
Viterbi algorithm, 26, 373 inference in CRF, 380   
VITERBI ALGORITHM, 373   
vocoder, 349   
vocoding, 349   
voice user interface, 325   
VSO language, 265   
wake word, 353   
Wall Street Journal Wall Street Journal speech recognition of, 355   
warping, 355   
wavefile format, 336   
WaveNet, 351   
Wavenet, 351   
weight tying, 165, 200   
well-formed substring table, 409   
WFST, 409   
wh-pronoun, 364   
wikification, 520   
wildcard, regular expression, 7   
Winograd Schema, 525   
Wizard-of-Oz system, 325   
word boundary, regular expression notation, 8 closed class, 363 definition of, 13 error rate, 334, 346 fragment, 13 function, 363, 383 open class, 363 punctuation as, 13 tokens, 14 types, 14   
word normalization, 23   
word segmentation, 18, 20   
word sense, 232   
word sense disambiguation, 232, see WSD   
word shape, 378   
word tokenization, 18   
word-word matrix, 109   
word2vec, 117   
wordform, 15 and lemma, 102 versus lemma, 15   
WordNet, 232   
wordpiece, 269   
WSD, 232   
Yonkers Racetrack, 49   
Yupik, 267   
z-score, 82   
zero anaphor, 505   
zero-shot, 246   
zero-width, 13   
zeros, 45