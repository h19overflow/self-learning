# Backpropagation. A Peek into the Mathematics of Optimization

![](images/478a8caeccfaf7d8aa980909eefff60b532264d45ab9fa285a9b816acb562bd5.jpg)

# 1 Motivation

In order to get a truly deep understanding of deep neural networks, one must look at the mathematics of it. As backpropagation is at the core of the optimization process, we wanted to introduce you to it. This is definitely not a necessary part of the course, as in TensorFlow, sk-learn, or any other machine learning package (as opposed to simply NumPy), will have backpropagation methods incorporated.

# 2 The specific net and notation we will examine

Here’s our simple network:

![](images/1a6d08ea3053e54ff5e71bcf79c6bea7bb07a52776b725df216f51a48f4c23b7.jpg)  
Figure 1: Backpropagation

We have two inputs: $x _ { 1 }$ and $x _ { 2 }$ . There is a single hidden layer with 3 units (nodes): $h _ { 1 }$ , $h _ { 2 }$ , and $h _ { 3 }$ . Finally, there are two outputs: $y _ { 1 }$ and $y _ { 2 }$ . The arrows that connect them are the weights. There are two weights matrices: w, and $\mathbf { u }$ . The w weights connect the input layer and the hidden layer. The $\mathbf { u }$ weights connect the hidden layer and the output layer. We have employed the letters $\mathbf { w }$ , and $\mathbf { u }$ , so it is easier to follow the computation to follow.

You can also see that we compare the outputs $y _ { 1 }$ and $y _ { 2 }$ with the targets $t _ { 1 }$ and $t _ { 2 }$ .

There is one last letter we need to introduce before we can get to the computations. Let $a$ be the linear combination prior to activation. Thus, we have: $\mathbf { a } ^ { ( 1 ) } = \mathbf { x w } + \mathbf { b } ^ { ( 1 ) }$ and $\mathbf { a } ^ { ( 2 ) } = \mathbf { h } \mathbf { u } + \mathbf { b } ^ { ( 2 ) }$ .

Since we cannot exhaust all activation functions and all loss functions, we will focus on two of the most common. A sigmoid activation and an L2-norm loss.

With this new information and the new notation, the output $y$ is equal to the activated linear combination. Therefore, for the output layer, we have $\mathbf { y } = \boldsymbol { \sigma } ( \mathbf { a } ^ { ( 2 ) } )$ , while for the hidden layer: $\mathbf { h } = \sigma ( \mathbf { a } ^ { ( 1 ) } )$ .

We will examine backpropagation for the output layer and the hidden layer separately, as the methodologies differ.

# 3 Useful formulas

I would like to remind you that:

$$
\operatorname { L 2 - n o r m } \operatorname { l o s s } \colon L = { \frac { 1 } { 2 } } \sum _ { i } ( y _ { i } - t _ { i } ) ^ { 2 }
$$

The sigmoid function is:

$$
\sigma ( x ) = \frac { 1 } { 1 + e ^ { - x } }
$$

and its derivative is:

$$
\sigma ^ { \prime } ( x ) = \sigma ( x ) ( 1 - \sigma ( x ) )
$$

# 4 Backpropagation for the output layer

In order to obtain the update rule:

$$
\mathbf { u }  \mathbf { u } - \eta \nabla _ { \mathbf { u } } L ( \mathbf { u } )
$$

we must calculate

$$
\nabla _ { \mathbf { u } } L ( \mathbf { u } )
$$

Let’s take a single weight $u _ { i j }$ . The partial derivative of the loss w.r.t. $u _ { i j }$ equals:

$$
\frac { \partial L } { \partial u _ { i j } } = \frac { \partial L } { \partial y _ { j } } \frac { \partial y _ { j } } { \partial a _ { j } ^ { ( 2 ) } } \frac { \partial a _ { j } ^ { ( 2 ) } } { \partial u _ { i j } }
$$

where i corresponds to the previous layer (input layer for this transformation) and j corresponds to the next layer (output layer of the transformation). The partial derivatives were computed simply following the chain rule.

$$
{ \frac { \partial L } { \partial y _ { j } } } = ( y _ { j } - t _ { j } )
$$

following the L2-norm loss derivative.

$$
\frac { \partial y _ { j } } { \partial a _ { j } ^ { ( 2 ) } } = \sigma ( a _ { j } ^ { ( 2 ) } ) ( 1 - \sigma ( a _ { j } ^ { ( 2 ) } ) ) = y _ { j } ( 1 - y _ { j } )
$$

following the sigmoid derivative.

Finally, the third partial derivative is simply the derivative of $\mathbf { a } ^ { ( 2 ) } = \mathbf { h } \mathbf { u } + \mathbf { b } ^ { ( 2 ) }$ . So,

$$
\frac { \partial a _ { j } ^ { ( 2 ) } } { \partial u _ { i j } } = h _ { i }
$$

Replacing the partial derivatives in the expression above, we get:

$$
\frac { \partial L } { \partial u _ { i j } } = \frac { \partial L } { \partial y _ { j } } \frac { \partial y _ { j } } { \partial a _ { j } ^ { ( 2 ) } } \frac { \partial a _ { j } ^ { ( 2 ) } } { \partial u _ { i j } } = ( y _ { j } - t _ { j } ) y _ { j } ( 1 - y _ { j } ) h _ { i } = \delta _ { j } h _ { i }
$$

Therefore, the update rule for a single weight for the output layer is given by:

$$
u _ { i j }  u _ { i j } - \eta \delta _ { j } h _ { i }
$$

# 5 Backpropagation of a hidden layer

Similarly to the backpropagation of the output layer, the update rule for a single weight, $w _ { i j }$ would depend on:

$$
\frac { \partial L } { \partial w _ { i j } } = \frac { \partial L } { \partial h _ { j } } \frac { \partial h _ { j } } { \partial a _ { j } ^ { ( 1 ) } } \frac { \partial a _ { j } ^ { ( 1 ) } } { \partial w _ { i j } }
$$

following the chain rule.

Taking advantage of the results we have so far for transformation using the sigmoid activation and the linear model, we get:

$$
\frac { \partial h _ { j } } { \partial a _ { j } ^ { ( 1 ) } } = \sigma ( a _ { j } ^ { ( 1 ) } ) ( 1 - \sigma ( a _ { j } ^ { ( 1 ) } ) ) = h _ { j } ( 1 - h _ { j } )
$$

and

$$
\frac { \partial a _ { j } ^ { ( 1 ) } } { \partial w _ { i j } } = x _ { i }
$$

The actual problem for backpropagation comes from the term $\frac { \partial L } { \partial h _ { j } }$ That’s due to the fact that there is no ”hidden” target. You can follow the solution for weight $w _ { 1 1 }$ below. It is advisable to also check Figure 1, while going through the computations.

$$
\begin{array} { c } { { \displaystyle \frac { \partial { \cal L } } { \partial h _ { 1 } } = \frac { \partial { \cal L } } { \partial y _ { 1 } } \frac { \partial y _ { 1 } } { \partial a _ { 1 } ^ { ( 2 ) } } \frac { \partial a _ { 1 } ^ { ( 2 ) } } { \partial h _ { 1 } } + \frac { \partial { \cal L } } { \partial y _ { 2 } } \frac { \partial y _ { 2 } } { \partial a _ { 2 } ^ { ( 2 ) } } \frac { \partial a _ { 2 } ^ { ( 2 ) } } { \partial h _ { 1 } } = } } \\ { { = ( y _ { 1 } - t _ { 1 } ) y _ { 1 } ( 1 - y _ { 1 } ) u _ { 1 1 } + ( y _ { 2 } - t _ { 2 } ) y _ { 2 } ( 1 - y _ { 2 } ) u _ { 1 2 } } } \end{array}
$$

From here, we can calculate $\frac { \partial L } { \partial w _ { 1 1 } }$ which was what we wanted. The final expression is:

$$
\frac { \partial L } { \partial w _ { 1 1 } } = \left[ ( y _ { 1 } - t _ { 1 } ) y _ { 1 } ( 1 - y _ { 1 } ) u _ { 1 1 } + ( y _ { 2 } - t _ { 2 } ) y _ { 2 } ( 1 - y _ { 2 } ) u _ { 1 2 } \right] h _ { 1 } ( 1 - h _ { 1 } ) x _ { 1 }
$$

The generalized form of this equation is:

$$
\frac { \partial L } { \partial w _ { i j } } = \sum _ { k } ( y _ { k } - t _ { k } ) y _ { k } ( 1 - y _ { k } ) u _ { j k } h _ { j } ( 1 - h _ { j } ) x _ { i }
$$

# 6 Backpropagation generalization

Using the results for backpropagation for the output layer and the hidden layer, we can put them together in one formula, summarizing backpropagation, in the presence of L2-norm loss and sigmoid activations.

$$
\frac { \partial L } { \partial w _ { i j } } = \delta _ { j } x _ { i }
$$

where for a hidden layer

$$
\delta _ { j } = \sum _ { k } \delta _ { k } w _ { j k } y _ { j } ( 1 - y _ { j } )
$$

Kudos to those of you who got to the end.

Thanks for reading.