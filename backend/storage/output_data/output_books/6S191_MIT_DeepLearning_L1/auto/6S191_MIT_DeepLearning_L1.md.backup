![](images/fa4c8f1e84ae0753880219e78bd92f53b6be3c1884dad0da129745936c465332.jpg)

# Introduction to Deep Learning

AlexanderAmini MIT Introduction to Deep Learning January 6,2025

# "Seeing" the progress of deep learning throughout the years

![](images/5b9a104bcbbe41dc786748043be5cc66fe092a2e2cb01d1b9b084cef09fa915e.jpg)  
2015 Goodfellow et al.

![](images/738ee22888d0425788762d8b123ce3beeefe5d8c32d50a4c1949b0c2f3e1e035.jpg)

![](images/5e3c4aed8908b5c8a3e4059f4482ec2c4af3c52ee7aa8787e9196052df54f81a.jpg)  
2018 Karras,Laine,Aila.

2020 MIT Intro to Deep Learning

Hieverybody,andwelcometo MIT6.S191 1.1Mviews 3months ago 88.2monthsago Thatiseasily thecleanest visualdeepfakelve ever seen.It must wOWwowwowiamamazed. havetakenages torender,because itjustlooksflawless.

![](images/91d0bd54ace32f99f33791a568e6cdbb3915fe1d6410c133db83cd3642282f6d.jpg)

# 2020

...creating this 2 minute video required...

2 hours of professional audio 50 hours of HD video Static,pre-defined script Over \$15K USD of compute

2020   
...creating this2 minute video requred..   
2 hours of professional audio   
50 hours of HD video   
Static,pre-defined script   
Over \$I5K USD of compute

![](images/d0b534376e5b6cbb076d9b20c313ff1186ce74752235eda9b50e6d52b7a912a3.jpg)

# What is Deep Learning?

# ARTIFICIALINTELLIGENCE

Any technique that enables computerstomimic human behavior

# MACHINE LEARNING

Abilityto learnwithout explicitly being programmed

# DEEP LEARNING

Extract patterns from data using neural networks

![](images/0b18ccf1960b8c558a1bcf9e6679baff558077d908f52d6e8d3f45e59f6e0abd.jpg)

![](images/0244d2a52ca8fff9f6d6e67c3268493a114e21d28fca4298b2c448edd8998b3f.jpg)

313472   
174235

# Teaching computers how to learn a task directly from raw data

# Lecture Schedule

Intro toDeepLearning

![](images/9782a83ced7534439130bd4cd9c6b1b9efdd53184a6b463029e0b5eee6ce398b.jpg)

![](images/95d89903dc83385818a5fc4d1179f219c61b8ca8e599ed7ab1d2a73302a893aa.jpg)

Deep Sequence Modeling

![](images/820536dbef5737d66ec2216ae7aa60147d04caabedec3df95e500d4448cd47ca.jpg)

Deep Learning in Python;Music Generation

Lecture 1   
/n.6,2025   
[Slides][Video] coming soon/   
Lecture 2   
/an.6,2025   
[Slides][Video] coming soon

Software Lab 1 [Code]

![](images/e2d7af203282f3dd8c9571e08da901ab38723e84794da8592244568014638b29.jpg)

Deep Computer Vision

![](images/15bf036de1e1e1747e79c1bb64e4ed786f8429c0607bfce0613d1f30f281f6a3.jpg)

![](images/8cfaf30a52b59b2a39ed52fb3eb1aad5153952cc950b88b88f6d4251b87d29ed.jpg)

Deep Generative Modeling

![](images/df3c237e0418e2b6da9dd4f0c34c7745c44fa6c9c03a3127ac493ee0047f6d12.jpg)

Facial Detection Systems

# Lecture 3

jan.7,2025 [Slides][Video] coming soon!

Lecture 4   
/an. 7,2025   
[Slides][Video] coming soon

Software Lab 2 [Paper][Code]

![](images/f3a1d2c736fa45cdbf3e53825cd6a235fb59ab801c456ce7d8e71cd1a0078375.jpg)

DeepReinforcement Learning

NewFrontiers

![](images/5cab16458ca8c167e9923df3353cb1848889ffe994d21c62846bb1ba0835e00c.jpg)

![](images/61695bf2598302cea540861deec913774acdf3e40c4a4a0dd515d996af3008f8.jpg)

Large Language Models

Lecture 5   
n.&,2025   
[Slides][video] coming soon!   
Lecture 6   
Jan.&,2025   
[Slides][Video] coming soon

Software Lab 3 [Code]

![](images/5572511be897f3831479714c107cd4cd6569d3f0478194915564334f5d169b0b.jpg)

Large Language Models

![](images/c36c41a4709d754255982a3eaf2902f3e1d74f98db6c944270a8295b8030a840.jpg)

Large Language Models(Il)

Lecture 7   
Jan.9,2025   
[Info][Slldes][Video] coming soon!   
Lecture 8   
Jan.9,2025   
[info][Slldes][Video]coming soon!

Final Project

![](images/f6cfbf28af411d64a678412d6559f359ba22a8e55d73cf2795efc24a56fca77c.jpg)

Work on final projects

Al in the Wild

![](images/daae0cca90830b46c996e954144700a140f42fc7c4394a8286473e9ce6ea4bff.jpg)

Al forBiology

![](images/82df56be783caa644beacf6a9beac5392a6b15b97105cbd635fb204955e0df44.jpg)

ProjectPresentations

![](images/e4aa7342ec7f209ce170b926fee931fd644c8a82f2685ab713d68ee5df48aab4.jpg)

Lecture 10   
/an. 10,2025   
[info][Slides]Video]coming soon!   
Lecture 9   
n. 10,2025   
[Info][Slides][Video] coming soont

Pitch your ideas,awards, and celebration!

Jan6-Janl0Ôºå2025 Lecture Breakdown Labs:Tensorflow&PyTorch Competitions Final Projects+Prizes!

# Updated Software Labs: TensorFlow and PyTorch

![](images/2498feaf4cc341596d7eac7a97aef7c30902cde586d00175f4a57a78af4034b4.jpg)

![](images/4ecf088f0788923b63f355177d2dadfc1213ee3b31a5e8bfdc6d07f77a60968d.jpg)  
PyTorch

TensorFlow

# Labs and Prizes

All due Thursday I/09 at II:59pm ET. Instructions: bit.ly/deeplearning-syllabus

![](images/16a66a32f549a36ee2d6765342c5812af2c328b23544ac2a1aa55655572fcea8.jpg)

# Music Generation

Build a neural network that can learn the genre of Irish folk songs and use it to generate brand new songs!

Prize:

![](images/2d8478a03b79b22ef586f0044e950cac0490f961e0193afb8360b11ff1d584a8.jpg)

# Labs and Prizes

All due Thursday I/09at II:59pm ET.Instructions:bit.ly/deeplearning-syllabus

![](images/a629ae7d0314cd7a5f7e996251b7ea1dfd10c5399e59a61d87da0506faccde6f.jpg)

# Computer Vision

Build a neural network that can detect and mitigate biases in computer vision facial recognition systems!

Prize:

![](images/367e5604434dece0e916731cef8ddc81e68305603484065cb86cc47797ee8aa2.jpg)

# Labs and Prizes

All due Thursday I/09 at II:59pm ET. Instructions: bit.ly/deeplearning-syllabus

![](images/17850743aa933243dc8ff4fc3bcdc3f0da62c325a4153a98c075f90e72fad99b.jpg)

# Large Language Models

Finetune the Gemma large language model (LLM) in a mystery style and evaluate withanAl judge!

Prize:

![](images/4bcf6f042bba98a576baa136b42e6a0cbc3509a5ed969d34fc246e96eb893c97.jpg)

# Project Pitch Competition

FridayI/Io. Instructions:bit.ly/deeplearning-syllabus

![](images/66d69be76d4a2a9dd1e414df2a85a85656c31312ab4cacc89f22e82e6f187798.jpg)

# Project Pitch Competition

Presenta novel deep learning research   
idea orapplication (5 minutes,strict)   
Presentationson Friday, JanI0   
Submit groups by WedI/08 II:59pm ET   
Submit slides byThuI/09II:59pmET   
Instructions: bit.ly/deeplearning-syllabus

# Prizes:

Gold: NVIDIA3070GPU

Silver: Smartwatch

Bronze: HDMonitor

ÈÄÅ

![](images/d4b3594ac9e1ba48396a5b43c97c985642c9a5a39bd1e57f34d4e3159a18b683.jpg)

![](images/ad946d81525778e5e2a763ad0ff7eae750e05813c2429faba746185d7609c465.jpg)

# Program Support

¬∑All lectures willbe held in person in 32-123   
¬∑Software labs $^ +$ office hours in 32-123   
¬∑ Piazza: piazza.com/mit/spring2025/6s19l ¬∑Useful for discussing labs &asking questions   
¬∑ Program Website: introtodeeplearning.com ¬∑ Lecture schedule ¬∑ Slides and lecture recordings ¬∑Software labs   
¬∑ Syllabus: bit.ly/6sl9l-syllabus   
¬∑ Labs: github.com/MITDeepLearning/introtodeeplearning   
¬∑ Email us: introtodeeplearning-staff@mit.edu

ÔºåÔºü

# Program Staff

![](images/103da07d349e74e67e989ec778eb25a90c73dcd5821082ad214814ad2e81e86c.jpg)

![](images/bab316a4cb6be74dd03f2115b18ab64a4d60c1eb9a764b759e27cf9581f9a1ce.jpg)

AlexanderAmini Lead Instructor

AvaAmini Lead Instructor

![](images/34f2e5874ab50b050307bb4d5dd8f31b7fc95f53cf873945d5960ed0c008f709.jpg)

![](images/9fbeb779c8084b35636da463fa93c749f651fc1f7227dd879fdbc0e077b8f8a7.jpg)  
Victory Yinka-Banjo LeadTA

Daniela Rus Director of CSAIL

# Program TAs

![](images/2d19c05793858f5f68ca0c9127a7aeb3883b7029035008290c80f99d897bc133.jpg)  
Maxi

![](images/127035238f38f10b8ef5c13b941c4a0781e31c0feda14d7249cb834be1b2da6b.jpg)  
Alex

![](images/54c73b1e60e4749d39d574442c8a295678ee32fa741f501b29781f27b4b12332.jpg)  
Sadhana

![](images/039f181f27507371549fadfcdbe68df93c1cf66f5b0e48381dfc80e68e3c1900.jpg)  
David

introtodeeplearning-staff@mit.edu

# Thanks to Sponsors!

![](images/7cfe5ecb8cac9ffe8e6a5b7f6bad682ff8bc0418a324102bc35ee4ff9de5bdcf.jpg)

# Why Deep Learning and Why Now?

# Why Deep Learning?

Hand engineered featuresare time consuming,britle,and not scalable in practice Can we learn the underlying features directly from data?

![](images/9cbb8ca3aa014a1dd27ed42cfd93779e60c052c224311e6a38f99f270490e893.jpg)  
LowLevel Features   
Lines& Edges

![](images/7b2490d0f92d224fc17477ef11cea7ebf3bba4f3005e8d151b96637298e0df50.jpg)  
Mid Level Features   
Eyes&Nose&Ears

![](images/f9ebbe7a0b08f2de76d3ea1a69b8ca7c5b3973c63ff97dbd3c54e4f15c0c784b.jpg)  
High Level Features   
Facial Structure

# Why I yNow?

Neural Networks date back decades,so why the dominance?

Stochastic Gradient Descent

1958

Perceptron LeamableWeights

Backpropagation Multi-Layer Perceptron

Deep Convolutional NN Digit Recognition

# 1. Big Data

¬∑Larger Datasets ¬∑Easier Collection & Storage

IMGENET

![](images/b7bb27bfb1370ef85213b83dc9a82000b92cc8d7b87c3baa713ae26917ba7634.jpg)

# 2. Hardware

¬∑Graphics Processing Units (GPUs) Massively Parallelizable

![](images/e629e95bb1c56f79ecfaf046df147d92e32093640e79de517b52f93aa66c2c80.jpg)

# 3. Software

Improved Techniques ¬∑ NewModels . Toolboxes

1K X

# The Perceptron The structural building block of deep learning

# The Perceptron: Forward Propagation

![](images/6bf4f35665dceed21c7d7e08b05ec095beb347d3dbbbabb8ae470f569a8f3786.jpg)

# The Perceptron: Forward Propagation

![](images/150fd1788dd7eaea227bbcc6cb8f388663737a09904fc813487c708b489495ee.jpg)

# The Perceptron: Forward Propagation

1 Wo S A $\widehat { y } \ = \ g \ \left( \ w _ { 0 } + \sum _ { i = 1 } ^ { m } x _ { i } \ w _ { i } \right)$ x1 W1   
W2 ¬£ y=gÔºàwo+XTWÔºâ   
x2 wm where: $\pmb { X } = \left[ \begin{array} { c } { x _ { 1 } } \\ { \vdots } \\ { x _ { m } } \end{array} \right] \mathrm { \overset { } { a n d } } \pmb { W } = \left[ \begin{array} { c } { w _ { 1 } } \\ { \vdots } \\ { w _ { m } } \end{array} \right]$ xm   
Inputs Weights Sum Non-Linearity Output

# The Perceptron: Forward Propagation

![](images/7ef424bdcf7298065f78782c793b2b7025b120e45a76b42bac726de1f9a0b785.jpg)

Activation Functions $\hat { y } = g \left( \mathbf { \nabla } w _ { 0 } + \mathbf { \nabla } \mathbf { X } ^ { T } \mathbf { W } \right)$

¬∑Example:sigmoid function

$$
g \left( z \right) = \sigma \left( z \right) = \frac { 1 } { 1 + e ^ { - z } }
$$

![](images/ee3e9b80bbd396fb0924066991f8eab56c6ee13982230e81c52b0785a4e51eac.jpg)

# Common Activation Functions

Sigmoid Function

Hyperbolic Tangent

Rectified Linear Unit (ReLU)

![](images/7895c25d0159b31c01ab5e1b3abc62239128263115a5ca4ac688c99bd7fb7acf.jpg)

![](images/4ee5907e8d2de0fe1ffafed4e0c90fdf4eb6854f4eeba7e5bc0f48d6cac97999.jpg)

![](images/925417c869719710bda92cbaa12c17e643dca18fbc8d94649ba3e5a3f810623b.jpg)

$$
g \left( z \right) = \frac { 1 } { 1 + e ^ { - z } }
$$

$$
g ^ { \prime } ( z ) = g ( z ) ( 1 - g ( z ) )
$$

![](images/6e4a657ea09c9538b63b6a44535fe620695952fce6a08bf25c589f86bf3fff76.jpg)

![](images/78dc820f26095fa2ad878a7e951c2cb57f447ca140bb10372296ed49da54b551.jpg)

![](images/a963abc77943a6270bb0ef7b9e09cb537552df94da3464c5ac07416cf87efb0f.jpg)

1F tf.math.tanh(z) torch.tanh(z)

1F tf.nn.relu(z) torch.nn.ReLU(z)

# Importance of Activation Functions

Thepurpose ofactivation functions is to introduce non-linearities into the network

![](images/9903220c2252a3f214046e157e6232bcb8f6bbcdd6826a12462b7f481732915d.jpg)

What if we wanted to build a neural network to distinguish green vs red points?

# Importance of Activation Functions

hepurposeofactivation functions is to introducenon-linearities into the network

![](images/d937d79250b7e780d143a7ad4a4cce66d43baa4db55103582ae3b8b531bae868.jpg)

Linearactivation functions produce linear decisions no matter the network size

# Importance of Activation Functions

hepurposeofactivation functions is to introduce non-linearitiesinto thenetwork

![](images/acc777c73a1696b959444bfd510726fa1fff5b41c68ba7ba7921ba3549d58c6b.jpg)

![](images/bf3e0e07a5ef3e3248c8d8ae19c396cf2e391be464eb0a0e0b5939f1539523e7.jpg)  
Non-linearitiesallowustoapproximate arbitrarily complexfunctions

Linearactivation functions produce linear decisions no matter the network size

# The Perceptron: Example

![](images/8267183efeee22d8599c0d2caf19c4e1304adfcb9dca15df7c3f46980651f7aa.jpg)

We hae: $w _ { 0 } = 1$ $\pmb { W } \ = \ \bigl [ _ { - 2 } ^ { 3 } \bigr ]$

$$
\begin{array} { r l } & { \hat { \boldsymbol { y } } = \boldsymbol { g } \left( \boldsymbol { w } _ { 0 } + \boldsymbol { X } ^ { T } \boldsymbol { W } \mathrm { \Sigma } \right) } \\ & { \mathrm { \Sigma } = \boldsymbol { g } \left( 1 + \left[ \boldsymbol { x } _ { 1 } \right] ^ { T } \left[ \begin{array} { c } { 3 } \\ { - 2 } \end{array} \right] \right) } \\ & { \hat { \boldsymbol { y } } = \boldsymbol { g } \left( 1 + 3 \boldsymbol { x } _ { 1 } - 2 \boldsymbol { x } _ { 2 } \right) } \end{array}
$$

This is just a line in 2DÔºÅ

# The Perceptron: Example

$\hat { y } = g ( 1 + 3 x _ { 1 } - 2 x _ { 2 } )$ S1 x2 $\sum \limits _ { i = 1 } ^ { \infty } i = 1$ 0 0= -2x2 3 x1 1+3x1 -2 x1 x2

# The Perceptron: Example

![](images/23d6be4b48f6c24eca46ac317e815404655ca0f3605059509c8cf2cf12012144.jpg)

# The Perceptron: Example

![](images/5469b1df394acf54889949027146ced98e05aadf7ab5543be7d65f91b4f3a57b.jpg)

# Building Neural Networks with Perceptrons

# The Perceptron: Simplified

$$
\hat { y } = g \left( \mathbf { \nabla } w _ { 0 } + \mathbf { \nabla } \mathbf { X } ^ { T } \mathbf { W } \right)
$$

![](images/1b3019c3a5612a1bb534c44baee6a1bb9efa403d807c934fb8f1af9a095e7a8b.jpg)

# The Perceptron: Simplified

![](images/7dab6271316ce1250f3e022729456b8c428f67e2c8d5f10387b5478892ad4a75.jpg)

# Multi Output Perceptron

Becauseallinputs are densely connected toalloutputs,these layers are called Dense layers

![](images/5be4c1adb0273257305c7ddb143cc425b531a0d98f5ae63d5dbe557540783faa.jpg)

# Dense layer fromscratch

# 1F

class MyDenseLayer(tf.keras.layers.Layer): def_init_(self,input_dim,output_dim) super(MyDenseLayer,self)._init__()

self.w self.add_weight([input_dim,output_dim]) self.b self.add_weight([l,output_dim])

def call(self,inputs):

-tf.matmul(inputs,self.w) self.b output tf.math.sigmoid(z) return output

class MyDenseLayer(nn.Module): def init(self, input_dim,output_dim)Ôºö super(MyDenseLayer, self)._init_()

Initialize weights and bias   
self.W=nn.Parameter(torch.randn(input_dim, output_dim,requires_grad=True)   
self.b=nn.Parameter(torch.randn(l,output_dim, requires_grad=True)

# def forward(self,inputs):

#Forward propagate the inputs Z torch.matmul(inputs,self.W)+self.b

Feed through a non-linear activation output torch.sigmoid(z) return output

# Multi Output Perceptron

Because allinputsare densely connected toall outputs,these layers are called Dense layers

![](images/b25627a02c98dbf8df989721f00c76be5314b0d6c9d3629f6a2a1b95ecd62d39.jpg)

# Single Layer Neural Network

![](images/d57473dbf05e3ec8d4c11c969b8ee63e25f64ab445ade96c54d860daa7894514.jpg)

# Single Layer Neural Network

![](images/d1bb6d228917cca121042cb7e8bcf26d4693c275d9a279f04cc3889edd7eb8a1.jpg)

# Multi Output Perceptron

import tensorflowastf   
model-tf.keras.Sequential([ S191 tf.keras.layers.Dense(n), tf.keras.layers.Dense(2)   
1Ôºâ ÂÖ¨1   
from torch import nn x1   
model nn.Sequential( Z2 nn.Linear(m,n), nn.ReLU(), x2 nn.Linear(n,2Ôºâ Z3 2 M Zn Inputs Hidden Output

# Deep Neural Network

Zk,1 1   
x1 ZK,2 y   
x2 ZK,3   
MIT ZK,nk Hidden Output $z _ { k , i } = w _ { 0 , i } ^ { ( k ) } + et { } { ^ { n _ { k - 1 } } } \sum _ { j = 1 } ^ { n _ { k - 1 } } g ( z _ { k - 1 , j } ) w _ { j , i } ^ { ( k ) }$

# Deep Neural Network

![](images/528508b48fdb12fe78f0fd78a001a8195b5b24e4aec0a9ee94e1e9fe1c9ba8dc.jpg)

import tensorflow astf model=tf.keras.Sequential([ tf.keras.layers.Dense(n), tf.keras.layers.Dense(n2), tf.keras.layers.Dense(2)

from torch import nn   
model = nn.Sequential( nn.Linear(m,n1)Ôºå nn.ReLU(), nn.ReLU(), nn.Linear(nK,2)

# Applying Neural Networks

# Example Problem

Will pass this class?

Let's start with a simple two feature model $x _ { 1 } =$ Number of lectures you attend X2= Hours spent on the final project

# Example Problem:Will I pass this class?

$\scriptstyle x _ { 2 } =$ Hours :.19   
spent on the   
final project Legend Pass Fail   
M. $x _ { 1 } =$ Number of lectures you attend

# Example Problem:Will I pass this class?

$\scriptstyle x _ { 2 } =$ Hours ¬∑.19   
spent on the   
final project Legend Pass [] Fail   
M. $x _ { 1 } =$ Number of lectures you attend

# Example Problem: Will  pass this class?

![](images/3b1690621180b5b7761c0c2b93252be19bb5d1f95c9c6de9fdbec515496d5e4d.jpg)

Predicted: 0.1

# Example Problem: Will  pass this class?

![](images/5fae7c0c010e7abcffe92ec5049b13ed6ac2fc5d232916d88cd5357c36fe90a4.jpg)

# Quantifying Loss

Theloss of our networkmeasures the cost incurred from incorrect predictions

Z1 x1 x(1)=[4,5] Predicted:0.1 Z2 y Actual: 1 x2 MI L(f(xi;W),y(ùëñ)) Z3 Predicted Actual

# Empirical Loss

Theempirical lossmeasures the total loss over our entire dataset

![](images/f0a5a7a36da40f4921076dce719ce50e3a18a99f712d64520cc5660f460939eb.jpg)

Also known as:

Objective function . Cost function Empirical Risk

![](images/642ca63b55ee7f70809474c9c946961dbe0272959d344e64d8532394bb643af0.jpg)

$$
\boldsymbol { J } ( \boldsymbol { W } ) = \frac { 1 } { n } { \sum } _ { i = 1 } ^ { n } \mathcal { L } \big ( \underline { { f \big ( \boldsymbol { x } ^ { ( i ) } ; \boldsymbol { W } \big ) } } , \underline { { y ^ { ( i ) } } } \big )
$$

# Binary Cross Entropy Loss

Cross entropy losscan be used with models that output a probability between O and l

f(x) y 5225 x1 Z1 [0.1] √ó√óv ‰∏Ä X= x2 Z2 y 0.8 0.6 1‚Ä¶ 0 Ôºö Z3 10 7 n y@1og(f(x‚ë•;w)Ôºâ+(1-y@)log(1-f(x‚ë†;)Ôºâ n Actual Predicted Actual Predicted

# Mean Squared Error Loss

Mean squared error losscan be used with regression models that output continuous real numbers

f(x) y 5225 x1 ZI 308 X= Z2 85 x2 Ôºö Ôºö MI $J ( W ) = \frac { 1 } { n } { \sum } _ { i = 1 } ^ { n } \underbrace { \Big ( y ^ { ( i ) } - f \big ( x ^ { ( i ) } ; W \big ) \Big ) ^ { 2 } } _ { }$ Z3 Final Grades (percentage) ActualPredicted

# Training Neural Networks

# Loss Optimization

We want to find the network weights that achieve the lowest loss

$$
W ^ { * } = \underset { W } { \mathrm { a r g m i n } } \frac { 1 } { n } { \sum _ { i = 1 } ^ { n } } \mathcal { L } \big ( f \big ( x ^ { ( i ) } ; W \big ) , y ^ { ( i ) } \big )
$$

$$
W ^ { * } = \underset { W } { \mathrm { a r g m i n } } J ( W )
$$

# Loss Optimization

We want to find the network weights that achieve the lowest loss

$$
W ^ { * } = \underset { W } { \mathrm { a r g m i n } } \frac { 1 } { n } { \sum _ { i = 1 } ^ { n } } \mathcal { L } \big ( f \big ( x ^ { ( i ) } ; W \big ) , y ^ { ( i ) } \big )
$$

$$
W ^ { * } = \underset { W } { \mathrm { a r g m i n } } J ( W )
$$

$$
{ \pmb W } = \left\{ { \pmb W } ^ { ( 0 ) } , { \pmb W } ^ { ( 1 ) } , \cdots \right\}
$$

# Loss Optimization

$$
W ^ { * } = \underset { W } { \mathrm { a r g m i n } } J ( W )
$$

Remember: Our loss isa function of the network weights!

![](images/2b5204c597ce3c5ec93a45c38b8d5e5914966cbbd182ec8c7b15144e2fc116b4.jpg)

J(Wo, W1)

# Loss Optimization

Randomly pick an initial $( w _ { 0 } , w _ { 1 } )$

![](images/9961fe1fe06942e4a65b84e52e78653bb47b421682c730b4bef5e0b5acdad84f.jpg)

J(Wo, W1)

# Loss Optimization

Compute gradient,J(W)

![](images/a6fe8808f8259032480bd7b22682076319312ac7a206bac261925c7ccd6f2a88.jpg)

J(Wo, W1)

# Loss Optimization

Take small step in opposite direction of gradient

![](images/28ae492dec0d2c30aac0743d9f82ee015e8ab14a9807a85555d72a216814ddd7.jpg)

# Gradient Descent

Repeat until convergence

![](images/36c26b323615b006cc0b6a484914f886745328bd268a08fd4dadbdc7ac9630b3.jpg)

J(Wo,W1)

# Gradient Descent

# Algorithm

I.Initialize weights randomly ${ \sim } \mathcal { N } ( 0 , \sigma ^ { 2 } )$   
2.Loop until convergence:   
3. Compute gradient, $\textstyle { \frac { \partial J ( W ) } { \partial W } }$   
4. Update weights, $\pmb { W } \gets \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial \pmb { W } }$   
5.Retum weights

# Gradient Descent

# Algorithm

1. Initialize weights randomly ${ \sim } \mathcal { N } ( 0 , \sigma ^ { 2 } )$   
2.Loop until convergence:   
3. Compute gradient, $\textstyle { \frac { \partial J ( W ) } { \partial W } }$   
4. Update weights, $\pmb { W } \gets \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial W }$   
5.Return weights

import tensorflow as tf weights=tf.Variable([tf.random.normal()])

while True: #loop forever withtf.GradientTape()asgÔºö loss=compute_loss(weights) gradient=g.gradient(loss,weights)

weights=weights-lr\*gradient

# Gradient Descent

# Algorithm

1. Initialize weights randomly ${ \sim } \mathcal { N } ( 0 , \sigma ^ { 2 } )$   
2.Loop until convergence:   
3. Compute gradient, $\textstyle { \frac { \partial J ( W ) } { \partial W } }$   
4. Update weights, $\pmb { W } \gets \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial W }$   
5.Return weights

import tensorflow as tf weights=tf.Variable([tf.random.normal()])

while True: #loop forever withtf.GradientTape() as gÔºö loss= compute_loss(weights) gradient-ggradient(loss,weights)

# Computing Gradients: Backpropagation

![](images/a0de3448dc9d4708c37a249351e037cc848c2b65168c11355448beb7c7934bfc.jpg)

How does a small change in one weight (ex. $w _ { 2 } ,$ afect thefinal lossJ(W)

# Computing Gradients: Backpropagation

![](images/998979d7ad60634a8c5e7ceecf5f13af568c4dadbb8677b03c512d4d1dddd6ee.jpg)

Let's use the chain rule!

# Computing Gradients: Backpropagation

![](images/a9db52b2b29817393923d3026d1456fe41c6ec48e190c7e9dd90bf36669836e5.jpg)

# Computing Gradients: Backpropagation

![](images/10d02420eb9b0856c465ac666a05836e9cd73a2b16f39503ccc75b2948ded74f.jpg)

# Computing Gradients: Backpropagation

![](images/2d8f32444901b03a0a1fa98916c80f674d52f523356901ecb9e82d9d78b05d53.jpg)

# Computing Gradients: Backpropagation

![](images/f1723323cc06a4672cfa2a136b8a1ebd382c80ba892122d77412b133f751efa0.jpg)

Repeatthis forevery weight inthenetworkusinggradients from later layers

# Neural Networks in Practice: Optimization

# Training Neural Networks is Dificult

![](images/f5cf5677cb74fdd51489cc95d17c2d8e850ec1b71b3cb46259949db172ade11f.jpg)

# Loss Functions Can Be Difficult to Optimize

# Remember:

Optimization through gradient descent

$$
\pmb { W }  \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial \pmb { W } }
$$

# Loss Functions Can Be Diffcult to Optimize

# Remember:

Optimization through gradient descent

How can we set the learning rate?

# Setting the Learning Rate

Small learningrateconverges slowlyand gets stuck in false local minima

250 200 MIT 6.S19 Initialguess 5 -2 -1 0 W

# Setting the Learning Rate

Large learningrates overshoot,becomeunstableand diverge

250200MIT 6.S19Initial guess3 -2 -1 0W

# Setting the Learning Rate

Stable learningrates converge smoothlyand avoid local minima

250 200 MUT 6.S19 Initialguss 3 -2 -1 0 W

# How to deal with this?

# Idea 1Ôºö

Try lots of diferent learning rates and see what works"just right"

# How to deal with this?

Idea 1Ôºö

Try lots of different learning rates and see what works"just right"

# Idea 2:

Do something smarter! Design an adaptive learning rate that "adapts"to the landscape

# Adaptive Learning Rates

¬∑Learning rates are no longer fixed Can be made larger or smaller depending on: ¬∑how large gradient is how fast learning is happening size of particular weights ‚ñ† etc..

# Gradient Descent Algorithms

# Algorithm

TF Implementation

Torch Implementation

# Reference

Kiefer&Wolfowitz,1952.

Kingma et al.,2014.

. SGD Ôºö Adam Adadelta ¬∑ Adagrad . RMSProp

1F tf.keras.optimizers.SGD   
1F tf.keras.optimizers.Adam   
1F tf.keras.optimizers.Adadelta   
1F tf.keras.optimizers.Adagrad   
1F tf.keras.optimizers.RMSProp

torch.optim.SGD torch.opt.im.Adam torch.optim.Adadelta torch.optim.Adagrad torch.opt.im.RMSProp

Zeiler et al.,2012.

Duchi et al.,2011.

# Putting it all together

import tensorflowas tf   
model=tf.keras.Sequential([...])   
# pick your favorite optimizer Can relace with   
optimizer=tf.keras.optimizer.SGD() optimizer!   
while True:# loop forever #forward pass through the network prediction =model(x) withtf.GradientTape()as tape: # compute the loss loss=compute_loss(y,prediction) #update the weights using the gradient grads=tape.gradient(loss,model.trainable_variables) optimizer.apply_gradients(zip(grads,model.trainable_variables)))

# Neural Networks in Practice: Mini-batches

# Gradient Descent

# Algorithm

I.Initialize weights randomly ${ \sim } \mathcal { N } ( 0 , \sigma ^ { 2 } )$   
2.Loop until convergence:   
3. Compute gradient, $\textstyle { \frac { \partial J ( W ) } { \partial W } }$   
4. Update weights, $\pmb { W }  \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial W }$   
5.Return weights

![](images/b7a12434e0a29b64d068e528d3e358f28b819baf9966e1afe89bfad19cb83dec.jpg)

# Gradient Descent

# Algorithm

I.Initialize weights randomly\~N(0,œÉ¬≤)   
2.Loop until convergence:   
3. Compute gradient,j(W 1 - 2‚àí 0-   
4. Update weights, $\pmb { W }  \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial \pmb { W } }$ -1‚àí 0 0.2 -3‚Üí 1 0.9 0.4   
5. Return weights M 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 ‚àó0.8 0.6 0 Can be very computationally intensive to compute!

# Stochastic Gradient Descent

# Algorithm

I.Initialize weights randomly\~N(0,œÉ2)   
2.Loop until convergence:   
3. Pick single data point i   
4. Compute gradient, $\frac { \partial J _ { i } ( \pmb { W } ) } { \partial \pmb { W } }$   
5. Update weights, $\pmb { W } \gets \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial W }$   
6.Return weights

![](images/236d3629582a83ed885637ea4365d6c1a146324d07eed3caa82131639f541924.jpg)

# Stochastic Gradient Descent

# Algorithm

I. Initialize weights randomly \~N(0,œÉ2)   
2.Loop until convergence:   
3. Pick single data point i   
4. Compute gradient,aj(W)   
5. Update weights, $\pmb { W } \gets \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial W }$   
6. Return weights Easy to compute but

![](images/c02a10cec67363553de1299bc1630e50a93ed3b2d5ebb903244c621fa0b264c3.jpg)

very noisy (stochastic)!

# Stochastic Gradient Descent

# Algorithm

I.Initialize weights randomly ${ \sim } \mathcal { N } ( 0 , \sigma ^ { 2 } )$   
2.Loop until convergence:   
3. Pick batch of $B$ data points   
4. Compute gradient,W) 1R=1W   
5. Update weights, $\pmb { W } \gets \pmb { W } - \eta \frac { \partial J ( \pmb { W } ) } { \partial W }$   
6.Return weights

![](images/b7f7d17d512322337adc1192e0a43104df4483ccfec5b569ed0d4e3e19553fe3.jpg)

# Stochastic Gradient Descent

# Algorithm

I.Initialize weights randomly ${ \sim } \mathcal { N } ( 0 , \sigma ^ { 2 } )$   
2.Loop until convergence:   
3. Pick batch of $B$ data points   
4. Compute gradient,(W) 1K-1 aw   
5. Update weights,W ‚Üê W-nW   
6.Return weights

![](images/bfaf589bb293f3eb2ab79c98660ff1d709077265737de71007138eb608108a10.jpg)

Fast to compute and a much better estimate of the true gradient!

# Mini-batches while training

More accurate estimation of gradient Smoother convergence Allows for larger learning rates

# Mini-batches while training

More accurate estimation of gradient Smoother convergence Allows for larger learning rates

# Mini-batches lead to fast training!

Can parallelize computation $^ +$ achieve significant speed increases on GPU's

# Neural Networks in Practice: Overfitting

# The Problem of Overfiting

![](images/c7eee06949e6514bdead36fea6a6c697a6d4e4e36e39a14ccbdd795e75102d30.jpg)

Model does not have capacity to fully learn the data

Too complex,extra parameters, doesnot generalizewell

# Regularization

What is it?

echnique thatconstrains ouroptimization problem to discouragecomplex model:

# Regularization

What is it? Technique that constrains our optimization problem to discourage complex models

# Why do we need it?

Improve generalization of our model on unseen data

# Regularization I: Dropout

¬∑During training,randomly set some activations to 0

![](images/23c49e8c59c3e7f712c9d8a0d5b074b45fcdfd5a0442e7cfa6b09714888bbf49.jpg)

# Regularization I: Dropout

¬∑ During training,randomly set some activations to 0

¬∑Typically'drop'50%of activations in layer . Forces network to not rely on any l node

tf.keras.layers.Dropout(p0.5)

![](images/d36271f8a18e0ad1744652663669489fc13485a1d97867635b3229cdd552a30e.jpg)

torch.nn.Dropout(p-0.5)

![](images/03eb1e836eb622a69d9a3115af5a98229e06177e0225cf7849218d324d305adb.jpg)

# Regularization I: Dropout

¬∑ During training,randomly set some activations to 0

¬∑Typically'drop'50%of activations in layer . Forces network to not rely on any l node

tf.keras.layers.Dropout(p0.5)

![](images/a6ad014318982ef3af7aacb1ee8d9efa99579e6f790a0cd203801853e4af188a.jpg)

torch.nn.Dropout(p0.5)

![](images/0a8850ec473745f2d170609f094b931f553bb8343f1cd802d4c5c3997c7a6e82.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overfit

![](images/3142255da170dcd9ec63f65b45d20ee2d6428543a21f7288cc1b99264108ae96.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overfit

![](images/3dcdfecdc07e2f7ae2ab7ea5f1567e4f903d72d844321e5fa2edac715cdf7f7d.jpg)

![](images/fdcd8bf05f33775a3a668c08f39f1ec659e8ae175c568588ef82122d4b73e2d5.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overfit

![](images/c9475231c0ea134ace72c9a256ace4c0e1520dbfab17f2fb7be1d038ed9bef36.jpg)

![](images/29b19c564d7b231498b5c6d782a66162ce0efb50abd3abc759d16e1d8fb13968.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overfit

![](images/2c4ed665f1a79dd552cf76151c446f923d5968ad12363df02c4f214b8bf2af4d.jpg)

![](images/881883d1ae9ab579fb7c856f288ad92a992ad5ff068dc621f9e9f4f2a6e80aa0.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overft

![](images/fb13a65072c580f93b23a62b2cb5e9e6e683905f8abba105f9f40e2c42f88671.jpg)

![](images/913a364fca765aa6b8d588bb2518eed66da971b410b1dc4003e839ebe5c45c26.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overft

6.S1 Legend Loss Testing Training Training Iterations

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overft

![](images/c5617456728c99bb31c1ad8c8bcedc4fee92325e7a58ac1610eed5ded0b218a9.jpg)

# Regularization 2: Early Stopping

¬∑ Stop training before we have a chance to overfit

![](images/f09f86f4f927d6d5f7dd0fe3cb149ababd2c6672311adbfaf342912451e3f29a.jpg)

# Core Foundation Review

# The Perceptron

# NeuralNetworks

# Training in Practice

¬∑Structural building blocks ¬∑Nonlinearactivation functions

¬∑Stacking Perceptrons to formneural networks ¬∑Optimization through backpropagation

¬∑ Adaptive learning Batching Regularization

![](images/2bf9c27f72b1b9f0ecf137313eba152fc5ad654adbda38b548576e4ee34f9c28.jpg)

![](images/5005a2c193640fd00e3c355af617dfa836bf4fe80e82df84e104011c12af224f.jpg)

![](images/4c8844efe7cae4b70c2d297792a35b2589fb1a22f3681ed74bd0ee1a97b25df8.jpg)

# MIT Introduction to Deep Learning

Labl: Introduction to Deep Learning in Python and Music Generation with RNNs

Link to download labs: http://introtodeeplearning.com#schedule

1.Open the lab in Google Colab Startexecuting code blocks and filing in the #TODOs 3ÔºéNeed help? Come to 32-123!